# Webizen - Root Concepts

## Software Development Concepts

### Entity Relations 

This is a more technical expansion of the concepts noted in the document about [[Centricity]].

Primary Inferred Entity:  Human

The concept of Human Centric - is that the informatics environment is augmented to primarily serve the human user.  Informatics might relate to groups of humans, in various ways; but fundamentally, the 'operating (natural) agents' in our 'biosphere' are human beings. 


Primary Interface:  Binary (Webizen)
The primary interface is a computing device that reduces instruction-sets to binary commands that then process through computational 'gates', etc. 

The 'webizen' may use various forms of abstration, but at the end of the day; it ends-up binary. 

Secondary Interface - Semantic Web

The secondary interface target is semantic web; although, if the models are not considered to be the best way of going about addressing a particular problem, then semantic web methods are not the primary (native webizen) method of doing something.


### Duality

<iframe width="560" height="315" src="https://www.youtube.com/embed/sUad0ZL-nBU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

There are various considerations relating to the duality of requiring a machine to process inputs from a human being, in-order for the effect of a human being to be made tangible, or able to be 'sensed'.  In-order to support [[FreedomOfThought]] this human-machine interface needs to be made private and entirely confidential; whereby the trigger for any external rules is a sharing event with any other agent, other than those that represent the individual themselves.  

In otherwords - a person becomes subject to scrutiny when there is an action of sharing something with someone else - not their 'internal' process of thinking about something, ie; a problem, which should not be unduely limited - the concern being, that human beings should be entitled to safely think about anything and the process of doing so is a requirement for enabling the human mind to consider the concepts / topics / issues.  Yet, when 'thought' turns into some sort of 'action' that relates to other people, then an issue that has some sort of implication upon other (ie: socially) becomes relavent in some way; without getting into the semantics of how that might be investigated and/or considered more later.

The machine side of the equation, doesn't 'understand' in a human like way.  This consideration may lead to various sorts of philosphical discussions about consciousness, sould and spirit - which is not something that is sought to be considered herein.

Rather; that there needs to be a computational model for categorising basic natural language, and performing computational logic processes, that end-up being processed as complex binary computations - effectively using mathematics.

### Investigative Hypothesis

I have a theory about a causal relationship between the evolution of computing, the hierarchy of 'values' (ie: matters / issues / use-cases of importance) that have been historically associated with these works; and, repercussive 'ripple effects' that are exhibited through the use of these systems; rendering, subtle but significant implications upon humanity at large. 

In the webizen designs, many factors relating to areas of consideration associated with the structure of informatics are being reconsidered; rightly or wrongly, hypothesis are defined in some way - that may be correct or incorrect or have importance nuances that are attributable, and thereafter processed - as part of a development process that seeks to revise, support a means to reconsider structures - from a present point of view (ie: it is now the year 2023); and then scaffold 'fit for purpose' methodologies, without necessarily requiring 1st class support for functional requirements that relate to systems that may have been designed in the 1960s or earlier (indeed also, later also - noting the practical process of scaffolding upon existing works, etc).  

### Socialisation Structures

There are 'socialisation' structures in both the computational environment; as does in-turn relate to the human environment, and in-turn implications with respect to our biosphere.

Importantly also; there are [[GuardianshipRelations]] and [[GuardianshipSemantics]] requirements. 


### The Role of Vocabulary

#### Human Users

The focus for Human Users presently is english, as that's the only natural spoken / written language i am competent in.  Phonetics, tonality and other factors play a role with spoken speech; punctuation and grammar plays a role with written natural language alongside spelling.

Courts use natural langague to process disputes, form common-sense and defined outcomes.

#### Computational Interference

There are many negative implications that develop when even basic AI related processes (ie: spellcheck helpers) re:define a term to another term, that changes the meaning of the sentence or concept that was intended to be communicated by the human being. 

These sorts of events create logs; and, complicate assessments - noting - i am not sure of a command that is `.wellknown` that allows a human being to assert a correction for an action that was made by a machine; or, any existing syntax that ensures a machines interference with a persons words is able to be logged, as part of that data-file.  

### Software Agents

Software agents fundamentally review english text on a computational basis - ending up with processes parsed in binary. 

However; there are various well-defined command structures that are in-effect a machine vocabulary that is contextual to the software / hardware environment. 

an instruction set, in-effect...

#### Human Centric Software Agent Considerations

Therein - one of the objective [[SafetyProtocols]] related considerations should include the implications of computer generated deterministic content or artifacts interfering with AI modelling techniques - due to AI related inputs that are not properly labelled.

The example being - 'autocorrect' issues...  changing the meaning of a sentence, and thereafter being employed to determine attributes about the natural agent. 

#### Natural Language Modelling 

The language - english - has evolved over 