<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Tutorials Demystifying Deep Reinforcement Learning (Part1) http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/ Deep Reinforceme…"/><meta data-react-helmet="true" name="twitter:title" content="Reinforcement Learning"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-28T19:22:29.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Tutorials Demystifying Deep Reinforcement Learning (Part1) http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/ Deep Reinforceme…"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Reinforcement Learning"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Tutorials Demystifying Deep Reinforcement Learning (Part1) http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/ Deep Reinforceme…"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,HGjBQ,fdzjHV,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.daTFSy{height:4px;padding:0;margin:24px 0;background-color:#e1e4e8;border:0;}/*!sc*/
data-styled.g29[id="horizontal-rule__HorizontalRule-sc-1731hye-0"]{content:"daTFSy,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Reinforcement Learning - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/embed-link/">Embed Link</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/posts/">Posts</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/privacy-policy/">Privacy Policy</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tutorials" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorials</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#simple-reinforcement-learning-with-tensorflow" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Simple Reinforcement Learning with Tensorflow</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#courses" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Courses</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#surveys" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Surveys</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#playing-doom" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Playing Doom</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#autonomous-vehicle-navigation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Autonomous vehicle navigation</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#play-flappy-bird" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Play Flappy Bird</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pong" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pong</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#tips-and-tricks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tips and Tricks</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#library" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Library</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#lets-make-a-dqn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Let’s make a DQN</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#books" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Books</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#reading-and-questions" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Reading and Questions</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Reinforcement Learning</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tutorials" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorials</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#simple-reinforcement-learning-with-tensorflow" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Simple Reinforcement Learning with Tensorflow</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#courses" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Courses</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#surveys" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Surveys</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#playing-doom" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Playing Doom</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#autonomous-vehicle-navigation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Autonomous vehicle navigation</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#play-flappy-bird" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Play Flappy Bird</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pong" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pong</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#tips-and-tricks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tips and Tricks</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#library" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Library</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#lets-make-a-dqn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Let’s make a DQN</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#books" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Books</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#reading-and-questions" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Reading and Questions</a></li></ul></div></div><h1 id="tutorials" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tutorials" color="auto.gray.8" aria-label="Tutorials permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tutorials</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Demystifying Deep Reinforcement Learning (Part1)</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" class="Link-sc-1brdqhf-0 cKRjba">http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning With Neon (Part2)</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/" class="Link-sc-1brdqhf-0 cKRjba">http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: David Silver, Google DeepMind</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1qWBOJGo" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1qWBOJGo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:  MLSS 2016. John Schulman<!-- -->[UC Berkeley]</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/index.html</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1jIatusA#path=%252F" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1jIatusA#path=%252F</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning: Pong from Pixels</strong></p><img src="http://karpathy.github.io/assets/rl/preview.jpeg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Andrej Karpathy</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://karpathy.github.io/2016/05/31/rl/" class="Link-sc-1brdqhf-0 cKRjba">http://karpathy.github.io/2016/05/31/rl/</a></li><li>gist: <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5" class="Link-sc-1brdqhf-0 cKRjba">https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>instructor: David Silver. RLDM 2015</li><li>video: <a target="_blank" rel="noopener noreferrer" href="http://videolectures.net/rldm2015_silver_reinforcement_learning/" class="Link-sc-1brdqhf-0 cKRjba">http://videolectures.net/rldm2015_silver_reinforcement_learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: David Silver <!-- -->[Google DeepMind]</li><li>video: <a target="_blank" rel="noopener noreferrer" href="http://techtalks.tv/talks/deep-reinforcement-learning/62360/" class="Link-sc-1brdqhf-0 cKRjba">http://techtalks.tv/talks/deep-reinforcement-learning/62360/</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://hunch.net/~beygel/deep_rl_tutorial.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://hunch.net/~beygel/deep_rl_tutorial.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Nuts and Bolts of Deep RL Research</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016, John Schulman, OpenAI</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1kVkBLkF" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1kVkBLkF</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ML Tutorial: Modern Reinforcement Learning and Video Games</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: by Marc Bellemare <!-- -->[DeepMind]</li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=WuFMrk3ZbkE" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=WuFMrk3ZbkE</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av17360035/" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av17360035/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement learning explained</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://www.oreilly.com/ideas/reinforcement-learning-explained" class="Link-sc-1brdqhf-0 cKRjba">https://www.oreilly.com/ideas/reinforcement-learning-explained</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beginner&#x27;s guide to Reinforcement Learning &amp; its implementation in Python</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/" class="Link-sc-1brdqhf-0 cKRjba">https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning on the Web</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Andrej Karpathy</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/presentation/d/1lcYrN56V2_SuX1rSmpzOUeMnheF6Jsu33-MsvLW9O_4/edit#slide=id.p" class="Link-sc-1brdqhf-0 cKRjba">https://docs.google.com/presentation/d/1lcYrN56V2_SuX1rSmpzOUeMnheF6Jsu33-MsvLW9O_4/edit#slide=id.p</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://alpha.openai.com/ak_rework_2017.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://alpha.openai.com/ak_rework_2017.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Q Learning with Keras and Gym</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://keon.io/rl/deep-q-learning-with-keras-and-gym/" class="Link-sc-1brdqhf-0 cKRjba">https://keon.io/rl/deep-q-learning-with-keras-and-gym/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/keon/deep-q-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/keon/deep-q-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>“Deep Reinforcement Learning, Decision Making, and Control</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2017 Tutorial</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/view/icml17deeprl" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/view/icml17deeprl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Tour of Reinforcement Learning: The View from Continuous Control</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: by Benjamin Recht, UC Berkeley</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/Recht_ICML_Control-RL_tutorial.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/Recht_ICML_Control-RL_tutorial.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Introduction to Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: McGill University &amp; Google Brain</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12560" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12560</a></li></ul><h2 id="simple-reinforcement-learning-with-tensorflow" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#simple-reinforcement-learning-with-tensorflow" color="auto.gray.8" aria-label="Simple Reinforcement Learning with Tensorflow permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Simple Reinforcement Learning with Tensorflow</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part 0: Q-Learning with Tables and Neural Networks</strong>
<a target="_blank" rel="noopener noreferrer" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.oo105wa2t" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.oo105wa2t</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part 1 - Two-armed Bandit</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.tk89k51ob" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.tk89k51ob</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part 2 - Policy-based Agents</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.n2wytg9q0" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.n2wytg9q0</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part 3 - Model-Based RL</strong>
<a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.742i2yj6p" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.742i2yj6p</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part 4: Deep Q-Networks and Beyond</strong>
<a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.jox069crz" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.jox069crz</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part 5: Visualizing an Agent’s Thoughts and Actions</strong>
<a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.pluh6cygm" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.pluh6cygm</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part 6: Partial Observability and Deep Recurrent Q-Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.3se46qkzy" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.3se46qkzy</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/awjuliani/35d2ab3409fc818011b6519f0f1629df" class="Link-sc-1brdqhf-0 cKRjba">https://gist.github.com/awjuliani/35d2ab3409fc818011b6519f0f1629df</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part 7: Action-Selection Strategies for Exploration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.8mcaa5nbe" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.8mcaa5nbe</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://awjuliani.github.io/exploration/index.html" class="Link-sc-1brdqhf-0 cKRjba">https://awjuliani.github.io/exploration/index.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dissecting Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>part 1: <a target="_blank" rel="noopener noreferrer" href="https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html" class="Link-sc-1brdqhf-0 cKRjba">https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html</a></li><li>part 2: <a target="_blank" rel="noopener noreferrer" href="https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html" class="Link-sc-1brdqhf-0 cKRjba">https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html</a></li><li>part 3: <a target="_blank" rel="noopener noreferrer" href="https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html" class="Link-sc-1brdqhf-0 cKRjba">https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mpatacchiola/dissecting-reinforcement-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mpatacchiola/dissecting-reinforcement-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>REINFORCE tutorial</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A small collection of code snippets and notes explaining the foundations of the REINFORCE algorithm.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mathias-madsen/reinforce_tutorial" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mathias-madsen/reinforce_tutorial</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Q-Learning Recap</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://blog.davidqiu.com/Research/%5B%20Recap%20%5D%20Deep%20Q-Learning%20Recap/" class="Link-sc-1brdqhf-0 cKRjba">http://blog.davidqiu.com/Research/%5B%20Recap%20%5D%20Deep%20Q-Learning%20Recap/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Introduction to Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Joelle Pineau <!-- -->[McGill University]</li><li>video: <a target="_blank" rel="noopener noreferrer" href="http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/" class="Link-sc-1brdqhf-0 cKRjba">http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://videolectures.net/site/normal_dl/tag=1051677/deeplearning2016_pineau_reinforcement_learning_01.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://videolectures.net/site/normal_dl/tag=1051677/deeplearning2016_pineau_reinforcement_learning_01.pdf</a></li></ul><h1 id="courses" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#courses" color="auto.gray.8" aria-label="Courses permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Courses</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Advanced Topics: RL</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UCL Course on RL</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>instructors: David Silver (Google DeepMind, AlphaGo)</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" class="Link-sc-1brdqhf-0 cKRjba">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1bnWGuIz/" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1bnWGuIz/</a></li><li>assignment: <a target="_blank" rel="noopener noreferrer" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CS 294: Deep Reinforcement Learning, Fall 2017</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>instructor: Sergey Levine</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://rll.berkeley.edu/deeprlcourse/" class="Link-sc-1brdqhf-0 cKRjba">http://rll.berkeley.edu/deeprlcourse/</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3</a></li><li>bilibili: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av21501169/" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av21501169/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CS 294: Deep Reinforcement Learning, Spring 2017</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>course page: <a target="_blank" rel="noopener noreferrer" href="http://rll.berkeley.edu/deeprlcoursesp17/" class="Link-sc-1brdqhf-0 cKRjba">http://rll.berkeley.edu/deeprlcoursesp17/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//txizzle/drl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//txizzle/drl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Berkeley CS 294: Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>instructors: John Schulman, Pieter Abbeel</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://rll.berkeley.edu/deeprlcourse/" class="Link-sc-1brdqhf-0 cKRjba">http://rll.berkeley.edu/deeprlcourse/</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1hsQcm1Y" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1hsQcm1Y</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>(Udacity) Reinforcement Learning - Offered at Georgia Tech as CS 8803</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>instructor: Charles Isbell, Michael Littman</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://www.udacity.com/course/reinforcement-learning--ud600" class="Link-sc-1brdqhf-0 cKRjba">https://www.udacity.com/course/reinforcement-learning--ud600</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://classroom.udacity.com/courses/ud820/lessons/684808907/concepts/6512308530923" class="Link-sc-1brdqhf-0 cKRjba">https://classroom.udacity.com/courses/ud820/lessons/684808907/concepts/6512308530923</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CS229 Lecture notes Part XIII: Reinforcement Learning and Control</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Andrew Ng</li><li>lecture notes: <a target="_blank" rel="noopener noreferrer" href="http://cs229.stanford.edu/notes/cs229-notes12.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cs229.stanford.edu/notes/cs229-notes12.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Practical_RL: A course in reinforcement learning in the wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yandexdataschool/Practical_RL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yandexdataschool/Practical_RL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning (COMP-762) Winter 2017</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>course page: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.mcgill.ca/~dprecup/courses/rl.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.mcgill.ca/~dprecup/courses/rl.html</a></li><li>lectures: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep RL Bootcamp - 26-27 August 2017 | Berkeley CA</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>lectures: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/view/deep-rl-bootcamp/lectures" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/view/deep-rl-bootcamp/lectures</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av15568836/" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av15568836/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CMPUT 366: Intelligent Systems and CMPUT 609: Reinforcement Learning &amp; Artificial Intelligence</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: by Rich Sutton, Adam White</li><li>lecture video: <a target="_blank" rel="noopener noreferrer" href="https://drive.google.com/drive/folders/0B3w765rOKuKAMG9lbmRacFdsLWM?direction=a" class="Link-sc-1brdqhf-0 cKRjba">https://drive.google.com/drive/folders/0B3w765rOKuKAMG9lbmRacFdsLWM?direction=a</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning and Control (Spring 2017, CMU 10703)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>instructors: Katerina Fragkiadaki, Ruslan Satakhutdinov</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://katefvision.github.io/" class="Link-sc-1brdqhf-0 cKRjba">https://katefvision.github.io/</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/playlist?list=PLpIxOj-HnDsNPFdu2UqCu2McJKHs-eWXv" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/playlist?list=PLpIxOj-HnDsNPFdu2UqCu2McJKHs-eWXv</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av18865689/" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av18865689/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Advanced Deep Learning &amp; Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind</li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs</a></li><li>bilibili: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av36621866/" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av36621866/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/RylanSchaeffer/ucl-adv-dl-rl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/RylanSchaeffer/ucl-adv-dl-rl</a></li></ul><h1 id="papers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers" color="auto.gray.8" aria-label="Papers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Playing Atari with Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind. NIPS Deep Learning Workshop 2013</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1312.5602" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1312.5602</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kristjankorjus/Replicating-DeepMind" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kristjankorjus/Replicating-DeepMind</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html" class="Link-sc-1brdqhf-0 cKRjba">http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Kaixhin/Atari" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Kaixhin/Atari</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/gliese581gg/DQN_tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gliese581gg/DQN_tensorflow</a></li><li>summary: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2014</li><li>keywords: DQN, MCTS</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging" class="Link-sc-1brdqhf-0 cKRjba">http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Replicating the Paper “Playing Atari with Deep Reinforcement Learning”</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Tartu</li><li>technical report: <a target="_blank" rel="noopener noreferrer" href="https://courses.cs.ut.ee/MTAT.03.291/2014_spring/uploads/Main/Replicating%20DeepMind.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://courses.cs.ut.ee/MTAT.03.291/2014_spring/uploads/Main/Replicating%20DeepMind.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Tutorial for Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://web.mst.edu/~gosavia/tutorial.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.mst.edu/~gosavia/tutorial.pdf</a></li><li>code(C): <a target="_blank" rel="noopener noreferrer" href="http://web.mst.edu/~gosavia/bookcodes.html" class="Link-sc-1brdqhf-0 cKRjba">http://web.mst.edu/~gosavia/bookcodes.html</a></li><li>code(Matlab): <a target="_blank" rel="noopener noreferrer" href="http://web.mst.edu/~gosavia/mrrl_website.html" class="Link-sc-1brdqhf-0 cKRjba">http://web.mst.edu/~gosavia/mrrl_website.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1507.00814" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1507.00814</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://www.evernote.com/shard/s189/sh/a4262b84-a322-4f77-9a76-569278be84af/b8c3e146a76ca3853f560bb03b60a481" class="Link-sc-1brdqhf-0 cKRjba">https://www.evernote.com/shard/s189/sh/a4262b84-a322-4f77-9a76-569278be84af/b8c3e146a76ca3853f560bb03b60a481</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Massively Parallel Methods for Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2015. DeepMind</li><li>keywords: DQN, Gorila</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1507.04296" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1507.04296</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Action-Conditional Video Prediction using Deep Networks in Atari Games</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1507.08750" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1507.08750</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/junhyukoh/nips2015-action-conditional-video-prediction" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/junhyukoh/nips2015-action-conditional-video-prediction</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="http://video.weibo.com/show?fid=1034:98062f3d83e41da6faa99cde5aa1ac97" class="Link-sc-1brdqhf-0 cKRjba">http://video.weibo.com/show?fid=1034:98062f3d83e41da6faa99cde5aa1ac97</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Recurrent Q-Learning for Partially Observable MDPs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1507.06527" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1507.06527</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Continuous control with deep reinforcement learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.02971" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.02971</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iassael/torch-policy-gradient" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iassael/torch-policy-gradient</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/stevenpjg/ddpg-aigym" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/stevenpjg/ddpg-aigym</a></li><li>github(TensorFlow + OpenAI Gym): <a target="_blank" rel="noopener noreferrer" href="https://github.com/SimonRamstedt/ddpg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SimonRamstedt/ddpg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Benchmarking for Bayesian Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.04064" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.04064</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mcastron/BBRL/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mcastron/BBRL/</a></li><li>reading: <a target="_blank" rel="noopener noreferrer" href="http://blogs.ulg.ac.be/damien-ernst/benchmarking-for-bayesian-reinforcement-learning/" class="Link-sc-1brdqhf-0 cKRjba">http://blogs.ulg.ac.be/damien-ernst/benchmarking-for-bayesian-reinforcement-learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning with Double Q-learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1509.06461" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1509.06461</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Giraffe: Using Deep Reinforcement Learning to Play Chess</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.01549" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.01549</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Human-level control through deep reinforcement learning</strong></p><img src="/assets/reinforcement-learning-materials/DeepMind_Atari_Deep_Q_Learner-breakout.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind. 2015 Nature</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D" class="Link-sc-1brdqhf-0 cKRjba">http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf</a></li><li>github(Lua/Torch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepmind/dqn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deepmind/dqn</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1kTiwzOF" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1kTiwzOF</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/a/deepmind.com/dqn/" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/a/deepmind.com/dqn/</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=V2wzkPmiB_A" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=V2wzkPmiB_A</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tambetm/simple_dqn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tambetm/simple_dqn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/devsisters/DQN-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/devsisters/DQN-tensorflow</a></li><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/2x4yy1/google_deepmind_nature_paper_humanlevel_control" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/2x4yy1/google_deepmind_nature_paper_humanlevel_control</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.02173" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.02173</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.08731" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.08731</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889" class="Link-sc-1brdqhf-0 cKRjba">https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06342" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06342</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/eparisotto/ActorMimic" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/eparisotto/ActorMimic</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MazeBase: A Sandbox for Learning from Games</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: New York University &amp; Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07401" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07401</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Simple Algorithms from Examples</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: New York University &amp; Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07275" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07275</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wojzaremba/algorithm-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wojzaremba/algorithm-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Algorithms from Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>PhD thesis: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.nyu.edu/media/publications/zaremba_wojciech.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.nyu.edu/media/publications/zaremba_wojciech.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wojzaremba/algorithm-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wojzaremba/algorithm-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multiagent Cooperation and Competition with Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.08779" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.08779</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Active Object Localization with Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06015" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06015</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.01124" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.01124</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.02011" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.02011</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>State of the Art Control of Atari Games Using Shallow Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.01563" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.01563</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Angrier Birds: Bayesian reinforcement learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.01297" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.01297</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/imanolarrieta/angrybirds" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/imanolarrieta/angrybirds</a></li><li>gitxiv: <a target="_blank" rel="noopener noreferrer" href="http://gitxiv.com/posts/Nr2N7j4YrR4gnCYK9/angrier-birds-bayesian-reinforcement-learning" class="Link-sc-1brdqhf-0 cKRjba">http://gitxiv.com/posts/Nr2N7j4YrR4gnCYK9/angrier-birds-bayesian-reinforcement-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Prioritized Experience Replay</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.05952" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.05952</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dueling Network Architectures for Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2016 best paper</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06581" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06581</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://hadovanhasselt.wordpress.com/2016/06/20/best-paper-at-icml-dueling-network-architectures-for-deep-reinforcement-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://hadovanhasselt.wordpress.com/2016/06/20/best-paper-at-icml-dueling-network-architectures-for-deep-reinforcement-learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Asynchronous Methods for Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.01783" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.01783</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/traai/async-deep-rl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/traai/async-deep-rl</a></li><li>github(Tensorflow+Keras+OpenAI Gym): <a target="_blank" rel="noopener noreferrer" href="https://github.com/coreylynch/async-rl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/coreylynch/async-rl</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/devsisters/async-rl-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/devsisters/async-rl-tensorflow</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/ikostrikov/pytorch-a3c" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ikostrikov/pytorch-a3c</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Graying the black box: Understanding DQNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.02658" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.02658</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.02672" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.02672</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Value Iteration Networks</strong></p><img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%204.58.42%20PM.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016, Best Paper Award. University of California, Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.02867" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.02867</a></li><li>github(official, Theano): <a target="_blank" rel="noopener noreferrer" href="https://github.com/avivt/VIN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/avivt/VIN</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/onlytailei/PyTorch-value-iteration-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/onlytailei/PyTorch-value-iteration-networks</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kentsommer/pytorch-value-iteration-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kentsommer/pytorch-value-iteration-networks</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/neka-nat/vin-keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/neka-nat/vin-keras</a></li><li>notes(by Andrej Karpathy): <a target="_blank" rel="noopener noreferrer" href="https://github.com/karpathy/paper-notes/blob/master/vin.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/karpathy/paper-notes/blob/master/vin.md</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Insights in Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MSc thesis</li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1bn51BYJ" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1bn51BYJ</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using Deep Q-Learning to Control Optimization Hyperparameters</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.04062" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.04062</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Continuous Deep Q-Learning with Model-based Acceleration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.00748" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.00748</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning from Self-Play in Imperfect-Information Games</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.01121" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.01121</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.06057" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.06057</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/EthanMacdonald/h-DQN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/EthanMacdonald/h-DQN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Benchmarking Deep Reinforcement Learning for Continuous Control</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.06778" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.06778</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rllab/rllab" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rllab/rllab</a></li><li>doc: <a target="_blank" rel="noopener noreferrer" href="https://rllab.readthedocs.org/en/latest/" class="Link-sc-1brdqhf-0 cKRjba">https://rllab.readthedocs.org/en/latest/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning</strong></p><img src="http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/dog_teaser.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xbpeng/DeepTerrainRL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xbpeng/DeepTerrainRL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.05359" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.05359</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Successor Reinforcement Learning (MIT)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1606.02396" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1606.02396</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Ardavans/DSR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Ardavans/DSR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Communicate with Deep Multi-Agent Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1605.06676" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1605.06676</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iassael/learning-to-communicate" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iassael/learning-to-communicate</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning with Regularized Convolutional Neural Fitted Q Iteration</strong>
<strong>RC-NFQ: Regularized Convolutional Neural Fitted Q Iteration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A batch algorithm for deep reinforcement learning.
Incorporates dropout regularization and convolutional neural networks with a separate target Q network.</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://machineintelligence.org/papers/rc-nfq.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://machineintelligence.org/papers/rc-nfq.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cosmoharrigan/rc-nfq" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cosmoharrigan/rc-nfq</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.02993" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.02993</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bayesian Reinforcement Learning: A Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.04436" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.04436</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Playing FPS Games with Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.05521" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.05521</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning with Stochastic Initial States</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Washington &amp; UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.01112" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.01112</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Utilization of Deep Reinforcement Learning for saccadic-based object visual search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.06492" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.06492</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Navigate in Complex Environments</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.03673" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.03673</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepmind/lab" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deepmind/lab</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=lNoaTyMZsWI" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=lNoaTyMZsWI</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning with Unsupervised Auxiliary Tasks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind. ICLR 2017 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05397" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05397</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to reinforcement learn</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05763" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05763</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Graduate Training Center of Neuroscience &amp; MSR</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07078" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07078</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploration for Multi-task Reinforcement Learning with Deep Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS Deep Reinforcement Learning Workshop 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09894" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09894</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Combinatorial Optimization with Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Brain</li><li>keywords: traveling salesman problem (TSP)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09940" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09940</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Loss is its own Reward: Self-Supervision for Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.07307" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.07307</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning Using Quantum Boltzmann Machines</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 1QB Information Technologies (1QBit)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.05695" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.05695</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning applied to the game Bubble Shooter</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>bachelor thesis: <a target="_blank" rel="noopener noreferrer" href="https://staff.fnwi.uva.nl/b.bredeweg/pdf/BSc/20152016/Samson.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://staff.fnwi.uva.nl/b.bredeweg/pdf/BSc/20152016/Samson.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/laurenssam/AlphaBubble" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/laurenssam/AlphaBubble</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=DPAKFenNgbs" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=DPAKFenNgbs</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning: An Overview</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.07274" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.07274</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Adversarial Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; Google Brain &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.02702" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.02702</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beating Atari with Natural Language Guided Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.05539" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.05539</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Imperial College London</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.06769" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.06769</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Nat-D/FeatureControlHRL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Nat-D/FeatureControlHRL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distral: Robust Multitask Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind</li><li>keywords: Distill, transfer learning</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.04175" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.04175</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning: Framework, Applications, and Embedded Implementations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Syracuse University &amp; University of California, Riverside</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.03792" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.03792</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Deep Reinforcement Learning with Adversarial Attacks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.03632" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.03632</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Variational Deep Q Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Second workshop on Bayesian Deep Learning (NIPS 2017). Columbia University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.11225" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.11225</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>On Monte Carlo Tree Search and Reinforcement Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.jair.org/media/5507/live-5507-10333-jair.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.jair.org/media/5507/live-5507-10333-jair.pdf</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: deepsense.ai &amp; Intel &amp; Polish Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02852" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02852</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com//anonymous-author1/DDRL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//anonymous-author1/DDRL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GAN Q-learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.04874" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.04874</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Visual Geometry Group, University of Oxford &amp; Element AI &amp; Polytechnique Montreal, Mila &amp; Canada CIFAR AI Chair</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01318" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01318</a></li></ul><h2 id="surveys" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#surveys" color="auto.gray.8" aria-label="Surveys permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Surveys</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning: A Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: JAIR 1996</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/rl-survey.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/rl-survey.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/cs/9605103" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/cs/9605103</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Brief Survey of Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Signal Processing Magazine, Special Issue on Deep Learning for Image Understanding</li><li>intro: Imperial College London &amp; Arizona State University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.05866" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.05866</a></li></ul><h2 id="playing-doom" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#playing-doom" color="auto.gray.8" aria-label="Playing Doom permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Playing Doom</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning</strong></p><img src="http://vizdoom.cs.put.edu.pl/user/pages/01.home/depthbuffer.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.02097" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.02097</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Marqt/ViZDoom" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Marqt/ViZDoom</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vizdoom.cs.put.edu.pl/" class="Link-sc-1brdqhf-0 cKRjba">http://vizdoom.cs.put.edu.pl/</a></li><li>tutorial: <a target="_blank" rel="noopener noreferrer" href="http://vizdoom.cs.put.edu.pl/tutorial" class="Link-sc-1brdqhf-0 cKRjba">http://vizdoom.cs.put.edu.pl/tutorial</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning From Raw Pixels in Doom</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Bachelor&#x27;s thesis</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.02164" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.02164</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Playing Doom with SLAM-Augmented Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Oxford</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.00380" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.00380</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning via Recurrent Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.02392" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.02392</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tanmayshankar/RCNN_MDP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tanmayshankar/RCNN_MDP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Shallow Updates for Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Technion &amp; UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.07461" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.07461</a></li><li>github(Official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Shallow-Updates-for-Deep-RL/Shallow_Updates_for_Deep_RL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Shallow-Updates-for-Deep-RL/Shallow_Updates_for_Deep_RL</a></li></ul><h1 id="projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TorchQLearning</strong></p><img src="https://raw.githubusercontent.com/SeanNaren/TorchQLearningExample/master/images/torchplayscatch.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SeanNaren/TorchQLearningExample" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SeanNaren/TorchQLearningExample</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>General_Deep_Q_RL: General deep Q learning framework</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/VinF/General_Deep_Q_RL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/VinF/General_Deep_Q_RL</a></li><li>wiki: <a target="_blank" rel="noopener noreferrer" href="https://github.com/VinF/General_Deep_Q_RL/wiki" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/VinF/General_Deep_Q_RL/wiki</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Snake: Toy example of deep reinforcement model playing the game of snake</strong></p><img src="https://raw.githubusercontent.com/bitwise-ben/Snake/master/images/snake.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bitwise-ben/Snake" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bitwise-ben/Snake</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using Deep Q Networks to Learn Video Game Strategies</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/asrivat1/DeepLearningVideoGames" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/asrivat1/DeepLearningVideoGames</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>qlearning4k: Q-learning for Keras</strong></p><img src="https://raw.githubusercontent.com/farizrahman4u/qlearning4k/master/gifs/catch.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><img src="https://raw.githubusercontent.com/farizrahman4u/qlearning4k/master/gifs/snake.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;Qlearning4k is a reinforcement learning add-on for the python deep learning library Keras.
Its simple, and is ideal for rapid prototyping.&quot;</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/farizrahman4u/qlearning4k" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/farizrahman4u/qlearning4k</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>rlenvs: Reinforcement learning environments for Torch7, inspired by RL-Glue</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Kaixhin/rlenvs" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Kaixhin/rlenvs</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep_rl_ale: An implementation of Deep Reinforcement Learning / Deep Q-Networks for Atari games in TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Jabberwockyll/deep_rl_ale" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Jabberwockyll/deep_rl_ale</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Chimp: General purpose framework for deep reinforcement learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sisl/Chimp" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sisl/Chimp</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Q Learning for ATARI using Tensorflow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mrkulk/deepQN_tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mrkulk/deepQN_tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepQLearning: A powerful machine learning algorithm utilizing Q-Learning and Neural Networks, implemented using Torch and Lua.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/blakeMilner/DeepQLearning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/blakeMilner/DeepQLearning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OpenAI Gym: A toolkit for developing and comparing reinforcement learning algorithms</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://gym.openai.com/" class="Link-sc-1brdqhf-0 cKRjba">https://gym.openai.com/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/openai/gym" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/openai/gym</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeeR: DEEp Reinforcement learning framework</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/VinF/deer/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/VinF/deer/</a></li><li>docs: <a target="_blank" rel="noopener noreferrer" href="http://deer.readthedocs.io/en/latest/" class="Link-sc-1brdqhf-0 cKRjba">http://deer.readthedocs.io/en/latest/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>KeRLym: A Deep Reinforcement Learning Toolbox in Keras</strong></p><img src="https://raw.githubusercontent.com/osh/kerlym/master/examples/example.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://oshearesearch.com/index.php/2016/06/14/kerlym-a-deep-reinforcement-learning-toolbox-in-keras/" class="Link-sc-1brdqhf-0 cKRjba">https://oshearesearch.com/index.php/2016/06/14/kerlym-a-deep-reinforcement-learning-toolbox-in-keras/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/osh/kerlym" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/osh/kerlym</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pack of Drones: Layered reinforcement learning for complex behaviors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MickyDowns/deep-theano-rnn-lstm-car" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MickyDowns/deep-theano-rnn-lstm-car</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=WrLRGzbfeZc" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=WrLRGzbfeZc</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RL Helicopter Game: Q-Learning and DQN Reinforcement Learning to play the Helicopter Game - Keras based!</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://dandxy89.github.io/rf_helicopter/" class="Link-sc-1brdqhf-0 cKRjba">http://dandxy89.github.io/rf_helicopter/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dandxy89/rf_helicopter" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dandxy89/rf_helicopter</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Playing Mario with Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aleju/mario-ai" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aleju/mario-ai</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Attention Recurrent Q-Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Deep Reinforcement Learning Workshop, NIPS 2015. DeepHack Game</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1512.01693" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1512.01693</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/5vision/DARQN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/5vision/DARQN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning in TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TensorFlow implementation of Deep Reinforcement Learning papers</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/carpedm20/deep-rl-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/carpedm20/deep-rl-tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>rltorch: A RL package for Torch that can also be used with openai gym</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ludc/rltorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ludc/rltorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep_q_rl: Theano-based implementation of Deep Q-learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/spragunr/deep_q_rl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/spragunr/deep_q_rl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement-trading</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: This project uses reinforcement learning on stock market and agent tries to learn trading.
The goal is to check if the agent can learn to read tape.
The project is dedicated to hero in life great Jesse Livermore.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deependersingla/deep_trader" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deependersingla/deep_trader</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>dist-dqn：Distributed Reinforcement Learning using Deep Q-Network in TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/viswanathgs/dist-dqn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/viswanathgs/dist-dqn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning for Keras</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/matthiasplappert/keras-rl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/matthiasplappert/keras-rl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RL4J: Reinforcement Learning for the JVM</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Reinforcement learning framework integrated with deeplearning4j.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deeplearning4j/rl4j" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deeplearning4j/rl4j</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Teaching Your Computer To Play Super Mario Bros. – A Fork of the Google DeepMind Atari Machine Learning Project</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/" class="Link-sc-1brdqhf-0 cKRjba">http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ehrenbrav/DeepQNetwork" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ehrenbrav/DeepQNetwork</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>dprl: Deep reinforcement learning package for torch7</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PoHsunSu/dprl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PoHsunSu/dprl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning for Torch: Introducing torch-twrl</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl" class="Link-sc-1brdqhf-0 cKRjba">https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/twitter/torch-twrl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/twitter/torch-twrl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Alpha Toe - Using Deep learning to master Tic-Tac-Toe - Daniel Slater</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.danielslater.net/2016/10/alphatoe.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.danielslater.net/2016/10/alphatoe.html</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=Meb5hApAnj4" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=Meb5hApAnj4</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DanielSlater/AlphaToe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DanielSlater/AlphaToe</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tensorflow-Reinforce: Implementation of Reinforcement Learning Models in Tensorflow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yukezhu/tensorflow-reinforce" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yukezhu/tensorflow-reinforce</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep RL hacking on minecraft with malmo</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/matpalm/malmomo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/matpalm/malmomo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ReinforcementLearning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MC control, Q-learning, SARSA, Cross Entropy Method</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/janivanecky/ReinforcementLearning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/janivanecky/ReinforcementLearning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>markovjs: Reinforcement Learning in JavaScript</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lsunsi/markovjs" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lsunsi/markovjs</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Q: Deep reinforcement learning with TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tobegit3hub/deep_q" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tobegit3hub/deep_q</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Q-Learning Network in pytorch</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/transedward/pytorch-dqn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/transedward/pytorch-dqn</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tensorflow-RL: Implementations of deep RL papers and random experimentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/steveKapturowski/tensorflow-rl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/steveKapturowski/tensorflow-rl</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Minimal and Clean Reinforcement Learning Examples</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/rlcode/reinforcement-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rlcode/reinforcement-learning</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepRL: Highly modularized implementation of popular deep RL algorithms by PyTorch</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ShangtongZhang/DeepRL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ShangtongZhang/DeepRL</a></p><h2 id="autonomous-vehicle-navigation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#autonomous-vehicle-navigation" color="auto.gray.8" aria-label="Autonomous vehicle navigation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Autonomous vehicle navigation</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-Driving-Car-AI</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A simple self-driving car AI python script using the deep Q-learning algorithm</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//JianyangZhang/Self-Driving-Car-AI" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//JianyangZhang/Self-Driving-Car-AI</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Autonomous vehicle navigation based on Deep Reinforcement Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com//kaihuchen/DRL-AutonomousVehicles" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//kaihuchen/DRL-AutonomousVehicles</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Car Racing using Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford University</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf</a></li></ul><h2 id="play-flappy-bird" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#play-flappy-bird" color="auto.gray.8" aria-label="Play Flappy Bird permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Play Flappy Bird</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using Deep Q-Network to Learn How To Play Flappy Bird</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yenchenlin/DeepLearningFlappyBird" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yenchenlin/DeepLearningFlappyBird</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Playing Flappy Bird Using Deep Reinforcement Learning (Based on Deep Q Learning DQN using Tensorflow)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://blog.csdn.net/songrotek/article/details/50951537" class="Link-sc-1brdqhf-0 cKRjba">http://blog.csdn.net/songrotek/article/details/50951537</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/songrotek/DRL-FlappyBird" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/songrotek/DRL-FlappyBird</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Playing Flappy Bird Using Deep Reinforcement Learning (Based on Deep Q Learning DQN)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/li-haoran/DRL-FlappyBird" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/li-haoran/DRL-FlappyBird</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MXNET-Scala Playing Flappy Bird Using Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/DRLFlappyBird" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/DRLFlappyBird</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Flappy Bird Bot using Reinforcement Learning in Python</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chncyhn/flappybird-qlearning-bot" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chncyhn/flappybird-qlearning-bot</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using Keras and Deep Q-Network to Play FlappyBird</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html" class="Link-sc-1brdqhf-0 cKRjba">https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yanpanlau/Keras-FlappyBird" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yanpanlau/Keras-FlappyBird</a></li></ul><h1 id="pong" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pong" color="auto.gray.8" aria-label="Pong permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pong</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Building a Pong playing AI in just 1 hour(plus 4 days training...)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>sildes: <a target="_blank" rel="noopener noreferrer" href="https://speakerdeck.com/danielslater/building-a-pong-ai" class="Link-sc-1brdqhf-0 cKRjba">https://speakerdeck.com/danielslater/building-a-pong-ai</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DanielSlater/PyDataLondon2016" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DanielSlater/PyDataLondon2016</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=n8NdT_3y9oY" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=n8NdT_3y9oY</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pong Neural Network(LIVE)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=Hqf__FlRlzg" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=Hqf__FlRlzg</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/llSourcell/pong_neural_network_live" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/llSourcell/pong_neural_network_live</a></li></ul><h2 id="tips-and-tricks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tips-and-tricks" color="auto.gray.8" aria-label="Tips and Tricks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tips and Tricks</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepRLHacks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Nuts and Bolts of Deep RL Research</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/williamFalcon/DeepRLHacks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/williamFalcon/DeepRLHacks</a></li></ul><h1 id="library" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#library" color="auto.gray.8" aria-label="Library permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Library</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BURLAP: Brown-UMBC Reinforcement Learning and Planning (BURLAP) java code library</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: for the use and development of single or multi-agent planning and learning algorithms and domains to accompany them</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://burlap.cs.brown.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://burlap.cs.brown.edu/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AgentNet: Deep Reinforcement Learning library for humans</strong></p><img src="https://camo.githubusercontent.com/5593b2c8184c4bc08372f919063e826d9bcc2c67/687474703a2f2f7333332e706f7374696d672e6f72672f79747836336b7763762f7768617469735f6167656e746e65745f706e672e706e67" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A lightweight library to build and train deep reinforcement learning and custom recurrent networks using Theano+Lasagne </li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yandexdataschool/AgentNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yandexdataschool/AgentNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Atari Multitask &amp; Transfer Learning Benchmark (AMTLB)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Atari gauntlet for RL agents</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://ai-on.org/projects/multitask-and-transfer-learning.html" class="Link-sc-1brdqhf-0 cKRjba">http://ai-on.org/projects/multitask-and-transfer-learning.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deontologician/atari_multitask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deontologician/atari_multitask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Coach: a python reinforcement learning research framework containing implementation of many state-of-the-art algorithms</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Reinforcement Learning Coach by Intel® Nervana™ enables easy experimentation with state of the art Reinforcement Learning algorithms</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://coach.nervanasys.com/" class="Link-sc-1brdqhf-0 cKRjba">http://coach.nervanasys.com/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NervanaSystems/coach" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NervanaSystems/coach</a></li></ul><h1 id="blogs" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#blogs" color="auto.gray.8" aria-label="Blogs permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blogs</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement learning’s foundational flaw</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://thegradient.pub/why-rl-is-flawed/" class="Link-sc-1brdqhf-0 cKRjba">https://thegradient.pub/why-rl-is-flawed/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Short Introduction To Some Reinforcement Learning Algorithms</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://webdocs.cs.ualberta.ca/~vanhasse/rl_algs/rl_algs.html" class="Link-sc-1brdqhf-0 cKRjba">http://webdocs.cs.ualberta.ca/~vanhasse/rl_algs/rl_algs.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Painless Q-Learning Tutorial</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://mnemstudio.org/path-finding-q-learning-tutorial.htm" class="Link-sc-1brdqhf-0 cKRjba">http://mnemstudio.org/path-finding-q-learning-tutorial.htm</a></p><hr class="horizontal-rule__HorizontalRule-sc-1731hye-0 daTFSy"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning - Part 1</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://outlace.com/Reinforcement-Learning-Part-1/" class="Link-sc-1brdqhf-0 cKRjba">http://outlace.com/Reinforcement-Learning-Part-1/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning - Monte Carlo Methods</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://outlace.com/Reinforcement-Learning-Part-2/" class="Link-sc-1brdqhf-0 cKRjba">http://outlace.com/Reinforcement-Learning-Part-2/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Q-learning with Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://outlace.com/Reinforcement-Learning-Part-3/" class="Link-sc-1brdqhf-0 cKRjba">http://outlace.com/Reinforcement-Learning-Part-3/</a></p><hr class="horizontal-rule__HorizontalRule-sc-1731hye-0 daTFSy"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Guest Post (Part I): Demystifying Deep Reinforcement Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.nervanasys.com/demystifying-deep-reinforcement-learning/" class="Link-sc-1brdqhf-0 cKRjba">http://www.nervanasys.com/demystifying-deep-reinforcement-learning/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using reinforcement learning in Python to teach a virtual car to avoid obstacles: An experiment in Q-learning, neural networks and Pygame.</strong></p><img src="https://cdn-images-1.medium.com/max/800/1*Ht0KPSlYVsLUp-wqr-ab7Q.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.p8ug6snri" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.p8ug6snri</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/harvitronix/reinforcement-learning-car" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/harvitronix/reinforcement-learning-car</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement learning in Python to teach a virtual car to avoid obstacles — part 2</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.i0o643m1h" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.i0o643m1h</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Some Reinforcement Learning Algorithms in Python, C++</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>pan: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1mhcYf3M#path=%252FImplementations%2520of%2520Some%2520Reinforcement%2520Learning%2520Algorithms" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1mhcYf3M#path=%252FImplementations%2520of%2520Some%2520Reinforcement%2520Learning%2520Algorithms</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>learning to do laps with reinforcement learning and neural nets</strong></p><img src="http://matpalm.com/blog/imgs/2016/drivebot/walk.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><img src="http://matpalm.com/blog/imgs/2016/drivebot/runt.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://matpalm.com/blog/drivebot/" class="Link-sc-1brdqhf-0 cKRjba">http://matpalm.com/blog/drivebot/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/matpalm/drivebot" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/matpalm/drivebot</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Get a taste of reinforcement learning — implement a tic tac toe agent</strong></p><img src="https://cdn-images-1.medium.com/max/800/1*Ntrov1dzaerfesi9vRKdow.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4#.59bx71a2h" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4#.59bx71a2h</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Best reinforcement learning libraries?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Super Simple Reinforcement Learning Tutorial</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>part 1: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149</a></li><li>part 2: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.dyhxww1u6" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.dyhxww1u6</a></li><li>part 3: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.r4c7i7tjq" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.r4c7i7tjq</a></li><li>gist: <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/awjuliani/16608e1c4968baaa692b9b8c7dd94d04" class="Link-sc-1brdqhf-0 cKRjba">https://gist.github.com/awjuliani/16608e1c4968baaa692b9b8c7dd94d04</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning in Python</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NathanEpstein/pydata-reinforce" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NathanEpstein/pydata-reinforce</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Skynet Salesman</strong></p><img src="http://multithreaded.stitchfix.com/assets/posts/2016-07-18-skynet-salesman/np4_5_w.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keyworkds: traveling salesman problem (TSP), deep Q learning</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://multithreaded.stitchfix.com/blog/2016/07/21/skynet-salesman/" class="Link-sc-1brdqhf-0 cKRjba">http://multithreaded.stitchfix.com/blog/2016/07/21/skynet-salesman/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jn2clark/ReinforcementLearning/tree/master/DeepQ" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jn2clark/ReinforcementLearning/tree/master/DeepQ</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Apprenticeship learning using Inverse Reinforcement Learning</strong></p><img src="https://jangirrishabh.github.io/assets/IRL/irl_des.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/" class="Link-sc-1brdqhf-0 cKRjba">https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jangirrishabh/toyCarIRL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jangirrishabh/toyCarIRL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning and DQN, learning to play from pixels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html" class="Link-sc-1brdqhf-0 cKRjba">https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning in a Nutshell: Reinforcement Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Write an AI to win at Pong from scratch with Reinforcement Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0#.n1pgn9chr" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0#.n1pgn9chr</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Reinforcement Learning (with Code, Exercises and Solutions)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.wildml.com/2016/10/learning-reinforcement-learning/" class="Link-sc-1brdqhf-0 cKRjba">http://www.wildml.com/2016/10/learning-reinforcement-learning/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dennybritz/reinforcement-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dennybritz/reinforcement-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning: Playing a Racing Game</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html" class="Link-sc-1brdqhf-0 cKRjba">https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Experimenting with Reinforcement Learning and Active Inference</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.araya.org/archives/955" class="Link-sc-1brdqhf-0 cKRjba">http://www.araya.org/archives/955</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/arayabrain/BinarySearchLSTM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/arayabrain/BinarySearchLSTM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep reinforcement learning, battleship</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://efavdb.com/battleship/" class="Link-sc-1brdqhf-0 cKRjba">http://efavdb.com/battleship/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/EFavDB/battleship" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/EFavDB/battleship</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Research Review Week 2: Reinforcement Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning" class="Link-sc-1brdqhf-0 cKRjba">https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning: Artificial Intelligence in Game Playing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/@pavelkordik/reinforcement-learning-the-hardest-part-of-machine-learning-b667a22995ca#.jjiitflok" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@pavelkordik/reinforcement-learning-the-hardest-part-of-machine-learning-b667a22995ca#.jjiitflok</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Artificial Intelligence’s Next Big Step: Reinforcement Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://thenewstack.io/reinforcement-learning-ready-real-world/" class="Link-sc-1brdqhf-0 cKRjba">http://thenewstack.io/reinforcement-learning-ready-real-world/</a></p><h2 id="lets-make-a-dqn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#lets-make-a-dqn" color="auto.gray.8" aria-label="Let’s make a DQN permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Let’s make a DQN</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Let’s make a DQN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>Theory: <a target="_blank" rel="noopener noreferrer" href="https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/" class="Link-sc-1brdqhf-0 cKRjba">https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/</a></li><li>Implementation: <a target="_blank" rel="noopener noreferrer" href="https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/" class="Link-sc-1brdqhf-0 cKRjba">https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/</a></li><li>Debugging: <a target="_blank" rel="noopener noreferrer" href="https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/" class="Link-sc-1brdqhf-0 cKRjba">https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/</a></li><li>Full DQN: <a target="_blank" rel="noopener noreferrer" href="https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/" class="Link-sc-1brdqhf-0 cKRjba">https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py</a></li></ul><h1 id="books" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#books" color="auto.gray.8" aria-label="Books permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Books</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning: State-of-the-Art</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;The main goal of this book is to present an up-to-date series of survey articles
on the main contemporary sub-fields of reinforcement learning.
This includes surveys on partially observable environments, hierarchical task decompositions,
relational knowledge representation and predictive state representations.
Furthermore, topics such as transfer, evolutionary methods and continuous spaces in reinforcement learning are surveyed.
In addition, several chapters review reinforcement learning methods in robotics, in games, and in computational neuroscience.
In total seventeen different subfields are presented by mostly young experts in those areas,
and together they truly represent a state-of-the-art of current reinforcement learning research.&quot;</li><li>book: <a target="_blank" rel="noopener noreferrer" href="http://www.springer.com/gp/book/9783642276446#" class="Link-sc-1brdqhf-0 cKRjba">http://www.springer.com/gp/book/9783642276446#</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning: An Introduction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Mononofu/reinforcement-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Mononofu/reinforcement-learning</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" class="Link-sc-1brdqhf-0 cKRjba">http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html</a></li><li>course: <a target="_blank" rel="noopener noreferrer" href="http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html" class="Link-sc-1brdqhf-0 cKRjba">http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html</a></li><li>book(1st edition): <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1jkaMq" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1jkaMq</a></li><li>book(2rd edition): <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1dDnNEnR" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1dDnNEnR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning: An Introduction (Second edition, Draft)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>book: <a target="_blank" rel="noopener noreferrer" href="https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1slrMYkP" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1slrMYkP</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ShangtongZhang/reinforcement-learning-an-introduction</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Self Learning Quant</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: explain and show the concept of self reinforcement learning combined with a neural network</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@danielzakrisson/the-self-learning-quant-d3329fcc9915#.9lsa5rh3e" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@danielzakrisson/the-self-learning-quant-d3329fcc9915#.9lsa5rh3e</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/danielzak/sl-quant" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/danielzak/sl-quant</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforcement Learning: An Introduction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>author: Richard S. Sutton and Andrew G. Barto</li><li>book: <a target="_blank" rel="noopener noreferrer" href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html" class="Link-sc-1brdqhf-0 cKRjba">https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html</a></li><li>solutions: <a target="_blank" rel="noopener noreferrer" href="https://github.com/btaba/intro-to-rl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/btaba/intro-to-rl</a></li></ul><h1 id="resources" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resources" color="auto.gray.8" aria-label="Resources permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Resources</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning Papers</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/junhyukoh/deep-reinforcement-learning-papers" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/junhyukoh/deep-reinforcement-learning-papers</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>website: <a target="_blank" rel="noopener noreferrer" href="http://aikorea.org/awesome-rl/?utm_content=buffer5d0f3&amp;utm_medium=social&amp;utm_source=plus.google.com&amp;utm_campaign=buffer#online-demos" class="Link-sc-1brdqhf-0 cKRjba">http://aikorea.org/awesome-rl/?utm_content=buffer5d0f3&amp;utm_medium=social&amp;utm_source=plus.google.com&amp;utm_campaign=buffer#online-demos</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aikorea/awesome-rl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aikorea/awesome-rl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning Papers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/muupan/deep-reinforcement-learning-papers" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/muupan/deep-reinforcement-learning-papers</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning 深度增强学习资源</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/20885568" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/20885568</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep-reinforcement-learning-networks: A list of deep neural network architectures for reinforcement learning tasks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/5vision/deep-reinforcement-learning-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/5vision/deep-reinforcement-learning-networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Studying Reinforcement Learning Guide</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/0bserver07/Study-Reinforcement-Learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/0bserver07/Study-Reinforcement-Learning</a></li></ul><h1 id="reading-and-questions" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#reading-and-questions" color="auto.gray.8" aria-label="Reading and Questions permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reading and Questions</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What are the best books about reinforcement learning?</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.quora.com/What-are-the-best-books-about-reinforcement-learning" class="Link-sc-1brdqhf-0 cKRjba">https://www.quora.com/What-are-the-best-books-about-reinforcement-learning</a></p><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/28/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>