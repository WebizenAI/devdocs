<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Papers Using Very Deep Autoencoders for Content-Based Image Retrieval intro: ESANN 2011. Alex Krizhevsky, and Geoffrey E. Hinton paper:  ht…"/><meta data-react-helmet="true" name="twitter:title" content="Image Retrieval"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-28T19:22:29.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Papers Using Very Deep Autoencoders for Content-Based Image Retrieval intro: ESANN 2011. Alex Krizhevsky, and Geoffrey E. Hinton paper:  ht…"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Image Retrieval"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Papers Using Very Deep Autoencoders for Content-Based Image Retrieval intro: ESANN 2011. Alex Krizhevsky, and Geoffrey E. Hinton paper:  ht…"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,HGjBQ,fdzjHV,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Image Retrieval - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/embed-link/">Embed Link</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/posts/">Posts</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/privacy-policy/">Privacy Policy</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#hashing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Hashing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#cross-modal-retrieval" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Cross Modal Retrieval</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-indexing--retrieval" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Indexing / Retrieval</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#learning-to-rank" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Learning to Rank</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-metric-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Metric Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#talks--slides" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Talks / Slides</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects-1" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tutorials" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorials</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Image Retrieval</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#hashing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Hashing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#cross-modal-retrieval" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Cross Modal Retrieval</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-indexing--retrieval" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Indexing / Retrieval</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#learning-to-rank" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Learning to Rank</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-metric-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Metric Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#talks--slides" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Talks / Slides</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects-1" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tutorials" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorials</a></li></ul></div></div><h1 id="papers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers" color="auto.gray.8" aria-label="Papers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using Very Deep Autoencoders for Content-Based Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ESANN 2011. Alex Krizhevsky, and Geoffrey E. Hinton</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1312.4740" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1312.4740</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://legacy.openreview.net/document/90fc8dad-ad02-4ddc-ab06-e7b55706869d#90fc8dad-ad02-4ddc-ab06-e7b55706869d" class="Link-sc-1brdqhf-0 cKRjba">http://legacy.openreview.net/document/90fc8dad-ad02-4ddc-ab06-e7b55706869d#90fc8dad-ad02-4ddc-ab06-e7b55706869d</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Codes for Image Retrieval</strong></p><img src="http://sites.skoltech.ru/app/data/uploads/sites/25/2014/11/example-e1404721339557.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2014</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://sites.skoltech.ru/compvision/projects/neuralcodes/" class="Link-sc-1brdqhf-0 cKRjba">http://sites.skoltech.ru/compvision/projects/neuralcodes/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1404.1777" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1404.1777</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/arbabenko/Spoc" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/arbabenko/Spoc</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient On-the-fly Category Retrieval using ConvNets and GPUs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1407.4764" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1407.4764</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning visual similarity for product design with convolutional neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SIGGRAPH 2015</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.cornell.edu/~kb/publications/SIG15ProductNet.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.cornell.edu/~kb/publications/SIG15ProductNet.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://dl.acm.org.sci-hub.cc/citation.cfm?doid=2809654.2766959" class="Link-sc-1brdqhf-0 cKRjba">http://dl.acm.org.sci-hub.cc/citation.cfm?doid=2809654.2766959</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploiting Local Features from Deep Networks for Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR DeepVision Workshop 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1504.05133" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1504.05133</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual Search at Pinterest</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge and Discovery and Data Mining, 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.07647" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.07647</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://engineering.pinterest.com/blog/introducing-new-way-visually-search-pinterest" class="Link-sc-1brdqhf-0 cKRjba">https://engineering.pinterest.com/blog/introducing-new-way-visually-search-pinterest</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Aggregating Deep Convolutional Features for Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>intro: Sum pooing</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.07493" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.07493</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Particular object retrieval with integral max-pooling of CNN activations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: use max-pooling to aggregate the deep descriptors, R-MAC (regional maximum activation of convolutions)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1511.05879" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1511.05879</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Group Invariant Deep Representations for Image Instance Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.02093" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.02093</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Where to Buy It: Matching Street Clothing Photos in Online Shops</strong></p><img src="http://www.tamaraberg.com/street2shop/header.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>hmepage: <a target="_blank" rel="noopener noreferrer" href="http://www.tamaraberg.com/street2shop/" class="Link-sc-1brdqhf-0 cKRjba">http://www.tamaraberg.com/street2shop/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.tamaraberg.com/papers/street2shop.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.tamaraberg.com/papers/street2shop.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Kiapour_Where_to_Buy_ICCV_2015_paper.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Kiapour_Where_to_Buy_ICCV_2015_paper.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Natural Language Object Retrieval</strong></p><img src="http://ronghanghu.com/wp-content/uploads/method-900x353.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://ronghanghu.com/text_obj_retrieval/" class="Link-sc-1brdqhf-0 cKRjba">http://ronghanghu.com/text_obj_retrieval/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.04164" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.04164</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://ronghanghu.com/slides/cvpr16_text_obj_retrieval_slides.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://ronghanghu.com/slides/cvpr16_text_obj_retrieval_slides.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ronghanghu/natural-language-object-retrieval" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ronghanghu/natural-language-object-retrieval</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/andrewliao11/Natural-Language-Object-Retrieval-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andrewliao11/Natural-Language-Object-Retrieval-tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Image Retrieval: Learning global representations for image search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.xrce.xerox.com/Research-Development/Computer-Vision/Learning-Visual-Representations/Deep-Image-Retrieval" class="Link-sc-1brdqhf-0 cKRjba">http://www.xrce.xerox.com/Research-Development/Computer-Vision/Learning-Visual-Representations/Deep-Image-Retrieval</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.01325" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.01325</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.slideshare.net/xavigiro/deep-image-retrieval-learning-global-representations-for-image-search" class="Link-sc-1brdqhf-0 cKRjba">http://www.slideshare.net/xavigiro/deep-image-retrieval-learning-global-representations-for-image-search</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end Learning of Deep Visual Representations for Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCV 2017. Extended version of our ECCV2016 paper &quot;Deep Image Retrieval: Learning global representations for image search&quot;</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.xrce.xerox.com/Research-Development/Computer-Vision/Learning-Visual-Representations/Deep-Image-Retrieval" class="Link-sc-1brdqhf-0 cKRjba">http://www.xrce.xerox.com/Research-Development/Computer-Vision/Learning-Visual-Representations/Deep-Image-Retrieval</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.07940" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.07940</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bags of Local Convolutional Features for Scalable Instance Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICMR 2016. Best Poster Award at ICMR 2016.</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://imatge-upc.github.io/retrieval-2016-icmr/" class="Link-sc-1brdqhf-0 cKRjba">https://imatge-upc.github.io/retrieval-2016-icmr/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.04653" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.04653</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/imatge-upc/retrieval-2016-icmr" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/imatge-upc/retrieval-2016-icmr</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.slideshare.net/xavigiro/convolutional-features-for-instance-search" class="Link-sc-1brdqhf-0 cKRjba">http://www.slideshare.net/xavigiro/convolutional-features-for-instance-search</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Faster R-CNN Features for Instance Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepVision Workshop in CVPR 2016</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://imatge-upc.github.io/retrieval-2016-deepvision/" class="Link-sc-1brdqhf-0 cKRjba">http://imatge-upc.github.io/retrieval-2016-deepvision/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.08893" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.08893</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/imatge-upc/retrieval-2016-deepvision" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/imatge-upc/retrieval-2016-deepvision</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Where to Focus: Query Adaptive Matching for Instance Retrieval Using Convolutional Feature Maps</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: query adaptive matching (QAM), Feature Map Pooling, Overlapped Spatial Pyramid Pooling (OSPP)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.06811" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.06811</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Training For Sketch Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.02748" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.02748</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016. DeepBit</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Learning_Compact_Binary_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Learning_Compact_Binary_CVPR_2016_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kevinlin311tw/cvpr16-deepbit" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kevinlin311tw/cvpr16-deepbit</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Training of Triplet-based Deep Binary Embedding Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1603.02844" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1603.02844</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhuang_Fast_Training_of_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhuang_Fast_Training_of_CVPR_2016_paper.pdf</a></li><li>bitbucket(official): <a target="_blank" rel="noopener noreferrer" href="https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks" class="Link-sc-1brdqhf-0 cKRjba">https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>intro: vehicle re-identification, vehicle retrieval. coupled clusters loss</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Relative_Distance_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Relative_Distance_CVPR_2016_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</strong></p><img src="http://personal.ie.cuhk.edu.hk/~lz013/projects/deepfashion/intro.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016. FashionNet</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples</strong></p><img src="http://ptak.felk.cvut.cz/personal/radenfil/siamac/siamac.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>project page(paper+code+data): <a target="_blank" rel="noopener noreferrer" href="http://cmp.felk.cvut.cz/~radenfil/projects/siamac.html" class="Link-sc-1brdqhf-0 cKRjba">http://cmp.felk.cvut.cz/~radenfil/projects/siamac.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.02426" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.02426</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://cmp.felk.cvut.cz/~radenfil/publications/Radenovic-ECCV16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cmp.felk.cvut.cz/~radenfil/publications/Radenovic-ECCV16.pdf</a></li><li>code(Matlab): <a target="_blank" rel="noopener noreferrer" href="http://ptak.felk.cvut.cz/personal/radenfil/siamac/siaMAC_code.tar.gz" class="Link-sc-1brdqhf-0 cKRjba">http://ptak.felk.cvut.cz/personal/radenfil/siamac/siaMAC_code.tar.gz</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PicHunt: Social Media Image Retrieval for Improved Law Enforcement</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.00905" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.00905</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SIFT Meets CNN: A Decade Survey of Instance Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.01807" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.01807</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://sketchy.eye.gatech.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://sketchy.eye.gatech.edu/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cc.gatech.edu/~hays/tmp/sketchy-database.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cc.gatech.edu/~hays/tmp/sketchy-database.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/janesjanes/sketchy" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/janesjanes/sketchy</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01640" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01640</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Retrieval with Deep Local Features and Attention-based Keypoints</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.05478" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.05478</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Internet-Based Image Retrieval Using End-to-End Trained Deep Distributions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.07697" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.07697</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compression of Deep Neural Networks for Image Instance Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DCC 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.04923" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.04923</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Effective Multi-Query Expansions: Collaborative Deep Networks for Robust Landmark Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.05003" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.05003</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.00338" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.00338</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Geometric Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.06383" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.06383</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context Aware Query Image Representation for Particular Object Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.arxiv.org/abs/1703.01226" class="Link-sc-1brdqhf-0 cKRjba">https://www.arxiv.org/abs/1703.01226</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An End-to-End Approach to Natural Language Object Retrieval via Context-Aware Deep Reinforcement Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07579" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07579</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AMC: Attention guided Multi-modal Correlation Learning for Image Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.00763" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.00763</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kanchen-usc/amc_att" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kanchen-usc/amc_att</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video2Shop: Exactly Matching Clothes in Videos to Online Shopping Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>keywrods: AsymNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.05287" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.05287</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep image representations using caption generators</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.09142" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.09142</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual Search at eBay</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 23rd SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.03154" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.03154</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sampling Matters in Deep Embedding Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UT Austin &amp; A9/Amazon</li><li>keywords: distance weighted sampling</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.07567" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.07567</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One-Shot Fine-Grained Instance Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.00811" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.00811</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Selective Deep Convolutional Features for Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.00809" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.00809</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Class-Weighted Convolutional Features for Visual Instance Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017. Universitat Politecnica de Catalunya Barcelona &amp; CSIRO</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://imatge-upc.github.io/retrieval-2017-cam/" class="Link-sc-1brdqhf-0 cKRjba">http://imatge-upc.github.io/retrieval-2017-cam/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.02581" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.02581</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/imatge-upc/retrieval-2017-cam" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/imatge-upc/retrieval-2017-cam</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Repression Network for Precise Vehicle Search</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02386" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02386</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SUBIC: A supervised, structured binary code for image search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017 (Spotlight). Technicolor &amp; INRIA Rennes &amp; Amazon</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02932" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02932</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pruning Convolutional Neural Networks for Image Instance Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.05455" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.05455</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image2song: Song Retrieval via Bridging Image Content and Lyric Words</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. Chinese Academy of Sciences &amp; Northwestern Polytechnical University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.05851" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.05851</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Region-Based Image Retrieval Revisited</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia 2017 (Oral)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.09106" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.09106</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Part Models: Person Retrieval with Refined Part Pooling</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09349" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09349</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Query-Adaptive R-CNN for Open-Vocabulary Object Detection and Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09509" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09509</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Saliency Weighted Convolutional Features for Instance Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Dublin City University &amp; Universitat Politecnica de Catalunya</li><li>keywords: local convolutional features (BLCF), human visual attention models (saliency)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://imatge-upc.github.io/salbow/" class="Link-sc-1brdqhf-0 cKRjba">https://imatge-upc.github.io/salbow/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10795" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10795</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10795" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10795</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepStyle: Multimodal Search Engine for Fashion and Interior Design</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.03002" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.03002</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>From Selective Deep Convolutional Features to Compact Binary Representations for Image Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02899" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02899</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Web-Scale Responsive Visual Search at Bing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.04914" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.04914</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Approximate Query Matching for Image Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05401" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05401</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Captioning and Retrieval with Natural Language</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06152" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06152</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Triplet-Center Loss for Multi-View 3D Object Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06189" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06189</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Collaborative Multi-modal deep learning for the personalized product retrieval in Facebook Marketplace</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook
= arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.12312" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.12312</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepFirearm: Learning Discriminative Feature Representation for Fine-grained Firearm Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.02984" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.02984</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jdhao/deep_firearm" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jdhao/deep_firearm</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance Search via Instance Level Segmentation and Feature Representation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.03576" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.03576</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Feature Aggregation with Heat Diffusion for Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.08587" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.08587</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pangsm0415/HeW" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pangsm0415/HeW</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single Shot Scene Text Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.09044" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.09044</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Embeddings for Product Visual Search with Triplet Loss and Online Sampling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Yahoo Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.04652" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.04652</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention-aware Generalized Mean Pooling for Image Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.00202" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.00202</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchy-based Image Embeddings for Semantic Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.09924" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.09924</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cvjena/semantic-embeddings" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cvjena/semantic-embeddings</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mean Local Group Average Precision (mLGAP): A New Performance Metric for Hashing-based Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.09763" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.09763</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance-level Sketch-based Retrieval by Deep Triplet Classification Siamese Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11375" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11375</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detect-to-Retrieve: Efficient Regional Aggregation for Image Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Cambridge &amp; Google AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.01584" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.01584</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning with Average Precision: Training Image Retrieval with a Listwise Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NAVER LABS Europe</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.07589" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.07589</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Benchmark on Tricks for Large-scale Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.11854" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.11854</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.12163" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.12163</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Keypoint-Aligned Embeddings for Image Retrieval and Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.11368" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.11368</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tasks Integrated Networks: Joint Detection and Retrieval for Image Search</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.01438" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.01438</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance-level Image Retrieval using Reranking Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Virginia &amp; eBay Computer Vision</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.12236" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.12236</a></li></ul><h1 id="hashing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#hashing" color="auto.gray.8" aria-label="Hashing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Hashing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Supervised Hashing for Image Retrieval via Image Representation Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2014. Sun Yat-Sen University &amp; National University of Singapore</li><li>keywords: CNNH (Convolutional Neural Network Hashing)</li><li>paper: <a class="Link-sc-1brdqhf-0 cKRjba" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8137/8861/">www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8137/8861</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://pdfs.semanticscholar.org/f633/8f23860f9c4808586bbc7e8907d33836147f.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://pdfs.semanticscholar.org/f633/8f23860f9c4808586bbc7e8907d33836147f.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Simultaneous Feature Learning and Hash Coding with Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015. Sun Yat-Sen University &amp; National University of Singapore</li><li>keywords: NINH (NIN Hashing), DNNH (Deep Neural Network Hashing)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1504.03410" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1504.03410</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lai_Simultaneous_Feature_Learning_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lai_Simultaneous_Feature_Learning_2015_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hashing by Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IBM T. J. Watson Research Center</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.columbia.edu/~wliu/WeiLiu_DLHash.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.columbia.edu/~wliu/WeiLiu_DLHash.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015. DSRH (Deep Semantic Ranking Hashing)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1501.06272" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1501.06272</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning of Binary Hash Codes for Fast Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR Workshop 2015</li><li>keywords: MNIST, CIFAR-10, Yahoo-1M. DLBHC (Deep Learning of Binary Hash Codes)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvprw15.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvprw15.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kevinlin311tw/caffe-cvprw15" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kevinlin311tw/caffe-cvprw15</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Supervised Learning of Semantics-Preserving Hashing via Deep Neural Networks for Large-Scale Image Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SSDH</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1507.00101" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1507.00101</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kevinlin311tw/Caffe-DeepBinaryCode" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kevinlin311tw/Caffe-DeepBinaryCode</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image Retrieval and Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Image Processing 2015</li><li>keywords: DRSCH (Deep Regularized Similarity Comparison Hashing)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://vision.sysu.edu.cn/projects/deephashing/" class="Link-sc-1brdqhf-0 cKRjba">http://vision.sysu.edu.cn/projects/deephashing/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1508.04535" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1508.04535</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ruixuejianfei/BitScalableDeepHash" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ruixuejianfei/BitScalableDeepHash</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Supervised Hashing for Fast Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>keywords: DSH (Deep Supervised Hashing)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.jdl.ac.cn/doc/2011/201711214443668218_deep%20supervised%20hashing%20for%20fast%20image%20retrieval_cvpr2016.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.jdl.ac.cn/doc/2011/201711214443668218_deep%20supervised%20hashing%20for%20fast%20image%20retrieval_cvpr2016.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lhmRyan/deep-supervised-hashing-DSH" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lhmRyan/deep-supervised-hashing-DSH</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Hashing Network for Efficient Similarity Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2016</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12039" class="Link-sc-1brdqhf-0 cKRjba">http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12039</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Learning based Deep Supervised Hashing with Pairwise Labels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1511.03855" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1511.03855</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.ijcai.org/Proceedings/16/Papers/245.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.ijcai.org/Proceedings/16/Papers/245.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://cs.nju.edu.cn/lwj/paper/IJCAI16_DPSH.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://cs.nju.edu.cn/lwj/paper/IJCAI16_DPSH.pdf</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://cs.nju.edu.cn/lwj/code/DPSH_code.rar" class="Link-sc-1brdqhf-0 cKRjba">http://cs.nju.edu.cn/lwj/code/DPSH_code.rar</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Cross-Modal Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1602.02255" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1602.02255</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.11013" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.11013</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SSDH: Semi-supervised Deep Hashing for Large Scale Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.08477" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.08477</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Semantic-Preserving and Ranking-Based Hashing for Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/Deep-Semantic-Preserving-and-Ranking-Based-Hashing-for-Image-Retrieval.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/Deep-Semantic-Preserving-and-Ranking-Based-Hashing-for-Image-Retrieval.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Hashing: A Joint Approach for Image Signature Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03658" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03658</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transitive Hashing Network for Heterogeneous Multimedia Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: state of the art on NUS-WIDE, ImageNet-YahooQA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04307" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04307</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Residual Hashing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.05400" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.05400</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Region Hashing for Efficient Large-scale Instance Search from Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Columbia University &amp; University of Electronic Science and Technology of China</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.07901" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.07901</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HashNet: Deep Learning to Hash by Continuation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.00758" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.00758</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/thuml/HashNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/thuml/HashNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Triplet Hashing for Fast Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://www.arxiv.org/abs/1702.08798" class="Link-sc-1brdqhf-0 cKRjba">https://www.arxiv.org/abs/1702.08798</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Sketch Hashing: Fast Free-hand Sketch-Based Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 spotlight paper</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.05605" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.05605</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Robust Hash Codes for Multiple Instance Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.05724" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.05724</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Simultaneous Feature Aggregating and Hashing for Large-scale Image Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.00860" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.00860</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Hash</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://cs.nju.edu.cn/lwj/L2H.html" class="Link-sc-1brdqhf-0 cKRjba">https://cs.nju.edu.cn/lwj/L2H.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hashing as Tie-Aware Learning to Rank</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08562" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08562</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Hashing Network for Unsupervised Domain Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.07522" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.07522</a></li><li>github(MatConvNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/hemanthdv/da-hash" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hemanthdv/da-hash</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Binary Reconstruction for Cross-modal Hashing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.05127" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.05127</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Revisit on Deep Hashings for Large-scale Content Based Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University</li><li>arixv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06016" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06016</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: finegrained sketch-based image retrieval (FG-SBIR) and Person Re-identification (person ReID)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08106" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08106</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ForestHash: Semantic Hashing With Shallow Random Forests and Tiny Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08364" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08364</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Supervised Hashing with End-to-End Binary Deep Neural Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08901" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08901</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transfer Adversarial Hashing for Hamming Space Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.04616" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.04616</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dual Asymmetric Deep Hashing Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.08360" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.08360</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attribute-Guided Network for Cross-Modal Zero-Shot Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.01943" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.01943</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning for Image Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02904" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02904</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hashing with Mutual Information</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.00974" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.00974</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Sketch-Image Hashing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 spotlight</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.02284" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.02284</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance Similarity Deep Hashing for Multi-Label Image Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.02987" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.02987</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Class-Wise Hashing: Semantics-Preserving Hashing via Class-wise Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: City University of Hong Kong</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.04137" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.04137</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Semantic Deep Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06911" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06911</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.01401" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.01401</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Deep Binary Embedding Networks by Order-aware Reweighting of Triplets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Sun Yat-sen University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.06061" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.06061</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Semantic Hashing with Generative Adversarial Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SIGIR 2017 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.08275" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.08275</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Ordinal Hashing with Spatial Attention</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.02459" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.02459</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient end-to-end learning for quantizable representations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2018. Seoul National University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.05809" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.05809</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/maestrojeong/Deep-Hash-Table-ICML18" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/maestrojeong/Deep-Hash-Table-ICML18</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Deep Image Hashing through Tag Embeddings</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.05804" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.05804</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Learning for Fine-grained Image Search</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.02247" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.02247</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Error Correction Maximization for Deep Image Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.01942" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.01942</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Priority Hashing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2018 Poster</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.01238" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.01238</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neurons Merging Layer: Towards Progressive Redundancy Reduction for Deep Supervised Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.02302" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.02302</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep LDA Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.03402" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.03402</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Triplet Quantization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia 2018 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.00153" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.00153</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SADIH: Semantic-Aware DIscrete Hashing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01739" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01739</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Pyramid Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02325" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02325</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Global Hashing System for Fast Image Search</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.08685" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.08685</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-Distilled Hashing for Deep Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Seoul National University &amp; NAVER/LINE Vision</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.08816" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.08816</a></li></ul><h1 id="cross-modal-retrieval" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#cross-modal-retrieval" color="auto.gray.8" aria-label="Cross Modal Retrieval permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Cross Modal Retrieval</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>intro: DARN, cross-entropy loss, triplet loss</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.07922" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.07922</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Content-Based, Cross-Modal Retrieval of Videos and Music</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.06761" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.06761</a></li><li>supplementary: <a target="_blank" rel="noopener noreferrer" href="https://youtu.be/ZyINqDMo3Fg" class="Link-sc-1brdqhf-0 cKRjba">https://youtu.be/ZyINqDMo3Fg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Binaries: Encoding Semantic-Rich Cues for Efficient Textual-Visual Cross Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02531" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02531</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MHTN: Modal-adversarial Hybrid Transfer Network for Cross-modal Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.04308" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.04308</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-Domain Image Retrieval with Attention Modeling</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.01784" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.01784</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06420" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06420</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09347" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09347</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Objects that Sound</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind, VGG</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.06651" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.06651</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-modal Embeddings for Video and Audio Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02200" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02200</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/surisdi/youtube-8m" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/surisdi/youtube-8m</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learnable PINs: Cross-Modal Embeddings for Person Identity</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: VGG</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.00833" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.00833</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting Cross Modal Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCVW (MULA 2018)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.07364" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.07364</a></li></ul><h2 id="projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HABIR哈希图像检索工具箱</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Various hashing methods for image retrieval and serves as the baselines</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://yongyuan.name/habir/" class="Link-sc-1brdqhf-0 cKRjba">http://yongyuan.name/habir/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/willard-yuan/hashing-baseline-for-image-retrieval" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/willard-yuan/hashing-baseline-for-image-retrieval</a></li></ul><h1 id="video-indexing--retrieval" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-indexing--retrieval" color="auto.gray.8" aria-label="Video Indexing / Retrieval permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Indexing / Retrieval</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Video Retrieval via Deep Learning of Binary Hash Representations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11893/12117" class="Link-sc-1brdqhf-0 cKRjba">https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11893/12117</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Based Semantic Video Indexing and Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1601.07754" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1601.07754</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Joint Representations of Videos and Sentences with Web Image Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 4th Workshop on Web-scale Vision and Social Media (VSM), ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.02367" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.02367</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-View Product Image Search Using ConvNets Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03462" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03462</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalisation and Sharing in Triplet Convnets for Sketch based Visual Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05301" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05301</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Binary Subspace Coding for Query-by-Image Video Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01657" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01657</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Action Search: Learning to Search for Human Activities in Untrimmed Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.04269" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.04269</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Supervised Hashing with Triplet Labels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03900" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03900</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Supervised Deep Hashing for Hierarchical Labeled Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.02088" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.02088</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Localizing Moments in Video with Natural Language</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.01641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.01641</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dress like a Star: Retrieving Fashion Products from Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Aston University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.07198" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.07198</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Hashing with Category Mask for Fast Video Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.08315" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.08315</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Focus: Querying Large Video Datasets with Low Latency and Low Cost</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.03493" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.03493</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Boston University, University of British Columbia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.05113" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.05113</a></li></ul><h1 id="learning-to-rank" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#learning-to-rank" color="auto.gray.8" aria-label="Learning to Rank permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Learning to Rank</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Simple to Complex Cross-modal Learning to Rank</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Xi’an Jiaotong University &amp; University of Technology Sydney &amp; National University of Singapore &amp; CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.01229" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.01229</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SoDeep: a Sorting Deep net to learn ranking loss surrogates</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.04272" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.04272</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/technicolor-research/sodeep" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/technicolor-research/sodeep</a></li></ul><h1 id="deep-metric-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-metric-learning" color="auto.gray.8" aria-label="Deep Metric Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Metric Learning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep metric learning using Triplet network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1412.6622" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1412.6622</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://tce.technion.ac.il/wp-content/uploads/sites/8/2016/01/Elad-Hofer.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://tce.technion.ac.il/wp-content/uploads/sites/8/2016/01/Elad-Hofer.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/eladhoffer/TripletNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/eladhoffer/TripletNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improved Deep Metric Learning with Multi-class N-pair Loss Objective</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Metric Learning with Adaptive Density Discrimination</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2016. Facebook AI Research &amp; UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1511.05939" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1511.05939</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pumpikano/tf-magnet-loss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pumpikano/tf-magnet-loss</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vithursant/MagnetLoss-PyTorch/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vithursant/MagnetLoss-PyTorch/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hard-Aware Deeply Cascaded Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05720" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05720</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaded-Embedding_release" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaded-Embedding_release</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaed-Embedding" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaed-Embedding</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learnable Structured Clustering Framework for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01213" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01213</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning via Lifted Structured Feature Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>project page(code+data): <a target="_blank" rel="noopener noreferrer" href="http://cvgl.stanford.edu/projects/lifted_struct/" class="Link-sc-1brdqhf-0 cKRjba">http://cvgl.stanford.edu/projects/lifted_struct/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Song_Deep_Metric_Learning_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Song_Deep_Metric_Learning_CVPR_2016_paper.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://cvgl.stanford.edu/papers/song_cvpr16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cvgl.stanford.edu/papers/song_cvpr16.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rksltnl/Deep-Metric-Learning-CVPR16" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rksltnl/Deep-Metric-Learning-CVPR16</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rksltnl/Caffe-Deep-Metric-Learning-CVPR16" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rksltnl/Caffe-Deep-Metric-Learning-CVPR16</a></li><li>dataset: <a target="_blank" rel="noopener noreferrer" href="ftp://cs.stanford.edu/cs/cvgl/Stanford_Online_Products.zip" class="Link-sc-1brdqhf-0 cKRjba">ftp://cs.stanford.edu/cs/cvgl/Stanford_Online_Products.zip</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-modal Deep Metric Learning with Multi-task Regularization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07026" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07026</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Smart Mining for Deep Metric Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.01285" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.01285</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TuSimple</li><li>keywords: pedestrian re-identification</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.01220" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.01220</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning with Angular Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.01682" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.01682</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.04815" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.04815</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.09662" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.09662</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalization in Metric Learning: Should the Embedding Layer be the Embedding Layer?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Georgia Tech</li><li>keywords: Cars-196, CUB-200-2011 and Stanford Online Product</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.03310" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.03310</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/bnulihaixia/Deep_metric" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bnulihaixia/Deep_metric</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention-based Ensemble for Deep Metric Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00382" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00382</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Online Deep Metric Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.05510" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.05510</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Randomized Ensembles for Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.04469" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.04469</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/littleredxh/DREML" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/littleredxh/DREML</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning with Hierarchical Triplet Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.06951" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.06951</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ranked List Loss for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.03238" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.03238</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hardness-Aware Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.05503" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.05503</a></li><li>github(official, Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/wzzheng/HDML" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wzzheng/HDML</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02616" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02616</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.06627" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.06627</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MalongTech/research-ms-loss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MalongTech/research-ms-loss</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning Beyond Binary Supervision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.09626" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.09626</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.05235" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.05235</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Group Loss for Deep Metric Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.00385" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.00385</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: NAVER Corp.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.02546" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.02546</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Proxy Anchor Loss for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.13911" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.13911</a></li><li>github(official, Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/tjddus9597/Proxy-Anchor-CVPR2020" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tjddus9597/Proxy-Anchor-CVPR2020</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spherical Feature Transform for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.01469" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.01469</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diversified Mutual Learning for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV Workshop 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.04170" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.04170</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning with Spherical Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.02785" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.02785</a></li><li>github(Pytorch):<a target="_blank" rel="noopener noreferrer" href="https://github.com/Dyfine/SphericalEmbedding" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Dyfine/SphericalEmbedding</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Intra-Batch Connections for Deep Metric Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.07753" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.07753</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.09335" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.09335</a></li></ul><h1 id="talks--slides" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#talks--slides" color="auto.gray.8" aria-label="Talks / Slides permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Talks / Slides</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TiefVision: end-to-end image similarity search engine</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: It covers image classification, image location ( OverFeat ) and image similarity ( Deep Ranking).</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/presentation/d/16hrXJhOzkbmla9AL7JCreCuBsa5L80gm71Pfrjo7F9Y/edit#slide=id.p" class="Link-sc-1brdqhf-0 cKRjba">https://docs.google.com/presentation/d/16hrXJhOzkbmla9AL7JCreCuBsa5L80gm71Pfrjo7F9Y/edit#slide=id.p</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/paucarre/tiefvision" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/paucarre/tiefvision</a></li></ul><h1 id="projects-1" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Open source deep learning based image retrieval toolbox based on PyTorch</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.02154" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.02154</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PyRetri/PyRetri" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PyRetri/PyRetri</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>图像检索：CNN卷积神经网络与实战</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNN for Image Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://yongyuan.name/blog/CBIR-CNN-and-practice.html" class="Link-sc-1brdqhf-0 cKRjba">http://yongyuan.name/blog/CBIR-CNN-and-practice.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/willard-yuan/CNN-for-Image-Retrieval" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/willard-yuan/CNN-for-Image-Retrieval</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://yongyuan.name/pic/" class="Link-sc-1brdqhf-0 cKRjba">http://yongyuan.name/pic/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual Search Server</strong></p><img src="https://raw.githubusercontent.com/AKSHAYUBHAT/VisualSearchServer/master/appcode/static/alpha3.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A simple implementation of Visual Search using features extracted from Tensorflow inception model and Approximate Nearest Neighbors </li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/AKSHAYUBHAT/VisualSearchServer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/AKSHAYUBHAT/VisualSearchServer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Retrieval: vehicle image retrieval using k CNNs ensemble method</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ranked 1st and won the special prize in the final of
the 3rd National Gradute Contest on Smart-CIty Technology and Creative Design, China</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://www.pkuml.org/resources/pku-vehicleid.html" class="Link-sc-1brdqhf-0 cKRjba">https://www.pkuml.org/resources/pku-vehicleid.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iamhankai/vehicle-retrieval-kCNNs" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iamhankai/vehicle-retrieval-kCNNs</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A visual search engine based on Elasticsearch and Tensorflow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: faster r-cnn</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tuan3w/visual_search" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tuan3w/visual_search</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Siamese and triplet networks with online pair/triplet mining in PyTorch</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/adambielski/siamese-triplet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/adambielski/siamese-triplet</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Triplet Loss and Online Triplet Mining in TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://omoindrot.github.io/triplet-loss" class="Link-sc-1brdqhf-0 cKRjba">https://omoindrot.github.io/triplet-loss</a></li><li>gtihub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/omoindrot/tensorflow-triplet-loss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/omoindrot/tensorflow-triplet-loss</a></li></ul><h1 id="blogs" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#blogs" color="auto.gray.8" aria-label="Blogs permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blogs</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Where can I buy a chair like that? – This app will tell you</strong></p><img src="http://www.news.cornell.edu/sites/chronicle.cornell/files/GrokStyleApp.png?itok=3jd_S2R7" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.news.cornell.edu/stories/2016/08/where-can-i-buy-chair-app-will-tell-you" class="Link-sc-1brdqhf-0 cKRjba">http://www.news.cornell.edu/stories/2016/08/where-can-i-buy-chair-app-will-tell-you</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using Sketches to Search for Products Online</strong></p><img src="http://sketchx.eecs.qmul.ac.uk/wp-content/uploads/sites/27/2016/04/slider_template_cvpr4-1.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://sketchx.eecs.qmul.ac.uk/" class="Link-sc-1brdqhf-0 cKRjba">http://sketchx.eecs.qmul.ac.uk/</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://news.developer.nvidia.com/using-sketches-to-search-for-products-online/" class="Link-sc-1brdqhf-0 cKRjba">https://news.developer.nvidia.com/using-sketches-to-search-for-products-online/</a></li></ul><h1 id="tutorials" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tutorials" color="auto.gray.8" aria-label="Tutorials permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tutorials</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Image Retrieval: Learning global representations for image search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=yT52xDML6ys" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=yT52xDML6ys</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Instance Retrieval: Overview of state-of-the-art</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=EYq-rpaZn1o" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=EYq-rpaZn1o</a></li></ul><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/28/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>