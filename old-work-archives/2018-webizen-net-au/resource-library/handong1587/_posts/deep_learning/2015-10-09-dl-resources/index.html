<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="ImageNet Single-model on 224x224 Method top1 top5 Model Size Speed ResNet-101 78.0% 94.0% ResNet-200 78.3% 94.2% Inception-v3 Inception-v4 …"/><meta data-react-helmet="true" name="twitter:title" content="Deep Learning Resources"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-30T11:53:44.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="ImageNet Single-model on 224x224 Method top1 top5 Model Size Speed ResNet-101 78.0% 94.0% ResNet-200 78.3% 94.2% Inception-v3 Inception-v4 …"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Deep Learning Resources"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="ImageNet Single-model on 224x224 Method top1 top5 Model Size Speed ResNet-101 78.0% 94.0% ResNet-200 78.3% 94.2% Inception-v3 Inception-v4 …"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,fdzjHV,HGjBQ,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.cxpRJj{font-size:20px;}/*!sc*/
data-styled.g25[id="heading__StyledH3-sc-1fu06k9-3"]{content:"cxpRJj,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.eooEiq{display:block;width:100%;margin:0 0 16px;overflow:auto;}/*!sc*/
.eooEiq th{font-weight:600;}/*!sc*/
.eooEiq th,.eooEiq td{padding:8px 16px;border:1px solid #e1e4e8;}/*!sc*/
.eooEiq tr{background-color:#ffffff;border-top:1px solid #e1e4e8;}/*!sc*/
.eooEiq tr:nth-child(2n){background-color:#f6f8fa;}/*!sc*/
.eooEiq img{background-color:transparent;}/*!sc*/
data-styled.g35[id="table__Table-sc-ixm5yk-0"]{content:"eooEiq,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Deep Learning Resources - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/HyperMedia Library/">HyperMedia Library</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">awesomeLists</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018 - Web Civics BizPlan/">EXECUTIVE SUMMARY</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#imagenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ImageNet</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#alexnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">AlexNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#network-in-network" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Network In Network</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#googlenet-inception-v1" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">GoogLeNet (Inception V1)</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#vggnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">VGGNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#inception-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Inception-V2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#inception-v3" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Inception-V3</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#resnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ResNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#resnet-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ResNet-V2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#inception-v4--inception-resnet-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Inception-V4 / Inception-ResNet-V2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#resnext" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ResNeXt</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#resnest" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ResNeSt</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#residual-networks-variants" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Residual Networks Variants</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#densenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DenseNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#densenet-20" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DenseNet 2.0</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#xception" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Xception</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mobilenets" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MobileNets</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mobilenetv2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MobileNetV2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#shufflenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ShuffleNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#shufflenet-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ShuffleNet V2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#senet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">SENet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#genet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">GENet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#imagenet-projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ImageNet Projects</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pre-training" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pre-training</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#transformers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Transformers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#semi-supervised-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Semi-Supervised Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#multi-label-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Multi-label Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#multi-task-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Multi-task Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#multi-modal-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Multi-modal Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#debugging-deep-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Debugging Deep Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#understanding-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Understanding CNN</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning Networks</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#convolutions--filters" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Convolutions / Filters</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#highway-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Highway Networks</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#spatial-transformer-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Spatial Transformer Networks</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#fractalnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">FractalNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#generative-models" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Generative Models</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-and-robots" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning and Robots</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-on-mobile--embedded-devices" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning on Mobile / Embedded Devices</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#benchmarks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Benchmarks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#tutorials-and-surveys" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorials and Surveys</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mathematics-of-deep-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Mathematics of Deep Learning</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#local-minima" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Local Minima</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#dive-into-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Dive Into CNN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#separable-convolutions--grouped-convolutions" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Separable Convolutions / Grouped Convolutions</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#stdp" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">STDP</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#target-propagation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Target Propagation</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#zero-shot-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Zero Shot Learning</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#incremental-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Incremental Learning</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#ensemble-deep-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Ensemble Deep Learning</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#domain-adaptation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Domain Adaptation</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#embedding" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Embedding</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#regression" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Regression</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#capsnets" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">CapsNets</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#low-light" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Low Light</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#computer-vision" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Computer Vision</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#readings-and-questions" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Readings and Questions</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#arxiv-pages" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Arxiv Pages</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#arxiv-sanity-preserver" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Arxiv Sanity Preserver</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#papers-with-code" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers with Code</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tools" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tools</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#challenges--hackathons" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Challenges / Hackathons</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#books" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Books</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Deep Learning Resources</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#imagenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ImageNet</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#alexnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">AlexNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#network-in-network" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Network In Network</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#googlenet-inception-v1" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">GoogLeNet (Inception V1)</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#vggnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">VGGNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#inception-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Inception-V2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#inception-v3" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Inception-V3</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#resnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ResNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#resnet-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ResNet-V2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#inception-v4--inception-resnet-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Inception-V4 / Inception-ResNet-V2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#resnext" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ResNeXt</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#resnest" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ResNeSt</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#residual-networks-variants" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Residual Networks Variants</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#densenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DenseNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#densenet-20" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DenseNet 2.0</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#xception" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Xception</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mobilenets" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MobileNets</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mobilenetv2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MobileNetV2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#shufflenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ShuffleNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#shufflenet-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ShuffleNet V2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#senet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">SENet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#genet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">GENet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#imagenet-projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ImageNet Projects</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pre-training" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pre-training</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#transformers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Transformers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#semi-supervised-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Semi-Supervised Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#multi-label-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Multi-label Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#multi-task-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Multi-task Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#multi-modal-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Multi-modal Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#debugging-deep-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Debugging Deep Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#understanding-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Understanding CNN</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning Networks</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#convolutions--filters" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Convolutions / Filters</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#highway-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Highway Networks</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#spatial-transformer-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Spatial Transformer Networks</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#fractalnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">FractalNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#generative-models" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Generative Models</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-and-robots" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning and Robots</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-on-mobile--embedded-devices" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning on Mobile / Embedded Devices</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#benchmarks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Benchmarks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#tutorials-and-surveys" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorials and Surveys</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mathematics-of-deep-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Mathematics of Deep Learning</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#local-minima" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Local Minima</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#dive-into-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Dive Into CNN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#separable-convolutions--grouped-convolutions" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Separable Convolutions / Grouped Convolutions</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#stdp" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">STDP</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#target-propagation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Target Propagation</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#zero-shot-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Zero Shot Learning</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#incremental-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Incremental Learning</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#ensemble-deep-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Ensemble Deep Learning</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#domain-adaptation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Domain Adaptation</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#embedding" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Embedding</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#regression" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Regression</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#capsnets" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">CapsNets</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#low-light" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Low Light</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#computer-vision" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Computer Vision</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#readings-and-questions" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Readings and Questions</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#arxiv-pages" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Arxiv Pages</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#arxiv-sanity-preserver" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Arxiv Sanity Preserver</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#papers-with-code" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers with Code</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tools" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tools</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#challenges--hackathons" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Challenges / Hackathons</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#books" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Books</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li></ul></div></div><h1 id="imagenet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#imagenet" color="auto.gray.8" aria-label="ImageNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ImageNet</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Single-model on 224x224</p><table class="table__Table-sc-ixm5yk-0 eooEiq"><thead><tr><th align="center">Method</th><th align="center">top1</th><th align="center">top5</th><th align="center">Model Size</th><th align="center">Speed</th></tr></thead><tbody><tr><td align="center">ResNet-101</td><td align="center">78.0%</td><td align="center">94.0%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">ResNet-200</td><td align="center">78.3%</td><td align="center">94.2%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Inception-v3</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Inception-v4</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Inception-ResNet-v2</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">ResNet-50</td><td align="center">77.8%</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">ResNet-101</td><td align="center">79.6%</td><td align="center">94.7%</td><td align="center"></td><td align="center"></td></tr></tbody></table><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Single-model on 320×320 / 299×299</p><table class="table__Table-sc-ixm5yk-0 eooEiq"><thead><tr><th align="center">Method</th><th align="center">top1</th><th align="center">top5</th><th align="center">Model Size</th><th align="center">Speed</th></tr></thead><tbody><tr><td align="center">ResNet-101</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">ResNet-200</td><td align="center">79.9%</td><td align="center">95.2%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Inception-v3</td><td align="center">78.8%</td><td align="center">94.4%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Inception-v4</td><td align="center">80.0%</td><td align="center">95.0%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Inception-ResNet-v2</td><td align="center">80.1%</td><td align="center">95.1%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">ResNet-50</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">ResNet-101</td><td align="center">80.9%</td><td align="center">95.6%</td><td align="center"></td><td align="center"></td></tr></tbody></table><h2 id="alexnet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#alexnet" color="auto.gray.8" aria-label="AlexNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>AlexNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>nips-page: <a target="_blank" rel="noopener noreferrer" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-" class="Link-sc-1brdqhf-0 cKRjba">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://code.google.com/p/cuda-convnet/" class="Link-sc-1brdqhf-0 cKRjba">https://code.google.com/p/cuda-convnet/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dnouri/cuda-convnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dnouri/cuda-convnet</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://code.google.com/p/cuda-convnet2/" class="Link-sc-1brdqhf-0 cKRjba">https://code.google.com/p/cuda-convnet2/</a></li></ul><h2 id="network-in-network" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#network-in-network" color="auto.gray.8" aria-label="Network In Network permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Network In Network</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Network In Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2014</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1312.4400" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1312.4400</a></li><li>gitxiv: <a target="_blank" rel="noopener noreferrer" href="http://gitxiv.com/posts/PA98qGuMhsijsJzgX/network-in-network-nin" class="Link-sc-1brdqhf-0 cKRjba">http://gitxiv.com/posts/PA98qGuMhsijsJzgX/network-in-network-nin</a></li><li>code(Caffe, official): <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/mavenlin/d802a5849de39225bcc6" class="Link-sc-1brdqhf-0 cKRjba">https://gist.github.com/mavenlin/d802a5849de39225bcc6</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Batch-normalized Maxout Network in Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.02583" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.02583</a></li></ul><h2 id="googlenet-inception-v1" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#googlenet-inception-v1" color="auto.gray.8" aria-label="GoogLeNet (Inception V1) permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GoogLeNet (Inception V1)</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Going Deeper with Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1409.4842" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1409.4842</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/inception" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/google/inception</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/soumith/inception.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/soumith/inception.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Building a deeper understanding of images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://googleresearch.blogspot.jp/2014/09/building-deeper-understanding-of-images.html" class="Link-sc-1brdqhf-0 cKRjba">http://googleresearch.blogspot.jp/2014/09/building-deeper-understanding-of-images.html</a></li></ul><h2 id="vggnet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#vggnet" color="auto.gray.8" aria-label="VGGNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>VGGNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Very Deep Convolutional Networks for Large-Scale Image Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" class="Link-sc-1brdqhf-0 cKRjba">http://www.robots.ox.ac.uk/~vgg/research/very_deep/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1409.1556" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1409.1556</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://llcao.net/cu-deeplearning15/presentation/cc3580_Simonyan.pptx" class="Link-sc-1brdqhf-0 cKRjba">http://llcao.net/cu-deeplearning15/presentation/cc3580_Simonyan.pptx</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://deeplearning.cs.cmu.edu/slides.2015/25.simonyan.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://deeplearning.cs.cmu.edu/slides.2015/25.simonyan.pdf</a></li><li>github(official, deprecated Caffe API): <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8" class="Link-sc-1brdqhf-0 cKRjba">https://gist.github.com/ksimonyan/211839e770f7b538e2d8</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ruimashita/caffe-train" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ruimashita/caffe-train</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tensorflow VGG16 and VGG19</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/machrisaa/tensorflow-vgg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/machrisaa/tensorflow-vgg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RepVGG: Making VGG-style ConvNets Great Again</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BNRist &amp; Tsinghua University &amp; MEGVII Technology &amp; Hong Kong University of Science and Technology &amp; Aberystwyth University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.03697" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.03697</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DingXiaoH/RepVGG" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DingXiaoH/RepVGG</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/megvii-model/RepVGG" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/megvii-model/RepVGG</a></li></ul><h2 id="inception-v2" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#inception-v2" color="auto.gray.8" aria-label="Inception-V2 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Inception-V2</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ImageNet top-5 error: 4.82%</li><li>keywords: internal covariate shift problem</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1502.03167" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1502.03167</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/" class="Link-sc-1brdqhf-0 cKRjba">https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://blog.csdn.net/happynear/article/details/44238541" class="Link-sc-1brdqhf-0 cKRjba">http://blog.csdn.net/happynear/article/details/44238541</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lim0606/caffe-googlenet-bn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lim0606/caffe-googlenet-bn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ImageNet pre-trained models with batch normalization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01452" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01452</a></li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.inf-cv.uni-jena.de/Research/CNN+Models.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.inf-cv.uni-jena.de/Research/CNN+Models.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cvjena/cnn-models" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cvjena/cnn-models</a></li></ul><h2 id="inception-v3" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#inception-v3" color="auto.gray.8" aria-label="Inception-V3 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Inception-V3</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Inception-V3 = Inception-V2 + BN-auxiliary (fully connected layer of the auxiliary classifier is also batch-normalized,
not just the convolutions)</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking the Inception Architecture for Computer Vision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network;
3.5% top-5 error and 17.3% top-1 error With an ensemble of 4 models and multi-crop evaluation.&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.00567" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.00567</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Moodstocks/inception-v3.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Moodstocks/inception-v3.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Inception in TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: demonstrate how to train the Inception v3 architecture</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/inception" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/inception</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Train your own image classifier with Inception in TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Inception-v3</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Notes on the TensorFlow Implementation of Inception v3</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://pseudoprofound.wordpress.com/2016/08/28/notes-on-the-tensorflow-implementation-of-inception-v3/" class="Link-sc-1brdqhf-0 cKRjba">https://pseudoprofound.wordpress.com/2016/08/28/notes-on-the-tensorflow-implementation-of-inception-v3/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training an InceptionV3-based image classifier with your own dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/danielvarga/keras-finetuning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/danielvarga/keras-finetuning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Inception-BN full for Caffe: Inception-BN ImageNet (21K classes) model for Caffe</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pertusa/InceptionBN-21K-for-Caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pertusa/InceptionBN-21K-for-Caffe</a></li></ul><h2 id="resnet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resnet" color="auto.gray.8" aria-label="ResNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ResNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Residual Learning for Image Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016 Best Paper Award</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.03385" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.03385</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf</a></li><li>gitxiv: <a target="_blank" rel="noopener noreferrer" href="http://gitxiv.com/posts/LgPRdTY3cwPBiMKbm/deep-residual-learning-for-image-recognition" class="Link-sc-1brdqhf-0 cKRjba">http://gitxiv.com/posts/LgPRdTY3cwPBiMKbm/deep-residual-learning-for-image-recognition</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KaimingHe/deep-residual-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KaimingHe/deep-residual-networks</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ry/tensorflow-resnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ry/tensorflow-resnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Third-party re-implementations</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/KaimingHe/deep-residual-networks#third-party-re-implementations" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KaimingHe/deep-residual-networks#third-party-re-implementations</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training and investigating Residual Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://torch.ch/blog/2016/02/04/resnets.html" class="Link-sc-1brdqhf-0 cKRjba">http://torch.ch/blog/2016/02/04/resnets.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebook/fb.resnet.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebook/fb.resnet.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>resnet.torch: an updated version of fb.resnet.torch with many changes.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/erogol/resnet.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/erogol/resnet.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Highway Networks and Deep Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://yanran.li/peppypapers/2016/01/10/highway-networks-and-deep-residual-networks.html" class="Link-sc-1brdqhf-0 cKRjba">http://yanran.li/peppypapers/2016/01/10/highway-networks-and-deep-residual-networks.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Interpretating Deep Residual Learning Blocks as Locally Recurrent Connections</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://matrixmashing.wordpress.com/2016/01/29/interpretating-deep-residual-learning-blocks-as-locally-recurrent-connections/" class="Link-sc-1brdqhf-0 cKRjba">https://matrixmashing.wordpress.com/2016/01/29/interpretating-deep-residual-learning-blocks-as-locally-recurrent-connections/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lab41 Reading Group: Deep Residual Learning for Image Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f" class="Link-sc-1brdqhf-0 cKRjba">https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>50-layer ResNet, trained on ImageNet, classifying webcam</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://ml4a.github.io/demos/keras.js/" class="Link-sc-1brdqhf-0 cKRjba">https://ml4a.github.io/demos/keras.js/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reproduced ResNet on CIFAR-10 and CIFAR-100 dataset.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/resnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/resnet</a></li></ul><h2 id="resnet-v2" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resnet-v2" color="auto.gray.8" aria-label="ResNet-V2 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ResNet-V2</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Identity Mappings in Deep Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016. ResNet-v2</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.05027" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.05027</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KaimingHe/resnet-1k-layers" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KaimingHe/resnet-1k-layers</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tornadomeet/ResNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tornadomeet/ResNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Residual Networks for Image Classification with Python + NumPy</strong></p><img src="https://dnlcrl.github.io/assets/thesis-post/Diagramma.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://dnlcrl.github.io/projects/2016/06/22/Deep-Residual-Networks-for-Image-Classification-with-Python+NumPy.html" class="Link-sc-1brdqhf-0 cKRjba">https://dnlcrl.github.io/projects/2016/06/22/Deep-Residual-Networks-for-Image-Classification-with-Python+NumPy.html</a></li></ul><h2 id="inception-v4--inception-resnet-v2" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#inception-v4--inception-resnet-v2" color="auto.gray.8" aria-label="Inception-V4 / Inception-ResNet-V2 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Inception-V4 / Inception-ResNet-V2</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Inception-V4, Inception-Resnet And The Impact Of Residual Connections On Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Workshop track - ICLR 2016. 3.08 % top-5 error on ImageNet CLS</li><li>intro: &quot;achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.07261" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.07261</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/kentsommer/keras-inceptionV4" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kentsommer/keras-inceptionV4</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The inception-resnet-v2 models trained from scratch via torch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lim0606/torch-inception-resnet-v2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lim0606/torch-inception-resnet-v2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Inception v4 in Keras</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Inception-v4, Inception - Resnet-v1 and v2</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/titu1994/Inception-v4" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/titu1994/Inception-v4</a></li></ul><h2 id="resnext" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resnext" color="auto.gray.8" aria-label="ResNeXt permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ResNeXt</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Aggregated Residual Transformations for Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. UC San Diego &amp; Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05431" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05431</a></li><li>github(Torch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/ResNeXt" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/ResNeXt</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol/resnext.py" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol/resnext.py</a></li><li>dataset: <a target="_blank" rel="noopener noreferrer" href="http://data.dmlc.ml/models/imagenet/resnext/" class="Link-sc-1brdqhf-0 cKRjba">http://data.dmlc.ml/models/imagenet/resnext/</a></li><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/5haml9/p_implementation_of_aggregated_residual/" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/5haml9/p_implementation_of_aggregated_residual/</a></li></ul><h2 id="resnest" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resnest" color="auto.gray.8" aria-label="ResNeSt permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ResNeSt</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ResNeSt: Split-Attention Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Amazon &amp; University of California</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.08955" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.08955</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhanghang1989/ResNeSt" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhanghang1989/ResNeSt</a></li></ul><h2 id="residual-networks-variants" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#residual-networks-variants" color="auto.gray.8" aria-label="Residual Networks Variants permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Residual Networks Variants</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Resnet in Resnet: Generalizing Residual Architectures</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://beta.openreview.net/forum?id=lx9l4r36gU2OVPy8Cv9g" class="Link-sc-1brdqhf-0 cKRjba">http://beta.openreview.net/forum?id=lx9l4r36gU2OVPy8Cv9g</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.08029" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.08029</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Residual Networks are Exponential Ensembles of Relatively Shallow Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.06431" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.06431</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Wide Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.07146" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.07146</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/szagoruyko/wide-residual-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/szagoruyko/wide-residual-networks</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/asmith26/wide_resnets_keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/asmith26/wide_resnets_keras</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ritchieng/wideresnet-tensorlayer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ritchieng/wideresnet-tensorlayer</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xternalz/WideResNet-pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xternalz/WideResNet-pytorch</a></li><li>github(Torch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/meliketoy/wide-residual-network" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/meliketoy/wide-residual-network</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Residual Networks of Residual Networks: Multilevel Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.02908" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.02908</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.05672" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.05672</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/masoudabd/multi-resnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/masoudabd/multi-resnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Pyramidal Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PyramidNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.02915" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.02915</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jhkim89/PyramidNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jhkim89/PyramidNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Identity Mappings with Residual Gates</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01260" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01260</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Wider or Deeper: Revisiting the ResNet Model for Visual Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: image classification, semantic image segmentation</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.10080" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.10080</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/itijyou/ademxapp" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/itijyou/ademxapp</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Pyramidal Residual Networks with Separated Stochastic Depth</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01230" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01230</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatially Adaptive Computation Time for Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Higher School of Economics &amp; Google &amp; CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02297" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02297</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShaResNet: reducing residual network parameter number by sharing weights</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.08782" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.08782</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aboulch/sharesnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aboulch/sharesnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sharing Residual Units Through Collective Tensor Factorization in Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Collective Residual Networks</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.02180" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.02180</a></li><li>github(MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/cypw/CRU-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cypw/CRU-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Residual Attention Network for Image Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 Spotlight. SenseTime Group Limited &amp; Tsinghua University &amp; The Chinese University of Hong Kong</li><li>intro: ImageNet (4.8% single model and single crop, top-5 error)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.06904" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.06904</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/buptwangfei/residual-attention-network" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/buptwangfei/residual-attention-network</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dilated Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Princeton University &amp; Intel Labs</li><li>keywords: Dilated Residual Networks (DRN)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://vladlen.info/publications/dilated-residual-networks/" class="Link-sc-1brdqhf-0 cKRjba">http://vladlen.info/publications/dilated-residual-networks/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.09914" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.09914</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://vladlen.info/papers/DRN.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://vladlen.info/papers/DRN.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Steerable Blocks in Deep Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Amsterdam &amp; ESAT-PSI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.00598" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.00598</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep ResNet Blocks Sequentially using Boosting Theory</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft Research &amp; Princeton University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.04964" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.04964</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Strict Identity Mappings in Deep Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: epsilon-ResNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.01661" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.01661</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spiking Deep Residual Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.01352" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.01352</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Norm-Preservation: Why Residual Networks Can Become Extremely Deep?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Central Florida</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07477" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07477</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Carnegie Mellon University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.08453" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.08453</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/szq0214/MEAL-V2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/szq0214/MEAL-V2</a></li></ul><h2 id="densenet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#densenet" color="auto.gray.8" aria-label="DenseNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DenseNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Densely Connected Convolutional Networks</strong></p><img src="https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 best paper. Cornell University &amp; Tsinghua University. DenseNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.06993" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.06993</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liuzhuang13/DenseNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liuzhuang13/DenseNet</a></li><li>github(Lasagne): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Lasagne/Recipes/tree/master/papers/densenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Lasagne/Recipes/tree/master/papers/densenet</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/liuzhuang13/DenseNetCaffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liuzhuang13/DenseNetCaffe</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/YixuanLi/densenet-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YixuanLi/densenet-tensorflow</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/titu1994/DenseNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/titu1994/DenseNet</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/bamos/densenet.pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bamos/densenet.pytorch</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/andreasveit/densenet-pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andreasveit/densenet-pytorch</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/ikhlestov/vision_networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ikhlestov/vision_networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Memory-Efficient Implementation of DenseNets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Cornell University &amp; Fudan University &amp; Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06990" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06990</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liuzhuang13/DenseNet/tree/master/models" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liuzhuang13/DenseNet/tree/master/models</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gpleiss/efficient_densenet_pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gpleiss/efficient_densenet_pytorch</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/taineleau/efficient_densenet_mxnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/taineleau/efficient_densenet_mxnet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tongcheng/DN_CaffeScript" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Tongcheng/DN_CaffeScript</a></li></ul><h2 id="densenet-20" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#densenet-20" color="auto.gray.8" aria-label="DenseNet 2.0 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DenseNet 2.0</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CondenseNet: An Efficient DenseNet using Learned Group Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09224" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09224</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//ShichenLiu/CondenseNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//ShichenLiu/CondenseNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multimodal Densenet</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07407" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07407</a></p><h2 id="xception" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#xception" color="auto.gray.8" aria-label="Xception permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Xception</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning with Separable Convolutions</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Xception: Deep Learning with Depthwise Separable Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Extreme Inception</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.02357" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.02357</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://keras.io/applications/#xception" class="Link-sc-1brdqhf-0 cKRjba">https://keras.io/applications/#xception</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/fchollet/deep-learning-models/blob/master/xception.py" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fchollet/deep-learning-models/blob/master/xception.py</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/culurciello/554c8e56d3bbaf7c66bf66c6089dc221" class="Link-sc-1brdqhf-0 cKRjba">https://gist.github.com/culurciello/554c8e56d3bbaf7c66bf66c6089dc221</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kwotsin/Tensorflow-Xception" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kwotsin/Tensorflow-Xception</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//bruinxiong/xception.mxnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//bruinxiong/xception.mxnet</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1610.02357" class="Link-sc-1brdqhf-0 cKRjba">http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1610.02357</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards a New Interpretation of Separable Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.04489" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.04489</a></li></ul><h2 id="mobilenets" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#mobilenets" color="auto.gray.8" aria-label="MobileNets permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MobileNets</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.04861" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.04861</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rcmalli/keras-mobilenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rcmalli/keras-mobilenet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/marvis/pytorch-mobilenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/marvis/pytorch-mobilenet</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zehaos/MobileNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Zehaos/MobileNet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/shicai/MobileNet-Caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shicai/MobileNet-Caffe</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hollance/MobileNet-CoreML" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hollance/MobileNet-CoreML</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KeyKy/mobilenet-mxnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KeyKy/mobilenet-mxnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MobileNets: Open-Source Models for Efficient On-Device Vision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Google’s MobileNets on the iPhone</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/" class="Link-sc-1brdqhf-0 cKRjba">http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hollance/MobileNet-CoreML" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hollance/MobileNet-CoreML</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Depth_conv-for-mobileNet</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com//LamHoCN/Depth_conv-for-mobileNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//LamHoCN/Depth_conv-for-mobileNet</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Enhanced Hybrid MobileNet</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.04698" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.04698</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.03750" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.03750</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Quantization-Friendly Separable Convolution for MobileNets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: THE 1ST WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND COGNITIVE COMPUTING FOR EMBEDDED APPLICATIONS (EMC2)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08607" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08607</a></li></ul><h2 id="mobilenetv2" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#mobilenetv2" color="auto.gray.8" aria-label="MobileNetV2 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MobileNetV2</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Inverted Residuals and Linear Bottlenecks: Mobile Networks forClassification, Detection and Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google</li><li>keywords: MobileNetV2, SSDLite, DeepLabv3</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.04381" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.04381</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liangfu/mxnet-mobilenet-v2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liangfu/mxnet-mobilenet-v2</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PydMobileNet: Improved Version of MobileNets with Pyramid Depthwise Separable Convolution</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07083" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07083</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.13549" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.13549</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zeiss-microscopy/BSConv" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zeiss-microscopy/BSConv</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Bottleneck Structure for Efficient Mobile Network Design</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: National University of Singapore &amp; Yitu Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.02269" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.02269</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhoudaquan/rethinking_bottleneck_design" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhoudaquan/rethinking_bottleneck_design</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mobile-Former: Bridging MobileNet and Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft &amp; University of Science and Technology of China</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.05895" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.05895</a></li></ul><h2 id="shufflenet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#shufflenet" color="auto.gray.8" aria-label="ShuffleNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ShuffleNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii Inc (Face++)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.01083" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.01083</a></li></ul><h2 id="shufflenet-v2" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#shufflenet-v2" color="auto.gray.8" aria-label="ShuffleNet V2 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ShuffleNet V2</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018. Megvii Inc (Face++) &amp; Tsinghua University</li><li>arxiv: <!-- -->[https://arxiv.org/abs/1807.11164]<!-- -->（<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.11164" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.11164</a></li></ul><h2 id="senet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#senet" color="auto.gray.8" aria-label="SENet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SENet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Squeeze-and-Excitation Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>intro: ILSVRC 2017 image classification winner. Momenta &amp; University of Oxford</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.01507" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.01507</a></li><li>github(official, Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/SENet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hujie-frank/SENet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bruinxiong/SENet.mxnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bruinxiong/SENet.mxnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Competitive Inner-Imaging Squeeze and Excitation for Residual Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08920" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08920</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/scut-aitcm/CompetitiveSENet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/scut-aitcm/CompetitiveSENet</a></li></ul><h2 id="genet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#genet" color="auto.gray.8" aria-label="GENet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GENet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2018</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hujie-frank/GENet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hujie-frank/GENet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A ConvNet for the 2020s</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research (FAIR) &amp; UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2201.03545" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2201.03545</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/ConvNeXt" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/ConvNeXt</a></li></ul><h2 id="imagenet-projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#imagenet-projects" color="auto.gray.8" aria-label="ImageNet Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ImageNet Projects</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training an Object Classifier in Torch-7 on multiple GPUs over ImageNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: an imagenet example in torch</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/soumith/imagenet-multiGPU.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/soumith/imagenet-multiGPU.torch</a></li></ul><h1 id="pre-training" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pre-training" color="auto.gray.8" aria-label="Pre-training permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pre-training</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring the Limits of Weakly Supervised Pretraining</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/" class="Link-sc-1brdqhf-0 cKRjba">https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking ImageNet Pre-training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08883" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08883</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting Pre-training: An Efficient Training Method for Image Classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.09347" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.09347</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Pre-training and Self-training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020</li><li>intro: Google Research, Brain Team</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.06882" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.06882</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/self_training" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/self_training</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring the Limits of Large Scale Pre-training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.02095" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.02095</a></li></ul><h1 id="transformers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#transformers" color="auto.gray.8" aria-label="Transformers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Transformers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: National University of Singapore &amp; YITU Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.11986" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.11986</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yitu-opensource/T2T-ViT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yitu-opensource/T2T-ViT</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Incorporating Convolution Designs into Visual Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SenseTime Research &amp; Nanyang Technological University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11816" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11816</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepViT: Towards Deeper Vision Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: National University of Singapore &amp; ByteDance US AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11886" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11886</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021 best paper</li><li>intro: Microsoft Research Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.14030" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.14030</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/Swin-Transformer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/microsoft/Swin-Transformer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking the Design Principles of Robust Vision Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.07926" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.07926</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vtddggg/Robust-Vision-Transformer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vtddggg/Robust-Vision-Transformer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research &amp; DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2109.10686" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2109.10686</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How Do Vision Transformers Work?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2022 Spotlight</li><li>intro: Yonsei University &amp; NAVER AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2202.06709" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2202.06709</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xxxnell/how-do-vits-work" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xxxnell/how-do-vits-work</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MulT: An End-to-End Multitask Learning Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://ivrl.github.io/MulT/" class="Link-sc-1brdqhf-0 cKRjba">https://ivrl.github.io/MulT/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2205.08303" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2205.08303</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EfficientFormer: Vision Transformers at MobileNet Speed</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Snap Inc. &amp; Northeastern University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.01191" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.01191</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/snap-research/EfficientFormer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/snap-research/EfficientFormer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SimA: Simple Softmax-free Attention for Vision Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland &amp; University of California</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.08898" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.08898</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/UCDvision/sima" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/UCDvision/sima</a></li></ul><h1 id="semi-supervised-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#semi-supervised-learning" color="auto.gray.8" aria-label="Semi-Supervised Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Semi-Supervised Learning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semi-Supervised Learning with Graphs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Label Propagation</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://pages.cs.wisc.edu/~jerryzhu/pub/thesis.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://pages.cs.wisc.edu/~jerryzhu/pub/thesis.pdf</a></li><li>blog(&quot;标签传播算法（Label Propagation）及Python实现&quot;): <a target="_blank" rel="noopener noreferrer" href="http://blog.csdn.net/zouxy09/article/details/49105265" class="Link-sc-1brdqhf-0 cKRjba">http://blog.csdn.net/zouxy09/article/details/49105265</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semi-Supervised Learning with Ladder Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1507.02672" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1507.02672</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CuriousAI/ladder" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CuriousAI/ladder</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rinuboney/ladder" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rinuboney/ladder</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semi-supervised Feature Transfer: The Practical Benefit of Deep Learning Today?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.kdnuggets.com/2016/07/semi-supervised-feature-transfer-deep-learning.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.kdnuggets.com/2016/07/semi-supervised-feature-transfer-deep-learning.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Ensembling for Semi-Supervised Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.02242" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.02242</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/smlaine2/tempens" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/smlaine2/tempens</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2017 best paper award</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.05755" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.05755</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/8505222ea1f26692df05e65e35824c6c71929bb5/privacy" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/8505222ea1f26692df05e65e35824c6c71929bb5/privacy</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Infinite Variational Autoencoder for Semi-Supervised Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07800" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07800</a></li></ul><h1 id="multi-label-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#multi-label-learning" color="auto.gray.8" aria-label="Multi-label Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multi-label Learning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNN: Single-label to Multi-label</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1406.5726" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1406.5726</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Multi-label Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1502.05988" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1502.05988</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="http://meka.sourceforge.net" class="Link-sc-1brdqhf-0 cKRjba">http://meka.sourceforge.net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Predicting Unseen Labels using Label Hierarchies in Large-Scale Multi-label Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECML 2015</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.kdsl.tu-darmstadt.de/fileadmin/user_upload/Group_KDSL/PUnL_ECML2015_camera_ready.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.kdsl.tu-darmstadt.de/fileadmin/user_upload/Group_KDSL/PUnL_ECML2015_camera_ready.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning with a Wasserstein Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://cbcl.mit.edu/wasserstein/" class="Link-sc-1brdqhf-0 cKRjba">http://cbcl.mit.edu/wasserstein/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.05439" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.05439</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://cbcl.mit.edu/wasserstein/yfcc100m_labels.tar.gz" class="Link-sc-1brdqhf-0 cKRjba">http://cbcl.mit.edu/wasserstein/yfcc100m_labels.tar.gz</a></li><li>MIT news: <a target="_blank" rel="noopener noreferrer" href="http://news.mit.edu/2015/more-flexible-machine-learning-1001" class="Link-sc-1brdqhf-0 cKRjba">http://news.mit.edu/2015/more-flexible-machine-learning-1001</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.02068" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.02068</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gokceneraslan/SparseMax.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gokceneraslan/SparseMax.torch</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Unbabel/sparsemax" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Unbabel/sparsemax</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNN-RNN: A Unified Framework for Multi-label Image Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.04573" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.04573</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Multi-label Learning with Missing Labels by Structured Semantic Correlations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.01441" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.01441</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking &amp; Other Missing Label Applications</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Indian Institute of Technology Delhi &amp; MSR</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://manikvarma.github.io/pubs/jain16.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://manikvarma.github.io/pubs/jain16.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Label Image Classification with Regional Latent Semantic Dependencies</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Regional Latent Semantic Dependencies model (RLSD), RNN, RPN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01082" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01082</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Privileged Multi-label Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peking University &amp; University of Technology Sydney &amp; University of Sydney</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.07194" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.07194</a></li></ul><h1 id="multi-task-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#multi-task-learning" color="auto.gray.8" aria-label="Multi-task Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multi-task Learning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multitask Learning / Domain Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.cornell.edu/~kilian/research/multitasklearning/multitasklearning.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.cornell.edu/~kilian/research/multitasklearning/multitasklearning.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>multi-task learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>discussion: <a target="_blank" rel="noopener noreferrer" href="https://github.com/memect/hao/issues/93" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/memect/hao/issues/93</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning and Transferring Multi-task Deep Representation for Face Alignment</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1408.3967" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1408.3967</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-task learning of facial landmarks and expression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.uoguelph.ca/~gwtaylor/publications/gwtaylor_crv2014.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.uoguelph.ca/~gwtaylor/publications/gwtaylor_crv2014.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Task Deep Visual-Semantic Embedding for Video Thumbnail Selection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:  CVPR 2015</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Multi-Task_Deep_Visual-Semantic_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Multi-Task_Deep_Visual-Semantic_2015_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Multiple Tasks with Deep Relationship Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1506.02117" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1506.02117</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning deep representation of multityped objects and tasks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.01359" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.01359</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-stitch Networks for Multi-task Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.03539" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.03539</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Task Learning in Tensorflow (Part 1)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://jg8610.github.io/Multi-Task/" class="Link-sc-1brdqhf-0 cKRjba">https://jg8610.github.io/Multi-Task/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Multi-Task Learning with Shared Memory</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: EMNLP 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.07222" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.07222</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Push by Grasping: Using multiple tasks for effective learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.09025" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.09025</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Identifying beneficial task relations for multi-task learning in deep neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: EACL 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.08303" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.08303</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jbingel/eacl2017_mtl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jbingel/eacl2017_mtl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Cambridge</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.07115" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.07115</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One Model To Learn Them All</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Brain &amp; University of Toronto</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.05137" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.05137</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/tensor2tensor" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/tensor2tensor</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MultiModel: Multi-Task Machine Learning Across Domains</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Overview of Multi-Task Learning in Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Aylien Ltd</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.05098" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.05098</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05769" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05769</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/arunmallya/packnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/arunmallya/packnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Multi-Task Learning with Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Imperial College London</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10704" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10704</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-connected Networks for Multi-task Learning of Detection and Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.05569" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.05569</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Auxiliary Tasks in Multi-task Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06334" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06334</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>K For The Price Of 1: Parameter Efficient Multi-task And Transfer Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Chicago &amp; Google</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.10703" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.10703</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Which Tasks Should Be Learned Together in Multi-task Learning?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2020</li><li>intro: Stanford</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://taskgrouping.stanford.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://taskgrouping.stanford.edu/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.07553" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.07553</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OmniNet: A unified architecture for multi-modal multi-task learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.07804" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.07804</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/subho406/OmniNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/subho406/OmniNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Elastic Networks with Model Selection for Multi-Task Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.04860" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.04860</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Boston University &amp; IBM Research &amp; MIT-IBM Watson AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.12423" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.12423</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Task Learning for Dense Prediction Tasks: A Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: T-PAMI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.13379" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.13379</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 spotlight</li><li>keywords: MTI-Net</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2001.06902" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2001.06902</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring Relational Context for Multi-Task Dense Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ETH Zurich</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.13874" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.13874</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-task Attention Mechanism for Dense Multi-task Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Inria, France &amp; Valeo.ai, France</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.08927" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.08927</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cv-rits/DenseMTL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cv-rits/DenseMTL</a></li></ul><h1 id="multi-modal-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#multi-modal-learning" color="auto.gray.8" aria-label="Multi-modal Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multi-modal Learning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multimodal Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://ai.stanford.edu/~ang/papers/nipsdlufl10-MultimodalDeepLearning.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://ai.stanford.edu/~ang/papers/nipsdlufl10-MultimodalDeepLearning.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multimodal Convolutional Neural Networks for Matching Image and Sentence</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://mcnn.noahlab.com.hk/project.html" class="Link-sc-1brdqhf-0 cKRjba">http://mcnn.noahlab.com.hk/project.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://mcnn.noahlab.com.hk/ICCV2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://mcnn.noahlab.com.hk/ICCV2015.pdf</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.06063" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.06063</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A C++ library for Multimodal Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.06927" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.06927</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Jian-23/Deep-Learning-Library" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Jian-23/Deep-Learning-Library</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multimodal Learning for Image Captioning and Visual Question Answering</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://research.microsoft.com/pubs/264769/UCB_XiaodongHe.6.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://research.microsoft.com/pubs/264769/UCB_XiaodongHe.6.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi modal retrieval and generation with deep distributed models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.slideshare.net/roelofp/multi-modal-retrieval-and-generation-with-deep-distributed-models" class="Link-sc-1brdqhf-0 cKRjba">http://www.slideshare.net/roelofp/multi-modal-retrieval-and-generation-with-deep-distributed-models</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1kUSjn4z" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1kUSjn4z</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Aligned Cross-Modal Representations from Weakly Aligned Data</strong></p><img src="http://projects.csail.mit.edu/cmplaces/imgs/teaser.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://projects.csail.mit.edu/cmplaces/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://projects.csail.mit.edu/cmplaces/index.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://projects.csail.mit.edu/cmplaces/content/paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://projects.csail.mit.edu/cmplaces/content/paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Variational methods for Conditional Multimodal Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.01801" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.01801</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016. University of California &amp; Pinterest</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08321" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08321</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Multi-Modal Image Correspondence Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01225" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01225</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multimodal Deep Learning (D4L4 Deep Learning for Speech and Language UPC 2017)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017" class="Link-sc-1brdqhf-0 cKRjba">http://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multimodal Learning with Transformers: A Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Oxford &amp; University of Surrey</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.06488" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.06488</a></li></ul><h1 id="debugging-deep-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#debugging-deep-learning" color="auto.gray.8" aria-label="Debugging Deep Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Debugging Deep Learning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Some tips for debugging deep learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.lab41.org/some-tips-for-debugging-in-deep-learning-2/" class="Link-sc-1brdqhf-0 cKRjba">http://www.lab41.org/some-tips-for-debugging-in-deep-learning-2/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Introduction to debugging neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://russellsstewart.com/notes/0.html" class="Link-sc-1brdqhf-0 cKRjba">http://russellsstewart.com/notes/0.html</a></li><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/4du7gv/introduction_to_debugging_neural_networks" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/4du7gv/introduction_to_debugging_neural_networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How to Visualize, Monitor and Debug Neural Network Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://deeplearning4j.org/visualization" class="Link-sc-1brdqhf-0 cKRjba">http://deeplearning4j.org/visualization</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning from learning curves</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Kaggle</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@dsouza.amanda/learning-from-learning-curves-1a82c6f98f49#.o5synrvvl" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@dsouza.amanda/learning-from-learning-curves-1a82c6f98f49#.o5synrvvl</a></li></ul><h1 id="understanding-cnn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#understanding-cnn" color="auto.gray.8" aria-label="Understanding CNN permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Understanding CNN</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.toronto.edu/~wenjie/papers/nips16/top.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.toronto.edu/~wenjie/papers/nips16/top.pdf</a></li></ul><h1 id="deep-learning-networks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-networks" color="auto.gray.8" aria-label="Deep Learning Networks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning Networks</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PCANet: A Simple Deep Learning Baseline for Image Classification?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arixv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1404.3606" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1404.3606</a></li><li>code(Matlab): <a target="_blank" rel="noopener noreferrer" href="http://mx.nthu.edu.tw/~tsunghan/download/PCANet_demo_pyramid.rar" class="Link-sc-1brdqhf-0 cKRjba">http://mx.nthu.edu.tw/~tsunghan/download/PCANet_demo_pyramid.rar</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1mg24b3a" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1mg24b3a</a></li><li>github(C++): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Ldpe2G/PCANet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Ldpe2G/PCANet</a></li><li>github(Python): <a target="_blank" rel="noopener noreferrer" href="https://github.com/IshitaTakeshi/PCANet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/IshitaTakeshi/PCANet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Kernel Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2014</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1406.3332" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1406.3332</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deeply-supervised Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DSN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1409.5185" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1409.5185</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/" class="Link-sc-1brdqhf-0 cKRjba">http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/s9xie/DSN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/s9xie/DSN</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://zhangliliang.com/2014/11/02/paper-note-dsn/" class="Link-sc-1brdqhf-0 cKRjba">http://zhangliliang.com/2014/11/02/paper-note-dsn/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FitNets: Hints for Thin Deep Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1412.6550" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1412.6550</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/adri-romsor/FitNets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/adri-romsor/FitNets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Striving for Simplicity: The All Convolutional Net</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR-2015 workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.6806" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.6806</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How these researchers tried something unconventional to come out with a smaller yet better Image Recognition.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: All Convolutional Network: (<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1412.6806#" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1412.6806#</a>) implementation in Keras</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@matelabs_ai/how-these-researchers-tried-something-unconventional-to-came-out-with-a-smaller-yet-better-image-544327f30e72#.pfdbvdmuh" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@matelabs_ai/how-these-researchers-tried-something-unconventional-to-came-out-with-a-smaller-yet-better-image-544327f30e72#.pfdbvdmuh</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MateLabs/All-Conv-Keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MateLabs/All-Conv-Keras</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pointer Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1506.03134" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1506.03134</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vshallc/PtrNets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vshallc/PtrNets</a></li><li>github(TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/ikostrikov/TensorFlow-Pointer-Networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ikostrikov/TensorFlow-Pointer-Networks</a></li><li>github(TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/devsisters/pointer-network-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/devsisters/pointer-network-tensorflow</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/pointer-networks.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/pointer-networks.md</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pointer Networks in TensorFlow (with sample code)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264#.sxipqfj30" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264#.sxipqfj30</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/devnag/tensorflow-pointer-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/devnag/tensorflow-pointer-networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rectified Factor Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1502.06464" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1502.06464</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/untom/librfn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/untom/librfn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Correlational Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.07225" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.07225</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/apsarath/CorrNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/apsarath/CorrNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diversity Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.05077" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.05077</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Competitive Multi-scale Convolution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.05635" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.05635</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/22377389" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/22377389</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Unified Approach for Learning the Parameters of Sum-Product Networks (SPN)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;The Sum-Product Network (SPN) is a new type of machine learning model
with fast exact probabilistic inference over many layers.&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.00318" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.00318</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://spn.cs.washington.edu/index.shtml" class="Link-sc-1brdqhf-0 cKRjba">http://spn.cs.washington.edu/index.shtml</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://spn.cs.washington.edu/code.shtml" class="Link-sc-1brdqhf-0 cKRjba">http://spn.cs.washington.edu/code.shtml</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome Sum-Product Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/arranger1044/awesome-spn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/arranger1044/awesome-spn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07356" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07356</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Honari_Recombinator_Networks_Learning_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Honari_Recombinator_Networks_Learning_CVPR_2016_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SinaHonari/RCN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SinaHonari/RCN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Capacity Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07838" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07838</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/beopst/dcn.tf" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/beopst/dcn.tf</a></li><li>review: <a target="_blank" rel="noopener noreferrer" href="http://www.erogol.com/1314-2/" class="Link-sc-1brdqhf-0 cKRjba">http://www.erogol.com/1314-2/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bitwise Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://paris.cs.illinois.edu/pubs/minje-icmlw2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://paris.cs.illinois.edu/pubs/minje-icmlw2015.pdf</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://minjekim.com/demo_bnn.html" class="Link-sc-1brdqhf-0 cKRjba">http://minjekim.com/demo_bnn.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Discriminative Features via Label Consistent Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.01168" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.01168</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Theory of Generative ConvNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.stat.ucla.edu/~ywu/GenerativeConvNet/main.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.stat.ucla.edu/~ywu/GenerativeConvNet/main.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.03264" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.03264</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://www.stat.ucla.edu/~ywu/GenerativeConvNet/doc/code.zip" class="Link-sc-1brdqhf-0 cKRjba">http://www.stat.ucla.edu/~ywu/GenerativeConvNet/doc/code.zip</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.02282" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.02282</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Group Equivariant Convolutional Networks (G-CNNs)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.07576" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.07576</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Spiking Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.08323" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.08323</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/petered/spiking-mlp" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/petered/spiking-mlp</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Low-rank passthrough neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.03116" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.03116</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Avmb/lowrank-gru" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Avmb/lowrank-gru</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single Image 3D Interpreter Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016 (oral)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.08685" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.08685</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deeply-Fused Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.07716" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.07716</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SNN: Stacked Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.08512" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.08512</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Universal Correspondence Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016 full oral presentation. Stanford University &amp; NEC Laboratories America</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://cvgl.stanford.edu/projects/ucn/" class="Link-sc-1brdqhf-0 cKRjba">http://cvgl.stanford.edu/projects/ucn/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.03558" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.03558</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.04671" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.04671</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/synpon/prog_nn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/synpon/prog_nn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yao62995/A3C" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yao62995/A3C</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Holistic SparseCNN: Forging the Trident of Accuracy, Speed, and Size</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.01409" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.01409</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mollifying Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>author: Caglar Gulcehre, Marcin Moczulski, Francesco Visin, Yoshua Bengio</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04980" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04980</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain Separation Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>intro: Google Brain &amp; Imperial College London &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1608.06019" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1608.06019</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/domain_adaptation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/domain_adaptation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Local Binary Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.06049" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.06049</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CliqueCNN: Deep Unsupervised Exemplar Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.08792" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.08792</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/asanakoy/cliquecnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/asanakoy/cliquecnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convexified Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.01000" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.01000</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-scale brain networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.08828" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.08828</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.11473" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.11473</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Input Convex Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.07152" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.07152</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/locuslab/icnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/locuslab/icnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HyperNetworks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1609.09106" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1609.09106</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://blog.otoro.net/2016/09/28/hyper-networks/" class="Link-sc-1brdqhf-0 cKRjba">http://blog.otoro.net/2016/09/28/hyper-networks/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hardmaru/supercell/blob/master/assets/MNIST_Static_HyperNetwork_Example.ipynb" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hardmaru/supercell/blob/master/assets/MNIST_Static_HyperNetwork_Example.ipynb</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HyperLSTM</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hardmaru/supercell/blob/master/supercell.py" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hardmaru/supercell/blob/master/supercell.py</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.00163" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.00163</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tensor Switching Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arixiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.10087" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.10087</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/coxlab/tsnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/coxlab/tsnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Harvard University</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.eecs.harvard.edu/~htk/publication/2016-icpr-teerapittayanon-mcdanel-kung.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eecs.harvard.edu/~htk/publication/2016-icpr-teerapittayanon-mcdanel-kung.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kunglab/branchynet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kunglab/branchynet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spectral Convolution Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05378" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05378</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DelugeNets: Deep Networks with Massive and Flexible Cross-layer Information Inflows</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05552" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05552</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xternalz/DelugeNets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xternalz/DelugeNets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PolyNet: A Pursuit of Structural Diversity in Very Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05725" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05725</a></li><li>poster: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/cu_deeplink/polynet_poster.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/cu_deeplink/polynet_poster.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weakly Supervised Cascaded Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08258" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08258</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepSetNet: Predicting Sets with Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: multi-class image classification and pedestrian detection</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08998" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08998</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Steerable CNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Amsterdam</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.08498" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.08498</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feedback Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://feedbacknet.stanford.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://feedbacknet.stanford.edu/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.09508" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.09508</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://youtu.be/MY5Uhv38Ttg" class="Link-sc-1brdqhf-0 cKRjba">https://youtu.be/MY5Uhv38Ttg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Oriented Response Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.01833" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.01833</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OptNet: Differentiable Optimization as a Layer in Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.00443" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.00443</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/locuslab/optnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/locuslab/optnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A fast and differentiable QP solver for PyTorch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/locuslab/qpth" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/locuslab/qpth</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Meta Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.00837" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.00837</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deformable Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017 oral. Microsoft Research Asia</li><li>keywords: deformable convolution, deformable RoI pooling</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.06211" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.06211</a></li><li>sliedes: <a target="_blank" rel="noopener noreferrer" href="http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/msracver/Deformable-ConvNets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/msracver/Deformable-ConvNets</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/felixlaumon/deform-conv" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/felixlaumon/deform-conv</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/oeway/pytorch-deform-conv" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/oeway/pytorch-deform-conv</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Deformable ConvNets v2: More Deformable, Better Results**</p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; Microsoft Research Asia</li><li>keywords: DCNv2</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11168" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11168</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/msracver/Deformable-ConvNets/tree/master/DCNv2_op" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/msracver/Deformable-ConvNets/tree/master/DCNv2_op</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Second-order Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.06817" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.06817</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gabor Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.01450" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.01450</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Rotation Equivariant Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08623" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08623</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dense Transformer Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Washington State University &amp; University of California, Davis</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08881" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08881</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/divelab/dtn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/divelab/dtn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Complex Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: [Université de Montréal &amp; INRS-EMT &amp; Microsoft Maluuba</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.09792" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.09792</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ChihebTrabelsi/deep_complex_networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ChihebTrabelsi/deep_complex_networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Quaternion Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Louisiana</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.04604" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.04604</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DiracNets: Training Very Deep Neural Networks Without Skip-Connections</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Université Paris-Est</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.00388" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.00388</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/szagoruyko/diracnets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/szagoruyko/diracnets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dual Path Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: National University of Singapore</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.01629" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.01629</a></li><li>github(MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/cypw/DPNs" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cypw/DPNs</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Primal-Dual Group Convolutions for Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Interleaved Group Convolutions for Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>keywords: interleaved group convolutional neural networks (IGCNets), IGCV1</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.02725" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.02725</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hellozting/InterleavedGroupConvolutions" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hellozting/InterleavedGroupConvolutions</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IGCV2: Interleaved Structured Sparse Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.06202" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.06202</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Scinence and Technology of China &amp; Microsoft Reserach Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.00178" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.00178</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/homles11/IGCV3" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/homles11/IGCV3</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sensor Transformation Attention Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.01015" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.01015</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sparsity Invariant CNNs</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.06500" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.06500</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SPARCNN: SPAtially Related Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.07522" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.07522</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.01686" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.01686</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Polar Transformer Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.01889" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.01889</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tensor Product Generation Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.09118" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.09118</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Competitive Pathway Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACML 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.10282" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.10282</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JiaRenChang/CoPaNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JiaRenChang/CoPaNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context Embedding Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.01691" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.01691</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalization in Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT &amp; University of Montreal</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.05468" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.05468</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding Deep Learning Generalization by Maximum Entropy</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; Beijing Jiaotong University &amp; Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07758" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07758</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Do Convolutional Neural Networks Learn Class Hierarchy?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Bosch Research North America &amp; Michigan State University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.06501" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.06501</a></li><li>video demo: <a target="_blank" rel="noopener noreferrer" href="https://vimeo.com/228263798" class="Link-sc-1brdqhf-0 cKRjba">https://vimeo.com/228263798</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Hyperspherical Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.03189" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.03189</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Sparsity: Tree Regularization of Deep Models for Interpretability</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06178" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06178</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Motifs: Scene Graph Parsing with Global Context</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Stacked Motif Networks</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06640" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06640</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Priming Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05918" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05918</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Three Factors Influencing Minima in SGD</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.04623" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.04623</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06959" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06959</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BlockDrop: Dynamic Inference Paths in Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UMD &amp; UT Austin &amp; IBM Research &amp; Fusemachines Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08393" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08393</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Wasserstein Introspective Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08875" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08875</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SkipNet: Learning Dynamic Routing in Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09485" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09485</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Do Convolutional Neural Networks act as Compositional Nearest Neighbors?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; West Virginia University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10683" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10683</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.11443" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.11443</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Broadcasting Convolutional Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02517" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02517</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Point-wise Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Singapore University of Technology and Design</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.05245" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.05245</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ScreenerNet: Learning Curriculum for Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Intel Corporation &amp; Allen Institute for Artificial Intelligence</li><li>keywords: curricular learning, deep learning, deep q-learning</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00904" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00904</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sparsely Connected Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.05895" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.05895</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spherical CNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2018 best paper award. University of Amsterdam &amp; EPFL</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.10130" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.10130</a></li><li>github(official, PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/jonas-koehler/s2cnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jonas-koehler/s2cnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Going Deeper in Spiking Neural Networks: VGG and Residual Architectures</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Purdue University &amp; Oculus Research &amp; Facebook Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02627" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02627</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02950" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02950</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Neural Networks with Alternately Updated Clique</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.10419" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.10419</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iboing/CliqueNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iboing/CliqueNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Decoupled Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 (Spotlight)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.08071" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.08071</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Optical Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06082" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06082</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Regularization Learning Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Weizmann Institute of Science</li><li>keywords: Regularization Learning Networks (RLNs), Counterfactual Loss, tabular datasets</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06440" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06440</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bilinear Attention Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07932" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07932</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cautious Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.09460" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.09460</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Perturbative Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>intro: We introduce a very simple, yet effective, module called a perturbation layer as an alternative to a convolutional layer</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://xujuefei.com/pnn.html" class="Link-sc-1brdqhf-0 cKRjba">http://xujuefei.com/pnn.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.01817" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.01817</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lightweight Probabilistic Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.11327" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.11327</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Channel Gating Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.12549" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.12549</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Evenly Cascaded Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.00456" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.00456</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SGAD: Soft-Guided Adaptively-Dropped Neural Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.01430" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.01430</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Explainable Neural Computation via Stack Neural Module Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08556" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08556</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rank-1 Convolutional Neural Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.04303" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.04303</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Network Encapsulation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.03749" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.03749</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Penetrating the Fog: the Path to Efficient CNN Models</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.04231" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.04231</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A2-Nets: Double Attention Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.11579" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.11579</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Global Second-order Pooling Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12006" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12006</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Washington &amp; Allen Institute for AI (AI2) &amp; XNOR.AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11431" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11431</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sacmehta/ESPNetv2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sacmehta/ESPNetv2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Kernel Transformer Networks for Compact Spherical Convolution</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.03115" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.03115</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UAN: Unified Attention Network for Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.05376" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.05376</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One-Class Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.08688" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.08688</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/otkupjnoz/oc-cnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/otkupjnoz/oc-cnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Selective Kernel Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>inrtro: Nanjing University of Science and Technology &amp; Momenta &amp; Nanjing University &amp; Tsinghua University]</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.06586" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.06586</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/implus/SKNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/implus/SKNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Universally Slimmable Networks and Improved Training Techniques</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.05134" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.05134</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Slimmable Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.13258" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.13258</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/changlin31/DS-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/changlin31/DS-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptively Connected Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.03579" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.03579</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wanggrun/Adaptively-Connected-Neural-Networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wanggrun/Adaptively-Connected-Neural-Networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transformable Bottleneck Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.06458" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.06458</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pixel-Adaptive Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05373" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05373</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention Augmented Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Brain</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.09925" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.09925</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.09646" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.09646</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/implus/PytorchInsight" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/implus/PytorchInsight</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EnsembleNet: End-to-End Optimization of Multi-headed Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.09979" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.09979</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MixNet: Mixed Depthwise Convolutional Kernels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.09595" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.09595</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HarDNet: A Low Memory Traffic Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: National Tsing Hua University &amp; University of Michigan</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.00948" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.00948</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Π− nets: Deep Polynomial Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.03828" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.03828</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Circle Loss: A Unified Perspective of Pair Similarity Optimization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 1Megvii Inc. &amp; Beihang University &amp; Australian National University &amp; Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.10857" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.10857</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Designing Network Design Spaces</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: Facebook AI Research (FAIR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.13678" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.13678</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>WeightNet: Revisiting the Design Space of Weight Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.11823" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.11823</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/megvii-model/WeightNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/megvii-model/WeightNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Disentangled Non-Local Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.06668" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.06668</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Neural Networks: A Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.04906" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.04906</a></li></ul><h2 id="convolutions--filters" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#convolutions--filters" color="auto.gray.8" aria-label="Convolutions / Filters permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Convolutions / Filters</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Warped Convolutions: Efficient Invariance to Spatial Transformations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.04382" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.04382</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Coordinating Filters for Faster Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.09746" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.09746</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wenwei202/caffe/tree/sfm" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wenwei202/caffe/tree/sfm</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08141" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08141</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatially-Adaptive Filter Units for Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Ljubljana &amp; University of Birmingham</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.11473" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.11473</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>clcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.06145" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.06145</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DCFNet: Deep Neural Network with Decomposed Convolutional Filters</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.04145" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.04145</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast End-to-End Trainable Guided Filter</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://wuhuikai.me/DeepGuidedFilterProject/" class="Link-sc-1brdqhf-0 cKRjba">http://wuhuikai.me/DeepGuidedFilterProject/</a></li><li>gtihub(official, PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/wuhuikai/DeepGuidedFilter" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wuhuikai/DeepGuidedFilter</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09926" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09926</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/clavichord93/diagonalwise-refactorization-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/clavichord93/diagonalwise-refactorization-tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Use of symmetric kernels for convolutional neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICDSIAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.09421" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.09421</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EasyConvPooling: Random Pooling with Easy Convolution for Accelerating Training and Testing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.01729" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.01729</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Targeted Kernel Networks: Faster Convolutions with Attentive Regularization</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.00523" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.00523</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2018</li><li>intro: Uber AI Labs &amp; Uber Technologies</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.03247" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.03247</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/uber-research/CoordConv" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/uber-research/CoordConv</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=8yFQc6elePA" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=8yFQc6elePA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Network Decoupling: From Regular to Depthwise Separable Convolutions</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.05517" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.05517</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Partial Convolution based Padding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NVIDIA Corporation</li><li>arxiv; <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11718" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11718</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/partialconv" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NVIDIA/partialconv</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DSConv: Efficient Convolution Operator</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.01928" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.01928</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CircConv: A Structured Convolution with Low Complexity</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.11268" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.11268</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating Large-Kernel Convolution Using Summed-Area Tables</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Princeton University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.11367" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.11367</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mapped Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of North Carolina at Chapel Hill</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.11096" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.11096</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Universal Pooling -- A New Pooling Method for Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.11440" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.11440</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dilated Point Convolutions: On the Receptive Field of Point Convolutions</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.12046" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.12046</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LIP: Local Importance-based Pooling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.04156" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.04156</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Generalized Max Pooling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICDAR</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05040" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05040</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MixConv: Mixed Depthwise Convolutional Kernels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:BMVC 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.09595" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.09595</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UC Berkeley &amp; USTC &amp; MSRA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.02940" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.02940</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Convolution: Attention over Convolution Kernels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 oral</li><li>intro: Microsoft</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.03458" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.03458</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.11538" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.11538</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iduta/pyconv" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iduta/pyconv</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iduta/pyconvsegnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iduta/pyconvsegnet</a></li></ul><h2 id="highway-networks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#highway-networks" color="auto.gray.8" aria-label="Highway Networks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Highway Networks</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Highway Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2015 Deep Learning workshop</li><li>intro: shortcut connections with gating functions. These gates are data-dependent and have parameters</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.00387" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.00387</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/analvikingur/pytorch_Highway" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/analvikingur/pytorch_Highway</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Highway Networks with TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa#.71fgztsb6" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa#.71fgztsb6</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Very Deep Learning with Highway Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage(papers+code+FAQ): <a target="_blank" rel="noopener noreferrer" href="http://people.idsia.ch/~rupesh/very_deep_learning/" class="Link-sc-1brdqhf-0 cKRjba">http://people.idsia.ch/~rupesh/very_deep_learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Very Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Extends <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1505.00387" class="Link-sc-1brdqhf-0 cKRjba">Highway Networks</a></li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://people.idsia.ch/~rupesh/very_deep_learning/" class="Link-sc-1brdqhf-0 cKRjba">http://people.idsia.ch/~rupesh/very_deep_learning/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1507.06228" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1507.06228</a></li></ul><h2 id="spatial-transformer-networks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#spatial-transformer-networks" color="auto.gray.8" aria-label="Spatial Transformer Networks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Spatial Transformer Networks</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial Transformer Networks</strong></p><img src="https://camo.githubusercontent.com/bb81d6267f2123d59979453526d958a58899bb4f/687474703a2f2f692e696d6775722e636f6d2f4578474456756c2e706e67" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.02025" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.02025</a></li><li>gitxiv: <a target="_blank" rel="noopener noreferrer" href="http://gitxiv.com/posts/5WTXTLuEA4Hd8W84G/spatial-transformer-networks" class="Link-sc-1brdqhf-0 cKRjba">http://gitxiv.com/posts/5WTXTLuEA4Hd8W84G/spatial-transformer-networks</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/daerduoCarey/SpatialTransformerLayer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daerduoCarey/SpatialTransformerLayer</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/qassemoquab/stnbhwd" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/qassemoquab/stnbhwd</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/skaae/transformer_network" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/skaae/transformer_network</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/happynear/SpatialTransformerLayer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/happynear/SpatialTransformerLayer</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/daviddao/spatial-transformer-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daviddao/spatial-transformer-tensorflow</a></li><li>caffe-issue: <a target="_blank" rel="noopener noreferrer" href="https://github.com/BVLC/caffe/issues/3114" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/BVLC/caffe/issues/3114</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://lasagne.readthedocs.org/en/latest/modules/layers/special.html#lasagne.layers.TransformerLayer" class="Link-sc-1brdqhf-0 cKRjba">https://lasagne.readthedocs.org/en/latest/modules/layers/special.html#lasagne.layers.TransformerLayer</a></li><li>ipn(Lasagne): <a target="_blank" rel="noopener noreferrer" href="http://nbviewer.jupyter.org/github/Lasagne/Recipes/blob/master/examples/spatial_transformer_network.ipynb" class="Link-sc-1brdqhf-0 cKRjba">http://nbviewer.jupyter.org/github/Lasagne/Recipes/blob/master/examples/spatial_transformer_network.ipynb</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://www.evernote.com/shard/s189/sh/ad8a38de-9e98-4e06-b09e-574bd62893ff/32f72798c095dd7672f4cb017a32d9b4" class="Link-sc-1brdqhf-0 cKRjba">https://www.evernote.com/shard/s189/sh/ad8a38de-9e98-4e06-b09e-574bd62893ff/32f72798c095dd7672f4cb017a32d9b4</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=6NOQC_fl1hQ" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=6NOQC_fl1hQ</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The power of Spatial Transformer Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://torch.ch/blog/2015/09/07/spatial_transformers.html" class="Link-sc-1brdqhf-0 cKRjba">http://torch.ch/blog/2015/09/07/spatial_transformers.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/moodstocks/gtsrb.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/moodstocks/gtsrb.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Spatial Transformer Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.05329" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.05329</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Paper Implementations: Spatial Transformer Networks - Part I</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://kevinzakka.github.io/2017/01/10/stn-part1/" class="Link-sc-1brdqhf-0 cKRjba">https://kevinzakka.github.io/2017/01/10/stn-part1/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kevinzakka/blog-code/tree/master/spatial_transformer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kevinzakka/blog-code/tree/master/spatial_transformer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Top-down Flow Transformer Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02400" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02400</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Non-Parametric Transformation Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.04520" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.04520</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Spatial Transformer Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.09467" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.09467</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial Transformer Introspective Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University &amp; Shanghai University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06447" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06447</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeSTNet: Densely Fused Spatial Transformer Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.04050" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.04050</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MIST: Multiple Instance Spatial Transformer Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.10725" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.10725</a></p><h2 id="fractalnet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#fractalnet" color="auto.gray.8" aria-label="FractalNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>FractalNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FractalNet: Ultra-Deep Neural Networks without Residuals</strong></p><img src="http://people.cs.uchicago.edu/~larsson/fractalnet/overview.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project: <a target="_blank" rel="noopener noreferrer" href="http://people.cs.uchicago.edu/~larsson/fractalnet/" class="Link-sc-1brdqhf-0 cKRjba">http://people.cs.uchicago.edu/~larsson/fractalnet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.07648" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.07648</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gustavla/fractalnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gustavla/fractalnet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/edgelord/FractalNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/edgelord/FractalNet</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/snf/keras-fractalnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/snf/keras-fractalnet</a></li></ul><h1 id="generative-models" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#generative-models" color="auto.gray.8" aria-label="Generative Models permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generative Models</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Max-margin Deep Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.06787" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.06787</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhenxuan00/mmdgm" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhenxuan00/mmdgm</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Discriminative Regularization for Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.03220" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.03220</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vdumoulin/discgen" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vdumoulin/discgen</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Auxiliary Deep Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.05473" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.05473</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/larsmaaloee/auxiliary-deep-generative-models" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/larsmaaloee/auxiliary-deep-generative-models</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sampling Generative Networks: Notes on a Few Effective Techniques</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.04468" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.04468</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dribnet/plat" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dribnet/plat</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Conditional Image Synthesis With Auxiliary Classifier GANs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.09585" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.09585</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/buriburisuri/ac-gan" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/buriburisuri/ac-gan</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/lukedeo/keras-acgan" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lukedeo/keras-acgan</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>On the Quantitative Analysis of Decoder-Based Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Toronto &amp; OpenAI &amp; CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.04273" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.04273</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tonywu95/eval_gen" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tonywu95/eval_gen</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Boosted Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.08484" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.08484</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openreview.net/pdf?id=HyY4Owjll" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.net/pdf?id=HyY4Owjll</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Architecture for Deep, Hierarchical Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.04739" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.04739</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Philip-Bachman/MatNets-NIPS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Philip-Bachman/MatNets-NIPS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning and Hierarchal Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016. MIT</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.09057" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.09057</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Probabilistic Torch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Probabilistic Torch is library for deep generative models that extends PyTorch</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/probtorch/probtorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/probtorch/probtorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tutorial on Deep Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UAI 2017 Tutorial: Shakir Mohamed &amp; Danilo Rezende (DeepMind)</li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=JrO5fSskISY" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=JrO5fSskISY</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av16428277/" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av16428277/</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.shakirm.com/slides/DeepGenModelsTutorial.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.shakirm.com/slides/DeepGenModelsTutorial.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Note on the Inception Score</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.01973" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.01973</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AISTATS 2018. The University of Tokyo</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02227" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02227</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Batch Normalization in the final layer of generative networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07389" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07389</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Structured Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.03877" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.03877</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VFunc: a Deep Generative Model for Functions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2018 workshop on Prediction and Generative Modeling in Reinforcement Learning. Microsoft Research &amp; McGill University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.04106" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.04106</a></li></ul><h1 id="deep-learning-and-robots" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-and-robots" color="auto.gray.8" aria-label="Deep Learning and Robots permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning and Robots</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robot Learning Manipulation Action Plans by &quot;Watching&quot; Unconstrained Videos from the World Wide Web</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2015</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.umiacs.umd.edu/~yzyang/paper/YouCookMani_CameraReady.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.umiacs.umd.edu/~yzyang/paper/YouCookMani_CameraReady.pdf</a></li><li>author page: <a target="_blank" rel="noopener noreferrer" href="http://www.umiacs.umd.edu/~yzyang/" class="Link-sc-1brdqhf-0 cKRjba">http://www.umiacs.umd.edu/~yzyang/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Training of Deep Visuomotor Policies</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.00702" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.00702</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Comment on Open AI’s Efforts to Robot Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://gridworld.wordpress.com/2016/07/28/comment-on-open-ais-efforts-to-robot-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://gridworld.wordpress.com/2016/07/28/comment-on-open-ais-efforts-to-robot-learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Curious Robot: Learning Visual Representations via Physical Interactions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.01360" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.01360</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How to build a robot that “sees” with $100 and TensorFlow</strong></p><img src="https://d3ansictanv2wj.cloudfront.net/Figure_5-5b104cf7a53a9c1ee95110b78fb14256.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with-100-and-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with-100-and-tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Visual Foresight for Planning Robot Motion</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/site/brainrobotdata/" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/site/brainrobotdata/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.00696" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.00696</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/site/robotforesight/" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/site/robotforesight/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sim-to-Real Robot Learning from Pixels with Progressive Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.04286" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.04286</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.00201" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.00201</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Differentiable Physics Engine for Deep Learning in Robotics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openreview.net/pdf?id=SyEiHNKxx" class="Link-sc-1brdqhf-0 cKRjba">http://openreview.net/pdf?id=SyEiHNKxx</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep-learning in Mobile Robotics - from Perception to Control Systems: A Survey on Why and Why not</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: City University of Hong Kong &amp; Hong Kong University of Science and Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.07139" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.07139</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Robotic Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: <a target="_blank" rel="noopener noreferrer" href="https://simons.berkeley.edu/talks/sergey-levine-01-24-2017-1" class="Link-sc-1brdqhf-0 cKRjba">https://simons.berkeley.edu/talks/sergey-levine-01-24-2017-1</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=jtjW5Pye_44" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=jtjW5Pye_44</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning in Robotics: A Review of Recent Research</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07217" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07217</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Robotics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: by Pieter Abbeel</li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.facebook.com/nipsfoundation/videos/1554594181298482/" class="Link-sc-1brdqhf-0 cKRjba">https://www.facebook.com/nipsfoundation/videos/1554594181298482/</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av17078186/" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av17078186/</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://www.dropbox.com/s/4fhczb9cxkuqalf/2017_11_xx_BARS-Abbeel.pdf?dl=0" class="Link-sc-1brdqhf-0 cKRjba">https://www.dropbox.com/s/4fhczb9cxkuqalf/2017_11_xx_BARS-Abbeel.pdf?dl=0</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DroNet: Learning to Fly by Driving</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://rpg.ifi.uzh.ch/dronet.html" class="Link-sc-1brdqhf-0 cKRjba">http://rpg.ifi.uzh.ch/dronet.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/uzh-rpg/rpg_public_dronet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/uzh-rpg/rpg_public_dronet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Survey on Deep Learning Methods for Robot Vision</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10862" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10862</a></p><h1 id="deep-learning-on-mobile--embedded-devices" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-on-mobile--embedded-devices" color="auto.gray.8" aria-label="Deep Learning on Mobile / Embedded Devices permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning on Mobile / Embedded Devices</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional neural networks on the iPhone with VGGNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://matthijshollemans.com/2016/08/30/vggnet-convolutional-neural-network-iphone/" class="Link-sc-1brdqhf-0 cKRjba">http://matthijshollemans.com/2016/08/30/vggnet-convolutional-neural-network-iphone/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hollance/VGGNet-Metal" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hollance/VGGNet-Metal</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TensorFlow for Mobile Poets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/" class="Link-sc-1brdqhf-0 cKRjba">https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Convolutional Neural Network(CNN) for Android</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CnnForAndroid:A Classification Project using Convolutional Neural Network(CNN) in Android platform。It also support Caffe Model</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhangqianhui/CnnForAndroid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhangqianhui/CnnForAndroid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TensorFlow on Android</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://www.oreilly.com/learning/tensorflow-on-android" class="Link-sc-1brdqhf-0 cKRjba">https://www.oreilly.com/learning/tensorflow-on-android</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Experimenting with TensorFlow on Android</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>part 1: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@mgazar/experimenting-with-tensorflow-on-android-pt-1-362683b31838#.5gbp2d4st" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@mgazar/experimenting-with-tensorflow-on-android-pt-1-362683b31838#.5gbp2d4st</a></li><li>part 2: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@mgazar/experimenting-with-tensorflow-on-android-part-2-12f3dc294eaf#.2gx3o65f5" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@mgazar/experimenting-with-tensorflow-on-android-part-2-12f3dc294eaf#.2gx3o65f5</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MostafaGazar/tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MostafaGazar/tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>XNOR.ai frees AI from the prison of the supercomputer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://techcrunch.com/2017/01/19/xnor-ai-frees-ai-from-the-prison-of-the-supercomputer/" class="Link-sc-1brdqhf-0 cKRjba">https://techcrunch.com/2017/01/19/xnor-ai-frees-ai-from-the-prison-of-the-supercomputer/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Embedded and mobile deep learning research resources</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/csarron/emdl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/csarron/emdl</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Modeling the Resource Requirements of Convolutional Neural Networks on Mobile Devices</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.09118" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.09118</a></p><h1 id="benchmarks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#benchmarks" color="auto.gray.8" aria-label="Benchmarks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Benchmarks</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning’s Accuracy</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://deeplearning4j.org/accuracy.html" class="Link-sc-1brdqhf-0 cKRjba">http://deeplearning4j.org/accuracy.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Benchmarks for popular CNN models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Benchmarks for popular convolutional neural network models on CPU and different GPUs, with and without cuDNN.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jcjohnson/cnn-benchmarks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jcjohnson/cnn-benchmarks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Benchmarks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://add-for.com/deep-learning-benchmarks/" class="Link-sc-1brdqhf-0 cKRjba">http://add-for.com/deep-learning-benchmarks/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>cudnn-rnn-benchmarks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MaximumEntropy/cudnn_rnn_theano_benchmarks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MaximumEntropy/cudnn_rnn_theano_benchmarks</a></li></ul><h1 id="papers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers" color="auto.gray.8" aria-label="Papers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reweighted Wake-Sleep</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1406.2751" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1406.2751</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jbornschein/reweighted-ws" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jbornschein/reweighted-ws</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1502.05336" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1502.05336</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HIPS/Probabilistic-Backpropagation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HIPS/Probabilistic-Backpropagation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deeply-Supervised Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1409.5185" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1409.5185</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mbhenaff/spectral-lib" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mbhenaff/spectral-lib</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Nature 2015</li><li>author: Yann LeCun, Yoshua Bengio &amp; Geoffrey Hinton</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>On the Expressive Power of Deep Learning: A Tensor Analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.05009" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.05009</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding and Predicting Image Memorability at a Large Scale</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT. ICCV 2015</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://memorability.csail.mit.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://memorability.csail.mit.edu/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://people.csail.mit.edu/khosla/papers/iccv2015_khosla.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://people.csail.mit.edu/khosla/papers/iccv2015_khosla.pdf</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://memorability.csail.mit.edu/download.html" class="Link-sc-1brdqhf-0 cKRjba">http://memorability.csail.mit.edu/download.html</a></li><li>reviews: <a target="_blank" rel="noopener noreferrer" href="http://petapixel.com/2015/12/18/how-memorable-are-times-top-10-photos-of-2015-to-a-computer/" class="Link-sc-1brdqhf-0 cKRjba">http://petapixel.com/2015/12/18/how-memorable-are-times-top-10-photos-of-2015-to-a-computer/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Open Set Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06233" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06233</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/abhijitbendale/OSDN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/abhijitbendale/OSDN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Structured Prediction Energy Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2016. SPEN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06350" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06350</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/davidBelanger/SPEN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/davidBelanger/SPEN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Neural Networks predict Hierarchical Spatio-temporal Cortical Dynamics of Human Visual Object Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.02970" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.02970</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://brainmodels.csail.mit.edu/dnn/drawCNN/" class="Link-sc-1brdqhf-0 cKRjba">http://brainmodels.csail.mit.edu/dnn/drawCNN/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recent Advances in Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.07108" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.07108</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding Deep Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.04920" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.04920</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepCare: A Deep Dynamic Memory Model for Predictive Medicine</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.00357" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.00357</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploiting Cyclic Symmetry in Convolutional Neural Networks</strong></p><img src="http://benanne.github.io/images/cyclicroll.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.02660" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.02660</a></li><li>github(Winning solution for the National Data Science Bowl competition on Kaggle (plankton classification)): <a target="_blank" rel="noopener noreferrer" href="https://github.com/benanne/kaggle-ndsb" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/benanne/kaggle-ndsb</a></li><li>ref(use Cyclic pooling): <a target="_blank" rel="noopener noreferrer" href="http://benanne.github.io/2015/03/17/plankton.html" class="Link-sc-1brdqhf-0 cKRjba">http://benanne.github.io/2015/03/17/plankton.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-dimensional Weighting for Aggregated Deep Convolutional Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.04065" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.04065</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yahoo/crow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yahoo/crow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding Visual Concepts with Continuation Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://willwhitney.github.io/understanding-visual-concepts/" class="Link-sc-1brdqhf-0 cKRjba">http://willwhitney.github.io/understanding-visual-concepts/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.06822" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.06822</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/willwhitney/understanding-visual-concepts" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/willwhitney/understanding-visual-concepts</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Efficient Algorithms with Hierarchical Attentive Memory</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.03218" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.03218</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Smerity/tf-ham" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Smerity/tf-ham</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.00917" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.00917</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bigaidream-projects/drmad" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bigaidream-projects/drmad</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.05691" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.05691</a></li><li>review: <a target="_blank" rel="noopener noreferrer" href="http://www.erogol.com/paper-review-deep-convolutional-nets-really-need-deep-even-convolutional/" class="Link-sc-1brdqhf-0 cKRjba">http://www.erogol.com/paper-review-deep-convolutional-nets-really-need-deep-even-convolutional/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Harnessing Deep Neural Networks with Logic Rules</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.06318" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.06318</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Degrees of Freedom in Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.09260" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.09260</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Networks with Stochastic Depth</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.09382" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.09382</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yueatsprograms/Stochastic_Depth" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yueatsprograms/Stochastic_Depth</a></li><li>notes(&quot;Stochastic Depth Networks will Become the New Normal&quot;): <a target="_blank" rel="noopener noreferrer" href="http://deliprao.com/archives/134" class="Link-sc-1brdqhf-0 cKRjba">http://deliprao.com/archives/134</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dblN/stochastic_depth_keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dblN/stochastic_depth_keras</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yasunorikudo/chainer-ResDrop" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yasunorikudo/chainer-ResDrop</a></li><li>review: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@tim_nth/review-deep-networks-with-stochastic-depth-51bd53acfe72" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@tim_nth/review-deep-networks-with-stochastic-depth-51bd53acfe72</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LIFT: Learned Invariant Feature Transform</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.09114" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.09114</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/cvlab-epfl/LIFT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cvlab-epfl/LIFT</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.03640" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.03640</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://prlab.tudelft.nl/sites/default/files/rnnResnetCortex.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://prlab.tudelft.nl/sites/default/files/rnnResnetCortex.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding How Image Quality Affects Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.04004" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.04004</a></li><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/4exk3u/dcnns_are_more_sensitive_to_blur_and_noise_than/" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/4exk3u/dcnns_are_more_sensitive_to_blur_and_noise_than/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Embedding for Spatial Role Labeling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.08474" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.08474</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/oswaldoludwig/visually-informed-embedding-of-word-VIEW-" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/oswaldoludwig/visually-informed-embedding-of-word-VIEW-</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unreasonable Effectiveness of Learning Neural Nets: Accessible States and Robust Ensembles</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.06444" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.06444</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Representation for Imbalanced Classification</strong></p><img src="http://mmlab.ie.cuhk.edu.hk/projects/LMLE/method.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>keywords: Deep Learning Large Margin Local Embedding (LMLE)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/LMLE.html" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/LMLE.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~ccloy/files/cvpr_2016_imbalanced.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~ccloy/files/cvpr_2016_imbalanced.pdf</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/LMLE/lmle_code.zip" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/LMLE/lmle_code.zip</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images</strong></p><img src="http://allenai.org/images/projects/plato_newton.png?cb=1466683222538" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://allenai.org/plato/newtonian-understanding/" class="Link-sc-1brdqhf-0 cKRjba">http://allenai.org/plato/newtonian-understanding/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.04048" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.04048</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/roozbehm/newtonian" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/roozbehm/newtonian</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepMath - Deep Sequence Models for Premise Selection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.04442" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.04442</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/deepmath" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/deepmath</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Neural Networks Analyzed via Convolutional Sparse Coding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.08194" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.08194</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Systematic evaluation of CNN advances on the ImageNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1606.02228" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1606.02228</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ducha-aiki/caffenet-benchmark" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ducha-aiki/caffenet-benchmark</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Why does deep and cheap learning work so well?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Harvard and MIT</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.08225" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.08225</a></li><li>review: <a target="_blank" rel="noopener noreferrer" href="https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe/" class="Link-sc-1brdqhf-0 cKRjba">https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A scalable convolutional neural network for task-specified scenarios via knowledge distillation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.05695" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.05695</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Alternating Back-Propagation for Generator Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page(code+data): <a target="_blank" rel="noopener noreferrer" href="http://www.stat.ucla.edu/~ywu/ABP/main.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.stat.ucla.edu/~ywu/ABP/main.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.stat.ucla.edu/~ywu/ABP/doc/arXivABP.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.stat.ucla.edu/~ywu/ABP/doc/arXivABP.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Novel Representation of Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.01549" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.01549</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE ICACSIS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.02306" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.02306</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Uncertainty in Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PhD Thesis. Cambridge Machine Learning Group</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://mlg.eng.cam.ac.uk/yarin/blog_2248.html" class="Link-sc-1brdqhf-0 cKRjba">http://mlg.eng.cam.ac.uk/yarin/blog_2248.html</a></li><li>thesis: <a target="_blank" rel="noopener noreferrer" href="http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Convolutional Neural Network Design Patterns</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.00847" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.00847</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iPhysicist/CNNDesignPatterns" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iPhysicist/CNNDesignPatterns</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Extensions and Limitations of the Neural GPU</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.00736" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.00736</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/openai/ecprice-neural-gpu" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/openai/ecprice-neural-gpu</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Functional Programming</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01988" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01988</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Information Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01232" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01232</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compressed Learning: A Deep Neural Network Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.09615" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.09615</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A backward pass through a CNN using a generative model of its activations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.02767" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.02767</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding deep learning requires rethinking generalization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2017 best paper. MIT &amp; Google Brain &amp; UC Berkeley &amp; Google DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.03530" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.03530</a></li><li>example code: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pluskid/fitting-random-labels" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pluskid/fitting-random-labels</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://theneuralperspective.com/2017/01/24/understanding-deep-learning-requires-rethinking-generalization/" class="Link-sc-1brdqhf-0 cKRjba">https://theneuralperspective.com/2017/01/24/understanding-deep-learning-requires-rethinking-generalization/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning the Number of Neurons in Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06321" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06321</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Survey of Expressivity in Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems</li><li>intro: Google Brain &amp; Cornell University &amp; Stanford University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08083" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08083</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Designing Neural Network Architectures using Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://bowenbaker.github.io/metaqnn/" class="Link-sc-1brdqhf-0 cKRjba">https://bowenbaker.github.io/metaqnn/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.02167" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.02167</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Robust Deep Neural Networks with BANG</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Colorado</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.00138" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.00138</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Quantization: Encoding Convolutional Activations with Deep Generative Model</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; MSR</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09502" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09502</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Probabilistic Theory of Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1504.00641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1504.00641</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Probabilistic Framework for Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Rice University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01936" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01936</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03928" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03928</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/szagoruyko/attention-transfer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/szagoruyko/attention-transfer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Risk versus Uncertainty in Deep Learning: Bayes, Bootstrap and the Dangers of Dropout</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Deepmind</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://bayesiandeeplearning.org/papers/BDL_4.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://bayesiandeeplearning.org/papers/BDL_4.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Brain &amp; Jagiellonian University</li><li>keywords: Sparsely-Gated Mixture-of-Experts layer (MoE), language modeling and machine translation</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.06538" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.06538</a></li><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/5pud72/research_outrageously_large_neural_networks_the/" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/5pud72/research_outrageously_large_neural_networks_the/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Network Guided Proof Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research &amp; University of Innsbruck</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.06972" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.06972</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PathNet: Evolution Channels Gradient Descent in Super Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind &amp; Google Brain</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.08734" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.08734</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273#.8f0o6w3en" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273#.8f0o6w3en</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.01135" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.01135</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Power of Sparsity in Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.06257" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.06257</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning across scales - A multiscale method for Convolution Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.02009" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.02009</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stacking-based Deep Neural Network: Deep Analytic Network on Convolutional Spectral Histogram Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.01396" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.01396</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Compositional Object-Based Approach to Learning Physical Dynamics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2017. Neural Physics Engine</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openreview.net/pdf?id=Bkab5dqxe" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.net/pdf?id=Bkab5dqxe</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mbchang/dynamics" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mbchang/dynamics</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Genetic CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.01513" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.01513</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/aqibsaeed/Genetic-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aqibsaeed/Genetic-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Sets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Amazon Web Services &amp; CMU</li><li>keywords: statistic estimation, point cloud classification, set expansion, and image tagging</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.06114" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.06114</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multiscale Hierarchical Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.04140" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.04140</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jhjacobsen/HierarchicalCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jhjacobsen/HierarchicalCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Neural Networks Do Not Recognize Negative Images</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.06857" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.06857</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Failures of Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07950" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07950</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/shakedshammah/failures_of_DL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shakedshammah/failures_of_DL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Scale Dense Convolutional Networks for Efficient Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Cornell University &amp; Tsinghua University &amp; Fudan University &amp; Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.09844" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.09844</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gaohuang/MSDNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gaohuang/MSDNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scaling the Scattering Transform: Deep Hybrid Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.08961" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.08961</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/edouardoyallon/scalingscattering" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/edouardoyallon/scalingscattering</a></li><li>github(CuPy/PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/edouardoyallon/pyscatwave" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/edouardoyallon/pyscatwave</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning is Robust to Massive Label Noise</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.10694" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.10694</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Input Fast-Forwarding for Better Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIAR 2017</li><li>keywords: Fast-Forward Network (FFNet)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08479" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08479</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Mutual Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>keywords: deep mutual learning (DML)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.00384" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.00384</a></li><li>github(official, TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/YingZhangDUT/Deep-Mutual-Learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YingZhangDUT/Deep-Mutual-Learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Automated Problem Identification: Regression vs Classification via Evolutionary Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Cape Town</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.00703" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.00703</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research &amp; CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.02968" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.02968</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Layer Aggregation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06484" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06484</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Robustness of Feature Representations to Image Deformations using Powered Convolution in CNNs</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07830" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07830</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning uncertainty in regression tasks by deep neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Free University of Berlin</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07287" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07287</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalizing the Convolution Operator in Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09864" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09864</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolution with Logarithmic Filter Groups for Efficient Shallow CNN</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09855" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09855</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Multi-View Learning with Stochastic Decorrelation Loss</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09669" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09669</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Take it in your stride: Do we need striding in CNNs?</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02502" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02502</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Security Risks in Deep Learning Implementation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Qihoo 360 Security Research Lab &amp; University of Georgia &amp; University of Virginia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.11008" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.11008</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Online Learning with Gated Linear Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01897" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01897</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>On the Information Bottleneck Theory of Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://openreview.net/forum?id=ry_WPG-A-&amp;noteId=ry_WPG-A" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.net/forum?id=ry_WPG-A-<!-- -->¬<!-- -->eId=ry_WPG-A</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://richzhang.github.io/PerceptualSimilarity/" class="Link-sc-1brdqhf-0 cKRjba">https://richzhang.github.io/PerceptualSimilarity/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.03924" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.03924</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//richzhang/PerceptualSimilarity" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//richzhang/PerceptualSimilarity</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of California, Davis</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02850" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02850</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards an Understanding of Neural Networks in Natural-Image Spaces</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.09097" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.09097</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Private-Feature Extraction</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.03151" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.03151</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Not All Samples Are Created Equal: Deep Learning with Importance Sampling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Idiap Research Institute</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.00942" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.00942</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Label Refinery: Improving ImageNet Classification through Label Progression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Using a Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNet from 59.3 to 67.2,
(2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from 50.6 to 55.59,
(4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to 74.47.</li><li>intro: XNOR AI, University of Washington, Allen AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.02641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.02641</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hessamb/label-refinery" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hessamb/label-refinery</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How Many Samples are Needed to Learn a Convolutional Neural Network?</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07883" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07883</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VisualBackProp for learning using privileged information with CNNs</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.09474" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.09474</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BAM: Bottleneck Attention Module</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018 (oral). Lunit Inc. &amp; Adobe Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.06514" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.06514</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CBAM: Convolutional Block Attention Module</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018. Lunit Inc. &amp; Adobe Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.06521" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.06521</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale equivariance in CNNs with vector fields</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML/FAIM 2018 workshop on Towards learning with limited labels: Equivariance, Invariance, and Beyond (oral presentation)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.11783" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.11783</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Downsampling leads to Image Memorization in Convolutional Autoencoders</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.10333" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.10333</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Do Normalization Layers in a Deep ConvNet Really Need to Be Distinct?</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07727" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07727</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Are All Training Examples Created Equal? An Empirical Study</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12569" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12569</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12231" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12231</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Chinese University of Hong Kong &amp; SenseTime Research</li><li>keywords: Match R-CNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.07973" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.07973</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Comprehensive Overhaul of Feature Distillation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01866" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01866</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mesh R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research (FAIR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.02739" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.02739</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ViP: Virtual Pooling for Accelerating CNN-based Image Classification and Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.07912" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.07912</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VarGNet: Variable Group Convolutional Neural Network for Efficient Embedded Computing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.05653" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.05653</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anchor Loss: Modulating Loss Scale based on Prediction Difficulty</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.11155" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.11155</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.05720" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.05720</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/max-andr/relu_networks_overconfident" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/max-andr/relu_networks_overconfident</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Space Augmentation for Long-Tailed Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.03673" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.03673</a></li></ul><h2 id="tutorials-and-surveys" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tutorials-and-surveys" color="auto.gray.8" aria-label="Tutorials and Surveys permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tutorials and Surveys</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Survey: Time Travel in Deep Learning Space: An Introduction to Deep Learning Models and How Deep Learning Models Evolved from the Initial Ideas</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.04781" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.04781</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>On the Origin of Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU. 70 pages, 200 references</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.07800" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.07800</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Processing of Deep Neural Networks: A Tutorial and Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.09039" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.09039</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">{<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.01164%7D(https://arxiv.org/abs/1803.01164)" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.01164}(https://arxiv.org/abs/1803.01164)</a></p><h2 id="mathematics-of-deep-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#mathematics-of-deep-learning" color="auto.gray.8" aria-label="Mathematics of Deep Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Mathematics of Deep Learning</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.06293" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.06293</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mathematics of Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University &amp; New York University &amp; Tel-Aviv University &amp; University of California, Los Angeles</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.04741" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.04741</a></li></ul><h2 id="local-minima" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#local-minima" color="auto.gray.8" aria-label="Local Minima permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Local Minima</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Local minima in training of deep networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06310" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06310</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep linear neural networks with arbitrary loss: All local minima are global</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; University of Southern California &amp; Facebook Artificial Intelligence Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00779" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00779</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gradient Descent Learns One-hidden-layer CNN: Don&#x27;t be Afraid of Spurious Local Minima</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Loyola Marymount University &amp; California State University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01473" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01473</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNNs are Globally Optimal Given Multi-Layer Support</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02501" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02501</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.08968" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.08968</a></p><h2 id="dive-into-cnn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#dive-into-cnn" color="auto.gray.8" aria-label="Dive Into CNN permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Dive Into CNN</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Structured Receptive Fields in CNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1605.02971" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1605.02971</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jhjacobsen/RFNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jhjacobsen/RFNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How ConvNets model Non-linear Transformations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.07664" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.07664</a></li></ul><h2 id="separable-convolutions--grouped-convolutions" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#separable-convolutions--grouped-convolutions" color="auto.gray.8" aria-label="Separable Convolutions / Grouped Convolutions permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Separable Convolutions / Grouped Convolutions</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Factorized Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial &quot;Bottleneck&quot; Structure</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04337" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04337</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>XSepConv: Extremely Separated Convolution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; University College London</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.12046" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.12046</a></li></ul><h2 id="stdp" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#stdp" color="auto.gray.8" aria-label="STDP permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>STDP</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A biological gradient descent for prediction through a combination of STDP and homeostatic plasticity</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1206.4812" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1206.4812</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An objective function for STDP</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.05936" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.05936</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards a Biologically Plausible Backprop</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.05179" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.05179</a></li></ul><h2 id="target-propagation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#target-propagation" color="auto.gray.8" aria-label="Target Propagation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Target Propagation</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1407.7906" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1407.7906</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Difference Target Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.7525" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.7525</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/donghyunlee/dtp" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/donghyunlee/dtp</a></li></ul><h2 id="zero-shot-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#zero-shot-learning" color="auto.gray.8" aria-label="Zero Shot Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Zero Shot Learning</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Deep Embedding Model for Zero-Shot Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05088" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05088</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot (Deep) Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://amundtveit.com/2016/11/18/zero-shot-deep-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://amundtveit.com/2016/11/18/zero-shot-deep-learning/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-shot learning experiments by deep learning.</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Elyorcv/zsl-deep-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Elyorcv/zsl-deep-learning</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Learning - The Good, the Bad and the Ugly</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.04394" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.04394</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Autoencoder for Zero-Shot Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://elyorcv.github.io/projects/sae" class="Link-sc-1brdqhf-0 cKRjba">https://elyorcv.github.io/projects/sae</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.08345" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.08345</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Elyorcv/SAE" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Elyorcv/SAE</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Learning via Category-Specific Visual-Semantic Mapping</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06167" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06167</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Learning via Class-Conditioned Deep Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05820" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05820</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Generating Networks for Zero-Shot Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00981" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00981</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01928" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01928</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Combining Deep Universal Features, Semantic Attributes, and Hierarchical Classification for Zero-Shot Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: extension to work published in conference proceedings of 2017 IAPR MVA Conference</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.03151" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.03151</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Context Label Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Multi-Context Label Embedding (MCLE) </li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.01199" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.01199</a></li></ul><h2 id="incremental-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#incremental-learning" color="auto.gray.8" aria-label="Incremental Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Incremental Learning</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>iCaRL: Incremental Classifier and Representation Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07725" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07725</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FearNet: Brain-Inspired Model for Incremental Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10563" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10563</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Purdue University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02719" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02719</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Incremental Classifier Learning with Generative Adversarial Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.00853" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.00853</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learn the new, keep the old: Extending pretrained models with new anatomy and images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MICCAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.00265" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.00265</a></li></ul><h2 id="ensemble-deep-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#ensemble-deep-learning" color="auto.gray.8" aria-label="Ensemble Deep Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Ensemble Deep Learning</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Neural Fabrics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1606.02492" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1606.02492</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/shreyassaxena/convolutional-neural-fabrics" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shreyassaxena/convolutional-neural-fabrics</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.07839" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.07839</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=KjUfMtZjyfg&amp;feature=youtu.be" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=KjUfMtZjyfg&amp;feature=youtu.be</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Snapshot Ensembles: Train 1, Get M for Free</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openreview.net/pdf?id=BJYwwY9ll" class="Link-sc-1brdqhf-0 cKRjba">http://openreview.net/pdf?id=BJYwwY9ll</a></li><li>github(Torch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/gaohuang/SnapshotEnsemble" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gaohuang/SnapshotEnsemble</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/titu1994/Snapshot-Ensembles" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/titu1994/Snapshot-Ensembles</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ensemble Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://amundtveit.com/2016/12/02/ensemble-deep-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://amundtveit.com/2016/12/02/ensemble-deep-learning/</a></li></ul><h2 id="domain-adaptation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#domain-adaptation" color="auto.gray.8" aria-label="Domain Adaptation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Domain Adaptation</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Discriminative Domain Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UC Berkeley &amp; Stanford University &amp; Boston University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.05464" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.05464</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//corenel/pytorch-adda" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//corenel/pytorch-adda</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Parameter Reference Loss for Unsupervised Domain Adaptation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07170" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07170</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Residual Parameter Transfer for Deep Domain Adaptation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07714" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07714</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Feature Augmentation for Unsupervised Domain Adaptation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08561" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08561</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image to Image Translation for Domain Adaptation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00479" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00479</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Incremental Adversarial Domain Adaptation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.07436" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.07436</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Visual Domain Adaptation: A Survey</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.03601" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.03601</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Domain Adaptation: A Multi-task Learning-based Method</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09208" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09208</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Importance Weighted Adversarial Nets for Partial Domain Adaptation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09210" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09210</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Open Set Domain Adaptation by Backpropagation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.10427" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.10427</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Sampling Policies for Domain Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07641</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Adversarial Domain Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018 Oral.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.02176" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.02176</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Domain Adaptation: An Adaptive Feature Norm Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Sun Yat-sen University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07456" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07456</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jihanyang/AFN/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jihanyang/AFN/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-source Distilling Domain Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.11554" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.11554</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/daoyuan98/MDDA" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daoyuan98/MDDA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>awsome-domain-adaptation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/zhaoxin94/awsome-domain-adaptation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhaoxin94/awsome-domain-adaptation</a></p><h2 id="embedding" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#embedding" color="auto.gray.8" aria-label="Embedding permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Embedding</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Embeddings with Histogram Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.00822" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.00822</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Full-Network Embedding in a Multimodal Embedding Pipeline</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09872" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09872</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Clustering-driven Deep Embedding with Pairwise Constraints</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08457" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08457</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Mixture of Experts via Shallow Embedding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.01531" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.01531</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Learn from Web Data through Deep Semantic Embeddings</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV MULA Workshop 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.06368" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.06368</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Heated-Up Softmax Embedding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.04157" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.04157</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Virtual Class Enhanced Discriminative Embedding Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12611" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12611</a></li></ul><h2 id="regression" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#regression" color="auto.gray.8" aria-label="Regression permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Regression</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Comprehensive Analysis of Deep Regression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08450" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08450</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Motifs: Scene Graph Parsing with Global Context</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. University of Washington</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://rowanzellers.com/neuralmotifs/" class="Link-sc-1brdqhf-0 cKRjba">http://rowanzellers.com/neuralmotifs/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06640" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06640</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rowanz/neural-motifs" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rowanz/neural-motifs</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://rowanzellers.com/scenegraph2/" class="Link-sc-1brdqhf-0 cKRjba">https://rowanzellers.com/scenegraph2/</a></li></ul><h2 id="capsnets" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#capsnets" color="auto.gray.8" aria-label="CapsNets permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CapsNets</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Routing Between Capsules</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Sara Sabour, Nicholas Frosst, Geoffrey E Hinton</li><li>intro: Google Brain, Toronto</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.09829" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.09829</a></li><li>github(official, Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Sarasra/models/tree/master/research/capsules" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Sarasra/models/tree/master/research/capsules</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Capsule Networks (CapsNets) – Tutorial</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=pPN8d0E3900" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=pPN8d0E3900</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="http://www.bilibili.com/video/av16594836/" class="Link-sc-1brdqhf-0 cKRjba">http://www.bilibili.com/video/av16594836/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improved Explainability of Capsule Networks: Relevance Path by Agreement</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Concordia University &amp; University of Toronto</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.10204" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.10204</a></li></ul><h2 id="low-light" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#low-light" color="auto.gray.8" aria-label="Low Light permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Low Light</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring Image Enhancement for Salient Object Detection in Low Light Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Transactions on Multimedia Computing, Communications, and Applications</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.16124" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.16124</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NOD: Taking a Closer Look at Detection under Extreme Low-Light Conditions with Night Object Detection Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.10364" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.10364</a></li></ul><h2 id="computer-vision" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#computer-vision" color="auto.gray.8" aria-label="Computer Vision permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Computer Vision</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Taxonomy of Deep Convolutional Neural Nets for Computer Vision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.06615" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.06615</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>On the usability of deep networks for object-based image analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: GEOBIA 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.06845" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.06845</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Recursive Filters for Low-Level Vision via a Hybrid Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.sifeiliu.net/linear-rnn" class="Link-sc-1brdqhf-0 cKRjba">http://www.sifeiliu.net/linear-rnn</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://faculty.ucmerced.edu/mhyang/papers/eccv16_rnn_filter.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://faculty.ucmerced.edu/mhyang/papers/eccv16_rnn_filter.pdf</a></li><li>poster: <a target="_blank" rel="noopener noreferrer" href="http://www.eccv2016.org/files/posters/O-3A-03.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eccv2016.org/files/posters/O-3A-03.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Liusifei/caffe-lowlevel" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Liusifei/caffe-lowlevel</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Toward Geometric Deep SLAM</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Magic Leap, Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07410" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07410</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Dual Convolutional Neural Networks for Low-Level Vision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.05020" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.05020</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Not just a matter of semantics: the relationship between visual similarity and semantic similarity</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07120" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07120</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DF-SLAM: A Deep-Learning Enhanced Visual SLAM System based on Deep Local Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BUPT &amp; Megvii</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.07223" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.07223</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GN-Net: The Gauss-Newton Loss for Deep Direct SLAM</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Technical University of Munich &amp; Artisense</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.11932" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.11932</a></li></ul><h3 id="all-in-one-network" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH3-sc-1fu06k9-3 ffNRvO gCPbFb cxpRJj Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#all-in-one-network" color="auto.gray.8" aria-label="All-In-One Network permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>All-In-One Network</h3><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1603.01249" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1603.01249</a></li><li>summary: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aleju/papers/blob/master/neural-nets/HyperFace.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aleju/papers/blob/master/neural-nets/HyperFace.md</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UberNet: Training a `Universal&#x27; Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.02132" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.02132</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://cvn.ecp.fr/ubernet/" class="Link-sc-1brdqhf-0 cKRjba">http://cvn.ecp.fr/ubernet/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An All-In-One Convolutional Neural Network for Face Analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: simultaneous face detection, face alignment, pose estimation, gender recognition, smile detection, age estimation and face recognition </li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.00851" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.00851</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: first place on Kitti Road Segmentation.
joint classification, detection and semantic segmentation via a unified architecture, less than 100 ms to perform all tasks</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.07695" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.07695</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MarvinTeichmann/MultiNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MarvinTeichmann/MultiNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.09806" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.09806</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual Person Understanding through Multi-Task and Multi-Dataset Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: RWTH Aachen University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.03019" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.03019</a></li></ul><h3 id="deep-learning-for-data-structures" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH3-sc-1fu06k9-3 ffNRvO gCPbFb cxpRJj Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-for-data-structures" color="auto.gray.8" aria-label="Deep Learning for Data Structures permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning for Data Structures</h3><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Case for Learned Index Structures</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT &amp; Google</li><li>keywords: B-Tree-Index, Hash-Index, BitMap-Index</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01208" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01208</a></li></ul><h1 id="projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Top Deep Learning Projects</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aymericdamien/TopDeepLearning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aymericdamien/TopDeepLearning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deepnet: Implementation of some deep learning algorithms</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nitishsrivastava/deepnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nitishsrivastava/deepnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepNeuralClassifier(Julia): Deep neural network using rectified linear units to classify hand written digits from the MNIST dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jostmey/DeepNeuralClassifier" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jostmey/DeepNeuralClassifier</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Clarifai Node.js Demo</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/patcat/Clarifai-Node-Demo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/patcat/Clarifai-Node-Demo</a></li><li>blog(&quot;How to Make Your Web App Smarter with Image Recognition&quot;): <a target="_blank" rel="noopener noreferrer" href="http://www.sitepoint.com/how-to-make-your-web-app-smarter-with-image-recognition/" class="Link-sc-1brdqhf-0 cKRjba">http://www.sitepoint.com/how-to-make-your-web-app-smarter-with-image-recognition/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning in Rust</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog(&quot;baby steps&quot;): <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@tedsta/deep-learning-in-rust-7e228107cccc#.t0pskuwkm" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@tedsta/deep-learning-in-rust-7e228107cccc#.t0pskuwkm</a></li><li>blog(&quot;a walk in the park&quot;): <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@tedsta/deep-learning-in-rust-a-walk-in-the-park-fed6c87165ea#.pucj1l5yx" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@tedsta/deep-learning-in-rust-a-walk-in-the-park-fed6c87165ea#.pucj1l5yx</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tedsta/deeplearn-rs" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tedsta/deeplearn-rs</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Implementation of state-of-art models in Torch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aciditeam/torch-models" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aciditeam/torch-models</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning (Python, C, C++, Java, Scala, Go)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yusugomori/DeepLearning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yusugomori/DeepLearning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deepmark: THE Deep Learning Benchmarks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DeepMark/deepmark" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DeepMark/deepmark</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Siamese Net</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;This package shows how to train a siamese network using Lasagne and Theano and includes network definitions
for state-of-the-art networks including: DeepID, DeepID2, Chopra et. al, and Hani et. al.
We also include one pre-trained model using a custom convolutional network.&quot;</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Kadenze/siamese_net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Kadenze/siamese_net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PRE-TRAINED CONVNETS AND OBJECT LOCALISATION IN KERAS</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://blog.heuritech.com/2016/04/26/pre-trained-convnets-and-object-localisation-in-keras/" class="Link-sc-1brdqhf-0 cKRjba">https://blog.heuritech.com/2016/04/26/pre-trained-convnets-and-object-localisation-in-keras/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/heuritech/convnets-keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/heuritech/convnets-keras</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning algorithms with TensorFlow: Ready to use implementations of various Deep Learning algorithms using TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.gabrieleangeletti.com/" class="Link-sc-1brdqhf-0 cKRjba">http://www.gabrieleangeletti.com/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/blackecho/Deep-Learning-TensorFlow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/blackecho/Deep-Learning-TensorFlow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Multi-threaded VGG 19 Feature Extractor</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/coreylynch/vgg-19-feature-extractor" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/coreylynch/vgg-19-feature-extractor</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Live demo of neural network classifying images</strong></p><img src="/assets/cnn-materials/nn_classify_images_live_demo.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://ml4a.github.io/dev/demos/cifar_confusion.html#" class="Link-sc-1brdqhf-0 cKRjba">http://ml4a.github.io/dev/demos/cifar_confusion.html#</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>mojo cnn: c++ convolutional neural network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: the fast and easy header only c++ convolutional neural network package</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gnawice/mojo-cnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gnawice/mojo-cnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepHeart: Neural networks for monitoring cardiac data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jisaacso/DeepHeart" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jisaacso/DeepHeart</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Water: Deep Learning in H2O using Native GPU Backends</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Native implementation of Deep Learning models for GPU backends (mxnet, Caffe, TensorFlow, etc.)</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/h2oai/deepwater" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/h2oai/deepwater</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Greentea LibDNN: Greentea LibDNN - a universal convolution implementation supporting CUDA and OpenCL</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/naibaf7/libdnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/naibaf7/libdnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dracula: A spookily good Part of Speech Tagger optimized for Twitter</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A deep, LSTM-based part of speech tagger and sentiment analyser using character embeddings instead of words.
Compatible with Theano and TensorFlow. Optimized for Twitter.</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://dracula.sentimentron.co.uk/" class="Link-sc-1brdqhf-0 cKRjba">http://dracula.sentimentron.co.uk/</a></li><li>speech tagging demo: <a target="_blank" rel="noopener noreferrer" href="http://dracula.sentimentron.co.uk/pos-demo/" class="Link-sc-1brdqhf-0 cKRjba">http://dracula.sentimentron.co.uk/pos-demo/</a></li><li>sentiment demo: <a target="_blank" rel="noopener noreferrer" href="http://dracula.sentimentron.co.uk/sentiment-demo/" class="Link-sc-1brdqhf-0 cKRjba">http://dracula.sentimentron.co.uk/sentiment-demo/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Sentimentron/Dracula" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Sentimentron/Dracula</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Trained image classification models for Keras</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Keras code and weights files for popular deep learning models.</li><li>intro: VGG16, VGG19, ResNet50, Inception v3</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fchollet/deep-learning-models" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fchollet/deep-learning-models</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PyCNN: Cellular Neural Networks Image Processing Python Library</strong></p><img src="https://camo.githubusercontent.com/0c5fd234a144b3d2145a133466766b2ecd9d3f3c/687474703a2f2f7777772e6973697765622e65652e6574687a2e63682f6861656e6767692f434e4e5f7765622f434e4e5f666967757265732f626c6f636b6469616772616d2e676966" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://blog.ankitaggarwal.me/PyCNN/" class="Link-sc-1brdqhf-0 cKRjba">http://blog.ankitaggarwal.me/PyCNN/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ankitaggarwal011/PyCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ankitaggarwal011/PyCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>regl-cnn: Digit recognition with Convolutional Neural Networks in WebGL</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TensorFlow, WebGL, <a target="_blank" rel="noopener noreferrer" href="https://github.com/mikolalysenko/regl" class="Link-sc-1brdqhf-0 cKRjba">regl</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Erkaman/regl-cnn/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Erkaman/regl-cnn/</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://erkaman.github.io/regl-cnn/src/demo.html" class="Link-sc-1brdqhf-0 cKRjba">https://erkaman.github.io/regl-cnn/src/demo.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>dagstudio: Directed Acyclic Graph Studio with Javascript D3</strong></p><img src="https://raw.githubusercontent.com/TimZaman/dagstudio/master/misc/20160907_dagstudio_ex.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TimZaman/dagstudio" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TimZaman/dagstudio</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NEUGO: Neural Networks in Go</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wh1t3w01f/neugo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wh1t3w01f/neugo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>gvnn: Neural Network Library for Geometric Computer Vision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.07405" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.07405</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ankurhanda/gvnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ankurhanda/gvnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepForge: A development environment for deep learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dfst/deepforge" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dfst/deepforge</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Implementation of recent Deep Learning papers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DenseNet / DeconvNet / DenseRecNet</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tdeboissiere/DeepLearningImplementations" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tdeboissiere/DeepLearningImplementations</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GPU-accelerated Theano &amp; Keras on Windows 10 native</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/philferriere/dlwin" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/philferriere/dlwin</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Head Pose and Gaze Direction Estimation Using Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mpatacchiola/deepgaze" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mpatacchiola/deepgaze</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://01.org/mkl-dnn" class="Link-sc-1brdqhf-0 cKRjba">https://01.org/mkl-dnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/01org/mkl-dnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/01org/mkl-dnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep CNN and RNN - Deep convolution/recurrent neural network project with TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tobegit3hub/deep_cnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tobegit3hub/deep_cnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Experimental implementation of novel neural network structures</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: binarynet / ternarynet / qrnn / vae / gcnn</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DingKe/nn_playground" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DingKe/nn_playground</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>WaterNet: A convolutional neural network that identifies water in satellite images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/treigerm/WaterNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/treigerm/WaterNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Kur: Descriptive Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepgram/kur" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deepgram/kur</a></li><li>docs: <a target="_blank" rel="noopener noreferrer" href="http://kur.deepgram.com/" class="Link-sc-1brdqhf-0 cKRjba">http://kur.deepgram.com/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Development of JavaScript-based deep learning platform and application to distributed training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Workshop paper for ICLR2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.01846" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.01846</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mil-tokyo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mil-tokyo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NewralNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A lightweight, easy to use and open source Java library for experimenting with
feed-forward neural nets and deep learning.</li><li>gitlab: <a target="_blank" rel="noopener noreferrer" href="https://gitlab.com/flimmerkiste/NewralNet" class="Link-sc-1brdqhf-0 cKRjba">https://gitlab.com/flimmerkiste/NewralNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FeatherCNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: FeatherCNN is a high performance inference engine for convolutional neural networks</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tencent/FeatherCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Tencent/FeatherCNN</a></li></ul><h1 id="readings-and-questions" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#readings-and-questions" color="auto.gray.8" aria-label="Readings and Questions permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Readings and Questions</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What you wanted to know about AI</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://fastml.com/what-you-wanted-to-know-about-ai/" class="Link-sc-1brdqhf-0 cKRjba">http://fastml.com/what-you-wanted-to-know-about-ai/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Epoch vs iteration when training neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>stackoverflow: <a target="_blank" rel="noopener noreferrer" href="http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks" class="Link-sc-1brdqhf-0 cKRjba">http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Questions to Ask When Applying Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://deeplearning4j.org/questions.html" class="Link-sc-1brdqhf-0 cKRjba">http://deeplearning4j.org/questions.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How can I know if Deep Learning works better for a specific problem than SVM or random forest?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rasbt/python-machine-learning-book/blob/master/faq/deeplearn-vs-svm-randomforest.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rasbt/python-machine-learning-book/blob/master/faq/deeplearn-vs-svm-randomforest.md</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What is the difference between deep learning and usual machine learning?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>note: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md</a></li></ul><h1 id="resources" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resources" color="auto.gray.8" aria-label="Resources permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Resources</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ChristosChristofidis/awesome-deep-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ChristosChristofidis/awesome-deep-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome-deep-vision: A curated list of deep learning resources for computer vision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>website: <a target="_blank" rel="noopener noreferrer" href="http://jiwonkim.org/awesome-deep-vision/" class="Link-sc-1brdqhf-0 cKRjba">http://jiwonkim.org/awesome-deep-vision/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kjw0612/awesome-deep-vision" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kjw0612/awesome-deep-vision</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Applied Deep Learning Resources: A collection of research articles, blog posts, slides and code snippets about deep learning in applied settings.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kristjankorjus/applied-deep-learning-resources" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kristjankorjus/applied-deep-learning-resources</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Libraries by Language</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>website: <a target="_blank" rel="noopener noreferrer" href="http://www.teglor.com/b/deep-learning-libraries-language-cm569/" class="Link-sc-1brdqhf-0 cKRjba">http://www.teglor.com/b/deep-learning-libraries-language-cm569/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Resources</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://yanirseroussi.com/deep-learning-resources/" class="Link-sc-1brdqhf-0 cKRjba">http://yanirseroussi.com/deep-learning-resources/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Resources</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://omtcyfz.github.io/2016/08/29/Deep-Learning-Resources.html" class="Link-sc-1brdqhf-0 cKRjba">https://omtcyfz.github.io/2016/08/29/Deep-Learning-Resources.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Turing Machine: musings on theory &amp; code(DEEP LEARNING REVOLUTION, summer 2015, state of the art &amp; topnotch links)</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://vzn1.wordpress.com/2015/09/01/deep-learning-revolution-summer-2015-state-of-the-art-topnotch-links/" class="Link-sc-1brdqhf-0 cKRjba">https://vzn1.wordpress.com/2015/09/01/deep-learning-revolution-summer-2015-state-of-the-art-topnotch-links/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BICV Group: Biologically Inspired Computer Vision research group</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.bicv.org/deep-learning/" class="Link-sc-1brdqhf-0 cKRjba">http://www.bicv.org/deep-learning/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://rt.dgyblog.com/ref/ref-learning-deep-learning.html" class="Link-sc-1brdqhf-0 cKRjba">http://rt.dgyblog.com/ref/ref-learning-deep-learning.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Summaries and notes on Deep Learning research papers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dennybritz/deeplearning-papernotes" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dennybritz/deeplearning-papernotes</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Glossary</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;Simple, opinionated explanations of various things encountered in Deep Learning / AI / ML.&quot;</li><li>author: Ryan Dahl, author of NodeJS. </li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ry/deep_learning_glossary" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ry/deep_learning_glossary</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Deep Learning Playbook</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/@jiefeng/deep-learning-playbook-c5ebe34f8a1a#.eg9cdz5ak" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@jiefeng/deep-learning-playbook-c5ebe34f8a1a#.eg9cdz5ak</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Study: Study of HeXA@UNIST in Preparation for Submission</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/carpedm20/deep-learning-study" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/carpedm20/deep-learning-study</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Books</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://machinelearningmastery.com/deep-learning-books/" class="Link-sc-1brdqhf-0 cKRjba">http://machinelearningmastery.com/deep-learning-books/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>awesome-very-deep-learning: A curated list of papers and code about very deep neural networks (50+ layers)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/daviddao/awesome-very-deep-learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daviddao/awesome-very-deep-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Resources and Tutorials using Keras and Lasagne</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Vict0rSch/deep_learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Vict0rSch/deep_learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning: Definition, Resources, Comparison with Machine Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.datasciencecentral.com/profiles/blogs/deep-learning-definition-resources-comparison-with-machine-learni" class="Link-sc-1brdqhf-0 cKRjba">http://www.datasciencecentral.com/profiles/blogs/deep-learning-definition-resources-comparison-with-machine-learni</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome - Most Cited Deep Learning Papers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/terryum/awesome-deep-learning-papers" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/terryum/awesome-deep-learning-papers</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The most cited papers in computer vision and deep learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://computervisionblog.wordpress.com/2016/06/19/the-most-cited-papers-in-computer-vision-and-deep-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://computervisionblog.wordpress.com/2016/06/19/the-most-cited-papers-in-computer-vision-and-deep-learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep learning papers: A place to collect papers that are related to deep learning and computational biology</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pimentel/deep_learning_papers" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pimentel/deep_learning_papers</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>papers-I-read</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;I am trying a new initiative - a-paper-a-week. This repository will hold all those papers and related summaries and notes.&quot;</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/shagunsodhani/papers-I-read" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shagunsodhani/papers-I-read</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LEARNING DEEP LEARNING - MY TOP-FIVE LIST</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://thegrandjanitor.com/2016/08/15/learning-deep-learning-my-top-five-resource/" class="Link-sc-1brdqhf-0 cKRjba">http://thegrandjanitor.com/2016/08/15/learning-deep-learning-my-top-five-resource/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>awesome-free-deep-learning-papers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HFTrader/awesome-free-deep-learning-papers" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HFTrader/awesome-free-deep-learning-papers</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepLearningBibliography: Bibliography for Publications about Deep Learning using GPU</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://memkite.com/deep-learning-bibliography/" class="Link-sc-1brdqhf-0 cKRjba">http://memkite.com/deep-learning-bibliography/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/memkite/DeepLearningBibliography" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/memkite/DeepLearningBibliography</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Papers Reading Roadmap</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep-learning-papers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Papers about deep learning ordered by task, date. Current state-of-the-art papers are labelled.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sbrugman/deep-learning-papers/blob/master/README.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sbrugman/deep-learning-papers/blob/master/README.md</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning and applications in Startups, CV, Text Mining, NLP</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lipiji/app-dl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lipiji/app-dl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ml4a-guides - a collection of practical resources for working with machine learning software, including code and tutorials</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://ml4a.github.io/guides/" class="Link-sc-1brdqhf-0 cKRjba">http://ml4a.github.io/guides/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep-learning-resources</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A Collection of resources I have found useful on my journey finding my way through the world of Deep Learning.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chasingbob/deep-learning-resources" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chasingbob/deep-learning-resources</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>21 Deep Learning Videos, Tutorials &amp; Courses on Youtube from 2016</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.analyticsvidhya.com/blog/2016/12/21-deep-learning-videos-tutorials-courses-on-youtube-from-2016/" class="Link-sc-1brdqhf-0 cKRjba">https://www.analyticsvidhya.com/blog/2016/12/21-deep-learning-videos-tutorials-courses-on-youtube-from-2016/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome Deep learning papers and other resources</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/endymecy/awesome-deeplearning-resources" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/endymecy/awesome-deeplearning-resources</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>awesome-deep-vision-web-demo</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A curated list of awesome deep vision web demo</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hwalsuklee/awesome-deep-vision-web-demo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hwalsuklee/awesome-deep-vision-web-demo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Summaries of machine learning papers</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/aleju/papers" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aleju/papers</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome Deep Learning Resources</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/guillaume-chevalier/awesome-deep-learning-resources" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guillaume-chevalier/awesome-deep-learning-resources</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Virginia Tech Vision and Learning Reading Group</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com//vt-vl-lab/reading_group" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//vt-vl-lab/reading_group</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MEGALODON: ML/DL Resources At One Place</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Various ML/DL Resources organised at a single place.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://github.com//vyraun/Megalodon" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//vyraun/Megalodon</a></li></ul><h2 id="arxiv-pages" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#arxiv-pages" color="auto.gray.8" aria-label="Arxiv Pages permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Arxiv Pages</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural and Evolutionary Computing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/list/cs.NE/recent" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/list/cs.NE/recent</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/list/cs.LG/recent" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/list/cs.LG/recent</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Computer Vision and Pattern Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/list/cs.CV/recent" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/list/cs.CV/recent</a></p><h2 id="arxiv-sanity-preserver" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#arxiv-sanity-preserver" color="auto.gray.8" aria-label="Arxiv Sanity Preserver permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Arxiv Sanity Preserver</h2><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Built by @karpathy to accelerate research.</li><li>page: <a target="_blank" rel="noopener noreferrer" href="http://www.arxiv-sanity.com/" class="Link-sc-1brdqhf-0 cKRjba">http://www.arxiv-sanity.com/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Today&#x27;s Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://todaysdeeplearning.com/" class="Link-sc-1brdqhf-0 cKRjba">http://todaysdeeplearning.com/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>arXiv Analytics</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://arxitics.com/" class="Link-sc-1brdqhf-0 cKRjba">http://arxitics.com/</a></p><h2 id="papers-with-code" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers-with-code" color="auto.gray.8" aria-label="Papers with Code permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers with Code</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Papers with Code</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://paperswithcode.com/" class="Link-sc-1brdqhf-0 cKRjba">https://paperswithcode.com/</a></p><h1 id="tools" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tools" color="auto.gray.8" aria-label="Tools permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tools</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DNNGraph - A deep neural network model generation DSL in Haskell</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://ajtulloch.github.io/dnngraph/" class="Link-sc-1brdqhf-0 cKRjba">http://ajtulloch.github.io/dnngraph/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep playground: an interactive visualization of neural networks, written in typescript using d3.js</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.23990&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification" class="Link-sc-1brdqhf-0 cKRjba">http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle<!-- -->®<!-- -->Dataset=reg-plane&amp;learningRate=0.03<!-- -->®<!-- -->ularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.23990&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/playground" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/playground</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Network Package</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: This package provides an easy and modular way to build and train simple or complex neural networks using Torch</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/torch/nn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/torch/nn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deepdish: Deep learning and data science tools from the University of Chicago</strong>
<strong>deepdish: Serving Up Chicago-Style Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://deepdish.io/" class="Link-sc-1brdqhf-0 cKRjba">http://deepdish.io/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/uchicago-cs/deepdish" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/uchicago-cs/deepdish</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AETROS CLI: Console application to manage deep neural network training in AETROS Trainer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Create, train and monitor deep neural networks using a model designer.</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://aetros.com/" class="Link-sc-1brdqhf-0 cKRjba">http://aetros.com/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aetros/aetros-cli" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aetros/aetros-cli</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Studio: Cloud platform for designing Deep Learning AI without programming</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://deepcognition.ai/" class="Link-sc-1brdqhf-0 cKRjba">http://deepcognition.ai/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>cuda-on-cl: Build NVIDIA® CUDA™ code for OpenCL™ 1.2 devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hughperkins/cuda-on-cl" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hughperkins/cuda-on-cl</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Receptive Field Calculator</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://fomoro.com/tools/receptive-fields/" class="Link-sc-1brdqhf-0 cKRjba">http://fomoro.com/tools/receptive-fields/</a></li><li>example: <a target="_blank" rel="noopener noreferrer" href="http://fomoro.com/tools/receptive-fields/#3,1,1,VALID;3,1,1,VALID;3,1,1,VALID" class="Link-sc-1brdqhf-0 cKRjba">http://fomoro.com/tools/receptive-fields/#3,1,1,VALID;3,1,1,VALID;3,1,1,VALID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>receptivefield</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: (PyTorch/Keras/TensorFlow)Gradient based receptive field estimation for Convolutional Neural Networks</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//fornaxai/receptivefield" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//fornaxai/receptivefield</a></li></ul><h1 id="challenges--hackathons" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#challenges--hackathons" color="auto.gray.8" aria-label="Challenges / Hackathons permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Challenges / Hackathons</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Open Images Challenge 2018</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://storage.googleapis.com/openimages/web/challenge.html" class="Link-sc-1brdqhf-0 cKRjba">https://storage.googleapis.com/openimages/web/challenge.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VisionHack 2017</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 10 - 14 Sep 2017, Moscow, Russia</li><li>intro: a full-fledged hackathon that will last three full days</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://visionhack.misis.ru/" class="Link-sc-1brdqhf-0 cKRjba">http://visionhack.misis.ru/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NVIDIA AI City Challenge Workshop at CVPR 2018</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.aicitychallenge.org/" class="Link-sc-1brdqhf-0 cKRjba">http://www.aicitychallenge.org/</a></p><h1 id="books" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#books" color="auto.gray.8" aria-label="Books permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Books</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>author: Ian Goodfellow, Aaron Courville and Yoshua Bengio</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.deeplearningbook.org/" class="Link-sc-1brdqhf-0 cKRjba">http://www.deeplearningbook.org/</a></li><li>website: <a target="_blank" rel="noopener noreferrer" href="http://goodfeli.github.io/dlbook/" class="Link-sc-1brdqhf-0 cKRjba">http://goodfeli.github.io/dlbook/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HFTrader/DeepLearningBook" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HFTrader/DeepLearningBook</a></li><li>notes(&quot;Deep Learning for Beginners&quot;): <a target="_blank" rel="noopener noreferrer" href="http://randomekek.github.io/deep/deeplearning.html" class="Link-sc-1brdqhf-0 cKRjba">http://randomekek.github.io/deep/deeplearning.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fundamentals of Deep Learning: Designing Next-Generation Artificial Intelligence Algorithms</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>author: Nikhil Buduma</li><li>book review: <a target="_blank" rel="noopener noreferrer" href="http://www.opengardensblog.futuretext.com/archives/2015/08/book-review-fundamentals-of-deep-learning-designing-next-generation-artificial-intelligence-algorithms-by-nikhil-buduma.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.opengardensblog.futuretext.com/archives/2015/08/book-review-fundamentals-of-deep-learning-designing-next-generation-artificial-intelligence-algorithms-by-nikhil-buduma.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FIRST CONTACT WITH TENSORFLOW: Get started with with Deep Learning programming</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>author: Jordi Torres</li><li>book: <a target="_blank" rel="noopener noreferrer" href="http://www.jorditorres.org/first-contact-with-tensorflow/" class="Link-sc-1brdqhf-0 cKRjba">http://www.jorditorres.org/first-contact-with-tensorflow/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>《解析卷积神经网络—深度学习实践手册》</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: by 魏秀参（Xiu-Shen WEI）</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://lamda.nju.edu.cn/weixs/book/CNN_book.html" class="Link-sc-1brdqhf-0 cKRjba">http://lamda.nju.edu.cn/weixs/book/CNN_book.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Make Your Own Neural Network: IPython Neural Networks on a Raspberry Pi Zero</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>book: <a target="_blank" rel="noopener noreferrer" href="http://makeyourownneuralnetwork.blogspot.jp/2016/03/ipython-neural-networks-on-raspberry-pi.html" class="Link-sc-1brdqhf-0 cKRjba">http://makeyourownneuralnetwork.blogspot.jp/2016/03/ipython-neural-networks-on-raspberry-pi.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork</a></li></ul><h1 id="blogs" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#blogs" color="auto.gray.8" aria-label="Blogs permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blogs</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Networks and Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://neuralnetworksanddeeplearning.com" class="Link-sc-1brdqhf-0 cKRjba">http://neuralnetworksanddeeplearning.com</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Reading List</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://deeplearning.net/reading-list/" class="Link-sc-1brdqhf-0 cKRjba">http://deeplearning.net/reading-list/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>WILDML: A BLOG ABOUT MACHINE LEARNING, DEEP LEARNING AND NLP.</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.wildml.com/" class="Link-sc-1brdqhf-0 cKRjba">http://www.wildml.com/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Andrej Karpathy blog</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://karpathy.github.io/" class="Link-sc-1brdqhf-0 cKRjba">http://karpathy.github.io/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rodrigob&#x27;s github page</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://rodrigob.github.io/" class="Link-sc-1brdqhf-0 cKRjba">http://rodrigob.github.io/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>colah&#x27;s blog</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://colah.github.io/" class="Link-sc-1brdqhf-0 cKRjba">http://colah.github.io/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What My Deep Model Doesn&#x27;t Know...</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html" class="Link-sc-1brdqhf-0 cKRjba">http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Christoph Feichtenhofer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PhD Student, Graz University of Technology</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://feichtenhofer.github.io/" class="Link-sc-1brdqhf-0 cKRjba">http://feichtenhofer.github.io/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image recognition is not enough: As with language, photos need contextual intelligence</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/@ken_getquik/image-recognition-is-not-enough-293cd7d58004#.dex817l2z" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@ken_getquik/image-recognition-is-not-enough-293cd7d58004#.dex817l2z</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ResNets, HighwayNets, and DenseNets, Oh My!</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@awjuliani/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32#.pgltg8pro" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@awjuliani/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32#.pgltg8pro</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/awjuliani/TF-Tutorials/blob/master/Deep%20Network%20Comparison.ipynb" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/awjuliani/TF-Tutorials/blob/master/Deep%20Network%20Comparison.ipynb</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Frontiers of Memory and Attention in Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>sldies: <a target="_blank" rel="noopener noreferrer" href="http://slides.com/smerity/quora-frontiers-of-memory-and-attention#/" class="Link-sc-1brdqhf-0 cKRjba">http://slides.com/smerity/quora-frontiers-of-memory-and-attention#/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Design Patterns for Deep Learning Architectures</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.deeplearningpatterns.com/doku.php" class="Link-sc-1brdqhf-0 cKRjba">http://www.deeplearningpatterns.com/doku.php</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Building a Deep Learning Powered GIF Search Engine</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@zan2434/building-a-deep-learning-powered-gif-search-engine-a3eb309d7525" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@zan2434/building-a-deep-learning-powered-gif-search-engine-a3eb309d7525</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>850k Images in 24 hours: Automating Deep Learning Dataset Creation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://gab41.lab41.org/850k-images-in-24-hours-automating-deep-learning-dataset-creation-60bdced04275#.xhq9feuxx" class="Link-sc-1brdqhf-0 cKRjba">https://gab41.lab41.org/850k-images-in-24-hours-automating-deep-learning-dataset-creation-60bdced04275#.xhq9feuxx</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How six lines of code + SQL Server can bring Deep Learning to ANY App</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://blogs.technet.microsoft.com/dataplatforminsider/2017/01/05/how-six-lines-of-code-sql-server-can-bring-deep-learning-to-any-app/" class="Link-sc-1brdqhf-0 cKRjba">https://blogs.technet.microsoft.com/dataplatforminsider/2017/01/05/how-six-lines-of-code-sql-server-can-bring-deep-learning-to-any-app/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Microsoft/SQL-Server-R-Services-Samples/tree/master/Galaxies" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Microsoft/SQL-Server-R-Services-Samples/tree/master/Galaxies</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Network Architectures</strong></p><img src="https://culurciello.github.io/assets/nets/acc_vs_net_vs_ops.svg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba#.m8y39oih6" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba#.m8y39oih6</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://culurciello.github.io/tech/2016/06/04/nets.html" class="Link-sc-1brdqhf-0 cKRjba">https://culurciello.github.io/tech/2016/06/04/nets.html</a></li></ul><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/30/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>