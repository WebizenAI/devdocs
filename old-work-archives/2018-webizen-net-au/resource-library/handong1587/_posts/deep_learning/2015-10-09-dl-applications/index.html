<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Applications DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations arxiv:  http://arxiv.org/abs/1510.02927 Some …"/><meta data-react-helmet="true" name="twitter:title" content="Deep Learning Applications"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-28T19:22:29.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Applications DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations arxiv:  http://arxiv.org/abs/1510.02927 Some …"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Deep Learning Applications"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Applications DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations arxiv:  http://arxiv.org/abs/1510.02927 Some …"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,HGjBQ,fdzjHV,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Deep Learning Applications - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">awesomeLists</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#applications" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Applications</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#boundary--edge--contour-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Boundary / Edge / Contour Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-processing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Processing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-text" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image-Text</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#age-estimation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Age Estimation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#face-aging" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Aging</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#emotion-recognition--expression-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Emotion Recognition / Expression Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#attribution-prediction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Attribution Prediction</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#place-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Place Recognition</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#camera-relocalization" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Camera Relocalization</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#activity-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Activity Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#music-classification--sound-classification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Music Classification / Sound Classification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#nsfw-detection--classification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">NSFW Detection / Classification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-reconstruction--inpainting" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Reconstruction / Inpainting</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-restoration" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Restoration</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#face-completion" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Completion</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-denoising" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Denoising</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-dehazing--image-haze-removal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Dehazing / Image Haze Removal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-rain-removal--de-raining" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Rain Removal / De-raining</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#fence-removal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Fence Removal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#snow-removal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Snow Removal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blur-detection-and-removal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blur Detection and Removal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-compression" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Compression</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-quality-assessment" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Quality Assessment</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-blending" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Blending</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-enhancement" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Enhancement</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#abnormality-detection--anomaly-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Abnormality Detection / Anomaly Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#depth-prediction--depth-estimation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Depth Prediction / Depth Estimation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#texture-synthesis" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Texture Synthesis</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-cropping" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Cropping</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-synthesis" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Synthesis</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-tagging" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Tagging</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-matching" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Matching</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-editing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Editing</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#face-swap--face-editing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Swap &amp; Face Editing</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#stereo" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Stereo</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#3d" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">3D</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-for-makeup" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning for Makeup</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#music-tagging" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Music Tagging</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#action-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Action Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#ctr-prediction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">CTR Prediction</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#cryptography" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Cryptography</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#cyber-security" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Cyber Security</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#lip-reading" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Lip Reading</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#event-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Event Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#trajectory-prediction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Trajectory Prediction</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#human-object-interaction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Human-Object Interaction</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-in-finance" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning in Finance</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-in-speech" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning in Speech</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#wavenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">WaveNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-for-sound--music" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning for Sound / Music</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#sound" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Sound</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#music" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Music</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-in-medicine-and-biology" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning in Medicine and Biology</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-for-fashion" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning for Fashion</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#others" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Others</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Deep Learning Applications</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#applications" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Applications</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#boundary--edge--contour-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Boundary / Edge / Contour Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-processing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Processing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-text" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image-Text</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#age-estimation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Age Estimation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#face-aging" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Aging</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#emotion-recognition--expression-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Emotion Recognition / Expression Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#attribution-prediction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Attribution Prediction</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#place-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Place Recognition</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#camera-relocalization" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Camera Relocalization</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#activity-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Activity Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#music-classification--sound-classification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Music Classification / Sound Classification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#nsfw-detection--classification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">NSFW Detection / Classification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-reconstruction--inpainting" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Reconstruction / Inpainting</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-restoration" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Restoration</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#face-completion" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Completion</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-denoising" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Denoising</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-dehazing--image-haze-removal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Dehazing / Image Haze Removal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-rain-removal--de-raining" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Rain Removal / De-raining</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#fence-removal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Fence Removal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#snow-removal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Snow Removal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blur-detection-and-removal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blur Detection and Removal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-compression" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Compression</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-quality-assessment" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Quality Assessment</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-blending" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Blending</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-enhancement" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Enhancement</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#abnormality-detection--anomaly-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Abnormality Detection / Anomaly Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#depth-prediction--depth-estimation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Depth Prediction / Depth Estimation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#texture-synthesis" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Texture Synthesis</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-cropping" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Cropping</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-synthesis" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Synthesis</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-tagging" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Tagging</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-matching" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Matching</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#image-editing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Image Editing</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#face-swap--face-editing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Swap &amp; Face Editing</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#stereo" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Stereo</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#3d" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">3D</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-for-makeup" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning for Makeup</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#music-tagging" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Music Tagging</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#action-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Action Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#ctr-prediction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">CTR Prediction</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#cryptography" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Cryptography</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#cyber-security" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Cyber Security</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#lip-reading" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Lip Reading</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#event-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Event Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#trajectory-prediction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Trajectory Prediction</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#human-object-interaction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Human-Object Interaction</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-in-finance" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning in Finance</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-in-speech" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning in Speech</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#wavenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">WaveNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-for-sound--music" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning for Sound / Music</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#sound" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Sound</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#music" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Music</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-in-medicine-and-biology" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning in Medicine and Biology</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-learning-for-fashion" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Learning for Fashion</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#others" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Others</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li></ul></div></div><h1 id="applications" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#applications" color="auto.gray.8" aria-label="Applications permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Applications</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.02927" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.02927</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Some like it hot - visual guidance for preference prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.07867" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.07867</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://howhot.io/" class="Link-sc-1brdqhf-0 cKRjba">http://howhot.io/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.03131" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.03131</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Relative Attributes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.04103" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.04103</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yassersouri/ghiaseddin" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yassersouri/ghiaseddin</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep-Spying: Spying using Smartwatch and Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.05616" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.05616</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tonybeltramelli/Deep-Spying" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tonybeltramelli/Deep-Spying</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Camera identification with deep convolutional networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>key word: copyright infringement cases, ownership attribution</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.01068" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.01068</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Analysis of Deep Neural Network Models for Practical Applications</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.07678" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.07678</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>8 Inspirational Applications of Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Colorization of Black and White Images, Adding Sounds To Silent Movies, Automatic Machine Translation
Object Classification in Photographs, Automatic Handwriting Generation, Character Text Generation,
Image Caption Generation, Automatic Game Playing</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://machinelearningmastery.com/inspirational-applications-deep-learning/" class="Link-sc-1brdqhf-0 cKRjba">http://machinelearningmastery.com/inspirational-applications-deep-learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>16 Open Source Deep Learning Models Running as Microservices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Places 365 Classifier, Deep Face Recognition, Real Estate Classifier, Colorful Image Colorization,
Illustration Tagger, InceptionNet, Parsey McParseface, ArtsyNetworks</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://blog.algorithmia.com/2016/07/open-source-deep-learning-algorithm-roundup/" class="Link-sc-1brdqhf-0 cKRjba">http://blog.algorithmia.com/2016/07/open-source-deep-learning-algorithm-roundup/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Cascaded Bi-Network for Face Hallucination</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/CBN.html" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/CBN.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.05046" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.05046</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation</strong></p><img src="http://sites.skoltech.ru/compvision/projects/deepwarp/images/pipeline.svg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://yaroslav.ganin.net/static/deepwarp/" class="Link-sc-1brdqhf-0 cKRjba">http://yaroslav.ganin.net/static/deepwarp/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.07215" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.07215</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Autoencoding Blade Runner</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.9kckqg7cq" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.9kckqg7cq</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/terrybroad/Learned-Sim-Autoencoder-For-Video-Frames" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/terrybroad/Learned-Sim-Autoencoder-For-Video-Frames</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A guy trained a machine to &quot;watch&quot; Blade Runner. Then things got seriously sci-fi.</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.vox.com/2016/6/1/11787262/blade-runner-neural-network-encoding" class="Link-sc-1brdqhf-0 cKRjba">http://www.vox.com/2016/6/1/11787262/blade-runner-neural-network-encoding</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Convolution Networks for Compression Artifacts Reduction</strong></p><img src="http://mmlab.ie.cuhk.edu.hk/projects/ARCNN/img/fig1.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>project page(code): <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.02778" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.02778</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep GDashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Deep Genomic Dashboard (Deep GDashboard)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03644" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03644</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instagram photos reveal predictive markers of depression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03282" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03282</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How an Algorithm Learned to Identify Depressed Individuals by Studying Their Instagram Photos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>review: <a target="_blank" rel="noopener noreferrer" href="https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/" class="Link-sc-1brdqhf-0 cKRjba">https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IM2CAD</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.05137" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.05137</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/melicher" class="Link-sc-1brdqhf-0 cKRjba">https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/melicher</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cupslab/neural_network_cracking" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cupslab/neural_network_cracking</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Defeating Image Obfuscation with Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.00408" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.00408</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting Music BPM using Neural Networks</strong></p><img src="https://nlml.github.io/images/convnet_diagram.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: BPM (Beats Per Minutes)</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/" class="Link-sc-1brdqhf-0 cKRjba">https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nlml/bpm" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nlml/bpm</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generative Visual Manipulation on the Natural Image Manifold</strong></p><img src="https://raw.githubusercontent.com/junyanz/iGAN/master/pics/demo_teaser.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://people.eecs.berkeley.edu/~junyanz/projects/gvm/" class="Link-sc-1brdqhf-0 cKRjba">https://people.eecs.berkeley.edu/~junyanz/projects/gvm/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.03552" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.03552</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/junyanz/iGAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/junyanz/iGAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Impression: Audiovisual Deep Residual Networks for Multimodal Apparent Personality Trait Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.05119" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.05119</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Gold: Using Convolution Networks to Find Minerals</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df#.lgh95ub4a" class="Link-sc-1brdqhf-0 cKRjba">https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df#.lgh95ub4a</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/scottvallance/DeepGold" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/scottvallance/DeepGold</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Predicting First Impressions with Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.08119" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.08119</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Judging a Book By its Cover</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.09204" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.09204</a></li><li>review: <a target="_blank" rel="noopener noreferrer" href="https://www.technologyreview.com/s/602807/deep-neural-network-learns-to-judge-books-by-their-covers/" class="Link-sc-1brdqhf-0 cKRjba">https://www.technologyreview.com/s/602807/deep-neural-network-learns-to-judge-books-by-their-covers/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Credibility Analysis with Effective Domain Transferred Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05328" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05328</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A novel image tag completion method based on convolutional neural network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://www.arxiv.org/abs/1703.00586" class="Link-sc-1brdqhf-0 cKRjba">https://www.arxiv.org/abs/1703.00586</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image operator learning coupled with CNN classification and its application to staff line removal</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICDAR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.06476" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.06476</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Image Filtering with Deep Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of California, Merced &amp; Virginia Tech &amp; University of Illinois</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://vllab1.ucmerced.edu/~yli62/DJF_residual/" class="Link-sc-1brdqhf-0 cKRjba">http://vllab1.ucmerced.edu/~yli62/DJF_residual/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.04200" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.04200</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Yijunmaverick/DeepJointFilter" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Yijunmaverick/DeepJointFilter</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.02470" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.02470</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aiff22/DPED" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aiff22/DPED</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Scene De-rendering</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://nsd.csail.mit.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://nsd.csail.mit.edu/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jiajunwu/nsd" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jiajunwu/nsd</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image2GIF: Generating Cinemagraphs using Recurrent Deep Q-Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://bvision11.cs.unc.edu/bigpen/yipin/WACV2018/" class="Link-sc-1brdqhf-0 cKRjba">http://bvision11.cs.unc.edu/bigpen/yipin/WACV2018/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.09042" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.09042</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Neural Networks In Fully Connected CRF For Image Labeling With Social Network Metadata</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.09108" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.09108</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single Image Reflection Removal Using Deep Encoder-Decoder Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.00094" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.00094</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CRRN: Multi-Scale Guided Concurrent Reflection Removal Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.11802" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.11802</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Convolutional Networks for Demosaicing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.03769" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.03769</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully convolutional watermark removal attack</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/marcbelmont/cnn-watermark-removal" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/marcbelmont/cnn-watermark-removal</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10562" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10562</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Prinsphield/ELEGANT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Prinsphield/ELEGANT</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to See in the Dark</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://web.engr.illinois.edu/~cchen156/SID.html" class="Link-sc-1brdqhf-0 cKRjba">http://web.engr.illinois.edu/~cchen156/SID.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.01934" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.01934</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cchen156/Learning-to-See-in-the-Dark" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cchen156/Learning-to-See-in-the-Dark</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=qWKUFK7MWvg&amp;feature=youtu.be" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=qWKUFK7MWvg&amp;feature=youtu.be</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av23195280/" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av23195280/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generative Smoke Removal</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.00311" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.00311</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.10683" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.10683</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Blind Visual Motif Removal from a Single Image</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02756" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02756</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Camera Simulators</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.05237" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.05237</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lighting the Darkness in the Deep Learning Era</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.10729" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.10729</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open</a></li></ul><h1 id="boundary--edge--contour-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#boundary--edge--contour-detection" color="auto.gray.8" aria-label="Boundary / Edge / Contour Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Boundary / Edge / Contour Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Holistically-Nested Edge Detection</strong></p><img src="https://camo.githubusercontent.com/da32e7e3275c2a9693dd2a6925b03a1151e2b098/687474703a2f2f70616765732e756373642e6564752f7e7a74752f6865642e6a7067" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015, Marr Prize</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.06375" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.06375</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/s9xie/hed" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/s9xie/hed</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/moabitcoin/holy-edge" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/moabitcoin/holy-edge</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Learning of Edges</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016. Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.04166" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.04166</a></li><li>zn-blog: <a target="_blank" rel="noopener noreferrer" href="http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pushing the Boundaries of Boundary Detection using Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07386" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07386</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Oriented Boundaries</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.02755" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.02755</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.vision.ee.ethz.ch/~cvlsegmentation/" class="Link-sc-1brdqhf-0 cKRjba">http://www.vision.ee.ethz.ch/~cvlsegmentation/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.04658" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.04658</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kmaninis/COB" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kmaninis/COB</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Richer Convolutional Features for Edge Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>keywords: richer convolutional features (RCF)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02103" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02103</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yun-liu/rcf" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yun-liu/rcf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Contour Detection from Deep Patch-level Boundary Prediction</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.03159" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.03159</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CASENet: Deep Category-Aware Semantic Edge Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. CMU &amp; Mitsubishi Electric Research Laboratories (MERL)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.09759" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.09759</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://www.merl.com/research/license#CASENet" class="Link-sc-1brdqhf-0 cKRjba">http://www.merl.com/research/license#CASENet</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=BNE1hAP6Qho" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=BNE1hAP6Qho</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00524" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00524</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Crisp Boundaries: From Boundaries to Higher-level Tasks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02439" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02439</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DOOBNet: Deep Object Occlusion Boundary Detection from an Image</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.03772" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.03772</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Feature Fusion for Semantic Edge Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.09104" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.09104</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EDTER: Edge Detection with Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.08566" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.08566</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MengyangPu/EDTER" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MengyangPu/EDTER</a></li></ul><h1 id="image-processing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-processing" color="auto.gray.8" aria-label="Image Processing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Processing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Image Processing with Fully-Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. Qifeng Chen (陈启峰)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cqf.io/ImageProcessing/" class="Link-sc-1brdqhf-0 cKRjba">http://www.cqf.io/ImageProcessing/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.00643" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.00643</a></li><li>supp: <a target="_blank" rel="noopener noreferrer" href="https://youtu.be/eQyfHgLx8Dc" class="Link-sc-1brdqhf-0 cKRjba">https://youtu.be/eQyfHgLx8Dc</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CQFIO/FastImageProcessing" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CQFIO/FastImageProcessing</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepISP: Learning End-to-End Image Processing Pipeline</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06724" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06724</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.04323" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.04323</a></li></ul><h1 id="image-text" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-text" color="auto.gray.8" aria-label="Image-Text permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image-Text</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Two-Branch Neural Networks for Image-Text Matching Tasks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03470" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03470</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dual-Path Convolutional Image-Text Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05535" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05535</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//layumi/Image-Text-Embedding" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//layumi/Image-Text-Embedding</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Conditional Image-Text Embedding Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08389" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08389</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10485" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10485</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stacked Cross Attention for Image-Text Matching</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08024" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08024</a></p><h1 id="age-estimation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#age-estimation" color="auto.gray.8" aria-label="Age Estimation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Age Estimation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deeply-Learned Feature for Age Estimation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7045931&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7045931" class="Link-sc-1brdqhf-0 cKRjba">http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7045931&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7045931</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Age and Gender Classification using Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf</a></li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.openu.ac.il/home/hassner/projects/cnn_agegender/" class="Link-sc-1brdqhf-0 cKRjba">http://www.openu.ac.il/home/hassner/projects/cnn_agegender/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/GilLevi/AgeGenderDeepLearning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/GilLevi/AgeGenderDeepLearning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Group-Aware Deep Feature Learning For Facial Age Estimation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.sciencedirect.com/science/article/pii/S0031320316303417" class="Link-sc-1brdqhf-0 cKRjba">http://www.sciencedirect.com/science/article/pii/S0031320316303417</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Local Deep Neural Networks for Age and Gender Classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.08497" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.08497</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding and Comparing Deep Neural Networks for Age and Gender Classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.07689" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.07689</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Age Group and Gender Estimation in the Wild with Deep RoR Architecture</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE ACCESS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.02985" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.02985</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Age and gender estimation based on Convolutional Neural Network and TensorFlow</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BoyuanJiang/Age-Gender-Estimate-TF" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/BoyuanJiang/Age-Gender-Estimate-TF</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Regression Forests for Age Estimation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai University &amp; Johns Hopkins University &amp; Nankai University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.07195" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.07195</a></li></ul><h1 id="face-aging" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#face-aging" color="auto.gray.8" aria-label="Face Aging permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Face Aging</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Face Aging</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a class="Link-sc-1brdqhf-0 cKRjba" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf/">www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Aging With Conditional Generative Adversarial Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.01983" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.01983</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Face Age Progression: A Pyramid Architecture of GANs</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10352" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10352</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Aging with Contextual Generative Adversarial Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.00237" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.00237</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recursive Chaining of Reversible Image-to-image Translators For Face Aging</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.05023" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.05023</a></p><h1 id="emotion-recognition--expression-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#emotion-recognition--expression-recognition" color="auto.gray.8" aria-label="Emotion Recognition / Expression Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Emotion Recognition / Expression Recognition</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-time emotion recognition for gaming using deep convolutional network features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1408.3750v1" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1408.3750v1</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zebreu/ConvolutionalEmotion" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Zebreu/ConvolutionalEmotion</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/" class="Link-sc-1brdqhf-0 cKRjba">http://www.openu.ac.il/home/hassner/projects/cnn_emotions/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/GilLevi/54aee1b8b0397721aa4b" class="Link-sc-1brdqhf-0 cKRjba">https://gist.github.com/GilLevi/54aee1b8b0397721aa4b</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://gilscvblog.com/2017/01/31/emotion-recognition-in-the-wild-via-convolutional-neural-networks-and-mapped-binary-patterns/" class="Link-sc-1brdqhf-0 cKRjba">https://gilscvblog.com/2017/01/31/emotion-recognition-in-the-wild-via-convolutional-neural-networks-and-mapped-binary-patterns/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeXpression: Deep Convolutional Neural Network for Expression Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.05371" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.05371</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DEX: Deep EXpectation of apparent age from a single image</strong></p><img src="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/img/pipeline.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.vision.ee.ethz.ch/en/publications/papers/proceedings/eth_biwi_01229.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.vision.ee.ethz.ch/en/publications/papers/proceedings/eth_biwi_01229.pdf</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/" class="Link-sc-1brdqhf-0 cKRjba">https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EmotioNet: EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://cbcsl.ece.ohio-state.edu/cvpr16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cbcsl.ece.ohio-state.edu/cvpr16.pdf</a></li><li>database: <a target="_blank" rel="noopener noreferrer" href="http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html" class="Link-sc-1brdqhf-0 cKRjba">http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How Deep Neural Networks Can Improve Emotion Recognition on Video Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.07377" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.07377</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Peak-Piloted Deep Network for Facial Expression Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.06997" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.06997</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.01041" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.01041</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Recursive Framework for Expression Recognition: From Web Images to Deep Models to Game Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.01647" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.01647</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.06591" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.06591</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EmotionNet Challenge</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homrepage: <a target="_blank" rel="noopener noreferrer" href="http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html</a></li><li>dataset: <a target="_blank" rel="noopener noreferrer" href="http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html" class="Link-sc-1brdqhf-0 cKRjba">http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Baseline CNN structure analysis for facial expression recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: RO-MAN2016 Conference</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.04251" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.04251</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Facial Expression Recognition using Convolutional Neural Networks: State of the Art</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02903" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02903</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.04280" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.04280</a></li><li>api: <a target="_blank" rel="noopener noreferrer" href="https://www.sighthound.com/products/cloud" class="Link-sc-1brdqhf-0 cKRjba">https://www.sighthound.com/products/cloud</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep generative-contrastive networks for facial expression recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07140" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07140</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Neural Networks for Facial Expression Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.06756" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.06756</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Multimodal Emotion Recognition using Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Imperial College London</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.08619" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.08619</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial-Temporal Recurrent Neural Network for Emotion Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.04515" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.04515</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.01509" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.01509</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Multimodal Fusion for Video Emotion Classification in the Wild</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.07200" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.07200</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Island Loss for Learning Discriminative Features in Facial Expression Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.03144" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.03144</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-time Convolutional Neural Networks for Emotion and Gender Classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.07557" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.07557</a></p><h1 id="attribution-prediction" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#attribution-prediction" color="auto.gray.8" aria-label="Attribution Prediction permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Attribution Prediction</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PANDA: Pose Aligned Networks for Deep Attribute Modeling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook. CVPR 2014</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1311.5591" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1311.5591</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebook/pose-aligned-deep-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebook/pose-aligned-deep-networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Predicting psychological attributions from face photographs with a deep neural network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.01289" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.01289</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Human Identity from Motion Patterns</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.03908" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.03908</a></li></ul><h1 id="place-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#place-recognition" color="auto.gray.8" aria-label="Place Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Place Recognition</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NetVLAD: CNN architecture for weakly supervised place recognition</strong></p><img src="http://www.di.ens.fr/willow/research/netvlad/images/teaser.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>intro: Google Street View Time Machine, soft-assignment, Weakly supervised triplet ranking loss</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.di.ens.fr/willow/research/netvlad/" class="Link-sc-1brdqhf-0 cKRjba">http://www.di.ens.fr/willow/research/netvlad/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07247" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07247</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PlaNet - Photo Geolocation with Convolutional Neural Networks</strong></p><img src="https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=590&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.05314" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.05314</a></li><li>review(&quot;Google Unveils Neural Network with “Superhuman” Ability to Determine the Location of Almost Any Image&quot;): <a target="_blank" rel="noopener noreferrer" href="https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/" class="Link-sc-1brdqhf-0 cKRjba">https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/</a></li><li>github(&quot;City-Recognition: CS231n Project for Winter 2016&quot;): <a target="_blank" rel="noopener noreferrer" href="https://github.com/dmakian/LittlePlaNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dmakian/LittlePlaNet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wulfebw/LittlePlaNet-Models" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wulfebw/LittlePlaNet-Models</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual place recognition using landmark distribution descriptors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04274" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04274</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Low-effort place recognition with WiFi fingerprints using deep learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.02049" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.02049</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aqibsaeed/Place-Recognition-using-Autoencoders-and-NN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aqibsaeed/Place-Recognition-using-Autoencoders-and-NN</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Features at Scale for Visual Place Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICRA 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.05105" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.05105</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Place recognition: An Overview of Vision Perspective</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.03470" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.03470</a></p><h2 id="camera-relocalization" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#camera-relocalization" color="auto.gray.8" aria-label="Camera Relocalization permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Camera Relocalization</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.07427" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.07427</a></li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://mi.eng.cam.ac.uk/projects/relocalisation/#results" class="Link-sc-1brdqhf-0 cKRjba">http://mi.eng.cam.ac.uk/projects/relocalisation/#results</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/alexgkendall/caffe-posenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/alexgkendall/caffe-posenet</a></li><li>github(TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/kentsommer/tensorflow-posenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kentsommer/tensorflow-posenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Modelling Uncertainty in Deep Learning for Camera Relocalization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.05909" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.05909</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Random Forests versus Neural Networks - What&#x27;s Best for Camera Relocalization?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.05797" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.05797</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Convolutional Neural Network for 6-DOF Image Localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.02776" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.02776</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DSAC - Differentiable RANSAC for Camera Localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05705" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05705</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image-based Localization with Spatial LSTMs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07890" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07890</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VidLoc: 6-DoF Video-Clip Relocalization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.06521" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.06521</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards CNN Map Compression for camera relocalisation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://www.arxiv.org/abs/1703.00845" class="Link-sc-1brdqhf-0 cKRjba">https://www.arxiv.org/abs/1703.00845</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Aalto University &amp; Indian Institute of Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09733" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09733</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MapNet: Geometry-Aware Learning of Maps for Camera Localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Georgia Institute of Technology &amp; NVIDIA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.03342" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.03342</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image-to-GPS Verification Through A Bottom-Up Pattern Matching Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07288" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07288</a></p><h1 id="activity-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#activity-recognition" color="auto.gray.8" aria-label="Activity Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Activity Recognition</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Implementing a CNN for Human Activity Recognition in Tensorflow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/" class="Link-sc-1brdqhf-0 cKRjba">http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Concurrent Activity Recognition with Multimodal CNN-LSTM Structure</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.01638" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.01638</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CERN: Confidence-Energy Recurrent Network for Group Activity Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03058" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03058</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deploying Tensorflow model on Andorid device for Human Activity Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://aqibsaeed.github.io/2017-05-02-deploying-tensorflow-model-andorid-device-human-activity-recognition/" class="Link-sc-1brdqhf-0 cKRjba">http://aqibsaeed.github.io/2017-05-02-deploying-tensorflow-model-andorid-device-human-activity-recognition/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/tree/master/ActivityRecognition" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/tree/master/ActivityRecognition</a></li></ul><h1 id="music-classification--sound-classification" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#music-classification--sound-classification" color="auto.gray.8" aria-label="Music Classification / Sound Classification permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Music Classification / Sound Classification</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Explaining Deep Convolutional Neural Networks on Music Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.02444" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.02444</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://keunwoochoi.wordpress.com/2015/12/09/ismir-2015-lbd-auralisation-of-deep-convolutional-neural-networks-listening-to-learned-features-auralization/" class="Link-sc-1brdqhf-0 cKRjba">https://keunwoochoi.wordpress.com/2015/12/09/ismir-2015-lbd-auralisation-of-deep-convolutional-neural-networks-listening-to-learned-features-auralization/</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://keunwoochoi.wordpress.com/2016/03/23/what-cnns-see-when-cnns-see-spectrograms/" class="Link-sc-1brdqhf-0 cKRjba">https://keunwoochoi.wordpress.com/2016/03/23/what-cnns-see-when-cnns-see-spectrograms/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/keunwoochoi/Auralisation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/keunwoochoi/Auralisation</a></li><li>audio samples: <a target="_blank" rel="noopener noreferrer" href="https://soundcloud.com/kchoi-research" class="Link-sc-1brdqhf-0 cKRjba">https://soundcloud.com/kchoi-research</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04363" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04363</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Recurrent Neural Networks for Music Classification</strong></p><img src="https://keunwoochoi.files.wordpress.com/2016/09/screen-shot-2016-09-14-at-20-38-27.png?w=1200" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.04243" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.04243</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://keunwoochoi.wordpress.com/2016/09/15/paper-is-out-convolutional-recurrent-neural-networks-for-music-classification/" class="Link-sc-1brdqhf-0 cKRjba">https://keunwoochoi.wordpress.com/2016/09/15/paper-is-out-convolutional-recurrent-neural-networks-for-music-classification/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/keunwoochoi/music-auto_tagging-keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/keunwoochoi/music-auto_tagging-keras</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNN Architectures for Large-Scale Audio Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1609.09430" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1609.09430</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=oAAo_r7ZT8U&amp;feature=youtu.be" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=oAAo_r7ZT8U&amp;feature=youtu.be</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SoundNet: Learning Sound Representations from Unlabeled Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT. NIPS 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://projects.csail.mit.edu/soundnet/" class="Link-sc-1brdqhf-0 cKRjba">http://projects.csail.mit.edu/soundnet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.09001" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.09001</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://web.mit.edu/vondrick/soundnet.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.mit.edu/vondrick/soundnet.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cvondrick/soundnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cvondrick/soundnet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/eborboihuc/SoundNet-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/eborboihuc/SoundNet-tensorflow</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=yJCjVvIY4dU" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=yJCjVvIY4dU</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning &#x27;ahem&#x27; detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/worldofpiggy/deeplearning-ahem-detector" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/worldofpiggy/deeplearning-ahem-detector</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/presentation/d/1QXQEOiAMj0uF2_Gafr2bn-kMniUJAIM1PLTFm1mUops/edit#slide=id.g35f391192_00" class="Link-sc-1brdqhf-0 cKRjba">https://docs.google.com/presentation/d/1QXQEOiAMj0uF2_Gafr2bn-kMniUJAIM1PLTFm1mUops/edit#slide=id.g35f391192_00</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1c2KGlwO" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1c2KGlwO</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GenreFromAudio: Finding the genre of a song with Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A pipeline to build a dataset from your own music library and use it to fill the missing genres</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/despoisj/DeepAudioClassification" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/despoisj/DeepAudioClassification</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10667" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10667</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>On the Robustness of Deep Convolutional Neural Networks for Music Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Queen Mary University of London &amp; New York University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.02361" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.02361</a></li></ul><h1 id="nsfw-detection--classification" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#nsfw-detection--classification" color="auto.gray.8" aria-label="NSFW Detection / Classification permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NSFW Detection / Classification</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Nipple Detection using Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/over18?dest=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F33n77s%2Fandroid_app_nipple_detection_using_convolutional%2F" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/over18?dest=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F33n77s%2Fandroid_app_nipple_detection_using_convolutional%2F</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Applying deep learning to classify pornographic images and videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.08899" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.08899</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MODERATE, FILTER, OR CURATE ADULT CONTENT WITH CLARIFAI’S NSFW MODEL</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://blog.clarifai.com/moderate-filter-or-curate-adult-content-with-clarifais-nsfw-model/#.VzVhM-yECZY" class="Link-sc-1brdqhf-0 cKRjba">http://blog.clarifai.com/moderate-filter-or-curate-adult-content-with-clarifais-nsfw-model/#.VzVhM-yECZY</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>WHAT CONVOLUTIONAL NEURAL NETWORKS LOOK AT WHEN THEY SEE NUDITY</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity#.VzVh_-yECZY" class="Link-sc-1brdqhf-0 cKRjba">http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity#.VzVh_-yECZY</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Open Sourcing a Deep Learning Solution for Detecting NSFW Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Yahoo</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for" class="Link-sc-1brdqhf-0 cKRjba">https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yahoo/open_nsfw" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yahoo/open_nsfw</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Miles Deep - AI Porn Video Editor</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Deep Learning Porn Video Classifier/Editor with Caffe</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ryanjay0/miles-deep" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ryanjay0/miles-deep</a></li></ul><h1 id="image-reconstruction--inpainting" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-reconstruction--inpainting" color="auto.gray.8" aria-label="Image Reconstruction / Inpainting permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Reconstruction / Inpainting</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context Encoders: Feature Learning by Inpainting</strong></p><img src="http://www.cs.berkeley.edu/~pathak/context_encoder/resources/result_fig.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>intro: Unsupervised Feature Learning by Image Inpainting using GANs</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.berkeley.edu/~pathak/context_encoder/" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.berkeley.edu/~pathak/context_encoder/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.07379" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.07379</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/pathak22/context-encoder" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pathak22/context-encoder</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/BoyuanJiang/context_encoder_pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/BoyuanJiang/context_encoder_pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Image Inpainting with Perceptual and Contextual Losses</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Image Inpainting with Deep Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Deep Convolutional Generative Adversarial Network (DCGAN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.07539" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.07539</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bamos/dcgan-completion.tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bamos/dcgan-completion.tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Southern California &amp; Adobe Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09969" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09969</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Image Reconstruction from Deep Templates</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.arxiv.org/abs/1703.00832" class="Link-sc-1brdqhf-0 cKRjba">https://www.arxiv.org/abs/1703.00832</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning-Guided Image Reconstruction from Incomplete Data</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.00584" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.00584</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Inpainting using Multi-Scale Feature Image Translation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08590" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08590</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Inpainting for High-Resolution Textures using CNN Texture Synthesis</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.03111" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.03111</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context-Aware Semantic Inpainting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.07778" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.07778</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Blind Image Inpainting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.09078" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.09078</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Stacked Networks with Residual Polishing for Image Inpainting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00289" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00289</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Light-weight pixel context encoders for image inpainting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.05585" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.05585</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Structured Energy-Based Image Inpainting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.07939" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.07939</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Shift-Net: Image Inpainting via Deep Feature Rearrangement</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.09392" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.09392</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascade context encoder for improved inpainting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.04033" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.04033</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SPG-Net: Segmentation Prediction and Guidance Network for Image Inpainting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Southern California &amp; Baidu Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.03356" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.03356</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Free-Form Image Inpainting with Gated Convolution</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.03589" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.03589</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Keras implementation of Image OutPainting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford CS230 project</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://cs230.stanford.edu/projects_spring_2018/posters/8265861.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://cs230.stanford.edu/projects_spring_2018/posters/8265861.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bendangnuksung/Image-OutPainting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bendangnuksung/Image-OutPainting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Inpainting via Generative Multi-column Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.08771" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.08771</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Inception Generative Network for Cognitive Image Inpainting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.01458" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.01458</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Foreground-aware Image Inpainting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Rochester &amp; University of Illinois at Urbana-Champaign &amp; Adobe Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.05945" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.05945</a></li></ul><h1 id="image-restoration" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-restoration" color="auto.gray.8" aria-label="Image Restoration permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Restoration</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.09056" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.09056</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1606.08921" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1606.08921</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Completion with Deep Learning in TensorFlow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://bamos.github.io/2016/08/09/deep-completion/" class="Link-sc-1brdqhf-0 cKRjba">http://bamos.github.io/2016/08/09/deep-completion/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deeply Aggregated Alternating Minimization for Image Restoration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.06508" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.06508</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Seoul National University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.06190" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.06190</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MemNet: A Persistent Memory Network for Image Restoration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017 (Spotlight presentation)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02209" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02209</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tyshiwo/MemNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tyshiwo/MemNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Mean-Shift Priors for Image Restoration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.03749" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.03749</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>xUnit: Learning a Spatial Activation Function for Efficient Image Restoration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06445" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06445</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kligvasser/xUnit" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kligvasser/xUnit</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Image Prior</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Skolkovo Institute of Science and Technology &amp; University of Oxford</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://dmitryulyanov.github.io/deep_image_prior" class="Link-sc-1brdqhf-0 cKRjba">https://dmitryulyanov.github.io/deep_image_prior</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10925" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10925</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://sites.skoltech.ru/app/data/uploads/sites/25/2017/11/deep_image_prior.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://sites.skoltech.ru/app/data/uploads/sites/25/2017/11/deep_image_prior.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//DmitryUlyanov/deep-image-prior" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//DmitryUlyanov/deep-image-prior</a></li><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/7gls3j/r_deep_image_prior_deep_superresolution/" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/7gls3j/r_deep_image_prior_deep_superresolution/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MemNet: A Persistent Memory Network for Image Restoration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017 spotlight</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//tyshiwo/MemNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//tyshiwo/MemNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Denoising Prior Driven Deep Neural Network for Image Restoration</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06756" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06756</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Globally and Locally Consistent Image Completion</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SIGGRAPH 2017</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/" class="Link-sc-1brdqhf-0 cKRjba">http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/satoshiiizuka/siggraph2017_inpainting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/satoshiiizuka/siggraph2017_inpainting</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/akmtn/pytorch-siggraph2017-inpainting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/akmtn/pytorch-siggraph2017-inpainting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-level Wavelet-CNN for Image Restoration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 NTIRE Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07071" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07071</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Non-Local Recurrent Network for Image Restoration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Illinois at Urbana-Champaign &amp; The Chinese University of Hong Kong</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.02919" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.02919</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Residual Non-local Attention Networks for Image Restoration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.10082" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.10082</a></li></ul><h2 id="face-completion" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#face-completion" color="auto.gray.8" aria-label="Face Completion permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Face Completion</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generative Face Completion</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.05838" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.05838</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>High Resolution Face Completion with Multiple Controllable Attributes via Fully End-to-End Progressive Generative Adversarial Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: North Carolina State University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.07632" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.07632</a></li></ul><h1 id="image-denoising" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-denoising" color="auto.gray.8" aria-label="Image Denoising permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Denoising</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03981" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03981</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cszn/DnCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cszn/DnCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Medical image denoising using convolutional denoising autoencoders</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04667" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04667</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rectifier Neural Network with a Dual-Pathway Architecture for Image Denoising</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.03024" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.03024</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Non-Local Color Image Denoising with Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06757" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06757</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Visual Denoising and Classification using Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01075" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01075</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ganggit/jointmodel" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ganggit/jointmodel</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Convolutional Denoising of Low-Light Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.01687" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.01687</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Class Aware Denoising</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.01698" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.01698</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Learning for Structured Prediction Energy Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Massachusetts &amp; CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.05667" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.05667</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Block-Matching Convolutional Neural Network for Image Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.00524" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.00524</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.04284" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.04284</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Wide Inference Network for Image Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="hmttps://arxiv.org/abs/1707.05414" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.05414</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Pixel-Distribution Prior with Wider Convolution for Image Denoising</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09135" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09135</a></li><li>github(MatConvNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/cswin/WIN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cswin/WIN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Denoising via CNNs: An Adversarial Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Indian Institute of Science</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.00159" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.00159</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An ELU Network with Total Variation for Image Denoising</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 24th International Conference on Neural Information Processing (2017)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.04317" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.04317</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dilated Residual Network for Image Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.05473" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.05473</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.04026" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.04026</a></li><li>github(MatConvNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/cszn/FFDNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cszn/FFDNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Universal Denoising Networks : A Novel CNN-based Network Architecture for Image Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07807" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07807</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Burst Denoising with Kernel Prediction Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://people.eecs.berkeley.edu/~bmild/kpn/" class="Link-sc-1brdqhf-0 cKRjba">http://people.eecs.berkeley.edu/~bmild/kpn/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02327" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02327</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Chaining Identity Mapping Modules for Image Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02933" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02933</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Burst Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.05790" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.05790</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast, Trainable, Multiscale Denoising</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.06130" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.06130</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Deep Learning based Denoisers without Ground Truth Data</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.01314" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.01314</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.05229" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.05229</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Class-Aware Fully-Convolutional Gaussian and Poisson Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.06562" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.06562</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Connecting Image Denoising and High-Level Vision Tasks via Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.01826" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.01826</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Ding-Liu/DeepDenoising" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Ding-Liu/DeepDenoising</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DN-ResNet: Efficient Deep Residual Network for Image Denoising</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.06766" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.06766</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Image Denoising: A Survey</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.05052" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.05052</a></p><h1 id="image-dehazing--image-haze-removal" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-dehazing--image-haze-removal" color="auto.gray.8" aria-label="Image Dehazing / Image Haze Removal permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Dehazing / Image Haze Removal</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DehazeNet: An End-to-End System for Single Image Haze Removal</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.07661" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.07661</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An All-in-One Network for Dehazing and Beyond</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: All-in-One Dehazing Network (AOD-Net)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06543" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06543</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Transmission Map Estimation and Dehazing using Deep Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.00581" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.00581</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End United Video Dehazing and Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.03919" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.03919</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Dehazing using Bilinear Composition Loss Function</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.00279" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.00279</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06787" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06787</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CANDY: Conditional Adversarial Networks based Fully End-to-End System for Single Image Haze Removal</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02892" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02892</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>C2MSNet: A Novel approach for single image haze removal</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.08406" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.08406</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Cascaded Convolutional Neural Network for Single Image Dehazing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE ACCESS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.07955" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.07955</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Densely Connected Pyramid Dehazing Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08396" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08396</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hezhangsprinter/DCPDN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hezhangsprinter/DCPDN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gated Fusion Network for Single Image Dehazing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/site/renwenqi888/research/dehazing/gfn" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/site/renwenqi888/research/dehazing/gfn</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00213" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00213</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Single-Image Dehazing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.05624" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.05624</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Perceptually Optimized Generative Adversarial Network for Single Image Dehazing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.01084" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.01084</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PAD-Net: A Perception-Aided Single Image Dehazing Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.03146" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.03146</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/guanlongzhao/single-image-dehazing" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guanlongzhao/single-image-dehazing</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Effectiveness of Instance Normalization: a Strong Baseline for Single Image Dehazing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.03305" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.03305</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPRW: NTIRE 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.05308" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.05308</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep learning for dehazing: Comparison and analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVCS 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.10923" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.10923</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generic Model-Agnostic Convolutional Neural Network for Single Image Dehazing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.02862" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.02862</a></p><h1 id="image-rain-removal--de-raining" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-rain-removal--de-raining" color="auto.gray.8" aria-label="Image Rain Removal / De-raining permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Rain Removal / De-raining</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Clearing the Skies: A deep network architecture for single-image rain removal</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DerainNet</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://smartdsp.xmu.edu.cn/derainNet.html" class="Link-sc-1brdqhf-0 cKRjba">http://smartdsp.xmu.edu.cn/derainNet.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.02087" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.02087</a></li><li>code(Matlab): <a target="_blank" rel="noopener noreferrer" href="http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/derainNet/code.zip" class="Link-sc-1brdqhf-0 cKRjba">http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/derainNet/code.zip</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Rain Detection and Removal via Iterative Region Dependent Multi-Task Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.07769" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.07769</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image De-raining Using a Conditional Generative Adversarial Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.05957" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.05957</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.06830" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.06830</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep joint rain and haze removal from single images</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06769" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06769</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Density-aware Single Image De-raining using a Multi-stream Dense Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08396" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08396</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hezhangsprinter/DID-MDN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hezhangsprinter/DID-MDN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10433" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10433</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Single Image Rain Removal via a Deep Decomposition-Composition Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.02688" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.02688</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Residual-Guide Feature Fusion Network for Single Image Deraining</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.07493" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.07493</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lightweight Pyramid Networks for Image Deraining</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06173" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06173</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.05698" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.05698</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://xialipku.github.io/RESCAN/" class="Link-sc-1brdqhf-0 cKRjba">https://xialipku.github.io/RESCAN/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.01491" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.01491</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gated Context Aggregation Network for Image Dehazing and Deraining</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08747" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08747</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Deep Tree-Structured Fusion Model for Single Image Deraining</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08632" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08632</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A^2Net: Adjacent Aggregation Networks for Image Raindrop Removal</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.09780" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.09780</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single Image Deraining: A Comprehensive Benchmark Analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.08558" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.08558</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lsy17096535/Single-Image-Deraining" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lsy17096535/Single-Image-Deraining</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>project pge: <a target="_blank" rel="noopener noreferrer" href="https://stevewongv.github.io/derain-project.html" class="Link-sc-1brdqhf-0 cKRjba">https://stevewongv.github.io/derain-project.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01538" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01538</a></li></ul><h1 id="fence-removal" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#fence-removal" color="auto.gray.8" aria-label="Fence Removal permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fence Removal</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>My camera can see through fences: A deep learning approach for image de-fencing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACPR 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07442" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07442</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep learning based fence segmentation and removal from an image using a video sequence</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV Workshop on Video Segmentation, 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.07727" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.07727</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accurate and efficient video de-fencing using convolutional neural networks and temporal information</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.10781" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.10781</a></p><h1 id="snow-removal" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#snow-removal" color="auto.gray.8" aria-label="Snow Removal permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Snow Removal</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DesnowNet: Context-Aware Deep Network for Snow Removal</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.04512" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.04512</a></p><h1 id="blur-detection-and-removal" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#blur-detection-and-removal" color="auto.gray.8" aria-label="Blur Detection and Removal permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blur Detection and Removal</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Deblur</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1406.7444" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1406.7444</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1503.00593" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1503.00593</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Learning for Image Burst Deblurring</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.04433" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.04433</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Video Deblurring</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 spotlight paper</li><li>project page(code+dataset): <a target="_blank" rel="noopener noreferrer" href="http://www.cs.ubc.ca/labs/imager/tr/2017/DeepVideoDeblurring/" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.ubc.ca/labs/imager/tr/2017/DeepVideoDeblurring/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08387" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08387</a><a target="_blank" rel="noopener noreferrer" href="https://github.com/shuochsu/DeepVideoDeblurring" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shuochsu/DeepVideoDeblurring</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02177" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02177</a></li><li>github(official. Torch)): <a target="_blank" rel="noopener noreferrer" href="https://github.com/SeungjunNah/DeepDeblur_release" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SeungjunNah/DeepDeblur_release</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02583" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02583</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Motion Deblurring in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.01486" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.01486</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Face Deblurring</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.08772" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.08772</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Blind Motion Deblurring</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.04208" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.04208</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Generative Filter for Motion Deblurring</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.03481" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.03481</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Ukrainian Catholic University &amp; CTU in Prague</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07064" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07064</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KupynOrest/DeblurGAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KupynOrest/DeblurGAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepDeblur: Fast one-step blurry face images restoration</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09515" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09515</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reblur2Deblur: Deblurring Videos via Self-Supervised Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.05117" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.05117</a></li><li>supplementary: <a target="_blank" rel="noopener noreferrer" href="https://drive.google.com/file/d/17Itta-z89lpWUdvUjpafKzJRSLoxHF5c/view" class="Link-sc-1brdqhf-0 cKRjba">https://drive.google.com/file/d/17Itta-z89lpWUdvUjpafKzJRSLoxHF5c/view</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale-recurrent Network for Deep Image Deblurring</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CUHK &amp; Tecent &amp; Megvii Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.01770" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.01770</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Semantic Face Deblurring</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. Beijing Institute of Technology &amp; University of California, Merced &amp; Nvidia Research</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.03345" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.03345</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Motion deblurring of faces</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.03330" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.03330</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Discriminative Prior for Blind Image Deblurring</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.03363" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.03363</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Spatio-Temporal Learning for Video Deblurring</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00533" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00533</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Deblur Images with Exemplars</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PAMI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.05503" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.05503</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networks for Non-Uniform Single Image Deblurring</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.10157" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.10157</a></p><h1 id="image-compression" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-compression" color="auto.gray.8" aria-label="Image Compression permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Compression</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An image compression and encryption scheme based on deep learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.05001" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.05001</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Full Resolution Image Compression with Recurrent Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.05148" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.05148</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/compression" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/compression</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Compression with Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lossy Image Compression With Compressive Autoencoders</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openreview.net/pdf?id=rJiNwv9gg" class="Link-sc-1brdqhf-0 cKRjba">http://openreview.net/pdf?id=rJiNwv9gg</a></li><li>review: <a target="_blank" rel="noopener noreferrer" href="http://qz.com/835569/twitter-is-getting-close-to-making-all-your-pictures-just-a-little-bit-smaller/" class="Link-sc-1brdqhf-0 cKRjba">http://qz.com/835569/twitter-is-getting-close-to-making-all-your-pictures-just-a-little-bit-smaller/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end Optimized Image Compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01704" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01704</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://blog.acolyer.org/2017/05/08/end-to-end-optimized-image-compression/" class="Link-sc-1brdqhf-0 cKRjba">https://blog.acolyer.org/2017/05/08/end-to-end-optimized-image-compression/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07233" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07233</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Perceptual Image Compression using Deep Convolution Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Accepted to Data Compression Conference</li><li>intro: Semantic JPEG image compression using deep convolutional neural network (CNN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.08712" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.08712</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iamaaditya/image-compression-cnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iamaaditya/image-compression-cnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generative Compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.01467" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.01467</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10114" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10114</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Convolutional Networks for Content-weighted Image Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10553" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10553</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-Time Adaptive Image Compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2017</li><li>keywords: GAN</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.wave.one/icml2017" class="Link-sc-1brdqhf-0 cKRjba">http://www.wave.one/icml2017</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.05823" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.05823</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Inpaint for Image Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.08855" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.08855</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Trimmed Convolutional Arithmetic Encoding for Lossless Image Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.04662" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.04662</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Conditional Probability Models for Deep Image Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.04260" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.04260</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multiple Description Convolutional Neural Networks for Image Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06611" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06611</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Near-lossless L-infinity constrained Multi-rate Image Decompression via Deep Neural Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.07987" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.07987</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepSIC: Deep Semantic Image Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.09468" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.09468</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatially adaptive image compression using a tiled deep network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02629" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02629</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05787" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05787</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression Framework</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DAC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05788" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05788</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Effects of JPEG and JPEG2000 Compression on Attacks using Adversarial Examples</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10418" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10418</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generative Adversarial Networks for Extreme Learned Image Compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ETH Zurich</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://data.vision.ee.ethz.ch/aeirikur/extremecompression/" class="Link-sc-1brdqhf-0 cKRjba">https://data.vision.ee.ethz.ch/aeirikur/extremecompression/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.02958" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.02958</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deformation Aware Image Compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.04593" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.04593</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Multi-scale Image Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06386" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06386</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Image Compression via End-to-End Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.01496" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.01496</a></p><h1 id="image-quality-assessment" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-quality-assessment" color="auto.gray.8" aria-label="Image Quality Assessment permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Quality Assessment</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01697" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01697</a></li></ul><h1 id="image-blending" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-blending" color="auto.gray.8" aria-label="Image Blending permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Blending</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GP-GAN: Towards Realistic High-Resolution Image Blending</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://wuhuikai.github.io/GP-GAN-Project/" class="Link-sc-1brdqhf-0 cKRjba">https://wuhuikai.github.io/GP-GAN-Project/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07195" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07195</a></li><li>github(Official, Chainer): <a target="_blank" rel="noopener noreferrer" href="https://github.com/wuhuikai/GP-GAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wuhuikai/GP-GAN</a></li></ul><h1 id="image-enhancement" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-enhancement" color="auto.gray.8" aria-label="Image Enhancement permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Enhancement</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Bilateral Learning for Real-Time Image Enhancement</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.02880" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.02880</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Aesthetic-Driven Image Enhancement by Adversarial Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CUHK</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.05251" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.05251</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learned Perceptual Image Enhancement</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02864" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02864</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Underwater Image Enhancement</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.03528" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.03528</a></p><h1 id="abnormality-detection--anomaly-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#abnormality-detection--anomaly-detection" color="auto.gray.8" aria-label="Abnormality Detection / Anomaly Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Abnormality Detection / Anomaly Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Toward a Taxonomy and Computational Models of Abnormalities in Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.01325" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.01325</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06725" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06725</a></p><h1 id="depth-prediction--depth-estimation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#depth-prediction--depth-estimation" color="auto.gray.8" aria-label="Depth Prediction / Depth Estimation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Depth Prediction / Depth Estimation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Convolutional Neural Fields for Depth Estimation from a Single Image</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1411.6387" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1411.6387</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE T. Pattern Analysis and Machine Intelligence</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1502.07411" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1502.07411</a></li><li>bitbucket: <a target="_blank" rel="noopener noreferrer" href="https://bitbucket.org/fayao/dcnf-fcsp" class="Link-sc-1brdqhf-0 cKRjba">https://bitbucket.org/fayao/dcnf-fcsp</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1603.049921" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1603.04992</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Ravi-Garg/Unsupervised_Depth_Estimation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Ravi-Garg/Unsupervised_Depth_Estimation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>project pag: <a target="_blank" rel="noopener noreferrer" href="http://ttic.uchicago.edu/~ayanc/mdepth/" class="Link-sc-1brdqhf-0 cKRjba">http://ttic.uchicago.edu/~ayanc/mdepth/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.07081" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.07081</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ayanc/mdepth/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ayanc/mdepth/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deeper Depth Prediction with Fully Convolutional Residual Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.00373" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.00373</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/iro-cp/FCRN-DepthPrediction" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/iro-cp/FCRN-DepthPrediction</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single image depth estimation by dilated deep residual convolutional neural network and soft-weight-sum inference</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.00534" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.00534</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Monocular Depth Estimation with Hierarchical Fusion of Dilated CNNs and Soft-Weighted-Sum Inference</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Northwestern Polytechnical University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02287" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02287</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.07492" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.07492</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=vNIIT_M7x7Y" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=vNIIT_M7x7Y</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fangchangma/sparse-to-dense" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fangchangma/sparse-to-dense</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Size-to-depth: A New Perspective for Single Image Depth Estimation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.04461" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.04461</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.00150" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.00150</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Monocular Depth Estimation with Adversarial Training</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.07528" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.07528</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://webdiis.unizar.es/~jmfacil/camconvs/" class="Link-sc-1brdqhf-0 cKRjba">http://webdiis.unizar.es/~jmfacil/camconvs/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02028" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02028</a></li></ul><h1 id="texture-synthesis" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#texture-synthesis" color="auto.gray.8" aria-label="Texture Synthesis permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Texture Synthesis</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Texture Synthesis Using Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.07376" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.07376</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Texture Networks: Feed-forward Synthesis of Textures and Stylized Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IMCL 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.03417" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.03417</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DmitryUlyanov/texture_nets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DmitryUlyanov/texture_nets</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/" class="Link-sc-1brdqhf-0 cKRjba">https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.04382" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.04382</a></li><li>github(Torch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/chuanli11/MGANs" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chuanli11/MGANs</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Texture Synthesis with Spatial Generative Adversarial Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08207" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08207</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Skolkovo Institute of Science and Technology &amp; Yandex &amp; University of Oxford</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.02096" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.02096</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep TEN: Texture Encoding Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://zhanghang1989.github.io/DeepEncoding/" class="Link-sc-1brdqhf-0 cKRjba">http://zhanghang1989.github.io/DeepEncoding/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02844" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02844</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhanghang1989/Deep-Encoding" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhanghang1989/Deep-Encoding</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/25013378" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/25013378</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diversified Texture Synthesis with Feed-forward Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. University of California &amp; Adobe Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.01664" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.01664</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Yijunmaverick/MultiTextureSynthesis" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Yijunmaverick/MultiTextureSynthesis</a></li></ul><h1 id="image-cropping" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-cropping" color="auto.gray.8" aria-label="Image Cropping permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Cropping</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Cropping via Attention Box Prediction and Aesthetics Assessment</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.08014" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.08014</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A2-RL: Aesthetics Aware Reinforcement Learning for Automatic Image Cropping</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://debangli.info/A2RL/" class="Link-sc-1brdqhf-0 cKRjba">http://debangli.info/A2RL/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.04595" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.04595</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/wuhuikai/TF-A2RL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wuhuikai/TF-A2RL</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://wuhuikai.me/TF-A2RL/" class="Link-sc-1brdqhf-0 cKRjba">http://wuhuikai.me/TF-A2RL/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Automatic Image Cropping for Visual Aesthetic Enhancement Using Deep Neural Networks and Cascaded Regression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Multimedia, 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.09048" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.09048</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Grid Anchor based Image Cropping: A New Benchmark and An Efficient Model</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.08989" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.08989</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Cropping with Composition and Saliency Aware Aesthetic Score Map</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.10492" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.10492</a></li></ul><h1 id="image-synthesis" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-synthesis" color="auto.gray.8" aria-label="Image Synthesis permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Synthesis</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.04589" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.04589</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generative Adversarial Text to Image Synthesis</strong></p><img src="https://camo.githubusercontent.com/1925e23b5b6e19efa60f45daa3787f1f4a098ef3/687474703a2f2f692e696d6775722e636f6d2f644e6c32486b5a2e6a7067" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.05396" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.05396</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/paarthneekhara/text-to-image" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/paarthneekhara/text-to-image</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Rutgers University &amp; Lehigh University &amp; The Chinese University of Hong Kong &amp; University of North Carolina at Charlotte</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03242" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03242</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hanzhanggit/StackGAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hanzhanggit/StackGAN</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/brangerbriz/docker-StackGAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/brangerbriz/docker-StackGAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Image Synthesis via Adversarial Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06873" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06873</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com//woozzu/dong_iccv_2017" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//woozzu/dong_iccv_2017</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Introduction to Image Synthesis with Generative Adversarial Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Illinois at Chicago &amp; Toutiao AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.04469" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.04469</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Text Guided Person Image Synthesis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intr: CVPR 2019</li><li>intro: Zhejiang University &amp; Nanjing University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05118" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05118</a></li></ul><h1 id="image-tagging" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-tagging" color="auto.gray.8" aria-label="Image Tagging permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Tagging</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Zero-Shot Image Tagging</strong></p><img src="http://crcv.ucf.edu/projects/fastzeroshot/overview.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project: <a target="_blank" rel="noopener noreferrer" href="http://crcv.ucf.edu/projects/fastzeroshot/" class="Link-sc-1brdqhf-0 cKRjba">http://crcv.ucf.edu/projects/fastzeroshot/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Flexible Image Tagging with Fast0Tag</strong></p><img src="https://cdn-images-1.medium.com/max/800/1*SsIf1Bhe-G4HmN6DPDogmQ.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7" class="Link-sc-1brdqhf-0 cKRjba">https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sampled Image Tagging and Retrieval Methods on User Generated Content</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06962" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06962</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lab41/attalos" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lab41/attalos</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Kill Two Birds with One Stone: Weakly-Supervised Neural Network for Image Annotation and Tag Refinement</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06998" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06998</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Multiple Instance Learning for Zero-shot Image Tagging</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06051" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06051</a></p><h1 id="image-matching" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-matching" color="auto.gray.8" aria-label="Image Matching permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Matching</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Fine-grained Image Similarity with Deep Ranking</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2014</li><li>intro: Triplet Sampling</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1404.4661" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1404.4661</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to compare image patches via convolutional neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015. siamese network</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://imagine.enpc.fr/~zagoruys/deepcompare.html" class="Link-sc-1brdqhf-0 cKRjba">http://imagine.enpc.fr/~zagoruys/deepcompare.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/szagoruyko/cvpr15deepcompare" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/szagoruyko/cvpr15deepcompare</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015. siamese network</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.unc.edu/~xufeng/cs/papers/cvpr15-matchnet.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.unc.edu/~xufeng/cs/papers/cvpr15-matchnet.pdf</a></li><li>extended abstract: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_114_ext.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_114_ext.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hanxf/matchnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hanxf/matchnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fashion Style in 128 Floats</strong></p><img src="http://hi.cs.waseda.ac.jp/~esimo/images/stylenet/fashionfeat.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016. StyleNet</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://hi.cs.waseda.ac.jp/~esimo/en/research/stylenet/" class="Link-sc-1brdqhf-0 cKRjba">http://hi.cs.waseda.ac.jp/~esimo/en/research/stylenet/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bobbens/cvpr2016_stylenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bobbens/cvpr2016_stylenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully-Trainable Deep Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://lear.inrialpes.fr/src/deepmatching/" class="Link-sc-1brdqhf-0 cKRjba">http://lear.inrialpes.fr/src/deepmatching/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.03532" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.03532</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Local Similarity-Aware Deep Feature Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.08904" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.08904</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional neural network architecture for geometric matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Inria</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.di.ens.fr/willow/research/cnngeometric/" class="Link-sc-1brdqhf-0 cKRjba">http://www.di.ens.fr/willow/research/cnngeometric/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.05593" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.05593</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ignacio-rocco/cnngeometric_matconvnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ignacio-rocco/cnngeometric_matconvnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Image Semantic Matching by Mining Consistent Features</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07641</a></p><h1 id="image-editing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#image-editing" color="auto.gray.8" aria-label="Image Editing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Image Editing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Photo Editing with Introspective Adversarial Networks</strong></p><img src="https://camo.githubusercontent.com/c66848752d9fa05c3194ae36d48b869ec9d21743/687474703a2f2f692e696d6775722e636f6d2f773155323045492e706e67" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Heriot-Watt University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.07093" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.07093</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ajbrock/Neural-Photo-Editor" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ajbrock/Neural-Photo-Editor</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Feature Interpolation for Image Content Changes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Cornell University &amp; Washington University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05507" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05507</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/paulu/deepfeatinterp" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/paulu/deepfeatinterp</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/slang03/dfi-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/slang03/dfi-tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Invertible Conditional GANs for image editing</strong></p><img src="https://raw.githubusercontent.com/Guim3/IcGAN/master/images/model_overview.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016 Workshop on Adversarial Training</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06355" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06355</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Guim3/IcGAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Guim3/IcGAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Facial Expression Editing using Autoencoded Flow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Illinois at Urbana-Champaign &amp; The Chinese University of Hong Kong &amp; Google</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09961" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09961</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Language-Based Image Editing with Recurrent Attentive Models</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06288" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06288</a></p><h2 id="face-swap--face-editing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#face-swap--face-editing" color="auto.gray.8" aria-label="Face Swap &amp; Face Editing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Face Swap &amp; Face Editing</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Face-swap Using Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Ghent University &amp; Twitter</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09577" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09577</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Face Editing with Intrinsic Image Disentangling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 oral</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html" class="Link-sc-1brdqhf-0 cKRjba">http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.04131" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.04131</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Arbitrary Facial Attribute Editing: Only Change What You Want</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10678" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10678</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/LynnHo/AttGAN-Tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/LynnHo/AttGAN-Tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RSGAN: Face Swapping and Editing using Face and Hair Representation in Latent Spaces</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.03447" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.03447</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FaceShop: Deep Sketch-based Face Image Editing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.08972" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.08972</a></p><h1 id="stereo" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#stereo" color="auto.gray.8" aria-label="Stereo permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Stereo</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Learning of Geometry and Context for Deep Stereo Regression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.04309" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.04309</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Adaptation for Deep Stereo</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://vision.disi.unibo.it/~mpoggi/papers/iccv2017_adaptation.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://vision.disi.unibo.it/~mpoggi/papers/iccv2017_adaptation.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.09204" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.09204</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>StereoConvNet: Stereo convolutional neural network for depth map prediction from stereo images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/LouisFoucard/StereoConvNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/LouisFoucard/StereoConvNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05196" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05196</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. SenseTime Research &amp; Sun Yat-sen University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06641</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pyramid Stereo Matching Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08669" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08669</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JiaRenChang/PSMNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JiaRenChang/PSMNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascaded multi-scale and multi-dimension convolutional neural network for stereo matching</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09437" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09437</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Left-Right Comparative Recurrent Model for Stereo Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00796" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00796</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.01677" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.01677</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Open-World Stereo Video Matching with Deep RNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.03959" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.03959</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-time self-adaptive deep stereo</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.05424" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.05424</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Group-wise Correlation Stereo Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.04025" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.04025</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/xy-guo/GwcNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xy-guo/GwcNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-calibrating Deep Photometric Stereo Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>intro: The University of Hong Kong &amp; University of Oxford &amp; Peking University &amp; Peng Cheng Laboratory &amp; Osaka University</li><li>keywords: Learning Based Uncalibrated Photometric Stereo for Non-Lambertian Surface</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://gychen.org/SDPS-Net/" class="Link-sc-1brdqhf-0 cKRjba">http://gychen.org/SDPS-Net/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.07366" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.07366</a></li><li>github(official, PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/guanyingc/SDPS-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guanyingc/SDPS-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Adapt for Stereo</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: University of Bologna &amp; University of Oxford &amp; Australian National University &amp; FiveAI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02957" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02957</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CVLAB-Unibo/Learning2AdaptForStereo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CVLAB-Unibo/Learning2AdaptForStereo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>StereoDRNet: Dilated Residual Stereo Net</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: University of North Carolina at Chapel Hill &amp; Facebook Reality Labs</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02251" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02251</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GA-Net: Guided Aggregation Net for End-to-end Stereo Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>intro: University of Oxford &amp; Baidu Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.06587" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.06587</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Scale Geometric Consistency Guided Multi-View Stereo</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.08103" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.08103</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Guided Stereo Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.10107" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.10107</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.06257" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.06257</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.05845" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.05845</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University</li><li>keywords: STereo TRansformer (STTR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.02910" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.02910</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mli0603/stereo-transformer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mli0603/stereo-transformer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tianjin University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.14055" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.14055</a></li></ul><h1 id="3d" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#3d" color="auto.gray.8" aria-label="3D permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>3D</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Spatiotemporal Features with 3D Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>C3D: Generic Features for Video Analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://vlg.cs.dartmouth.edu/c3d/" class="Link-sc-1brdqhf-0 cKRjba">http://vlg.cs.dartmouth.edu/c3d/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.0767" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.0767</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w07-conv3d.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w07-conv3d.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebook/C3D" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebook/C3D</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>C3D Model for Keras trained over Sports 1M</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://imatge.upc.edu/web/resources/c3d-model-keras-trained-over-sports-1m" class="Link-sc-1brdqhf-0 cKRjba">https://imatge.upc.edu/web/resources/c3d-model-keras-trained-over-sports-1m</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sports 1M C3D Network to Keras</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>notebook: <a target="_blank" rel="noopener noreferrer" href="http://nbviewer.jupyter.org/gist/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2/Sports1M%20C3D%20Network%20to%20Keras.ipynb" class="Link-sc-1brdqhf-0 cKRjba">http://nbviewer.jupyter.org/gist/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2/Sports1M%20C3D%20Network%20to%20Keras.ipynb</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep End2End Voxel2Voxel Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06681" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06681</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Aligning 3D Models to RGB-D Images of Cluttered Scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.berkeley.edu/~rbg/papers/cvpr15/align2rgbd.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.berkeley.edu/~rbg/papers/cvpr15/align2rgbd.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images</strong></p><img src="http://dss.cs.princeton.edu/teaser.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://dss.cs.princeton.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://dss.cs.princeton.edu/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.02300" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.02300</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-view 3D Models from Single Images with a Convolutional Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06702" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06702</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RotationNet: Learning Object Classification Using Unsupervised Viewpoint Estimation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.06208" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.06208</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding</strong></p><img src="http://deepcontext.cs.princeton.edu/teaser.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://deepcontext.cs.princeton.edu/paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://deepcontext.cs.princeton.edu/paper.pdf</a></li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://deepcontext.cs.princeton.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://deepcontext.cs.princeton.edu/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Volumetric and Multi-View CNNs for Object Classification on 3D Data</strong></p><img src="http://graphics.stanford.edu/projects/3dcnn/teaser.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://graphics.stanford.edu/projects/3dcnn/" class="Link-sc-1brdqhf-0 cKRjba">http://graphics.stanford.edu/projects/3dcnn/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.03265" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.03265</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/charlesq34/3dcnn.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/charlesq34/3dcnn.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep3D: Automatic 2D-to-3D Video Conversion with CNNs</strong></p><img src="https://raw.githubusercontent.com/piiswrong/deep3d/master/img/teaser.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html" class="Link-sc-1brdqhf-0 cKRjba">http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://homes.cs.washington.edu/~jxie/pdf/deep3d.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://homes.cs.washington.edu/~jxie/pdf/deep3d.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/piiswrong/deep3d" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/piiswrong/deep3d</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.03650" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.03650</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction</strong></p><img src="https://raw.githubusercontent.com/chrischoy/3D-R2N2/master/imgs/overview.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.00449" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.00449</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chrischoy/3D-R2N2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chrischoy/3D-R2N2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Body Meshes as Points</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: National University of Singapore &amp; ByteDance AI Lab &amp; Yitu Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.02467" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.02467</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jfzhang95/BMP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jfzhang95/BMP</a></li></ul><h1 id="deep-learning-for-makeup" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-for-makeup" color="auto.gray.8" aria-label="Deep Learning for Makeup permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning for Makeup</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Makeup like a superstar: Deep Localized Makeup Transfer Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.07102" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.07102</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Makeup-Go: Blind Reversion of Portrait Edit</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Chinese University of Hong Kong &amp; Tencent Youtu Lab</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://open.youtu.qq.com/content/file/iccv17_makeupgo.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://open.youtu.qq.com/content/file/iccv17_makeupgo.pdf</a></li></ul><h1 id="music-tagging" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#music-tagging" color="auto.gray.8" aria-label="Music Tagging permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Music Tagging</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Automatic tagging using deep convolutional neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.00298" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.00298</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/keunwoochoi/music-auto_tagging-keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/keunwoochoi/music-auto_tagging-keras</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Music tagging and feature extraction with MusicTaggerCRNN</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://keras.io/applications/#music-tagging-and-feature-extraction-with-musictaggercrnn" class="Link-sc-1brdqhf-0 cKRjba">https://keras.io/applications/#music-tagging-and-feature-extraction-with-musictaggercrnn</a></p><h1 id="action-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#action-recognition" color="auto.gray.8" aria-label="Action Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Action Recognition</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single Image Action Recognition by Predicting Space-Time Saliency</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.04641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.04641</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attentional Pooling for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2017</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://rohitgirdhar.github.io/AttentionalPoolingAction/" class="Link-sc-1brdqhf-0 cKRjba">https://rohitgirdhar.github.io/AttentionalPoolingAction/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.01467" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.01467</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rohitgirdhar/AttentionalPoolingAction/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rohitgirdhar/AttentionalPoolingAction/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Memory Attention Networks for Skeleton-based Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2018</li><li>keywords: Temporal Attention Recalibration Module (TARM) and a Spatio-Temporal Convolution Module (STCM)</li><li>arixv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.08254" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.08254</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/memory-attention-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/memory-attention-networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Analysis of CNN-based Spatio-temporal Representations for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT-IBM Watson AI Lab &amp; MIT</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.11757" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.11757</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/IBM/action-recognition-pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/IBM/action-recognition-pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690528.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690528.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kchengiva/DecoupleGCN-DropGraph" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kchengiva/DecoupleGCN-DropGraph</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal-Relational CrossTransformers for Few-Shot Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Bristol</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.06184" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.06184</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tobyperrett/trx" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tobyperrett/trx</a></li></ul><h1 id="ctr-prediction" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#ctr-prediction" color="auto.gray.8" aria-label="CTR Prediction permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CTR Prediction</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep CTR Prediction in Display Advertising</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia Conference 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1609.06018" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1609.06018</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Harbin Institute of Technology &amp; Huawei</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.04247" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.04247</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Interest Network for Click-Through Rate Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.06978" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.06978</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Matters: Jointly Train Advertising CTR Model with Image Representation of Ad and User Behavior</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06505" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06505</a></li></ul><h1 id="cryptography" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#cryptography" color="auto.gray.8" aria-label="Cryptography permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Cryptography</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Protect Communications with Adversarial Neural Cryptography</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Brain</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.06918" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.06918</a></li><li>github(Theano): <a target="_blank" rel="noopener noreferrer" href="https://github.com/nlml/adversarial-neural-crypt" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nlml/adversarial-neural-crypt</a></li><li>github(TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/ankeshanand/neural-cryptography-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ankeshanand/neural-cryptography-tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Neural Cryptography in Theano</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://nlml.github.io/neural-networks/adversarial-neural-cryptography/" class="Link-sc-1brdqhf-0 cKRjba">https://nlml.github.io/neural-networks/adversarial-neural-cryptography/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Embedding Watermarks into Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.04082" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.04082</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yu4u/dnn-watermark" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yu4u/dnn-watermark</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Digital Watermarking for Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: International Journal of Multimedia Information Retrieval</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02601" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02601</a></li></ul><h1 id="cyber-security" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#cyber-security" color="auto.gray.8" aria-label="Cyber Security permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Cyber Security</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Collection of Deep Learning Cyber Security Research Papers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042#.fcus2cu9m" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042#.fcus2cu9m</a></li></ul><h1 id="lip-reading" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#lip-reading" color="auto.gray.8" aria-label="Lip Reading permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Lip Reading</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LipNet: Sentence-level Lipreading</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LipNet: End-to-End Sentence-level Lipreading</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01599" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01599</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openreview.net/pdf?id=BkjLkSqxg" class="Link-sc-1brdqhf-0 cKRjba">http://openreview.net/pdf?id=BkjLkSqxg</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bshillingford/LipNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bshillingford/LipNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lip Reading Sentences in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Oxford &amp; Google DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05358" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05358</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=5aogzAUPilE" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=5aogzAUPilE</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Combining Residual Networks with LSTMs for Lipreading</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.04105" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.04105</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Multi-View Lipreading</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.00443" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.00443</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LCANet: End-to-End Lipreading with Cascaded Attention-CTC</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: FG 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.04988" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.04988</a></li></ul><h1 id="event-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#event-recognition" color="auto.gray.8" aria-label="Event Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Event Recognition</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Better Exploiting OS-CNNs for Better Event Recognition in Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.03979" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.03979</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.00162" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.00162</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IOD-CNN: Integrating Object Detection Networks for Event Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07431" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07431</a></p><h1 id="trajectory-prediction" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#trajectory-prediction" color="auto.gray.8" aria-label="Trajectory Prediction permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Trajectory Prediction</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Machine Learning for Autonomous Driving @ NeurIPS 2020</li><li>intro: Carnegie Mellon University &amp; Bosch Research Pittsburgh</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.14910" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.14910</a></li></ul><h1 id="human-object-interaction" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#human-object-interaction" color="auto.gray.8" aria-label="Human-Object Interaction permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Human-Object Interaction</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Human-Object Interactions by Graph Parsing Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.07962" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.07962</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SiyuanQi/gpnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SiyuanQi/gpnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Interact as You Intend: Intention-Driven Human-Object Interaction Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.09796" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.09796</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://gaochen315.github.io/iCAN/" class="Link-sc-1brdqhf-0 cKRjba">https://gaochen315.github.io/iCAN/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.10437" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.10437</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vt-vl-lab/iCAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vt-vl-lab/iCAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose-aware Multi-level Feature Network for Human Object Interaction Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.08453" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.08453</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Human Object Interaction Detection with HOI Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: MEGVII Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.04503" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.04503</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bbepoch/HoiTransformer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bbepoch/HoiTransformer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>intro: OPPO Research Institute &amp; Xmotors &amp; University of California</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.03278" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.03278</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hand-Object Contact Prediction via Motion-Based Pseudo-Labeling and Guided Progressive Label Correction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.10174" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.10174</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/takumayagi/hand_object_contact_prediction" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/takumayagi/hand_object_contact_prediction</a></li></ul><h1 id="deep-learning-in-finance" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-in-finance" color="auto.gray.8" aria-label="Deep Learning in Finance permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning in Finance</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning in Finance</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.06561" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.06561</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Survey of Deep Learning Techniques Applied to Trading</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://gregharris.info/a-survey-of-deep-learning-techniques-applied-to-trading/" class="Link-sc-1brdqhf-0 cKRjba">http://gregharris.info/a-survey-of-deep-learning-techniques-applied-to-trading/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning and Long-Term Investing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>part 1: <a target="_blank" rel="noopener noreferrer" href="http://www.euclidean.com/deep-learning-long-term-investing-1" class="Link-sc-1brdqhf-0 cKRjba">http://www.euclidean.com/deep-learning-long-term-investing-1</a></li><li>part 2: <a target="_blank" rel="noopener noreferrer" href="http://www.euclidean.com/deep-learning-investing-part-2-preprocessing-data" class="Link-sc-1brdqhf-0 cKRjba">http://www.euclidean.com/deep-learning-investing-part-2-preprocessing-data</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning in Trading</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=FoQKCeDuPiY" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=FoQKCeDuPiY</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1sltRra9" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1sltRra9</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Research to Products: Machine &amp; Human Intelligence in Finance</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peter Sarlin, Hanken School of Economics - Deep Learning in Finance Summit 2016 #reworkfin</li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=Fd7Cc-KOVXg" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=Fd7Cc-KOVXg</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1kVpZKur#list/path=%2F" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1kVpZKur#list/path=%2F</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>eep Neural Networks for Real-time Market Predictions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=Kzz2-wAEK7A" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=Kzz2-wAEK7A</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning the Stock Market</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.z752rf43u" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.z752rf43u</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/talolard/MarketVectors" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/talolard/MarketVectors</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>rl_portfolio</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: This Repository uses Reinforcement Learning and Supervised learning to Optimize portfolio allocation.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deependersingla/deep_portfolio" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deependersingla/deep_portfolio</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural networks for algorithmic trading. Multivariate time series</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Rachnog/Deep-Trading/tree/master/multivariate" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Rachnog/Deep-Trading/tree/master/multivariate</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep-Trading: Algorithmic trading with deep learning experiments</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Rachnog/Deep-Trading" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Rachnog/Deep-Trading</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural networks for algorithmic trading. Multimodal and multitask deep learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://becominghuman.ai/neural-networks-for-algorithmic-trading-multimodal-and-multitask-deep-learning-5498e0098caf" class="Link-sc-1brdqhf-0 cKRjba">https://becominghuman.ai/neural-networks-for-algorithmic-trading-multimodal-and-multitask-deep-learning-5498e0098caf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Rachnog/Deep-Trading/tree/master/multimodal" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Rachnog/Deep-Trading/tree/master/multimodal</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning with Python in Finance - Singapore Python User Group</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=xvm-M-R2fZY" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=xvm-M-R2fZY</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Xi’an Jiaotong-Liverpool University</li><li>keywords: PGPortfolio: Policy Gradient Portfolio</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.10059" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.10059</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//ZhengyaoJiang/PGPortfolio" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//ZhengyaoJiang/PGPortfolio</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stock Prediction: a method based on extraction of news features and recurrent neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peking University. The 22nd China Conference on Information Retrieval</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07585" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07585</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multidimensional LSTM Networks to Predict Bitcoin Price</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price" class="Link-sc-1brdqhf-0 cKRjba">http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jaungiers/Multidimensional-LSTM-BitCoin-Time-Series" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jaungiers/Multidimensional-LSTM-BitCoin-Time-Series</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Euclidean Technologies &amp; Amazon AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.04837" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.04837</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Findings from our Research on Applying Deep Learning to Long-Term Investing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.euclidean.com/paper-on-deep-learning-long-term-investing" class="Link-sc-1brdqhf-0 cKRjba">http://www.euclidean.com/paper-on-deep-learning-long-term-investing</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Predicting Cryptocurrency Prices With Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: This post brings together cryptos and deep learning in a desperate attempt for Reddit popularity</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Trading Agent</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Deep Reinforcement Learning based Trading Agent for Bitcoin</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://github.com/samre12/deep-trading-agent" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/samre12/deep-trading-agent</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Financial Trading as a Game: A Deep Reinforcement Learning Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: National Chiao Tung University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.02787" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.02787</a></li></ul><h1 id="deep-learning-in-speech" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-in-speech" color="auto.gray.8" aria-label="Deep Learning in Speech permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning in Speech</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Research, ICML 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1512.02595" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1512.02595</a></li><li>github(Neon): <a target="_blank" rel="noopener noreferrer" href="https://github.com/NervanaSystems/deepspeech" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NervanaSystems/deepspeech</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end speech recognition with neon</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://www.nervanasys.com/end-end-speech-recognition-neon/" class="Link-sc-1brdqhf-0 cKRjba">https://www.nervanasys.com/end-end-speech-recognition-neon/</a></li></ul><h2 id="wavenet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#wavenet" color="auto.gray.8" aria-label="WaveNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>WaveNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>WaveNet: A Generative Model for Raw Audio</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" class="Link-sc-1brdqhf-0 cKRjba">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view" class="Link-sc-1brdqhf-0 cKRjba">https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1gfmGWaJ" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1gfmGWaJ</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/usernaamee/keras-wavenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/usernaamee/keras-wavenet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ibab/tensorflow-wavenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ibab/tensorflow-wavenet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/monthly-hack/chainer-wavenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/monthly-hack/chainer-wavenet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/huyouare/WaveNet-Theano" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huyouare/WaveNet-Theano</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/basveeling/wavenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/basveeling/wavenet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ritheshkumar95/WaveNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ritheshkumar95/WaveNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A TensorFlow implementation of DeepMind&#x27;s WaveNet paper for text generation.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zeta36/tensorflow-tex-wavenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Zeta36/tensorflow-tex-wavenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Wavenet Generation Algorithm</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: An efficient Wavenet generation implementation</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09482" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09482</a></li><li>github <a target="_blank" rel="noopener noreferrer" href="https://github.com/tomlepaine/fast-wavenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tomlepaine/fast-wavenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition based on DeepMind&#x27;s WaveNet and tensorflow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/buriburisuri/speech-to-text-wavenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/buriburisuri/speech-to-text-wavenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.03193" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.03193</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TristouNet: Triplet Loss for Speaker Turn Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1609.04301" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1609.04301</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hbredin/TristouNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hbredin/TristouNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Speech Recognion and Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Research Silicon Valley AI Lab</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1qYrPkPQ" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1qYrPkPQ</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/baidu-research/ba-dls-deepspeech" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/baidu-research/ba-dls-deepspeech</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust end-to-end deep audiovisual speech recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06986" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06986</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Experimental Comparison of Deep Neural Networks for End-to-end Speech Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07174" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07174</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Deep Stacking Networks for Speech Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Ohio State University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.04675" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.04675</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Universite de Montreal &amp; CIFAR</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.02720" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.02720</a></li></ul><h1 id="deep-learning-for-sound--music" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-for-sound--music" color="auto.gray.8" aria-label="Deep Learning for Sound / Music permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning for Sound / Music</h1><h2 id="sound" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#sound" color="auto.gray.8" aria-label="Sound permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Sound</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Suggesting Sounds for Images from Video Collections</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ETH Zurich &amp; 2Disney Research</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20161014182443/Suggesting-Sounds-for-Images-from-Video-Collections-Paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20161014182443/Suggesting-Sounds-for-Images-from-Video-Collections-Paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Disney AI System Associates Images with Sounds</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://news.developer.nvidia.com/disneys-ai-system-associates-images-with-sounds/" class="Link-sc-1brdqhf-0 cKRjba">https://news.developer.nvidia.com/disneys-ai-system-associates-images-with-sounds/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Recurrent Neural Networks for Bird Audio Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.02317" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.02317</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual to Sound: Generating Natural Sound for Videos in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html" class="Link-sc-1brdqhf-0 cKRjba">http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01393" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01393</a></li></ul><h2 id="music" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#music" color="auto.gray.8" aria-label="Music permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Music</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Features of Music from Scratch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Washington. MusicNet</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://homes.cs.washington.edu/~thickstn/musicnet.html" class="Link-sc-1brdqhf-0 cKRjba">http://homes.cs.washington.edu/~thickstn/musicnet.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09827" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09827</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://homes.cs.washington.edu/~thickstn/demos.html" class="Link-sc-1brdqhf-0 cKRjba">http://homes.cs.washington.edu/~thickstn/demos.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepBach: a Steerable Model for Bach chorales generation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-generation/" class="Link-sc-1brdqhf-0 cKRjba">http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-generation/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01010" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01010</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SonyCSL-Paris/DeepBach" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SonyCSL-Paris/DeepBach</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=QiBM7-5hA6o" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=QiBM7-5hA6o</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Music</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://amundtveit.com/2016/11/22/deep-learning-for-music/" class="Link-sc-1brdqhf-0 cKRjba">https://amundtveit.com/2016/11/22/deep-learning-for-music/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>First International Workshop on Deep Learning and Music</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/html/1706.08675" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/html/1706.08675</a></p><h1 id="deep-learning-in-medicine-and-biology" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-in-medicine-and-biology" color="auto.gray.8" aria-label="Deep Learning in Medicine and Biology permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning in Medicine and Biology</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Low Data Drug Discovery with One-shot Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT &amp; Stanford University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.03199" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.03199</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://deepchem.io/" class="Link-sc-1brdqhf-0 cKRjba">http://deepchem.io/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepchem/deepchem" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deepchem/deepchem</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Democratizing Drug Discovery with DeepChem</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=sntikyFI8s8" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=sntikyFI8s8</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Introduction to Deep Learning in Medicine and Biology</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://a12d.com/deep-learning-biomedicine" class="Link-sc-1brdqhf-0 cKRjba">http://a12d.com/deep-learning-biomedicine</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Alzheimer Diagnostics and Decision Support</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://amundtveit.com/2016/11/18/deep-learning-for-alzheimer-diagnostics-and-decision-support/" class="Link-sc-1brdqhf-0 cKRjba">https://amundtveit.com/2016/11/18/deep-learning-for-alzheimer-diagnostics-and-decision-support/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Florida</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03211" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03211</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards biologically plausible deep learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Yoshua	Bengio, NIPS’2016 Workshops</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.iro.umontreal.ca/~bengioy/talks/Brains+Bits-NIPS2016Workshop.pptx.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.iro.umontreal.ca/~bengioy/talks/Brains+Bits-NIPS2016Workshop.pptx.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning and Its Applications to Machine Health Monitoring: A Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.07640" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.07640</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.01329" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.01329</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Applications in Medical Imaging</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://techemergence.com/deep-learning-medical-applications/" class="Link-sc-1brdqhf-0 cKRjba">http://techemergence.com/deep-learning-medical-applications/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dermatologist-level classification of skin cancer with deep neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford University. Nature 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature21056.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature21056.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Health Informatics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Imperial College London</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://ieeexplore.ieee.org/abstract/document/7801947/" class="Link-sc-1brdqhf-0 cKRjba">http://ieeexplore.ieee.org/abstract/document/7801947/</a></li></ul><h1 id="deep-learning-for-fashion" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-learning-for-fashion" color="auto.gray.8" aria-label="Deep Learning for Fashion permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Learning for Fashion</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Neural Networks for Fashion Classification and Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CS231N project</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://cs231n.stanford.edu/reports/BLAO_KJAG_CS231N_FinalPaperFashionClassification.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cs231n.stanford.edu/reports/BLAO_KJAG_CS231N_FinalPaperFashionClassification.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Fast and Accurate Fashion Item Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords:  MultiBox and Fast R-CNN, Kuznech-Fashion-156 and Kuznech-Fashion-205 fashion item detection datasets</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning at GILT</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: automated tagging, automatic dress faceting</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://tech.gilt.com/machine/learning,/deep/learning/2016/12/22/deep-learning-at-gilt" class="Link-sc-1brdqhf-0 cKRjba">http://tech.gilt.com/machine/learning,/deep/learning/2016/12/22/deep-learning-at-gilt</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Working with Fashion Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://making.lyst.com/2017/02/21/working-with-fashion-models/" class="Link-sc-1brdqhf-0 cKRjba">https://making.lyst.com/2017/02/21/working-with-fashion-models/</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=emr2qaCQOQs" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=emr2qaCQOQs</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fashion Forward: Forecasting Visual Style in Fashion</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Karlsruhe Institute of Technology &amp; The University of Texas at Austin</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.06394" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.06394</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>StreetStyle: Exploring world-wide clothing styles from millions of photos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://streetstyle.cs.cornell.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://streetstyle.cs.cornell.edu/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.01869" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.01869</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://streetstyle.cs.cornell.edu/trends.html" class="Link-sc-1brdqhf-0 cKRjba">http://streetstyle.cs.cornell.edu/trends.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fashioning with Networks: Neural Style Transfer to Design Clothes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ML4Fashion 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09899" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09899</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Our Way Through Fashion Week</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://inside.edited.com/deep-learning-our-way-through-fashion-week-ea55bf50bab8" class="Link-sc-1brdqhf-0 cKRjba">https://inside.edited.com/deep-learning-our-way-through-fashion-week-ea55bf50bab8</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Be Your Own Prada: Fashion Synthesis with Structural Coherence</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Be_Your_Own_ICCV_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Be_Your_Own_ICCV_2017_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhusz/ICCV17-fashionGAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhusz/ICCV17-fashionGAN</a></li></ul><h1 id="others" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#others" color="auto.gray.8" aria-label="Others permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Others</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Selfai: Predicting Facial Beauty in Selfies</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Selfai: A Method for Understanding Beauty in Selfies</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.erogol.com/selfai-predicting-facial-beauty-selfies/" class="Link-sc-1brdqhf-0 cKRjba">http://www.erogol.com/selfai-predicting-facial-beauty-selfies/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/erogol/beauty.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/erogol/beauty.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Enables You to Hide Screen when Your Boss is Approaching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/" class="Link-sc-1brdqhf-0 cKRjba">http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Hironsan/BossSensor" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Hironsan/BossSensor</a></li></ul><h1 id="blogs" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#blogs" color="auto.gray.8" aria-label="Blogs permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blogs</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>40 Ways Deep Learning is Eating the World</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d#.rxq8xvbfz" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d#.rxq8xvbfz</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Applications</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.deeplearningpatterns.com/doku.php/applications" class="Link-sc-1brdqhf-0 cKRjba">http://www.deeplearningpatterns.com/doku.php/applications</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Systematic Approach To Applications Of Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://gettocode.com/2016/11/25/systematic-approach-to-applications-of-deep-learning/" class="Link-sc-1brdqhf-0 cKRjba">https://gettocode.com/2016/11/25/systematic-approach-to-applications-of-deep-learning/</a></p><h1 id="resources" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resources" color="auto.gray.8" aria-label="Resources permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Resources</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Gallery - a curated collection of deep learning projects</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://deeplearninggallery.com/" class="Link-sc-1brdqhf-0 cKRjba">http://deeplearninggallery.com/</a></p><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/28/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>