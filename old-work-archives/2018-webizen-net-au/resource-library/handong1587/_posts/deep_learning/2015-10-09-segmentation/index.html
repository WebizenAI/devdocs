<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Papers Deep Joint Task Learning for Generic Object Extraction intro: NIPS 2014 homepage:  http://vision.sysu.edu.cn/projects/deep-joint-tas…"/><meta data-react-helmet="true" name="twitter:title" content="Segmentation"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-28T19:22:29.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Papers Deep Joint Task Learning for Generic Object Extraction intro: NIPS 2014 homepage:  http://vision.sysu.edu.cn/projects/deep-joint-tas…"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Segmentation"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Papers Deep Joint Task Learning for Generic Object Extraction intro: NIPS 2014 homepage:  http://vision.sysu.edu.cn/projects/deep-joint-tas…"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,fdzjHV,HGjBQ,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.daTFSy{height:4px;padding:0;margin:24px 0;background-color:#e1e4e8;border:0;}/*!sc*/
data-styled.g29[id="horizontal-rule__HorizontalRule-sc-1731hye-0"]{content:"daTFSy,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Segmentation - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/HyperMedia Library/">HyperMedia Library</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">awesomeLists</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018 - Web Civics BizPlan/">EXECUTIVE SUMMARY</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#u-net" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">U-Net</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#unified-image-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Unified Image Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#foreground-object-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Foreground Object Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#semantic-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Semantic Segmentation</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeplab" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeepLab</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeplab-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeepLab v2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeplab-v3" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeepLab v3</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeplabv3" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeepLabv3+</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeperlab" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeeperLab</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#auto-deeplab" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Auto-DeepLab</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#segnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">SegNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#setr" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">SETR</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#instance-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Instance Segmentation</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#human-instance-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Human Instance Segmentation</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#video-instance-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Instance Segmentation</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#panoptic-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Panoptic Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#nightime-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Nightime Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#face-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Parsing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#specific-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Specific Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#segment-proposal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Segment Proposal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#scene-labeling--scene-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Scene Labeling / Scene Parsing</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#pspnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">PSPNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#benchmarks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Benchmarks</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#challenges" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Challenges</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#human-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Human Parsing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#joint-detection-and-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Joint Detection and Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-object-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Object Segmentation</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#challenge" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Challenge</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#matting" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Matting</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#trimap-free-matting" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">trimap-free matting</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#3d-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">3D Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#line-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Line Parsing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#leaderboard" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Leaderboard</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tutorails--talks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorails / Talks</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Segmentation</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#u-net" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">U-Net</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#unified-image-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Unified Image Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#foreground-object-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Foreground Object Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#semantic-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Semantic Segmentation</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeplab" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeepLab</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeplab-v2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeepLab v2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeplab-v3" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeepLab v3</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeplabv3" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeepLabv3+</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#deeperlab" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DeeperLab</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#auto-deeplab" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Auto-DeepLab</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#segnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">SegNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#setr" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">SETR</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#instance-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Instance Segmentation</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#human-instance-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Human Instance Segmentation</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#video-instance-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Instance Segmentation</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#panoptic-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Panoptic Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#nightime-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Nightime Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#face-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Parsing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#specific-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Specific Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#segment-proposal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Segment Proposal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#scene-labeling--scene-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Scene Labeling / Scene Parsing</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#pspnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">PSPNet</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#benchmarks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Benchmarks</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#challenges" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Challenges</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#human-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Human Parsing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#joint-detection-and-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Joint Detection and Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-object-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Object Segmentation</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#challenge" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Challenge</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#matting" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Matting</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#trimap-free-matting" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">trimap-free matting</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#3d-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">3D Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#line-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Line Parsing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#leaderboard" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Leaderboard</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tutorails--talks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorails / Talks</a></li></ul></div></div><h1 id="papers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers" color="auto.gray.8" aria-label="Papers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Joint Task Learning for Generic Object Extraction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2014</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vision.sysu.edu.cn/projects/deep-joint-task-learning/" class="Link-sc-1brdqhf-0 cKRjba">http://vision.sysu.edu.cn/projects/deep-joint-task-learning/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://ss.sysu.edu.cn/~ll/files/NIPS2014_JointTask.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://ss.sysu.edu.cn/~ll/files/NIPS2014_JointTask.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaolonw/nips14_loc_seg_testonly" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xiaolonw/nips14_loc_seg_testonly</a></li><li>dataset: <a target="_blank" rel="noopener noreferrer" href="http://objectextraction.github.io/" class="Link-sc-1brdqhf-0 cKRjba">http://objectextraction.github.io/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1412.4526" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1412.4526</a></li><li>code(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://dl.dropboxusercontent.com/u/6448899/caffe.zip" class="Link-sc-1brdqhf-0 cKRjba">https://dl.dropboxusercontent.com/u/6448899/caffe.zip</a></li><li>author page: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~hsli/" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~hsli/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Segmentation from Natural Language Expressions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://ronghanghu.com/text_objseg/" class="Link-sc-1brdqhf-0 cKRjba">http://ronghanghu.com/text_objseg/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.06180" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.06180</a></li><li>github(TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/ronghanghu/text_objseg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ronghanghu/text_objseg</a></li><li>gtihub(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Seth-Park/text_objseg_caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Seth-Park/text_objseg_caffe</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Object Parsing with Graph LSTM</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.07063" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.07063</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fine Hand Segmentation using Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.07454" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.07454</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feedback Neural Network for Weakly Supervised Geo-Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook Connectivity Lab &amp; Facebook Core Data Science &amp; University of Illinois</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02766" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02766</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FusionNet: A deep fully residual convolutional neural network for image segmentation in connectomics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.05360" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.05360</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A deep learning model integrating FCNNs and CRFs for brain tumor segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.04528" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.04528</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Texture segmentation with Fully Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Dublin City University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.05230" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.05230</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast LIDAR-based Road Detection Using Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.03613" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.03613</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.04363" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.04363</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://gyglim.github.io/deep-value-net/" class="Link-sc-1brdqhf-0 cKRjba">https://gyglim.github.io/deep-value-net/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Annotating Object Instances with a Polygon-RNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. CVPR Best Paper Honorable Mention Award</li><li>intro: University of Toronto</li><li>keywords: PolygonRNN</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.toronto.edu/polyrnn/" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.toronto.edu/polyrnn/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.05548" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.05548</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>keywords: PolygonRNN++</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.toronto.edu/polyrnn/" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.toronto.edu/polyrnn/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09693" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09693</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/davidjesusacu/polyrnn-pp" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/davidjesusacu/polyrnn-pp</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Segmentation via Structured Patch Prediction, Context CRF and Guidance CRF</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Semantic_Segmentation_via_CVPR_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Semantic_Segmentation_via_CVPR_2017_paper.pdf</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com//FalongShen/SegModel" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//FalongShen/SegModel</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distantly Supervised Road Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV workshop CVRSUAD2017. Indiana University &amp; Preferred Networks</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.06118" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.06118</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ω-Net: Fully Automatic, Multi-View Cardiac MR Detection, Orientation, and Segmentation with Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ω-Net (Omega-Net): Fully Automatic, Multi-View Cardiac MR Detection, Orientation, and Segmentation with Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.01094" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.01094</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Superpixel clustering with deep features for unsupervised road segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Preferred Networks, Inc &amp; Indiana University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05998" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05998</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Segment Human by Watching YouTube</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.01457" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.01457</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>W-Net: A Deep Model for Fully Unsupervised Image Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08506" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08506</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end detection-segmentation network with ROI convolution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ISBI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02722" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02722</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Foreground Inference Network for Video Surveillance Using Multi-View Receptive Field</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06593" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06593</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Piecewise Flat Embedding for Image Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.03248" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.03248</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Pyramid CNN for Dense-Leaves Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Computer and Robot Vision, Toronto, May 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.01646" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.01646</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Capsules for Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: convolutional-deconvolutional capsule network, SegCaps, U-Net</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.04241" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.04241</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Object Co-Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.06423" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.06423</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Aware Attention Based Deep Object Co-segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.06859" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.06859</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Contextual Hourglass Networks for Segmentation and Density Estimation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.04009" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.04009</a></p><h2 id="u-net" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#u-net" color="auto.gray.8" aria-label="U-Net permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>U-Net</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>U-Net: Convolutional Networks for Biomedical Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: conditionally accepted at MICCAI 2015</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" class="Link-sc-1brdqhf-0 cKRjba">http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.04597" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.04597</a></li><li>code+data: <a target="_blank" rel="noopener noreferrer" href="http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz" class="Link-sc-1brdqhf-0 cKRjba">http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/orobix/retina-unet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/orobix/retina-unet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jakeret/tf_unet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jakeret/tf_unet</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://zongwei.leanote.com/post/Pa" class="Link-sc-1brdqhf-0 cKRjba">http://zongwei.leanote.com/post/Pa</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UNet++: A Nested U-Net Architecture for Medical Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 4th Deep Learning in Medical Image Analysis (DLMIA) Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.10165" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.10165</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICASSP 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.08790" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.08790</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZJUGiveLab/UNet-Version" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZJUGiveLab/UNet-Version</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepUNet: A Deep Fully Convolutional Network for Pixel-level Sea-Land Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.00201" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.00201</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Lyft Inc. &amp; MIT</li><li>intro: part of the winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.05746" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.05746</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ternaus/TernausNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ternaus/TernausNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Probabilistic U-Net for Segmentation of Ambiguous Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind &amp; German Cancer Research Center</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.05034" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.05034</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Dual Pyramid Network for Barcode Segmentation using Barcode-30k Database</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.11886" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.11886</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Smoke Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.00774" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.00774</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Smoothed Dilated Convolutions for Improved Dense Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: KDD 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.08931" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.08931</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/divelab/dilated" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/divelab/dilated</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DASNet: Reducing Pixel-level Annotations for Instance and Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.06013" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.06013</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Fast Segmentation With Teacher-student Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.08476" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.08476</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DSNet: An Efficient CNN for Road Scene Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05022" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05022</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Line Segment Detection Using Transformers without Edges</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.01909" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.01909</a></p><h1 id="unified-image-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#unified-image-segmentation" color="auto.gray.8" aria-label="Unified Image Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Unified Image Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>K-Net: Towards Unified Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2021</li><li>intro:  Nanyang Technological University &amp;  Chinese University of Hong Kon &amp; SenseTime Research &amp; Shanghai AI Laborator</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://www.mmlab-ntu.com/project/knet/index.html" class="Link-sc-1brdqhf-0 cKRjba">https://www.mmlab-ntu.com/project/knet/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.14855" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.14855</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZwwWayne/K-Net/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZwwWayne/K-Net/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Masked-attention Mask Transformer for Universal Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://bowenc0221.github.io/mask2former/" class="Link-sc-1brdqhf-0 cKRjba">https://bowenc0221.github.io/mask2former/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.01527" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.01527</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/Mask2Former" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/Mask2Former</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask2Former for Video Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Illinois at Urbana-Champaign (UIUC) &amp; Facebook AI Research (FAIR</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.10764" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.10764</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/Mask2Former" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/Mask2Former</a></li></ul><h1 id="foreground-object-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#foreground-object-segmentation" color="auto.gray.8" aria-label="Foreground Object Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Foreground Object Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pixel Objectness</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://vision.cs.utexas.edu/projects/pixelobjectness/" class="Link-sc-1brdqhf-0 cKRjba">http://vision.cs.utexas.edu/projects/pixelobjectness/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.05349" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.05349</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/suyogduttjain/pixelobjectness" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/suyogduttjain/pixelobjectness</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Deep Convolutional Neural Network for Background Subtraction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.01731" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.01731</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Multi-scale Features for Foreground Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.01477" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.01477</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lim-anggun/FgSegNet_v2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lim-anggun/FgSegNet_v2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Representations for Semantic Image Parsing: a Comprehensive Overview</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.04377" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.04377</a></p><h1 id="semantic-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#semantic-segmentation" color="auto.gray.8" aria-label="Semantic Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Semantic Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Networks for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015, PAMI 2016</li><li>keywords: deconvolutional layer, crop layer</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1411.4038" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1411.4038</a></li><li>arxiv(PAMI 2016): <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.06211" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.06211</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/presentation/d/1VeWFMpZ8XN7OC3URZP4WdXvOGYckoFWGVN7hApoXVnc" class="Link-sc-1brdqhf-0 cKRjba">https://docs.google.com/presentation/d/1VeWFMpZ8XN7OC3URZP4WdXvOGYckoFWGVN7hApoXVnc</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-pixels.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-pixels.pdf</a></li><li>talk: <a target="_blank" rel="noopener noreferrer" href="http://techtalks.tv/talks/fully-convolutional-networks-for-semantic-segmentation/61606/" class="Link-sc-1brdqhf-0 cKRjba">http://techtalks.tv/talks/fully-convolutional-networks-for-semantic-segmentation/61606/</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/shelhamer/fcn.berkeleyvision.org" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shelhamer/fcn.berkeleyvision.org</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MarvinTeichmann/tensorflow-fcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MarvinTeichmann/tensorflow-fcn</a></li><li>github(Chainer): <a target="_blank" rel="noopener noreferrer" href="https://github.com/wkentaro/fcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wkentaro/fcn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wkentaro/pytorch-fcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wkentaro/pytorch-fcn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/shekkizh/FCN.tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shekkizh/FCN.tensorflow</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://zhangliliang.com/2014/11/28/paper-note-fcn-segment/" class="Link-sc-1brdqhf-0 cKRjba">http://zhangliliang.com/2014/11/28/paper-note-fcn-segment/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>From Image-level to Pixel-level Labeling with Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>intro: &quot;Weakly Supervised Semantic Segmentation with Convolutional Networks&quot;</li><li>intro: performs semantic segmentation based only on image-level annotations in a multiple instance learning framework</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1411.6228" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1411.6228</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://ronan.collobert.com/pub/matos/2015_semisupsemseg_cvpr.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://ronan.collobert.com/pub/matos/2015_semisupsemseg_cvpr.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feedforward semantic segmentation with zoom-out features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015. Toyota Technological Institute at Chicago</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf</a></li><li>bitbuckt: <a target="_blank" rel="noopener noreferrer" href="https://bitbucket.org/m_mostajabi/zoom-out-release" class="Link-sc-1brdqhf-0 cKRjba">https://bitbucket.org/m_mostajabi/zoom-out-release</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=HvgvX1LXQa8" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=HvgvX1LXQa8</a></li></ul><h2 id="deeplab" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deeplab" color="auto.gray.8" aria-label="DeepLab permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DeepLab</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2015. DeepLab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.7062" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.7062</a></li><li>bitbucket: <a target="_blank" rel="noopener noreferrer" href="https://bitbucket.org/deeplab/deeplab-public/" class="Link-sc-1brdqhf-0 cKRjba">https://bitbucket.org/deeplab/deeplab-public/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TheLegendAli/DeepLab-Context" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TheLegendAli/DeepLab-Context</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepLab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1502.02734" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1502.02734</a></li><li>bitbucket: <a target="_blank" rel="noopener noreferrer" href="https://bitbucket.org/deeplab/deeplab-public/" class="Link-sc-1brdqhf-0 cKRjba">https://bitbucket.org/deeplab/deeplab-public/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TheLegendAli/DeepLab-Context" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TheLegendAli/DeepLab-Context</a></li></ul><h2 id="deeplab-v2" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deeplab-v2" color="auto.gray.8" aria-label="DeepLab v2 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DeepLab v2</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI</li><li>intro: 79.7% mIOU in the test set, PASCAL VOC-2012 semantic image segmentation task</li><li>intro: Updated version of our previous ICLR 2015 paper</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://liangchiehchen.com/projects/DeepLab.html" class="Link-sc-1brdqhf-0 cKRjba">http://liangchiehchen.com/projects/DeepLab.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.00915" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.00915</a></li><li>bitbucket: <a target="_blank" rel="noopener noreferrer" href="https://bitbucket.org/aquariusjay/deeplab-public-ver2" class="Link-sc-1brdqhf-0 cKRjba">https://bitbucket.org/aquariusjay/deeplab-public-ver2</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DrSleep/tensorflow-deeplab-resnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DrSleep/tensorflow-deeplab-resnet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/isht7/pytorch-deeplab-resnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/isht7/pytorch-deeplab-resnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepLabv2 (ResNet-101)</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://liangchiehchen.com/projects/DeepLabv2_resnet.html" class="Link-sc-1brdqhf-0 cKRjba">http://liangchiehchen.com/projects/DeepLabv2_resnet.html</a></p><h2 id="deeplab-v3" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deeplab-v3" color="auto.gray.8" aria-label="DeepLab v3 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DeepLab v3</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Atrous Convolution for Semantic Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google. DeepLabv3</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.05587" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.05587</a></li></ul><h2 id="deeplabv3" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deeplabv3" color="auto.gray.8" aria-label="DeepLabv3+ permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DeepLabv3+</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02611" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02611</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/research/deeplab" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/research/deeplab</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hualin95/Deeplab-v3plus" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hualin95/Deeplab-v3plus</a></li></ul><h2 id="deeperlab" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deeperlab" color="auto.gray.8" aria-label="DeeperLab permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DeeperLab</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeeperLab: Single-Shot Image Parser</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT &amp; Google Inc. &amp; UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.05093" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.05093</a></li></ul><h2 id="auto-deeplab" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#auto-deeplab" color="auto.gray.8" aria-label="Auto-DeepLab permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Auto-DeepLab</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>intro: Johns Hopkins University &amp; Google &amp; Stanford University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.02985" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.02985</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/research/deeplab" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/research/deeplab</a></li></ul><hr class="horizontal-rule__HorizontalRule-sc-1731hye-0 daTFSy"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Conditional Random Fields as Recurrent Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>intro: Oxford / Stanford / Baidu</li><li>keywords: CRF-RNN</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.robots.ox.ac.uk/~szheng/CRFasRNN.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.robots.ox.ac.uk/~szheng/CRFasRNN.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1502.03240" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1502.03240</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/torrvision/crfasrnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/torrvision/crfasrnn</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://www.robots.ox.ac.uk/~szheng/crfasrnndemo" class="Link-sc-1brdqhf-0 cKRjba">http://www.robots.ox.ac.uk/~szheng/crfasrnndemo</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/martinkersner/train-CRF-RNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/martinkersner/train-CRF-RNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1503.01640" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1503.01640</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient piecewise training of deep structured models for semantic segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.01013" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.01013</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deconvolution Network for Semantic Segmentation</strong></p><img src="http://cvlab.postech.ac.kr/research/deconvnet/images/overall.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>intro: two-stage training: train the network with easy examples first and
fine-tune the trained network with more challenging examples later</li><li>keywords: DeconvNet</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://cvlab.postech.ac.kr/research/deconvnet/" class="Link-sc-1brdqhf-0 cKRjba">http://cvlab.postech.ac.kr/research/deconvnet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.04366" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.04366</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w06-deconvnet.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w06-deconvnet.pdf</a></li><li>gitxiv: <a target="_blank" rel="noopener noreferrer" href="http://gitxiv.com/posts/9tpJKNTYksN5eWcHz/learning-deconvolution-network-for-semantic-segmentation" class="Link-sc-1brdqhf-0 cKRjba">http://gitxiv.com/posts/9tpJKNTYksN5eWcHz/learning-deconvolution-network-for-semantic-segmentation</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HyeonwooNoh/DeconvNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HyeonwooNoh/DeconvNet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HyeonwooNoh/caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HyeonwooNoh/caffe</a></li></ul><h2 id="segnet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#segnet" color="auto.gray.8" aria-label="SegNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SegNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.07293" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.07293</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/alexgkendall/caffe-segnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/alexgkendall/caffe-segnet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pfnet-research/chainer-segnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pfnet-research/chainer-segnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</strong></p><img src="http://mi.eng.cam.ac.uk/projects/segnet/images/segnet.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://mi.eng.cam.ac.uk/projects/segnet/" class="Link-sc-1brdqhf-0 cKRjba">http://mi.eng.cam.ac.uk/projects/segnet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.00561" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.00561</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/alexgkendall/caffe-segnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/alexgkendall/caffe-segnet</a></li><li>tutorial: <a target="_blank" rel="noopener noreferrer" href="http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html" class="Link-sc-1brdqhf-0 cKRjba">http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SegNet: Pixel-Wise Semantic Labelling Using a Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=xfNYAly1iXo" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=xfNYAly1iXo</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1gdUzDlD" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1gdUzDlD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Getting Started with SegNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html" class="Link-sc-1brdqhf-0 cKRjba">http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/alexgkendall/SegNet-Tutorial" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/alexgkendall/SegNet-Tutorial</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ParseNet: Looking Wider to See Better</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:ICLR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.04579" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.04579</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/weiliu89/caffe/tree/fcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/weiliu89/caffe/tree/fcn</a></li><li>caffe model zoo: <a target="_blank" rel="noopener noreferrer" href="https://github.com/BVLC/caffe/wiki/Model-Zoo#parsenet-looking-wider-to-see-better" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/BVLC/caffe/wiki/Model-Zoo#parsenet-looking-wider-to-see-better</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2016</li><li>keywords: DecoupledNet</li><li>project(paper+code): <a target="_blank" rel="noopener noreferrer" href="http://cvlab.postech.ac.kr/research/decouplednet/" class="Link-sc-1brdqhf-0 cKRjba">http://cvlab.postech.ac.kr/research/decouplednet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.04924" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.04924</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HyeonwooNoh/DecoupledNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HyeonwooNoh/DecoupledNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Image Segmentation via Deep Parsing Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015. CUHK</li><li>keywords: Deep Parsing Network (DPN), Markov Random Field (MRF)</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~lz013/projects/DPN.html" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~lz013/projects/DPN.html</a></li><li>arxiv.org: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.02634" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.02634</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liu_Semantic_Image_Segmentation_ICCV_2015_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liu_Semantic_Image_Segmentation_ICCV_2015_paper.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/presentation_dpn.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~pluo/pdf/presentation_dpn.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Scale Context Aggregation by Dilated Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2016.</li><li>intro: Dilated Convolution for Semantic Image Segmentation</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vladlen.info/publications/multi-scale-context-aggregation-by-dilated-convolutions/" class="Link-sc-1brdqhf-0 cKRjba">http://vladlen.info/publications/multi-scale-context-aggregation-by-dilated-convolutions/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07122" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07122</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fyu/dilation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fyu/dilation</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nicolov/segmentation_keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nicolov/segmentation_keras</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/" class="Link-sc-1brdqhf-0 cKRjba">http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance-aware Semantic Segmentation via Multi-task Network Cascades</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016 oral. 1st-place winner of MS COCO 2015 segmentation competition</li><li>keywords: RoI warping layer, Multi-task Network Cascades (MNC)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.04412" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.04412</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/daijifeng001/MNC" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daijifeng001/MNC</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Segmentation on SpaceNet via Multi-task Network Cascades (MNC)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/the-downlinq/object-segmentation-on-spacenet-via-multi-task-network-cascades-mnc-f1c89d790b42" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/the-downlinq/object-segmentation-on-spacenet-via-multi-task-network-cascades-mnc-f1c89d790b42</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lncohn/pascal_to_spacenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lncohn/pascal_to_spacenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network</strong></p><img src="http://cvlab.postech.ac.kr/research/transfernet/images/architecture.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TransferNet</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://cvlab.postech.ac.kr/research/transfernet/" class="Link-sc-1brdqhf-0 cKRjba">http://cvlab.postech.ac.kr/research/transfernet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.07928" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.07928</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/maga33/TransferNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/maga33/TransferNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.04871" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.04871</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1603.06098" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1603.06098</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kolesman/SEC" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kolesman/SEC</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup/" class="Link-sc-1brdqhf-0 cKRjba">http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.05144" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.05144</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Laplacian Reconstruction and Refinement for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1605.02264" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1605.02264</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.ics.uci.edu/~fowlkes/papers/gf-eccv16.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.ics.uci.edu/~fowlkes/papers/gf-eccv16.pdf</a></li><li>github(MatConvNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/golnazghiasi/LRR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/golnazghiasi/LRR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Natural Scene Image Segmentation Based on Multi-Layer Feature Extraction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.07586" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.07586</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Random Walk Networks for Semantic Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.07681" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.07681</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1606.02147" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1606.02147</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/e-lab/ENet-training" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/e-lab/ENet-training</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/TimoSaemann/ENet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TimoSaemann/ENet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PavlosMelissinos/enet-keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PavlosMelissinos/enet-keras</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kwotsin/TensorFlow-ENet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kwotsin/TensorFlow-ENet</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://culurciello.github.io/tech/2016/06/20/training-enet.html" class="Link-sc-1brdqhf-0 cKRjba">http://culurciello.github.io/tech/2016/06/20/training-enet.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1606.02585" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1606.02585</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Markov Random Field for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1606.07230" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1606.07230</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Region-based semantic segmentation with end-to-end training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.07671" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.07671</a></li><li>githun: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nightrome/matconvnet-calvin" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nightrome/matconvnet-calvin</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Built-in Foreground/Background Prior for Weakly-Supervised Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.00446" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.00446</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PixelNet: Towards a General Pixel-level Architecture</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: semantic segmentation, edge detection</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.06694" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.06694</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploiting Depth from Single Monocular Images for Object Detection and Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE T. Image Processing</li><li>intro: propose an RGB-D semantic segmentation method which applies a multi-task training scheme: semantic label prediction and depth value regression</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.01706" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.01706</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PixelNet: Representation of the pixels, by the pixels, and for the pixels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; Adobe Research</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.cmu.edu/~aayushb/pixelNet/" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.cmu.edu/~aayushb/pixelNet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.06506" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.06506</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/aayushbansal/PixelNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aayushbansal/PixelNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.06846" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.06846</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Structured Features for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.07916" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.07916</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNN-aware Binary Map for General Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2016 Best Paper / Student Paper Finalist</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1609.09220" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1609.09220</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Convolutional Neural Network with Binary Quantization Layer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06764" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06764</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mixed context networks for semantic segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Hikvision Research Institute</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.05854" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.05854</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>High-Resolution Semantic Labeling with Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01962" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01962</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gated Feedback Refinement Network for Dense Image Labeling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.umanitoba.ca/~ywang/papers/cvpr17.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.umanitoba.ca/~ywang/papers/cvpr17.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RefineNet: Multi-Path Refinement Networks with Identity Mappings for High-Resolution Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. IoU 83.4% on PASCAL VOC 2012</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06612" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06612</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/guosheng/refinenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guosheng/refinenet</a></li><li>leaderboard: <a target="_blank" rel="noopener noreferrer" href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_Multipath-RefineNet-Res152" class="Link-sc-1brdqhf-0 cKRjba">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_Multipath-RefineNet-Res152</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Light-Weight RefineNet for Real-Time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.03272" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.03272</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/drsleep/light-weight-refinenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/drsleep/light-weight-refinenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Full-Resolution Residual Units (FRRU), Full-Resolution Residual Networks (FRRNs)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08323" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08323</a></li><li>github(Theano/Lasagne): <a target="_blank" rel="noopener noreferrer" href="https://github.com/TobyPDE/FRRN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TobyPDE/FRRN</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=PNzQ4PNZSzc" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=PNzQ4PNZSzc</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Segmentation using Adversarial Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research &amp; INRIA. NIPS Workshop on Adversarial Training, Dec 2016, Barcelona, Spain</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08408" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08408</a></li><li>github(Chainer): <a target="_blank" rel="noopener noreferrer" href="https://github.com/oyam/Semantic-Segmentation-using-Adversarial-Networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/oyam/Semantic-Segmentation-using-Adversarial-Networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Fully Convolution Network for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08986" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08986</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Montreal Institute for Learning Algorithms &amp; Ecole Polytechnique de Montreal</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.09326" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.09326</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SimJeg/FC-DenseNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SimJeg/FC-DenseNet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/titu1994/Fully-Connected-DenseNets-Semantic-Segmentation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/titu1994/Fully-Connected-DenseNets-Semantic-Segmentation</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/0bserver07/One-Hundred-Layers-Tiramisu" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/0bserver07/One-Hundred-Layers-Tiramisu</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Bit Fully Convolutional Network for Fast Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.00212" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.00212</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation
with built-in awareness of semantically meaningful boundaries. &quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01337" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01337</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diverse Sampling for Self-Supervised Learning of Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01991" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01991</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mining Pixels: Weakly Supervised Semantic Segmentation Using Image Labels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Nankai University &amp; University of Oxford &amp; NUS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02101" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02101</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02649" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02649</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding Convolution for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UCSD &amp; CMU &amp; UIUC &amp; TuSimple</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.08502" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.08502</a></li><li>github(MXNet): <!-- -->[https://github.com/TuSimple/TuSimple-DUC]<a target="_blank" rel="noopener noreferrer" href="https://github.com/TuSimple/TuSimple-DUC" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TuSimple/TuSimple-DUC</a></li><li>pretrained-models: <a target="_blank" rel="noopener noreferrer" href="https://drive.google.com/drive/folders/0B72xLTlRb0SoREhISlhibFZTRmM" class="Link-sc-1brdqhf-0 cKRjba">https://drive.google.com/drive/folders/0B72xLTlRb0SoREhISlhibFZTRmM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Label Refinement Network for Coarse-to-Fine Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.arxiv.org/abs/1703.00551" class="Link-sc-1brdqhf-0 cKRjba">https://www.arxiv.org/abs/1703.00551</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Predicting Deeper into the Future of Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07684" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07684</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 (oral)</li><li>keywords: Adversarial Erasing (AE)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.08448" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.08448</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Guided Perturbations: Self Corrective Behavior in Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland &amp; GE Global Research Center</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07928" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07928</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via Deep Layer Cascade</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 spotlight paper</li><li>arxxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.01344" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.01344</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.02719" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.02719</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Loss Max-Pooling for Semantic Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.02966" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.02966</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reformulating Level Sets as Deep Recurrent Neural Network Approach to Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03593" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03593</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Review on Deep Learning Techniques Applied to Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.06857" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.06857</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Semantic and Motion Segmentation for dynamic scenes using Deep Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: [International Institute of Information Technology &amp; Max Planck Institute For Intelligent Systems</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.08331" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.08331</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ICNet for Real-Time Semantic Segmentation on High-Resolution Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CUHK &amp; Sensetime</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://hszhao.github.io/projects/icnet/" class="Link-sc-1brdqhf-0 cKRjba">https://hszhao.github.io/projects/icnet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.08545" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.08545</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hszhao/ICNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hszhao/ICNet</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=qWl9idsCuLQ" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=qWl9idsCuLQ</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Forwarding: Exploiting Encoder Representations for Efficient Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://codeac29.github.io/projects/linknet/" class="Link-sc-1brdqhf-0 cKRjba">https://codeac29.github.io/projects/linknet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.03718" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.03718</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/e-lab/LinkNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/e-lab/LinkNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pixel Deconvolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Washington State University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.06820" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.06820</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Incorporating Network Built-in Priors in Weakly-supervised Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE TPAMI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.02189" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.02189</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Semantic Segmentation for Automated Driving: Taxonomy, Roadmap and Challenges</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE ITSC 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.02432" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.02432</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Segmentation with Reverse Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017 oral. University of Southern California</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06426" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06426</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stacked Deconvolutional Network for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.04943" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.04943</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Dilation Factors for Semantic Segmentation of Street Scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: GCPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.01956" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.01956</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Self-aware Sampling Scheme to Efficiently Train Fully Convolutional Networks for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.02764" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.02764</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One-Shot Learning for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMWC 2017</li><li>arcxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.03410" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.03410</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lzzcd001/OSLSM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lzzcd001/OSLSM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Adaptive Sampling Scheme to Efficiently Train Fully Convolutional Networks for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.02764" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.02764</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Segmentation from Limited Training Data</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.07665" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.07665</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Domain Adaptation for Semantic Segmentation with GANs</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06969" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06969</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neuron-level Selective Context Aggregation for Scene Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08278" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08278</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Road Extraction by Deep Residual U-Net</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10684" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10684</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mix-and-Match Tuning for Self-Supervised Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/M&amp;M/" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/M&amp;M/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00661" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00661</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/XiaohangZhan/mix-and-match/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/XiaohangZhan/mix-and-match/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//liuziwei7/mix-and-match" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//liuziwei7/mix-and-match</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Error Correction for Dense Semantic Image Labeling</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.03812" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.03812</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.01317" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.01317</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RTSeg: Real-time Semantic Segmentation Comparative Study</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.02758" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.02758</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MSiam/TFSegmentation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MSiam/TFSegmentation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShuffleSeg: Real-time Semantic Segmentation Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Cairo University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.03816" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.03816</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic-structured Semantic Propagation Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06067" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06067</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://sacmehta.github.io/ESPNet/" class="Link-sc-1brdqhf-0 cKRjba">https://sacmehta.github.io/ESPNet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06815" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06815</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sacmehta/ESPNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sacmehta/ESPNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context Encoding for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>keywords: Synchronized Cross-GPU Batch Normalization</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08904" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08904</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhanghang1989/PyTorch-Encoding" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhanghang1989/PyTorch-Encoding</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Affinity Field for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UC Berkeley / ICSI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10335" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10335</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Predicting Future Instance Segmentations by Forecasting Convolutional Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research &amp; Univ. Grenoble Alpes</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.11496" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.11496</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Adaptation Networks for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018, Rank 1 in Segmentation Track of Visual Domain Adaptation Challenge 2017</li><li>keywords: Fully Convolutional Adaptation Networks (FCAN), Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.08286" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.08286</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Discriminative Feature Network for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.09337" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.09337</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Representation Learning for Domain Adaptation of Semantic Image Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.04141" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.04141</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional CRFs for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.04777" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.04777</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MarvinTeichmann/ConvCRF" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MarvinTeichmann/ConvCRF</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Toshiba Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.04554" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.04554</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DifNet: Semantic Segmentation by DiffusionNetworks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.08015" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.08015</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pyramid Attention Network for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.10180" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.10180</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Segmentation with Scarce Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2018 Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.00911" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.00911</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention to Refine through Multi-Scales for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.02917" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.02917</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Guided Upsampling Network for Real-Time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.07466" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.07466</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Semantic Segmentation on Minimal Hardware</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: RoboCup International Symposium 2018. University of Hertfordshire</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.05597" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.05597</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Future Semantic Segmentation with Convolutional LSTM</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.07946" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.07946</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.00897" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.00897</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dual Attention Network for Scene Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.02983" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.02983</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.04766" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.04766</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.06323" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.06323</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Image Segmentation by Scale-Adaptive Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/speedinghzl/Scale-Adaptive-Network" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/speedinghzl/Scale-Adaptive-Network</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Iterative Gating Networks for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08043" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08043</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CGNet: A Light-weight Context Guided Network for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08201" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08201</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wutianyiRosun/CGNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wutianyiRosun/CGNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CCNet: Criss-Cross Attention for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Huazhong University of Science and Technology &amp; Horizon Robotics &amp; University of Illinois at Urbana-Champaign</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11721" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11721</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/speedinghzl/CCNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/speedinghzl/CCNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShelfNet for Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Yale University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11254" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11254</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/juntang-zhuang/ShelfNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/juntang-zhuang/ShelfNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Semantic Segmentation via Video Propagation and Label Relaxation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.01593" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.01593</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/semantic-segmentation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NVIDIA/semantic-segmentation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.03353" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.03353</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chengyangfu/retinamask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chengyangfu/retinamask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast-SCNN: Fast Semantic Segmentation Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.04502" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.04502</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Structured Knowledge Distillation for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.04197" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.04197</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: University of Zagreb</li><li>keywords: SwiftNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.08469" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.08469</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/orsic/swiftnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/orsic/swiftnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese Academy of Sciences &amp; Deepwise AI Lab</li><li>keywords: Joint Pyramid Upsampling (JPU)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://wuhuikai.me/FastFCNProject/" class="Link-sc-1brdqhf-0 cKRjba">http://wuhuikai.me/FastFCNProject/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.11816" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.11816</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wuhuikai/FastFCN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wuhuikai/FastFCN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Significance-aware Information Bottleneck for Domain Adaptive Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: HUST &amp; UTS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.00876" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.00876</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GFF: Gated Fully Fusion for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01803" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01803</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DADA: Depth-aware Domain Adaptation in Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01886" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01886</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02216" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02216</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.09826" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.09826</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaoyufenfei/ESNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xiaoyufenfei/ESNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gated-SCNN: Gated Shape CNNs for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NVIDIA &amp; University of Waterloo &amp; University of Toronto &amp; Vector Institute</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://nv-tlabs.github.io/GSCNN/" class="Link-sc-1brdqhf-0 cKRjba">https://nv-tlabs.github.io/GSCNN/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.05740" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.05740</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.11830" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.11830</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Graph Message Passing Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.06955" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.06955</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Squeeze-and-Attention Networks for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.03402" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.03402</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Global Aggregation then Local Distribution in Fully Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.07229" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.07229</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lxtGH/GALD-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lxtGH/GALD-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Graph-guided Architecture Search for Real-time Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.06793" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.06793</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Pyramid Encoding Network for Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.08599" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.08599</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ACFNet: Attentional Class Feature Network for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.09408" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.09408</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Region Mutual Information Loss for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.12037" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.12037</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZJULearning/RMI" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZJULearning/RMI</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.13049" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.13049</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/RogerZhangzz/CAG_UDA" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/RogerZhangzz/CAG_UDA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficacy of Pixel-Level OOD Detection for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.02897" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.02897</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Location-aware Upsampling for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: LaU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.05250" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.05250</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FasterSeg: Searching for Faster Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2020</li><li>intro: Texas A&amp;M University &amp; Horizon Robotics Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.10917" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.10917</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AlignSeg: Feature-Aligned Segmentation Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.00872" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.00872</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Grouping Model for Unified Perceptual Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.11647" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.11647</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial Pyramid Based Graph Reasoning for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.10211" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.10211</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Dynamic Routing for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.10401" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.10401</a></li><li>giihub(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/yanwei-li/DynamicRouting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yanwei-li/DynamicRouting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Predict Context-adaptive Convolution for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.08222" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.08222</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transferring and Regularizing Prediction for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.06570" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.06570</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tensor Low-Rank Reconstruction for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: Top-1 performance on PASCAL-VOC12</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.00490" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.00490</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CWanli/RecoNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CWanli/RecoNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Representative Graph Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.05202" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.05202</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EfficientFCN: Holistically-guided Decoding for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.10487" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.10487</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Semantic Segmentation via Decoupled Body and Edge Supervision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.10035" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.10035</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lxtGH/DecoupleSegNets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lxtGH/DecoupleSegNets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.07930" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.07930</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PseudoSeg: Designing Pseudo Labels for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.09713" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.09713</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/googleinterns/wss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/googleinterns/wss</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.12440" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.12440</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pixel-Level Cycle Association: A New Perspective for Domain Adaptive Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.00147" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.00147</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kgl-prml/Pixel-Level-Cycle-Association" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kgl-prml/Pixel-Level-Cycle-Association</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CABiNet: Efficient Context Aggregation Network for Low-Latency Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Twente</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.00993" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.00993</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.12025" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.12025</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Channel-wise Distillation for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.13256" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.13256</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/drilistbox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/drilistbox</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BoxInst: High-Performance Instance Segmentation with Box Annotations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Adelaide</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.02310" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.02310</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aim-uofa/AdelaiDet/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aim-uofa/AdelaiDet/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scaling Semantic Segmentation Beyond 1K Classes on a Single GPU</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.07489" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.07489</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/shipra25jain/ESSNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shipra25jain/ESSNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-Domain Grouping and Alignment for Domain Adaptive Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.08226" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.08226</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI &amp; Tel Aviv University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.11582" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.11582</a></li></ul><h2 id="setr" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#setr" color="auto.gray.8" aria-label="SETR permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SETR</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Fudan University &amp; University of Oxford &amp; University of Surrey &amp; Tencent Youtu Lab &amp; Facebook AI</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://fudan-zvg.github.io/SETR/" class="Link-sc-1brdqhf-0 cKRjba">https://fudan-zvg.github.io/SETR/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.15840" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.15840</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fudan-zvg/SETR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fudan-zvg/SETR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring Cross-Image Pixel Contrast for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021 oral</li><li>intro: Computer Vision Lab, ETH Zurich &amp; SenseTime Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.11939" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.11939</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tfzhou/ContrastiveSeg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tfzhou/ContrastiveSeg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Active Boundary Loss for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.02696" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.02696</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Statistical Texture for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Beihang University &amp; SenseTime Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.04133" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.04133</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-Dataset Collaborative Learning for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Xilinx Inc. &amp; Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11351" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11351</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vision Transformers for Dense Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Intel Labs</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.13413" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.13413</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/intel-isl/DPT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/intel-isl/DPT</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>InverseForm: A Loss Function for Structured Boundary-Aware Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 oral</li><li>intro: Qualcomm AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.02745" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.02745</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking BiSeNet For Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Meituan</li><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.13188" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.13188</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MichaelFan01/STDC-Seg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MichaelFan01/STDC-Seg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Segmenter: Transformer for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Inria</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.05633" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.05633</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rstrudel/segmenter" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rstrudel/segmenter</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.15203" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.15203</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Per-Pixel Classification is Not All You Need for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: UIUC &amp; FAIR</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://bowenc0221.github.io/maskformer/" class="Link-sc-1brdqhf-0 cKRjba">https://bowenc0221.github.io/maskformer/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.06278" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.06278</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Unified Efficient Pyramid Transformer for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: School of Data Science, Fudan University &amp; Amazon Web Services &amp; University of California, Davis</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.14209" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.14209</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning for Open World Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.04562" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.04562</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Anchor Active Domain Adaptation for Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.08012" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.08012</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>intro: Indian Institute of Science &amp; Google Research</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/view/sfdaseg" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/view/sfdaseg</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.11249" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.11249</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HRFormer: High-Resolution Transformer for Dense Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2021</li><li>intro: University of Chinese Academy of Sciences &amp; Institute of Computing Technology, CAS &amp; Peking University &amp; Microsoft Research Asia &amp; Baidu</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.09408" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.09408</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HRNet/HRFormer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HRNet/HRFormer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Hierarchical Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.14335" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.14335</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/0liliulei/HieraSeg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/0liliulei/HieraSeg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.05525" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.05525</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hustvl/TopFormer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hustvl/TopFormer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Hong Kong University of Science and Technology &amp; Tsinghua University &amp; International Digital Economy Academy (IDEA) &amp; The Hong Kong University of Science and Technology (Guangzhou)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.02777" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.02777</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/IDEACVR/MaskDINO" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/IDEACVR/MaskDINO</a></li></ul><h1 id="instance-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#instance-segmentation" color="auto.gray.8" aria-label="Instance Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Instance Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Simultaneous Detection and Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2014</li><li>author: Bharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1407.1808" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1407.1808</a></li><li>github(Matlab): <a target="_blank" rel="noopener noreferrer" href="https://github.com/bharath272/sds_eccv2014" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bharath272/sds_eccv2014</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Feature Masking for Joint Object and Stuff Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>keywords: masking layers</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1412.1283" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1412.1283</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Proposal-free Network for Instance-level Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.02636" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.02636</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hypercolumns for object segmentation and fine-grained localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1411.5752" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1411.5752</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.berkeley.edu/~bharath2/pubs/pdfs/BharathCVPR2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.berkeley.edu/~bharath2/pubs/pdfs/BharathCVPR2015.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SDS using hypercolumns</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bharath272/sds" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bharath272/sds</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to decompose for object detection and instance segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2016 Workshop</li><li>keyword: CNN / RNN, MNIST, KITTI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06449" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06449</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>porject page: <a target="_blank" rel="noopener noreferrer" href="http://romera-paredes.com/ris" class="Link-sc-1brdqhf-0 cKRjba">http://romera-paredes.com/ris</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.08250" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.08250</a></li><li>github(Torch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/bernard24/ris" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bernard24/ris</a></li><li>poster: <a target="_blank" rel="noopener noreferrer" href="http://www.eccv2016.org/files/posters/P-4B-46.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eccv2016.org/files/posters/P-4B-46.pdf</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=l_WD2OWOqBk" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=l_WD2OWOqBk</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance-sensitive Fully Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016. instance segment proposal</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.08678" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.08678</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Amodal Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.08202" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.08202</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bridging Category-level and Instance-level Semantic Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: online bootstrapping</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.06885" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.06885</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bottom-up Instance Segmentation using Deep Higher-Order CRFs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.02583" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.02583</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepCut: Object Segmentation from Bounding Box Annotations using Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.07866" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.07866</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Instance Segmentation and Counting with Recurrent Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ReInspect</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.09410" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.09410</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Translation-aware Fully Convolutional Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Instance-aware Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:  CVPR 2017 Spotlight paper. winning entry of COCO segmentation challenge 2016</li><li>keywords:  TA-FCN / FCIS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07709" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07709</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/msracver/FCIS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/msracver/FCIS</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://onedrive.live.com/?cid=f371d9563727b96f&amp;id=F371D9563727B96F%2197213&amp;authkey=%21AEYOyOirjIutSVk" class="Link-sc-1brdqhf-0 cKRjba">https://onedrive.live.com/?cid=f371d9563727b96f&amp;id=F371D9563727B96F%2197213&amp;authkey=%21AEYOyOirjIutSVk</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>InstanceCut: from Edges to Instances with MultiCut</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08272" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08272</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Watershed Transform for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08303" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08303</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection Free Instance Segmentation With Labeling Transformations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08991" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08991</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Shape-aware Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03129" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03129</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Interpretable Structure-Evolving LSTM</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; Sun Yat-sen University &amp; National University of Singapore &amp; Adobe Research</li><li>intro: CVPR 2017 spotlight paper</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.03055" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.03055</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017 Best paper award. Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.06870" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.06870</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf</a></li><li>github(official, Caffe2): <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/Detectron" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/Detectron</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/maskrcnn-benchmark" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/maskrcnn-benchmark</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TuSimple/mx-maskrcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TuSimple/mx-maskrcnn</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://lmb.informatik.uni-freiburg.de/lectures/seminar_brox/seminar_ss17/maskrcnn_slides.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://lmb.informatik.uni-freiburg.de/lectures/seminar_brox/seminar_ss17/maskrcnn_slides.pdf</a></li><li>github(Keras+TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/matterport/Mask_RCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/matterport/Mask_RCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Faster Training of Mask R-CNN by Focusing on Instance Boundaries</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMW Car IT GmbH</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.07069" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.07069</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Boundary-preserving Mask R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: Huazhong University of Science and Technology &amp; Horizon Robotics Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.08921" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.08921</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hustvl/BMaskR-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hustvl/BMaskR-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Instance Segmentation via Deep Metric Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10277" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10277</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose2Instance: Harnessing Keypoints for Person Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.01152" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.01152</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pixelwise Instance Segmentation with a Dynamically Instantiated Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.02386" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.02386</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance-Level Salient Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03604" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03604</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MEnet: A Metric Expression Network for Salient Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.05638" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.05638</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Instance Segmentation with a Discriminative Loss Function</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Published at &quot;Deep Learning for Robotic Vision&quot;, workshop at CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02551" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02551</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Wizaron/instance-segmentation-pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Wizaron/instance-segmentation-pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.07158" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.07158</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>S4 Net: Single Stage Salient-Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07618" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07618</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/RuochenFan/S4Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/RuochenFan/S4Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Extreme Cut: From Extreme Points to Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09081" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09081</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Segment Every Thing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. UC Berkeley &amp; Facebook AI Research</li><li>keywords: MaskX R-CNN</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://ronghanghu.com/seg_every_thing/" class="Link-sc-1brdqhf-0 cKRjba">http://ronghanghu.com/seg_every_thing/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10370" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10370</a></li><li>gihtub(official, Caffe2): <a target="_blank" rel="noopener noreferrer" href="https://github.com/ronghanghu/seg_every_thing" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ronghanghu/seg_every_thing</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Neural Networks for Semantic Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://imatge-upc.github.io/rsis/" class="Link-sc-1brdqhf-0 cKRjba">https://imatge-upc.github.io/rsis/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00617" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00617</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/imatge-upc/rsis" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/imatge-upc/rsis</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Inc. &amp; RWTH Aachen University &amp; UCLA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.04837" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.04837</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Pixel Embedding for Instance Grouping</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: learning to embed pixels and group them into boundaries, object proposals, semantic segments and instances.</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.ics.uci.edu/~skong2/SMMMSG.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.ics.uci.edu/~skong2/SMMMSG.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.08273" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.08273</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_public_version.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_public_version.pdf</a></li><li>poster: <a target="_blank" rel="noopener noreferrer" href="http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_poster.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_poster.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Annotation-Free and One-Shot Learning for Instance Segmentation of Homogeneous Object Clusters</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.00383" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.00383</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Path Aggregation Network for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 Spotlight</li><li>intro: CUHK &amp; Peking University &amp; SenseTime Research &amp; YouTu Lab</li><li>keywords: PANet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.01534" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.01534</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ShuLiu1993/PANet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ShuLiu1993/PANet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Segment via Cut-and-Paste</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google</li><li>keywords: weakly-supervised, adversarial learning setup</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06414" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06414</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Cluster for Proposal-Free Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06459" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06459</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bayesian Semantic Instance Segmentation in Open Set World</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.00911" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.00911</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TernausNetV2: Fully Convolutional Network for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.00844" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.00844</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ternaus/TernausNetV2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ternaus/TernausNetV2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Multimodal Instance Segmentation guided by natural language queries</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.02257" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.02257</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/andfoy/query-objseg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andfoy/query-objseg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Traits &amp; Transferability of Adversarial Examples against Instance Segmentation &amp; Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.01452" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.01452</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Affinity Derivation and Graph Merge for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.10870" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.10870</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One-Shot Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Tubingen</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11507" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11507</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hybrid Task Cascade for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: The Chinese University of Hong Kong &amp; SenseTime Research &amp; Zhejiang University &amp; The University of Sydney &amp; Nanyang Technological University</li><li>intro: Winning entry of COCO 2018 Challenge (object detection task)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.07518" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.07518</a></li><li>github(mmdetection): <a target="_blank" rel="noopener noreferrer" href="https://github.com/open-mmlab/mmdetection/tree/master/configs/htc" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/open-mmlab/mmdetection/tree/master/configs/htc</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask Scoring R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: Huazhong University of Science and Technology &amp; Horizon Robotics Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.00241" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.00241</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zjhuang22/maskscoring_rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zjhuang22/maskscoring_rcnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TensorMask: A Foundation for Dense Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research (FAIR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.12174" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.12174</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Actor-Critic Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>keywords: reinforcement learning</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05126" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05126</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.11109" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.11109</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/davyneven/SpatialEmbeddings" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/davyneven/SpatialEmbeddings</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.07801" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.07801</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/GothicAi/Instaboost" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/GothicAi/Instaboost</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SSAP: Single-Shot Instance Segmentation With Affinity Pyramid</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: Chinese Academy of Sciences &amp; Horizon Robotics, Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.01616" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.01616</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLACT: Real-time Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: You Only Look At CoefficienTs</li><li>intro: University of California, Davis</li><li>keywords: one-stage, Fast NMS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02689" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02689</a></li><li>github(official, Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/dbolya/yolact" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dbolya/yolact</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLACT++: Better Real-time Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.06218" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.06218</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YolactEdge: Real-time Instance Segmentation on the Edge</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.12259" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.12259</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/haotian-liu/yolact_edge" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/haotian-liu/yolact_edge</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PolarMask: Single Shot Instance Segmentation with Polar Representation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.13226" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.13226</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xieenze/PolarMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xieenze/PolarMask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.02184" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.02184</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xieenze/PolarMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xieenze/PolarMask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CenterMask : Real-Time Anchor-Free Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.06667" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.06667</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/youngwanLEE/CenterMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/youngwanLEE/CenterMask</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/youngwanLEE/centermask2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/youngwanLEE/centermask2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CenterMask: single shot instance segmentation with point representation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: Meituan Dianping Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.04446" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.04446</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Shape-aware Feature Extraction for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.11263" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.11263</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PolyTransform: Deep Polygon Transformer for Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.02801" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.02801</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EmbedMask: Embedding Coupling for One-stage Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.01954" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.01954</a></li><li>gitub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yinghdb/EmbedMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yinghdb/EmbedMask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SAIS: Single-stage Anchor-free Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.01176" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.01176</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SOLO: Segmenting Objects by Locations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.04488" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.04488</a>
-github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/WXinlong/SOLO" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/WXinlong/SOLO</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SOLOv2: Dynamic, Faster and Stronger</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.10152" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.10152</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aim-uofa/AdelaiDet/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aim-uofa/AdelaiDet/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SOLO: A Simple Framework for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.15947" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.15947</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aim-uofa/AdelaiDet/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aim-uofa/AdelaiDet/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>intro: Chinese Academy of Sciences &amp; 2Horizon Robotics Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.05070" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.05070</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wangsr126/RDSNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wangsr126/RDSNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2001.00309" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2001.00309</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Conditional Convolutions for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 oral</li><li>intro: The University of Adelaide</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.05664" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.05664</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aim-uofa/adet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aim-uofa/adet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PointINS: Point-based Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CUHK &amp; MEGVII &amp; Chinese Academy of Sciences &amp; SmartMore</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.06148" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.06148</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>1st Place Solutions for OpenImage2019 -- Object Detection and Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.07557" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.07557</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask Encoding for Single Shot Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro:  Tongji University &amp; University of Adelaide &amp; Huawei Noah’s Ark Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.11712" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.11712</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Devil is in Classification: A Simple Framework for Long-tail Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.11978" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.11978</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/twangnh/SimCal" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/twangnh/SimCal</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Variational Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.11576" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.11576</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask Point R-CNN</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.00460" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.00460</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.05676" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.05676</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JialianW/Forest_RCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JialianW/Forest_RCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Seesaw Loss for Long-Tailed Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.10032" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.10032</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint COCO and Mapillary Workshop at ICCV 2019: COCO Instance Segmentation Challenge Track</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 1st Place Technical Report in ICCV2019/ ECCV2020: MegDetV2</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.02475" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.02475</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University &amp; Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.09876" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.09876</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Devil is in the Boundary: Exploiting Boundary Representation for Basis-based Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.13241" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.13241</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Instance Segmentation through Reasoning about Multi-Object Occlusion</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.02107" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.02107</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research &amp; UC Berkeley &amp; Cornell University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.07177" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.07177</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How Shift Equivariance Impacts Metric Learning for Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.05846" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.05846</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Nanyang Technological University &amp; Carnegie Mellon Universit</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.12867" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.12867</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.12340" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.12340</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lkeab/BCNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lkeab/BCNet</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=iHlGJppJGiQ" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=iHlGJppJGiQ</a></li><li>zhihu: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/378269087" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/378269087</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.14572" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.14572</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kreshuklab/spoco" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kreshuklab/spoco</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.00073" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.00073</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ISTR: End-to-End Instance Segmentation with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.00637" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.00637</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hujiecpp/ISTR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hujiecpp/ISTR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: The Hong Kong University of Science and Technology &amp; Kuaishou Technology</li><li>keywords: BCNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.12340" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.12340</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lkeab/BCNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lkeab/BCNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SOLQ: Segmenting Objects by Learning Queries</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MEGVII Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.02351" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.02351</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/megvii-research/SOLQ" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/megvii-research/SOLQ</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>1st Place Solution for YouTubeVOS Challenge 2021:Video Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CPVR 2021 Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.06649" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.06649</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rank &amp; Sort Loss for Object Detection and Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.11669" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.11669</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kemaloksuz/RankSortLoss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kemaloksuz/RankSortLoss</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SOTR: Segmenting Objects with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.06747" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.06747</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/easton-cau/SOTR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/easton-cau/SOTR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FaPN: Feature-aligned Pyramid Network for Dense Image Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.07058" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.07058</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/EMI-Group/FaPN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/EMI-Group/FaPN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instances as Queries</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>intro: HUST &amp; Tencent</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.01928" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.01928</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hustvl/QueryInst" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hustvl/QueryInst</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask Transfiner for High-Quality Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ETH Zurich &amp; HKUST &amp; Kuaishou Technology</li><li>arixv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.13673" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.13673</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SOIT: Segmenting Objects with Instance-Aware Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.11037" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.11037</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yuxiaodongHRI/SOIT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yuxiaodongHRI/SOIT</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ContrastMask: Contrastive Learning to Segment Every Thing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.09775" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.09775</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sparse Instance Activation for Real-Time Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro CVPR 2022</li><li>intro: Huazhong University of Science &amp; Technology &amp; Horizon Robotics &amp; CASIA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.12827" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.12827</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hustvl/SparseInst" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hustvl/SparseInst</a></li></ul><h2 id="human-instance-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#human-instance-segmentation" color="auto.gray.8" aria-label="Human Instance Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Human Instance Segmentation</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google, Inc.</li><li>keywords: Person detection and pose estimation, segmentation and grouping</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08225" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08225</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose2Seg: Detection Free Human Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: Tsinghua Unviersity &amp; BNRist &amp; Tencent AI Lab &amp; Cardiff University</li><li>keywords: Occluded Human (OCHuman)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.liruilong.cn/Pose2Seg/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.liruilong.cn/Pose2Seg/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10683" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10683</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liruilong940607/Pose2Seg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liruilong940607/Pose2Seg</a></li><li>dataset: <a target="_blank" rel="noopener noreferrer" href="https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman" class="Link-sc-1brdqhf-0 cKRjba">https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bounding Box Embedding for Single Shot Person Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.07674" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.07674</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Parsing R-CNN for Instance-Level Human Analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: COCO 2018 DensePose Challenge Winner</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12596" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12596</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/soeaver/Parsing-R-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/soeaver/Parsing-R-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Graphonomy: Universal Human Parsing via Graph Transfer Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.04536" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.04536</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Gaoyiminggithub/Graphonomy" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Gaoyiminggithub/Graphonomy</a></li></ul><h2 id="video-instance-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-instance-segmentation" color="auto.gray.8" aria-label="Video Instance Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Instance Segmentation</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SipMask: Spatial Information Preservation for Fast Image and Video Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.14772" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.14772</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JialeCao001/SipMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JialeCao001/SipMask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Video Instance Segmentation with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Meituan &amp; The University of Adelaide</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.14503" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.14503</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial Feature Calibration and Temporal Fusion for Effective One-stage Video Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: The HongKong Polytechnic University &amp; DAMO Academy, Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.05606" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.05606</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MinghanLi/STMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MinghanLi/STMask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tracking Instances as Queries</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: HUST &amp; Tencent PCG</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.11963" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.11963</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Mask Transfiner for High-Quality Video Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2022</li><li>intro: ETH Z¨urich &amp; The Hong Kong University of Science and Technology &amp; Kuaishou Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.14012" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.14012</a></li></ul><h1 id="panoptic-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#panoptic-segmentation" color="auto.gray.8" aria-label="Panoptic Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Panoptic Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research (FAIR) &amp; Heidelberg University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00868" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00868</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://presentations.cocodataset.org/COCO17-Invited-PanopticAlexKirillov.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://presentations.cocodataset.org/COCO17-Invited-PanopticAlexKirillov.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Panoptic Segmentation with a Joint Semantic and Instance Segmentation Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.02110" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.02110</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Fuse Things and Stuff</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Toyota Research Institute (TRI)</li><li>keywords: TASCNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.01192" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.01192</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention-guided Unified Network for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: University of Chinese Academy of Sciences &amp; Horizon Robotics, Inc. &amp; The Johns Hopkins University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.03904" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.03904</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Panoptic Feature Pyramid Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: FAIR</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.02446" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.02446</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UPSNet: A Unified Panoptic Segmentation Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Uber ATG &amp; University of Toronto &amp; The Chinese University of Hong Kong</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.03784" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.03784</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single Network Panoptic Segmentation for Street Scene Understanding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.02678" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.02678</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An End-to-End Network for Panoptic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.05027" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.05027</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Instance Occlusion for Panoptic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.05896" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.05896</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SpatialFlow: Bridging All Tasks for Panoptic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.08787" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.08787</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single-Shot Panoptic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.00764" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.00764</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SOGNet: Scene Overlap Graph Network for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020. Innovation Award in COCO 2019 challenge</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.07527" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.07527</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UIUC &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.10194" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.10194</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PanDA: Panoptic Data Augmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.12317" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.12317</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-Time Panoptic Segmentation from Dense Detections</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.01202" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.01202</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TRI-ML/realtime_panoptic" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TRI-ML/realtime_panoptic</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bipartite Conditional Random Fields for Panoptic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.05307" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.05307</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unifying Training and Inference for Panoptic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2001.04982" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2001.04982</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Bounding-Box Free Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SLAMcore Ltd. &amp; Imperial College London</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.07705" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.07705</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://semantic-kitti.org/" class="Link-sc-1brdqhf-0 cKRjba">http://semantic-kitti.org/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.02371" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.02371</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.07853" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.07853</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EPSNet: Efficient Panoptic Segmentation Network with Cross-layer Attention Fusion</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.10142" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.10142</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pixel Consensus Voting for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.01849" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.01849</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EfficientPS: Efficient Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.02307" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.02307</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DeepSceneSeg/EfficientPS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DeepSceneSeg/EfficientPS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 Oral</li><li>intro: KAIST &amp; Adobe Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.11339" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.11339</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mcahny/vps" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mcahny/vps</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PanoNet: Real-time Panoptic Segmentation through Position-Sensitive Feature Embedding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.00192" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.00192</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Vision Challenge 2020 -- 1st Place Report for Panoptic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.10112" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.10112</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Category- and Instance-Aware Pixel Embedding for Fast Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese Academy of Sciences &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.13342" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.13342</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Auto-Panoptic: Cooperative Multi-Component Architecture Search for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020</li><li>intro: Sun Yat-sen University &amp; Huawei Noah’s Ark Lab &amp; DarkMatter AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.16119" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.16119</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Jacobew/AutoPanoptic" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Jacobew/AutoPanoptic</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scaling Wide Residual Networks for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research &amp; Johns Hopkins University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.11675" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.11675</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Networks for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese University of Hong Kong &amp; University of Oxford &amp; University of Hong Kong &amp; MEGVII Technology4</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.00720" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.00720</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yanwei-li/PanopticFCN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yanwei-li/PanopticFCN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.00759" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.00759</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ada-Segment: Automated Multi-loss Adaptation for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>intro: Sun Yat-Sen University &amp; Huawei Noah’s Ark Lab &amp; Shanghai Jiao Tong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.03603" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.03603</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.05258" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.05258</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/joe-siyuan-qiao/ViP-DeepLab" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/joe-siyuan-qiao/ViP-DeepLab</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>STEP: Segmenting and Tracking Every Pixel</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Technical University Munich &amp; Google Research &amp; RWTH Aachen University &amp; MPI-IS and University of Tubingen</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.11859" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.11859</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-View Regularization for Domain Adaptive Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.02584" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.02584</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University &amp; Google Research</li><li>arixv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.00759" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.00759</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Panoptic Segmentation Forecasting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.03962" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.03962</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exemplar-Based Open-Set Panoptic Segmentation Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Seoul National University &amp; Adobe Research</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://cv.snu.ac.kr/research/EOPSN/" class="Link-sc-1brdqhf-0 cKRjba">https://cv.snu.ac.kr/research/EOPSN/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.08336" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.08336</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jd730/EOPSN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jd730/EOPSN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Lovász Embeddings for Proposal-free Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.04555" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.04555</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">P<strong>art-aware Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.06351" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.06351</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tue-mps/panoptic_parts" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tue-mps/panoptic_parts</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Panoptic SegFormer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Nanjing University &amp; The University of Hong Kong &amp; NVIDIA &amp; Caltech</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2109.03814" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2109.03814</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Slot-VPS: Object-centric Representation Learning for Video Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Samsung Research China - Beijing (SRC-B) &amp; 2Samsung Advanced Institute of Technology (SAIT) &amp; University of Oxford &amp; The University of Hong Kong</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.08949" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.08949</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CFNet: Learning Correlation Functions for One-Stage Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University &amp; Tencent Youtu Lab &amp; Shanghai Jiao Tong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2201.04796" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2201.04796</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Panoptic, Instance and Semantic Relations: A Relational Context Encoder to Enhance Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>intro: Qualcomm AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.05370" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.05370</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PanopticDepth: A Unified Framework for Depth-aware Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>intro: Chinese Academy of Sciences &amp; University of Chinese Academy of Sciences &amp; Horizon Robotics, Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.00468" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.00468</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022 Oral</li><li>intro: Johns Hopkins University &amp; KAIST &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.08948" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.08948</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Uncertainty-aware Panoptic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Technical University Nurnberg</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.14554" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.14554</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>k-means Mask Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2022</li><li>intro: Johns Hopkins University &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.04044" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.04044</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/google-research/deeplab2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/google-research/deeplab2</a></li></ul><h1 id="nightime-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#nightime-segmentation" color="auto.gray.8" aria-label="Nightime Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Nightime Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Nighttime sky/cloud image segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.10583" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.10583</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: International Conference on Intelligent Transportation Systems (ITSC 2018)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.02575" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.02575</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Nighttime Image Segmentation with Synthetic Stylized Data, Gradual Adaptation and Uncertainty-Aware Evaluation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: ETH Zurich &amp; KU Leuven</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.05946" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.05946</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bi-Mix: Bidirectional Mixing for Domain Adaptive Nighttime Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.10339" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.10339</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ygjwd12345/BiMix" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ygjwd12345/BiMix</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 oral</li><li>intro: University of South Carolina &amp; Farsee2 Technology Ltd</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.10834" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.10834</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/W-zx-Y/DANNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/W-zx-Y/DANNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.05538" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.05538</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xdeng7/NightLab" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xdeng7/NightLab</a></li></ul><h1 id="face-parsing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#face-parsing" color="auto.gray.8" aria-label="Face Parsing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Face Parsing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Parsing via Recurrent Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.01936" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.01936</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Parsing via a Fully-Convolutional Continuous CRF Neural Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.03736" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.03736</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Parsing with RoI Tanh-Warping</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Software School of Xiamen University &amp; Microsoft Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.01342" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.01342</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Face Parsing via Interlinked Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.04831" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.04831</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RoI Tanh-polar Transformer Network for Face Parsing in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.02717" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.02717</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://ibug.doc.ic.ac.uk/resources/ibugmask/" class="Link-sc-1brdqhf-0 cKRjba">https://ibug.doc.ic.ac.uk/resources/ibugmask/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Decoupled Multi-task Learning with Cyclical Self-Regulation for Face Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.14448" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.14448</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr</a></li></ul><h1 id="specific-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#specific-segmentation" color="auto.gray.8" aria-label="Specific Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Specific Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A CNN Cascade for Landmark Guided Semantic Part Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://aaronsplace.co.uk/" class="Link-sc-1brdqhf-0 cKRjba">http://aaronsplace.co.uk/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://aaronsplace.co.uk/papers/jackson2016guided/jackson2016guided.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://aaronsplace.co.uk/papers/jackson2016guided/jackson2016guided.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.03305" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.03305</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Boundary-sensitive Network for Portrait Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.08675" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.08675</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Boundary-Aware Network for Fast and High-Accuracy Portrait Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.03814" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.03814</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beef Cattle Instance Segmentation Using Fully Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.01972" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.01972</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Mask Extraction in Video Sequence</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: ConvLSTM &amp; FCN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.09207" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.09207</a></li></ul><h1 id="segment-proposal" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#segment-proposal" color="auto.gray.8" aria-label="Segment Proposal permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Segment Proposal</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Segment Object Candidates</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research (FAIR)</li><li>intro: DeepMask. learning segmentation proposals</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.06204" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.06204</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/deepmask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/deepmask</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/abbypa/NNProject_DeepMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/abbypa/NNProject_DeepMask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Refine Object Segments</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016. Facebook AI Research (FAIR)</li><li>intro: SharpMask. an extension of DeepMask which generates higher-fidelity masks using an additional top-down refinement step.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.08695" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.08695</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/deepmask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/deepmask</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FastMask: Segment Object Multi-scale Candidates in One Shot</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. University of California &amp; Fudan University &amp; Megvii Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.08843" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.08843</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/voidrank/FastMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/voidrank/FastMask</a></li></ul><h1 id="scene-labeling--scene-parsing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#scene-labeling--scene-parsing" color="auto.gray.8" aria-label="Scene Labeling / Scene Parsing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scene Labeling / Scene Parsing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Indoor Semantic Segmentation using depth information</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1301.3572" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1301.3572</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Convolutional Neural Networks for Scene Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1306.2795" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1306.2795</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://people.ee.duke.edu/~lcarin/Yizhe8.14.2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://people.ee.duke.edu/~lcarin/Yizhe8.14.2015.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NP-coder/CLPS1520Project" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NP-coder/CLPS1520Project</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rkargon/Scene-Labeling" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rkargon/Scene-Labeling</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning hierarchical features for scene labeling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-modal unsupervised feature learning for rgb-d scene labeling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2014</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www3.ntu.edu.sg/home/wanggang/WangECCV2014.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www3.ntu.edu.sg/home/wanggang/WangECCV2014.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scene Labeling with LSTM Recurrent Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attend, Infer, Repeat: Fast Scene Understanding with Generative Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.08575" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.08575</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://www.shortscience.org/paper?bibtexKey=journals/corr/EslamiHWTKH16" class="Link-sc-1brdqhf-0 cKRjba">http://www.shortscience.org/paper?bibtexKey=journals/corr/EslamiHWTKH16</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>&quot;Semantic Segmentation for Scene Understanding: Algorithms and Implementations&quot; tutorial</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 2016 Embedded Vision Summit</li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=pQ318oCGJGY" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=pQ318oCGJGY</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Understanding of Scenes through the ADE20K Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1608.05442" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1608.05442</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Representations for Scene Labeling with Guided Supervision</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Representations for Scene Labeling with Semantic Context Guided Supervision</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CUHK</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.02493" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.02493</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial As Deep: Spatial CNN for Traffic Scene Understanding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.06080" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.06080</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Path Feedback Recurrent Neural Network for Scene Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.07706" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.07706</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scene Labeling using Recurrent Neural Networks with Explicit Long Range Contextual Dependency</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07485" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07485</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.01587" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.01587</a></li></ul><h2 id="pspnet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pspnet" color="auto.gray.8" aria-label="PSPNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PSPNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pyramid Scene Parsing Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>intro: mIoU score as 85.4% on PASCAL VOC 2012 and 80.2% on Cityscapes,
ranked 1st place in ImageNet Scene Parsing Challenge 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://appsrv.cse.cuhk.edu.hk/~hszhao/projects/pspnet/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://appsrv.cse.cuhk.edu.hk/~hszhao/projects/pspnet/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01105" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01105</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://image-net.org/challenges/talks/2016/SenseCUSceneParsing.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://image-net.org/challenges/talks/2016/SenseCUSceneParsing.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hszhao/PSPNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hszhao/PSPNet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Open Vocabulary Scene Parsing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.08769" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.08769</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Contextual Recurrent Residual Networks for Scene Labeling</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03594" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03594</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Scene Understanding for Autonomous Driving</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Published at &quot;Deep Learning for Vehicle Perception&quot;, workshop at the IEEE Symposium on Intelligent Vehicles 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02550" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02550</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FoveaNet: Perspective-aware Urban Scene Parsing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02421" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02421</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BlitzNet: A Real-Time Deep Network for Scene Understanding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: INRIA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02813" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02813</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Foggy Scene Understanding with Synthetic Data</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.07819" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.07819</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale-adaptive Convolutions for Scene Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Scale-Adaptive_Convolutions_for_ICCV_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Scale-Adaptive_Convolutions_for_ICCV_2017_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Restricted Deformable Convolution based Road Scene Semantic Segmentation Using Surround View Cameras</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00708" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00708</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dense Recurrent Neural Networks for Scene Labeling</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06831" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06831</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DenseASPP for Semantic Segmentation in Street Scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DeepMotionAIResearch/DenseASPP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DeepMotionAIResearch/DenseASPP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OCNet: Object Context Network for Scene Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.00916" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.00916</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PkuRainBow/OCNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PkuRainBow/OCNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PSANet: Point-wise Spatial Attention Network for Scene Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://hszhao.github.io/projects/psanet/" class="Link-sc-1brdqhf-0 cKRjba">https://hszhao.github.io/projects/psanet/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://hszhao.github.io/papers/eccv18_psanet.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://hszhao.github.io/papers/eccv18_psanet.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit#slide=id.p" class="Link-sc-1brdqhf-0 cKRjba">https://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit#slide=id.p</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hszhao/PSANet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hszhao/PSANet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Context Network for Scene Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.01664" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.01664</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Flow for Fast and Accurate Scene Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.10120" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.10120</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/donnyyou/torchcv" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/donnyyou/torchcv</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Strip Pooling: Rethinking Spatial Pooling for Scene Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.13328" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.13328</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Andrew-Qibin/SPNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Andrew-Qibin/SPNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>S3-Net: A Fast and Lightweight Video Scene Understanding Network by Single-shot Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.02265" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.02265</a></li></ul><h2 id="benchmarks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#benchmarks" color="auto.gray.8" aria-label="Benchmarks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Benchmarks</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MIT Scene Parsing Benchmark</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://sceneparsing.csail.mit.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://sceneparsing.csail.mit.edu/</a></li><li>github(devkit): <a target="_blank" rel="noopener noreferrer" href="https://github.com/CSAILVision/sceneparsing" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CSAILVision/sceneparsing</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Understanding of Urban Street Scenes: Benchmark Suite</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.cityscapes-dataset.com/benchmarks/" class="Link-sc-1brdqhf-0 cKRjba">https://www.cityscapes-dataset.com/benchmarks/</a></p><h2 id="challenges" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#challenges" color="auto.gray.8" aria-label="Challenges permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Challenges</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-scale Scene Understanding Challenge</strong></p><img src="http://lsun.cs.princeton.edu/img/overview_4crop.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://lsun.cs.princeton.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://lsun.cs.princeton.edu/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Places2 Challenge</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://places2.csail.mit.edu/challenge.html" class="Link-sc-1brdqhf-0 cKRjba">http://places2.csail.mit.edu/challenge.html</a></p><h1 id="human-parsing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#human-parsing" color="auto.gray.8" aria-label="Human Parsing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Human Parsing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Human Parsing with Contextualized Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Liang_Human_Parsing_With_ICCV_2015_paper.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Liang_Human_Parsing_With_ICCV_2015_paper.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. SYSU &amp; CMU</li><li>keywords: Look Into Person (LIP)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://hcp.sysu.edu.cn/lip/" class="Link-sc-1brdqhf-0 cKRjba">http://hcp.sysu.edu.cn/lip/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.05446" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.05446</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Engineering-Course/LIP_SSL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Engineering-Course/LIP_SSL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multiple-Human Parsing in the Wild</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.07206" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.07206</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Look into Person: Joint Body Parsing &amp; Pose Estimation Network and A New Benchmark</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: T-PAMI 2018</li><li>keywords: Joint Body Parsing &amp; Pose Estimation Network (JPPNet)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.01984" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.01984</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Engineering-Course/LIP_JPPNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Engineering-Course/LIP_JPPNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-domain Human Parsing via Adversarial Feature and Label Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.01260" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.01260</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fusing Hierarchical Convolutional Features for Human Body Segmentation and Clothing Fashion Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Wuhan University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.03415" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.03415</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.03287" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.03287</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZhaoJ9014/Multi-Human-Parsing" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZhaoJ9014/Multi-Human-Parsing</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Macro-Micro Adversarial Network for Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>keywords: Macro-Micro Adversarial Net (MMAN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08260" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08260</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/RoyalVane/MMAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/RoyalVane/MMAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance-level Human Parsing via Part Grouping Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.00157" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.00157</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Temporal Encoding Network for Video Instance-level Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2018
= arixv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.00661" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.00661</a></li><li>github(official, TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/HCPLab-SYSU/ATEN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HCPLab-SYSU/ATEN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Devil in the Details: Towards Accurate Single and Multiple Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Context Embedding with Edge Perceiving (CE2P)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.05996" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.05996</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liutinglt/CE2P" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liutinglt/CE2P</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-Domain Complementary Learning with Synthetic Data for Multi-Person Part Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Washington &amp; Microsof</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.05193" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.05193</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-Correction for Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.09777" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.09777</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PeikeLi/Self-Correction-Human-Parsing" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PeikeLi/Self-Correction-Human-Parsing</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Grapy-ML: Graph Pyramid Mutual Learning for Cross-dataset Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.12053" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.12053</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Charleshhy/Grapy-ML" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Charleshhy/Grapy-ML</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Semantic Neural Tree for Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Institute of Software Chinese Academy of Sciences &amp; State University of New York &amp; JD Finance America Corporation &amp; Tencent Youtu Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.09622" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.09622</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://isrc.iscas.ac.cn/gitlab/research/sematree" class="Link-sc-1brdqhf-0 cKRjba">https://isrc.iscas.ac.cn/gitlab/research/sematree</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-Learning with Rectification Strategy for Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.08055" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.08055</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Correlating Edge, Pose with Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.01431" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.01431</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ziwei-zh/CorrPM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ziwei-zh/CorrPM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Affinity-aware Compression and Expansion Network for Human Parsing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.10191" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.10191</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Renovating Parsing R-CNN for Accurate Multiple Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: BUPT &amp; Noah’s Ark Lab, Huawei Technologies</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.09447" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.09447</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/soeaver/RP-R-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/soeaver/RP-R-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive One-shot Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.11810" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.11810</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Charleshhy/One-shot-Human-Parsing" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Charleshhy/One-shot-Human-Parsing</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.04570" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.04570</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tfzhou/MG-HumanParsing" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tfzhou/MG-HumanParsing</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quality-Aware Network for Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BUPT &amp; Institute of Automation Chinese Academy of Sciences &amp; 3Noah’s Ark Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.05997" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.05997</a></li><li>github(Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/soeaver/QANet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/soeaver/QANet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end One-shot Human Parsing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.01241" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.01241</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CDGNet: Class Distribution Guided Network for Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Ajou University &amp; Tiangong University &amp; Incheon National University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.14173" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.14173</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AIParsing: Anchor-free Instance-level Human Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Image Processing (TIP)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.06854" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.06854</a></li></ul><h1 id="joint-detection-and-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#joint-detection-and-segmentation" color="auto.gray.8" aria-label="Joint Detection and Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Joint Detection and Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Triply Supervised Decoder Networks for Joint Detection and Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.09299" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.09299</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>D2Det: Towards High Quality Object Detection and Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_D2Det_Towards_High_Quality_Object_Detection_and_Instance_Segmentation_CVPR_2020_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_D2Det_Towards_High_Quality_Object_Detection_and_Instance_Segmentation_CVPR_2020_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JialeCao001/D2Det" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JialeCao001/D2Det</a></li></ul><h1 id="video-object-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-object-segmentation" color="auto.gray.8" aria-label="Video Object Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Object Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast object segmentation in unconstrained video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://calvin.inf.ed.ac.uk/software/fast-video-segmentation/" class="Link-sc-1brdqhf-0 cKRjba">http://calvin.inf.ed.ac.uk/software/fast-video-segmentation/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://calvin.inf.ed.ac.uk/wp-content/uploads/Publications/papazoglouICCV2013-camera-ready.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://calvin.inf.ed.ac.uk/wp-content/uploads/Publications/papazoglouICCV2013-camera-ready.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Fully Convolutional Networks for Video Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.00487" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.00487</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection, Tracking, and Motion Segmentation for Object-level Video Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03066" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03066</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Clockwork Convnets for Video Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016 Workshops</li><li>intro: evaluated on the Youtube-Objects, NYUD, and Cityscapes video datasets</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03609" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03609</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/shelhamer/clockwork-fcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/shelhamer/clockwork-fcn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>STFCN: Spatio-Temporal FCN for Semantic Video Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.05971" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.05971</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One-Shot Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: OSVOS</li><li>project: <a target="_blank" rel="noopener noreferrer" href="http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/" class="Link-sc-1brdqhf-0 cKRjba">http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05198" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05198</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/kmaninis/OSVOS-caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kmaninis/OSVOS-caffe</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/scaelles/OSVOS-TensorFlow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/scaelles/OSVOS-TensorFlow</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/kmaninis/OSVOS-PyTorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kmaninis/OSVOS-PyTorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DAVIS: Densely Annotated VIdeo Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://davischallenge.org/" class="Link-sc-1brdqhf-0 cKRjba">http://davischallenge.org/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.00675" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.00675</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Segmentation Without Temporal Information</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.06031" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.06031</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Gated Recurrent Networks for Video Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05435" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05435</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Video Object Segmentation from Static Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.02646" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.02646</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Video Segmentation by Gated Recurrent Flow Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.08871" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.08871</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://vision.cs.utexas.edu/projects/fusionseg/" class="Link-sc-1brdqhf-0 cKRjba">http://vision.cs.utexas.edu/projects/fusionseg/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.05384" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.05384</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/suyogduttjain/fusionseg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/suyogduttjain/fusionseg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised learning from video to detect foreground objects in single images</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10901" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10901</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantically-Guided Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.01926" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.01926</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Video Object Segmentation with Visual Memory</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.05737" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.05737</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Flow-free Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.09544" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.09544</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Online Adaptation of Convolutional Neural Networks for Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.09364" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.09364</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Segmentation using Tracked Object Proposals</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR-2017 workshop, DAVIS-2017 Challenge</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06545" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06545</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Segmentation with Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 Workshop, DAVIS Challenge on Video Object Segmentation 2017 (Winning Entry)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.00197" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.00197</a></li><li>github(official, PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/lxx1991/VS-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lxx1991/VS-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pixel-Level Matching for Video Object Segmentation using Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.05137" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.05137</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MaskRNN: Instance Level Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.11187" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.11187</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SegFlow: Joint Learning for Video Object Segmentation and Optical Flow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/site/yihsuantsai/research/iccv17-segflow" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/site/yihsuantsai/research/iccv17-segflow</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.06750" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.06750</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JingchunCheng/SegFlow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JingchunCheng/SegFlow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Semantic Object Segmentation by Self-Adaptation of DCNN</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08180" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08180</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Segment Moving Objects</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01127" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01127</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance Embedding Transfer to Unsupervised Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Southern California &amp; Google Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00908" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00908</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@barvinograd1/instance-embedding-instance-segmentation-without-proposals-31946a7c53e1" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@barvinograd1/instance-embedding-instance-segmentation-without-proposals-31946a7c53e1</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Video Object Segmentation via Network Modulation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Snap Inc. &amp; Northwestern University &amp; Google Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.01218" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.01218</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>intro: CUHK</li><li>keywords: DyeNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.04242" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.04242</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Segmentation with Language Referring Expressions</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08006" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08006</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Video Segmentation Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>keywords: DVSNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00931" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00931</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/XUSean0118/DVSNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/XUSean0118/DVSNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Low-Latency Video Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 Spotlight</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00389" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00389</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.03131" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.03131</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Video Object Segmentation for Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Waterloo</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07780" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07780</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast and Accurate Online Video Object Segmentation via Tracking Parts</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.02323" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.02323</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JingchunCheng/FAVOS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JingchunCheng/FAVOS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ReConvNet: Video Object Segmentation with Spatio-Temporal Features Modulation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR Workshop - DAVIS Challenge 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.05510" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.05510</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Spatio-Temporal Random Fields for Efficient Video Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.03148" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.03148</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Video Object Segmentation by Reference-Guided Mask Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1029.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1029.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/seoungwugoh/RGMP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/seoungwugoh/RGMP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PReMVOS: Proposal-generation, Refinement and Merging for Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.09190" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.09190</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YouTube-VOS: Sequence-to-Sequence Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018. Adobe Research &amp; Snapchat Research &amp; UIUC</li><li>project page:<a target="_blank" rel="noopener noreferrer" href="https://youtube-vos.org/" class="Link-sc-1brdqhf-0 cKRjba">https://youtube-vos.org/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.00461" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.00461</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VideoMatch: Matching based Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.01123" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.01123</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask Propagation Network for Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ByteDance AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.10289" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.10289</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tukey-Inspired Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07958" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07958</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Generative Appearance Model for End-to-end Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11611" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11611</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unseen Object Segmentation in Videos via Transferable Representations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACCV 2018 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.02444" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.02444</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wenz116/TransferSeg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wenz116/TransferSeg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: RWTH Aachen University &amp; Google Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.09513" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.09513</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RVOS: End-to-End Recurrent Network for Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://imatge-upc.github.io/rvos/" class="Link-sc-1brdqhf-0 cKRjba">https://imatge-upc.github.io/rvos/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.05612" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.05612</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: University of Michigan</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.11779" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.11779</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/griffbr/BubbleNets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/griffbr/BubbleNets</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=0kNmm8SBnnU&amp;feature=youtu.be" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=0kNmm8SBnnU&amp;feature=youtu.be</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast video object segmentation with Spatio-Temporal GANs</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.12161" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.12161</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Segmentation using Space-Time Memory Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: Yonsei University &amp; Adobe Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.00607" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.00607</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/seoungwugoh/STM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/seoungwugoh/STM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatiotemporal CNN for Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">[https://arxiv.org/abs/1904.02363]</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Architecture Search of Dynamic Cells for Semantic Video Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02371" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02371</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BoLTVOS: Box-Level Tracking for Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.04552" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.04552</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MAIN: Multi-Attention Instance Network for Video Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05847" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05847</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.08141" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.08141</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: ByteDance AI Lab &amp; UIUC &amp; Adobe Research</li><li>keywords: MaskTrack R-CNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.04804" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.04804</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/youtubevos/MaskTrackRCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/youtubevos/MaskTrackRCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OVSNet : Towards One-Pass Real-Time Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University &amp; SenseTime Research &amp; Tianjin University]</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.10064" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.10064</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Proposal, Tracking and Segmentation (PTS): A Cascaded Network for Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Huazhong University of Science and Technology &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.01203" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.01203</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sydney0zq/PTSNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sydney0zq/PTSNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RANet: Ranking Attention Network for Fast Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.06647" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.06647</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Storife/RANet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Storife/RANet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DMM-Net: Differentiable Mask-Matching Network for Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.12471" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.12471</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.00132" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.00132</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Good Practices for Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ByteDance AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.13583" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.13583</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anchor Diffusion for Unsupervised Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.10895" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.10895</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Spatio-Temporal Embedding for Video Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Cambridge</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.08969" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.08969</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Semantic Video Segmentation with Per-frame Inference</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: The University of Adelaide &amp; Huazhong University of Science and Technology &amp; Microsoft Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.11433" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.11433</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/irfanICMLL/ETC-Real-time-Per-frame-Semantic-video-segmentation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/irfanICMLL/ETC-Real-time-Per-frame-Semantic-video-segmentation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>State-Aware Tracker for Real-Time Video Object Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.00482" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.00482</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MegviiDetection/video_analyst" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MegviiDetection/video_analyst</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Segmentation with Adaptive Feature Bank and Uncertain-Region Refinement</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.07958" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.07958</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SwiftNet: Real-time Video Object Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.04604" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.04604</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SG-Net: Spatial Granularity Network for One-Stage Video Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.10284" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.10284</a></p><h2 id="challenge" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#challenge" color="auto.gray.8" aria-label="Challenge permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Challenge</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DAVIS Challenge on Video Object Segmentation 2017</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://davischallenge.org/challenge2017/publications.html" class="Link-sc-1brdqhf-0 cKRjba">http://davischallenge.org/challenge2017/publications.html</a></p><h1 id="matting" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#matting" color="auto.gray.8" aria-label="Matting permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Matting</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>intro: Beckman Institute for Advanced Science and Technology &amp; Adobe Research</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/view/deepimagematting" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/view/deepimagematting</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.03872" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.03872</a></li><li>github(unofficial): <a target="_blank" rel="noopener noreferrer" href="https://github.com/open-mmlab/mmediting/tree/master/configs/mattors/dim" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/open-mmlab/mmediting/tree/master/configs/mattors/dim</a></li><li>github(unofficial): <a target="_blank" rel="noopener noreferrer" href="https://github.com/foamliu/Deep-Image-Matting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/foamliu/Deep-Image-Matting</a></li><li>github(unofficial): <a target="_blank" rel="noopener noreferrer" href="https://github.com/foamliu/Deep-Image-Matting-PyTorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/foamliu/Deep-Image-Matting-PyTorch</a></li><li>github(unofficial): <a target="_blank" rel="noopener noreferrer" href="https://github.com/huochaitiantang/pytorch-deep-image-matting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huochaitiantang/pytorch-deep-image-matting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Deep Matting for Portrait Animation on Mobile Phone</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia Conference (MM) 2017</li><li>intro: does not need any interaction and can realize real-time matting with 15 fps</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.08289" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.08289</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-time deep hair matting on mobile devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ModiFace Inc, University of Toronto</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.07168" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.07168</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TOM-Net: Learning Transparent Object Matting from a Single Image</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://gychen.org/TOM-Net/" class="Link-sc-1brdqhf-0 cKRjba">http://gychen.org/TOM-Net/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.04636" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.04636</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/guanyingc/TOM-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guanyingc/TOM-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Video Portraits</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SIGGRAPH 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.11714" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.11714</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=qc5P2bvfl44" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=qc5P2bvfl44</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Inductive Guided Filter: Real-time Deep Image Matting with Weakly Annotated Masks on Mobile Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University &amp; Versa</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.06747" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.06747</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Indices Matter: Learning to Index for Deep Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.00672" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.00672</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/poppinace/indexnet_matting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/poppinace/indexnet_matting</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/open-mmlab/mmediting/tree/master/configs/mattors/indexnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/open-mmlab/mmediting/tree/master/configs/mattors/indexnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Disentangled Image Matting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.04686" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.04686</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Natural Image Matting via Guided Contextual Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2001.04069" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2001.04069</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Yaoyi-Li/GCA-Matting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Yaoyi-Li/GCA-Matting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>F, B, Alpha Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.07711" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.07711</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MarcoForte/FBA_Matting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MarcoForte/FBA_Matting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Background Matting: The World is Your Green Screen</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: University of Washington</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://grail.cs.washington.edu/projects/background-matting/" class="Link-sc-1brdqhf-0 cKRjba">https://grail.cs.washington.edu/projects/background-matting/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.00626" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.00626</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/senguptaumd/Background-Matting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/senguptaumd/Background-Matting</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635" class="Link-sc-1brdqhf-0 cKRjba">https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Opacity Propagation for Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.03249" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.03249</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Yaoyi-Li/HOP-Matting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Yaoyi-Li/HOP-Matting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>High-Resolution Deep Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UIUC &amp; Adobe Research &amp; University of Oregon</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.06613" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.06613</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Affinity-Aware Upsampling for Deep Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Adelaide &amp; Huazhong University of Science and Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.14288" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.14288</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-Time High-Resolution Background Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://grail.cs.washington.edu/projects/background-matting-v2/" class="Link-sc-1brdqhf-0 cKRjba">https://grail.cs.washington.edu/projects/background-matting-v2/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.07810" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.07810</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PeterL1n/BackgroundMattingV2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PeterL1n/BackgroundMattingV2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Video Matting via Spatio-Temporal Alignment and Aggregation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.11208" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.11208</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nowsyn/DVM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nowsyn/DVM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Trimap-guided Feature Mining and Fusion Network for Natural Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University &amp; ByteDance Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.00510" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.00510</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Boosting Robustness of Image Matting with Context Assembling and Strong Data Augmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Adelaide &amp; Adobe Inc. &amp; Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2201.06889" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2201.06889</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MatteFormer: Transformer-Based Image Matting via Prior-Tokens</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Seoul National University &amp; NAVER WEBTOON AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.15662" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.15662</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Referring Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Sydney &amp; JD Explore Academy</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.05149" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.05149</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JizhiziLi/RIM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JizhiziLi/RIM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One-Trimap Video Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.13353" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.13353</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Hongje/OTVM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Hongje/OTVM</a></li></ul><h2 id="trimap-free-matting" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#trimap-free-matting" color="auto.gray.8" aria-label="trimap-free matting permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>trimap-free matting</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Human Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.01354" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.01354</a></li><li>github(unofficial): <a target="_blank" rel="noopener noreferrer" href="https://github.com/lizhengwei1992/Semantic_Human_Matting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lizhengwei1992/Semantic_Human_Matting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance Segmentation based Semantic Matting for Compositing Applications</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CRV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05457" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05457</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Late Fusion CNN for Digital Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: Zhejiang University &amp; Alibaba Group &amp; University of Texas at Austin</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_A_Late_Fusion_CNN_for_Digital_Matting_CVPR_2019_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_A_Late_Fusion_CNN_for_Digital_Matting_CVPR_2019_paper.pdf</a></li><li>github(official, Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/yunkezhang/FusionMatting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yunkezhang/FusionMatting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention-Guided Hierarchical Structure Aggregation for Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://wukaoliu.github.io/HAttMatting/" class="Link-sc-1brdqhf-0 cKRjba">https://wukaoliu.github.io/HAttMatting/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Attention-Guided_Hierarchical_Structure_Aggregation_for_Image_Matting_CVPR_2020_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Attention-Guided_Hierarchical_Structure_Aggregation_for_Image_Matting_CVPR_2020_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wukaoliu/CVPR2020-HAttMatting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wukaoliu/CVPR2020-HAttMatting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Boosting Semantic Human Matting with Coarse Annotations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Group &amp; Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.04955" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.04955</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end Animal Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Glance and Focus Matting network (GFM), AM-2k dataset, BG-20k dataset</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.16188" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.16188</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JizhiziLi/animal-matting/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JizhiziLi/animal-matting/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Is a Green Screen Really Necessary for Real-Time Human Matting?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: City University of Hong Kong &amp; SenseTime Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.11961" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.11961</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZHKKKe/MODNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZHKKKe/MODNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-scale Information Assembly for Image Matting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.02391" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.02391</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Salient Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Fynd &amp; University of Michigan</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.12337" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.12337</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask Guided Matting via Progressive Refinement Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: The Johns Hopkins University &amp; Adobe</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.06722" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.06722</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yucornetto/MGMatting" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yucornetto/MGMatting</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Privacy-Preserving Portrait Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Sydney &amp; JD Explore Academy</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.14222" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.14222</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SHI-Labs/Pseudo-IoU-for-Anchor-Free-Object-Detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SHI-Labs/Pseudo-IoU-for-Anchor-Free-Object-Detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Highly Efficient Natural Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.12748" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.12748</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PP-HumanSeg: Connectivity-Aware Portrait Segmentation with a Large-Scale Teleconferencing Video Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2021 workshop</li><li>intro: Baidu, Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.07146" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.07146</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaddlePaddle/PaddleSeg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PaddlePaddle/PaddleSeg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Situational Perception Guided Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: OPPO Research Institute &amp; PicUp.AI &amp; Xmotors</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.09276" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.09276</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PP-Matting: High-Accuracy Natural Image Matting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Inc.</li><li>arixv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.09433" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.09433</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaddlePaddle/PaddleSeg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PaddlePaddle/PaddleSeg</a></li></ul><h1 id="3d-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#3d-segmentation" color="auto.gray.8" aria-label="3D Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>3D Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford University</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://stanford.edu/~rqi/pointnet/" class="Link-sc-1brdqhf-0 cKRjba">http://stanford.edu/~rqi/pointnet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.00593" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.00593</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/charlesq34/pointnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/charlesq34/pointnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DA-RNN: Semantic Mapping with Data Associated Recurrent Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.03098" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.03098</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.07368" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.07368</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SEGCloud: Semantic Segmentation of 3D Point Clouds</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: International Conference of 3D Vision (3DV) 2017 (Spotlight). Stanford University</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://segcloud.stanford.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://segcloud.stanford.edu/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.07563" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.07563</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>3D Instance Segmentation via Multi-task Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: KAUST &amp; ETH Zurich</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.08650" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.08650</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: RWTH Aachen University &amp; Google &amp; Technical University Munich</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://www.vision.rwth-aachen.de/publication/00199/" class="Link-sc-1brdqhf-0 cKRjba">https://www.vision.rwth-aachen.de/publication/00199/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.13867" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.13867</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.01658" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.01658</a></li></ul><h1 id="line-parsing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#line-parsing" color="auto.gray.8" aria-label="Line Parsing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Line Parsing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Line Parsing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>intro: UESTC &amp; UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.11207" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.11207</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Delay-Xili/F-Clip" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Delay-Xili/F-Clip</a></li></ul><h1 id="projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TF Image Segmentation: Image Segmentation framework</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Image Segmentation framework based on Tensorflow and TF-Slim library</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/warmspringwinds/tf-image-segmentation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/warmspringwinds/tf-image-segmentation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>KittiSeg: A Kitti Road Segmentation model implemented in tensorflow.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: MultiNet</li><li>intro: KittiSeg performs segmentation of roads by utilizing an FCN based model.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MarvinTeichmann/KittiBox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MarvinTeichmann/KittiBox</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Segmentation Architectures Implemented in PyTorch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Segnet/FCN/U-Net/Link-Net</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/meetshah1995/pytorch-semseg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/meetshah1995/pytorch-semseg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PyTorch for Semantic Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ZijunDeng/pytorch-semantic-segmentation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZijunDeng/pytorch-semantic-segmentation</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LightNet: Light-weight Networks for Semantic Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://ansleliu.github.io/LightNet.html" class="Link-sc-1brdqhf-0 cKRjba">https://ansleliu.github.io/LightNet.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ansleliu/LightNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ansleliu/LightNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://ansleliu.github.io/LightNet.html" class="Link-sc-1brdqhf-0 cKRjba">https://ansleliu.github.io/LightNet.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ansleliu/LightNetPlusPlus" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ansleliu/LightNetPlusPlus</a></li></ul><h1 id="leaderboard" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#leaderboard" color="auto.gray.8" aria-label="Leaderboard permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Leaderboard</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Segmentation Results: VOC2012 BETA: Competition &quot;comp6&quot; (train on own data)</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6" class="Link-sc-1brdqhf-0 cKRjba">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6</a></p><h1 id="blogs" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#blogs" color="auto.gray.8" aria-label="Blogs permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blogs</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mobile Real-time Video Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2018/03/mobile-real-time-video-segmentation.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2018/03/mobile-real-time-video-segmentation.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Natural Image Segmentation Priors</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://cs.brown.edu/courses/csci2951-t/finals/ghope/" class="Link-sc-1brdqhf-0 cKRjba">http://cs.brown.edu/courses/csci2951-t/finals/ghope/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Segmentation Using DIGITS 5</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://devblogs.nvidia.com/parallelforall/image-segmentation-using-digits-5/" class="Link-sc-1brdqhf-0 cKRjba">https://devblogs.nvidia.com/parallelforall/image-segmentation-using-digits-5/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Segmentation with Tensorflow using CNNs and Conditional Random Fields</strong>
<a target="_blank" rel="noopener noreferrer" href="http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/18/image-segmentation-with-tensorflow-using-cnns-and-conditional-random-fields/" class="Link-sc-1brdqhf-0 cKRjba">http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/18/image-segmentation-with-tensorflow-using-cnns-and-conditional-random-fields/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Networks (FCNs) for Image Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://warmspringwinds.github.io/tensorflow/tf-slim/2017/01/23/fully-convolutional-networks-(fcns)-for-image-segmentation/" class="Link-sc-1brdqhf-0 cKRjba">http://warmspringwinds.github.io/tensorflow/tf-slim/2017/01/23/fully-convolutional-networks-(fcns)-for-image-segmentation/</a></li><li>ipn: <a target="_blank" rel="noopener noreferrer" href="https://github.com/warmspringwinds/tensorflow_notes/blob/master/fully_convolutional_networks.ipynb" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/warmspringwinds/tensorflow_notes/blob/master/fully_convolutional_networks.ipynb</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image segmentation with Neural Net</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@m.zaradzki/image-segmentation-with-neural-net-d5094d571b1e#.s5f711g1q" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@m.zaradzki/image-segmentation-with-neural-net-d5094d571b1e#.s5f711g1q</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mzaradzki/neuralnets/tree/master/vgg_segmentation_keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mzaradzki/neuralnets/tree/master/vgg_segmentation_keras</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A 2017 Guide to Semantic Segmentation with Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review" class="Link-sc-1brdqhf-0 cKRjba">http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review</a></p><h1 id="tutorails--talks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tutorails--talks" color="auto.gray.8" aria-label="Tutorails / Talks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tutorails / Talks</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Unified Architecture for Instance and Semantic Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: FPN</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep learning for image segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PyData Warsaw - Mateusz Opala &amp; Michał Jamroż</li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=W6r_a5crqGI" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=W6r_a5crqGI</a></li></ul><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/28/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>