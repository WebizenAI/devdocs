<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Method backbone test size VOC2007 VOC2010 VOC2012 ILSVRC 2013 MSCOCO 2015 Speed OverFeat 24.3% R-CNN AlexNet 58.5% 53.7% 53.3% 31.4% R-CNN …"/><meta data-react-helmet="true" name="twitter:title" content="Object Detection"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-28T19:22:29.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Method backbone test size VOC2007 VOC2010 VOC2012 ILSVRC 2013 MSCOCO 2015 Speed OverFeat 24.3% R-CNN AlexNet 58.5% 53.7% 53.3% 31.4% R-CNN …"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Object Detection"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Method backbone test size VOC2007 VOC2010 VOC2012 ILSVRC 2013 MSCOCO 2015 Speed OverFeat 24.3% R-CNN AlexNet 58.5% 53.7% 53.3% 31.4% R-CNN …"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,HGjBQ,fdzjHV,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.daTFSy{height:4px;padding:0;margin:24px 0;background-color:#e1e4e8;border:0;}/*!sc*/
data-styled.g29[id="horizontal-rule__HorizontalRule-sc-1731hye-0"]{content:"daTFSy,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.eooEiq{display:block;width:100%;margin:0 0 16px;overflow:auto;}/*!sc*/
.eooEiq th{font-weight:600;}/*!sc*/
.eooEiq th,.eooEiq td{padding:8px 16px;border:1px solid #e1e4e8;}/*!sc*/
.eooEiq tr{background-color:#ffffff;border-top:1px solid #e1e4e8;}/*!sc*/
.eooEiq tr:nth-child(2n){background-color:#f6f8fa;}/*!sc*/
.eooEiq img{background-color:transparent;}/*!sc*/
data-styled.g35[id="table__Table-sc-ixm5yk-0"]{content:"eooEiq,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Object Detection - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">awesomeLists</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#densebox" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DenseBox</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#ohem" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">OHEM</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#r-fcn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">R-FCN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#feature-pyramid-network-fpn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Feature Pyramid Network (FPN)</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#two-stage-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Two-Stage Object Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#r-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">R-CNN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#fast-r-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Fast R-CNN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#faster-r-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Faster R-CNN</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#single-shot-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Single-Shot Object Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolo" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLO</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolov2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLOv2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolov3" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLOv3</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolov4" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLOv4</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolov7" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLOv7</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#ssd" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">SSD</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#retinanet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">RetinaNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#anchor-free" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Anchor-free</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#transformers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Transformers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#non-maximum-suppression-nms" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Non-Maximum Suppression (NMS)</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#nms-free" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">NMS-free</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#adversarial-examples" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Adversarial Examples</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#knowledge-distillation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Knowledge Distillation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#rotated-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Rotated Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#long-tailed-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Long-Tailed Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#weakly-supervised-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Weakly Supervised Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#object-detection-on-mobile-devices" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Object Detection on Mobile Devices</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#object-detection-on-rgb-d" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Object Detection on RGB-D</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#zero-shot-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Zero-Shot Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#visual-relationship-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Visual Relationship Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#face-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mtcnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MTCNN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#detect-small-faces" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Detect Small Faces</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#person-head-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Person Head Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pedestrian-detection--people-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pedestrian Detection / People Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#pedestrian-detection-in-a-crowd" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pedestrian Detection in a Crowd</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#occluded-pedestrian-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Occluded Pedestrian Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#multispectral-pedestrian-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Multispectral Pedestrian Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#vehicle-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#traffic-sign-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Traffic-Sign Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#skeleton-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Skeleton Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#fruit-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Fruit Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#shadow-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Shadow Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#others-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Others Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#object-proposal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Object Proposal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#localization" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Localization</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tutorials--talks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorials / Talks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#leaderboard" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Leaderboard</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tools" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tools</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Object Detection</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#densebox" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">DenseBox</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#ohem" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">OHEM</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#r-fcn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">R-FCN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#feature-pyramid-network-fpn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Feature Pyramid Network (FPN)</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#two-stage-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Two-Stage Object Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#r-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">R-CNN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#fast-r-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Fast R-CNN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#faster-r-cnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Faster R-CNN</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#single-shot-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Single-Shot Object Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolo" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLO</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolov2" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLOv2</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolov3" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLOv3</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolov4" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLOv4</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#yolov7" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">YOLOv7</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#ssd" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">SSD</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#retinanet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">RetinaNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#anchor-free" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Anchor-free</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#transformers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Transformers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#non-maximum-suppression-nms" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Non-Maximum Suppression (NMS)</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#nms-free" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">NMS-free</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#adversarial-examples" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Adversarial Examples</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#knowledge-distillation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Knowledge Distillation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#rotated-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Rotated Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#long-tailed-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Long-Tailed Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#weakly-supervised-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Weakly Supervised Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#object-detection-on-mobile-devices" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Object Detection on Mobile Devices</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#object-detection-on-rgb-d" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Object Detection on RGB-D</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#zero-shot-object-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Zero-Shot Object Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#visual-relationship-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Visual Relationship Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#face-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mtcnn" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MTCNN</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#detect-small-faces" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Detect Small Faces</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#person-head-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Person Head Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pedestrian-detection--people-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pedestrian Detection / People Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#pedestrian-detection-in-a-crowd" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pedestrian Detection in a Crowd</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#occluded-pedestrian-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Occluded Pedestrian Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#multispectral-pedestrian-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Multispectral Pedestrian Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#vehicle-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#traffic-sign-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Traffic-Sign Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#skeleton-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Skeleton Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#fruit-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Fruit Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#shadow-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Shadow Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#others-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Others Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#object-proposal" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Object Proposal</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#localization" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Localization</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tutorials--talks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tutorials / Talks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#leaderboard" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Leaderboard</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tools" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tools</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li></ul></div></div><table class="table__Table-sc-ixm5yk-0 eooEiq"><thead><tr><th align="center">Method</th><th align="center">backbone</th><th align="center">test size</th><th align="center">VOC2007</th><th align="center">VOC2010</th><th align="center">VOC2012</th><th align="center">ILSVRC 2013</th><th align="center">MSCOCO 2015</th><th align="center">Speed</th></tr></thead><tbody><tr><td align="center">OverFeat</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">24.3%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">R-CNN</td><td align="center">AlexNet</td><td align="center"></td><td align="center">58.5%</td><td align="center">53.7%</td><td align="center">53.3%</td><td align="center">31.4%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">R-CNN</td><td align="center">VGG16</td><td align="center"></td><td align="center">66.0%</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">SPP_net</td><td align="center">ZF-5</td><td align="center"></td><td align="center">54.2%</td><td align="center"></td><td align="center"></td><td align="center">31.84%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">DeepID-Net</td><td align="center"></td><td align="center"></td><td align="center">64.1%</td><td align="center"></td><td align="center"></td><td align="center">50.3%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">NoC</td><td align="center">73.3%</td><td align="center"></td><td align="center">68.8%</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Fast-RCNN</td><td align="center">VGG16</td><td align="center"></td><td align="center">70.0%</td><td align="center">68.8%</td><td align="center">68.4%</td><td align="center"></td><td align="center">19.7%(@<!-- -->[0.5-0.95]<!-- -->), 35.9%(@0.5)</td><td align="center"></td></tr><tr><td align="center">MR-CNN</td><td align="center">78.2%</td><td align="center"></td><td align="center">73.9%</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Faster-RCNN</td><td align="center">VGG16</td><td align="center"></td><td align="center">78.8%</td><td align="center"></td><td align="center">75.9%</td><td align="center"></td><td align="center">21.9%(@<!-- -->[0.5-0.95]<!-- -->), 42.7%(@0.5)</td><td align="center">198ms</td></tr><tr><td align="center">Faster-RCNN</td><td align="center">ResNet101</td><td align="center"></td><td align="center">85.6%</td><td align="center"></td><td align="center">83.8%</td><td align="center"></td><td align="center">37.4%(@<!-- -->[0.5-0.95]<!-- -->), 59.0%(@0.5)</td><td align="center"></td></tr><tr><td align="center">YOLO</td><td align="center"></td><td align="center"></td><td align="center">63.4%</td><td align="center"></td><td align="center">57.9%</td><td align="center"></td><td align="center"></td><td align="center">45 fps</td></tr><tr><td align="center">YOLO VGG-16</td><td align="center"></td><td align="center"></td><td align="center">66.4%</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">21 fps</td></tr><tr><td align="center">YOLOv2</td><td align="center"></td><td align="center">448x448</td><td align="center">78.6%</td><td align="center"></td><td align="center">73.4%</td><td align="center"></td><td align="center">21.6%(@<!-- -->[0.5-0.95]<!-- -->), 44.0%(@0.5)</td><td align="center">40 fps</td></tr><tr><td align="center">SSD</td><td align="center">VGG16</td><td align="center">300x300</td><td align="center">77.2%</td><td align="center"></td><td align="center">75.8%</td><td align="center"></td><td align="center">25.1%(@<!-- -->[0.5-0.95]<!-- -->), 43.1%(@0.5)</td><td align="center">46 fps</td></tr><tr><td align="center">SSD</td><td align="center">VGG16</td><td align="center">512x512</td><td align="center">79.8%</td><td align="center"></td><td align="center">78.5%</td><td align="center"></td><td align="center">28.8%(@<!-- -->[0.5-0.95]<!-- -->), 48.5%(@0.5)</td><td align="center">19 fps</td></tr><tr><td align="center">SSD</td><td align="center">ResNet101</td><td align="center">300x300</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">28.0%(@<!-- -->[0.5-0.95]<!-- -->)</td><td align="center">16 fps</td></tr><tr><td align="center">SSD</td><td align="center">ResNet101</td><td align="center">512x512</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">31.2%(@<!-- -->[0.5-0.95]<!-- -->)</td><td align="center">8 fps</td></tr><tr><td align="center">DSSD</td><td align="center">ResNet101</td><td align="center">300x300</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">28.0%(@<!-- -->[0.5-0.95]<!-- -->)</td><td align="center">8 fps</td></tr><tr><td align="center">DSSD</td><td align="center">ResNet101</td><td align="center">500x500</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">33.2%(@<!-- -->[0.5-0.95]<!-- -->)</td><td align="center">6 fps</td></tr><tr><td align="center">ION</td><td align="center"></td><td align="center"></td><td align="center">79.2%</td><td align="center"></td><td align="center">76.4%</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">CRAFT</td><td align="center"></td><td align="center"></td><td align="center">75.7%</td><td align="center"></td><td align="center">71.3%</td><td align="center">48.5%</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">OHEM</td><td align="center"></td><td align="center"></td><td align="center">78.9%</td><td align="center"></td><td align="center">76.3%</td><td align="center"></td><td align="center">25.5%(@<!-- -->[0.5-0.95]<!-- -->), 45.9%(@0.5)</td><td align="center"></td></tr><tr><td align="center">R-FCN</td><td align="center">ResNet50</td><td align="center"></td><td align="center">77.4%</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">0.12sec(K40), 0.09sec(TitianX)</td></tr><tr><td align="center">R-FCN</td><td align="center">ResNet101</td><td align="center"></td><td align="center">79.5%</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">0.17sec(K40), 0.12sec(TitianX)</td></tr><tr><td align="center">R-FCN(ms train)</td><td align="center">ResNet101</td><td align="center"></td><td align="center">83.6%</td><td align="center"></td><td align="center">82.0%</td><td align="center"></td><td align="center">31.5%(@<!-- -->[0.5-0.95]<!-- -->), 53.2%(@0.5)</td><td align="center"></td></tr><tr><td align="center">PVANet 9.0</td><td align="center"></td><td align="center"></td><td align="center">84.9%</td><td align="center"></td><td align="center">84.2%</td><td align="center"></td><td align="center"></td><td align="center">750ms(CPU), 46ms(TitianX)</td></tr><tr><td align="center">RetinaNet</td><td align="center">ResNet101-FPN</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Light-Head R-CNN</td><td align="center">Xception<!-- -->*</td><td align="center">800/1200</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">31.5%@<!-- -->[0.5:0.95]</td><td align="center">95 fps</td></tr><tr><td align="center">Light-Head R-CNN</td><td align="center">Xception<!-- -->*</td><td align="center">700/1100</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">30.7%@<!-- -->[0.5:0.95]</td><td align="center">102 fps</td></tr></tbody></table><h1 id="papers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers" color="auto.gray.8" aria-label="Papers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Neural Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1312.6229" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1312.6229</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sermanet/OverFeat" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sermanet/OverFeat</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://cilvr.nyu.edu/doku.php?id=software:overfeat:start" class="Link-sc-1brdqhf-0 cKRjba">http://cilvr.nyu.edu/doku.php?id=software:overfeat:start</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scalable Object Detection using Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: first MultiBox. Train a CNN to predict Region of Interest.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1312.2249" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1312.2249</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/multibox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/google/multibox</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scalable, High-Quality Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: second MultiBox</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.1441" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.1441</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/multibox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/google/multibox</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2014 / TPAMI 2015</li><li>keywords: SPP-Net</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1406.4729" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1406.4729</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ShaoqingRen/SPP_net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ShaoqingRen/SPP_net</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://zhangliliang.com/2014/09/13/paper-note-sppnet/" class="Link-sc-1brdqhf-0 cKRjba">http://zhangliliang.com/2014/09/13/paper-note-sppnet/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PAMI 2016</li><li>intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.5661" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.5661</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detectors Emerge in Deep Scene CNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.6856" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.6856</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://places.csail.mit.edu/slide_iclr2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://places.csail.mit.edu/slide_iclr2015.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>project(code+data): <a target="_blank" rel="noopener noreferrer" href="https://www.cs.toronto.edu/~yukun/segdeepm.html" class="Link-sc-1brdqhf-0 cKRjba">https://www.cs.toronto.edu/~yukun/segdeepm.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1502.04275" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1502.04275</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/YknZhu/segDeepM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YknZhu/segDeepM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection Networks on Convolutional Feature Maps</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI 2015</li><li>keywords: NoC</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.06066" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.06066</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.03293" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.03293</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/YutingZhang/fgs-obj" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YutingZhang/fgs-obj</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepBox: Learning Objectness with Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: DeepBox</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.02146" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.02146</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/weichengkuo/DeepBox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/weichengkuo/DeepBox</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object detection via a multi-region &amp; semantic segmentation-aware CNN model</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>keywords: MR-CNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.01749" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.01749</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gidariss/mrcnn-object-detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gidariss/mrcnn-object-detection</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/" class="Link-sc-1brdqhf-0 cKRjba">http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/" class="Link-sc-1brdqhf-0 cKRjba">http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AttentionNet: Aggregating Weak Directions for Accurate Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>intro: state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 human detection task</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.07704" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.07704</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://image-net.org/challenges/talks/lunit-kaist-slide.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://image-net.org/challenges/talks/lunit-kaist-slide.pdf</a></li></ul><h2 id="densebox" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#densebox" color="auto.gray.8" aria-label="DenseBox permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>DenseBox</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DenseBox: Unifying Landmark Localization with End to End Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.04874" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.04874</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1mgoWWsS" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1mgoWWsS</a></li><li>KITTI result: <a target="_blank" rel="noopener noreferrer" href="http://www.cvlibs.net/datasets/kitti/eval_object.php" class="Link-sc-1brdqhf-0 cKRjba">http://www.cvlibs.net/datasets/kitti/eval_object.php</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding-box regression
and 1.15s per image with it&quot;.</li><li>keywords: Inside-Outside Net (ION)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.04143" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.04143</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf</a></li><li>coco-leaderboard: <a target="_blank" rel="noopener noreferrer" href="http://mscoco.org/dataset/#detections-leaderboard" class="Link-sc-1brdqhf-0 cKRjba">http://mscoco.org/dataset/#detections-leaderboard</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Object Detection Using Adjacency and Zoom Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016. AZ-Net</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.07711" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.07711</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/luyongxi/az-net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/luyongxi/az-net</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=YmFtuNwxaNM" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=YmFtuNwxaNM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>G-CNN: an Iterative Grid Based Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.07729" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.07729</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>We don&#x27;t need no bounding-boxes: Training object class detectors using only human verification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.08405" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.08405</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.00600" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.00600</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A MultiPath Network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2016. Facebook AI Research (FAIR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.02135" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.02135</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/multipathnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/multipathnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CRAFT Objects from Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016. Cascade Region-proposal-network And FasT-rcnn. an extension of Faster R-CNN</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://byangderek.github.io/projects/craft.html" class="Link-sc-1brdqhf-0 cKRjba">http://byangderek.github.io/projects/craft.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.03239" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.03239</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/byangderek/CRAFT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/byangderek/CRAFT</a></li></ul><h2 id="ohem" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#ohem" color="auto.gray.8" aria-label="OHEM permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>OHEM</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Region-based Object Detectors with Online Hard Example Mining</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016 Oral. Online hard example mining (OHEM)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.03540" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.03540</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf</a></li><li>github(Official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/abhi2610/ohem" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/abhi2610/ohem</a></li><li>author page: <a target="_blank" rel="noopener noreferrer" href="http://abhinav-shrivastava.info/" class="Link-sc-1brdqhf-0 cKRjba">http://abhinav-shrivastava.info/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>S-OHEM: Stratified Online Hard Example Mining for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.02233" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.02233</a></p><hr class="horizontal-rule__HorizontalRule-sc-1731hye-0 daTFSy"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>keywords: scale-dependent pooling  (SDP), cascaded rejection classifiers (CRC)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf</a></li></ul><h2 id="r-fcn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#r-fcn" color="auto.gray.8" aria-label="R-FCN permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>R-FCN</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>R-FCN: Object Detection via Region-based Fully Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.06409" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.06409</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/daijifeng001/R-FCN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daijifeng001/R-FCN</a></li><li>github(MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/msracver/Deformable-ConvNets/tree/master/rfcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/msracver/Deformable-ConvNets/tree/master/rfcn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Orpine/py-R-FCN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Orpine/py-R-FCN</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PureDiors/pytorch_RFCN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PureDiors/pytorch_RFCN</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bharatsingh430/py-R-FCN-multiGPU" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bharatsingh430/py-R-FCN-multiGPU</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xdever/RFCN-tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xdever/RFCN-tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>R-FCN-3000 at 30fps: Decoupling Detection and Classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01802" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01802</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recycle deep features for better object detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.05066" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.05066</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>intro: 640×480: 15 fps, 960×720: 8 fps</li><li>keywords: MS-CNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.07155" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.07155</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhaoweicai/mscnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhaoweicai/mscnn</a></li><li>poster: <a target="_blank" rel="noopener noreferrer" href="http://www.eccv2016.org/files/posters/P-2B-38.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eccv2016.org/files/posters/P-2B-38.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-stage Object Detection with Group Recursive Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: VOC2007: 78.6%, VOC2012: 74.9%</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.05159" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.05159</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2017. SubCNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.04693" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.04693</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tanshen/SubCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tanshen/SubCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PVANet: Lightweight Deep Neural Networks for Real-time Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural Networks (EMDNN).
Continuation of <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1608.08021" class="Link-sc-1brdqhf-0 cKRjba">arXiv:1608.08021</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08588" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08588</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sanghoon/pva-faster-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sanghoon/pva-faster-rcnn</a></li><li>leaderboard(PVANet 9.0): <a target="_blank" rel="noopener noreferrer" href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4" class="Link-sc-1brdqhf-0 cKRjba">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gated Bi-directional CNN for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Chinese University of Hong Kong &amp; Sensetime Group Limited</li><li>keywords: GBD-Net</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22" class="Link-sc-1brdqhf-0 cKRjba">http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="https://pan.baidu.com/s/1dFohO7v" class="Link-sc-1brdqhf-0 cKRjba">https://pan.baidu.com/s/1dFohO7v</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Crafting GBD-Net for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: winner of the ImageNet object detection challenge of 2016. CUImage and CUVideo</li><li>intro: gated bi-directional CNN (GBD-Net)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.02579" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.02579</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/craftGBD/craftGBD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/craftGBD/craftGBD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>StuffNet: Using &#x27;Stuff&#x27; to Improve Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.05861" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.05861</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.09609" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.09609</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Object Detection with Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Deep Reinforcement Learning Workshop (NIPS 2016)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://imatge-upc.github.io/detection-2016-nipsws/" class="Link-sc-1brdqhf-0 cKRjba">https://imatge-upc.github.io/detection-2016-nipsws/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.03718" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.03718</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning" class="Link-sc-1brdqhf-0 cKRjba">http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/imatge-upc/detection-2016-nipsws" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/imatge-upc/detection-2016-nipsws</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://jorditorres.org/nips/" class="Link-sc-1brdqhf-0 cKRjba">http://jorditorres.org/nips/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to detect and localize many objects from few examples</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05664" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05664</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Speed/accuracy trade-offs for modern convolutional object detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.10012" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.10012</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01051" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01051</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/BichenWuUCB/squeezeDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/BichenWuUCB/squeezeDet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fregu856/2D_detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fregu856/2D_detection</a></li></ul><h2 id="feature-pyramid-network-fpn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#feature-pyramid-network-fpn" color="auto.gray.8" aria-label="Feature Pyramid Network (FPN) permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Feature Pyramid Network (FPN)</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Pyramid Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03144" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03144</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Feature Pyramid Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University &amp; Noah’s Ark Lab &amp; Westlake University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.00779" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.00779</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Implicit Feature Pyramid Network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MEGVII Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.13563" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.13563</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>You Should Look at All Objects</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2022</li><li>intro: The University of Hong Kong &amp; Bytedance &amp; University of Rochester</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.07889" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.07889</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CharlesPikachu/YSLAO" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CharlesPikachu/YSLAO</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Action-Driven Object Detection with Top-Down Visual Attentions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.06704" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.06704</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Skip Connections: Top-Down Modulation for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; UC Berkeley &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.06851" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.06851</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Wide-Residual-Inception Networks for Real-time Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Inha University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.01243" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.01243</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attentional Network for Visual Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland &amp; Mitsubishi Electric Research Laboratories</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.01478" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.01478</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Chained Deep Features and Classifiers for Cascade in Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keykwords: CC-Net</li><li>intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.07054" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.07054</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017 (poster)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10295" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10295</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03944" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03944</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial Memory for Context Reasoning in Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.04224" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.04224</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.05775" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.05775</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Embedded Vision Workshop in CVPR. UC San Diego &amp; Qualcomm Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.05922" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.05922</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Point Linking Network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Point Linking Network (PLN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.03646" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.03646</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Perceptual Generative Adversarial Networks for Small Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.05274" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.05274</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Few-shot Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.08249" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.08249</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Yes-Net: An effective Detector Based on Global Information</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.09180" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.09180</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards lightweight convolutional neural networks for object detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.01395" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.01395</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RON: Reverse Connection with Objectness Prior Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.01691" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.01691</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/taokong/RON" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/taokong/RON</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deformable Part-based Fully Convolutional Network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017 (oral). Sorbonne Universités &amp; CEDRIC</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06175" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06175</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06399" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06399</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Scale Approximation for Object Detection in CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>keywords: Recurrent Scale Approximation (RSA)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09531" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09531</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sciencefans/RSA-for-object-detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sciencefans/RSA-for-object-detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DSOD: Learning Deeply Supervised Object Detectors from Scratch</strong></p><img src="https://user-images.githubusercontent.com/3794909/28934967-718c9302-78b5-11e7-89ee-8b514e53e23c.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. Fudan University &amp; Tsinghua University &amp; Intel Labs China</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.01241" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.01241</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/szq0214/DSOD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/szq0214/DSOD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection from Scratch with Deep Supervision</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.09294" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.09294</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CoupleNet: Coupling Global Structure with Local Parts for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02863" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02863</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Incremental Learning of Object Detectors without Catastrophic Forgetting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. Inria</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.06977" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.06977</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.04347" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.04347</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.05788" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.05788</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Zoom-in Network for Fast Object Detection in Large Images</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05187" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05187</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Annotation Object Detection with Web Knowledge Transfer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NTU, Singapore &amp; Amazon</li><li>keywords: multi-instance multi-label domain adaption learning framework</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05954" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05954</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MegDet: A Large Mini-Batch Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peking University &amp; Tsinghua University &amp; Megvii Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07240" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07240</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Receptive Field Block Net for Accurate and Fast Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: RFBNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07767" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07767</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//ruinmessi/RFBNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//ruinmessi/RFBNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Analysis of Scale Invariance in Object Detection - SNIP</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08189" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08189</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bharatsingh430/snip" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bharatsingh430/snip</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Selective Networks for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08879" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08879</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Rotation Invariant Detector with Rotatable Bounding Box</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09405" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09405</a></li><li>github(official, Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/liulei01/DRBox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liulei01/DRBox</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scalable Object Detection for Stylized Objects</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft AI &amp; Research Munich</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09822" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09822</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00886" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00886</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/szq0214/GRP-DSOD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/szq0214/GRP-DSOD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Regionlets for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: region selection network, gating network</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02408" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02408</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training and Testing Object Detectors with Virtual Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE/CAA Journal of Automatica Sinica</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.08470" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.08470</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-Scale Object Discovery and Detector Adaptation from Unlabeled Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: object mining, object tracking, unsupervised object discovery by appearance-based clustering, self-supervised detector adaptation</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.08832" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.08832</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spot the Difference by Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; JD Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.01051" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.01051</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Localization-Aware Active Learning for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.05124" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.05124</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection with Mask-based Feature Encoding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.03934" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.03934</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LSTD: A Low-Shot Transfer Detector for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.01529" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.01529</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pseudo Mask Augmented Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05858" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05858</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting RCNN: On Awakening the Classification Power of Faster RCNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>keywords: DCR V1</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06799" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06799</a></li><li>github(official, MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/bowenc0221/Decoupled-Classification-Refinement" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bowenc0221/Decoupled-Classification-Refinement</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Decoupled Classification Refinement: Hard False Positive Suppression for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: DCR V2</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.04002" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.04002</a></li><li>github(official, MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/bowenc0221/Decoupled-Classification-Refinement" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bowenc0221/Decoupled-Classification-Refinement</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Region Features for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peking University &amp; MSRA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.07066" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.07066</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection for Comics using Manga109 Annotations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Tokyo &amp; National Institute of Informatics, Japan</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08670" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08670</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Task-Driven Super Resolution: Object Detection in Low-resolution Images</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.11316" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.11316</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transferring Common-Sense Knowledge for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.01077" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.01077</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-scale Location-aware Kernel Representation for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00428" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00428</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Hwang64/MLKP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Hwang64/MLKP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Loss Rank Mining: A General Hard Example Mining Method for Real-time Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: National University of Defense Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.04606" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.04606</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DetNet: A Backbone network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; Megvii Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.06215" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.06215</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AdvDetPatch: Attacking Object Detectors with Adversarial Patches</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.02299" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.02299</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attacking Object Detectors via Imperceptible Patches on Background</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.05966" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.05966</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Physical Adversarial Examples for Object Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WOOT 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.07769" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.07769</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object detection at 200 Frames Per Second</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: United Technologies Research Center-Ireland</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06361" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06361</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection using Domain Randomization and Generative Adversarial Refinement of Synthetic Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 Deep Vision Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.11778" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.11778</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SNIPER: Efficient Multi-Scale Training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland</li><li>keywords: SNIPER (Scale Normalization for Image Pyramid with Efficient Resampling)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.09300" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.09300</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mahyarnajibi/SNIPER" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mahyarnajibi/SNIPER</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Soft Sampling for Robust Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.06986" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.06986</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MetaAnchor: Learning to Detect Objects with Customized Anchors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii Inc (Face++) &amp; Fudan University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.00980" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.00980</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Localization Recall Precision (LRP): A New Performance Metric for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018. Middle East Technical University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.01696" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.01696</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cancam/LRP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cancam/LRP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pooling Pyramid Network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google AI Perception</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.03284" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.03284</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Modeling Visual Context is Key to Augmenting Object Detection Datasets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.07428" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.07428</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Acquisition of Localization Confidence for Accurate Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.11590" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.11590</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vacancy/PreciseRoIPooling" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vacancy/PreciseRoIPooling</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CornerNet: Detecting Objects as Paired Keypoints</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>keywords: IoU-Net, PreciseRoIPooling</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.01244" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.01244</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/umich-vl/CornerNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/umich-vl/CornerNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Hard Example Mining from Videos for Improved Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.04285" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.04285</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SAN: Learning Relationship between Convolutional Features for Multi-Scale Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.04974" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.04974</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Survey of Modern Object Detection Literature using Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.07256" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.07256</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tiny-DSOD: Lightweight Object Detection for Resource-Restricted Usages</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.11013" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.11013</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lyxok1/Tiny-DSOD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lyxok1/Tiny-DSOD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Feature Pyramid Reconfiguration for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.07993" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.07993</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MDCN: Multi-Scale, Deep Inception Convolutional Neural Networks for Efficient Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.01791" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.01791</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.03193" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.03193</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Generic Object Detection: A Survey</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.02165" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.02165</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Confidence-Calibrated Classifier for Detecting Out-of-Distribution Samples</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://github.com/alinlab/Confident_classifier" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/alinlab/Confident_classifier</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast and accurate object detection in high resolution 4K and 8K video using GPUs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Best Paper Finalist at IEEE High Performance Extreme Computing Conference (HPEC) 2018</li><li>intro: Carnegie Mellon University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.10551" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.10551</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hybrid Knowledge Routed Modules for Large-scale Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.12681" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.12681</a></li><li>github(official, PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/chanyn/HKRM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chanyn/HKRM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BAN: Focusing on Boundary Context for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.05243" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.05243</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>R2CNN++: Multi-Dimensional Attention Based Rotation Invariant Detector with Robust Anchor Strategy</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07126" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07126</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DetectionTeamUCAS/R2CNN-Plus-Plus_Tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DetectionTeamUCAS/R2CNN-Plus-Plus_Tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeRPN: Taking a further step toward more general object detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>intro: South China University of Technology</li><li>ariv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.06700" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.06700</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HCIILAB/DeRPN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HCIILAB/DeRPN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Efficient Object Detection Using Selective Attention</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07502" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07502</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sampling Techniques for Large-Scale Object Detection from Sparsely Annotated Objects</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.10862" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.10862</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Coarse-to-Fine Non-Local Module for the Detection of Small Objects</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12152" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12152</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Regionlets: Blended Representation and Deep Learning for Generic Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11318" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11318</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transferable Adversarial Attacks for Image and Video Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12641</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anchor Box Optimization for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Illinois at Urbana-Champaign &amp; Microsoft Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.00469" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.00469</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AutoFocus: Efficient Multi-Scale Inference</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.01600" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.01600</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Few-shot Object Detection via Feature Reweighting</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.01866" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.01866</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Practical Adversarial Attack Against Object Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.10217" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.10217</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale-Aware Trident Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Chinese Academy of Sciences &amp; TuSimple</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.01892" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.01892</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TuSimple/simpledet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TuSimple/simpledet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Region Proposal by Guided Anchoring</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: CUHK - SenseTime Joint Lab &amp; Amazon Rekognition &amp; Nanyang Technological University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.03278" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.03278</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bottom-up Object Detection by Grouping Extreme and Center Points</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: ExtremeNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.08043" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.08043</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xingyizhou/ExtremeNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xingyizhou/ExtremeNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bag of Freebies for Training Object Detection Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Amazon Web Services</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.04103" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.04103</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Augmentation for small object detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.07296" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.07296</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.09630" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.09630</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TuSimple</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.05831" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.05831</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tusimple/simpledet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tusimple/simpledet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Toronto</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.03838" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.03838</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DetNAS: Neural Architecture Search on Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese Academy of Sciences &amp; Megvii Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.10979" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.10979</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ThunderNet: Towards Real-time Generic Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.11752" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.11752</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Intertwiner for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2019</li><li>intro: CUHK &amp; SenseTime &amp; The University of Sydney</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.11851" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.11851</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Object Detection with Inverted Attention</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.12255" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.12255</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What Object Should I Use? - Task Driven Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.03000" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.03000</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Universal Object Detection by Domain Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.04402" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.04402</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Prime Sample Attention in Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.04821" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.04821</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BAOD: Budget-Aware Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05443" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05443</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Analysis of Pre-Training on Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05871" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05871</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.06883" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.06883</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: Google Brain</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.07392" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.07392</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Objects as Points</strong></p><img src="https://raw.githubusercontent.com/xingyizhou/CenterNet/master/readme/fig2.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Object detection, 3D detection, and pose estimation using center point detection</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.07850" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.07850</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xingyizhou/CenterNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xingyizhou/CenterNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>intro: ZF Friedrichshafen AG, Artificial Intelligence Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.05060" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.05060</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CenterNet: Object Detection with Keypoint Triplets</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CenterNet: Keypoint Triplets for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.08189" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.08189</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Duankaiwen/CenterNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Duankaiwen/CenterNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CornerNet-Lite: Efficient Keypoint Based Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Princeton University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.08900" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.08900</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/princeton-vl/CornerNet-Lite" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/princeton-vl/CornerNet-Lite</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CenterNet++ for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.08394" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.08394</a></li><li>github; <a target="_blank" rel="noopener noreferrer" href="https://github.com/Duankaiwen/PyCenterNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Duankaiwen/PyCenterNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Automated Focal Loss for Image based Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.09048" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.09048</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring Object Relation in Mean Teacher for Cross-Domain Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.11245" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.11245</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 CEFRL Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.09730" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.09730</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RepPoints: Point Set Representation for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: Peking University &amp; Tsinghua University &amp; Microsoft Research Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.11490" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.11490</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/RepPoints" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/microsoft/RepPoints</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dense RepPoints: Representing Visual Objects with Dense Point Sets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peking University &amp; CUHK &amp; Zhejiang University &amp; Shanghai Jiao Tong University &amp; University of Toronto &amp; MSRA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.11473" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.11473</a></li><li>github(official, mmdetection): <a target="_blank" rel="noopener noreferrer" href="https://github.com/justimyhxu/Dense-RepPoints" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/justimyhxu/Dense-RepPoints</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RepPoints V2: Verification Meets Regression for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft Research Asia &amp; Peking University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.08508" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.08508</a></li><li>github(official, mmdetection): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Scalsol/RepPointsV2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Scalsol/RepPointsV2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection in 20 Years: A Survey</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.05055" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.05055</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Light-Weight RetinaNet for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.10011" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.10011</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Data Augmentation Strategies for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research, Brain Team</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.11172" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.11172</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/tpu/tree/master/models/official/detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/tpu/tree/master/models/official/detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Adversarially Robust Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: Baidu Research, Sunnyvale USA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.10310" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.10310</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-adversarial Faster-RCNN for Unrestricted Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.10343" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.10343</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object as Distribution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2019</li><li>intro: MIT</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.12929" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.12929</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05217" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05217</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05612" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05612</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Thinklab-SJTU/R3Det_Tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Thinklab-SJTU/R3Det_Tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://yangxue0827.github.io/SCRDet++.html" class="Link-sc-1brdqhf-0 cKRjba">https://yangxue0827.github.io/SCRDet++.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.13316" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.13316</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Relation Distillation Networks for Video Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.09511" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.09511</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Imbalance Problems in Object Detection: A Review</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.00169" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.00169</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kemaloksuz/ObjectDetectionImbalance" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kemaloksuz/ObjectDetectionImbalance</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FreeAnchor: Learning to Match Anchors for Visual Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.02466" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.02466</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.02293" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.02293</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.00597" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.00597</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CBNet: A Novel Composite Backbone Network Architecture for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>keywords: Composite Backbone Network (CBNet)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.03625" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.03625</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://aaai.org/Papers/AAAI/2020GB/AAAI-LiuY.1833.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://aaai.org/Papers/AAAI/2020GB/AAAI-LiuY.1833.pdf</a></li><li>github(Caffe2): <a target="_blank" rel="noopener noreferrer" href="https://github.com/PKUbahuangliuhe/CBNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PKUbahuangliuhe/CBNet</a></li><li>github(mmdetection): <a target="_blank" rel="noopener noreferrer" href="https://github.com/VDIGPKU/CBNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/VDIGPKU/CBNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CBNetV2: A Composite Backbone Network Architecture for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.00420" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.00420</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/VDIGPKU/CBNetV2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/VDIGPKU/CBNetV2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A System-Level Solution for Low-Power Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019 Low-Power Computer Vision Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.10964" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.10964</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anchor Loss: Modulating Loss Scale based on Prediction Difficulty</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.11155" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.11155</a></li><li>github(Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/slryou41/AnchorLoss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/slryou41/AnchorLoss</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.08287" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.08287</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zzh-tju/DIoU" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Zzh-tju/DIoU</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zzh-tju/CIoU" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Zzh-tju/CIoU</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zzh-tju/DIoU-darknet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Zzh-tju/DIoU-darknet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Curriculum Self-Paced Learning for Cross-Domain Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.06849" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.06849</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multiple Anchor Learning for Visual Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.02252" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.02252</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google AI &amp; Google Brain</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.01106" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.01106</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AugFPN: Improving Multi-scale Feature Learning for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: CASIA &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.05384" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.05384</a></li><li>github(official, mmdetection): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Gus-Guo/AugFPN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Gus-Guo/AugFPN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection as a Positive-Unlabeled Problem</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.04672" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.04672</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Universal-RCNN: Universal Object Detector via Transferable Graph R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>intro: Huawei Noah’s Ark Lab &amp; South China University of Technology &amp; Sun Yat-Sen University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.07417" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.07417</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BiDet: An Efficient Binarized Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.03961" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.03961</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZiweiWangTHU/BiDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZiweiWangTHU/BiDet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting the Sibling Head in Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 &amp; Method of Champion of OpenImage Challenge 2019, detection track</li><li>intro: SenseTime X-Lab &amp; CUHK</li><li>keywords: task-aware spatial disentanglement (TSD)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.07540" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.07540</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Sense-X/TSD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Sense-X/TSD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Extended Feature Pyramid Network for Small Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.07021" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.07021</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SaccadeNet: A Fast and Accurate Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland &amp; Wormpex AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.12125" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.12125</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale-Equalizing Pyramid Convolution for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: SenseTime Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.03101" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.03101</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jshilong/SEPC" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jshilong/SEPC</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Refinement Network for Oriented and Densely Packed Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 oral</li><li>keywords: SKU110K-R</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.09973" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.09973</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Anymake/DRN_CVPR2020" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Anymake/DRN_CVPR2020</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Object Detection under Occlusion with Context-Aware CompositionalNets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: Johns Hopkins University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.11643" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.11643</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University &amp; Google Research</li><li>intro: COCO test-dev 54.7% box AP</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.02334" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.02334</a></li><li>github(official, mmdetection): <a target="_blank" rel="noopener noreferrer" href="https://github.com/joe-siyuan-qiao/DetectoRS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/joe-siyuan-qiao/DetectoRS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Unified Sample Weighting Network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.06568" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.06568</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/caiqi/sample-weighting-network" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/caiqi/sample-weighting-network</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>2nd Place Solution for Waymo Open Dataset Challenge -- 2D Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Horizon Robotics Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.15507" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.15507</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.01571" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.01571</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AQD: Towards Accurate Quantized Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: South China University of Technology &amp; University of Adelaide &amp; Monash University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.06919" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.06919</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/blueardour/model-quantization" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/blueardour/model-quantization</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Probabilistic Anchor Assignment with IoU Prediction for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.08103" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.08103</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/kkhoot/PAA" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kkhoot/PAA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BorderDet: Border Feature for Dense Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.11056" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.11056</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Megvii-BaseDetection/BorderDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Megvii-BaseDetection/BorderDet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quantum-soft QUBO Suppression for Accurate Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.13992" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.13992</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VarifocalNet: An IoU-aware Dense Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Queensland University of Technology &amp; University of Queensland</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.13367" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.13367</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hyz-xmaster/VarifocalNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hyz-xmaster/VarifocalNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The 1st Tiny Object Detection Challenge:Methods and Results</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV2020 Workshop on Real-world Computer Vision from Inputs with Limited Quality (RLQ) and Tiny Object Detection Challenge</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.07506" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.07506</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: SenseTime &amp; CUHK</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.11528" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.11528</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SEA: Bridging the Gap Between One- and Two-stage Detector Distillation via SEmantic-aware Alignment</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Chinese University of Hong Kong &amp; SmartMore</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.00862" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.00862</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020 spotlight</li><li>intro: Middle East Technical University</li><li>keywords: average Localization-Recall-Precision (aLRP)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.13592" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.13592</a></li><li>github(official, Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/kemaloksuz/aLRPLoss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kemaloksuz/aLRPLoss</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Effective Fusion Factor in FPN for Tiny Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.02298" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.02298</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bi-Dimensional Feature Alignment for Cross-Domain Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 TASK-CV Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.07205" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.07205</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Transformer-based Set Prediction for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Carnegie Mellon University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.10881" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.10881</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Object Detection with LiDAR Clues</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SenseTime &amp; USTC &amp; CASIA &amp; CAS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.12953" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.12953</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-EMD: Self-Supervised Object Detection without ImageNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MEGVII Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.13677" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.13677</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Object Detection with Fully Convolutional Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii Technology &amp; Xi’an Jiaotong University</li><li>keywords: Prediction-aware One- To-One (POTO) label assignment, 3D Max Filtering (3DMF)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.03544" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.03544</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Megvii-BaseDetection/DeFCN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Megvii-BaseDetection/DeFCN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fine-Grained Dynamic Head for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.03519" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.03519</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/StevenGrove/DynamicHead" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/StevenGrove/DynamicHead</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Focal and Efficient IOU Loss for Accurate Bounding Box Regression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: South China University of Technology &amp; 2Horizon Robotics &amp; Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.08158" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.08158</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale Normalized Image Pyramids with AutoFocus for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: T-PAMI 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.05646" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.05646</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mahyarnajibi/SNIPER" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mahyarnajibi/SNIPER</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DetCo: Unsupervised Contrastive Learning for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Hong Kong &amp; Huawei Noah’s Ark Lab &amp; Wuhan University &amp; Nanjing University &amp; Chinese University of Hong Kong</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.04803" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.04803</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xieenze/DetCo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xieenze/DetCo</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/open-mmlab/OpenSelfSup" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/open-mmlab/OpenSelfSup</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RMOPP: Robust Multi-Objective Post-Processing for Effective Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.04582" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.04582</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance Localization for Self-supervised Detection Pretraining</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese University of Hong Kong &amp; Microsoft Research Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.08318" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.08318</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Localization Distillation for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.12252" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.12252</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HikariTJU/LD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HikariTJU/LD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>General Instance Distillation for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.02340" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.02340</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Open World Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.02603" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.02603</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JosephKJ/OWOD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JosephKJ/OWOD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Data Augmentation for Object Detection via Differentiable Neural Rendering</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.02852" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.02852</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Guanghan/DANR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Guanghan/DANR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting the Loss Weight Adjustment in Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; University of Michigan</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.09488" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.09488</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ywx-hub/ALWA" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ywx-hub/ALWA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>You Only Look One-level Feature</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.09460" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.09460</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/megvii-model/YOLOF" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/megvii-model/YOLOF</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Optimization for Oriented Object Detection via Representation Invariance Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11636" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11636</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ming71/RIDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ming71/RIDet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Anchor Learning for Arbitrary-Oriented Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.04150" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.04150</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ming71/DAL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ming71/DAL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Control Distance IoU and Control Distance IoU Loss Function for Better Bounding Box Regression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11696" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11696</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OTA: Optimal Transport Assignment for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.14259" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.14259</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Megvii-BaseDetection/OTA" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Megvii-BaseDetection/OTA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distilling Object Detectors via Decoupled Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.14475" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.14475</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ggjy/DeFeat.pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ggjy/DeFeat.pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distilling a Powerful Student Model via Online Knowledge Distillation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.14473" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.14473</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SJLeo/FFSD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SJLeo/FFSD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IQDet: Instance-wise Quality Distribution Sampling for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Megvii Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.06936" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.06936</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Huazhong University of Science &amp; Technology, Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.00666" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.00666</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hustvl/YOLOS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hustvl/YOLOS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Augmenting Anchors by the Detector Itself</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.14086" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.14086</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Training from Scratch for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.03112" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.03112</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Head: Unifying Object Detection Heads with Attentions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Microsoft</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.08322" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.08322</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/DynamicHead" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/microsoft/DynamicHead</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Disentangle Your Dense Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.02963" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.02963</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zehuichen123/DDOD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zehuichen123/DDOD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Object Detection by Label Assignment Distillation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.10520" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.10520</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cybercore-co-ltd/CoLAD_paper" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cybercore-co-ltd/CoLAD_paper</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive Hard-case Mining across Pyramid Levels in Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:  Baidu Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2109.07217" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2109.07217</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zimoqingfeng/UMOP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zimoqingfeng/UMOP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Scale Aligned Distillation for Low-Resolution Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: The Chinese University of Hong Kong &amp; Adobe Research &amp; SmartMore</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2109.06875" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2109.06875</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dvlab-research/MSAD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dvlab-research/MSAD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pix2seq: A Language Modeling Framework for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google Research, Brain Team</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2109.10852" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2109.10852</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2021</li><li>intro: Shanghai Jiao Tong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.14191" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.14191</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bootstrap Your Object Detector via Mixed Training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2021 Spotlight</li><li>intro: Huazhong University of Science and Technology &amp; Xi’an Jiaotong University &amp; Microsoft Research Asia</li><li>keywords: MixTraining</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.03056" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.03056</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MendelXu/MixTraining" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MendelXu/MixTraining</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.00902" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.00902</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaddlePaddle/PaddleDetection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PaddlePaddle/PaddleDetection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Toward Minimal Misalignment at Minimal Cost in One-Stage and Anchor-Free Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.08902" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.08902</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GiraffeDet: A Heavy-Neck Paradigm for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2202.04256" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2202.04256</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Dual Weighting Label Assignment Scheme for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.09730" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.09730</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/strongwolf/DW" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/strongwolf/DW</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.09136" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.09136</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ChenhongyiYang/QueryDet-PyTorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ChenhongyiYang/QueryDet-PyTorch</a></li></ul><h1 id="two-stage-object-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#two-stage-object-detection" color="auto.gray.8" aria-label="Two-Stage Object Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Two-Stage Object Detection</h1><h2 id="r-cnn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#r-cnn" color="auto.gray.8" aria-label="R-CNN permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>R-CNN</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rich feature hierarchies for accurate object detection and semantic segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: R-CNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1311.2524" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1311.2524</a></li><li>supp: <a target="_blank" rel="noopener noreferrer" href="http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rbgirshick/rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rbgirshick/rcnn</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://zhangliliang.com/2014/07/23/paper-note-rcnn/" class="Link-sc-1brdqhf-0 cKRjba">http://zhangliliang.com/2014/07/23/paper-note-rcnn/</a></li><li>caffe-pr(&quot;Make R-CNN the Caffe detection example&quot;): <a target="_blank" rel="noopener noreferrer" href="https://github.com/BVLC/caffe/pull/482" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/BVLC/caffe/pull/482</a> </li></ul><h2 id="fast-r-cnn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#fast-r-cnn" color="auto.gray.8" aria-label="Fast R-CNN permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fast R-CNN</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.08083" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.08083</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rbgirshick/fast-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rbgirshick/fast-rcnn</a></li><li>github(COCO-branch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/rbgirshick/fast-rcnn/tree/coco" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rbgirshick/fast-rcnn/tree/coco</a></li><li>webcam demo: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rbgirshick/fast-rcnn/pull/29" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rbgirshick/fast-rcnn/pull/29</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/" class="Link-sc-1brdqhf-0 cKRjba">http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/</a></li><li>notes: <a target="_blank" rel="noopener noreferrer" href="http://blog.csdn.net/linj_m/article/details/48930179" class="Link-sc-1brdqhf-0 cKRjba">http://blog.csdn.net/linj_m/article/details/48930179</a></li><li>github(&quot;Fast R-CNN in MXNet&quot;): <a target="_blank" rel="noopener noreferrer" href="https://github.com/precedenceguo/mx-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/precedenceguo/mx-rcnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mahyarnajibi/fast-rcnn-torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mahyarnajibi/fast-rcnn-torch</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/apple2373/chainer-simple-fast-rnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/apple2373/chainer-simple-fast-rnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zplizzi/tensorflow-fast-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zplizzi/tensorflow-fast-rcnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03414" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03414</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaolonw/adversarial-frcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xiaolonw/adversarial-frcnn</a></li></ul><h2 id="faster-r-cnn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#faster-r-cnn" color="auto.gray.8" aria-label="Faster R-CNN permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Faster R-CNN</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.01497" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.01497</a></li><li>gitxiv: <a target="_blank" rel="noopener noreferrer" href="http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region" class="Link-sc-1brdqhf-0 cKRjba">http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf</a></li><li>github(official, Matlab): <a target="_blank" rel="noopener noreferrer" href="https://github.com/ShaoqingRen/faster_rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ShaoqingRen/faster_rcnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rbgirshick/py-faster-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rbgirshick/py-faster-rcnn</a></li><li>github(MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/msracver/Deformable-ConvNets/tree/master/faster_rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/msracver/Deformable-ConvNets/tree/master/faster_rcnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//jwyang/faster-rcnn.pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//jwyang/faster-rcnn.pytorch</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mitmul/chainer-faster-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mitmul/chainer-faster-rcnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/andreaskoepf/faster-rcnn.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andreaskoepf/faster-rcnn.torch</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ruotianluo/Faster-RCNN-Densecap-torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ruotianluo/Faster-RCNN-Densecap-torch</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/smallcorgi/Faster-RCNN_TF" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/smallcorgi/Faster-RCNN_TF</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CharlesShang/TFFRCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CharlesShang/TFFRCNN</a></li><li>github(C++ demo): <a target="_blank" rel="noopener noreferrer" href="https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yhenon/keras-frcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yhenon/keras-frcnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Eniac-Xie/faster-rcnn-resnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Eniac-Xie/faster-rcnn-resnet</a></li><li>github(C++): <a target="_blank" rel="noopener noreferrer" href="https://github.com/D-X-Y/caffe-faster-rcnn/tree/dev" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/D-X-Y/caffe-faster-rcnn/tree/dev</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>R-CNN minus R</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.06981" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.06981</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Faster R-CNN in MXNet with distributed implementation and data parallelization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dmlc/mxnet/tree/master/example/rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dmlc/mxnet/tree/master/example/rcnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Contextual Priming and Feedback for Faster R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016. Carnegie Mellon University</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://abhinavsh.info/context_priming_feedback.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://abhinavsh.info/context_priming_feedback.pdf</a></li><li>poster: <a target="_blank" rel="noopener noreferrer" href="http://www.eccv2016.org/files/posters/P-1A-20.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eccv2016.org/files/posters/P-1A-20.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Implementation of Faster RCNN with Study for Region Sampling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Technical Report, 3 pages. CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.02138" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.02138</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/endernewton/tf-faster-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/endernewton/tf-faster-rcnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Interpretable R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: North Carolina State University &amp; Alibaba</li><li>keywords: AND-OR Graph (AOG)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05226" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05226</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Light-Head R-CNN: In Defense of Two-Stage Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; Megvii Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07264" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07264</a></li><li>github(official, Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/zengarden/light_head_rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zengarden/light_head_rcnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/terrychenism/Deformable-ConvNets/blob/master/rfcn/symbols/resnet_v1_101_rfcn_light.py#L784" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/terrychenism/Deformable-ConvNets/blob/master/rfcn/symbols/resnet_v1_101_rfcn_light.py#L784</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascade R-CNN: Delving into High Quality Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. UC San Diego</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00726" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00726</a></li><li>github(Caffe, official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhaoweicai/cascade-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhaoweicai/cascade-rcnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascade R-CNN: High Quality Object Detection and Instance Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"> -arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.09756" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.09756</a></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github(Caffe, official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhaoweicai/cascade-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhaoweicai/cascade-rcnn</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhaoweicai/Detectron-Cascade-RCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhaoweicai/Detectron-Cascade-RCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2019 spotlight</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.06720" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.06720</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/thangvubk/Cascade-RPN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/thangvubk/Cascade-RPN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SMC Faster R-CNN: Toward a scene-specialized multi-object detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.10217" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.10217</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain Adaptive Faster R-CNN for Object Detection in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. ETH Zurich &amp; ESAT/PSI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.03243" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.03243</a></li><li>github(official. Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/yuhuayc/da-faster-rcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yuhuayc/da-faster-rcnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Physical Adversarial Attack on Faster R-CNN Object Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.05810" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.05810</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Auto-Context R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Rejected by ECCV18</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.02842" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.02842</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Grid R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: SenseTime</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12030" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12030</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Grid R-CNN Plus: Faster and Better</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SenseTime Research &amp; CUHK &amp; Beihang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.05688" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.05688</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/STVIR/Grid-R-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/STVIR/Grid-R-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Few-shot Adaptive Faster R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.09372" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.09372</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Libra R-CNN: Towards Balanced Learning for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02701" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02701</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Classification and Localization in R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Northeastern University &amp; Microsoft</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.06493" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.06493</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reprojection R-CNN: A Fast and Accurate Object Detector for 360° Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peking University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.11830" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.11830</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Classification and Localization for Cascade R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.11914" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.11914</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IoU-uniform R-CNN: Breaking Through the Limitations of RPN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.05190" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.05190</a></li><li>github(mmdetection): <a target="_blank" rel="noopener noreferrer" href="https://github.com/zl1994/IoU-Uniform-R-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zl1994/IoU-Uniform-R-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.06002" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.06002</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hkzhang95/DynamicRCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hkzhang95/DynamicRCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Delving into the Imbalance of Positive Proposals in Two-stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Waseda University &amp; Tencent AI Lab &amp; Nanjing University of Science and Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.11472" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.11472</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Context Embedding for Region-based Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: Nanjing University &amp; Megvii Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.01338" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.01338</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sparse R-CNN: End-to-End Object Detection with Learnable Proposals</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: The University of Hong Kong &amp; Tongji University &amp; ByteDance AI Lab 4University of California</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.12450" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.12450</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PeizeSun/SparseR-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PeizeSun/SparseR-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Sparse R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2205.02101" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2205.02101</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Featurized Query R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Huazhong University of Science &amp; Technology &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.06258" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.06258</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hustvl/Featurized-QueryRCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hustvl/Featurized-QueryRCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Augmenting Proposals by the Detector Itself</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.11789" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.11789</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Probabilistic two-stage detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UT Austin &amp; Intel Labs</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.07461" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.07461</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xingyizhou/CenterNet2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xingyizhou/CenterNet2</a></li></ul><h1 id="single-shot-object-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#single-shot-object-detection" color="auto.gray.8" aria-label="Single-Shot Object Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Single-Shot Object Detection</h1><h2 id="yolo" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#yolo" color="auto.gray.8" aria-label="YOLO permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>YOLO</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>You Only Look Once: Unified, Real-Time Object Detection</strong></p><img src="https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.02640" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.02640</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://pjreddie.com/darknet/yolo/" class="Link-sc-1brdqhf-0 cKRjba">http://pjreddie.com/darknet/yolo/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pjreddie/darknet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pjreddie/darknet</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://pjreddie.com/publications/yolo/" class="Link-sc-1brdqhf-0 cKRjba">https://pjreddie.com/publications/yolo/</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p" class="Link-sc-1brdqhf-0 cKRjba">https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p</a></li><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gliese581gg/YOLO_tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gliese581gg/YOLO_tensorflow</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xingwangsfu/caffe-yolo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xingwangsfu/caffe-yolo</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/frankzhangrui/Darknet-Yolo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/frankzhangrui/Darknet-Yolo</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/BriSkyHekun/py-darknet-yolo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/BriSkyHekun/py-darknet-yolo</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tommy-qichang/yolo.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tommy-qichang/yolo.torch</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/frischzenger/yolo-windows" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/frischzenger/yolo-windows</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/AlexeyAB/yolo-windows" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/AlexeyAB/yolo-windows</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nilboy/tensorflow-yolo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nilboy/tensorflow-yolo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp" class="Link-sc-1brdqhf-0 cKRjba">https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/thtrieu/darkflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/thtrieu/darkflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Start Training YOLO with Our Own Data</strong></p><img src="http://guanghan.info/blog/en/wp-content/uploads/2015/12/images-40.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: train with customized data and class numbers/labels. Linux / Windows version for darknet.</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://guanghan.info/blog/en/my-works/train-yolo/" class="Link-sc-1brdqhf-0 cKRjba">http://guanghan.info/blog/en/my-works/train-yolo/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Guanghan/darknet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Guanghan/darknet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLO: Core ML versus MPSNNGraph</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API.</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://machinethink.net/blog/yolo-coreml-versus-mps-graph/" class="Link-sc-1brdqhf-0 cKRjba">http://machinethink.net/blog/yolo-coreml-versus-mps-graph/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hollance/YOLO-CoreML-MPSNNGraph" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hollance/YOLO-CoreML-MPSNNGraph</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TensorFlow YOLO object detection on Android</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Real-time object detection on Android using the YOLO network with TensorFlow</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/natanielruiz/android-yolo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/natanielruiz/android-yolo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Computer Vision in iOS – Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/" class="Link-sc-1brdqhf-0 cKRjba">https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/</a></li><li>github:<a target="_blank" rel="noopener noreferrer" href="https://github.com/r4ghu/iOS-CoreML-Yolo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/r4ghu/iOS-CoreML-Yolo</a></li></ul><h2 id="yolov2" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#yolov2" color="auto.gray.8" aria-label="YOLOv2 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>YOLOv2</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLO9000: Better, Faster, Stronger</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.08242" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.08242</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://pjreddie.com/yolo9000/" class="Link-sc-1brdqhf-0 cKRjba">http://pjreddie.com/yolo9000/</a></li><li>github(Chainer): <a target="_blank" rel="noopener noreferrer" href="https://github.com/leetenki/YOLOv2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/leetenki/YOLOv2</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/allanzelener/YAD2K" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/allanzelener/YAD2K</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/longcw/yolo2-pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/longcw/yolo2-pytorch</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/hizhangp/yolo_tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hizhangp/yolo_tensorflow</a></li><li>github(Windows): <a target="_blank" rel="noopener noreferrer" href="https://github.com/AlexeyAB/darknet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/AlexeyAB/darknet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/choasUp/caffe-yolo9000" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/choasUp/caffe-yolo9000</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/philipperemy/yolo-9000" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/philipperemy/yolo-9000</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>darknet_scripts</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Auxilary scripts to work with (YOLO) darknet deep learning famework. AKA -&gt; How to generate YOLO anchors?</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Jumabek/darknet_scripts" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Jumabek/darknet_scripts</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/AlexeyAB/Yolo_mark" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/AlexeyAB/Yolo_mark</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LightNet: Bringing pjreddie&#x27;s DarkNet out of the shadows</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com//explosion/lightnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//explosion/lightnet</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLO v2 Bounding Box Tool</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Bounding box labeler tool to generate the training data in the format YOLO v2 requires.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Cartucho/yolo-boundingbox-labeler-GUI" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Cartucho/yolo-boundingbox-labeler-GUI</a></li></ul><h2 id="yolov3" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#yolov3" color="auto.gray.8" aria-label="YOLOv3 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>YOLOv3</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLOv3: An Incremental Improvement</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://pjreddie.com/darknet/yolo/" class="Link-sc-1brdqhf-0 cKRjba">https://pjreddie.com/darknet/yolo/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.02767" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.02767</a></li><li>githb: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DeNA/PyTorch_YOLOv3" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DeNA/PyTorch_YOLOv3</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/eriklindernoren/PyTorch-YOLOv3" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/eriklindernoren/PyTorch-YOLOv3</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.04620" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.04620</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLO-LITE: A Real-Time Object Detection Algorithm Optimized for Non-GPU Computers</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.05588" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.05588</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spiking-YOLO: Spiking Neural Network for Real-time Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.06530" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.06530</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.01271" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.01271</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>REQ-YOLO: A Resource-Aware, Efficient Quantization Framework for Object Detection on FPGAs</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.13396" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.13396</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.13243" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.13243</a></li><li>gitlab: <a target="_blank" rel="noopener noreferrer" href="https://gitlab.com/irafm-ai/poly-yolo" class="Link-sc-1brdqhf-0 cKRjba">https://gitlab.com/irafm-ai/poly-yolo</a></li></ul><h2 id="yolov4" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#yolov4" color="auto.gray.8" aria-label="YOLOv4 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>YOLOv4</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLOv4: Optimal Speed and Accuracy of Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT), Mish-activation</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.10934" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.10934</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/AlexeyAB/darknet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/AlexeyAB/darknet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/WongKinYiu/PyTorch_YOLOv4" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/WongKinYiu/PyTorch_YOLOv4</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLOX: Exceeding YOLO Series in 2021</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.08430" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.08430</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Megvii-BaseDetection/YOLOX" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Megvii-BaseDetection/YOLOX</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PP-YOLO: An Effective and Efficient Implementation of Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.12099" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.12099</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaddlePaddle/PaddleDetection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PaddlePaddle/PaddleDetection</a></li></ul><h2 id="yolov7" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#yolov7" color="auto.gray.8" aria-label="YOLOv7 permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>YOLOv7</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.02696" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.02696</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/WongKinYiu/yolov7" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/WongKinYiu/yolov7</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-time Object Detection for Streaming Perception</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.12338" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.12338</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yancie-yjr/StreamYOLO" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yancie-yjr/StreamYOLO</a></li></ul><h2 id="ssd" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#ssd" color="auto.gray.8" aria-label="SSD permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>SSD</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SSD: Single Shot MultiBox Detector</strong></p><img src="https://camo.githubusercontent.com/ad9b147ed3a5f48ffb7c3540711c15aa04ce49c6/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373642e706e67" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.02325" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.02325</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.unc.edu/~wliu/papers/ssd.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.unc.edu/~wliu/papers/ssd.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf</a></li><li>github(Official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/weiliu89/caffe/tree/ssd" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/weiliu89/caffe/tree/ssd</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973" class="Link-sc-1brdqhf-0 cKRjba">http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhreshold/mxnet-ssd" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhreshold/mxnet-ssd</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhreshold/mxnet-ssd.cpp" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhreshold/mxnet-ssd.cpp</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rykov8/ssd_keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rykov8/ssd_keras</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/balancap/SSD-Tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/balancap/SSD-Tensorflow</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/amdegroot/ssd.pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/amdegroot/ssd.pytorch</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/chuanqi305/MobileNet-SSD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chuanqi305/MobileNet-SSD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What&#x27;s the diffience in performance between this new code you pushed and the previous code? #327</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/weiliu89/caffe/issues/327" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/weiliu89/caffe/issues/327</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DSSD : Deconvolutional Single Shot Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UNC Chapel Hill &amp; Amazon Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.06659" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.06659</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chengyangfu/caffe/tree/dssd" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chengyangfu/caffe/tree/dssd</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MTCloudVision/mxnet-dssd" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MTCloudVision/mxnet-dssd</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4" class="Link-sc-1brdqhf-0 cKRjba">http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Enhancement of SSD by concatenating feature maps for object detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: rainbow SSD (R-SSD)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.09587" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.09587</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context-aware Single-Shot Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs),  theoretical receptive fields (TRFs)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.08682" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.08682</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature-Fused SSD: Fast Detection for Small Objects</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.05054" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.05054</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FSSD: Feature Fusion Single Shot Multibox Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00960" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00960</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weaving Multi-scale Context for Single Shot Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WeaveNet</li><li>keywords: fuse multi-scale information</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.03149" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.03149</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Extend the shallow part of Single Shot MultiBox Detector via Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: ESSD</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.05918" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.05918</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.06488" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.06488</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MDSSD: Multi-scale Deconvolutional Single Shot Detector for small objects</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhengzhou University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07009" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07009</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accurate Single Stage Detector Using Recurrent Rolling Convolution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. SenseTime</li><li>keywords: Recurrent Rolling Convolution (RRC)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.05776" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.05776</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaohaoChen/rrc_detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xiaohaoChen/rrc_detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Residual Features and Unified Prediction Network for Single Stage Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.05031" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.05031</a></p><h2 id="retinanet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#retinanet" color="auto.gray.8" aria-label="RetinaNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>RetinaNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Focal Loss for Dense Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017 Best student paper award. Facebook AI Research</li><li>keywords: RetinaNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02002" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02002</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascade RetinaNet: Maintaining Consistency for Single-Stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>keywords: Cas-RetinaNet, Feature Consistency Module</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.06881" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.06881</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Focal Loss Dense Detector for Vehicle Surveillance</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.01114" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.01114</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single-Shot Refinement Neural Network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06897" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06897</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sfzhang15/RefineDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sfzhang15/RefineDet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MTCloudVision/RefineDet-Mxnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MTCloudVision/RefineDet-Mxnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single-Shot Bidirectional Pyramid Networks for High-Quality Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Singapore Management University &amp; Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08208" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08208</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dual Refinement Network for Single-Shot Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08638" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08638</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ScratchDet:Exploring to Train Single-Shot Object Detectors from Scratch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.08425" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.08425</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KimSoybean/ScratchDethttps://github.com/KimSoybean/ScratchDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KimSoybean/ScratchDet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gradient Harmonized Single-stage Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.05181" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.05181</a></li><li>gihtub(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/libuyu/GHM_Detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/libuyu/GHM_Detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.04533" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.04533</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/qijiezhao/M2Det" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/qijiezhao/M2Det</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-layer Pruning Framework for Compressing Single Shot MultiBox Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08342" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08342</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Consistent Optimization for Single-Shot Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.06563" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.06563</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/55416312" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/55416312</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Single-shot Object Detector with Feature Aggragation and Enhancement</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.02923" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.02923</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Accurate One-Stage Object Detection with AP-Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: Shanghai Jiao Tong University &amp; Intel Labs &amp; Malaysia Multimedia University &amp; Tencent YouTu Lab &amp; Peking University</li><li>keywords: Average-Precision loss (AP-loss)</li><li>arxiv: {<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.06373%7D(https://arxiv.org/abs/1904.06373)" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.06373}(https://arxiv.org/abs/1904.06373)</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AP-Loss for Accurate One-Stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE TPAMI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.07294" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.07294</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cccorn/AP-loss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cccorn/AP-loss</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Searching Parameterized AP Loss for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2021</li><li>intro: 1Tsinghua University &amp; Zhejiang University &amp; SenseTime Research &amp; Shanghai Jiao Tong University &amp; Beijing Academy of Artificial Intelligence</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.05138" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.05138</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fundamentalvision/Parameterized-AP-Loss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fundamentalvision/Parameterized-AP-Loss</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Featurized Image Pyramid Network for Single Shot Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Pang_Efficient_Featurized_Image_Pyramid_Network_for_Single_Shot_Detector_CVPR_2019_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_CVPR_2019/papers/Pang_Efficient_Featurized_Image_Pyramid_Network_for_Single_Shot_Detector_CVPR_2019_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vaesl/LFIP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vaesl/LFIP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DR Loss: Improving Object Detection by Distributional Ranking</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.10156" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.10156</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HAR-Net: Joint Learning of Hybrid Attention for Single-stage Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.11141" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.11141</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Propose-and-Attend Single Shot Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.12736" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.12736</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting Feature Alignment for One-stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Chinese Academy of Sciences &amp; TuSimple</li><li>keywords: AlignDet, RoIConv</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.01570" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.01570</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IoU-balanced Loss Functions for Single-stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: HUST</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05641</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PosNeg-Balanced Anchors with Aligned Features for Single-Shot Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese Academy of Sciences &amp; University of Chinese Academy of Sciences</li><li>keywords: Anchor Promotion Module (APM), Feature Alignment Module (FAM)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.03295" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.03295</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05612" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05612</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Shot Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>keywords: reg-offset-cls (ROC) module</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.pdf</a></li><li>github(official, Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/JialeCao001/HSD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JialeCao001/HSD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning from Noisy Anchors for One-stage Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.05086" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.05086</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Nanjing University of Science and Technology &amp; Momenta &amp; Nanjing University &amp; Microsoft Research &amp; Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.04388" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.04388</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/implus/GFocal" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/implus/GFocal</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Nanjing University of Science and Technology &amp; Momenta &amp; Nanjing University &amp; Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.12885" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.12885</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/implus/GFocalV2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/implus/GFocalV2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Single-Shot Two-Pronged Detector with Rectified IoU Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.03511" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.03511</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OneNet: Towards End-to-End One-Stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Hong Kong &amp; ByteDance AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.05780" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.05780</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PeizeSun/OneNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PeizeSun/OneNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TOOD: Task-aligned One-stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021 Oral</li><li>intro: Intellifusion Inc. &amp; Meituan Inc. &amp; ByteDance Inc. &amp; Malong LLC &amp; Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.07755" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.07755</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fcjian/TOOD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fcjian/TOOD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking the Aligned and Misaligned Features in One-stage Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.12176" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.12176</a></p><h1 id="anchor-free" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#anchor-free" color="auto.gray.8" aria-label="Anchor-free permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Anchor-free</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Selective Anchor-Free Module for Single-Shot Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>keywords: feature selective anchor-free (FSAF) module</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.00621" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.00621</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FCOS: Fully Convolutional One-Stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Adelaide</li><li>keywords: anchor-free</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01355" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01355</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tianzhi0549/FCOS/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tianzhi0549/FCOS/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FoveaBox: Beyond Anchor-based Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; BNRist &amp; ByteDance AI Lab &amp; University of Pennsylvania</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.03797" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.03797</a></li><li>github(official, mmdetection): <a target="_blank" rel="noopener noreferrer" href="https://github.com/taokong/FoveaBox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/taokong/FoveaBox</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IMMVP: An Efficient Daytime and Nighttime On-Road Object Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.06573" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.06573</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EfficientDet: Scalable and Efficient Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.09070" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.09070</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/automl/tree/master/efficientdet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/google/automl/tree/master/efficientdet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain Adaptation for Object Detection via Style Consistency</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.10033" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.10033</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Soft Anchor-Point Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: Carnegie Mellon University</li><li>keywords: Soft Anchor-Point Detector (SAPD)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.12448" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.12448</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IPG-Net: Image Pyramid Guidance Network for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.00632" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.00632</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.02424" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.02424</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sfzhang15/ATSS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sfzhang15/ATSS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Localization Uncertainty Estimation for Anchor-Free Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Gaussian-FCOS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.15607" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.15607</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Corner Proposal Network for Anchor-free, Two-stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.13816" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.13816</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Duankaiwen/CPNDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Duankaiwen/CPNDet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dive Deeper Into Box for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>keywords: DDBNet, anchor free</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.14350" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.14350</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Corner Proposal Network for Anchor-free, Two-stage Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.13816" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.13816</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Duankaiwen/CPNDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Duankaiwen/CPNDet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reducing Label Noise in Anchor-Free Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.01167" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.01167</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nerminsamet/ppdet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nerminsamet/ppdet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.13763" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.13763</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PAFNet: An Efficient Anchor-Free Object Detector Guidance</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Inc.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.13534" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.13534</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaddlePaddle/PaddleDetection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PaddlePaddle/PaddleDetection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pseudo-IoU: Improving Label Assignment in Anchor-Free Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 Workshop</li><li>intro: UIUC &amp; MIT-IBM Watson AI Lab &amp; IBM T.J. Watson Research Center &amp; NVIDIA &amp; University of Oregon &amp; Picsart AI Research (PAIR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.14082" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.14082</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ObjectBox: From Centers to Boxes for Anchor-Free Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2022 Oral</li><li>intro: Ingenuity Labs Research Institute &amp; Queen’s University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.06985" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.06985</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MohsenZand/ObjectBox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MohsenZand/ObjectBox</a></li></ul><h1 id="transformers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#transformers" color="auto.gray.8" aria-label="Transformers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Transformers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Object Detection with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI</li><li>keywords: DEtection TRansformer (DETR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.12872" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.12872</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/detr" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/detr</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deformable DETR: Deformable Transformers for End-to-End Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SenseTime Research &amp; USTC &amp; CUHK</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.04159" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.04159</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fundamentalvision/Deformable-DETR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fundamentalvision/Deformable-DETR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS2020 Spotlight</li><li>intro: CAS &amp; MSRA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.15831" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.15831</a></li><li>github:<a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/RelationNet2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/microsoft/RelationNet2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UP-DETR: Unsupervised Pre-training for Object Detection with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: South China University of Technology &amp; Tencent Wechat AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.09094" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.09094</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Conditional DETR for Fast Training Convergence</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>intro: University of Science and Technology of China &amp; Peking University &amp; Microsoft Research Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.06152" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.06152</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Atten4Vis/ConditionalDETR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Atten4Vis/ConditionalDETR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Object Detection with Adaptive Clustering Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peking University &amp; The Chinese University of Hong Kong</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.09315" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.09315</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Toward Transformer-Based Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Pinterest</li><li>keywords: ViT-FRCNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.09958" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.09958</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient DETR: Improving End-to-End Object Detector with Dense Prior</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.01318" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.01318</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anchor DETR: Query Design for Transformer-Based Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MEGVII Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2109.07107" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2109.07107</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/megvii-model/AnchorDETR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/megvii-model/AnchorDETR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2201.12329" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2201.12329</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SlongLiu/DAB-DETR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SlongLiu/DAB-DETR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.03605" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.03605</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/IDEACVR/DINO" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/IDEACVR/DINO</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Oriented Object Detection with Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University at Buffalo &amp; Beihang University &amp; Baidu Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.03146" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.03146</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ViDT: An Efficient and Effective Fully Transformer-based Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NAVER AI Lab &amp; Google Research &amp; University of California at Merced</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.03921" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.03921</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/naver-ai/vidt" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/naver-ai/vidt</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Extendable, Efficient and Effective Transformer-based Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.07962" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.07962</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/naver-ai/vidt" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/naver-ai/vidt</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Omni-DETR: Omni-Supervised Object Detection with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.16089" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.16089</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating DETR Convergence via Semantic-Aligned Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.06883" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.06883</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZhangGongjie/SAM-DETR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZhangGongjie/SAM-DETR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AdaMixer: A Fast-Converging Query-Based Object Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022 oral</li><li>intro: Nanjing University, MYbank Ant Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.16507" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.16507</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MCG-NJU/AdaMixer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MCG-NJU/AdaMixer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring Plain Vision Transformer Backbones for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.16527" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.16527</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Decoder-free Object Detection with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:  Tencent Youtu Lab &amp; Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2206.06829" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2206.06829</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Pealing/DFFT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Pealing/DFFT</a></li></ul><h1 id="non-maximum-suppression-nms" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#non-maximum-suppression-nms" color="auto.gray.8" aria-label="Non-Maximum Suppression (NMS) permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Non-Maximum Suppression (NMS)</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1411.5309" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1411.5309</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A convnet for non-maximum suppression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06437" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06437</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Object Detection With One Line of Code</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Soft-NMS -- Improving Object Detection With One Line of Code</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. University of Maryland</li><li>keywords: Soft-NMS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.04503" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.04503</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bharatsingh430/soft-nms" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bharatsingh430/soft-nms</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; Megvii Inc. (Face++)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.08545" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.08545</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yihui-he/softer-NMS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yihui-he/softer-NMS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning non-maximum suppression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/learning-nms/" class="Link-sc-1brdqhf-0 cKRjba">https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/learning-nms/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.02950" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.02950</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hosang/gossipnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hosang/gossipnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Relation Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.11575" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.11575</a></li><li>github(official, MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/msracver/Relation-Networks-for-Object-Detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/msracver/Relation-Networks-for-Object-Detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Pairwise Relationship for Multi-object Detection in Crowded Scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Pairwise-NMS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.03796" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.03796</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Daedalus: Breaking Non-Maximum Suppression in Object Detection via Adversarial Examples</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.02067" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.02067</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: Waseda University &amp; Tencent AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.12729" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.12729</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hashing-based Non-Maximum Suppression for Crowded Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.11426" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.11426</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/hnms" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/microsoft/hnms</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visibility Guided NMS: Efficient Boosting of Amodal Object Detection in Crowded Traffic Scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2019, Machine Learning for Autonomous Driving Workshop</li><li>intro: Mercedes-Benz AG, R&amp;D &amp; University of Jena</li><li>keywords: Visibility Guided NMS (vg-NMS)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.08547" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.08547</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Determinantal Point Process as an alternative to NMS</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.11451" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.11451</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University &amp; Nanyang Technological University &amp; Tencent AI Lab &amp; Columbia University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.01449" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.01449</a></li></ul><h1 id="nms-free" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#nms-free" color="auto.gray.8" aria-label="NMS-free permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>NMS-free</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection Made Simpler by Eliminating Heuristic NMS</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Group &amp; Monash University &amp; The University of Adelaide</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.11782" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.11782</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/txdet/FCOSPss" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/txdet/FCOSPss</a></li></ul><h1 id="adversarial-examples" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#adversarial-examples" color="auto.gray.8" aria-label="Adversarial Examples permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adversarial Examples</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Examples that Fool Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Illinois</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02494" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02494</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://nicholas.carlini.com/code/nn_breaking_detection/" class="Link-sc-1brdqhf-0 cKRjba">http://nicholas.carlini.com/code/nn_breaking_detection/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.07263" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.07263</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/carlini/nn_breaking_detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/carlini/nn_breaking_detection</a></li></ul><h1 id="knowledge-distillation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#knowledge-distillation" color="auto.gray.8" aria-label="Knowledge Distillation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Knowledge Distillation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mimicking Very Efficient Network for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. SenseTime &amp; Beihang University</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quantization Mimic: Towards Very Tiny CNN for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.02152" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.02152</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Efficient Detector with Semi-supervised Adaptive Distillation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SenseTime Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.00366" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.00366</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tangshitao/Semi-supervised-Adaptive-Distillation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Tangshitao/Semi-supervised-Adaptive-Distillation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distilling Object Detectors with Fine-grained Feature Imitation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: National University of Singapore &amp; Huawei Noah’s Ark Lab</li><li>keywords: mimic</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.03609" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.03609</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/twangnh/Distilling-Object-Detectors" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/twangnh/Distilling-Object-Detectors</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GAN-Knowledge Distillation for one-stage Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.08467" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.08467</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Lightweight Pedestrian Detector with Hierarchical Knowledge Distillation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.09325" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.09325</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2021 poster</li><li>openreview: <a target="_blank" rel="noopener noreferrer" href="https://openreview.net/forum?id=uKhGRvM8QNH" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.net/forum?id=uKhGRvM8QNH</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openreview.net/pdf?id=uKhGRvM8QNH" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.net/pdf?id=uKhGRvM8QNH</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ArchipLab-LinfengZhang/Object-Detection-Knowledge-Distillation-ICLR2021" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ArchipLab-LinfengZhang/Object-Detection-Knowledge-Distillation-ICLR2021</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Hong Kong University of Science and Technology &amp; Huawei Noah’s Ark Lab</li><li>intro: ICCV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.07482" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.07482</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LGD: Label-guided Self-distillation for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MEGVII Technology &amp; Xi’an Jiaotong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2109.11496" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2109.11496</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Structured Instance Graph for Distilling Object Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>intro: The Chinese University of Hong Kong &amp; SmartMore</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2109.12862" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2109.12862</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dvlab-research/Dsig" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dvlab-research/Dsig</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Instance-Conditional Knowledge Distillation for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2021 poster</li><li>intro: Xi’an Jiaotong University &amp; MEGVII Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.12724" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.12724</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distilling Object Detectors with Feature Richness</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; CAS &amp; Cambricon Technologies &amp; University of Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.00674" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.00674</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Focal and Global Knowledge Distillation for Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua Shenzhen International Graduate School &amp; ByteDance Inc &amp; BeiHang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.11837" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.11837</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yzd-v/FGD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yzd-v/FGD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Prediction-Guided Distillation for Dense Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Edinburgh &amp; Heriot-Watt University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.05469" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.05469</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ChenhongyiYang/PGD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ChenhongyiYang/PGD</a></li></ul><h1 id="rotated-object-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#rotated-object-detection" color="auto.gray.8" aria-label="Rotated Object Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Rotated Object Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University &amp; Huawei Inc. &amp; Beijing Institute of Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.11952" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.11952</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yangxue0827/RotationDetection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yangxue0827/RotationDetection</a></li></ul><h1 id="long-tailed-object-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#long-tailed-object-detection" color="auto.gray.8" aria-label="Long-Tailed Object Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Long-Tailed Object Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Factors in Finetuning Deep Model for object detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016.rank 3rd for provided data and 2nd for external data on ILSVRC 2015 object detection</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.05150" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.05150</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.10408" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.10408</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/FishYuLi/BalancedGroupSoftmax" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/FishYuLi/BalancedGroupSoftmax</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tongji University &amp; SenseTime Research &amp; Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.08548" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.08548</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Ohio State University &amp; University of Central Florida &amp; University of Southern California &amp; Google Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.08884" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.08884</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Class Suppression Loss for Long-Tail Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.00885" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.00885</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CASIA-IVA-Lab/ACSL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CASIA-IVA-Lab/ACSL</a></li></ul><h1 id="weakly-supervised-object-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#weakly-supervised-object-detection" color="auto.gray.8" aria-label="Weakly Supervised Object Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Weakly Supervised Object Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.05766" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.05766</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weakly supervised object detection using pseudo-strong labels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.04731" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.04731</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Saliency Guided End-to-End Learning for Weakly Supervised Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.06768" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.06768</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI 2017. National Institutes of Health (NIH) Clinical Center</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.03145" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.03145</a></li></ul><h1 id="video-object-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-object-detection" color="auto.gray.8" aria-label="Video Object Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Object Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Object Class Detectors from Weakly Annotated Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2012</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Analysing domain shift factors between videos and images for object detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1501.01186" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1501.01186</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx" class="Link-sc-1brdqhf-0 cKRjba">http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Saliency Prediction in Natural Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Submitted on 12 Jan 2016</li><li>keywords: Deep learning, saliency map, optical flow, convolution network, contrast features</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://hal.archives-ouvertes.fr/hal-01251614/document" class="Link-sc-1brdqhf-0 cKRjba">https://hal.archives-ouvertes.fr/hal-01251614/document</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Winning solution in ILSVRC2015 Object Detection from Video(VID) Task</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.02532" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.02532</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/myfavouritekk/T-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/myfavouritekk/T-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection from Video Tubelets with Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016 Spotlight paper</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.04053" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.04053</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/myfavouritekk/vdetlib" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/myfavouritekk/vdetlib</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection in Videos with Tubelets and Multi-context Cues</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SenseTime Group</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context Matters: Refining Object Detection in Video with Recurrent Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2016</li><li>keywords: pseudo-labeler</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.04648" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.04648</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNN Based Object Detection in Large Video Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WangTao @ 爱奇艺</li><li>keywords: object retrieval, object detection, scene classification</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection in Videos with Tubelet Proposal Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.06355" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.06355</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Flow-Guided Feature Aggregation for Video Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MSRA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10025" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10025</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Object Detection using Faster R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://andrewliao11.github.io/object_detection/faster_rcnn/" class="Link-sc-1brdqhf-0 cKRjba">http://andrewliao11.github.io/object_detection/faster_rcnn/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/andrewliao11/py-faster-rcnn-imagenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andrewliao11/py-faster-rcnn-imagenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Context Modeling for Video Object Detection and Tracking</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf" class="Link-sc-1brdqhf-0 cKRjba">http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Dynamic Graph LSTM for Action-driven Video Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.00666" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.00666</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mobile Video Object Detection with Temporally-Aware Feature Maps</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06368" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06368</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards High Performance Video Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.11577" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.11577</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Impression Network for Video Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.05896" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.05896</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial-Temporal Memory Networks for Video Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.06317" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.06317</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>3D-DETNet: a Single Stage Video-Based Vehicle Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.01769" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.01769</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection in Videos by Short and Long Range Object Linking</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.09823" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.09823</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection in Video with Spatiotemporal Sampling Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Pennsylvania, 2Dartmouth College</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05549" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05549</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards High Performance Video Object Detection for Mobiles</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Microsoft Research Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.05830" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.05830</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Optimizing Video Object Detection via a Scale-Time Lattice</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.05472" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.05472</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hellock/scale-time-lattice" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hellock/scale-time-lattice</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pack and Detect: Fast Object Detection in Videos Using Region-of-Interest Packing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.01701" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.01701</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Object Detection in Compressed Video</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11057" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11057</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tube-CNN: Modeling temporal evolution of appearance for object detection in video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: INRIA/ENS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.02619" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.02619</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AdaScale: Towards Real-time Video Object Detection Using Adaptive Scaling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SysML 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.02910" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.02910</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SCNN: A General Distribution based Statistical Convolutional Neural Network with Application to Video Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.07663" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.07663</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Looking Fast and Slow: Memory-Guided Mobile Video Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Cornell University &amp; Google AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.10172" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.10172</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive Sparse Local Attention for Video object detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NLPR,CASIA &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.09126" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.09126</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sequence Level Semantics Aggregation for Video Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.06390" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.06390</a></li><li>github(MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/happywu/Sequence-Level-Semantics-Aggregation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/happywu/Sequence-Level-Semantics-Aggregation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection in Video with Spatial-temporal Context Aggregation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Huazhong University of Science and Technology &amp; Horizon Robotics Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.04988" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.04988</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Delay Metric for Video Object Detection: What Average Precision Fails to Tell</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.06368" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.06368</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Minimum Delay Object Detection From Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.11092" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.11092</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Motion Priors for Efficient Video Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.05253" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.05253</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object-aware Feature Aggregation for Video Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Beihang University &amp; Capital Normal University &amp; The University of Hong Kong &amp; Baidu, Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.12573" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.12573</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Video Object Detection with Spatial-Temporal Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.10920" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.10920</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SJTU-LuHe/TransVOD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SJTU-LuHe/TransVOD</a></li></ul><h1 id="object-detection-on-mobile-devices" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#object-detection-on-mobile-devices" color="auto.gray.8" aria-label="Object Detection on Mobile Devices permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Object Detection on Mobile Devices</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pelee: A Real-Time Object Detection System on Mobile Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2018 workshop track</li><li>intro: based on the SSD</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.06882" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.06882</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Robert-JunWang/Pelee" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Robert-JunWang/Pelee</a></li></ul><h1 id="object-detection-on-rgb-d" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#object-detection-on-rgb-d" color="auto.gray.8" aria-label="Object Detection on RGB-D permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Object Detection on RGB-D</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Rich Features from RGB-D Images for Object Detection and Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1407.5736" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1407.5736</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Differential Geometry Boosts Convolutional Neural Networks for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.03347" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.03347</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-Modal Attentional Context Learning for RGB-D Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Image Processing</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.12829" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.12829</a></li></ul><h1 id="zero-shot-object-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#zero-shot-object-detection" color="auto.gray.8" aria-label="Zero-Shot Object Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Zero-Shot Object Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Australian National University</li><li>keywords: YOLO</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.07113" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.07113</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.04340" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.04340</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Australian National University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.06049" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.06049</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Zero-Shot Object Detection by Hybrid Region Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Middle East Technical University &amp; Hacettepe University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06157" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06157</a></li></ul><h1 id="visual-relationship-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#visual-relationship-detection" color="auto.gray.8" aria-label="Visual Relationship Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Visual Relationship Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual Relationship Detection with Language Priors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016 oral</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Visual Phrase reasoning Convolutional Neural Network (ViP-CNN), Visual Phrase Reasoning Structure (VPRS)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.07191" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.07191</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visual Translation Embedding Network for Visual Relation Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://www.arxiv.org/abs/1702.08319" class="Link-sc-1brdqhf-0 cKRjba">https://www.arxiv.org/abs/1702.08319</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 spotlight paper</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.03054" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.03054</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting Visual Relationships with Deep Relational Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 oral. The Chinese University of Hong Kong</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03114" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03114</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Identifying Spatial Relations in Images using Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.04215" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.04215</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.01956" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.01956</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Natural Language Guided Visual Relationship Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06032" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06032</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting Visual Relationships Using Box Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google AI &amp; IST Austria</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.02136" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.02136</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Google AI Open Images - Visual Relationship Track</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Detect pairs of objects in particular relationships</li><li>kaggle: <a target="_blank" rel="noopener noreferrer" href="https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track" class="Link-sc-1brdqhf-0 cKRjba">https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context-Dependent Diffusion Network for Visual Relationship Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 2018 ACM Multimedia Conference</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.06213" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.06213</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Problem Reduction Approach for Visual Relationships Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018 Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.09828" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.09828</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring the Semantics for Visual Relationship Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02104" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02104</a></p><h1 id="face-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#face-detection" color="auto.gray.8" aria-label="Face Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Face Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-view Face Detection Using Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Yahoo</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1502.02766" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1502.02766</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/guoyilin/FaceDetection_CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guoyilin/FaceDetection_CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>From Facial Parts Responses to Face Detection: A Deep Learning Approach</strong></p><img src="http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/support/index.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015. CUHK</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1509.06451" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1509.06451</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compact Convolutional Neural Network Cascade for Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1508.01292" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1508.01292</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Bkmz21/FD-Evaluation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Bkmz21/FD-Evaluation</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Bkmz21/CompactCNNCascade" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Bkmz21/CompactCNNCascade</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Detection with End-to-End Integration of a ConvNet and a 3D Model</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.00850" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.00850</a></li><li>github(MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/tfwu/FaceDetection-ConvNet-3D" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tfwu/FaceDetection-ConvNet-3D</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.05413" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.05413</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards a Deep Learning Framework for Unconstrained Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: overlap with CMS-RCNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.05322" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.05322</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Supervised Transformer Network for Efficient Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.05477" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.05477</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UnitBox: An Advanced Object Detection Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2016</li><li>intro: University of Illinois at Urbana−Champaign &amp; Megvii Inc</li><li>keywords: IOULoss</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.01471" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.01471</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bootstrapping Face Detection with Hard Negative Examples</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>author: 万韶华 @ 小米.</li><li>intro: Faster R-CNN, hard negative mining. state-of-the-art on the FDDB dataset</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.02236" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.02236</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Grid Loss: Detecting Occluded Faces</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1609.00129" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1609.00129</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf</a></li><li>poster: <a target="_blank" rel="noopener noreferrer" href="http://www.eccv2016.org/files/posters/P-2A-34.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eccv2016.org/files/posters/P-2A-34.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Multi-Scale Cascade Fully Convolutional Network Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.03536" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.03536</a></li></ul><h2 id="mtcnn" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#mtcnn" color="auto.gray.8" aria-label="MTCNN permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MTCNN</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks</strong></p><img src="https://kpzhang93.github.io/MTCNN_face_detection_alignment/support/index.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html" class="Link-sc-1brdqhf-0 cKRjba">https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.02878" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.02878</a></li><li>github(official, Matlab): <a target="_blank" rel="noopener noreferrer" href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kpzhang93/MTCNN_face_detection_alignment</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pangyupo/mxnet_mtcnn_face_detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pangyupo/mxnet_mtcnn_face_detection</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DaFuCoding/MTCNN_Caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DaFuCoding/MTCNN_Caffe</a></li><li>github(MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Seanlinx/mtcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Seanlinx/mtcnn</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/foreverYoungGitHub/MTCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/foreverYoungGitHub/MTCNN</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CongWeilin/mtcnn-caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CongWeilin/mtcnn-caffe</a></li><li>github(OpenCV+OpenBlas): <a target="_blank" rel="noopener noreferrer" href="https://github.com/AlphaQi/MTCNN-light" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/AlphaQi/MTCNN-light</a></li><li>github(Tensorflow+golang): <a target="_blank" rel="noopener noreferrer" href="https://github.com/jdeng/goface" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jdeng/goface</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Detection using Deep Learning: An Improved Faster RCNN Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepIR Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.08289" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.08289</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Faceness-Net: Face Detection through Deep Facial Part Responses</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: An extended version of ICCV 2015 paper</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.08393" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.08393</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Path Region-Based Convolutional Neural Network for Accurate Detection of Unconstrained &quot;Hard Faces&quot;</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. MP-RCNN, MP-RPN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.09145" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.09145</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-To-End Face Detection and Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10818" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10818</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face R-CNN</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.01061" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.01061</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Detection through Scale-Friendly Deep Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.02863" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.02863</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale-Aware Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. SenseTime &amp; Tsinghua University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.09876" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.09876</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting Faces Using Inside Cascaded Contextual CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Tencent AI Lab &amp; SenseTime</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://ai.tencent.com/ailab/media/publications/Detecting_Faces_Using_Inside_Cascaded_Contextual_CNN.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://ai.tencent.com/ailab/media/publications/Detecting_Faces_Using_Inside_Cascaded_Contextual_CNN.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Branch Fully Convolutional Network for Face Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06330" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06330</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SSH: Single Stage Headless Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. University of Maryland</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.03979" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.03979</a></li><li>github(official, Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/mahyarnajibi/SSH" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mahyarnajibi/SSH</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dockerface: an easy to install and use Faster R-CNN face detector in a Docker container</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.04370" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.04370</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FaceBoxes: A CPU Real-time Face Detector with High Accuracy</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCB 2017</li><li>keywords: Rapidly Digested Convolutional Layers (RDCL), Multiple Scale Convolutional Layers (MSCL)</li><li>intro: the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.05234" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.05234</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/sfzhang15/FaceBoxes" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sfzhang15/FaceBoxes</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/zeusees/FaceBoxes" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zeusees/FaceBoxes</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>S3FD: Single Shot Scale-invariant Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. Chinese Academy of Sciences</li><li>intro: can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.05237" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.05237</a></li><li>github(Caffe, official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/sfzhang15/SFD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sfzhang15/SFD</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//clcarwin/SFD_pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//clcarwin/SFD_pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting Faces Using Region-based Fully Convolutional Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.05256" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.05256</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.07326" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.07326</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Attention Network: An effective Face Detector for the Occluded Faces</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07246" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07246</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Agglomeration Networks for Single Stage Face Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.00721" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.00721</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face Detection Using Improved Faster RCNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Huawei Cloud BU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02142" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02142</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PyramidBox: A Context-assisted Single Shot Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu, Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.07737" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.07737</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PyramidBox++: High Performance Detector for Finding Tiny Face</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese Academy of Sciences &amp; Baidu, Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.00386" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.00386</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Fast Face Detection Method via Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Neurocomputing</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10103" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10103</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. Beihang University &amp; CUHK &amp; Sensetime</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.05197" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.05197</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.06039" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.06039</a></li><li>github(binary library): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Jack-CV/PCN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Jack-CV/PCN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SFace: An Efficient Network for Face Detection in Large Scale Variations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Beihang University &amp; Megvii Inc. (Face++)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.06559" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.06559</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Survey of Face Detection on Low-quality Images</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.07362" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.07362</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anchor Cascade for Efficient Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Sydney</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.03363" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.03363</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE MMSP</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.12302" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.12302</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Selective Refinement Network for High Performance Face Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.02693" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.02693</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DSFD: Dual Shot Face Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.10220" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.10220</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Better Features for Face Detection with Feature Fusion and Segmentation Supervision</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08557" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08557</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FA-RPN: Floating Region Proposals for Face Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.05586" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.05586</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust and High Performance Face Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.02350" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.02350</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DAFE-FD: Density Aware Feature Enrichment for Face Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.05375" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.05375</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improved Selective Refinement Network for Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese Academy of Sciences &amp; JD AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.06651" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.06651</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting a single-stage method for face detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.01559" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.01559</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MSFD:Multi-Scale Receptive Field Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.04147" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.04147</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LFFD: A Light and Fast Face Detector for Edge Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.10633" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.10633</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/YonghaoHe/A-Light-and-Fast-Face-Detector-for-Edge-Devices" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YonghaoHe/A-Light-and-Fast-Face-Detector-for-Edge-Devices</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RetinaFace: Single-stage Dense Face Localisation in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.00641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.00641</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepinsight/insightface/tree/master/RetinaFace" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deepinsight/insightface/tree/master/RetinaFace</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR Workshop on Computer Vision for Augmented and Virtual Reality, 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.05047" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.05047</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HAMBox: Delving into Online High-quality Anchors Mining for Detecting Outer Faces</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Inc. &amp;  Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.09231" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.09231</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>KPNet: Towards Minimal Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.07543" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.07543</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ASFD: Automatic and Scalable Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Youtu Lab, Tencent &amp; Southeast University &amp; Xiamen University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.11228" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.11228</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TinaFace: Strong but Simple Baseline for Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Media Intelligence Technology Co.,Ltd</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.13183" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.13183</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Media-Smart/vedadet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Media-Smart/vedadet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MogFace: Rethinking Scale Augmentation on the Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Group &amp; Imperial College</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11139" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11139</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HLA-Face: Joint High-Low Adaptation for Low Light Face Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Peking University</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://daooshee.github.io/HLA-Face-Website/" class="Link-sc-1brdqhf-0 cKRjba">https://daooshee.github.io/HLA-Face-Website/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.01984" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.01984</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/daooshee/HLA-Face-Code" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daooshee/HLA-Face-Code</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>1st Place Solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tomorrow Advancing Life (TAL) Education Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.00818" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.00818</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MOS: A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.10953" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.10953</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect</a></li></ul><h2 id="detect-small-faces" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#detect-small-faces" color="auto.gray.8" aria-label="Detect Small Faces permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Detect Small Faces</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Finding Tiny Faces</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. CMU</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.cmu.edu/~peiyunh/tiny/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.cmu.edu/~peiyunh/tiny/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.04402" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.04402</a></li><li>github(official, Matlab): <a target="_blank" rel="noopener noreferrer" href="https://github.com/peiyunh/tiny" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/peiyunh/tiny</a></li><li>github(inference-only): <a target="_blank" rel="noopener noreferrer" href="https://github.com/chinakook/hr101_mxnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chinakook/hr101_mxnet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cydonia999/Tiny_Faces_in_Tensorflow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cydonia999/Tiny_Faces_in_Tensorflow</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting and counting tiny faces</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ENS Paris-Saclay. ExtendedTinyFaces</li><li>intro: Detecting and counting small objects - Analysis, review and application to counting</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06504" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06504</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/alexattia/ExtendedTinyFaces" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/alexattia/ExtendedTinyFaces</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Seeing Small Faces from Robust Anchor&#x27;s Perspective</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.09058" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.09058</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Face-MagNet: Magnifying Feature Maps to Detect Small Faces</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2018</li><li>keywords: Face Magnifier Network (Face-MageNet)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05258" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05258</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/po0ya/face-magnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/po0ya/face-magnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Face Detection via Learning Small Faces on Hard Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Johns Hopkins University &amp; Stanford University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11662" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11662</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bairdzhang/smallhardface" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bairdzhang/smallhardface</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SFA: Small Faces Attention Face Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Jilin University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.08402" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.08402</a></li></ul><h1 id="person-head-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#person-head-detection" color="auto.gray.8" aria-label="Person Head Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Person Head Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context-aware CNNs for person head detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.di.ens.fr/willow/research/headdetection/" class="Link-sc-1brdqhf-0 cKRjba">http://www.di.ens.fr/willow/research/headdetection/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07917" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07917</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aosokin/cnn_head_detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aosokin/cnn_head_detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting Heads using Feature Refine Net and Cascaded Multi-scale Architecture</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09256" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09256</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.03336" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.03336</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FCHD: A fast and accurate head detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.08766" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.08766</a></li><li>github(PyTorch, official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Relational Learning for Joint Head and Human Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: JointDet, head-body Relationship Discriminating Module (RDM)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.10674" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.10674</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Body-Face Joint Detection via Embedding and Head Hook</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wan_Body-Face_Joint_Detection_via_Embedding_and_Head_Hook_ICCV_2021_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://openaccess.thecvf.com/content/ICCV2021/papers/Wan_Body-Face_Joint_Detection_via_Embedding_and_Head_Hook_ICCV_2021_paper.pdf</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/AibeeDetect/BFJDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/AibeeDetect/BFJDet</a></li></ul><h1 id="pedestrian-detection--people-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pedestrian-detection--people-detection" color="auto.gray.8" aria-label="Pedestrian Detection / People Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pedestrian Detection / People Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pedestrian Detection aided by Deep Learning Semantic Tasks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.0069" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.0069</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Strong Parts for Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015. CUHK. DeepParts</li><li>intro: Achieving 11.89% average miss rate on Caltech Pedestrian Dataset</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Taking a Deeper Look at Pedestrians</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1501.05790" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1501.05790</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Channel Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1504.07339" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1504.07339</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/byangderek/CCF" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/byangderek/CCF</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end people detection in crowded scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.04878" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.04878</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Russell91/reinspect" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Russell91/reinspect</a></li><li>ipn: <a target="_blank" rel="noopener noreferrer" href="http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb" class="Link-sc-1brdqhf-0 cKRjba">http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=QeWl0h3kQ24" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=QeWl0h3kQ24</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Complexity-Aware Cascades for Deep Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1507.05348" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1507.05348</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep convolutional neural networks for pedestrian detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.03608" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.03608</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DenisTome/DeepPed" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DenisTome/DeepPed</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale-aware Fast R-CNN for Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1510.08160" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1510.08160</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>New algorithm improves speed and accuracy of pedestrian detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.eurekalert.org/pub_releases/2016-02/uoc--nai020516.php" class="Link-sc-1brdqhf-0 cKRjba">http://www.eurekalert.org/pub_releases/2016-02/uoc--nai020516.php</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pushing the Limits of Deep CNNs for Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from 11.7% to 8.9%&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.04525" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.04525</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Real-Time Deep Learning Pedestrian Detector for Robot Navigation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.04436" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.04436</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Real-Time Pedestrian Detector using Deep Learning for Human-Aware Navigation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.04441" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.04441</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Is Faster R-CNN Doing Well for Pedestrian Detection?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.07032" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.07032</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Deep Domain Adaptation for Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV Workshop 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.03269" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.03269</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reduced Memory Region Based Deep Convolutional Neural Network Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE 2016 ICCE-Berlin</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.02500" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.02500</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.03466" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.03466</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting People in Artwork with CNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016 Workshops</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.08871" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.08871</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Multi-camera People Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.04593" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.04593</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/" class="Link-sc-1brdqhf-0 cKRjba">http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.06283" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.06283</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/huangshiyu13/RPNplus" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huangshiyu13/RPNplus</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What Can Help Pedestrian Detection?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Tsinghua University &amp; Peking University &amp; Megvii Inc.</li><li>keywords: Faster R-CNN, HyperLearner</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.02757" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.02757</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Mao_What_Can_Help_CVPR_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2017/papers/Mao_What_Can_Help_CVPR_2017_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.08564" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.08564</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rotational Rectification Network for Robust Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; Volvo Construction</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.08917" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.08917</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>STD-PD: Generating Synthetic Training Data for Pedestrian Detection in Unannotated Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of North Carolina at Chapel Hill</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09100" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09100</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Too Far to See? Not Really! --- Pedestrian Detection with Scale-aware Localization Policy</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.00235" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.00235</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Aggregated Channels Network for Real-Time Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00476" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00476</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring Multi-Branch and High-Level Semantic Networks for Improving Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00872" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00872</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.02047" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.02047</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PCN: Part and Context Information for Pedestrian Detection with CNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: British Machine Vision Conference(BMVC) 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.04483" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.04483</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>intro: Hikvision Research Institute</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.01438" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.01438</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bi-box Regression for Pedestrian Detection and Occlusion Estimation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf</a></li><li>github(Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/rainofmine/Bi-box_Regression" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rainofmine/Bi-box_Regression</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pedestrian Detection with Autoregressive Network Phases</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Michigan State University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.00440" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.00440</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.09080" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.09080</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Center and Scale Prediction: A Box-free Approach for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: National University of Defense Technology &amp; Chinese Academy of Sciences &amp; Inception Institute of Artificial Intelligence (IIAI) &amp; Horizon Robotics Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02948" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02948</a></li><li>github(official, Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/liuwei16/CSP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liuwei16/CSP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Evading Real-Time Person Detectors by Adversarial T-shirt</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.11099" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.11099</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Coupled Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.08661" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.08661</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale Match for Tiny Person Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.10664" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.10664</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ucas-vg/TinyBenchmark" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ucas-vg/TinyBenchmark</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SM+: Refined Scale Match for Tiny Person Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.03558" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.03558</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Resisting the Distracting-factors in Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Beihang University &amp; Arizona State University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.07344" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.07344</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SADet: Learning An Efficient and Accurate Pedestrian Detector</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.13119" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.13119</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2020</li><li>intro: Tencent Youtu Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.13376" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.13376</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anchor-free Small-scale Multispectral Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.08418" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.08418</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HensoldtOptronicsCV/MultispectralPedestrianDetection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HensoldtOptronicsCV/MultispectralPedestrianDetection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LLA: Loss-aware Label Assignment for Dense Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.04307" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.04307</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Megvii-BaseDetection/LLA" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Megvii-BaseDetection/LLA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DETR for Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.06785" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.06785</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>V2F-Net: Explicit Decomposition of Occluded Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MEGVII Technology &amp; Texas A&amp;M University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.03106" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.03106</a></li></ul><h2 id="pedestrian-detection-in-a-crowd" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pedestrian-detection-in-a-crowd" color="auto.gray.8" aria-label="Pedestrian Detection in a Crowd permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pedestrian Detection in a Crowd</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Repulsion Loss: Detecting Pedestrians in a Crowd</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07752" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07752</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08407" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08407</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive NMS: Refining Pedestrian Detection in a Crowd</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.03629" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.03629</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PedHunter: Occlusion Robust Pedestrian Detector in Crowded Scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: SUR-PED</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.06826" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.06826</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Double Anchor R-CNN for Human Detection in a Crowd</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii Inc. (Face++) &amp; Tsinghua University &amp; Xi’an Jiaotong University &amp; Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.09998" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.09998</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CSID: Center, Scale, Identity and Density-aware Pedestrian Detection in a Crowd</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.09188" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.09188</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Head Enhanced Pedestrian Detection in a Crowd</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.11985" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.11985</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detection in Crowded Scenes: One Proposal, Multiple Predictions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.09163" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.09163</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Purkialo/CrowdDet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Purkialo/CrowdDet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Visible Feature Guidance for Crowd Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 RLQ Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.09993" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.09993</a></li></ul><h1 id="occluded-pedestrian-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#occluded-pedestrian-detection" color="auto.gray.8" aria-label="Occluded Pedestrian Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Occluded Pedestrian Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask-Guided Attention Network for Occluded Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.06160" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.06160</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Leotju/MGAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Leotju/MGAN</a></li></ul><h2 id="multispectral-pedestrian-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#multispectral-pedestrian-detection" color="auto.gray.8" aria-label="Multispectral Pedestrian Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Multispectral Pedestrian Detection</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multispectral Deep Neural Networks for Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2016 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.02644" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.02644</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: State Key Lab of CAD&amp;CG, Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05347" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05347</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multispectral Pedestrian Detection via Simultaneous Detection and Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.04818" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.04818</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Cross-Modality Disparity Problem in Multispectral Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.02645" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.02645</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Box-level Segmentation Supervised Deep Neural Networks for Accurate and Real-time Multispectral Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.05291" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.05291</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GFD-SSD: Gated Fusion Double SSD for Multispectral Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.06999" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.06999</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Domain Adaptation for Multispectral Pedestrian Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.03692" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.03692</a></p><h1 id="vehicle-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#vehicle-detection" color="auto.gray.8" aria-label="Vehicle Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Vehicle Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DAVE: A Unified Framework for Fast Vehicle Detection and Annotation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.04564" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.04564</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Evolving Boxes for fast Vehicle Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.00254" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.00254</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fine-Grained Car Detection for Visual Census Estimation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.02480" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.02480</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SINet: A Scale-insensitive Convolutional Neural Network for Fast Vehicle Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Intelligent Transportation Systems (T-ITS)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00433" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00433</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Label and Sample: Efficient Training of Vehicle Object Detector from Sparsely Labeled Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.08603" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.08603</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain Randomization for Scene-Specific Car Detection and Pose Estimation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.05939" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.05939</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShuffleDet: Real-Time Vehicle Detection Network in On-board Embedded UAV Imagery</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018, UAVision 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.06318" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.06318</a></li></ul><h1 id="traffic-sign-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#traffic-sign-detection" color="auto.gray.8" aria-label="Traffic-Sign Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Traffic-Sign Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Traffic-Sign Detection and Classification in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>project page(code+dataset): <a target="_blank" rel="noopener noreferrer" href="http://cg.cs.tsinghua.edu.cn/traffic-sign/" class="Link-sc-1brdqhf-0 cKRjba">http://cg.cs.tsinghua.edu.cn/traffic-sign/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf</a></li><li>code &amp; model: <a target="_blank" rel="noopener noreferrer" href="http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip" class="Link-sc-1brdqhf-0 cKRjba">http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Evaluating State-of-the-art Object Detector on Challenging Traffic Light Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 workshop</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w9/papers/Jensen_Evaluating_State-Of-The-Art_Object_CVPR_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2017_workshops/w9/papers/Jensen_Evaluating_State-Of-The-Art_Object_CVPR_2017_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting Small Signs from Large Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Conference on Information Reuse and Integration (IRI) 2017 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.08574" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.08574</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Localized Traffic Sign Detection with Multi-scale Deconvolution Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.10428" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.10428</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting Traffic Lights by Single Shot Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ITSC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.02523" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.02523</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE 15th Conference on Computer and Robot Vision</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.07987" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.07987</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=_YmogPzBXOw&amp;feature=youtu.be" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=_YmogPzBXOw&amp;feature=youtu.be</a></li></ul><h1 id="skeleton-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#skeleton-detection" color="auto.gray.8" aria-label="Skeleton Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Skeleton Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs</strong></p><img src="https://camo.githubusercontent.com/88a65f132aa4ae4b0477e3ad02c13cdc498377d9/687474703a2f2f37786e37777a2e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f44656570536b656c65746f6e2e706e673f696d61676556696577322f322f772f353030" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.09446" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.09446</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zeakey/DeepSkeleton" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zeakey/DeepSkeleton</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.03659" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.03659</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SRN: Side-output Residual Network for Object Symmetry Detection in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.02243" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.02243</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KevinKecc/SRN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KevinKecc/SRN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hi-Fi: Hierarchical Feature Integration for Skeleton Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.01849" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.01849</a></p><h1 id="fruit-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#fruit-detection" color="auto.gray.8" aria-label="Fruit Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fruit Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Fruit Detection in Orchards</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.03677" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.03677</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Journal of Field Robotics in May 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://confluence.acfr.usyd.edu.au/display/AGPub/" class="Link-sc-1brdqhf-0 cKRjba">http://confluence.acfr.usyd.edu.au/display/AGPub/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.08120" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.08120</a></li></ul><h2 id="shadow-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#shadow-detection" color="auto.gray.8" aria-label="Shadow Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Shadow Detection</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.09283" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.09283</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A+D-Net: Shadow Detection with Adversarial Shadow Attenuation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01361" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01361</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02478" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02478</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Direction-aware Spatial Context Features for Shadow Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.04142" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.04142</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Direction-aware Spatial Context Features for Shadow Detection and Removal</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Chinese University of Hong Kong &amp; The Hong Kong Polytechnic University</li><li>arxiv:  <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.04635" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.04635</a></li></ul><h1 id="others-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#others-detection" color="auto.gray.8" aria-label="Others Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Others Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Deformation Network for Object Landmark Localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.01014" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.01014</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fashion Landmark Detection in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03049" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03049</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/liuziwei7/fashion-landmarks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liuziwei7/fashion-landmarks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Fast and Accurate Fashion Item Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Kuznech Inc.</li><li>intro: MultiBox and Fast R-CNN</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OSMDeepOD - OSM and Deep Learning based Object Detection from Aerial Imagery (formerly known as &quot;OSM-Crosswalk-Detection&quot;)</strong></p><img src="https://raw.githubusercontent.com/geometalab/OSMDeepOD/master/imgs/process.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/geometalab/OSMDeepOD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/geometalab/OSMDeepOD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Selfie Detection by Synergy-Constraint Based Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:  IEEE SITIS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.04357" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.04357</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Associative Embedding:End-to-End Learning for Joint Detection and Grouping</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05424" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05424</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Cuboid Detection: Beyond 2D Bounding Boxes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; Magic Leap</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.10010" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.10010</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03019" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03019</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Logo Detection with Data Expansion by Synthesising Context</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.09322" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.09322</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scalable Deep Learning Logo Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.11417" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.11417</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.00307" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.00307</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Automatic Handgun Detection Alarm in Videos Using Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.05147" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.05147</a></li><li>results: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SihamTabik/Pistol-Detection-in-Videos" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SihamTabik/Pistol-Detection-in-Videos</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Objects as context for part detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.09529" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.09529</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using Deep Networks for Drone Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AVSS 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.05726" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.05726</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.01642" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.01642</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Target Driven Instance Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.04610" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.04610</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepVoting: An Explainable Framework for Semantic Part Detection under Partial Occlusion</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.04577" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.04577</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.06288" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.06288</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SeokjuLee/VPGNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SeokjuLee/VPGNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Grab, Pay and Eat: Semantic Food Detection for Smart Restaurants</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05128" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05128</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ReMotENet: Efficient Relevant Motion Event Detection for Large-scale Home Surveillance Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02031" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02031</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Object Detection Methods for Ecological Camera Trap Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Conference of Computer and Robot Vision. University of Guelph</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10842" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10842</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.05525" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.05525</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards End-to-End Lane Detection: an Instance Segmentation Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.05591" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.05591</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MaybeShewill-CV/lanenet-lane-detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MaybeShewill-CV/lanenet-lane-detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Densely Supervised Grasp Detector (DSGD)</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.03962" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.03962</a></p><h1 id="object-proposal" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#object-proposal" color="auto.gray.8" aria-label="Object Proposal permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Object Proposal</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.04445" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.04445</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aghodrati/deepproposal" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aghodrati/deepproposal</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale-aware Pixel-wise Object Proposal Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Image Processing</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.04798" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.04798</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2016. AttractioNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.04446" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.04446</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gidariss/AttractioNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gidariss/AttractioNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Segment Object Proposals via Recursive Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01057" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01057</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Detection with Diverse Proposals</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>keywords: differentiable Determinantal Point Process (DPP) layer, Learning Detection with Diverse Proposals (LDDP)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03533" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03533</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: product detection</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.06752" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.06752</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Small Object Proposals for Company Logo Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICMR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.08881" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.08881</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Open Logo Detection Challenge</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>keywords: QMUL-OpenLogo</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://qmul-openlogo.github.io/" class="Link-sc-1brdqhf-0 cKRjba">https://qmul-openlogo.github.io/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.01964" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.01964</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AttentionMask: Attentive, Efficient Object Proposal Generation Focusing on Small Objects</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACCV 2018 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08728" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08728</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chwilms/AttentionMask" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chwilms/AttentionMask</a></li></ul><h1 id="localization" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#localization" color="auto.gray.8" aria-label="Localization permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Localization</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Bounding Boxes: Precise Localization of Objects in Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PhD Thesis</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html</a></li><li>phd-thesis: <a target="_blank" rel="noopener noreferrer" href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf</a></li><li>github(&quot;SDS using hypercolumns&quot;): <a target="_blank" rel="noopener noreferrer" href="https://github.com/bharath272/sds" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bharath272/sds</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1503.00949" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1503.00949</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weakly Supervised Object Localization Using Size Estimates</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04314" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04314</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Active Object Localization with Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>keywords: Markov Decision Process</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1511.06015" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1511.06015</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Localizing objects using referring expressions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>keywords: LSTM, multiple instance learning (MIL)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/varun-nagaraja/referring-expressions" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/varun-nagaraja/referring-expressions</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LocNet: Improving Localization Accuracy for Object Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.07763" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.07763</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gidariss/LocNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gidariss/LocNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Features for Discriminative Localization</strong></p><img src="http://cnnlocalization.csail.mit.edu/framework.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://cnnlocalization.csail.mit.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://cnnlocalization.csail.mit.edu/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.04150" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.04150</a></li><li>github(Tensorflow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/jazzsaxmafia/Weakly_detector" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jazzsaxmafia/Weakly_detector</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/metalbubble/CAM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/metalbubble/CAM</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tdeboissiere/VGG16CAM-keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tdeboissiere/VGG16CAM-keras</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization</strong></p><img src="http://www.di.ens.fr/willow/research/contextlocnet/model.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.di.ens.fr/willow/research/contextlocnet/" class="Link-sc-1brdqhf-0 cKRjba">http://www.di.ens.fr/willow/research/contextlocnet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.04331" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.04331</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vadimkantorov/contextlocnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vadimkantorov/contextlocnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ensemble of Part Detectors for Simultaneous Classification and Localization</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.10034" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.10034</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>STNet: Selective Tuning of Convolutional Networks for Object Localization</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.06418" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.06418</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Soft Proposal Networks for Weakly Supervised Object Localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.01829" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.01829</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.08295" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.08295</a></li></ul><h1 id="tutorials--talks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tutorials--talks" color="auto.gray.8" aria-label="Tutorials / Talks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tutorials / Talks</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Feature Maps: Elements of efficient (and accurate) CNN-based object detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Good Practices for Recognition &amp; Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Hikvision Research Institute. Supervised Data Augmentation (SDA)</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Work in progress: Improving object detection and instance segmentation for small objects</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/presentation/d/1OTfGn6mLe1VWE8D0q6Tu_WwFTSoLGd4OF8WCYnOWcVo/edit#slide=id.g37418adc7a_0_229" class="Link-sc-1brdqhf-0 cKRjba">https://docs.google.com/presentation/d/1OTfGn6mLe1VWE8D0q6Tu_WwFTSoLGd4OF8WCYnOWcVo/edit#slide=id.g37418adc7a_0_229</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection with Deep Learning: A Review</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.05511" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.05511</a></p><h1 id="projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detectron</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: FAIR&#x27;s research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/Detectron" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/Detectron</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detectron2</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Detectron2 is FAIR&#x27;s next-generation platform for object detection and segmentation.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/detectron2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/detectron2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MMDetection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MMDetection: Open MMLab Detection Toolbox and Benchmark</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.07155" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.07155</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/open-mmlab/mmdetection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/open-mmlab/mmdetection</a></li><li>docs: <a target="_blank" rel="noopener noreferrer" href="https://mmdetection.readthedocs.io/en/latest/" class="Link-sc-1brdqhf-0 cKRjba">https://mmdetection.readthedocs.io/en/latest/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A Simple and Versatile Framework for Object Detection and Instance Recognition</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TuSimple/simpledet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TuSimple/simpledet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AdelaiDet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AdelaiDet is an open source toolbox for multiple instance-level detection and recognition tasks.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aim-uofa/AdelaiDet/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aim-uofa/AdelaiDet/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TensorBox: a simple framework for training neural networks to detect objects in images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;The basic model implements the simple and robust GoogLeNet-OverFeat algorithm.
We additionally provide an implementation of the <a target="_blank" rel="noopener noreferrer" href="https://github.com/Russell91/ReInspect/" class="Link-sc-1brdqhf-0 cKRjba">ReInspect</a> algorithm&quot;</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Russell91/TensorBox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Russell91/TensorBox</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NanoDet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Super fast and lightweight anchor-free object detection model. Real-time on mobile devices.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://github.com/RangiLyu/nanodet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/RangiLyu/nanodet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object detection in torch: Implementation of some object detection frameworks in torch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fmassa/object-detection.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fmassa/object-detection.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using DIGITS to train an Object Detection network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FCN-MultiBox Detector</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Full convolution MultiBox Detector (like SSD) implemented in Torch.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/teaonly/FMD.torch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/teaonly/FMD.torch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>KittiBox: A car detection model implemented in Tensorflow.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: MultiNet</li><li>intro: KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MarvinTeichmann/KittiBox" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MarvinTeichmann/KittiBox</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deformable Convolutional Networks + MST + Soft-NMS</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bharatsingh430/Deformable-ConvNets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bharatsingh430/Deformable-ConvNets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How to Build a Real-time Hand-Detector using Neural Networks (SSD) on Tensorflow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://towardsdatascience.com/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce" class="Link-sc-1brdqhf-0 cKRjba">https://towardsdatascience.com/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//victordibia/handtracking" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//victordibia/handtracking</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Metrics for object detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Most popular metrics used to evaluate object detection algorithms</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rafaelpadilla/Object-Detection-Metrics" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rafaelpadilla/Object-Detection-Metrics</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MobileNetv2-SSDLite</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Caffe implementation of SSD and SSDLite detection on MobileNetv2, converted from tensorflow.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chuanqi305/MobileNetv2-SSDLite" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chuanqi305/MobileNetv2-SSDLite</a></li></ul><h1 id="leaderboard" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#leaderboard" color="auto.gray.8" aria-label="Leaderboard permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Leaderboard</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detection Results: VOC2012</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Competition &quot;comp4&quot; (train on additional data)</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4" class="Link-sc-1brdqhf-0 cKRjba">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4</a></li></ul><h1 id="tools" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tools" color="auto.gray.8" aria-label="Tools permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tools</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BeaverDam: Video annotation tool for deep learning training labels</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/antingshen/BeaverDam" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/antingshen/BeaverDam</a></p><h1 id="blogs" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#blogs" color="auto.gray.8" aria-label="Blogs permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blogs</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Neural Networks for Object Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://rnd.azoft.com/convolutional-neural-networks-object-detection/" class="Link-sc-1brdqhf-0 cKRjba">http://rnd.azoft.com/convolutional-neural-networks-object-detection/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Introducing automatic object detection to visual search (Pinterest)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Faster R-CNN</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search" class="Link-sc-1brdqhf-0 cKRjba">https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4" class="Link-sc-1brdqhf-0 cKRjba">https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4</a></li><li>review: <a target="_blank" rel="noopener noreferrer" href="https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D" class="Link-sc-1brdqhf-0 cKRjba">https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Object Detection with DIGITS</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/" class="Link-sc-1brdqhf-0 cKRjba">https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Analyzing The Papers Behind Facebook&#x27;s Computer Vision Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: DeepMask, SharpMask, MultiPathNet</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook&#x27;s-Computer-Vision-Approach/" class="Link-sc-1brdqhf-0 cKRjba">https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook&#x27;s-Computer-Vision-Approach/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Easily Create High Quality Object Detectors with Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: dlib v19.2</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://blog.dlib.net/2016/10/easily-create-high-quality-object.html" class="Link-sc-1brdqhf-0 cKRjba">http://blog.dlib.net/2016/10/easily-create-high-quality-object.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How to Train a Deep-Learned Object Detection Model in the Microsoft Cognitive Toolkit</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/" class="Link-sc-1brdqhf-0 cKRjba">https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Detection in Satellite Imagery, a Low Overhead Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>part 1: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9</a></li><li>part 2: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>part 1: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of</a></li><li>part 2: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Faster R-CNN Pedestrian and Car Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/" class="Link-sc-1brdqhf-0 cKRjba">https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/</a></li><li>ipn: <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb" class="Link-sc-1brdqhf-0 cKRjba">https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/bigsnarfdude/Faster-RCNN_TF" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/bigsnarfdude/Faster-RCNN_TF</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Small U-Net for vehicle detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Region of interest pooling explained</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://deepsense.io/region-of-interest-pooling-explained/" class="Link-sc-1brdqhf-0 cKRjba">https://deepsense.io/region-of-interest-pooling-explained/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepsense-io/roi-pooling" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/deepsense-io/roi-pooling</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Supercharge your Computer Vision models with the TensorFlow Object Detection API</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/object_detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/object_detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Understanding SSD MultiBox — Real-Time Object Detection In Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab" class="Link-sc-1brdqhf-0 cKRjba">https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One-shot object detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://machinethink.net/blog/object-detection/" class="Link-sc-1brdqhf-0 cKRjba">http://machinethink.net/blog/object-detection/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An overview of object detection: one-stage methods</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.jeremyjordan.me/object-detection-one-stage/" class="Link-sc-1brdqhf-0 cKRjba">https://www.jeremyjordan.me/object-detection-one-stage/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep learning object detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A paper list of object detection using deep learning.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hoya012/deep_learning_object_detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hoya012/deep_learning_object_detection</a></li></ul><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/28/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>