<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Papers High-Performance Neural Networks for Visual Object Classification intro: &quot;reduced network parameters by randomly removing connection…"/><meta data-react-helmet="true" name="twitter:title" content="Acceleration and Model Compression"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-28T19:22:29.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Papers High-Performance Neural Networks for Visual Object Classification intro: &quot;reduced network parameters by randomly removing connection…"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Acceleration and Model Compression"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Papers High-Performance Neural Networks for Visual Object Classification intro: &quot;reduced network parameters by randomly removing connection…"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,fdzjHV,HGjBQ,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Acceleration and Model Compression - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/HyperMedia Library/">HyperMedia Library</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">awesomeLists</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018 - Web Civics BizPlan/">EXECUTIVE SUMMARY</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#compressing-deep-neural-network" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Compressing Deep Neural Network</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pruning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pruning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#low-precision-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Low-Precision Networks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#quantized-neural-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Quantized Neural Networks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#binary-convolutional-neural-networks--binarized-neural-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Binary Convolutional Neural Networks / Binarized Neural Networks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#accelerating--fast-algorithms" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Accelerating / Fast Algorithms</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#code-optimization" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Code Optimization</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#optnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">OptNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#talks--videos" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Talks / Videos</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Acceleration and Model Compression</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#compressing-deep-neural-network" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Compressing Deep Neural Network</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pruning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pruning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#low-precision-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Low-Precision Networks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#quantized-neural-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Quantized Neural Networks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#binary-convolutional-neural-networks--binarized-neural-networks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Binary Convolutional Neural Networks / Binarized Neural Networks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#accelerating--fast-algorithms" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Accelerating / Fast Algorithms</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#code-optimization" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Code Optimization</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#optnet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">OptNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#blogs" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Blogs</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#talks--videos" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Talks / Videos</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li></ul></div></div><h1 id="papers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers" color="auto.gray.8" aria-label="Papers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>High-Performance Neural Networks for Visual Object Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;reduced network parameters by randomly removing connections before training&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1102.0183" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1102.0183</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Predicting Parameters in Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;decomposed the weighting matrix into two low-rank matrices&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1306.0543" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1306.0543</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neurons vs Weights Pruning in Artificial Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://journals.ru.lv/index.php/ETR/article/view/166" class="Link-sc-1brdqhf-0 cKRjba">http://journals.ru.lv/index.php/ETR/article/view/166</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploiting Linear Structure Within Convolutional Networks for Efﬁcient Evaluation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;presented a series of low-rank decomposition designs for convolutional kernels.
singular value decomposition was adopted for the matrix factorization&quot;</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>cuDNN: Efficient Primitives for Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1410.0759" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1410.0759</a></li><li>download: <a target="_blank" rel="noopener noreferrer" href="https://developer.nvidia.com/cudnn" class="Link-sc-1brdqhf-0 cKRjba">https://developer.nvidia.com/cudnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient and accurate approximations of nonlinear convolutional networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;considered the subsequent nonlinear units while learning the low-rank decomposition&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1411.4229" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1411.4229</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Neural Networks at Constrained Time Cost</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1412.1710" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1412.1710</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Flattened Convolutional Neural Networks for Feedforward Acceleration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.5474" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.5474</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jhjin/flattened-cnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jhjin/flattened-cnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compressing Deep Convolutional Networks using Vector Quantization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;this paper showed that vector quantization had a clear advantage
over matrix factorization methods in compressing fully-connected layers.&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.6115" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.6115</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;a low-rank CPdecomposition was adopted to
transform a convolutional layer into multiple layers of lower complexity&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.6553" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.6553</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Fried Convnets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;fully-connected layers were replaced by a single “Fastfood” layer for end-to-end training with convolutional layers&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.7149" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.7149</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Convolutional Nets With fbfft: A GPU Performance Evaluation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook. ICLR 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1412.7580" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1412.7580</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="http://facebook.github.io/fbcunn/fbcunn/" class="Link-sc-1brdqhf-0 cKRjba">http://facebook.github.io/fbcunn/fbcunn/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Caffe con Troll: Shallow Ideas to Speed Up Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.04343" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.04343</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compressing Neural Networks with the Hashing Trick</strong></p><img src="http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/hashednets.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: HashedNets. ICML 2015</li><li>intro: &quot;randomly grouped connection weights into hash buckets, and then fine-tuned network parameters with back-propagation&quot;</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.04788" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.04788</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/HashedNets.zip" class="Link-sc-1brdqhf-0 cKRjba">http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/HashedNets.zip</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1504.08362" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1504.08362</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mfigurnov/perforated-cnn-matconvnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mfigurnov/perforated-cnn-matconvnet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mfigurnov/perforated-cnn-caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mfigurnov/perforated-cnn-caffe</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating Very Deep Convolutional Networks for Classification and Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;considered the subsequent nonlinear units while learning the low-rank decomposition&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.06798" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.06798</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast ConvNets Using Group-wise Brain Damage</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;applied group-wise pruning to the convolutional tensor
to decompose it into the multiplications of thinned dense matrices&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.02515" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.02515</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning both Weights and Connections for Efficient Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.02626" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.02626</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Data-free parameter pruning for Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;proposed to remove redundant neurons instead of network connections&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1507.06149" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1507.06149</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2016 Best Paper</li><li>intro: &quot;reduced the size of AlexNet by 35x from 240MB to 6.9MB, the size of VGG16 by 49x from 552MB to 11.3MB, with no loss of accuracy&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.00149" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.00149</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="http://videolectures.net/iclr2016_han_deep_compression/" class="Link-sc-1brdqhf-0 cKRjba">http://videolectures.net/iclr2016_han_deep_compression/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Structured Transforms for Small-Footprint Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1510.01722" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1510.01722</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning" class="Link-sc-1brdqhf-0 cKRjba">https://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.06706" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.06706</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/seung-lab/znn-release" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/seung-lab/znn-release</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reducing the Training Time of Neural Networks by Partitioning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.02954" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.02954</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional neural networks with low-rank regularization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06067" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06067</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chengtaipu/lowrankcnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chengtaipu/lowrankcnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNNdroid: Open Source Library for GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1511.07376" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1511.07376</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://dl.acm.org/authorize.cfm?key=N14731" class="Link-sc-1brdqhf-0 cKRjba">http://dl.acm.org/authorize.cfm?key=N14731</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://sharif.edu/~matin/pub/2016_mm_slides.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://sharif.edu/~matin/pub/2016_mm_slides.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ENCP/CNNdroid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ENCP/CNNdroid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EIE: Efficient Inference Engine on Compressed Deep Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ISCA 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.01528" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.01528</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Tables Ensemble: classification in microseconds</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.04489" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.04489</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepScale &amp; UC Berkeley</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1602.07360" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1602.07360</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DeepScale/SqueezeNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DeepScale/SqueezeNet</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://songhan.github.io/SqueezeNet-Deep-Compression/" class="Link-sc-1brdqhf-0 cKRjba">http://songhan.github.io/SqueezeNet-Deep-Compression/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/songhan/SqueezeNet-Deep-Compression" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/songhan/SqueezeNet-Deep-Compression</a></li><li>note: <a target="_blank" rel="noopener noreferrer" href="https://www.evernote.com/shard/s146/sh/108eea91-349b-48ba-b7eb-7ac8f548bee9/5171dc6b1088fba05a4e317f7f5d32a3" class="Link-sc-1brdqhf-0 cKRjba">https://www.evernote.com/shard/s146/sh/108eea91-349b-48ba-b7eb-7ac8f548bee9/5171dc6b1088fba05a4e317f7f5d32a3</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/DT42/squeezenet_demo" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DT42/squeezenet_demo</a></li><li>github(Keras): <a target="_blank" rel="noopener noreferrer" href="https://github.com/rcmalli/keras-squeezenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/rcmalli/keras-squeezenet</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/gsp-27/pytorch_Squeezenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gsp-27/pytorch_Squeezenet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SqueezeNet-Residual</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Residual-SqueezeNet improves the top-1 accuracy of SqueezeNet by 2.9% on ImageNet without changing the model size(only 4.8MB).</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/songhan/SqueezeNet-Residual" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/songhan/SqueezeNet-Residual</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lab41 Reading Group: SqueezeNet</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medium.com/m/global-identity?redirectUrl=https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/m/global-identity?redirectUrl=https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Simplified_SqueezeNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: An improved version of SqueezeNet networks</li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/NidabaSystems/Simplified_SqueezeNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NidabaSystems/Simplified_SqueezeNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SqueezeNet Keras Dogs vs. Cats demo</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chasingbob/squeezenet-keras" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chasingbob/squeezenet-keras</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Neural Networks using Logarithmic Data Representation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.01025" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.01025</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://niclane.org/pubs/deepx_ipsn.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://niclane.org/pubs/deepx_ipsn.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hardware-oriented Approximation of Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.03168" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.03168</a></li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://ristretto.lepsucd.com/" class="Link-sc-1brdqhf-0 cKRjba">http://ristretto.lepsucd.com/</a></li><li>github(&quot;Ristretto: Caffe-based approximation of convolutional neural networks&quot;): <a target="_blank" rel="noopener noreferrer" href="https://github.com/pmgysel/caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pmgysel/caffe</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Neural Networks Under Stress</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.03498" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.03498</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MicaelCarvalho/DNNsUnderStress" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MicaelCarvalho/DNNsUnderStress</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks using Angle Sensitive Pixels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.03621" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.03621</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU).
For the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters,
while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1605.06489" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1605.06489</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Functional Hashing for Compressing Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: FunHashNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.06560" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.06560</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.06402" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.06402</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YodaNN: An Ultra-Low Power Convolutional Neural Network Accelerator Based on Binary Weights</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.05487" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.05487</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Structured Sparsity in Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03665" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03665</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial &quot;Bottleneck&quot; Structure</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1608.04337" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1608.04337</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Network Surgery for Efficient DNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>intro: compress the number of parameters in LeNet-5 and AlexNet by a factor of 108× and 17.7× respectively</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04493" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04493</a></li><li>github(official. Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/yiwenguo/Dynamic-Network-Surgery" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yiwenguo/Dynamic-Network-Surgery</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scalable Compression of Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.07365" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.07365</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pruning Filters for Efficient ConvNets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS Workshop on Efficient Methods for Deep Neural Networks (EMDNN), 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.08710" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.08710</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fixed-point Factorized Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01972" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01972</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ultimate tensorization: compressing convolutional and FC layers alike</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016 workshop: Learning with Tensors: Why Now and How?</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.03214" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.03214</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/timgaripov/TensorNet-TF" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/timgaripov/TensorNet-TF</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05128" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05128</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Net-Trim: A Layer-wise Convex Pruning of Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05162" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05162</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LCNN: Lookup-based Convolutional Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;Our fastest LCNN offers 37.6x speed up over AlexNet while maintaining 44.3% top-1 accuracy.&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06473" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06473</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Tensor Convolution on Multicores</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: present the first practical CPU implementation of tensor convolution optimized for deep networks of small kernels</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06565" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06565</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Sparse Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06694" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06694</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FINN: A Framework for Fast, Scalable Binarized Neural Network Inference</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Xilinx Research Labs &amp; Norwegian University of Science and Technology &amp; University of Sydney</li><li>intro: 25th International Symposium on Field-Programmable Gate Arrays</li><li>keywords: FPGA</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.07119" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.07119</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Xilinx/BNN-PYNQ" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Xilinx/BNN-PYNQ</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning with INT8 Optimization on Xilinx Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;Xilinx&#x27;s integrated DSP architecture can achieve 1.75X solution-level performance
at INT8 deep learning operations than other FPGA DSP architectures&quot;</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Parameter Compression of Recurrent Neural Networks and Degredation of Short-term Memory</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.00891" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.00891</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An OpenCL(TM) Deep Learning Accelerator on Arria 10</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: FPGA 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.03534" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.03534</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU &amp; Universitat Paderborn]</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.04465" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.04465</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DL-gleaning: An Approach For Improving Inference Speed And Accuracy</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Electronics Telecommunications Research Institute (ETRI)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openreview.net/pdf?id=Hynn8SHOx" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.net/pdf?id=Hynn8SHOx</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Energy Saving Additive Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Middle East Technical University &amp; Bilkent University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.02676" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.02676</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Soft Weight-Sharing for Neural Network Compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2017. University of Amsterdam</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.04008" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.04008</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.04071" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.04071</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ARM Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.03073" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.03073</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DyVEDeep: Dynamic Variable Effort Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.01137" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.01137</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bayesian Compression for Deep Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08665" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08665</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Kernel Redundancy Removing Policy for Convolutional Neural Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.10748" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.10748</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gated XNOR Networks: Deep Neural Networks with Ternary Weights and Activations under a Unified Discretization Framework</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: discrete state transition (DST)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.09283" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.09283</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SEP-Nets: Small and Effective Pattern Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Iowa &amp; Snap Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.03912" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.03912</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MEC: Memory-efficient Convolution for Deep Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.06873" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.06873</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Data-Driven Sparse Structure Selection for Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.01213" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.01213</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An End-to-End Compression Framework Based on Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.00838" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.00838</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain-adaptive deep network compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.01041" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.01041</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mmasana/DALR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mmasana/DALR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Binary-decomposed DCNN for accelerating computation and compressing model without retraining</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.04731" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.04731</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Efficiency in Convolutional Neural Network with Multilinear Filters</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.09902" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.09902</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Survey of Model Compression and Acceleration for Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Signal Processing Magazine. IBM Thoms J. Watson Research Center &amp; Tsinghua University &amp; Huazhong University of Science and Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.09282" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.09282</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compression-aware Training of Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.02638" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.02638</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06528" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06528</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reducing Deep Network Complexity with Fourier Transform Methods</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Harvard University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.01451" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.01451</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EffNet: An Efficient Structure for Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Aptiv &amp; University of Wupperta</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06434" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06434</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Universal Deep Neural Network Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02271" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02271</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Paraphrasing Complex Network: Network Compression via Factor Transfer</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.04977" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.04977</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compressing Neural Networks using the Variational Information Bottleneck</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; ShanghaiTech University &amp; Microsoft Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.10399" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.10399</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Network Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10750" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10750</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Expanding a robot&#x27;s life: Low power object recognition via FPGA-based DCNN deployment</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MOCAST 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00512" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00512</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating CNN inference on FPGAs: A Survey</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: <!-- -->[Institut Pascal]</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.01683" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.01683</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Doubly Nested Network for Resource-Efficient Inference</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.07568" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.07568</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Smallify: Learning Network Size while Training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MIT</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.03723" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.03723</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on Embedded FPGAs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 27th International Symposium on Field-Programmable Gate Arrays, February 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08634" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08634</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascaded Projection: End-to-End Network Compression and Acceleration</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.04988" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.04988</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FALCON: Fast and Lightweight Convolution for Compressing and Accelerating CNN</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.11321" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.11321</a></p><h1 id="compressing-deep-neural-network" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#compressing-deep-neural-network" color="auto.gray.8" aria-label="Compressing Deep Neural Network permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Compressing Deep Neural Network</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICML 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.09228" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.09228</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Sandbox3aster/Deep-K-Means-pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Sandbox3aster/Deep-K-Means-pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Optimize Deep Convolutional Neural Network with Ternarized Weights and High Accuracy</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Central Florida &amp; Tencent AI lab, Seattle</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.07948" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.07948</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.05240" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.05240</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.01330" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.01330</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Framework for Fast and Efficient Neural Network Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12781" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12781</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12673" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12673</a></p><h1 id="pruning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pruning" color="auto.gray.8" aria-label="Pruning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pruning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. Nanjing University &amp; Shanghai Jiao Tong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06342" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06342</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Roll920/ThiNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Roll920/ThiNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neuron Pruning for Compressing Deep Networks using Maxout Architectures</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: GCPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06838" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06838</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional Network with Bayesian Optimization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017 oral. Simon Fraser University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09102" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09102</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Prune the Convolutional Neural Networks with Sparse Shrink</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02439" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02439</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NISP: Pruning Networks using Neuron Importance Score Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland &amp; IBM T. J. Watson Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.05908" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.05908</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Automated Pruning for Deep Neural Network Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01721" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01721</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Prune Filters in Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.07365" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.07365</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.10447" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.10447</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A novel channel pruning method for deep neural network compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.11394" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.11394</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PCAS: Pruning Channels with Attention Statistics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Oki Electric Industry Co., Ltd</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.05382" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.05382</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.06866" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.06866</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/he-y/soft-filter-pruning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/he-y/soft-filter-pruning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive Deep Neural Networks Acceleration via Soft Filter Pruning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.07471" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.07471</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pruning neural networks: is it time to nip it in the bud?</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.04622" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.04622</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking the Value of Network Pruning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.05270" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.05270</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Channel Pruning: Feature Boosting and Suppression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.05331" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.05331</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Interpretable Convolutional Filter Pruning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.07322" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.07322</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive Weight Pruning of Deep Neural Networks using ADMM</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.07378" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.07378</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pruning Deep Neural Networks using Partial Least Squares</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.07610" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.07610</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/arturjordao/PruningNeuralNetworks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/arturjordao/PruningNeuralNetworks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hybrid Pruning: Thinner Sparse Networks for Fast Inference on Edge Devices</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.00482" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.00482</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Discrimination-aware Channel Pruning for Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.11809" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.11809</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stability Based Filter Pruning for Accelerating Deep CNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08321" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08321</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Structured Pruning for Efficient ConvNets via Incremental Regularization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2018 workshop on &quot;Compact Deep Neural Network Representation with Industrial Applications&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08390" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08390</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08589" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08589</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Layer Decomposition-Recomposition Framework for Neuron Pruning towards Accurate Lightweight Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019 as oral</li><li>intro: Hikvision Research Institute</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.06611" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.06611</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quantized Guided Pruning for Efficient Hardware Implementations of Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.11337" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.11337</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.07827" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.07827</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Partition Pruning: Parallelization-Aware Pruning for Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.11391" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.11391</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pruning from Scratch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; Ant Financial &amp; Huawei Noah’s Ark Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.12579" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.12579</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Global Sparse Momentum SGD for Pruning Very Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.12778" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.12778</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>openreview: <a target="_blank" rel="noopener noreferrer" href="https://openreview.net/forum?id=rJeUPlrYvr" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.et/forum?id=rJeUPlrYvr</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://openreview.net/pdf?id=rJeUPlrYvr" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.net/pdf?id=rJeUPlrYvr</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/anonymous47823493/FNNP" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/anonymous47823493/FNNP</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pruning Filter in Filter</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020</li><li>intro: Harbin Institute of Technology &amp; Tencent Youtu Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.14410" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.14410</a></li></ul><h1 id="low-precision-networks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#low-precision-networks" color="auto.gray.8" aria-label="Low-Precision Networks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Low-Precision Networks</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating Deep Convolutional Networks using low-precision and sparsity</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Intel Labs</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.00324" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.00324</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning with Low Precision by Half-wave Gaussian Quantization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: HWGQ-Net</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.00953" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.00953</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.03044" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.03044</a></li><li>openreview: <a target="_blank" rel="noopener noreferrer" href="https://openreview.net/forum?id=HyQJ-mclg&amp;noteId=HyQJ-mclg" class="Link-sc-1brdqhf-0 cKRjba">https://openreview.net/forum?id=HyQJ-mclg<!-- -->¬<!-- -->eId=HyQJ-mclg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.02393" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.02393</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gudovskiy/ShiftCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gudovskiy/ShiftCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Group</li><li>keywords: alternating direction method of multipliers (ADMM)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.09870" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.09870</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.01001" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.01001</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compressing Low Precision Deep Neural Networks Using Sparsity-Induced Regularization in Ternary Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICONIP 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.06262" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.06262</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Low Precision Deep Neural Networks through Regularization</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.00095" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.00095</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.04191" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.04191</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SQuantizer: Simultaneous Learning for Both Sparse and Low-precision Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Movidius, AIPG, Intel</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.08301" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.08301</a></li></ul><h1 id="quantized-neural-networks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#quantized-neural-networks" color="auto.gray.8" aria-label="Quantized Neural Networks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Quantized Neural Networks</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quantized Convolutional Neural Networks for Mobile Devices</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Q-CNN</li><li>intro: &quot;Extensive experiments on the ILSVRC-12 benchmark demonstrate
4 ∼ 6× speed-up and 15 ∼ 20× compression with merely one percentage loss of classification accuracy&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.06473" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.06473</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jiaxiang-wu/quantized-cnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jiaxiang-wu/quantized-cnn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Quantized Nets: A Deeper Understanding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland &amp; Cornell University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.02379" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.02379</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: the top-5 error rate of 4-bit quantized GoogLeNet model is 12.7%</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.07145" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.07145</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. Google</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.05877" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.05877</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Neural Network Compression with Single and Multiple Level Quantization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018. Shanghai Jiao Tong University &amp; University of Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.03289" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.03289</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quantizing deep convolutional networks for efficient inference: A whitepaper</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.08342" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.08342</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CascadeCNN: Pushing the Performance Limits of Quantisation in Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 28th International Conference on Field Programmable Logic &amp; Applications (FPL), 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.05053" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.05053</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bridging the Accuracy Gap for 2-bit Quantized Neural Networks (QNN)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IBM Research AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.06964" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.06964</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Training of Low-Precision Neural Network with Quantization Interval Parameters</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.05779" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.05779</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Differentiable Fine-grained Quantization for Deep Neural Network Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.10351" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.10351</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HAQ: Hardware-Aware Automated Quantization</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08886" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08886</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DNQ: Dynamic Network Quantization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University &amp; Qualcomm AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.02375" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.02375</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Trained Rank Pruning for Efficient Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University &amp; Qualcomm AI Research &amp; Duke University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.02402" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.02402</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Quantized Network with Auxiliary Gradient Module</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The University of Adelaide &amp; Australian Centre for Robotic Vision &amp; South China University of Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.11236" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.11236</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FLightNNs: Lightweight Quantized Deep Neural Networks for Fast and Accurate Inference</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Carnegie Mellon University</li><li>intro: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02835" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02835</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>And the Bit Goes Down: Revisiting the Quantization of Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research &amp; Univ Rennes</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.05686" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.05686</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05033" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05033</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bit Efficient Quantization for Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS workshop 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.04877" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.04877</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quantization Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.09464" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.09464</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Loss-aware Quantization for Multi-bit Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.08883" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.08883</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distribution Adaptive INT8 Quantization for Training CNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>intro: Machine Intelligence Technology Lab, Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.04782" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.04782</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distance-aware Quantization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.06983" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.06983</a></li></ul><h1 id="binary-convolutional-neural-networks--binarized-neural-networks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#binary-convolutional-neural-networks--binarized-neural-networks" color="auto.gray.8" aria-label="Binary Convolutional Neural Networks / Binarized Neural Networks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Binary Convolutional Neural Networks / Binarized Neural Networks</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1602.02830" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1602.02830</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1609.07061" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1609.07061</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.05279" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.05279</a></li><li>github(Torch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/mrastegari/XNOR-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mrastegari/XNOR-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>XNOR-Net++: Improved Binary Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.13863" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.13863</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.06160" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.06160</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A 7.663-TOPS 8.2-W Energy-efficient FPGA Accelerator for Binary Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.06392" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.06392</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Espresso: Efficient Forward Propagation for BCNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.07175" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.07175</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fpeder/espresso" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fpeder/espresso</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Binary Neural Networks (BNNs)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.09864" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.09864</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hpi-xnor" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hpi-xnor</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.02393" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.02393</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Embedded Vision Workshop (CVPRW). UC San Diego &amp; UC Los Angeles &amp; Cornell University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.04693" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.04693</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Embedded Binarized Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.02260" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.02260</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compact Hash Code Learning with Binary Deep Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Singapore University of Technology and Design</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02956" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02956</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Build a Compact Binary Neural Network through Bit-level Sensitivity and Data Pruning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.00904" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.00904</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>From Hashing to CNNs: Training BinaryWeight Networks via Hashing</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.02733" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.02733</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Energy Efficient Hadamard Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Binary Weight and Hadamard-transformed Image Network (BWHIN), Binary Weight Network (BWN), Hadamard-transformed Image Network (HIN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.05421" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.05421</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.07550" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.07550</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.00278" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.00278</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Compact Neural Networks with Binary Weights and Low Precision Activations</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.02631" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.02631</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training wide residual networks for deployment using a single bit for each weight</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.08530" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.08530</a></li><li>github(official, PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/szagoruyko/binary-wide-resnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/szagoruyko/binary-wide-resnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Composite Binary Decomposition Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.06668" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.06668</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Competitive Binary Neural Networks from Scratch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Potsdam</li><li>intro: BMXNet v2: An Open-Source Binary Neural Network Implementation Based on MXNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.01965" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.01965</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hpi-xnor/BMXNet-v2" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hpi-xnor/BMXNet-v2</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Regularizing Activation Distribution for Training Binarized Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Carnegie Mellon University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02823" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02823</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GBCNs: Genetic Binary Convolutional Networks for Enhancing the Performance of 1-bit DCNNs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.11634" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.11634</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Training Binary Neural Networks with Real-to-Binary Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2020</li><li>intro: Samsung AI Research Center, Cambridge, UK &amp; The University of Nottingham</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.11535" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.11535</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/brais-martinez/real2binary" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/brais-martinez/real2binary</a></li></ul><h1 id="accelerating--fast-algorithms" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#accelerating--fast-algorithms" color="auto.gray.8" aria-label="Accelerating / Fast Algorithms permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Accelerating / Fast Algorithms</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Algorithms for Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;2.6x as fast as Caffe when comparing CPU implementations&quot;</li><li>keywords: Winograd&#x27;s minimal filtering algorithms</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.09308" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.09308</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/andravin/wincnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andravin/wincnn</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://homes.cs.washington.edu/~cdel/presentations/Fast_Algorithms_for_Convolutional_Neural_Networks_Slides_reading_group_uw_delmundo_slides.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://homes.cs.washington.edu/~cdel/presentations/Fast_Algorithms_for_Convolutional_Neural_Networks_Slides_reading_group_uw_delmundo_slides.pdf</a></li><li>discussion: <a target="_blank" rel="noopener noreferrer" href="https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895</a></li><li>reddit: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/?" class="Link-sc-1brdqhf-0 cKRjba">https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/?</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Hong Kong Baptist University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.07724" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.07724</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.01406" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.01406</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Channel Pruning for Accelerating Very Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017. Megvii Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06168" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06168</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yihui-he/channel-pruning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yihui-he/channel-pruning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.04728" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.04728</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Efficient Convolutional Networks through Network Slimming</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.06519" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.06519</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SparCE: Sparsity aware General Purpose Core Extensions to Accelerate Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06315" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06315</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating Convolutional Neural Networks for Continuous Mobile Vision via Cache Reuse</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: CNNCache</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01670" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01670</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.07493" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.07493</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SBNet: Sparse Blocks Network for Fast Inference</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Uber</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://eng.uber.com/sbnet/" class="Link-sc-1brdqhf-0 cKRjba">https://eng.uber.com/sbnet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.02108" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.02108</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/uber/sbnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/uber/sbnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating deep neural networks with tensor decompositions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning" class="Link-sc-1brdqhf-0 cKRjba">https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jacobgil/pytorch-tensor-decompositions" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jacobgil/pytorch-tensor-decompositions</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Survey on Acceleration of Deep Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.00939" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.00939</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Residual Module for Fast Inference in Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.09723" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.09723</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Co-Design of Deep Neural Nets and Neural Net Accelerators for Embedded Vision Applications</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: UC Berkeley &amp; Samsung Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.10642" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.10642</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Efficient Convolutional Neural Network for Domain-Specific Applications on FPGA</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.03318" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.03318</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating Deep Neural Networks with Spatial Bottleneck Modules</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.02601" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.02601</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FPGA Implementation of Convolutional Neural Networks with Fixed-Point Calculations</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.09945" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.09945</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Extended Bit-Plane Compression for Convolutional Neural Network Accelerators</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.03979" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.03979</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DAC: Data-free Automatic Acceleration of Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2019</li><li>intro: Qualcomm AI Research &amp; Lehigh University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.08374" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.08374</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Instance-wise Sparsity for Accelerating Deep Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.11840" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.11840</a></li></ul><h1 id="code-optimization" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#code-optimization" color="auto.gray.8" aria-label="Code Optimization permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Code Optimization</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Production Deep Learning with NVIDIA GPU Inference Engine</strong></p><img src="https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/06/GIE_GoogLeNet_top10kernels-1.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: convolution, bias, and ReLU layers are fused to form a single layer: CBR</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/" class="Link-sc-1brdqhf-0 cKRjba">https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>speed improvement by merging batch normalization and scale #5</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github issue: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sanghoon/pva-faster-rcnn/issues/5" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sanghoon/pva-faster-rcnn/issues/5</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Add a tool to merge &#x27;Conv-BN-Scale&#x27; into a single &#x27;Conv&#x27; layer.</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Low-memory GEMM-based convolution algorithms for deep neural networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.03395" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.03395</a></p><h1 id="projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerate Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;This tool aims to accelerate the test-time computation and decrease number of parameters of deep CNNs.&quot;</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dmlc/mxnet/tree/master/tools/accnn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dmlc/mxnet/tree/master/tools/accnn</a></li></ul><h2 id="optnet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#optnet" color="auto.gray.8" aria-label="OptNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>OptNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OptNet - reducing memory usage in torch neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/fmassa/optimize-net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/fmassa/optimize-net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NNPACK: Acceleration package for neural networks on multi-core CPUs</strong></p><img src="https://camo.githubusercontent.com/376828536285f7a1a4f054aaae998e805023f489/68747470733a2f2f6d6172617479737a637a612e6769746875622e696f2f4e4e5041434b2f4e4e5041434b2e706e67" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Maratyszcza/NNPACK" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Maratyszcza/NNPACK</a></li><li>comments(Yann LeCun): <a target="_blank" rel="noopener noreferrer" href="https://www.facebook.com/yann.lecun/posts/10153459577707143" class="Link-sc-1brdqhf-0 cKRjba">https://www.facebook.com/yann.lecun/posts/10153459577707143</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Compression on AlexNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/songhan/Deep-Compression-AlexNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/songhan/Deep-Compression-AlexNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tiny Darknet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="http://pjreddie.com/darknet/tiny-darknet/" class="Link-sc-1brdqhf-0 cKRjba">http://pjreddie.com/darknet/tiny-darknet/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CACU: Calculate deep convolution neurAl network on Cell Unit</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/luhaofang/CACU" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/luhaofang/CACU</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>keras_compressor: Model Compression CLI Tool for Keras</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://nico-opendata.jp/ja/casestudy/model_compression/index.html" class="Link-sc-1brdqhf-0 cKRjba">https://nico-opendata.jp/ja/casestudy/model_compression/index.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nico-opendata/keras_compressor" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nico-opendata/keras_compressor</a></li></ul><h1 id="blogs" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#blogs" color="auto.gray.8" aria-label="Blogs permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Blogs</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Networks Are Impressively Good At Compression</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/" class="Link-sc-1brdqhf-0 cKRjba">https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>“Mobile friendly” deep convolutional neural networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>part 1: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-1-331120ad40f9#.uy64ladvz" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-1-331120ad40f9#.uy64ladvz</a></li><li>part 2: <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9#.u58fkuak3" class="Link-sc-1brdqhf-0 cKRjba">https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9#.u58fkuak3</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lab41 Reading Group: Deep Compression</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.hbqzn8wfu" class="Link-sc-1brdqhf-0 cKRjba">https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.hbqzn8wfu</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Accelerating Machine Learning</strong></p><img src="http://www.linleygroup.com/mpr/h/2016/11561/U26_F4v2.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.linleygroup.com/mpr/article.php?id=11561" class="Link-sc-1brdqhf-0 cKRjba">http://www.linleygroup.com/mpr/article.php?id=11561</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Compressing and regularizing deep neural networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks" class="Link-sc-1brdqhf-0 cKRjba">https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>How fast is my model?</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://machinethink.net/blog/how-fast-is-my-model/" class="Link-sc-1brdqhf-0 cKRjba">http://machinethink.net/blog/how-fast-is-my-model/</a></p><h1 id="talks--videos" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#talks--videos" color="auto.gray.8" aria-label="Talks / Videos permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Talks / Videos</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep compression and EIE: Deep learning model compression, design space exploration and hardware acceleration</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=baZOmGSSUAg" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=baZOmGSSUAg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Compression, DSD Training and EIE: Deep Neural Network Model Compression, Regularization and Hardware Acceleration</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://research.microsoft.com/apps/video/default.aspx?id=266664" class="Link-sc-1brdqhf-0 cKRjba">http://research.microsoft.com/apps/video/default.aspx?id=266664</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tailoring Convolutional Neural Networks for Low-Cost, Low-Power Implementation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: tutorial at the May 2015 Embedded Vision Summit</li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=xACJBACStaU" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=xACJBACStaU</a></li></ul><h1 id="resources" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resources" color="auto.gray.8" aria-label="Resources permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Resources</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>awesome-model-compression-and-acceleration</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/sun254/awesome-model-compression-and-acceleration" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sun254/awesome-model-compression-and-acceleration</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Embedded-Neural-Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: collection of works aiming at reducing model sizes or the ASIC/FPGA accelerator for machine learning</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZhishengWang/Embedded-Neural-Network" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZhishengWang/Embedded-Neural-Network</a></li></ul><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/28/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>