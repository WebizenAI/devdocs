<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Papers You Lead, We Exceed: Labor-Free Video Concept Learningby Jointly Exploiting Web Videos and Images intro: CVPR 2016 intro: Lead–Excee…"/><meta data-react-helmet="true" name="twitter:title" content="Video Applications"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-28T19:22:29.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Papers You Lead, We Exceed: Labor-Free Video Concept Learningby Jointly Exploiting Web Videos and Images intro: CVPR 2016 intro: Lead–Excee…"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Video Applications"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Papers You Lead, We Exceed: Labor-Free Video Concept Learningby Jointly Exploiting Web Videos and Images intro: CVPR 2016 intro: Lead–Excee…"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,fdzjHV,HGjBQ,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.cxpRJj{font-size:20px;}/*!sc*/
data-styled.g25[id="heading__StyledH3-sc-1fu06k9-3"]{content:"cxpRJj,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Video Applications - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/HyperMedia Library/">HyperMedia Library</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">awesomeLists</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018 - Web Civics BizPlan/">EXECUTIVE SUMMARY</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-classification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Classification</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#action-detection--activity-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Action Detection / Activity Recognition</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#event-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Event Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#event-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Event Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#abnormality--anomaly-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Abnormality / Anomaly Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-prediction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Prediction</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#prednet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">PredNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-tagging" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Tagging</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#shot-boundary-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Shot Boundary Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-action-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Action Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video2gif" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video2GIF</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video2speech" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video2Speech</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-captioning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Captioning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-summarization" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Summarization</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-highlight-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Highlight Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-understanding" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Understanding</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#challenges" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Challenges</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Video Applications</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-classification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Classification</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#action-detection--activity-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Action Detection / Activity Recognition</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#event-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Event Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#event-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Event Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#abnormality--anomaly-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Abnormality / Anomaly Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-prediction" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Prediction</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#prednet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">PredNet</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-tagging" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Tagging</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#shot-boundary-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Shot Boundary Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-action-segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Action Segmentation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video2gif" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video2GIF</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video2speech" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video2Speech</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-captioning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Captioning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-summarization" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Summarization</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-highlight-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Highlight Detection</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-understanding" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Understanding</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#challenges" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Challenges</a></li></ul></div></div><h1 id="papers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers" color="auto.gray.8" aria-label="Papers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>You Lead, We Exceed: Labor-Free Video Concept Learningby Jointly Exploiting Web Videos and Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>intro: Lead–Exceed Neural Network (LENN), LSTM</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/CVPR16_webly_final.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/CVPR16_webly_final.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Fill in the Blank with Merging LSTMs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: for Large Scale Movie Description and Understanding Challenge (LSMDC) 2016, &quot;Movie fill-in-the-blank&quot; Challenge, UCF_CRCV</li><li>intro: Video-Fill-in-the-Blank (ViFitB)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.04062" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.04062</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Pixel Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google DeepMind</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.00527" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.00527</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Video Synchronization using Unsupervised Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.05985" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.05985</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Propagation Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Max Planck Institute for Intelligent Systems &amp; Bernstein Center for Computational Neuroscience</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://varunjampani.github.io/vpn/" class="Link-sc-1brdqhf-0 cKRjba">https://varunjampani.github.io/vpn/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.05478" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.05478</a></li><li>github(Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/varunjampani/video_prop_networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/varunjampani/video_prop_networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Frame Synthesis using Deep Voxel Flow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://liuziwei7.github.io/projects/VoxelFlow.html" class="Link-sc-1brdqhf-0 cKRjba">https://liuziwei7.github.io/projects/VoxelFlow.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.02463" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.02463</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Optimizing Deep CNN-Based Queries over Video Streams at Scale</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford InfoLab</li><li>keywords: NoScope. difference detectors, specialized models</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.02529" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.02529</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/stanford-futuredata/noscope" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/stanford-futuredata/noscope</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/stanford-futuredata/tensorflow-noscope" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/stanford-futuredata/tensorflow-noscope</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NoScope: 1000x Faster Deep Learning Queries over Video</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://dawn.cs.stanford.edu/2017/06/22/noscope/" class="Link-sc-1brdqhf-0 cKRjba">http://dawn.cs.stanford.edu/2017/06/22/noscope/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Stanford University &amp; University of Southern California</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.02521" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.02521</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ProcNets: Learning to Segment Procedures in Untrimmed and Unconstrained Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.09788" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.09788</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Learning Layers for Video Analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baidu Research</li><li>intro: &quot;The experiments demonstrated the potential applications of UL layers and online learning algorithm to head orientation estimation and moving object localization&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08918" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08918</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Look, Listen and Learn</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind</li><li>intro: &quot;Audio-Visual Correspondence&quot; learning</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08168" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08168</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Imagination from a Single Image with Transformation Generation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Peking University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.04124" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.04124</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gitpub327/VideoImagination" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gitpub327/VideoImagination</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Learn from Noisy Web Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. Stanford University &amp; CMU &amp; Simon Fraser University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.02884" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.02884</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Long Short-Term Memory Networks for Recognizing First Person Interactions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Accepted on the second International Workshop on Egocentric Perception, Interaction and Computing(EPIC) at International Conference on Computer Vision(ICCV-17)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.06495" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.06495</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Binary Residual Representations for Domain-specific Video Streaming</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://research.nvidia.com/publication/2018-02_Learning-Binary-Residual" class="Link-sc-1brdqhf-0 cKRjba">http://research.nvidia.com/publication/2018-02_Learning-Binary-Residual</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.05087" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.05087</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Representation Learning Using Discriminative Pooling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10628" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10628</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking the Faster R-CNN Architecture for Temporal Action Localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.07667" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.07667</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Keyframe Detection in Human Action Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: two-stream ConvNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.10021" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.10021</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FFNet: Video Fast-Forwarding via Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.02792" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.02792</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast forwarding Egocentric Videos by Listening and Watching</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.04620" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.04620</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scanner: Efficient Video Analysis at Scale</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07339" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07339</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Massively Parallel Video Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DeepMind &amp; University of Oxford</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.03863" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.03863</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Object Level Visual Reasoning in Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: LIRIS &amp; Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.06157" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.06157</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Time: Properties, Encoders and Evaluation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.06980" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.06980</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Inserting Videos into Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.06571" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.06571</a></li></ul><h1 id="video-classification" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-classification" color="auto.gray.8" aria-label="Video Classification permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Classification</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-scale Video Classification with Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2014</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://cs.stanford.edu/people/karpathy/deepvideo/" class="Link-sc-1brdqhf-0 cKRjba">http://cs.stanford.edu/people/karpathy/deepvideo/</a></li><li>paper: <a class="Link-sc-1brdqhf-0 cKRjba" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf/">www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploiting Image-trained CNN Architectures for Unconstrained Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Video-level event detection. extracting deep features for each frame, averaging frame-level deep features</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1503.04144" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1503.04144</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Short Snippets: Deep Networks for Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CNN + LSTM</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1503.08909" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1503.08909</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1eQ9zLZk" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1eQ9zLZk</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia, 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1504.01561" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1504.01561</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Content Recognition with Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>author: Zuxuan Wu, Fudan University</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://vision.ouc.edu.cn/valse/slides/20160420/Zuxuan%20Wu%20-%20Video%20Content%20Recognition%20with%20Deep%20Learning-Zuxuan%20Wu.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://vision.ouc.edu.cn/valse/slides/20160420/Zuxuan%20Wu%20-%20Video%20Content%20Recognition%20with%20Deep%20Learning-Zuxuan%20Wu.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Content Recognition with Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>author: Yu-Gang Jiang, Lab for Big Video Data Analytics (BigVid), Fudan University</li><li>slides: <a target="_blank" rel="noopener noreferrer" href="http://www.yugangjiang.info/slides/DeepVideoTalk-2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.yugangjiang.info/slides/DeepVideoTalk-2015.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Large Scale Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.06250" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.06250</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fusing Multi-Stream Deep Networks for Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1509.06086" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1509.06086</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning End-to-end Video Classification with Rank-Pooling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://jmlr.org/proceedings/papers/v48/fernando16.html" class="Link-sc-1brdqhf-0 cKRjba">http://jmlr.org/proceedings/papers/v48/fernando16.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://jmlr.csail.mit.edu/proceedings/papers/v48/fernando16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://jmlr.csail.mit.edu/proceedings/papers/v48/fernando16.pdf</a></li><li>summary(by Hugo Larochelle): <a target="_blank" rel="noopener noreferrer" href="http://www.shortscience.org/paper?bibtexKey=conf/icml/FernandoG16#hlarochelle" class="Link-sc-1brdqhf-0 cKRjba">http://www.shortscience.org/paper?bibtexKey=conf/icml/FernandoG16#hlarochelle</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning for Video Classification and Captioning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.06782" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.06782</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Video Classification via Adaptive Cascading of Deep Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.06453" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.06453</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Feature Flow for Video Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>intro: It provides a simple, fast, accurate, and end-to-end framework for video recognition (e.g., object detection and semantic segmentation in videos)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.07715" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.07715</a></li><li>github(official, MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/msracver/Deep-Feature-Flow" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/msracver/Deep-Feature-Flow</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=J0rMHE6ehGw" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=J0rMHE6ehGw</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-Scale YouTube-8M Video Understanding with Deep Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.04488" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.04488</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Methods for Efficient Large Scale Video Labeling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Solution to the Kaggle&#x27;s competition Google Cloud &amp; YouTube-8M Video Understanding Challenge</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.04572" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.04572</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mpekalski/Y8M" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mpekalski/Y8M</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learnable pooling with Context Gating for video classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR17 Youtube 8M workshop. Kaggle 1st place</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.06905" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.06905</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/antoine77340/LOUPE" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/antoine77340/LOUPE</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Aggregating Frame-level Features for Large-Scale Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Youtube-8M Challenge, 4th place</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.00803" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.00803</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tensor-Train Recurrent Neural Networks for Video Classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.01786" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.01786</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Deep Recurrent Architecture for Video Understanding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Classification Challenge Track paper in CVPR 2017 Workshop on YouTube-8M Large-Scale Video Understanding</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.03296" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.03296</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-scale Video Classification guided by Batch Normalized LSTM Translator</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR2017 Workshop on Youtube-8M Large-scale Video Understanding</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.04045" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.04045</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UTS submission to Google YouTube-8M Challenge 2017</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR&#x27;17 Workshop on YouTube-8M</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.04143" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.04143</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ffmpbgrnn/yt8m" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ffmpbgrnn/yt8m</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A spatiotemporal model with visual attention for video classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.02069" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.02069</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cultivating DNN Diversity for Large Scale Video Labelling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 Youtube-8M Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.04272" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.04272</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention Transfer from Web Images for Video Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia, 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.00973" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.00973</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Non-local Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. CMU &amp; Facebook AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07971" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07971</a></li><li>github(Caffe2): <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/video-nonlocal-net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/video-nonlocal-net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08200" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08200</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Appearance-and-Relation Networks for Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09125" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09125</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wanglimin/ARTNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wanglimin/ARTNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018. Google Research &amp; University of California San Diego</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.04851" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.04851</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Long Activity Video Understanding using Functional Object-Oriented Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.00983" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.00983</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Architectures and Ensembles for Semantic Video Classification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.01026" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.01026</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Discriminative Model for Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08259" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08259</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Video Color Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxuv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.03232" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.03232</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Non-local NetVLAD Encoding for Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018 workshop on YouTube-8M Large-Scale Video Understanding</li><li>intro: Tencent AI Lab &amp; Fudan University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.00207" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.00207</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learnable Pooling Methods for Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Youtube 8M ECCV18 Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.00530" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.00530</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pomonam/LearnablePoolingMethods" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pomonam/LearnablePoolingMethods</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018 workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.05014" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.05014</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/linrongc/youtube-8m" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/linrongc/youtube-8m</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>High Order Neural Networks for Video Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Fudan University, Carnegie Mellon University, Qiniu Inc., ByteDance AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07519" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07519</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TSM: Temporal Shift Module for Efficient Video Understanding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: MIT &amp; MIT-IBM Watson AI Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08383" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08383</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mit-han-lab/temporal-shift-module" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mit-han-lab/temporal-shift-module</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SlowFast Networks for Video Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI Research (FAIR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.03982" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.03982</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Video Classification Using Fewer Frames</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.10640" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.10640</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Classification with Channel-Separated Convolutional Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Facebook AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02811" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02811</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Two-Stream Video Classification with Cross-Modality Attention</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.00497" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.00497</a></p><h2 id="action-detection--activity-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#action-detection--activity-recognition" color="auto.gray.8" aria-label="Action Detection / Activity Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Action Detection / Activity Recognition</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>3d convolutional neural networks for human action recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.odu.edu/~sji/papers/pdf/Ji_ICML10.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.odu.edu/~sji/papers/pdf/Ji_ICML10.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sequential Deep Learning for Human Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://liris.cnrs.fr/Documents/Liris-5228.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://liris.cnrs.fr/Documents/Liris-5228.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Two-stream convolutional networks for action recognition in videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1406.2199" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1406.2199</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Finding action tubes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;built action models from shape and motion cues.
They start from the image proposals and select the motion salient subset of them and
extract saptio-temporal features to represent the video using the CNNs.&quot;</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1411.6031" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1411.6031</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015. TDD</li><li>paper: <a class="Link-sc-1brdqhf-0 cKRjba" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf/">www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf</a></li><li>ext: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_105_ext.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_105_ext.pdf</a></li><li>poster: <a target="_blank" rel="noopener noreferrer" href="https://wanglimin.github.io/papers/WangQT_CVPR15_Poster.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://wanglimin.github.io/papers/WangQT_CVPR15_Poster.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wanglimin/TDD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wanglimin/TDD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Action Recognition by Hierarchical Mid-level Action Elements</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://cvgl.stanford.edu/papers/tian2015.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cvgl.stanford.edu/papers/tian2015.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Contextual Action Recognition with R CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1505.01197" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1505.01197</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gkioxari/RstarCNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gkioxari/RstarCNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Good Practices for Very Deep Two-Stream ConvNets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1507.02159" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1507.02159</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yjxiong/caffe" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yjxiong/caffe</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Action Recognition using Visual Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: LSTM / RNN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.04119" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.04119</a></li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://shikharsharma.com/projects/action-recognition-attention/" class="Link-sc-1brdqhf-0 cKRjba">http://shikharsharma.com/projects/action-recognition-attention/</a></li><li>github(Python/Theano): <a target="_blank" rel="noopener noreferrer" href="https://github.com/kracwarlock/action-recognition-visual-attention" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/kracwarlock/action-recognition-visual-attention</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end Learning of Action Detection from Frame Glimpses in Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://ai.stanford.edu/~syyeung/frameglimpses.html" class="Link-sc-1brdqhf-0 cKRjba">http://ai.stanford.edu/~syyeung/frameglimpses.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06984" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06984</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://vision.stanford.edu/pdf/yeung2016cvpr.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://vision.stanford.edu/pdf/yeung2016cvpr.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-velocity neural networks for gesture recognition in videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1603.06829" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1603.06829</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Active Learning for Online Recognition of Human Activities from Streaming Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.02855" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.02855</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Two-Stream Network Fusion for Video Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.06573" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.06573</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/feichtenhofer/twostreamfusion" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/feichtenhofer/twostreamfusion</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.08880" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.08880</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Semantic Action Discovery from Video Collections</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.03324" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.03324</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anticipating Visual Representations from Unlabeled Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://web.mit.edu/vondrick/prediction.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.mit.edu/vondrick/prediction.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VideoLSTM Convolves, Attends and Flows for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.01794" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.01794</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Attention Network for Action Recognition in Videos (HAN)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.06416" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.06416</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.07043" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.07043</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Connectionist Temporal Modeling for Weakly Supervised Action Labeling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.08584" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.08584</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CUHK &amp; ETHZ &amp; SIAT Submission to ActivityNet Challenge 2016</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: won the 1st place in the untrimmed video classification task of ActivityNet Challenge 2016. TSN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.00797" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.00797</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yjxiong/anet2016-cuhk" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yjxiong/anet2016-cuhk</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Actionness Estimation Using Hybrid FCNs</strong></p><img src="http://wanglimin.github.io/actionness_hfcn/actionness.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016. H-FCN</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://wanglimin.github.io/actionness_hfcn/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://wanglimin.github.io/actionness_hfcn/index.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wanglimin/actionness-estimation/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wanglimin/actionness-estimation/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-time Action Recognition with Enhanced Motion Vector CNNs</strong></p><img src="http://zbwglory.github.io/MV-CNN/framework.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://zbwglory.github.io/MV-CNN/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://zbwglory.github.io/MV-CNN/index.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://wanglimin.github.io/papers/ZhangWWQW_CVPR16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://wanglimin.github.io/papers/ZhangWWQW_CVPR16.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zbwglory/MV-release" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zbwglory/MV-release</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016. HMDB51: 69.4%, UCF101: 94.2%</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.00859" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.00859</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yjxiong/temporal-segment-networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yjxiong/temporal-segment-networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Segment Networks for Action Recognition in Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: An extension of submission <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.00859" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.00859</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.02953" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.02953</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Attention Network for Action Recognition in Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.06416" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.06416</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepCAMP: Deep Convolutional Action &amp; Attribute Mid-Level Patterns</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.03217" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.03217</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.04339" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.04339</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Image Networks for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://users.cecs.anu.edu.au/~sgould/papers/cvpr16-dynamic_images.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://users.cecs.anu.edu.au/~sgould/papers/cvpr16-dynamic_images.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hbilen/dynamic-image-nets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hbilen/dynamic-image-nets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Human Action Recognition without Human</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.07876" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.07876</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Convolutional Networks: A Unified Approach to Action Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.08242" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.08242</a></li><li>ECCV 2016 workshop: <a target="_blank" rel="noopener noreferrer" href="http://bravenewmotion.github.io/" class="Link-sc-1brdqhf-0 cKRjba">http://bravenewmotion.github.io/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Bachelor Thesis Report at ETSETB TelecomBCN</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://imatge-upc.github.io/activitynet-2016-cvprw/" class="Link-sc-1brdqhf-0 cKRjba">https://imatge-upc.github.io/activitynet-2016-cvprw/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1608.08128" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1608.08128</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/imatge-upc/activitynet-2016-cvprw" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/imatge-upc/activitynet-2016-cvprw</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sequential Deep Trajectory Descriptor for Action Recognition with Three-stream CNN</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.03056" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.03056</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at Extremely Low Resolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.03898" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.03898</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatiotemporal Residual Networks for Video Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.02155" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.02155</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Action Recognition Based on Joint Trajectory Maps Using Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.02447" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.02447</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Recurrent Neural Network for Mobile Human Activity Recognition with High Throughput</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.03607" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.03607</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Network based Attention for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05215" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05215</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Convolutional Networks for Action Segmentation and Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05267" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05267</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08240" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08240</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ActionFlowNet: Learning Motion Representation for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03052" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03052</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Higher-order Pooling of CNN Features via Kernel Linearization for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Australian Center for Robotic Vision &amp; Data61/CSIRO</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.05432" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.05432</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.10664" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.10664</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Action Detection with Structured Segment Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://yjxiong.me/others/ssn/" class="Link-sc-1brdqhf-0 cKRjba">http://yjxiong.me/others/ssn/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.06228" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.06228</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yjxiong/action-detection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yjxiong/action-detection</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Residual Learning for Action Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1706.08807" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1706.08807</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Multi-scale Attention Networks for Action Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.07590" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.07590</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Two-stream Flow-guided Convolutional Attention Networks for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: International Conference of Computer Vision Workshop (ICCVW), 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.09268" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.09268</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Action Classification and Highlighting in Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.09522" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.09522</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-Time Action Detection in Video Surveillance using Sub-Action Descriptor with Multi-CNN</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.03383" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.03383</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end Video-level Representation Learning for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Deep networks with Temporal Pyramid Pooling (DTPP)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.04161" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.04161</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low Resolution Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.03983" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.03983</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DiscrimNet: Semi-Supervised Action Recognition from Videos using Generative Adversarial Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.07230" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.07230</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Fusion of Appearance based CNNs and Temporal evolution of Skeleton with LSTM for Daily Living Action Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.00421" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.00421</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-Time End-to-End Action Detection with Two-Stream Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.08362" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.08362</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Closer Look at Spatiotemporal Convolutions for Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. Facebook Research</li><li>intro: R(2+1)D and Mixed-Convolutions for Action Recognition.</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://dutran.github.io/R2Plus1D/" class="Link-sc-1brdqhf-0 cKRjba">https://dutran.github.io/R2Plus1D/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.11248" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.11248</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/R2Plus1D" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/facebookresearch/R2Plus1D</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VideoCapsuleNet: A Simplified Network for Action Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.08162" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.08162</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Where and When to Look? Spatio-temporal Attention for Action Recognition in Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.04511" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.04511</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Relational Long Short-Term Memory for Video Action Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07059" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07059</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Recurrent Networks for Online Action Detection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07391" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.073910</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Action Transformer Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Carnegie Mellon University &amp; DeepMind &amp; University of Oxford</li><li>intro: Ranked first on the AVA (computer vision only) leaderboard of the ActivityNet Challenge 2018</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://rohitgirdhar.github.io/ActionTransformer/" class="Link-sc-1brdqhf-0 cKRjba">https://rohitgirdhar.github.io/ActionTransformer/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.02707" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.02707</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>D3D: Distilled 3D Networks for Video Action Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google &amp; University of Michigan &amp; Princeton University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.08249" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.08249</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.13417" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.13417</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deformable Tube Network for Action Detection in Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.01847" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.01847</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.06644" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.06644</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TubeR: Tube-Transformer for Action Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Amsterdam &amp; Amazon Web Service</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.00969" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.00969</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting Skeleton-based Action Recognition</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.13586" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.13586</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-end Temporal Action Detection with Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.10271" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.10271</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xlliu7/TadTR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xlliu7/TadTR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OadTR: Online Action Detection with Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.11149" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.11149</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wangxiang1230/OadTR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wangxiang1230/OadTR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VideoLightFormer: Lightweight Action Recognition using Transformers</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.00451" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.00451</a></p><h3 id="projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH3-sc-1fu06k9-3 ffNRvO gCPbFb cxpRJj Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h3><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Torch Library for Action Recognition and Detection Using CNNs and LSTMs</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CS231n student project report</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://cs231n.stanford.edu/reports2016/221_Report.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cs231n.stanford.edu/reports2016/221_Report.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/garythung/torch-lrcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/garythung/torch-lrcn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>2016 ActivityNet action recognition challenge. CNN + LSTM approach. Multi-threaded loading.</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jrbtaylor/ActivityNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jrbtaylor/ActivityNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LSTM for Human Activity Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/</a></li><li>github(MXNet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scanner: Efficient Video Analysis at Scale</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Locate and recognize faces in a video, Detect shots in a film, Search videos by image</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/scanner-research/scanner" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/scanner-research/scanner</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Charades Starter Code for Activity Classification and Localization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Activity Recognition Algorithms for the Charades Dataset</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gsig/charades-algorithms" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gsig/charades-algorithms</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NonLocalNetwork and Sequeeze-Excitation Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: MXNet implementation of Non-Local and Squeeze-Excitation network</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/WillSuen/NonLocalandSEnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/WillSuen/NonLocalandSEnet</a></li></ul><h1 id="event-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#event-recognition" color="auto.gray.8" aria-label="Event Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Event Recognition</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TagBook: A Semantic Video Representation without Supervision for Event Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.02899" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.02899</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AENet: Learning Deep Audio Features for Video Analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.00599" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.00599</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/znaoya/aenet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/znaoya/aenet</a></li></ul><h1 id="event-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#event-detection" color="auto.gray.8" aria-label="Event Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Event Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DevNet: A Deep Event Network for Multimedia Event Detection and Evidence Recounting</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://120.52.72.47/winsty.net/c3pr90ntcsf0/papers/devnet.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://120.52.72.47/winsty.net/c3pr90ntcsf0/papers/devnet.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Detecting events and key actors in multi-person videos</strong></p><img src="https://tctechcrunch2011.files.wordpress.com/2016/06/basketball_actors.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.02917" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.02917</a></li><li>paper: <a class="Link-sc-1brdqhf-0 cKRjba" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf/">www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://vision.stanford.edu/pdf/johnson2016cvpr.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://vision.stanford.edu/pdf/johnson2016cvpr.pdf</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://www.leiphone.com/news/201606/l1TKIRFLO3DUFNNu.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.leiphone.com/news/201606/l1TKIRFLO3DUFNNu.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: INTERSPEECH 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.07160" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.07160</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Action Detection in Untrimmed Videos via Multi-Task Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.07403" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.07403</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Event Detection and Description in Continuous Video Streams</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Joint Event Detection and Description Network (JEDDi-Net)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.10250" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.10250</a></li></ul><h1 id="abnormality--anomaly-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#abnormality--anomaly-detection" color="auto.gray.8" aria-label="Abnormality / Anomaly Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Abnormality / Anomaly Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Convolutional Neural Network for Fast Anomaly Detection in Crowded Scenes</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.00866" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.00866</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anomaly Detection in Video Using Predictive Convolutional Long Short-Term Memory Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Rochester Institute of Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.00390" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.00390</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Abnormal Event Detection in Videos using Spatiotemporal Autoencoder</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.01546" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.01546</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yshean/abnormal-spatiotemporal-ae" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yshean/abnormal-spatiotemporal-ae</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Abnormal Event Detection in Videos using Generative Adversarial Nets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Best Paper / Student Paper Award Finalist, IEEE International Conference on Image Processing (ICIP), 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.09644" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.09644</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.09121" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.09121</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Uncanny Vision Solutions</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.03149" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.03149</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>STAN: Spatio-Temporal Adversarial Networks for Abnormal Event Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICASSP 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.08381" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.08381</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.11223" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.11223</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attentioned Convolutional LSTM InpaintingNetwork for Anomaly Detection in Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.10228" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.10228</a></p><h1 id="video-prediction" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-prediction" color="auto.gray.8" aria-label="Video Prediction permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Prediction</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep multi-scale video prediction beyond mean square error</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICLR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.05440" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.05440</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/coupriec/VideoPredictionICLR2016" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/coupriec/VideoPredictionICLR2016</a></li><li>github(TensorFlow): <a target="_blank" rel="noopener noreferrer" href="https://github.com/dyelax/Adversarial_Video_Generation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dyelax/Adversarial_Video_Generation</a></li><li>demo: <a target="_blank" rel="noopener noreferrer" href="http://cs.nyu.edu/~mathieu/iclr2016.html" class="Link-sc-1brdqhf-0 cKRjba">http://cs.nyu.edu/~mathieu/iclr2016.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Learning for Physical Interaction through Video Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1605.07157" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1605.07157</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tensorflow/models/tree/master/video_prediction" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tensorflow/models/tree/master/video_prediction</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generating Videos with Scene Dynamics</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>intro: The model learns to generate tiny videos using adversarial networks</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://web.mit.edu/vondrick/tinyvideo/" class="Link-sc-1brdqhf-0 cKRjba">http://web.mit.edu/vondrick/tinyvideo/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://web.mit.edu/vondrick/tinyvideo/paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://web.mit.edu/vondrick/tinyvideo/paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cvondrick/videogan" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cvondrick/videogan</a></li></ul><h2 id="prednet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#prednet" color="auto.gray.8" aria-label="PredNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PredNet</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://coxlab.github.io/prednet/" class="Link-sc-1brdqhf-0 cKRjba">https://coxlab.github.io/prednet/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.08104" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.08104</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/coxlab/prednet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/coxlab/prednet</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/e-lab/torch-prednet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/e-lab/torch-prednet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diversity encouraged learning of unsupervised LSTM ensemble for neural activity video prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.04899" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.04899</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Ladder Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>inro: NIPS 2016 workshop on ML for Spatiotemporal Forecasting</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.01756" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.01756</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Learning of Long-Term Motion Dynamics for Videos</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.01821" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.01821</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NCCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.04125" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.04125</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fully Context-Aware Video Prediction</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ETH Zurich &amp; NNAISENSE</li><li>keywords: unsupervised learning through video prediction, Parallel Multi-Dimensional LSTM</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/view/contextvp" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/view/contextvp</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.08518" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.08518</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Novel Video Prediction for Large-scale Scene using Optical Flow</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Victoria &amp; Tongji University &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.12243" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.12243</a></li></ul><h1 id="video-tagging" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-tagging" color="auto.gray.8" aria-label="Video Tagging permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Tagging</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Automatic Image and Video Tagging</strong></p><img src="http://i2.wp.com/scottge.net/wp-content/uploads/2015/06/imagetagging.png?w=576" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://scottge.net/2015/06/30/automatic-image-and-video-tagging/" class="Link-sc-1brdqhf-0 cKRjba">http://scottge.net/2015/06/30/automatic-image-and-video-tagging/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tagging YouTube music videos with deep learning - Alexandre Passant</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Clarifai&#x27;s deep learning API</li><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://apassant.net/2015/07/03/tagging-youtube-music-clarifai-deep-learning/" class="Link-sc-1brdqhf-0 cKRjba">http://apassant.net/2015/07/03/tagging-youtube-music-clarifai-deep-learning/</a></li></ul><h1 id="shot-boundary-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#shot-boundary-detection" color="auto.gray.8" aria-label="Shot Boundary Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Shot Boundary Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-scale, Fast and Accurate Shot Boundary Detection through Spatio-temporal Convolutional Neural Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.03281" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.03281</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: obtains state-of-the-art results while running at an unprecedented speed of more than 120x real-time.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08214" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08214</a></li></ul><h1 id="video-action-segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-action-segmentation" color="auto.gray.8" aria-label="Video Action Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Action Segmentation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Rochester</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.07818" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.07818</a></li></ul><h1 id="video2gif" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video2gif" color="auto.gray.8" aria-label="Video2GIF permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video2GIF</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video2GIF: Automatic Generation of Animated GIFs from Video (Robust Deep RankNet)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 3D CNN, ranking model, Huber loss, 100K GIFs/video sources dataset</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.04850" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.04850</a></li><li>github(dataset): <a target="_blank" rel="noopener noreferrer" href="https://github.com/gyglim/video2gif_dataset" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gyglim/video2gif_dataset</a></li><li>results: <a target="_blank" rel="noopener noreferrer" href="http://video2gif.info/" class="Link-sc-1brdqhf-0 cKRjba">http://video2gif.info/</a></li><li>demo site: <a target="_blank" rel="noopener noreferrer" href="http://people.ee.ethz.ch/~gyglim/work_public/autogif/" class="Link-sc-1brdqhf-0 cKRjba">http://people.ee.ethz.ch/~gyglim/work_public/autogif/</a></li><li>review: <a target="_blank" rel="noopener noreferrer" href="http://motherboard.vice.com/read/these-fire-gifs-were-made-by-artificial-intelligence-yahoo" class="Link-sc-1brdqhf-0 cKRjba">http://motherboard.vice.com/read/these-fire-gifs-were-made-by-artificial-intelligence-yahoo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Creating Animated GIFs Automatically from Video</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://yahooresearch.tumblr.com/post/148009705216/creating-animated-gifs-automatically-from-video" class="Link-sc-1brdqhf-0 cKRjba">https://yahooresearch.tumblr.com/post/148009705216/creating-animated-gifs-automatically-from-video</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GIF2Video: Color Dequantization and Temporal Interpolation of GIF images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stony Brook University &amp; Megvii Research USA &amp; UCLA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.02840" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.02840</a></li></ul><h1 id="video2speech" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video2speech" color="auto.gray.8" aria-label="Video2Speech permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video2Speech</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vid2speech: Speech Reconstruction from Silent Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICASSP 2017</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.vision.huji.ac.il/vid2speech/" class="Link-sc-1brdqhf-0 cKRjba">http://www.vision.huji.ac.il/vid2speech/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.00495" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.00495</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/arielephrat/vid2speech" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/arielephrat/vid2speech</a></li></ul><h1 id="video-captioning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-captioning" color="auto.gray.8" aria-label="Video Captioning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Captioning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://handong1587.github.io/deep_learning/2015/10/09/image-video-captioning.html#video-captioning" class="Link-sc-1brdqhf-0 cKRjba">http://handong1587.github.io/deep_learning/2015/10/09/image-video-captioning.html#video-captioning</a></p><h1 id="video-summarization" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-summarization" color="auto.gray.8" aria-label="Video Summarization permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Summarization</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Video summarization produces a short summary of a full-length video and ideally encapsulates its most informative parts,
alleviates the problem of video browsing, editing and indexing.</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Summarization with Long Short-term Memory</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1605.08110" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1605.08110</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepVideo: Video Summarization using Temporal Sequence Modelling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CS231n student project report</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://cs231n.stanford.edu/reports2016/216_Report.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cs231n.stanford.edu/reports2016/216_Report.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Video Trailers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.01819" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.01819</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Summarization using Deep Semantic Features</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>inro: ACCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.08758" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.08758</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CNN-Based Prediction of Frame-Level Shot Importance for Video Summarization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: International Conference on new Trends in Computer Sciences (ICTCS), Amman-Jordan, 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.07023" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.07023</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Summarization with Attention-Based Encoder-Decoder Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.09545" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.09545</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018. Chinese Academy of Sciences &amp; Queen Mary University of London</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://kaiyangzhou.github.io/project_vsumm_reinforce/index.html" class="Link-sc-1brdqhf-0 cKRjba">https://kaiyangzhou.github.io/project_vsumm_reinforce/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00054" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00054</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//KaiyangZhou/vsumm-reinforce" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//KaiyangZhou/vsumm-reinforce</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Viewpoint-aware Video Summarization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.02843" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.02843</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DTR-GAN: Dilated Temporal Relational Adversarial Network for Video Summarization</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.11228" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.11228</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Video Summarization Using Unpaired Data</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.12174" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.12174</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Summarization Using Fully Convolutional Sequence Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.10538" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.10538</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Summarisation by Classification with Deep Reinforcement Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.03089" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.03089</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Query-Conditioned Three-Player Adversarial Network for Video Summarization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.06677" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.06677</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Discriminative Feature Learning for Unsupervised Video Summarization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.09791" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.09791</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking the Evaluation of Video Summaries</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 poster</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.11328" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.11328</a></li></ul><h1 id="video-highlight-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-highlight-detection" color="auto.gray.8" aria-label="Video Highlight Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Highlight Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>intro: rely on an assumption that highlights of an event category are more frequently captured in short videos than non-highlights</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1510.01442" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1510.01442</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: wearable device</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://research.microsoft.com/apps/pubs/default.aspx?id=264919" class="Link-sc-1brdqhf-0 cKRjba">http://research.microsoft.com/apps/pubs/default.aspx?id=264919</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Using Deep Learning to Find Basketball Highlights</strong></p><img src="https://cloud.githubusercontent.com/assets/10147637/7966603/228179fe-09f3-11e5-9ea7-31e76c8248fe.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://public.hudl.com/bits/archives/2015/06/05/highlights/?utm_source=tuicool&amp;utm_medium=referral" class="Link-sc-1brdqhf-0 cKRjba">http://public.hudl.com/bits/archives/2015/06/05/highlights/?utm_source=tuicool&amp;utm_medium=referral</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-Time Video Highlights for Yahoo Esports</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.08780" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.08780</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Deep Ranking Model for Spatio-Temporal Highlight Detection from a 360 Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.10312" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.10312</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Nanyang Technological University &amp; Google Research, Zurich</li><li>keywords: personalized highlight detection (PHD)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.06604" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.06604</a></li></ul><h1 id="video-understanding" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-understanding" color="auto.gray.8" aria-label="Video Understanding permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Understanding</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scale Up Video Understandingwith Deep Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 2016, Tsinghua University</li><li>slides: <a class="Link-sc-1brdqhf-0 cKRjba" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/iiis.tsinghua.edu.cn/~jianli/courses/ATCS2016spring/talk_chuang.pptx/">iiis.tsinghua.edu.cn/~jianli/courses/ATCS2016spring/talk_chuang.pptx</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Slicing Convolutional Neural Network for Crowd Video Understanding</strong></p><img src="http://www.ee.cuhk.edu.hk/~jshao/SCNN_files/fig_network5.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>intro: It aims at learning generic spatio-temporal features from crowd videos, especially for long-term temporal learning</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~jshao/SCNN.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~jshao/SCNN.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~jshao/papers_jshao/jshao_cvpr16_scnn.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~jshao/papers_jshao/jshao_cvpr16_scnn.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/amandajshao/Slicing-CNN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/amandajshao/Slicing-CNN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Spatiotemporal Feature Learning For Video Understanding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.04851" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.04851</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Video Understanding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.03316" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.03316</a></p><h1 id="challenges" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#challenges" color="auto.gray.8" aria-label="Challenges permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Challenges</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>THUMOS Challenge 2014</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://crcv.ucf.edu/THUMOS14/home.html" class="Link-sc-1brdqhf-0 cKRjba">http://crcv.ucf.edu/THUMOS14/home.html</a></li><li>download: <a target="_blank" rel="noopener noreferrer" href="http://crcv.ucf.edu/THUMOS14/download.html" class="Link-sc-1brdqhf-0 cKRjba">http://crcv.ucf.edu/THUMOS14/download.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>THUMOS Challenge 2015</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.thumos.info/" class="Link-sc-1brdqhf-0 cKRjba">http://www.thumos.info/</a></li><li>download: <a target="_blank" rel="noopener noreferrer" href="http://www.thumos.info/download.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.thumos.info/download.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ActivityNet Challenge 2016</strong></p><img src="http://activity-net.org/challenges/2016/images/anet_cover.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://activity-net.org/challenges/2016/" class="Link-sc-1brdqhf-0 cKRjba">http://activity-net.org/challenges/2016/</a></li></ul><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/28/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>