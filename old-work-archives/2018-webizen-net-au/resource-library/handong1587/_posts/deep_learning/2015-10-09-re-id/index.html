<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Leaderboard ReID Method backbone test size Market1501 CUHK03 (detected) CUHK03 (detected/new) CUHK03 (labeled/new) CUHK-SYSU DukeMTMC-reID …"/><meta data-react-helmet="true" name="twitter:title" content="Re-ID"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-28T19:22:29.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-10-09T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Leaderboard ReID Method backbone test size Market1501 CUHK03 (detected) CUHK03 (detected/new) CUHK03 (labeled/new) CUHK-SYSU DukeMTMC-reID …"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Re-ID"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Leaderboard ReID Method backbone test size Market1501 CUHK03 (detected) CUHK03 (detected/new) CUHK03 (labeled/new) CUHK-SYSU DukeMTMC-reID …"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,HGjBQ,fdzjHV,bQLMRL,ekSqTm,cKRjba,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.eooEiq{display:block;width:100%;margin:0 0 16px;overflow:auto;}/*!sc*/
.eooEiq th{font-weight:600;}/*!sc*/
.eooEiq th,.eooEiq td{padding:8px 16px;border:1px solid #e1e4e8;}/*!sc*/
.eooEiq tr{background-color:#ffffff;border-top:1px solid #e1e4e8;}/*!sc*/
.eooEiq tr:nth-child(2n){background-color:#f6f8fa;}/*!sc*/
.eooEiq img{background-color:transparent;}/*!sc*/
data-styled.g35[id="table__Table-sc-ixm5yk-0"]{content:"eooEiq,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Re-ID - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">awesomeLists</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/">3D</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/">Acceleration and Model Compression</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/">Adversarial Attacks and Defences</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/">Audio / Image / Video Generation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/">BEV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/">Classification / Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/">Deep Learning and Autonomous Driving</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/">Deep Learning Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/">Deep learning Courses</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/">Deep Learning Frameworks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/">Deep Learning Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/">Deep Learning Software and Hardware</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/">Deep Learning Tricks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/">Deep Learning Tutorials</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/">Deep Learning with Machine Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/">Face Recognition</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/">Fun With Deep Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/">Generative Adversarial Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/">Graph Convolutional Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/">Image / Video Captioning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/">Image Retrieval</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/">Keep Up With New Trends</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/">LiDAR 3D Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/">Natural Language Processing</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/">Neural Architecture Search</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/">Object Counting</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/">Object Detection</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/">OCR</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/">Optical Flow</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/">Re-ID</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/">Recommendation System</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/">Reinforcement Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/">RNN and LSTM</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/">Segmentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/">Style Transfer</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/">Super-Resolution</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/">Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/">Training Deep Neural Networks</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/">Transfer Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/">Unsupervised Learning</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/">Video Applications</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/">Visual Question Answering</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/">Visualizing and Interpreting Convolutional Neural Network</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#leaderboard" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Leaderboard</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#reid" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ReID</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#personsearch" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">PersonSearch</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#person-re-identification--person-retrieval" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Person Re-identification / Person Retrieval</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#person-search" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Person Search</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pose--viewpoint-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pose / Viewpoint for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#gan-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">GAN for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#human-parsing-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Human Parsing for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#occluded-person-re-id--partial-person-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Occluded Person Re-ID / Partial Person Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#cross-modality-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Cross-modality Re-ID</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#rgb-ir-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">RGB-IR Re-ID</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#depth-based-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Depth-Based Re-ID</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#low-resolution-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Low Resolution Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#reinforcement-learning-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Reinforcement Learning for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#attributes-prediction-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Attributes Prediction for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-person-re-identification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Person Re-Identification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#re-ranking" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Re-ranking</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#unsupervised-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Unsupervised Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#weakly-supervised-person-re-identification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Weakly Supervised Person Re-identification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#vehicle-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-metric-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Metric Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#evaluation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Evaluation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#talks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Talks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Re-ID</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#leaderboard" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Leaderboard</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#reid" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ReID</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#personsearch" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">PersonSearch</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#papers" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Papers</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#person-re-identification--person-retrieval" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Person Re-identification / Person Retrieval</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#person-search" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Person Search</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pose--viewpoint-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pose / Viewpoint for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#gan-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">GAN for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#human-parsing-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Human Parsing for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#occluded-person-re-id--partial-person-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Occluded Person Re-ID / Partial Person Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#cross-modality-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Cross-modality Re-ID</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#rgb-ir-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">RGB-IR Re-ID</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#depth-based-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Depth-Based Re-ID</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#low-resolution-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Low Resolution Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#reinforcement-learning-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Reinforcement Learning for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#attributes-prediction-for-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Attributes Prediction for Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video-person-re-identification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video Person Re-Identification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#re-ranking" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Re-ranking</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#unsupervised-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Unsupervised Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#weakly-supervised-person-re-identification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Weakly Supervised Person Re-identification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#vehicle-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#deep-metric-learning" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Deep Metric Learning</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#projects" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Projects</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#evaluation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Evaluation</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#talks" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Talks</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li></ul></div></div><h1 id="leaderboard" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#leaderboard" color="auto.gray.8" aria-label="Leaderboard permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Leaderboard</h1><h2 id="reid" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#reid" color="auto.gray.8" aria-label="ReID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ReID</h2><table class="table__Table-sc-ixm5yk-0 eooEiq"><thead><tr><th align="center">Method</th><th align="center">backbone</th><th align="center">test size</th><th align="center">Market1501</th><th align="center">CUHK03 (detected)</th><th align="center">CUHK03 (detected/new)</th><th align="center">CUHK03 (labeled/new)</th><th align="center">CUHK-SYSU</th><th align="center">DukeMTMC-reID</th><th align="center">MARS</th></tr></thead><tbody><tr><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">rank1 / mAP</td><td align="center">rank1/rank5/rank10</td><td align="center">rank1 / mAP</td><td align="center">rank1 / mAP</td><td align="center">rank1 / mAP</td><td align="center"></td><td align="center">rank1 / mAP</td></tr><tr><td align="center"><a target="_blank" rel="noopener noreferrer" href="https://github.com/NVlabs/DG-Net" class="Link-sc-1brdqhf-0 cKRjba">DG-Net</a></td><td align="center">ResNet-50</td><td align="center">256×128</td><td align="center">94.8/ 86.0</td><td align="center"></td><td align="center">65.6/61.1</td><td align="center"></td><td align="center"></td><td align="center">86.6/74.8</td><td align="center"></td></tr><tr><td align="center">AlignedReID</td><td align="center">ResNet50-X</td><td align="center"></td><td align="center">92.6 / 82.3</td><td align="center">91.9 / 98.7 / 99.4</td><td align="center"></td><td align="center">86.8 / 79.1</td><td align="center"></td><td align="center"></td><td align="center">95.3 / 93.7</td></tr><tr><td align="center">Deep-Person</td><td align="center">ResNet-50</td><td align="center">256×128</td><td align="center">92.31 / 79.58</td><td align="center">89.4 / 98.2 / 99.1</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">80.90 / 64.80</td></tr><tr><td align="center">PCB</td><td align="center">ResNet-50</td><td align="center">384x128</td><td align="center">92.4 / 77.3</td><td align="center"></td><td align="center">61.3 / 54.2</td><td align="center"></td><td align="center"></td><td align="center">81.9 / 65.3</td><td align="center"></td></tr><tr><td align="center">PCB+RPP</td><td align="center">ResNet-50</td><td align="center">384x128</td><td align="center">93.8 / 81.6</td><td align="center"></td><td align="center">63.7 / 57.5</td><td align="center"></td><td align="center"></td><td align="center">83.3 / 69.2</td><td align="center"></td></tr><tr><td align="center">PN-GAN</td><td align="center">ResNet-50</td><td align="center"></td><td align="center">89.43 / 72.58</td><td align="center">79.76 / 96.24 / 98.56</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">73.58 / 53.20</td><td align="center"></td></tr><tr><td align="center">MGN</td><td align="center">ResNet-50</td><td align="center"></td><td align="center">95.7 /  86.9</td><td align="center"></td><td align="center">66.8 / 66.0</td><td align="center">68.0 / 67.4</td><td align="center"></td><td align="center">88.7 / 78.4</td><td align="center"></td></tr><tr><td align="center">HPM</td><td align="center">ResNet-50</td><td align="center">384x128</td><td align="center">94.2 /  82.7</td><td align="center"></td><td align="center">63.1 / 57.5</td><td align="center"></td><td align="center"></td><td align="center">86.6 / 74.3</td><td align="center"></td></tr><tr><td align="center">HPM+HRE</td><td align="center">ResNet-50</td><td align="center">384x128</td><td align="center">93.9 /  83.1</td><td align="center"></td><td align="center">63.2 / 59.7</td><td align="center"></td><td align="center"></td><td align="center">86.3 / 74.5</td><td align="center"></td></tr><tr><td align="center">SphereReID</td><td align="center">ResNet-50</td><td align="center">288×144</td><td align="center">94.4 /  83.6</td><td align="center">93.1 / 98.7 / 99.4</td><td align="center">63.2 / 59.7</td><td align="center"></td><td align="center">95.4 / 93.9</td><td align="center">83.9 / 68.5</td><td align="center"></td></tr><tr><td align="center">Auto-ReID</td><td align="center"></td><td align="center">384x128</td><td align="center">94.5 /  85.1</td><td align="center"></td><td align="center">73.3 / 69.3</td><td align="center">77.9 / 73.0</td><td align="center"></td><td align="center">88.5 / 75.1</td><td align="center"></td></tr></tbody></table><h2 id="personsearch" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#personsearch" color="auto.gray.8" aria-label="PersonSearch permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PersonSearch</h2><table class="table__Table-sc-ixm5yk-0 eooEiq"><thead><tr><th align="center">Method</th><th align="center">backbone</th><th align="center">CUHK-SYSU</th><th align="center">PRW</th><th align="center">PRW-mini</th></tr></thead><tbody><tr><td align="center"></td><td align="center"></td><td align="center">top1 / mAP</td><td align="center">top1 / mAP</td><td align="center">top1 / mAP</td></tr><tr><td align="center"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.01850" class="Link-sc-1brdqhf-0 cKRjba">Joint Detection and Identification Feature Learning for Person Search</a></td><td align="center">ResNet-50</td><td align="center">78.7 / 75.5</td><td align="center">-- / --</td><td align="center">-- / --</td></tr><tr><td align="center"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01830" class="Link-sc-1brdqhf-0 cKRjba">Learning Context Graph for Person Search</a></td><td align="center">ResNet-50</td><td align="center">86.5 / 84.1</td><td align="center">73.6 / 33.4</td><td align="center">-- / --</td></tr><tr><td align="center"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.01058" class="Link-sc-1brdqhf-0 cKRjba">Knowledge Distillation for End-to-End Person Search</a></td><td align="center">ResNet-50</td><td align="center">88.5 / 87.2</td><td align="center">-- / --</td><td align="center">70.0 / 33.1</td></tr><tr><td align="center"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.01203" class="Link-sc-1brdqhf-0 cKRjba">Query-guided End-to-End Person Search</a></td><td align="center">ResNet-50</td><td align="center">89.1 / 88.9</td><td align="center">76.7 / 37.1</td><td align="center">80.0 / 39.1</td></tr><tr><td align="center"><a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/8784982/" class="Link-sc-1brdqhf-0 cKRjba">Fast Person Search Pipeline</a></td><td align="center">ResNet-50</td><td align="center">89.87 / 86.99</td><td align="center">70.58/ 44.45</td><td align="center">-- / --</td></tr><tr><td align="center">End-to-End Thorough Body Perception for Person Search</td><td align="center">ResNet-50</td><td align="center">90.5 / 88.4</td><td align="center">68.9 / 42.9</td><td align="center">-- / --</td></tr><tr><td align="center"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.08580" class="Link-sc-1brdqhf-0 cKRjba">Re-ID Driven Localization Refinement for Person Search</a></td><td align="center">ResNet-50</td><td align="center">94.2 / 93.0</td><td align="center">70.2 / 42.9</td><td align="center">-- / --</td></tr></tbody></table><h1 id="papers" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#papers" color="auto.gray.8" aria-label="Papers permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Papers</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Uncertainty-Aware Multi-Shot Knowledge Distillation for Image-Based Object Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2001.05197" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2001.05197</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Re-Identification by Multiple Views Knowledge Distillation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: University of Modena and Reggio Emilia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.04174" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.04174</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/aimagelab/VKD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/aimagelab/VKD</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2020</li><li>intro: CUHK</li><li>project apge: <a target="_blank" rel="noopener noreferrer" href="https://geyixiao.com/projects/spcl.html" class="Link-sc-1brdqhf-0 cKRjba">https://geyixiao.com/projects/spcl.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.02713" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.02713</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yxgeee/SpCL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yxgeee/SpCL</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/open-mmlab/OpenUnReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/open-mmlab/OpenUnReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context-Aware Graph Convolution Network for Target Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>intro: SenseTime Research &amp; Xidian University &amp; Peking University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.04298" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.04298</a></li></ul><h1 id="person-re-identification--person-retrieval" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#person-re-identification--person-retrieval" color="auto.gray.8" aria-label="Person Re-identification / Person Retrieval permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Person Re-identification / Person Retrieval</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2014</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Li_DeepReID_Deep_Filter_2014_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Li_DeepReID_Deep_Filter_2014_CVPR_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An Improved Deep Learning Architecture for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2015</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Ranking for Person Re-identification via Joint Representation Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Image Processing (TIP), 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1505.06821" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1505.06821</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PersonNet: Person Re-identification with Deep Convolutional Neural Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.07255" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.07255</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.07528" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.07528</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Cysu/dgd_person_reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Cysu/dgd_person_reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Learning of Single-image and Cross-image Representations for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Joint_Learning_of_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Joint_Learning_of_CVPR_2016_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Comparative Attention Networks for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.04404" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.04404</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Multi-task Deep Network for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.05369" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.05369</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Siamese Long Short-Term Memory Architecture for Human Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.08381" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.08381</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>keywords: Market1501 rank1 = 65.9%</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1607.08378" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1607.08378</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Neural Networks with Inexact Matching for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2016</li><li>keywords: Normalized correlation layer, CUHK03/CUHK01/QMULGRID</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://papers.nips.cc/paper/6367-deep-neural-networks-with-inexact-matching-for-person-re-identification" class="Link-sc-1brdqhf-0 cKRjba">https://papers.nips.cc/paper/6367-deep-neural-networks-with-inexact-matching-for-person-re-identification</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/InnovArul/personreid_normxcorr" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/InnovArul/personreid_normxcorr</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification: Past, Present and Future</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.02984" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.02984</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Learning Prototype Domains for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.05047" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.05047</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Transfer Learning for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05244" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05244</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Discriminatively Learned CNN Embedding for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TOMM 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.05666" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.05666</a></li><li>github(official, MatConvnet): <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/2016_person_re-ID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/2016_person_re-ID</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/D-X-Y/caffe-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/D-X-Y/caffe-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-Identification via Recurrent Feature Aggregation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>keywords: recurrent feature aggregation network (RFA-Net)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.06351" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.06351</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/site/yanyichao91sjtu/" class="Link-sc-1brdqhf-0 cKRjba">https://sites.google.com/site/yanyichao91sjtu/</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/daodaofr/caffe-re-id" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daodaofr/caffe-re-id</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.04179" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.04179</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SVDNet for Pedestrian Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017 spotlight</li><li>intro: On the Market-1501 dataset, rank-1 accuracy is improved from 55.2% to 80.5% for CaffeNet,
and from 73.8% to 83.1% for ResNet-50</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.05693" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.05693</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>In Defense of the Triplet Loss for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07737" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07737</a></li><li>github(official, Theano): <a target="_blank" rel="noopener noreferrer" href="https://github.com/VisualComputingInstitute/triplet-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/VisualComputingInstitute/triplet-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond triplet loss: a deep quadruplet network for person re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.01719" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.01719</a></li><li>ppaper: <a target="_blank" rel="noopener noreferrer" href="http://cvip.computing.dundee.ac.uk/papers/Chen_CVPR_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://cvip.computing.dundee.ac.uk/papers/Chen_CVPR_2017_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quality Aware Network for Set to Set Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.03373" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.03373</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sciencefans/Quality-Aware-Network" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sciencefans/Quality-Aware-Network</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017. CASIA</li><li>keywords: Multi-Scale Context-Aware Network (MSCAN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.06555" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.06555</a></li><li>supplemental: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Learning_Deep_Context-Aware_2017_CVPR_supplemental.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Learning_Deep_Context-Aware_2017_CVPR_supplemental.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Point to Set Similarity Based Deep Feature Learning for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf</a></li><li>github(stay tuned): <a target="_blank" rel="noopener noreferrer" href="https://github.com/samaonline/Point-to-Set-Similarity-Based-Deep-Feature-Learning-for-Person-Re-identification" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/samaonline/Point-to-Set-Similarity-Based-Deep-Feature-Learning-for-Person-Re-identification</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Scalable Person Re-identification on Supervised Smoothed Manifold</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 spotlight</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.08359" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.08359</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=bESdJgalQrg" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=bESdJgalQrg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention-based Natural Language Person Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 Workshop (vision meets cognition)</li><li>keywords: Bidirectional Long Short- Term Memory (BLSTM)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.08923" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.08923</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part-based Deep Hashing for Large-scale Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Image Processing, 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.02145" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.02145</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Person Re-Identification with Improved Embedding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Person Re-Identification with Improved Embedding and Efficient Training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCB 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.03332" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.03332</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards a Principled Integration of Multi-Camera Re-Identification and Tracking through Optimal Bayes Filters</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.04608" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.04608</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/VisualComputingInstitute/towards-reid-tracking" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/VisualComputingInstitute/towards-reid-tracking</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-Identification by Deep Joint Learning of Multi-Loss Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.04724" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.04724</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Representation Learning with Part Loss for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Part Loss Networks</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.00798" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.00798</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pedestrian Alignment Network for Large-scale Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.00408" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.00408</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/Pedestrian_Alignment" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/Pedestrian_Alignment</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Efficient Image Representation for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.02319" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.02319</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification Using Visual Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07336" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07336</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>What-and-Where to Match: Deep Spatially Multiplicative Integration Networks for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07074" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07074</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Feature Learning via Structured Graph Laplacian Embedding for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07791" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07791</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large Margin Learning in Set to Set Similarity Comparison for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Multimedia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.05512" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.05512</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-scale Deep Learning Architectures for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.05165" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.05165</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-Identification by Deep Learning Multi-Scale Representations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>keywords: Deep Pyramid Feature Learning (DPFL)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w37/Chen_Person_Re-Identification_by_ICCV_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w37/Chen_Person_Re-Identification_by_ICCV_2017_paper.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.eecs.qmul.ac.uk/~sgg/papers/ChenEtAl_ICCV2017WK_CHI.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eecs.qmul.ac.uk/~sgg/papers/ChenEtAl_ICCV2017WK_CHI.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-Identification with Vision and Language</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.01202" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.01202</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1710.00478" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1710.00478</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pseudo-positive regularization for deep person re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.06500" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.06500</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Let Features Decide for Themselves: Feature Mask Network for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Feature Mask Network (FMN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07155" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07155</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Megvii Inc &amp; Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08184" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08184</a></li><li>evaluation website: (Market1501): <a target="_blank" rel="noopener noreferrer" href="http://reid-challenge.megvii.com/" class="Link-sc-1brdqhf-0 cKRjba">http://reid-challenge.megvii.com/</a></li><li>evaluation website: (CUHK03): <a target="_blank" rel="noopener noreferrer" href="http://reid-challenge.megvii.com/cuhk03" class="Link-sc-1brdqhf-0 cKRjba">http://reid-challenge.megvii.com/cuhk03</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Cosine Metric Learning for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://elib.dlr.de/116408/1/WACV2018.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://elib.dlr.de/116408/1/WACV2018.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/nwojke/cosine_metric_learning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nwojke/cosine_metric_learning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Region-based Quality Estimation Network for Large-scale Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08766" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08766</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Part Models: Person Retrieval with Refined Part Pooling</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>keywords: Part-based Convolutional Baseline (PCB), Refined Part Pooling (RPP)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.09349" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.09349</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/syfafterzy/PCB_RPP_for_reID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/syfafterzy/PCB_RPP_for_reID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep-Person: Learning Discriminative Deep Features for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10658" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10658</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Cross Network for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.06820" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.06820</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Re-ID done right: towards good practices for person re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.05339" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.05339</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Triplet-based Deep Similarity Learning for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV Workshops 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.03254" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.03254</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Group Consistent Similarity Learning via Deep CRFs for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 oral</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Group_Consistent_Similarity_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Group_Consistent_Similarity_CVPR_2018_paper.pdf</a></li><li>github(official, PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/dapengchen123/crf_affinity" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dapengchen123/crf_affinity</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>keywords: similarity preserving generative adversarial network (SPGAN), Siamese network, CycleGAN, domain adaptation</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07027" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07027</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Similarity-preserving Image-image Domain Adaptation for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.10551" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.10551</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Harmonious Attention Network for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>keywords: Harmonious Attention CNN (HA-CNN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.08122" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.08122</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Camera Style Adaptation for Person Re-identfication</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10295" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10295</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhunzhong07/CamStyle" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhunzhong07/CamStyle</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.07027" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.07027</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09937" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09937</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Level Factorisation Net for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>keywords: Multi-Level Factorisation Net (MLFN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09132" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09132</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Features for Multi-Target Multi-Camera Tracking and Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Good Appearance Features for Multi-Target Multi-Camera Tracking</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 spotlight</li><li>intro: Duke University</li><li>keywords: adaptive weighted triplet loss, hard-identity mining</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://vision.cs.duke.edu/DukeMTMC/" class="Link-sc-1brdqhf-0 cKRjba">http://vision.cs.duke.edu/DukeMTMC/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10859" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10859</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mask-guided Contrastive Attention Model for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>keywords: MGCAM</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/developfeng/MGCAM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/developfeng/MGCAM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient and Deep Person Re-Identification using Multi-Level Similarity</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.11353" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.11353</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification with Cascaded Pairwise Convolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Person_Re-Identification_With_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Person_Re-Identification_With_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention-Aware Compositional Network for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>intro: Sensets Technology Limited &amp; University of Sydney</li><li>keywords: Attention-Aware Compositional Network (AACN), Pose-guided Part Attention (PPA), Attention-aware Feature Composition (AFC)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.03344" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.03344</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Group-shuffling Random Walk for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/YantaoShen/kpm_rw_person_reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YantaoShen/kpm_rw_person_reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarially Occluded Samples for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/huanghoujing/AOS4ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huanghoujing/AOS4ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Easy Identification from Better Constraints: Multi-Shot Person Re-Identification from Reference Constraints</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Easy_Identification_From_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Easy_Identification_From_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Eliminating Background-bias for Robust Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Deep Kronecker-Product Matching for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/YantaoShen/kpm_rw_person_reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YantaoShen/kpm_rw_person_reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploiting Transitivity for Learning Person Re-identification Models on a Budget</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Resource Aware Person Re-identification across Multiple Resolutions</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Resource_Aware_Person_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Resource_Aware_Person_CVPR_2018_paper.pdf</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.08805" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.08805</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Channel Pyramid Person Matching Network for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 32nd AAAI Conference on Artificial Intelligence</li><li>keywords: Multi-Channel deep convolutional Pyramid Person Matching Network (MC-PPMN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.02558" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.02558</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pyramid Person Matching Network for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 9th Asian Conference on Machine Learning (ACML2017) JMLR Workshop and Conference Proceedings</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.02547" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.02547</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Virtual CNN Branching: Efficient Feature Ensemble for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.05872" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.05872</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weighted Bilinear Coding over Salient Body Parts for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08580" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08580</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Binary Coding for Efficient Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10914" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10914</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning View-Specific Deep Networks for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on image processing. Sun Yat-Sen University</li><li>keywords: cross-view Euclidean constraint (CV-EC), cross-view center loss (CV-CL)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.11333" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.11333</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Discriminative Features with Multiple Granularities for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia 2018</li><li>intro: Shanghai Jiao Tong University &amp; CloudWalk</li><li>keywords: Multiple Granularity Network (MGN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.01438" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.01438</a></li><li>video: <a target="_blank" rel="noopener noreferrer" href="https://edu.csdn.net/course/play/8426" class="Link-sc-1brdqhf-0 cKRjba">https://edu.csdn.net/course/play/8426</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Neural Networks for Person Re-identification Revisited</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Stanford University &amp; Google AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.03281" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.03281</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MaskReID: A Mask Based Deep Ranking Neural Network for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.03864" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.03864</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Horizontal Pyramid Matching for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>intro: UIUC &amp; IBM Research &amp; Cornell University &amp; Stevens Institute of Technology &amp; CloudWalk Technology</li><li>keywords: Horizontal Pyramid Matching (HPM), Horizontal Pyramid Pooling (HPP), horizontal random erasing (HRE)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.05275" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.05275</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/OasisYang/HPM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/OasisYang/HPM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.11027" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.11027</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Affinity based Pseudo Labeling for Semi-supervised Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.06118" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.06118</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantically Selective Augmentation for Deep Compact Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.04074" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.04074</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SphereReID: Deep Hypersphere Manifold Embedding for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Sphere Loss, feature normalization, weight normalization, balanced sampling</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.00537" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.00537</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>intro: University of Warwick &amp; Nanyang Technological University &amp; Charles Sturt University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.01440" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.01440</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Discriminative Feature Learning with Foreground Attention for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.01455" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.01455</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part-Aligned Bilinear Representations for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>intro: Seoul National University &amp; Microsoft Research &amp; Max Planck Institute &amp; University of Tubingen &amp; JD.COM</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.07094" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.07094</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yuminsuh/part_bilinear_reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yuminsuh/part_bilinear_reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018. Huazhong University of Science and Technology &amp; Horizon Robotics Inc.</li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.01571" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.01571</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Sequential Multi-camera Feature Fusion for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.07295" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.07295</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Deep Models of Person Re-identification for Cross-Dataset Usage</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AIAI 2018 (14th International Conference on Artificial Intelligence Applications and Innovations) proceeding</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08526" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08526</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Measuring the Temporal Behavior of Real-World Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.05499" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.05499</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Alignedreid＋+: Dynamically Matching Local Information for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/michuanhaohao/AlignedReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/michuanhaohao/AlignedReID</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sparse Label Smoothing for Semi-supervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.04976" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.04976</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jpainam/SLS_ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jpainam/SLS_ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>In Defense of the Classification Loss for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; Microsoft Research Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.05864" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.05864</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NIPS 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.02936" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.02936</a></li><li>github(Pytorch, official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/yxgeee/FD-GAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yxgeee/FD-GAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image-to-Video Person Re-Identification by Reusing Cross-modal Embeddings</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.03989" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.03989</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention Driven Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Pattern Recognition (PR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.05866" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.05866</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Coarse-to-fine Pyramidal Model for Person Re-identification via Multi-Loss Dynamic Training</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: YouTu Lab, Tencent</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.12193" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.12193</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.03768" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.03768</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Batch Feature Erasing for Person Re-identification and Beyond</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07130" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07130</a></li><li>github(official, Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/daizuozhuo/batch-feature-erasing-network" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daizuozhuo/batch-feature-erasing-network</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Re-Identification with Consistent Attentive Siamese Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07487" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07487</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One Shot Domain Adaptation for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.10144" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.10144</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Parameter-Free Spatial Attention Network for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.12150" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.12150</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HRanWang/Spatial-Attention" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HRanWang/Spatial-Attention</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spectral Feature Transformation for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Chinese Academy of Sciences &amp; TuSimple</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11405" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11405</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Identity Preserving Generative Adversarial Network for Cross-Domain Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.11510" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.11510</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dissecting Person Re-identification from the Viewpoint of Viewpoint</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.02162" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.02162</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast and Accurate Person Re-Identification with RMNet</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IOTG Computer Vision (ICV), Intel</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.02465" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.02465</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial-Temporal Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>intro: Sun Yat-sen University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.03282" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.03282</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Wanggcong/Spatial-Temporal-Re-identification</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Omni-directional Feature Learning for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tongji University</li><li>keywords: OIM loss</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.05319" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.05319</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Incremental Triplet Margin for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019 spotlight</li><li>intro: Hikvision Research Institute</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.06576" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.06576</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Densely Semantically Aligned Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: USTC &amp; MSRA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.08967" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.08967</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EANet: Enhancing Alignment for Cross-Domain Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CRISE &amp; CASIA &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.11369" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.11369</a></li><li>github(official, Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/huanghoujing/EANet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huanghoujing/EANet</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/53660395" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/53660395</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Backbone Can Not be Trained at Once: Rolling Back to Pre-trained Network for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>intro: Seoul National University &amp; Samsung SDS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.06140" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.06140</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Ensemble Feature for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: EnsembleNet</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.05798" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.05798</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adversarial Metric Attack for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Oxford &amp; Johns Hopkins University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.10650" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.10650</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Discovering Underlying Person Structure Pattern with Relative Local Distance for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SYSU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.10100" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.10100</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Wanggcong/RLD_codes" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Wanggcong/RLD_codes</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attributes-aided Part Detection and Refinement for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.10528" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.10528</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">B<strong>ags of Tricks and A Strong Baseline for Deep Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.07071" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.07071</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf</a></li><li>slides: <a target="_blank" rel="noopener noreferrer" href="https://drive.google.com/file/d/1h9SgdJenvfoNp9PTUxPiz5_K5HFCho-V/view" class="Link-sc-1brdqhf-0 cKRjba">https://drive.google.com/file/d/1h9SgdJenvfoNp9PTUxPiz5_K5HFCho-V/view</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/michuanhaohao/reid-strong-baseline" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/michuanhaohao/reid-strong-baseline</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Auto-ReID: Searching for a Part-aware ConvNet for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: NAS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.09776" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.09776</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/D-X-Y/Auto-ReID-and-Others" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/D-X-Y/Auto-ReID-and-Others</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Perceive Where to Focus: Learning Visibility-aware Part-level Features for Partial Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: Tsinghua University &amp; Megvii Technology</li><li>keywords: Visibility-aware Part Model (VPM)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.00537" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.00537</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pedestrian re-identification based on Tree branch network with local and global learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.00355" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.00355</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01990" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01990</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhunzhong07/ECN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhunzhong07/ECN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification with Bias-controlled Adversarial Training</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.00244" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.00244</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Relation-Aware Global Attention for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: University of Science and Technology of China &amp; Microsoft Research Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.02998" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.02998</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/Relation-Aware-Global-Attention-Networks" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/microsoft/Relation-Aware-Global-Attention-Networks</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification with Metric Learning using Privileged Information</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE TIP</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.05005" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.05005</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Scale Body-Part Mask Guided Attention for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Suning R&amp;D Center USA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.11041" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.11041</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Constrained Dominant Sets for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.11397" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.11397</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Illumination-Adaptive Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 1The University of Tokyo &amp; National Taiwan University &amp; National Institute of Informatics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.04525" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.04525</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain Adaptive Person Re-Identification via Camera Style Generation and Label Propagation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Sun Yat-Sen University &amp; Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.05382" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.05382</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Intra-modality Discrepancy: A Comprehensive Survey of Heterogeneous Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.10048" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.10048</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Multi-Index Hashing for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Nanjing University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.10980" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.10980</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention: A Big Surprise for Cross-Domain Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.12830" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.12830</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantics-Aligned Representation Learning for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: USTC &amp; MSRA</li><li>keywords: Semantics Aligning Network (SAN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.13143" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.13143</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Person Re-Identification with Confidence</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: EPFL</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.04692" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.04692</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CDPM: Convolutional Deformable Part Models for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.04976" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.04976</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Resolution-invariant Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2019</li><li>intro: Peking University &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.09748" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.09748</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Interaction-and-Aggregation Network for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.08435" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.08435</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Distilled Person Re-identification: Towards a More Scalable System</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>intro: Sun Yat-sen University &amp; YouTu Lab, Tencent</li><li>keywords: Multi-teacher Adaptive Similarity Distillation Framework, Log-Euclidean Similarity Distillation Loss,
Adaptive Knowledge Aggregator</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Universal Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Queen Mary University of London &amp; Vision Semantics Ltd.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.09511" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.09511</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ABD-Net: Attentive but Diverse Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.01114" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.01114</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TAMU-VITA/ABD-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TAMU-VITA/ABD-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fairest of Them All: Establishing a Strong Baseline for Cross-Domain Person ReID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Waterloo &amp; SPORTLOGiQ Inc.</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.12016" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.12016</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive Cross-camera Soft-label Learning for Semi-supervised Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05669" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05669</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mixed High-Order Attention Network for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05819" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05819</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chenbinghui1/MHN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chenbinghui1/MHN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.06052" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.06052</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HorNet: A Hierarchical Offshoot Recurrent Network for Improving Person Re-ID via Image Captioning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2019</li><li>intro: Nanjing Institute of Advanced Artificial Intelligence &amp; Horizon Robotics</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.04915" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.04915</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Knowledge Propagation for Image-to-Video Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.03885" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.03885</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.09086" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.09086</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Representations by Mutual Information for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.05860" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.05860</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Orthogonal Center Learning with Subspace Masking for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Youtu X-lab, Tencent</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.10535" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.10535</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Graph Representation Learning for Video Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.02240" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.02240</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>POD: Practical Object Detection with Scale-Sensitive Network</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.02225" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.02225</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Second-order Non-local Attention Networks for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.00295" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.00295</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.09675" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.09675</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>View Confusion Feature Learning for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.03849" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.03849</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Augmented Hard Example Mining for Generalizable Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.05280" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.05280</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Generalisable Omni-Scale Representations for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.06827" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.06827</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KaiyangZhou/deep-person-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KaiyangZhou/deep-person-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attention Network Robustification for Person ReID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DAMO Academy, Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.07038" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.07038</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>keywords: dual part-aligned block (DPB)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.10111" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.10111</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ggjy/P2Net.pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ggjy/P2Net.pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>An End-to-End Foreground-Aware Network for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.11547" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.11547</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Disentangled Representation for Robust Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2019</li><li>keywords: identity shuffle GAN (IS-GAN)</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://cvlab.yonsei.ac.kr/projects/ISGAN/" class="Link-sc-1brdqhf-0 cKRjba">https://cvlab.yonsei.ac.kr/projects/ISGAN/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.12003" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.12003</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ID-aware Quality for Set-based Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.09143" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.09143</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Calibrated Domain-Invariant Learning for Highly Generalizable Large Scale Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.11314" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.11314</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Collaborative Attention Network for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1911.13008" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1911.13008</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Viewpoint-Aware Loss with Angular Regularization for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.01300" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.01300</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zzhsysu/VA-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zzhsysu/VA-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AANet: Attribute Attention Network for Person Re-Identifications</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.09021" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.09021</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>In Defense of the Triplet Loss Again: Learning Robust Person Re-Identification with Fast Approximated Triplet Loss and Label Distillation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.07863" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.07863</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TAMU-VITA/FAT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TAMU-VITA/FAT</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking the Distribution Gap of Person Re-identification with Camera-based Batch Normalization</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>intro: Tsinghua University &amp; Huawei Inc. &amp; Hefei University of Technology &amp; University of Science and Technology of China</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2001.08680" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2001.08680</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/automan000/Camera-based-Person-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/automan000/Camera-based-Person-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Intra-Camera Supervised Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.05046" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.05046</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Precise Intra-camera Supervised Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University &amp; Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.04932" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.04932</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diversity-Achieving Slow-DropBlock Network for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.04414" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.04414</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MagnifierNet: Towards Semantic Regularization and Fusion for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.10979" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.10979</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Triplet Online Instance Matching Loss for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.10560" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2002.10560</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>When Person Re-identification Meets Changing Clothes</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.04070" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.04070</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-task Learning with Coarse Priors for Robust Part-aware Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: South China University of Technolog &amp; University of Sydney</li><li>keywords: Multi-task Part-aware Network (MPN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.08069" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.08069</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: To attack ReID, we propose a learning-to-mis-rank formulation to perturb the ranking of the system output</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.04199" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.04199</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Real-world Person Re-Identification via Degradation Invariance Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.04933" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.04933</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification in the 3D Space</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Technology Sydney</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.04569" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.04569</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/person-reid-3d" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/person-reid-3d</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Rethinking Classification Loss Designs for Person Re-identification with a Unified View</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; MicroSoft Research Asia &amp; Columbia University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.04991" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.04991</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multiple Expert Brainstorming for Domain Adaptive Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.01546" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.01546</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/YunpengZhai/MEB-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/YunpengZhai/MEB-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ESA-ReID: Entropy-Based Semantic Feature Alignment for Person re-ID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.04644" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.04644</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 oral</li><li>intro: Carnegie Mellon University &amp; NVIDIA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.10315" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.10315</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dual Distribution Alignment Network for Generalizable Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.13249" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.13249</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Bi-Directional Feature Perception Network for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.03509" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.03509</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.08528" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.08528</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xbq1994/HAA" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xbq1994/HAA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Faster Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.06826" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.06826</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wangguanan/light-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wangguanan/light-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Do Not Disturb Me: Person Re-identification Under the Interference of Other Pedestrians</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.06963" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.06963</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/X-BrainLab/PI-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/X-BrainLab/PI-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Apparel-invariant Feature Learning for Apparel-changed Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.06181" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.06181</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cluster-level Feature Alignment for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.06810" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.06810</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-Supervised Gait Encoding with Locality-Aware Attention for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.09435" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.09435</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Kali-Hac/SGE-LA" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Kali-Hac/SGE-LA</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Receptive Multi-granularity Representation for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Championship solution of NAIC 2019 Person re-ID Track</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.13450" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.13450</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Proxy Task Learning For Cross-Domain Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2020</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://scihub.wikicn.top/10.1109/ICME46284.2020.9102898" class="Link-sc-1brdqhf-0 cKRjba">https://scihub.wikicn.top/10.1109/ICME46284.2020.9102898</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/9102898" class="Link-sc-1brdqhf-0 cKRjba">https://ieeexplore.ieee.org/document/9102898</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/huanghoujing/PTL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huanghoujing/PTL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive Bilateral-Context Driven Model for Post-Processing Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.03098" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.03098</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/123ci/PBCmodel" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/123ci/PBCmodel</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Devil&#x27;s in the Detail: Graph-based Key-point Alignment and Embedding for Person Re-ID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tencent Youtu Lab &amp;  Sun Yat-sen University &amp; Huazhong University of Science and Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.05250" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.05250</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.07536" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.07536</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/libraflower/MutipleFeature-for-PRID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/libraflower/MutipleFeature-for-PRID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Beyond Triplet Loss: Person Re-identification with Fine-grained Difference-aware Pairwise Loss</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.10295" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.10295</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Batch Coherence-Driven Network for Part-aware Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.09692" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.09692</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FTN: Foreground-Guided Texture-Focused Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2009.11425" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2009.11425</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improve Person Re-Identification With Part Awareness Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TIP 2020</li><li>intro: Institute of Automation, Chinese Academy of Sciences &amp; Horizon Robotics, Inc</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/9124699" class="Link-sc-1brdqhf-0 cKRjba">https://ieeexplore.ieee.org/document/9124699</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/huanghoujing/PAL-MGN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huanghoujing/PAL-MGN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Performance Optimization for Federated Person Re-identification via Benchmark Analysis</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACMMM 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.11560" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.11560</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.11953" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.11953</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Domain Adversarial Feature Generalization for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.12563" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.12563</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.00417" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.00417</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Pre-training for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; Microsoft Research</li><li>intro: LUPerson dataset</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.03753" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.03753</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UnrealPerson: An Adaptive Pipeline towards Costless Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxov: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.04268" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.04268</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/FlyHighest/UnrealPerson" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/FlyHighest/UnrealPerson</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>One for More: Selecting Generalizable Samples for Generalizable ReID Model</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tencent Youtu Lab &amp; Sun Yat-sen University &amp; Southern University of Science and Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.05475" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.05475</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.08733" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.08733</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Camera-aware Proxies for Unsupervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>intro: 1Zhejiang University &amp; Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.10674" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.10674</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>1st Place Solution to VisDA-2020: Bias Elimination for Domain Adaptive Pedestrian Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 1st place solution to VisDA-2020 Challenge (ECCVW 2020)</li><li>intro: Zhejiang University &amp; Alibaba Group</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://ai.bu.edu/visda-2020/" class="Link-sc-1brdqhf-0 cKRjba">http://ai.bu.edu/visda-2020/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.13498" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.13498</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vimar-gu/Bias-Eliminate-DA-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vimar-gu/Bias-Eliminate-DA-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HAVANA: Hierarchical and Variation-Normalized Autoencoder for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.02568" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.02568</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AXM-Net: Cross-Modal Context Sharing Attention Network for Person Re-ID</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2101.08238" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2101.08238</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TransReID: Transformer-based Object Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Group &amp; Zhejiang University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.04378" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.04378</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Miner: A Deep and Multi-branch Network which Mines Rich and Diverse Features for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Digeiz AI Lab &amp; Ecole Polytechnique</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.09321" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.09321</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AttriMeter: An Attribute-guided Metric Interpreter for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; JD AI Research &amp; Ryerson University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.01451" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.01451</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Watching You: Global-guided Reciprocal Learning for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.04337" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.04337</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Lifelong Person Re-Identification via Adaptive Knowledge Accumulation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.12462" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.12462</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatiotemporal Transformer for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Beihang University &amp; Pengcheng Laboratory &amp; Tsinghua University &amp; University of Science and Technology of China &amp; Xidian University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.16469" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.16469</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AAformer: Auto-Aligned Transformer for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chinese Academy of Sciences &amp; Peng Cheng Laboratory &amp; Peking University &amp; Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.00921" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.00921</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Combined Depth Space based Architecture Search For Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.04163" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.04163</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Graph-based Person Signature for Person Re-Identifications</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 Workshops</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.06770" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.06770</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Multi-Source Domain Adaptation for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.12961" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.12961</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Generalizable Person Re-identification with Relevance-aware Mixture of Experts</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.09156" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.09156</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transformer-Based Deep Image Matching for Generalizable Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.14432" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.14432</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-Identification with a Locally Aware Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Maryland Baltimore County</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.03720" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.03720</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Instance-level Spatial-Temporal Patterns for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.00171" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.00171</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/RenMin1991/cleaned-DukeMTMC-reID/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/RenMin1991/cleaned-DukeMTMC-reID/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021 oral</li><li>keywords: Peking University &amp; Singapore University of Technology and Design &amp; Megvii Technology &amp; National University of Singapore &amp; Peng Cheng Laboratory</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.02413" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.02413</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/SikaStar/IDM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/SikaStar/IDM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Benchmarks for Corruption Invariant Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NeurIPS 2021 Track on Datasets and Benchmarks</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.00880" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.00880</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/MinghuiChen43/CIL-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/MinghuiChen43/CIL-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-Supervised Pre-Training for Transformer-Based Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.12084" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.12084</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/michuanhaohao/TransReID-SSL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/michuanhaohao/TransReID-SSL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stronger Baseline for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The third-place solution for ICCV2021 VIPriors Re-identification Challenge</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.01059" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.01059</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dual Cluster Contrastive learning for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Institute of Automation, CAS</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.04662" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.04662</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/htyao89/Dual-Cluster-Contrastive/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/htyao89/Dual-Cluster-Contrastive/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>META: Mimicking Embedding via oThers&#x27; Aggregation for Generalizable Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CASIA &amp; UCAS &amp; JD AI Reasearch</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.08684" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.08684</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Centroid Representation Network for Domain Adaptive Person Re-ID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.11689" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.11689</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.15210" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.15210</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-Scale Pre-training for Person Re-identification with Noisy Labels</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>intro: University of Science and Technology of China &amp; Microsoft Research &amp; Microsoft Cloud AI &amp; IDEA</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.16533" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.16533</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DengpanFu/LUPerson-NL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DengpanFu/LUPerson-NL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Free Lunch to Person Re-identification: Learning from Automatically Generated Noisy Tracklets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tsinghua University &amp; Alibaba Group</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.00891" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.00891</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Clothes-Changing Person Re-identification with RGB Modality Only</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.06890" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.06890</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/guxinqian/Simple-CCReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guxinqian/Simple-CCReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NFormer: Robust Person Re-identification with Neighbor Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022 poster</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.09331" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.09331</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/haochenheheda/NFormer" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/haochenheheda/NFormer</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature-Distribution Perturbation and Calibration for Generalized Person ReID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>iintro: Queen Mary University of London</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2205.11197" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2205.11197</a></li></ul><h1 id="person-search" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#person-search" color="auto.gray.8" aria-label="Person Search permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Person Search</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Deep Learning for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.01850v2" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.01850v1</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Detection and Identification Feature Learning for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 Spotlight</li><li>intro: The Chinese University of Hong Kong &amp; Sun Yat-Sen University &amp; SenseTime Group Limited</li><li>keywords: Online Instance Matching (OIM) loss function</li><li>homepage(dataset+code):<a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.01850" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.01850</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf</a></li><li>github(official. Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/ShuangLI59/person_search" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ShuangLI59/person_search</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 spotlight</li><li>intro: University of Technology Sydney &amp; UTSA &amp; USTC &amp; UCSD</li><li>keywords: PRW dataset</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.liangzheng.com.cn/Project/project_prw.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.liangzheng.com.cn/Project/project_prw.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.02531" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.02531</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liangzheng06/PRW-baseline" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liangzheng06/PRW-baseline</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=dbOGwBITJqo" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=dbOGwBITJqo</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>IAN: The Individual Aggregation Network for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Xi’an Jiaotong-Liverpool University &amp; National University of Singapore</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.05552" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.05552</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neural Person Search Machines</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>intro: Hefei University of Technology &amp; National University of Singapore &amp; Tencent AI Lab &amp; Panasonic R&amp;D Center Singapore &amp; Southwest Jiaotong University &amp; 360 AI Institute</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.06777" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.06777</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://ai.tencent.com/ailab/media/publications/Neural_Person_Search_Machines.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://ai.tencent.com/ailab/media/publications/Neural_Person_Search_Machines.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Efficient Person Search via Expert-Guided Knowledge Distillation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/document/8759990" class="Link-sc-1brdqhf-0 cKRjba">https://ieeexplore.ieee.org/document/8759990</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Detection and Re-identification Integrated Net for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Chongqing University &amp; Hefei University of Technology</li><li>keywords: I-Net</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00376" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00376</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Search via A Mask-guided Two-stream CNN Model</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>intro: Nanjing University of Science and Technology &amp; Youtu Lab, Tencent</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08107" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08107</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Search by Multi-Scale Matching</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>intro: Queen Mary University of London &amp; Vision Semantics Ltd</li><li>keywords: Cross-Level Semantic Alignment (CLSA)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.08582" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.08582</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RCAA: Relational Context-Aware Agents for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>intro: Carnegie Mellon University &amp; Chinese Academy of Sciences &amp; University of Technology Sydney</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaojun_Chang_RCAA_Relational_Context-Aware_ECCV_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaojun_Chang_RCAA_Relational_Context-Aware_ECCV_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fast Person Search Pipeline</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2019</li><li>intro: Sun Yat-sen University</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/8784982/" class="Link-sc-1brdqhf-0 cKRjba">https://ieeexplore.ieee.org/abstract/document/8784982/</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://sci-hub.tw/10.1109/ICME.2019.00195" class="Link-sc-1brdqhf-0 cKRjba">https://sci-hub.tw/10.1109/ICME.2019.00195</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Context Graph for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 Oral</li><li>intro: Shanghai Jiao Tong University &amp; Tencent YouTu Lab &amp;  Inception Institute of Artificial Intelligence, UAE</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01830" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01830</a></li><li>github(official, Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/sjtuzq/person_search_gcn" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sjtuzq/person_search_gcn</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Query-guided End-to-End Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 poster</li><li>intro: OSRAM GmbH &amp; Technische Universität München</li><li>keywords: query-guided end-to-end person search network (QEEPS)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.01203" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.01203</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/munjalbharti/Query-guided-End-to-End-Person-Search" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/munjalbharti/Query-guided-End-to-End-Person-Search</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Knowledge Distillation for End-to-End Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2019</li><li>intro: Technische Universität München &amp; OSRAM GmbH</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.01058" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.01058</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Re-ID Driven Localization Refinement for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: Huazhong University of Science and Technology &amp; Peking University &amp; Shanghai Jiao Tong University &amp; Megvii Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.08580" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.08580</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>End-to-End Thorough Body Perception for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>intro: Horizon Robotics &amp; Chinese Academy of Sciences</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://aaai.org/Papers/AAAI/2020GB/AAAI-TianK.2778.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://aaai.org/Papers/AAAI/2020GB/AAAI-TianK.2778.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Hierarchical Online Instance Matching for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://aaai.org/Papers/AAAI/2020GB/AAAI-ChenD.1557.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://aaai.org/Papers/AAAI/2020GB/AAAI-ChenD.1557.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DeanChan/HOIM-PyTorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DeanChan/HOIM-PyTorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Robust Partial Matching for Person Search in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: Peking University &amp; Intellifusion &amp; CUHK</li><li>keywords: Align-to-Part Network (APNet)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.09329" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.09329</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Person Objectness and Repulsion for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro:  Chinese Academy of Sciences</li><li>aarxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.00155" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.00155</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Norm-Aware Embedding for Efﬁcient Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Norm-Aware_Embedding_for_Efficient_Person_Search_CVPR_2020_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Norm-Aware_Embedding_for_Efficient_Person_Search_CVPR_2020_paper.pdf</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://github.com/DeanChan/NAE4PS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DeanChan/NAE4PS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Multi-task Joint Framework for Real-time Person Search</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.06418" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.06418</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diverse Knowledge Distillation for End-to-End Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2021</li><li>intro: Tongji University &amp; The University of Adelaide &amp; Monash University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.11187" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.11187</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhangxinyu-xyz/DKD-PersonSearch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhangxinyu-xyz/DKD-PersonSearch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Attribute Enhancement Network for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.07968" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.07968</a></li><li>gihtub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chenlq123/MAE" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chenlq123/MAE</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sequential End-to-end Network for Efficient Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tongji University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.10148" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.10148</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Anchor-Free Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Inception Institute of Artificial Intelligence (IIAI) &amp; University of Oxford</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11617" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11617</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/daodaofr/AlignPS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/daodaofr/AlignPS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploring Visual Context for Weakly Supervised Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.10506" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.10506</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ljpadam/CGPS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ljpadam/CGPS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Making Person Search Enjoy the Merits of Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.10536" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.10536</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Context-Aware Unsupervised Clustering for Person Search</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2110.01341" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2110.01341</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Context-Aware Embedding for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Beihang University &amp; MEGVII Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.14316" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.14316</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Subtask-dominated Transfer Learning for Long-tail Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.00527" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.00527</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Global-Local Context Network for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.02500" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.02500</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZhengPeng7/GLCNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ZhengPeng7/GLCNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascade Transformers for End-to-End Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.09642" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.09642</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Kitware/COAT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Kitware/COAT</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CGUA: Context-Guided and Unpaired-Assisted Weakly Supervised Person Search</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.14307" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.14307</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PSTR: End-to-End One-Step Person Search With Transformers</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.03340" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.03340</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JialeCao001/PSTR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JialeCao001/PSTR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OIMNet++: Prototypical Normalization and Localization-aware Learning for Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2022</li><li>intro: Yonsei University &amp; Korea Institute of Science and Technology</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://cvlab.yonsei.ac.kr/projects/OIMNetPlus/" class="Link-sc-1brdqhf-0 cKRjba">https://cvlab.yonsei.ac.kr/projects/OIMNetPlus/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.10320" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.10320</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cvlab-yonsei/OIMNetPlus" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cvlab-yonsei/OIMNetPlus</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain Adaptive Person Search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2022 Oral</li><li>intro: Shanghai Jiao Tong University &amp; Tencent Youtu Lab</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2207.11898" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2207.11898</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/caposerenity/DAPS" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/caposerenity/DAPS</a></li></ul><h1 id="pose--viewpoint-for-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pose--viewpoint-for-re-id" color="auto.gray.8" aria-label="Pose / Viewpoint for Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pose / Viewpoint for Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose Invariant Embedding for Deep Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: pose invariant embedding (PIE), PoseBox fusion (PBF) CNN</li><li>arixv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.07732" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.07732</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deeply-Learned Part-Aligned Representations for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.07256" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.07256</a></li><li>github(official, Caffe): <a target="_blank" rel="noopener noreferrer" href="https://github.com/zlmzju/part_reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zlmzju/part_reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/yokattame/SpindleNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/yokattame/SpindleNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose-driven Deep Convolutional Model for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.08325" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.08325</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10378" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10378</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/pse-ecn/pose-sensitive-embedding" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pse-ecn/pose-sensitive-embedding</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose-Driven Deep Models for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Masters thesis</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.08709" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.08709</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose Transferrable Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person re-identification with fusion of hand-crafted and deep pose-based body region features</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.10630" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.10630</a></p><h1 id="gan-for-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#gan-for-re-id" color="auto.gray.8" aria-label="GAN for Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GAN for Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.07717" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.07717</a></li><li>github(official, Matlab): <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/Person-reID_GAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/Person-reID_GAN</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/qiaoguan/Person-reid-GAN-pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/qiaoguan/Person-reid-GAN-pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 spotlight</li><li>intro: PTGAN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.08565" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.08565</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JoinWei-PKU/PTGAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JoinWei-PKU/PTGAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose-Normalized Image Generation for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: PN-GAN</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.02225" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.02225</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/naiq/PN_GAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/naiq/PN_GAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-pseudo Regularized Label for Generated Samples in Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.06742" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.06742</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Huang-3/MpRL-for-person-re-ID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Huang-3/MpRL-for-person-re-ID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Discriminative and Generative Learning for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>intro: NVIDIA &amp; University of Technology Sydney &amp; Australian National University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.07223" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.07223</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVlabs/DG-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NVlabs/DG-Net</a></li><li>youtube: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=ubCrEAIpQs4" class="Link-sc-1brdqhf-0 cKRjba">https://www.youtube.com/watch?v=ubCrEAIpQs4</a></li><li>bilibili: <a target="_blank" rel="noopener noreferrer" href="https://www.bilibili.com/video/av51439240" class="Link-sc-1brdqhf-0 cKRjba">https://www.bilibili.com/video/av51439240</a></li></ul><h1 id="human-parsing-for-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#human-parsing-for-re-id" color="auto.gray.8" aria-label="Human Parsing for Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Human Parsing for Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Human Semantic Parsing for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. SPReID</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00216" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00216</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/emrahbasaran/SPReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/emrahbasaran/SPReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: Saliency-Semantic Parsing Re-Identification (SSP-ReID)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.05618" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.05618</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Identity-Guided Human Semantic Parsing for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 spotlight</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.13467" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.13467</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CASIA-IVA-Lab/ISP-reID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CASIA-IVA-Lab/ISP-reID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Human Parsing Based Alignment with Multi-task Learning for Occluded Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2020 Oral</li><li>paper; <a target="_blank" rel="noopener noreferrer" href="https://scihub.wikicn.top/10.1109/ICME46284.2020.9102789" class="Link-sc-1brdqhf-0 cKRjba">https://scihub.wikicn.top/10.1109/ICME46284.2020.9102789</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/9102789" class="Link-sc-1brdqhf-0 cKRjba">https://ieeexplore.ieee.org/abstract/document/9102789</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/huanghoujing/HPNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/huanghoujing/HPNet</a></li></ul><h1 id="occluded-person-re-id--partial-person-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#occluded-person-re-id--partial-person-re-id" color="auto.gray.8" aria-label="Occluded Person Re-ID / Partial Person Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Occluded Person Re-ID / Partial Person Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Partial Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-Free Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>intro: Market1501 rank1=83.58%</li><li>keywords: Deep Spatial feature Reconstruction (DSR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1801.00881" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1801.00881</a></li><li>github(official, PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/lingxiao-he/Partial-Person-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lingxiao-he/Partial-Person-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Occluded Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2018</li><li>intro: Sun Yat-Sen University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.02792" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.02792</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Partial Person Re-identification with Alignment and Hallucination</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Imperial College London</li><li>keywords: Partial Matching Net (PMN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.09162" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.09162</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SCPNet: Spatial-Channel Parallelism Network for Joint Holistic and Partial Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACCV 2018</li><li>intro: Megvii Inc. (Face++) &amp; Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.06996" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.06996</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xfanplus/Open-SCPNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xfanplus/Open-SCPNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>STNReID : Deep Convolutional Networks with Pairwise Spatial Transformer Networks for Partial Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Zhejiang University &amp; Megvii Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.07072" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.07072</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.04975" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.04975</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Novel Teacher-Student Learning Framework For Occluded Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.03253" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.03253</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VRSTC: Occlusion-Free Video Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.08427" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.08427</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose-Guided Feature Alignment for Occluded Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: Baidu Research &amp; University of Technology Sydney</li><li>keywords: Pose-Guided Feature Alignment (PGFA)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://yu-wu.net/pdf/ICCV2019_Occluded-reID.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://yu-wu.net/pdf/ICCV2019_Occluded-reID.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lightas/Occluded-DukeMTMC-Dataset" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lightas/Occluded-DukeMTMC-Dataset</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: Institute of Automation, CAS &amp; Megvii Inc. &amp; Beijing Institute of Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.08177" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.08177</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wangguanan/HOReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wangguanan/HOReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose-guided Visible Part Matching for Occluded Person ReID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: Dalian University of Technology &amp; Pengcheng Lab &amp; The University of Sydney</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.00230" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.00230</a></li><li>github(official, pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/hh23333/PVPM" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hh23333/PVPM</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MHSA-Net: Multi-Head Self-Attention Network for Occluded Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2008.04015" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2008.04015</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hongchenphd/MHSA-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hongchenphd/MHSA-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Holistic Guidance for Occluded Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: École de technologie supérieure &amp; Genetec Inc</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.06524" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.06524</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Neighbourhood-guided Feature Reconstruction for Occluded Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.07345" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.07345</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.04095" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.04095</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Completion for Occluded Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.12733" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.12733</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/blue-blue272/OccludedReID-RFCnet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/blue-blue272/OccludedReID-RFCnet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Disentangled Representation Implicitly via Transformer for Occluded Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.02380" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.02380</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Anonymous-release-code/DRL-Net" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Anonymous-release-code/DRL-Net</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM MM 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.00139" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.00139</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Feature Erasing and Diffusion Network for Occluded Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Monash University &amp; SenseTime &amp; The University of Sydney &amp; Xidian University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.08740" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.08740</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Quality-aware Part Models for Occluded Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: South China University of Technology &amp; Baidu Inc &amp; Shenzhen University &amp; JD Explore Academy</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2201.00107" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2201.00107</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Motion-Aware Transformer For Occluded Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: NetEase &amp; China JiLiang University &amp; Hunan Institute of Engineering</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2202.04243" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2202.04243</a></li></ul><h1 id="cross-modality-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#cross-modality-re-id" color="auto.gray.8" aria-label="Cross-modality Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Cross-modality Re-ID</h1><h2 id="rgb-ir-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#rgb-ir-re-id" color="auto.gray.8" aria-label="RGB-IR Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>RGB-IR Re-ID</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RGB-Infrared Cross-Modality Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2017/papers/Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>keywords: Alignment Generative Adversarial Network (AlignGAN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.05839" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.05839</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>intro: Yonsei University</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://cvlab.yonsei.ac.kr/projects/LbA/" class="Link-sc-1brdqhf-0 cKRjba">https://cvlab.yonsei.ac.kr/projects/LbA/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.07422" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.07422</a></li></ul><h2 id="depth-based-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#depth-based-re-id" color="auto.gray.8" aria-label="Depth-Based Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Depth-Based Re-ID</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Karianakis_Reinforced_Temporal_Attention_ECCV_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Karianakis_Reinforced_Temporal_Attention_ECCV_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Cross-Modal Distillation Network for Person Re-identification in RGB-Depth</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1810.11641" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1810.11641</a></p><h1 id="low-resolution-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#low-resolution-re-id" color="auto.gray.8" aria-label="Low Resolution Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Low Resolution Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-scale Learning for Low-resolution Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Li_Multi-Scale_Learning_for_ICCV_2015_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Li_Multi-Scale_Learning_for_ICCV_2015_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cascaded SR-GAN for Scale-Adaptive Low Resolution Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IJCAI 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://www.ijcai.org/proceedings/2018/0541.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://www.ijcai.org/proceedings/2018/0541.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Low-Resolution Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2018</li><li>keywords: Super resolution and Identity joiNt learninG (SING)</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.eecs.qmul.ac.uk/~xiatian/papers/JiaoEtAl_2018AAAI.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eecs.qmul.ac.uk/~xiatian/papers/JiaoEtAl_2018AAAI.pdf</a></li></ul><h1 id="reinforcement-learning-for-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#reinforcement-learning-for-re-id" color="auto.gray.8" aria-label="Reinforcement Learning for Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reinforcement Learning for Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Reinforcement Learning Attention Selection for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.02785" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.02785</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.eecs.qmul.ac.uk/~sgg/papers/LanEtAl_2017BMVC.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.eecs.qmul.ac.uk/~sgg/papers/LanEtAl_2017BMVC.pdf</a></li></ul><h1 id="attributes-prediction-for-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#attributes-prediction-for-re-id" color="auto.gray.8" aria-label="Attributes Prediction for Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Attributes Prediction for Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Task Learning with Low Rank Attribute Embedding for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2015</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://legacydirs.umiacs.umd.edu/~fyang/papers/iccv15.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://legacydirs.umiacs.umd.edu/~fyang/papers/iccv15.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Attributes Driven Multi-Camera Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1605.03259" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1605.03259</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Improving Person Re-identification by Attribute and Identity Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1703.07220" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1703.07220</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification by Deep Learning Attribute-Complementary Information</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 workshop</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://sci-hub.tw/10.1109/CVPRW.2017.186" class="Link-sc-1brdqhf-0 cKRjba">https://sci-hub.tw/10.1109/CVPRW.2017.186</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CA3Net: Contextual-Attentional Attribute-Appearance Network for Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07544" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07544</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attribute analysis with synthetic dataset for person re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.07139" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.07139</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Taking A Closer Look at Synthesis: Fine-grained Attribute Analysis for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Shanghai Jiao Tong University &amp; National University of Defense Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.08145" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.08145</a></li></ul><h1 id="video-person-re-identification" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video-person-re-identification" color="auto.gray.8" aria-label="Video Person Re-Identification permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video Person Re-Identification</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Recurrent Convolutional Network for Video-based Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2016</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/niallmcl/Recurrent-Convolutional-Video-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/niallmcl/Recurrent-Convolutional-Video-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Recurrent Convolutional Networks for Video-based Person Re-identification: An End-to-End Approach</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1606.01609" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1606.01609</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.02286" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.02286</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Three-Stream Convolutional Networks for Video-based Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.01652" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.01652</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LVreID: Person Re-Identification with Long Sequence Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.07286" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.07286</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-shot Pedestrian Re-identification via Sequential Decision Making</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018. TuSimple</li><li>keywords: reinforcement learning</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.07257" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.07257</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/TuSimple/rl-multishot-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/TuSimple/rl-multishot-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Distributional Representation and Set Distance for Multi-shot Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CMU</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.01119" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.01119</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LVreID: Person Re-Identification with Long Sequence Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1712.07286" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1712.07286</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CUHK-SenseTime &amp; Argo AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09882" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09882</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 Poster</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Person Re-Identification With Competitive Snippet-Similarity Aggregation and Co-Attentive Snippet Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Revisiting Temporal Modeling for Video-based Person ReID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.02104" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.02104</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jiyanggao/Video-Person-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jiyanggao/Video-Person-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Person Re-identification by Temporal Residual Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1802.07918" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1802.07918</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Spatial and Temporal Features Mixture Model with Body Parts for Video-based Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.00975" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.00975</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video-based Person Re-identification via 3D Convolutional Networks and Non-local Attention</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China &amp; University of Chinese Academy of Sciences</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.05073" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.05073</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial-Temporal Synergic Residual Learning for Video Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.05799" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.05799</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SCAN: Self-and-Collaborative Attention Network for Video Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.05688" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.05688</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Where-and-When to Look: Deep Siamese Attention Networks for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: IEEE Transactions on Multimedia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.01911" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.01911</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>STA: Spatial-Temporal Attention for Large-Scale Video-based Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.04129" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.04129</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-scale 3D Convolution Network for Video Based Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07468" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07468</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Active Learning for Video-based Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.05785" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.05785</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial and Temporal Mutual Promotion for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.10305" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.10305</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>3D PersonVLAD: Learning Deep Global Representations for Video-based Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.10222" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.10222</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GAN-based Pose-aware Regulation for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Heriot-Watt University &amp; University of Edinburgh &amp; Queen’s University Belfast &amp; Anyvision</li><li>keywords: Weighted Fusion (WF) &amp; Weighted-Pose Regulation (WPR)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.11552" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.11552</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Convolutional Temporal Attention Model for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.04492" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.04492</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Person Re-Identification using Learned Clip Similarity Aggregation</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.08055" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.08055</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Person Re-ID: Fantastic Techniques and Where to Find Them</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.05295" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.05295</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ppriyank/Video-Person-Re-ID-Fantastic-Techniques-and-Where-to-Find-Them" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ppriyank/Video-Person-Re-ID-Fantastic-Techniques-and-Where-to-Find-Them</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Granularity Reference-Aided Attentive Feature Aggregation for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020</li><li>intro: 1University of Science and Technology of China &amp; Microsoft Research Asia</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.12224" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.12224</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Appearance-Preserving 3D Convolution for Video-based Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020 Oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.08434" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.08434</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/guxinqian/AP3D" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/guxinqian/AP3D</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Temporal Complementary Learning for Video Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.09357" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.09357</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/blue-blue272/VideoReID-TCLNet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/blue-blue272/VideoReID-TCLNet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Reference-Aided Part-Aligned Feature Disentangling for Video Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11319" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11319</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Spatial-Temporal Correlation and Topology Learning for Person Re-Identification in Videos</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.08241" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.08241</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.14783" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.14783</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video-based Person Re-identification without Bells and Whistles</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021 Biometrics Workshop</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.10678" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.10678</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jackie840129/CF-AAN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jackie840129/CF-AAN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Keypoint Message Passing for Video-based Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2111.08279" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2111.08279</a></p><h1 id="re-ranking" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#re-ranking" color="auto.gray.8" aria-label="Re-ranking permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Re-ranking</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Divide and Fuse: A Re-ranking Approach for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.04169" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.04169</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Re-ranking Person Re-identification with k-reciprocal Encoding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1701.08398" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1701.08398</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zhunzhong07/person-re-ranking" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhunzhong07/person-re-ranking</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1711.10378" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1711.10378</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/pse-ecn/expanded-cross-neighborhood" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pse-ecn/expanded-cross-neighborhood</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Re-ranking of Deep Feature for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.08561" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.08561</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Graph Convolution for Re-ranking in Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2107.02220" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2107.02220</a></p><h1 id="unsupervised-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#unsupervised-re-id" color="auto.gray.8" aria-label="Unsupervised Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Unsupervised Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Person Re-identification: Clustering and Fine-tuning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.10444" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.10444</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stepwise Metric Promotion for Unsupervised Video Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/lilithliu/StepwiseMetricPromotion-code" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/lilithliu/StepwiseMetricPromotion-code</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Dynamic Label Graph Matching for Unsupervised Video Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1709.09297" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1709.09297</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/mangye16/dgm_re-id" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mangye16/dgm_re-id</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.07293" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.07293</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ahangchen/TFusion" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ahangchen/TFusion</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/34778414" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/34778414</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cross-dataset Person Re-Identification Using Similarity Preserved Generative Adversarial Networks</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.04533" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.04533</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.07293" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.07293</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ahangchen/rank-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ahangchen/rank-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.09786" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.09786</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018 workshop. National Taiwan University &amp; Umbo Computer Vision</li><li>keywords: adaptation and re-identification network (ARN)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.09347" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.09347</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Domain Adaptation through Synthesis for Unsupervised Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.10094" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.10094</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Domain Adaptive Re-Identification: Theory and Practice</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Wuhan University &amp; Horizon Robotics &amp; Huazhong Univ. of Science and Technology</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1807.11334" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1807.11334</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/LcDog/DomainAdaptiveReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/LcDog/DomainAdaptiveReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Association Learning for Unsupervised Video Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: BMVC 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.07301" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.07301</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Support Neighbor Loss for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACM Multimedia (ACM MM) 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1808.06030" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1808.06030</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Person Re-identification by Deep Learning Tracklet Association</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2018 Oral</li><li>keywords: Tracklet Association Unsupervised Deep Learning (TAUDL)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.02874" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.02874</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019 oral</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.10144" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.10144</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/OasisYang/SSG" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/OasisYang/SSG</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Tracklet Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.00535" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.00535</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liminxian/DukeMTMC-SI-Tracklet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liminxian/DukeMTMC-SI-Tracklet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Person Re-identification by Deep Asymmetric Metric Embedding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TPAMI</li><li>keywords: DEep Clustering-based Asymmetric MEtric Learning (DECAMEL)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.10177" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.10177</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KovenYu/DECAMEL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KovenYu/DECAMEL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Person Re-identification by Soft Multilabel Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019 oral</li><li>intro: Sun Yat-sen University &amp; YouTu Lab &amp; Queen Mary University of London</li><li>keywords: MAR (MultilAbel Reference learning), soft multilabel-guided hard negative mining</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://kovenyu.com/publication/2019-cvpr-mar/" class="Link-sc-1brdqhf-0 cKRjba">https://kovenyu.com/publication/2019-cvpr-mar/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.06325" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.06325</a></li><li>github(official, Pytorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/KovenYu/MAR" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KovenYu/MAR</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Bottom-up Clustering Approach to Unsupervised Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2019 oral</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="https://vana77.github.io/vana77.github.io/images/AAAI19.pdf" class="Link-sc-1brdqhf-0 cKRjba">https://vana77.github.io/vana77.github.io/images/AAAI19.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.03425" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.03425</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards better Validity: Dispersion based Clustering for Unsupervised Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.01308" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.01308</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gddingcs/Dispersion-based-Clustering" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gddingcs/Dispersion-based-Clustering</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PAC-GAN: An Effective Pose Augmentation Scheme for Unsupervised Cross-View Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1906.01792" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1906.01792</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Adaptive Exploration for Unsupervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.04194" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.04194</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Adapt Invariance in Memory for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.00485" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.00485</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Consistent Cross-view Matching for Unsupervised Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1908.10486" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1908.10486</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning to Align Multi-Camera Domain for Unsupervised Video Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.13248" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.13248</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Progressive Unsupervised Person Re-identification by Tracklet Association with Spatio-Temporal Regularization</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.11560" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.11560</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Asymmetric Co-Teaching for Unsupervised Cross Domain Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: AAAI 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1912.01349" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1912.01349</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/FlyingRoastDuck/ACT_AAAI20" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/FlyingRoastDuck/ACT_AAAI20</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Memorizing Comprehensively to Learn Adaptively: Unsupervised Cross-Domain Person Re-ID with Multi-level Memory</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2001.04123" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2001.04123</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Domain Adaptation in the Dissimilarity Space for Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.13890" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.13890</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unsupervised Attention Based Instance Discriminative Learning for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: WACV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2011.01888" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2011.01888</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Generative and Contrastive Learning for Unsupervised Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2012.09071" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2012.09071</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chenhao2345/GCL" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chenhao2345/GCL</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Xiamen University &amp; University of Trento &amp; Minjiang Universit &amp; Minnan Normal University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.04618" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.04618</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/FlyingRoastDuck/MetaCam_DSCE" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/FlyingRoastDuck/MetaCam_DSCE</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cluster Contrast for Unsupervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Alibaba A.I. Labs &amp; Simon Fraser University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11568" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11568</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wangguangyuan/ClusterContrast" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wangguangyuan/ClusterContrast</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Intra-Inter Camera Similarity for Unsupervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>intro: Peking University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.11658" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.11658</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Group-aware Label Transfer for Domain Adaptive Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2103.12366" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2103.12366</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-Scale Unsupervised Person Re-Identification with Contrastive Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tongji University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2105.07914" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2105.07914</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Beijing University of Posts and Telecommunications</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.07846" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2106.07846</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Towards Discriminative Representation Learning for Unsupervised Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2021</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2108.03439" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2108.03439</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Unleashing the Potential of Unsupervised Pre-Training with Intra-Identity Regularization for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: University of Science and Technology of China</li><li>keywords: UP-ReID</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2112.00317" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2112.00317</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Frost-Yang-99/UP-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Frost-Yang-99/UP-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part-based Pseudo Label Refinement for Unsupervised Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>intro: KAIST</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.14675" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2203.14675</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Implicit Sample Extension for Unsupervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2022</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2204.06892" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2204.06892</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PaddlePaddle/PaddleClas" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PaddlePaddle/PaddleClas</a></li></ul><h1 id="weakly-supervised-person-re-identification" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#weakly-supervised-person-re-identification" color="auto.gray.8" aria-label="Weakly Supervised Person Re-identification permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Weakly Supervised Person Re-identification</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weakly Supervised Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2019</li><li>keywords: multi-instance multi-label learning (MIML), Cross-View MIML (CV-MIML)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.03832" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.03832</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weakly Supervised Person Re-identification: Cost-effective Learning with A New Benchmark</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>keywords: SYSU-30k</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.03845" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.03845</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Weakly Supervised Tracklet Person Re-Identification by Deep Feature-wise Mutual Learning</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.14333" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.14333</a></p><h1 id="vehicle-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#vehicle-re-id" color="auto.gray.8" aria-label="Vehicle Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Vehicle Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2017</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.03918" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.03918</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Viewpoint-Aware Attentive Multi-View Inference for Vehicle Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2018</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>RAM: A Region-Aware Deep Model for Vehicle Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICME 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1806.09283" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1806.09283</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Re-Identification in Context</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Pattern Recognition - 40th German Conference, (GCPR) 2018, Stuttgart</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://qmul-vric.github.io/" class="Link-sc-1brdqhf-0 cKRjba">https://qmul-vric.github.io/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1809.09409" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1809.09409</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Re-identification Using Quadruple Directional Deep Learning Features</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.05163" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.05163</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Coarse-to-fine: A RNN-based hierarchical attention model for vehicle re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ACCV 2018</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1812.04239" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1812.04239</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Re-Identification: an Efficient Baseline Using Triplet Embedding</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1901.01015" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1901.01015</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Two-Stream Siamese Neural Network for Vehicle Re-Identification by Using Non-Overlapping Cameras</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICIP 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1902.01496" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1902.01496</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Accepted for oral presentation at CVPR 2019 with review ratings of 2 strong accepts and 1 accept (work done during an internship at NVIDIA)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.09254" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.09254</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Re-identification in Aerial Imagery: Dataset and Approach</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Northwestern Polytechnical University</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1904.01400" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1904.01400</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attributes Guided Feature Learning for Vehicle Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1905.08997" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1905.08997</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A unified neural network for object detection, multiple object tracking and vehicle re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1907.03465" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1907.03465</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Part-Guided Attention Learning for Vehicle Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1909.06023" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1909.06023</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Re-identification with Viewpoint-aware Metric Learning</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>intro: Beihang University &amp; Tsinghua University &amp; Megvii Technology</li><li>keywords: Viewpoint-Aware Network (VANet)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.04104" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.04104</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICCV 2019</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2005.00673" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2005.00673</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NVlabs/PAMTRI" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NVlabs/PAMTRI</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Stripe-based and Attribute-aware Network: A Two-Branch Deep Model for Vehicle Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.05549" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.05549</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Background Segmentation for Vehicle Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1910.06613" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1910.06613</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Re-ID Collection</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/Vehicle_reID-Collection" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/Vehicle_reID-Collection</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>awesome-vehicle-re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/knwng/awesome-vehicle-re-identification" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/knwng/awesome-vehicle-re-identification</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DCDLearn: Multi-order Deep Cross-distance Learning for Vehicle Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2003.11315" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2003.11315</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Parsing-based View-aware Embedding Network for Vehicle Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 poster</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.05021" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.05021</a></li><li>github(official): <a target="_blank" rel="noopener noreferrer" href="https://github.com/silverbulletmdc/PVEN" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/silverbulletmdc/PVEN</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Domain Learning and Identity Mining for Vehicle Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Solution for AI City Challenge, CVPR2020 Workshop</li><li>intro: The 3rd Place Submission to AICity Challenge 2020 Track2</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.10547" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.10547</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/heshuting555/AICITY2020_DMT_VehicleReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/heshuting555/AICITY2020_DMT_VehicleReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VOC-ReID: Vehicle Re-identification based on Vehicle-Orientation-Camera</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2020 workshop, 2nd place solution for AICity2020 Challenge ReID track</li><li>intro: RuiyanAI (睿沿科技)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.09164" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.09164</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Xiangyu-CAS/AICity2020-VOC-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Xiangyu-CAS/AICity2020-VOC-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The 1st Place Submission to AICity Challenge 2020 re-id track (Baidu-UTS submission)</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2004.06305" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2004.06305</a></li><li>gtihub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/AICIty-reID-2020" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/AICIty-reID-2020</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/186905783" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/186905783</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Viewpoint-Aware Channel-Wise Attentive Network for Vehicle Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR Workshop 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.05810" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.05810</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Self-Supervised Visual Attention Learning for Vehicle Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.09221" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2010.09221</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>AttributeNet: Attribute Enhanced Vehicle Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2102.03898" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2102.03898</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Strong Baseline for Vehicle Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR Workshop 2021, 5th AI City Challenge</li><li>intro: Cybercore AI</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2104.10850" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2104.10850</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cybercore-co-ltd/track2_aicity_2021" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cybercore-co-ltd/track2_aicity_2021</a></li></ul><h1 id="deep-metric-learning" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#deep-metric-learning" color="auto.gray.8" aria-label="Deep Metric Learning permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Deep Metric Learning</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning for Person Re-Identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ICPR 2014</li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://www.cbsr.ia.ac.cn/users/zlei/papers/ICPR2014/Yi-ICPR-14.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://www.cbsr.ia.ac.cn/users/zlei/papers/ICPR2014/Yi-ICPR-14.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Deep Metric Learning for Practical Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1407.4979" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1407.4979</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Constrained Deep Metric Learning for Person Re-identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1511.07545" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1511.07545</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Embedding Deep Metric for Person Re-identication A Study Against Large Variations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2016</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.00137" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.00137</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: TuSimple</li><li>keywords: pedestrian re-identification</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1707.01220" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1707.01220</a></li></ul><h1 id="projects" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#projects" color="auto.gray.8" aria-label="Projects permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Projects</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Open-ReID: Open source person re-identification library in python</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Open-ReID is a lightweight library of person re-identification for research purpose. It aims to provide a uniform interface for different datasets, a full set of models and evaluation metrics, as well as examples to reproduce (near) state-of-the-art results.</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://cysu.github.io/open-reid/" class="Link-sc-1brdqhf-0 cKRjba">https://cysu.github.io/open-reid/</a></li><li>github(PyTorch): <a target="_blank" rel="noopener noreferrer" href="https://github.com/Cysu/open-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Cysu/open-reid</a></li><li>examples: <a target="_blank" rel="noopener noreferrer" href="https://cysu.github.io/open-reid/examples/training_id.html" class="Link-sc-1brdqhf-0 cKRjba">https://cysu.github.io/open-reid/examples/training_id.html</a></li><li>benchmarks: <a target="_blank" rel="noopener noreferrer" href="https://cysu.github.io/open-reid/examples/benchmarks.html" class="Link-sc-1brdqhf-0 cKRjba">https://cysu.github.io/open-reid/examples/benchmarks.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FastReID: A Pytorch Toolbox for Real-world Person Re-identification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SOTA ReID Methods and Toolbox</li><li>intro: JD AI Research</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2006.02631" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2006.02631</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/JDAI-CV/fast-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/JDAI-CV/fast-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>light-reid: a toolbox of light reid for fast feature extraction and search</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: a toolbox of light-reid learning for faster inference, speed both feature extraction and retrieval stages up to &gt;30x</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/wangguanan/light-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/wangguanan/light-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>caffe-PersonReID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Person Re-Identification: Multi-Task Deep CNN with Triplet Loss</li><li>gtihub: <a target="_blank" rel="noopener noreferrer" href="https://github.com/agjayant/caffe-Person-ReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/agjayant/caffe-Person-ReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person_reID_baseline_pytorch</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Pytorch implement of Person re-identification baseline</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/Person_reID_baseline_pytorch" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/Person_reID_baseline_pytorch</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>deep-person-reid</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Pytorch implementation of deep person re-identification models.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/KaiyangZhou/deep-person-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/KaiyangZhou/deep-person-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ReID_baseline</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Baseline model (with bottleneck) for person ReID (using softmax and triplet loss).</li><li>intro: 一个强力的ReID basemodel</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/L1aoXingyu/reid_baseline" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/L1aoXingyu/reid_baseline</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/40514536" class="Link-sc-1brdqhf-0 cKRjba">https://zhuanlan.zhihu.com/p/40514536</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>gluon-reid</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A code gallery for person re-identification with mxnet-gluon, and I will reproduce many STOA algorithm.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/xiaolai-sqlai/gluon-reid" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/xiaolai-sqlai/gluon-reid</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A PyTorch based strong baseline for person search</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DeepAlchemist/deep-person-search" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/DeepAlchemist/deep-person-search</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>OpenUnReID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PyTorch open-source toolbox for unsupervised or domain adaptive object re-ID</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/open-mmlab/OpenUnReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/open-mmlab/OpenUnReID</a></li></ul><h1 id="evaluation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#evaluation" color="auto.gray.8" aria-label="Evaluation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evaluation</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DukeMTMC-reID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The Person re-ID Evaluation Code for DukeMTMC-reID Dataset (Including Dataset Download)</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/DukeMTMC-reID_evaluation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/DukeMTMC-reID_evaluation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DukeMTMC-reID_baseline (Matlab)</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/DukeMTMC-reID_baseline" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/DukeMTMC-reID_baseline</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Code for IDE baseline on Market-1501</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/zhunzhong07/IDE-baseline-Market-1501" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zhunzhong07/IDE-baseline-Market-1501</a></p><h1 id="talks" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#talks" color="auto.gray.8" aria-label="Talks permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Talks</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>1st Workshop on Target Re-Identification and Multi-Target Multi-Camera Tracking</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://reid-mct.github.io/" class="Link-sc-1brdqhf-0 cKRjba">https://reid-mct.github.io/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Target Re-Identification and Multi-Target Multi-Camera Tracking</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/CVPR2017_workshops/CVPR2017_W17.py" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/CVPR2017_workshops/CVPR2017_W17.py</a></p><h1 id="resources" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resources" color="auto.gray.8" aria-label="Resources permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Resources</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Re-id Resources</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://wangzwhu.github.io/home/re_id_resources.html" class="Link-sc-1brdqhf-0 cKRjba">https://wangzwhu.github.io/home/re_id_resources.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-Identification - Papers With Code</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://paperswithcode.com/task/person-re-identification" class="Link-sc-1brdqhf-0 cKRjba">https://paperswithcode.com/task/person-re-identification</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome Person Re-Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/FDU-VTS/Awesome-Person-Re-Identification" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/FDU-VTS/Awesome-Person-Re-Identification</a></p><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/28/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>