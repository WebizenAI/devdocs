<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" name="twitter:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="twitter:description" content="Datasets who is the best at X ? blog:  http://rodrigob.github.io/are_we_there_yet/build/#datasets Computer Vision Datasets website:  http:/…"/><meta data-react-helmet="true" name="twitter:title" content="Computer Vision Datasets"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" property="article:section" content="None"/><meta data-react-helmet="true" property="article:author" content="http://examples.opengraphprotocol.us/profile.html"/><meta data-react-helmet="true" property="article:modified_time" content="2022-12-30T11:53:44.000Z"/><meta data-react-helmet="true" property="article:published_time" content="2015-09-24T00:00:00.000Z"/><meta data-react-helmet="true" property="og:description" content="Datasets who is the best at X ? blog:  http://rodrigob.github.io/are_we_there_yet/build/#datasets Computer Vision Datasets website:  http:/…"/><meta data-react-helmet="true" property="og:site_name"/><meta data-react-helmet="true" property="og:image:alt" content="This repo provides information about the webizen development objectives, considerations and related experimentation!"/><meta data-react-helmet="true" property="og:image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" property="og:url" content="https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" property="og:title" content="Computer Vision Datasets"/><meta data-react-helmet="true" name="image" content="https://devdocs.webizen.org/graph-visualisation.jpg"/><meta data-react-helmet="true" name="description" content="Datasets who is the best at X ? blog:  http://rodrigob.github.io/are_we_there_yet/build/#datasets Computer Vision Datasets website:  http:/…"/><meta name="generator" content="Gatsby 4.6.0"/><style data-href="/styles.d9e480e5c6375621c4fd.css" data-identity="gatsby-global-css">.tippy-box[data-animation=fade][data-state=hidden]{opacity:0}[data-tippy-root]{max-width:calc(100vw - 10px)}.tippy-box{background-color:#333;border-radius:4px;color:#fff;font-size:14px;line-height:1.4;outline:0;position:relative;transition-property:visibility,opacity,-webkit-transform;transition-property:transform,visibility,opacity;transition-property:transform,visibility,opacity,-webkit-transform;white-space:normal}.tippy-box[data-placement^=top]>.tippy-arrow{bottom:0}.tippy-box[data-placement^=top]>.tippy-arrow:before{border-top-color:initial;border-width:8px 8px 0;bottom:-7px;left:0;-webkit-transform-origin:center top;transform-origin:center top}.tippy-box[data-placement^=bottom]>.tippy-arrow{top:0}.tippy-box[data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:initial;border-width:0 8px 8px;left:0;top:-7px;-webkit-transform-origin:center bottom;transform-origin:center bottom}.tippy-box[data-placement^=left]>.tippy-arrow{right:0}.tippy-box[data-placement^=left]>.tippy-arrow:before{border-left-color:initial;border-width:8px 0 8px 8px;right:-7px;-webkit-transform-origin:center left;transform-origin:center left}.tippy-box[data-placement^=right]>.tippy-arrow{left:0}.tippy-box[data-placement^=right]>.tippy-arrow:before{border-right-color:initial;border-width:8px 8px 8px 0;left:-7px;-webkit-transform-origin:center right;transform-origin:center right}.tippy-box[data-inertia][data-state=visible]{transition-timing-function:cubic-bezier(.54,1.5,.38,1.11)}.tippy-arrow{color:#333;height:16px;width:16px}.tippy-arrow:before{border-color:transparent;border-style:solid;content:"";position:absolute}.tippy-content{padding:5px 9px;position:relative;z-index:1}.tippy-box[data-theme~=light]{background-color:#fff;box-shadow:0 0 20px 4px rgba(154,161,177,.15),0 4px 80px -8px rgba(36,40,47,.25),0 4px 4px -2px rgba(91,94,105,.15);color:#26323d}.tippy-box[data-theme~=light][data-placement^=top]>.tippy-arrow:before{border-top-color:#fff}.tippy-box[data-theme~=light][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#fff}.tippy-box[data-theme~=light][data-placement^=left]>.tippy-arrow:before{border-left-color:#fff}.tippy-box[data-theme~=light][data-placement^=right]>.tippy-arrow:before{border-right-color:#fff}.tippy-box[data-theme~=light]>.tippy-backdrop{background-color:#fff}.tippy-box[data-theme~=light]>.tippy-svg-arrow{fill:#fff}html{font-family:SF Pro SC,SF Pro Text,SF Pro Icons,PingFang SC,Helvetica Neue,Helvetica,Arial,sans-serif}body{word-wrap:break-word;-ms-hyphens:auto;-webkit-hyphens:auto;hyphens:auto;overflow-wrap:break-word;-ms-word-break:break-all;word-break:break-word}blockquote,body,dd,dt,fieldset,figure,h1,h2,h3,h4,h5,h6,hr,html,iframe,legend,p,pre,textarea{margin:0;padding:0}h1,h2,h3,h4,h5,h6{font-size:100%;font-weight:400}button,input,select{margin:0}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}img,video{height:auto;max-width:100%}iframe{border:0}table{border-collapse:collapse;border-spacing:0}td,th{padding:0}</style><style data-styled="" data-styled-version="5.3.5">.fnAJEh{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:14px;background-color:#005cc5;color:#ffffff;padding:16px;}/*!sc*/
.fnAJEh:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fnAJEh:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.czsBQU{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#ffffff;margin-right:16px;}/*!sc*/
.czsBQU:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.czsBQU:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kLOWMo{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#ffffff;}/*!sc*/
.kLOWMo:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kLOWMo:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.kEUvCO{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;color:inherit;margin-left:24px;}/*!sc*/
.kEUvCO:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.kEUvCO:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.fdzjHV{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;color:#24292e;display:block;}/*!sc*/
.fdzjHV:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.fdzjHV:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.HGjBQ{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;display:block;}/*!sc*/
.HGjBQ:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.HGjBQ:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.bQLMRL{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;font-size:16px;display:inline-block;padding-top:4px;padding-bottom:4px;color:#586069;font-weight:medium;}/*!sc*/
.bQLMRL:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.bQLMRL:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.bQLMRL{font-size:14px;}}/*!sc*/
.cKRjba{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cKRjba:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.cKRjba:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.ekSqTm{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;padding:8px;margin-left:-32px;color:#2f363d;}/*!sc*/
.ekSqTm:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.ekSqTm:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
.iLYDsn{color:#0366d6;-webkit-text-decoration:none;text-decoration:none;margin-bottom:4px;}/*!sc*/
.iLYDsn:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
.iLYDsn:is(button){display:inline-block;padding:0;font-size:inherit;white-space:nowrap;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-color:transparent;border:0;-webkit-appearance:none;-moz-appearance:none;appearance:none;}/*!sc*/
data-styled.g1[id="Link-sc-1brdqhf-0"]{content:"fnAJEh,czsBQU,kLOWMo,kEUvCO,fdzjHV,HGjBQ,bQLMRL,cKRjba,ekSqTm,iLYDsn,"}/*!sc*/
.EuMgV{z-index:20;width:auto;height:auto;-webkit-clip:auto;clip:auto;position:absolute;overflow:hidden;}/*!sc*/
.EuMgV:not(:focus){-webkit-clip:rect(1px,1px,1px,1px);clip:rect(1px,1px,1px,1px);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;width:1px;margin:-1px;padding:0;}/*!sc*/
data-styled.g2[id="skip-link__SkipLink-sc-1z0kjxc-0"]{content:"EuMgV,"}/*!sc*/
.fbaWCe{display:none;margin-left:8px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.fbaWCe{display:inline;}}/*!sc*/
.bLwTGz{font-weight:600;display:inline-block;margin-bottom:4px;}/*!sc*/
.cQAYyE{font-weight:600;}/*!sc*/
.gHwtLv{font-size:14px;color:#444d56;margin-top:4px;}/*!sc*/
data-styled.g4[id="Text-sc-1s3uzov-0"]{content:"fbaWCe,bLwTGz,cQAYyE,gHwtLv,"}/*!sc*/
.ifkhtm{background-color:#ffffff;color:#24292e;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.gSrgIV{top:0;z-index:1;position:-webkit-sticky;position:sticky;}/*!sc*/
.iTlzRc{padding-left:16px;padding-right:16px;background-color:#24292e;color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:66px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.iTlzRc{padding-left:24px;padding-right:24px;}}/*!sc*/
.kCrfOd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gELiHA{margin-left:24px;display:none;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gELiHA{display:block;}}/*!sc*/
.gYHnkh{position:relative;}/*!sc*/
.dMFMzl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhCmHN{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.jhCmHN{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}/*!sc*/
.elXfHl{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gjFLbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.gjFLbZ{display:none;}}/*!sc*/
.gucKKf{color:rgba(255,255,255,0.7);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;background-color:none;cursor:pointer;}/*!sc*/
.gucKKf:hover{fill:rgba(255,255,255,0.7);color:rgba(255,255,255,0.7);}/*!sc*/
.gucKKf svg{fill:rgba(255,255,255,0.7);}/*!sc*/
.fBMuRw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}/*!sc*/
.bQaVuO{color:#2f363d;background-color:#fafbfc;display:none;height:calc(100vh - 66px);min-width:260px;max-width:360px;position:-webkit-sticky;position:sticky;top:66px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){}/*!sc*/
@media screen and (min-width:1012px){.bQaVuO{display:block;}}/*!sc*/
.eeDmz{height:100%;border-style:solid;border-color:#e1e4e8;border-width:0;border-right-width:1px;border-radius:0;}/*!sc*/
.kSoTbZ{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}/*!sc*/
.nElVQ{padding:24px;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-top-width:1px;}/*!sc*/
.iXtyim{margin-left:0;padding-top:4px;padding-bottom:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.vaHQm{margin-bottom:4px;margin-top:4px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.hIjKHD{color:#586069;font-weight:400;display:block;}/*!sc*/
.icYakO{padding-left:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}/*!sc*/
.jLseWZ{margin-left:16px;padding-top:0;padding-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;border-style:solid;border-color:#e1e4e8;border-width:0;border-radius:0;border-bottom-width:0;}/*!sc*/
.kRSqJi{margin-bottom:0;margin-top:8px;font-size:14px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
.klfmeZ{max-width:1440px;-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
.TZbDV{padding:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;}/*!sc*/
@media screen and (min-width:544px){.TZbDV{padding:32px;}}/*!sc*/
@media screen and (min-width:768px){.TZbDV{padding:40px;}}/*!sc*/
@media screen and (min-width:1012px){.TZbDV{padding:48px;}}/*!sc*/
.DPDMP{display:none;max-height:calc(100vh - 66px - 24px);position:-webkit-sticky;position:sticky;top:90px;width:220px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin-left:40px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.DPDMP{display:block;}}/*!sc*/
.gUNLMu{margin:0;padding:0;}/*!sc*/
.bzTeHX{padding-left:0;}/*!sc*/
.bnaGYs{padding-left:16px;}/*!sc*/
.meQBK{width:100%;}/*!sc*/
.fkoaiG{margin-bottom:24px;}/*!sc*/
.biGwYR{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.jYYExC{margin-bottom:32px;background-color:#f6f8fa;display:block;border-width:1px;border-style:solid;border-color:#e1e4e8;border-radius:6px;}/*!sc*/
@media screen and (min-width:544px){}/*!sc*/
@media screen and (min-width:768px){.jYYExC{display:none;}}/*!sc*/
.hgiZBa{padding:16px;}/*!sc*/
.hnQOQh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/
.gEqaxf{padding:16px;border-top:1px solid;border-color:border.gray;}/*!sc*/
.ksEcN{margin-top:64px;padding-top:32px;padding-bottom:32px;border-style:solid;border-color:#e1e4e8;border-width:0;border-top-width:1px;border-radius:0;}/*!sc*/
.jsSpbO{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g5[id="Box-nv15kw-0"]{content:"ifkhtm,gSrgIV,iTlzRc,kCrfOd,gELiHA,gYHnkh,dMFMzl,jhCmHN,elXfHl,gjFLbZ,gucKKf,fBMuRw,bQaVuO,eeDmz,kSoTbZ,nElVQ,iXtyim,vaHQm,hIjKHD,icYakO,jLseWZ,kRSqJi,klfmeZ,TZbDV,DPDMP,gUNLMu,bzTeHX,bnaGYs,meQBK,fkoaiG,biGwYR,jYYExC,hgiZBa,hnQOQh,gEqaxf,ksEcN,jsSpbO,"}/*!sc*/
.cjGjQg{position:relative;display:inline-block;padding:6px 16px;font-family:inherit;font-weight:600;line-height:20px;white-space:nowrap;vertical-align:middle;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;-webkit-text-decoration:none;text-decoration:none;text-align:center;font-size:14px;}/*!sc*/
.cjGjQg:hover{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.cjGjQg:focus{outline:none;}/*!sc*/
.cjGjQg:disabled{cursor:default;}/*!sc*/
.cjGjQg:disabled svg{opacity:0.6;}/*!sc*/
data-styled.g6[id="ButtonBase-sc-181ps9o-0"]{content:"cjGjQg,"}/*!sc*/
.fafffn{margin-right:8px;}/*!sc*/
data-styled.g8[id="StyledOcticon-uhnt7w-0"]{content:"bhRGQB,fafffn,"}/*!sc*/
.fTkTnC{font-weight:600;font-size:32px;margin:0;font-size:12px;font-weight:500;color:#959da5;margin-bottom:4px;text-transform:uppercase;font-family:Content-font,Roboto,sans-serif;}/*!sc*/
.glhHOU{font-weight:600;font-size:32px;margin:0;margin-right:8px;}/*!sc*/
.ffNRvO{font-weight:600;font-size:32px;margin:0;}/*!sc*/
data-styled.g12[id="Heading-sc-1cjoo9h-0"]{content:"fTkTnC,glhHOU,ffNRvO,"}/*!sc*/
.ifFLoZ{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.ifFLoZ:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.ifFLoZ:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.ifFLoZ:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.ifFLoZ:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);}/*!sc*/
.fKTxJr:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.fKTxJr:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.fKTxJr:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.fKTxJr:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt{color:#0366d6;border:1px solid rgba(27,31,35,0.15);background-color:#fafbfc;box-shadow:0 1px 0 rgba(27,31,35,0.04);margin-left:16px;}/*!sc*/
.cXFtEt:hover{color:#ffffff;background-color:#0366d6;border-color:rgba(27,31,35,0.15);box-shadow:0 1px 0 rgba(27,31,35,0.1);}/*!sc*/
.cXFtEt:focus{border-color:rgba(27,31,35,0.15);box-shadow:0 0 0 3px rgba(0,92,197,0.4);}/*!sc*/
.cXFtEt:active{color:#ffffff;background-color:hsla(212,97%,40%,1);box-shadow:inset 0 1px 0 rgba(5,38,76,0.2);border-color:rgba(27,31,35,0.15);}/*!sc*/
.cXFtEt:disabled{color:rgba(3,102,214,0.5);background-color:#fafbfc;border-color:rgba(27,31,35,0.15);}/*!sc*/
data-styled.g13[id="ButtonOutline-sc-15gta9l-0"]{content:"ifFLoZ,fKTxJr,cXFtEt,"}/*!sc*/
.iEGqHu{color:rgba(255,255,255,0.7);background-color:transparent;border:1px solid #444d56;box-shadow:none;}/*!sc*/
data-styled.g14[id="dark-button__DarkButton-sc-bvvmfe-0"]{content:"iEGqHu,"}/*!sc*/
.ljCWQd{border:0;font-size:inherit;font-family:inherit;background-color:transparent;-webkit-appearance:none;color:inherit;width:100%;}/*!sc*/
.ljCWQd:focus{outline:0;}/*!sc*/
data-styled.g15[id="TextInput__Input-sc-1apmpmt-0"]{content:"ljCWQd,"}/*!sc*/
.dHfzvf{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;min-height:34px;font-size:14px;line-height:20px;color:#24292e;vertical-align:middle;background-repeat:no-repeat;background-position:right 8px center;border:1px solid #e1e4e8;border-radius:6px;outline:none;box-shadow:inset 0 1px 0 rgba(225,228,232,0.2);padding:6px 12px;width:240px;}/*!sc*/
.dHfzvf .TextInput-icon{-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;color:#959da5;margin:0 8px;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}/*!sc*/
.dHfzvf:focus-within{border-color:#0366d6;box-shadow:0 0 0 3px rgba(3,102,214,0.3);}/*!sc*/
@media (min-width:768px){.dHfzvf{font-size:14px;}}/*!sc*/
data-styled.g16[id="TextInput__Wrapper-sc-1apmpmt-1"]{content:"dHfzvf,"}/*!sc*/
.khRwtY{font-size:16px !important;color:rgba(255,255,255,0.7);background-color:rgba(255,255,255,0.07);border:1px solid transparent;box-shadow:none;}/*!sc*/
.khRwtY:focus{border:1px solid #444d56 outline:none;box-shadow:none;}/*!sc*/
data-styled.g17[id="dark-text-input__DarkTextInput-sc-1s2iwzn-0"]{content:"khRwtY,"}/*!sc*/
.bqVpte.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g19[id="nav-items__NavLink-sc-tqz5wl-0"]{content:"bqVpte,"}/*!sc*/
.kEKZhO.active{font-weight:600;color:#2f363d;}/*!sc*/
data-styled.g20[id="nav-items__NavBox-sc-tqz5wl-1"]{content:"kEKZhO,"}/*!sc*/
.gCPbFb{margin-top:24px;margin-bottom:16px;-webkit-scroll-margin-top:90px;-moz-scroll-margin-top:90px;-ms-scroll-margin-top:90px;scroll-margin-top:90px;}/*!sc*/
.gCPbFb .octicon-link{visibility:hidden;}/*!sc*/
.gCPbFb:hover .octicon-link,.gCPbFb:focus-within .octicon-link{visibility:visible;}/*!sc*/
data-styled.g22[id="heading__StyledHeading-sc-1fu06k9-0"]{content:"gCPbFb,"}/*!sc*/
.fGjcEF{margin-top:0;padding-bottom:4px;font-size:32px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g23[id="heading__StyledH1-sc-1fu06k9-1"]{content:"fGjcEF,"}/*!sc*/
.fvbkiW{padding-bottom:4px;font-size:24px;border-bottom:1px solid #e1e4e8;}/*!sc*/
data-styled.g24[id="heading__StyledH2-sc-1fu06k9-2"]{content:"fvbkiW,"}/*!sc*/
.elBfYx{max-width:100%;box-sizing:content-box;background-color:#ffffff;}/*!sc*/
data-styled.g30[id="image__Image-sc-1r30dtv-0"]{content:"elBfYx,"}/*!sc*/
.dFVIUa{padding-left:2em;margin-bottom:4px;}/*!sc*/
.dFVIUa ul,.dFVIUa ol{margin-top:0;margin-bottom:0;}/*!sc*/
.dFVIUa li{line-height:1.6;}/*!sc*/
.dFVIUa li > p{margin-top:16px;}/*!sc*/
.dFVIUa li + li{margin-top:8px;}/*!sc*/
data-styled.g32[id="list__List-sc-s5kxp2-0"]{content:"dFVIUa,"}/*!sc*/
.iNQqSl{margin:0 0 16px;}/*!sc*/
data-styled.g34[id="paragraph__Paragraph-sc-17pab92-0"]{content:"iNQqSl,"}/*!sc*/
.eooEiq{display:block;width:100%;margin:0 0 16px;overflow:auto;}/*!sc*/
.eooEiq th{font-weight:600;}/*!sc*/
.eooEiq th,.eooEiq td{padding:8px 16px;border:1px solid #e1e4e8;}/*!sc*/
.eooEiq tr{background-color:#ffffff;border-top:1px solid #e1e4e8;}/*!sc*/
.eooEiq tr:nth-child(2n){background-color:#f6f8fa;}/*!sc*/
.eooEiq img{background-color:transparent;}/*!sc*/
data-styled.g35[id="table__Table-sc-ixm5yk-0"]{content:"eooEiq,"}/*!sc*/
.drDDht{z-index:0;}/*!sc*/
data-styled.g37[id="layout___StyledBox-sc-7a5ttt-0"]{content:"drDDht,"}/*!sc*/
.flyUPp{list-style:none;}/*!sc*/
data-styled.g39[id="table-of-contents___StyledBox-sc-1jtv948-0"]{content:"flyUPp,"}/*!sc*/
.bPkrfP{grid-area:table-of-contents;overflow:auto;}/*!sc*/
data-styled.g40[id="post-page___StyledBox-sc-17hbw1s-0"]{content:"bPkrfP,"}/*!sc*/
</style><title data-react-helmet="true">Computer Vision Datasets - Webizen Development Related Documentation.</title><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><link rel="icon" href="/favicon-32x32.png?v=202c3b6fa23c481f8badd00dc2119591" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=202c3b6fa23c481f8badd00dc2119591"/><link rel="sitemap" type="application/xml" href="/sitemap/sitemap-index.xml"/><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><link as="script" rel="preload" href="/webpack-runtime-1fe3daf7582b39746d36.js"/><link as="script" rel="preload" href="/framework-6c63f85700e5678d2c2a.js"/><link as="script" rel="preload" href="/f0e45107-3309acb69b4ccd30ce0c.js"/><link as="script" rel="preload" href="/0e226fb0-1cb0709e5ed968a9c435.js"/><link as="script" rel="preload" href="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js"/><link as="script" rel="preload" href="/app-f28009dab402ccf9360c.js"/><link as="script" rel="preload" href="/commons-c89ede6cb9a530ac5a37.js"/><link as="script" rel="preload" href="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"/><link as="fetch" rel="preload" href="/page-data/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2230547434.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2320115945.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3495835395.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/451533639.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><a class="Link-sc-1brdqhf-0 fnAJEh skip-link__SkipLink-sc-1z0kjxc-0 EuMgV" color="auto.white" href="#skip-nav" font-size="1">Skip to content</a><div display="flex" color="text.primary" class="Box-nv15kw-0 ifkhtm"><div class="Box-nv15kw-0 gSrgIV"><div display="flex" height="66" color="header.text" class="Box-nv15kw-0 iTlzRc"><div display="flex" class="Box-nv15kw-0 kCrfOd"><a color="header.logo" mr="3" class="Link-sc-1brdqhf-0 czsBQU" href="/"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 bhRGQB" viewBox="0 0 16 16" width="32" height="32" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a><a color="header.logo" font-family="mono" class="Link-sc-1brdqhf-0 kLOWMo" href="/">Wiki</a><div display="none,,,block" class="Box-nv15kw-0 gELiHA"><div role="combobox" aria-expanded="false" aria-haspopup="listbox" aria-labelledby="downshift-search-label" class="Box-nv15kw-0 gYHnkh"><span class="TextInput__Wrapper-sc-1apmpmt-1 dHfzvf dark-text-input__DarkTextInput-sc-1s2iwzn-0 khRwtY TextInput-wrapper" width="240"><input type="text" aria-autocomplete="list" aria-labelledby="downshift-search-label" autoComplete="off" value="" id="downshift-search-input" placeholder="Search Wiki" class="TextInput__Input-sc-1apmpmt-0 ljCWQd"/></span></div></div></div><div display="flex" class="Box-nv15kw-0 dMFMzl"><div display="none,,,flex" class="Box-nv15kw-0 jhCmHN"><div display="flex" color="header.text" class="Box-nv15kw-0 elXfHl"><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://github.com/webizenai/devdocs/" class="Link-sc-1brdqhf-0 kEUvCO">Github</a><a display="block" color="inherit" target="_blank" rel="noopener noreferrer" href="https://twitter.com/webcivics" class="Link-sc-1brdqhf-0 kEUvCO">Twitter</a></div><button aria-label="Theme" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-sun" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z"></path></svg></button></div><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Search" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg fKTxJr iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-search" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z"></path></svg></button></div><button aria-label="Show Graph Visualisation" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg cXFtEt iEGqHu"><div title="Show Graph Visualisation" aria-label="Show Graph Visualisation" color="header.text" display="flex" class="Box-nv15kw-0 gucKKf"><svg t="1607341341241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path d="M512 512m-125.866667 0a125.866667 125.866667 0 1 0 251.733334 0 125.866667 125.866667 0 1 0-251.733334 0Z"></path><path d="M512 251.733333m-72.533333 0a72.533333 72.533333 0 1 0 145.066666 0 72.533333 72.533333 0 1 0-145.066666 0Z"></path><path d="M614.4 238.933333c0 4.266667 2.133333 8.533333 2.133333 12.8 0 19.2-4.266667 36.266667-12.8 51.2 81.066667 36.266667 138.666667 117.333333 138.666667 211.2C742.4 640 640 744.533333 512 744.533333s-230.4-106.666667-230.4-232.533333c0-93.866667 57.6-174.933333 138.666667-211.2-8.533333-14.933333-12.8-32-12.8-51.2 0-4.266667 0-8.533333 2.133333-12.8-110.933333 42.666667-189.866667 147.2-189.866667 273.066667 0 160 130.133333 292.266667 292.266667 292.266666S804.266667 672 804.266667 512c0-123.733333-78.933333-230.4-189.866667-273.066667z"></path><path d="M168.533333 785.066667m-72.533333 0a72.533333 72.533333 0 1 0 145.066667 0 72.533333 72.533333 0 1 0-145.066667 0Z"></path><path d="M896 712.533333m-61.866667 0a61.866667 61.866667 0 1 0 123.733334 0 61.866667 61.866667 0 1 0-123.733334 0Z"></path><path d="M825.6 772.266667c-74.666667 89.6-187.733333 147.2-313.6 147.2-93.866667 0-181.333333-32-249.6-87.466667-10.666667 19.2-25.6 34.133333-44.8 44.8C298.666667 942.933333 401.066667 981.333333 512 981.333333c149.333333 0 281.6-70.4 366.933333-177.066666-21.333333-4.266667-40.533333-17.066667-53.333333-32zM142.933333 684.8c-25.6-53.333333-38.4-110.933333-38.4-172.8C104.533333 288 288 104.533333 512 104.533333S919.466667 288 919.466667 512c0 36.266667-6.4 72.533333-14.933334 106.666667 23.466667 2.133333 42.666667 10.666667 57.6 25.6 12.8-42.666667 19.2-87.466667 19.2-132.266667 0-258.133333-211.2-469.333333-469.333333-469.333333S42.666667 253.866667 42.666667 512c0 74.666667 17.066667 142.933333 46.933333 204.8 14.933333-14.933333 32-27.733333 53.333333-32z"></path></svg><span display="none,,,inline" class="Text-sc-1s3uzov-0 fbaWCe">Show Graph Visualisation</span></div></button><div display="flex,,,none" class="Box-nv15kw-0 gjFLbZ"><button aria-label="Menu" aria-expanded="false" class="ButtonBase-sc-181ps9o-0 ButtonOutline-sc-15gta9l-0 dark-button__DarkButton-sc-bvvmfe-0 cjGjQg ifFLoZ iEGqHu"><svg aria-hidden="true" role="img" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path></svg></button></div></div></div></div><div display="flex" class="Box-nv15kw-0 layout___StyledBox-sc-7a5ttt-0 fBMuRw drDDht"><div display="none,,,block" height="calc(100vh - 66px)" color="auto.gray.8" class="Box-nv15kw-0 bQaVuO"><div height="100%" style="overflow:auto" class="Box-nv15kw-0 eeDmz"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><div class="Box-nv15kw-0 nElVQ"><div display="flex" class="Box-nv15kw-0 kSoTbZ"><h2 color="text.disabled" font-size="12px" font-weight="500" class="Heading-sc-1cjoo9h-0 fTkTnC">Categories</h2><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Commercial</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Services</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Core Technologies</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Database Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Host Service Requirements</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/HyperMedia Library/">HyperMedia Library</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">ICT Stack</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Implementation V1</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Non-HTTP(s) Protocols</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Old-Work-Archives</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">2018-Webizen-Net-Au</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Link_library_links</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/about/">about</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/">An Overview</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/">Resource Library</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">awesomeLists</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/">Handong1587</a><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">_Posts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_science</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Computer_vision</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-up" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M3.22 9.78a.75.75 0 010-1.06l4.25-4.25a.75.75 0 011.06 0l4.25 4.25a.75.75 0 01-1.06 1.06L8 6.06 4.28 9.78a.75.75 0 01-1.06 0z"></path></svg></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a aria-current="page" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte active" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/">Computer Vision Datasets</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/">Computer Vision Resources</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/">Features</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/">Recognition, Detection, Segmentation and Tracking</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/">Use FFmpeg to Capture I Frames of Video</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/">Working on OpenCV</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Deep_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Leisure</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Machine_learning</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Mathematics</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Programming_study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Reading_and_thoughts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Study</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_linux</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_mac</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Working_on_windows</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Drafts</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div></div></div></div><div display="flex" class="Box-nv15kw-0 jLseWZ"><div display="flex" font-size="1" class="Box-nv15kw-0 kRSqJi"><a class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 HGjBQ bqVpte" display="block" sx="[object Object]" href="/old-work-archives/2018 - Web Civics BizPlan/">EXECUTIVE SUMMARY</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><div color="text.secondary" font-weight="400" class="Box-nv15kw-0 nav-items__NavBox-sc-tqz5wl-1 hIjKHD kEKZhO" display="block">Webizen 2.0</div><div display="flex" class="Box-nv15kw-0 icYakO"><svg aria-hidden="true" role="img" class="octicon octicon-chevron-down" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M12.78 6.22a.75.75 0 010 1.06l-4.25 4.25a.75.75 0 01-1.06 0L3.22 7.28a.75.75 0 011.06-1.06L8 9.94l3.72-3.72a.75.75 0 011.06 0z"></path></svg></div></div></div><div display="flex" class="Box-nv15kw-0 iXtyim"><div display="flex" font-size="1" class="Box-nv15kw-0 vaHQm"><a color="text.primary" class="Link-sc-1brdqhf-0 nav-items__NavLink-sc-tqz5wl-0 fdzjHV bqVpte" display="block" sx="[object Object]" href="/">Webizen V1 Project Documentation</a><div display="flex" class="Box-nv15kw-0 icYakO"></div></div></div></div></div></div></div></div><main class="Box-nv15kw-0 klfmeZ"><div id="skip-nav" display="flex" width="100%" class="Box-nv15kw-0 TZbDV"><div display="none,,block" class="Box-nv15kw-0 post-page___StyledBox-sc-17hbw1s-0 DPDMP bPkrfP"><span display="inline-block" font-weight="bold" class="Text-sc-1s3uzov-0 bLwTGz">On this page</span><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#classification--recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Classification / Recognition</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#face" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#vehicle" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#scene-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Scene Recognition</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mnist" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MNIST</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#food" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Food</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#face-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Detection</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#pedestrian-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pedestrian Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#full-body-annotations" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Full-Body Annotations</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#vehicle-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#vehicle-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle Re-ID</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#logo-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Logo Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#head-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Head Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#detection-from-video" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Detection From Video</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Segmentation</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mapillary-vistas-dataset" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Mapillary Vistas Dataset</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pascal-voc" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">PASCAL VOC</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#augmented-pascal-voc" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Augmented Pascal VOC</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#supervisely-person" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Supervisely Person</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#microsoft-coco" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Microsoft COCO</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#the-oxford-iiit-pet-dataset" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">The Oxford-IIIT Pet Dataset</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#coco-stuff" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">COCO-Stuff</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#scene-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Scene Parsing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#imagenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ImageNet</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#captioning--description" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Captioning / Description</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#scene" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Scene</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#autonomous-driving" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Autonomous Driving</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#ocr" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">OCR</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#retrieval" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Retrieval</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#person-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Person Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#fashion" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Fashion</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#attribute-datasets" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Attribute Datasets</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#pedestrian-attribute-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pedestrian Attribute Recognition</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tracking" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tracking</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#color-classification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Color Classification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#license-plate-detection-and-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">License Plate Detection and Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#face-anti-spoofing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Anti-Spoofing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tools" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tools</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#artist" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Artist</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li></ul></div><div width="100%" class="Box-nv15kw-0 meQBK"><div class="Box-nv15kw-0 fkoaiG"><div display="flex" class="Box-nv15kw-0 biGwYR"><h1 class="Heading-sc-1cjoo9h-0 glhHOU">Computer Vision Datasets</h1></div></div><div display="block,,none" class="Box-nv15kw-0 jYYExC"><div class="Box-nv15kw-0 hgiZBa"><div display="flex" class="Box-nv15kw-0 hnQOQh"><span font-weight="bold" class="Text-sc-1s3uzov-0 cQAYyE">On this page</span></div></div><div class="Box-nv15kw-0 gEqaxf"><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#classification--recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Classification / Recognition</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#face" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#vehicle" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#scene-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Scene Recognition</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mnist" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">MNIST</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#food" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Food</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#face-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Detection</a></li><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#pedestrian-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pedestrian Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#full-body-annotations" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Full-Body Annotations</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#vehicle-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#vehicle-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Vehicle Re-ID</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#logo-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Logo Detection</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#head-detection" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Head Detection</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#detection-from-video" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Detection From Video</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#segmentation" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Segmentation</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#mapillary-vistas-dataset" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Mapillary Vistas Dataset</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#pascal-voc" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">PASCAL VOC</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#augmented-pascal-voc" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Augmented Pascal VOC</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#supervisely-person" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Supervisely Person</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#microsoft-coco" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Microsoft COCO</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#the-oxford-iiit-pet-dataset" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">The Oxford-IIIT Pet Dataset</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#coco-stuff" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">COCO-Stuff</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#scene-parsing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Scene Parsing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#imagenet" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">ImageNet</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#captioning--description" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Captioning / Description</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#video" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Video</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#scene" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Scene</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#autonomous-driving" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Autonomous Driving</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#ocr" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">OCR</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#retrieval" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Retrieval</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#person-re-id" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Person Re-ID</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#fashion" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Fashion</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#attribute-datasets" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Attribute Datasets</a><ul class="Box-nv15kw-0 table-of-contents___StyledBox-sc-1jtv948-0 gUNLMu flyUPp"><li class="Box-nv15kw-0 bnaGYs"><a display="inline-block" href="#pedestrian-attribute-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Pedestrian Attribute Recognition</a></li></ul></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tracking" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tracking</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#color-classification" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Color Classification</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#license-plate-detection-and-recognition" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">License Plate Detection and Recognition</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#face-anti-spoofing" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Face Anti-Spoofing</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#tools" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Tools</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#artist" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Artist</a></li><li class="Box-nv15kw-0 bzTeHX"><a display="inline-block" href="#resources" font-size="2,,1" color="auto.gray.6" class="Link-sc-1brdqhf-0 bQLMRL">Resources</a></li></ul></div></div><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Datasets who is the best at X ?</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="http://rodrigob.github.io/are_we_there_yet/build/#datasets" class="Link-sc-1brdqhf-0 cKRjba">http://rodrigob.github.io/are_we_there_yet/build/#datasets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Computer Vision Datasets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>website: <a target="_blank" rel="noopener noreferrer" href="http://clickdamage.com/sourcecode/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://clickdamage.com/sourcecode/index.html</a></li><li>code: <a target="_blank" rel="noopener noreferrer" href="http://clickdamage.com/sourcecode/cv_datasets.php" class="Link-sc-1brdqhf-0 cKRjba">http://clickdamage.com/sourcecode/cv_datasets.php</a></li><li>mirror: <a target="_blank" rel="noopener noreferrer" href="http://pan.baidu.com/s/1pJmqD4n" class="Link-sc-1brdqhf-0 cKRjba">http://pan.baidu.com/s/1pJmqD4n</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Introducing the Open Images Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://research.googleblog.com/2016/09/introducing-open-images-dataset.html" class="Link-sc-1brdqhf-0 cKRjba">https://research.googleblog.com/2016/09/introducing-open-images-dataset.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/openimages/dataset" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/openimages/dataset</a></li><li>Academic Torrents: <a target="_blank" rel="noopener noreferrer" href="http://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b" class="Link-sc-1brdqhf-0 cKRjba">http://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A parallel download util for Google&#x27;s open image dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ejlb/google-open-image-download" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ejlb/google-open-image-download</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Image &amp; Vision Group - Datasets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Image &amp; Vision , Clothing &amp; Fashion, Computer Graphics, Video Sequences</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://caiivg.weebly.com/dataset.html" class="Link-sc-1brdqhf-0 cKRjba">http://caiivg.weebly.com/dataset.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Huizhong Chen - Datasets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google I/O Dataset, Names 100 Dataset, Clothing Attributes Dataset,
Stanford Mobile Visual Search Dataset, CNN 2-Hours Videos Dataset</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://huizhongchen.github.io/datasets.html#clothingattributedataset" class="Link-sc-1brdqhf-0 cKRjba">http://huizhongchen.github.io/datasets.html#clothingattributedataset</a></li></ul><h1 id="classification--recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#classification--recognition" color="auto.gray.8" aria-label="Classification / Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Classification / Recognition</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Large-Scale Car Dataset for Fine-Grained Categorization and Verification</strong></p><img src="http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/illustration.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1506.08959" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1506.08959</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CIFAR-10 / CIFAR100</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.
There are 50000 training images and 10000 test images.</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.toronto.edu/~kriz/cifar.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.toronto.edu/~kriz/cifar.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Tencent ML-Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Largest multi-label image database; ResNet-101 model; 80.73% top-1 acc on ImageNet</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Tencent/tencent-ml-images" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Tencent/tencent-ml-images</a></li></ul><h2 id="face" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#face" color="auto.gray.8" aria-label="Face permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Face</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The MegaFace Benchmark: 1 Million Faces for Recognition at Scale</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://megaface.cs.washington.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://megaface.cs.washington.edu/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1512.00596" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1512.00596</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1607.08221" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1607.08221</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MSR Image Recognition Challenge (IRC)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/" class="Link-sc-1brdqhf-0 cKRjba">https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UMDFaces: An Annotated Face Dataset for Training Deep Networks</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1611.01484" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1611.01484</a></li></ul><h2 id="vehicle" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#vehicle" color="auto.gray.8" aria-label="Vehicle permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Vehicle</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Comprehensive Cars (CompCars) dataset</strong></p><img src="http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/illustration.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BoxCars: Improving Fine-Grained Recognition of Vehicles Using 3-D Bounding Boxes in Traffic Surveillance <!-- -->[IEEE T-ITS]</strong></p><img src="https://medusa.fit.vutbr.cz/traffic/wp-content/uploads/2017/03/boxcars_bb_estimation_pipeline.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/" class="Link-sc-1brdqhf-0 cKRjba">https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Make and Model Recognition Dataset (VMMRdb)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: containing 9,170 classes consisting of 291,752 images, covering models manufactured between 1950 to 2016</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vmmrdb.cecsresearch.org/" class="Link-sc-1brdqhf-0 cKRjba">http://vmmrdb.cecsresearch.org/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Cars Dataset</strong></p><img src="http://ai.stanford.edu/~jkrause/cars/class_montage_flop.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: contains 16,185 images of 196 classes of cars.</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://ai.stanford.edu/~jkrause/cars/car_dataset.html" class="Link-sc-1brdqhf-0 cKRjba">http://ai.stanford.edu/~jkrause/cars/car_dataset.html</a></li></ul><h2 id="scene-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#scene-recognition" color="auto.gray.8" aria-label="Scene Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scene Recognition</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Places: An Image Database for Deep Scene Understanding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://places.csail.mit.edu/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://places.csail.mit.edu/index.html</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1610.02055" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1610.02055</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Places2</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Places2 contains more than 10 million images comprising 400+ unique scene categories</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://places2.csail.mit.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://places2.csail.mit.edu/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Places365-CNNs for Scene Classification</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/CSAILVision/places365" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/CSAILVision/places365</a></li></ul><h2 id="mnist" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#mnist" color="auto.gray.8" aria-label="MNIST permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>MNIST</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EMNIST: an extension of MNIST to handwritten letters</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.05373" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.05373</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Fashion-MNIST</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1708.07747" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1708.07747</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/zalandoresearch/fashion-mnist" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/zalandoresearch/fashion-mnist</a></li><li>benchmark: <a target="_blank" rel="noopener noreferrer" href="http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/" class="Link-sc-1brdqhf-0 cKRjba">http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/</a></li></ul><h1 id="food" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#food" color="auto.gray.8" aria-label="Food permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Food</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>3 Million Instacart Orders, Open Sourced</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2" class="Link-sc-1brdqhf-0 cKRjba">https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2</a></p><h1 id="detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#detection" color="auto.gray.8" aria-label="Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: YouTube-BoundingBoxes (YT-BB)</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://research.google.com/youtubebb/" class="Link-sc-1brdqhf-0 cKRjba">https://research.google.com/youtubebb/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.00824" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.00824</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1804.00525" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1804.00525</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Exclusively Dark (ExDark) Image Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Exclusively Dark (ExDARK) dataset which to the best of our knowledge,
is the largest collection of low-light images taken in very low-light environments to twilight (i.e 10 different conditions)
to-date with image class and object level annotations.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/cs-chan/Exclusively-Dark-Image-Dataset" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/cs-chan/Exclusively-Dark-Image-Dataset</a></li></ul><h2 id="face-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#face-detection" color="auto.gray.8" aria-label="Face Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Face Detection</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FDDB: Face Detection Data Set and Benchmark</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vis-www.cs.umass.edu/fddb/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://vis-www.cs.umass.edu/fddb/index.html</a></li><li>results: <a target="_blank" rel="noopener noreferrer" href="http://vis-www.cs.umass.edu/fddb/results.html" class="Link-sc-1brdqhf-0 cKRjba">http://vis-www.cs.umass.edu/fddb/results.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>WIDER FACE: A Face Detection Benchmark</strong></p><img src="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/intro.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1511.06523" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1511.06523</a></li></ul><h2 id="pedestrian-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pedestrian-detection" color="auto.gray.8" aria-label="Pedestrian Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pedestrian Detection</h2><img src="https://sshao0516.github.io/CrowdHuman/images/fig1.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Caltech Pedestrian Detection Benchmark</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" class="Link-sc-1brdqhf-0 cKRjba">http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Caltech Pedestrian Dataset Converter</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mitmul/caltech-pedestrian-dataset-converter" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/mitmul/caltech-pedestrian-dataset-converter</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CityPersons: A Diverse Dataset for Pedestrian Detection</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1702.05693" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1702.05693</a></li><li>bitbucket: <a target="_blank" rel="noopener noreferrer" href="https://bitbucket.org/shanshanzhang/citypersons" class="Link-sc-1brdqhf-0 cKRjba">https://bitbucket.org/shanshanzhang/citypersons</a></li><li>supplemental: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CrowdHuman: A Benchmark for Detecting Human in a Crowd</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CrowdHuman contains 15000, 4370 and 5000 images for training, validation, and testing, respectively.
a total of 470K human instances from train and validation subsets and 23 persons per image, with various kinds of occlusions in the dataset</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://sshao0516.github.io/CrowdHuman/" class="Link-sc-1brdqhf-0 cKRjba">https://sshao0516.github.io/CrowdHuman/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>EuroCity Persons Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: collected on-board a moving vehicle in 31 cities of 12 European countries,
over 238200 person instances manually labeled in over 47300 images,
contains a large number of person orientation annotations (over 211200)</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://eurocity-dataset.tudelft.nl/" class="Link-sc-1brdqhf-0 cKRjba">https://eurocity-dataset.tudelft.nl/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1805.07193" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1805.07193</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild</strong></p><img src="http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/files/intro.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/" class="Link-sc-1brdqhf-0 cKRjba">http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/</a></li></ul><h1 id="full-body-annotations" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#full-body-annotations" color="auto.gray.8" aria-label="Full-Body Annotations permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Full-Body Annotations</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>COCO-WholeBody</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jin-s13/COCO-WholeBody" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jin-s13/COCO-WholeBody</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Halpe Full-Body Human Keypoints and HOI-Det dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Halpe: full body human pose estimation and human-object interaction detection dataset</li><li>github:<a target="_blank" rel="noopener noreferrer" href="https://github.com/Fang-Haoshu/Halpe-FullBody" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Fang-Haoshu/Halpe-FullBody</a></li></ul><h2 id="vehicle-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#vehicle-detection" color="auto.gray.8" aria-label="Vehicle Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Vehicle Detection</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Toyota Motor Europe (TME) Motorway Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: composed by 28 clips for a total of approximately 27 minutes (30000+ frames) with vehicle annotation</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://cmp.felk.cvut.cz/data/motorway/" class="Link-sc-1brdqhf-0 cKRjba">http://cmp.felk.cvut.cz/data/motorway/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Welcome to BIT-Vehicle Dataset</strong></p><img src="http://iitlab.bit.edu.cn/mcislab/vehicledb/dataset.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 9,850 vehicle images, sizes of 1600<em>1200 and 1920</em>1080 captured from two cameras at different time and places in the dataset</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://iitlab.bit.edu.cn/mcislab/vehicledb/" class="Link-sc-1brdqhf-0 cKRjba">http://iitlab.bit.edu.cn/mcislab/vehicledb/</a></li></ul><h1 id="vehicle-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#vehicle-re-id" color="auto.gray.8" aria-label="Vehicle Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Vehicle Re-ID</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Large-Scale Dataset for Vehicle Re-Identification in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/PKU-IMRE/VERI-Wild" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/PKU-IMRE/VERI-Wild</a></li></ul><h2 id="logo-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#logo-detection" color="auto.gray.8" aria-label="Logo Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Logo Detection</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>QMUL-OpenLogo: Open Logo Detection Challenge</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: QMUL-OpenLogo contains 27,083 images from 352 logo classes,
built by aggregating and refining 7 existing datasets and establishing an open logo detection evaluation protocol</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://qmul-openlogo.github.io/" class="Link-sc-1brdqhf-0 cKRjba">https://qmul-openlogo.github.io/</a></li></ul><h1 id="head-detection" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#head-detection" color="auto.gray.8" aria-label="Head Detection permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Head Detection</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SCUT-HEAD</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: SCUT HEAD is a large-scale head detection dataset, including 4405 images labeld with 111251 heads.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HollywoodHeads dataset</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.di.ens.fr/willow/research/headdetection/" class="Link-sc-1brdqhf-0 cKRjba">http://www.di.ens.fr/willow/research/headdetection/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Brainwash dataset.</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://exhibits.stanford.edu/data/catalog/sx925dc9385" class="Link-sc-1brdqhf-0 cKRjba">https://exhibits.stanford.edu/data/catalog/sx925dc9385</a></p><h2 id="detection-from-video" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#detection-from-video" color="auto.gray.8" aria-label="Detection From Video permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Detection From Video</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YouTube-Objects dataset v2.2</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://calvin.inf.ed.ac.uk/datasets/youtube-objects-dataset/" class="Link-sc-1brdqhf-0 cKRjba">http://calvin.inf.ed.ac.uk/datasets/youtube-objects-dataset/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ILSVRC2015: Object detection from video (VID)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vision.cs.unc.edu/ilsvrc2015/download-videos-3j16.php#vid" class="Link-sc-1brdqhf-0 cKRjba">http://vision.cs.unc.edu/ilsvrc2015/download-videos-3j16.php#vid</a></li></ul><h1 id="segmentation" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#segmentation" color="auto.gray.8" aria-label="Segmentation permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Segmentation</h1><h2 id="mapillary-vistas-dataset" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#mapillary-vistas-dataset" color="auto.gray.8" aria-label="Mapillary Vistas Dataset permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Mapillary Vistas Dataset</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Mapillary Vistas Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 25,000 high-resolution images, 100 object categories, 60 of those instance-specific
<a target="_blank" rel="noopener noreferrer" href="https://www.mapillary.com/dataset/" class="Link-sc-1brdqhf-0 cKRjba">https://www.mapillary.com/dataset/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Releasing the World’s Largest Street-level Imagery Dataset for Teaching Machines to See</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html" class="Link-sc-1brdqhf-0 cKRjba">http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Multi-Human Parsing</strong></p><img src="https://lv-mhp.github.io/static/images/3.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://lv-mhp.github.io/" class="Link-sc-1brdqhf-0 cKRjba">https://lv-mhp.github.io/</a></p><h1 id="pascal-voc" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pascal-voc" color="auto.gray.8" aria-label="PASCAL VOC permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>PASCAL VOC</h1><h2 id="augmented-pascal-voc" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#augmented-pascal-voc" color="auto.gray.8" aria-label="Augmented Pascal VOC permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Augmented Pascal VOC</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://home.bharathh.info/pubs/codes/SBD/download.html" class="Link-sc-1brdqhf-0 cKRjba">http://home.bharathh.info/pubs/codes/SBD/download.html</a></p><h1 id="supervisely-person" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#supervisely-person" color="auto.gray.8" aria-label="Supervisely Person permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Supervisely Person</h1><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://supervise.ly/" class="Link-sc-1brdqhf-0 cKRjba">https://supervise.ly/</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469" class="Link-sc-1brdqhf-0 cKRjba">https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469</a></li></ul><h1 id="microsoft-coco" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#microsoft-coco" color="auto.gray.8" aria-label="Microsoft COCO permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Microsoft COCO</h1><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://mscoco.org/" class="Link-sc-1brdqhf-0 cKRjba">http://mscoco.org/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/pdollar/coco" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/pdollar/coco</a></li></ul><h1 id="the-oxford-iiit-pet-dataset" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#the-oxford-iiit-pet-dataset" color="auto.gray.8" aria-label="The Oxford-IIIT Pet Dataset permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The Oxford-IIIT Pet Dataset</h1><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: a 37 category pet dataset with roughly 200 images for each class.
All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.robots.ox.ac.uk/~vgg/data/pets/" class="Link-sc-1brdqhf-0 cKRjba">http://www.robots.ox.ac.uk/~vgg/data/pets/</a></li></ul><h2 id="coco-stuff" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#coco-stuff" color="auto.gray.8" aria-label="COCO-Stuff permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>COCO-Stuff</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>COCO-Stuff: Thing and Stuff Classes in Context</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>COCO-Stuff 10K dataset v1.1</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.03716" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.03716</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/nightrome/cocostuff" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/nightrome/cocostuff</a></p><h1 id="scene-parsing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#scene-parsing" color="auto.gray.8" aria-label="Scene Parsing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scene Parsing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MIT Scene Parsing Benchmark</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://sceneparsing.csail.mit.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://sceneparsing.csail.mit.edu/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ADE20K</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: train: 20,120 images, val: 2000 images. contains 150 stuff/object category labels (e.g., wall, sky, and tree) and 1,038 imagelevel scene descriptors (e.g., airport terminal, bedroom, and street).</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" class="Link-sc-1brdqhf-0 cKRjba">http://groups.csail.mit.edu/vision/datasets/ADE20K/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Semantic Understanding of Scenes through the ADE20K Dataset</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1608.05442" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1608.05442</a></p><h1 id="imagenet" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#imagenet" color="auto.gray.8" aria-label="ImageNet permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ImageNet</h1><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>synsets: <a target="_blank" rel="noopener noreferrer" href="http://image-net.org/challenges/LSVRC/2014/browse-det-synsets" class="Link-sc-1brdqhf-0 cKRjba">http://image-net.org/challenges/LSVRC/2014/browse-det-synsets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ImageNet-Utils</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Utils to help download images by id, crop bounding box, label images, etc.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tzutalin/ImageNet_Utils" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tzutalin/ImageNet_Utils</a></li></ul><h1 id="captioning--description" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#captioning--description" color="auto.gray.8" aria-label="Captioning / Description permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Captioning / Description</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>TGIF: A New Dataset and Benchmark on Animated GIF Description</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1604.02748" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1604.02748</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/raingo/TGIF-Release" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/raingo/TGIF-Release</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Collecting Multilingual Parallel Video Descriptions Using Mechanical Turk</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 1970 YouTube video snippets: 1200 training, 100 validation, 670 test</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/" class="Link-sc-1brdqhf-0 cKRjba">http://www.cs.utexas.edu/users/ml/clamp/videoDescription/</a></li></ul><h1 id="video" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#video" color="auto.gray.8" aria-label="Video permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Video</h1><table class="table__Table-sc-ixm5yk-0 eooEiq"><thead><tr><th align="center">Dataset</th><th align="center"># Videos</th><th align="center"># Classes</th><th align="center">Year</th><th align="center">Manually Labeled ?</th></tr></thead><tbody><tr><td align="center">Kodak</td><td align="center">1,358</td><td align="center">25</td><td align="center">2007</td><td align="center">✓</td></tr><tr><td align="center">HMDB51</td><td align="center">7000</td><td align="center">51</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">Charades</td><td align="center">9848</td><td align="center">157</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">MCG-WEBV</td><td align="center">234,414</td><td align="center">15</td><td align="center">2009</td><td align="center">✓</td></tr><tr><td align="center">CCV</td><td align="center">9,317</td><td align="center">20</td><td align="center">2011</td><td align="center">✓</td></tr><tr><td align="center">UCF-101</td><td align="center">13,320</td><td align="center">101</td><td align="center">2012</td><td align="center">✓</td></tr><tr><td align="center">THUMOS-2</td><td align="center">18,394</td><td align="center">101</td><td align="center">2014</td><td align="center">✓</td></tr><tr><td align="center">MED-2014</td><td align="center">≈28,000</td><td align="center">20</td><td align="center">2014</td><td align="center">✓</td></tr><tr><td align="center">Sports-1M</td><td align="center">1M</td><td align="center">487</td><td align="center">2014</td><td align="center">✗</td></tr><tr><td align="center">ActivityNet</td><td align="center">27,801</td><td align="center">203</td><td align="center">2015</td><td align="center">✓</td></tr><tr><td align="center">FCVID</td><td align="center">91,223</td><td align="center">239</td><td align="center">2015</td><td align="center">✓</td></tr></tbody></table><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UCF101 - Action Recognition Data Set</strong></p><img src="http://crcv.ucf.edu/data/UCF101/UCF101.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://crcv.ucf.edu/data/UCF101.php" class="Link-sc-1brdqhf-0 cKRjba">http://crcv.ucf.edu/data/UCF101.php</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>HMDB51: A Large Video Database for Human Motion Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/" class="Link-sc-1brdqhf-0 cKRjba">http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://activity-net.org/" class="Link-sc-1brdqhf-0 cKRjba">http://activity-net.org/</a></li><li>download: <a target="_blank" rel="noopener noreferrer" href="http://activity-net.org/download.html" class="Link-sc-1brdqhf-0 cKRjba">http://activity-net.org/download.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/activitynet" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/activitynet</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Sports-1M</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/gtoderici/sports-1m-dataset/" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/gtoderici/sports-1m-dataset/</a></li><li>thumbnails: <a target="_blank" rel="noopener noreferrer" href="http://cs.stanford.edu/people/karpathy/deepvideo/classes.html" class="Link-sc-1brdqhf-0 cKRjba">http://cs.stanford.edu/people/karpathy/deepvideo/classes.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Charades Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: This dataset guides our research into unstructured video activity recogntion and commonsense reasoning for daily human activities.</li><li>intro: The dataset contains 66,500 temporal annotations for 157 action classes,
41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos.</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://allenai.org/plato/charades/" class="Link-sc-1brdqhf-0 cKRjba">http://allenai.org/plato/charades/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>FCVID: Fudan-Columbia Video Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://bigvid.fudan.edu.cn/FCVID/" class="Link-sc-1brdqhf-0 cKRjba">http://bigvid.fudan.edu.cn/FCVID/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>YouTube-8M: A Large-Scale Video Classification Benchmark</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://research.google.com/youtube8m/" class="Link-sc-1brdqhf-0 cKRjba">http://research.google.com/youtube8m/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1609.08675" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1609.08675</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>stabilized video frames</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 9 TB, 35,000,000 clips, 32 frames</li><li>intro: Generating Videos with Scene Dynamics</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://web.mit.edu/vondrick/tinyvideo/#data" class="Link-sc-1brdqhf-0 cKRjba">http://web.mit.edu/vondrick/tinyvideo/#data</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The Kinetics Human Action Video Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Google</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/" class="Link-sc-1brdqhf-0 cKRjba">https://deepmind.com/research/open-source/open-source-datasets/kinetics/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1705.06950" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1705.06950</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>e-Lab Video Data Set(s)</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: &quot;Currently, e-VDS35 has 35 classes and a total of 2050 videos of roughly 10 seconds each (see histogram below). We are aiming to collect overall 1750 (50 × 35) videos with your help.&quot;</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://engineering.purdue.edu/elab/eVDS" class="Link-sc-1brdqhf-0 cKRjba">https://engineering.purdue.edu/elab/eVDS</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Video Dataset Overview</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Sortable and searchable compilation of video dataset</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://www.di.ens.fr/~miech/datasetviz/" class="Link-sc-1brdqhf-0 cKRjba">https://www.di.ens.fr/~miech/datasetviz/</a></li></ul><h1 id="scene" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#scene" color="auto.gray.8" aria-label="Scene permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Scene</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Imperial College London</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://robotvault.bitbucket.org/scenenet-rgbd.html" class="Link-sc-1brdqhf-0 cKRjba">https://robotvault.bitbucket.org/scenenet-rgbd.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1612.05079" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1612.05079</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/jmccormac/pySceneNetRGBD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/jmccormac/pySceneNetRGBD</a></li></ul><h1 id="autonomous-driving" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#autonomous-driving" color="auto.gray.8" aria-label="Autonomous Driving permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Autonomous Driving</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BDD: Berkely Deep Drive</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 100,000 HD video sequences of over 1,100-hour driving experience across many different times in the day,
weather conditions, and driving scenarios</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://bdd-data.berkeley.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://bdd-data.berkeley.edu/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/ucbdrive/bdd-data" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/ucbdrive/bdd-data</a></li></ul><h1 id="ocr" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#ocr" color="auto.gray.8" aria-label="OCR permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>OCR</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vision.cornell.edu/se3/coco-text/" class="Link-sc-1brdqhf-0 cKRjba">http://vision.cornell.edu/se3/coco-text/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="http://arxiv.org/abs/1601.07140" class="Link-sc-1brdqhf-0 cKRjba">http://arxiv.org/abs/1601.07140</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Chinese Text in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 32,285 high resolution images, 1,018,402 character instances, 3,850 character categories, 6 kinds of attributes</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://ctwdataset.github.io/" class="Link-sc-1brdqhf-0 cKRjba">https://ctwdataset.github.io/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1803.00085" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1803.00085</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1903.10412" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1903.10412</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/chongshengzhang/shopsign" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/chongshengzhang/shopsign</a></li></ul><h1 id="retrieval" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#retrieval" color="auto.gray.8" aria-label="Retrieval permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Retrieval</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Oxford5k</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Paris6k</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">Oxford105k</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl">UKB</p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>NUS-WIDE</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ImageNet-YahooQA</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>University-1652</strong>: </p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><img src="https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/Data.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/>
<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2002.12186" class="Link-sc-1brdqhf-0 cKRjba">[Paper]</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_drone.jpg?raw=true" class="Link-sc-1brdqhf-0 cKRjba">[Explore Drone-view Data]</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_satellite.jpg?raw=true" class="Link-sc-1brdqhf-0 cKRjba">[Explore Satellite-view Data]</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_street.jpg?raw=true" class="Link-sc-1brdqhf-0 cKRjba">[Explore Street-view Data]</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/embed/dzxXPp8tVn4?vq=hd1080" class="Link-sc-1brdqhf-0 cKRjba">[Video Sample]</a>
<a target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/110987552" class="Link-sc-1brdqhf-0 cKRjba">[中文介绍]</a></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>Dataset and Baseline Code: <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/University1652-Baseline" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/University1652-Baseline</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DeepFashion: In-shop Clothes Retrieval</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 7,982 number of clothing items; 52,712 number of in-shop clothes images, and ~200,000 cross-pose/scale pairs; Each image is annotated by bounding box, clothing type and pose type.</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html</a></li></ul><h1 id="person-re-id" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#person-re-id" color="auto.gray.8" aria-label="Person Re-ID permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Person Re-ID</h1><table class="table__Table-sc-ixm5yk-0 eooEiq"><thead><tr><th align="center">Dataset</th><th align="center">Description</th></tr></thead><tbody><tr><td align="center">CUHK01</td><td align="center">971 identities, 3884 images, manually cropped</td></tr><tr><td align="center">CUHK02</td><td align="center">1816 identities, 7264 images, manually cropped</td></tr><tr><td align="center">CUHK03</td><td align="center">1360 identities, 13164 images, manually cropped + automatically detected</td></tr></tbody></table><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification Datasets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html" class="Link-sc-1brdqhf-0 cKRjba">http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/RSL-NEU/person-reid-benchmark" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/RSL-NEU/person-reid-benchmark</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CUHK Person Re-identification Datasets</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>PRW (Person Re-identification in the Wild) Dataset</strong></p><img src="http://www.liangzheng.com.cn/Project/pipeline_prw.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.liangzheng.com.cn/Project/project_prw.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.liangzheng.com.cn/Project/project_prw.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liangzheng06/PRW-baseline" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liangzheng06/PRW-baseline</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-identification in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: CVPR 2017 spotlight</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1604.02531" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1604.02531</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DukeMTMC-reID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DukeMTMC-reID is a subset of the DukeMTMC for image-based re-identification, in the format of the Market-1501 dataset</li><li>intro: 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/layumi/DukeMTMC-reID_evaluation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/layumi/DukeMTMC-reID_evaluation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DukeMTMC4ReID</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DukeMTMC4ReID dataset</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/NEU-Gou/DukeReID" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/NEU-Gou/DukeReID</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Person Re-ID (PRID) Dataset 2011</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/" class="Link-sc-1brdqhf-0 cKRjba">https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MARS (Motion Analysis and Re-identification Set) Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: an extension of the Market-1501 dataset</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.liangzheng.com.cn/Project/project_mars.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.liangzheng.com.cn/Project/project_mars.html</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/liangzheng06/MARS-evaluation" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/liangzheng06/MARS-evaluation</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>X-MARS Reordering of the MARS Dataset for Image to Video Evaluation</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: This repository provides the X-MARS dataset splits for image to video/tracklet evaluation</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/andreas-eberle/x-mars" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/andreas-eberle/x-mars</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>MSMT17</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 15-camera (12 outdoor cameras, 3 indoor cameras), 4,101 Identities, 126,441 BBoxes</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://www.pkuvmc.com/publications/longhui.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.pkuvmc.com/publications/longhui.html</a></li><li>soa: <a target="_blank" rel="noopener noreferrer" href="http://www.pkuvmc.com/publications/state_of_the_art.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.pkuvmc.com/publications/state_of_the_art.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Labeled Pedestrian in the Wild</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: train/test identities: 1,975/756</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://liuyu.us/dataset/lpw/" class="Link-sc-1brdqhf-0 cKRjba">http://liuyu.us/dataset/lpw/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>SenseReID</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://drive.google.com/file/d/0B56OfSrVI8hubVJLTzkwV2VaOWM/view" class="Link-sc-1brdqhf-0 cKRjba">https://drive.google.com/file/d/0B56OfSrVI8hubVJLTzkwV2VaOWM/view</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>3DPeS</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.openvisor.org/3dpes.asp" class="Link-sc-1brdqhf-0 cKRjba">http://www.openvisor.org/3dpes.asp</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>iQIYI-VID: A Large Dataset for Multi-modal Person Identification</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1811.07548" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1811.07548</a></p><h1 id="fashion" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#fashion" color="auto.gray.8" aria-label="Fashion permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fashion</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Large-scale Fashion (DeepFashion) Database</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Attribute Prediction, Consumer-to-shop Clothes Retrieval, In-shop Clothes Retrieval, and Landmark Detection</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Apparel classification with Style</strong></p><img src="http://people.ee.ethz.ch/~lbossard/projects/accv12/img/motivation.jpg" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 15 clothing classes, 88951 images</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://people.ee.ethz.ch/~lbossard/projects/accv12/index.html" class="Link-sc-1brdqhf-0 cKRjba">http://people.ee.ethz.ch/~lbossard/projects/accv12/index.html</a></li></ul><h1 id="attribute-datasets" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#attribute-datasets" color="auto.gray.8" aria-label="Attribute Datasets permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Attribute Datasets</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Attribute Datasets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: in total 41,585 pedestrian samples, each of which is annotated with 72 attributes
as well as viewpoints, occlusions, body parts information</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://www.ecse.rpi.edu/homepages/cvrl/database/AttributeDataset.htm" class="Link-sc-1brdqhf-0 cKRjba">https://www.ecse.rpi.edu/homepages/cvrl/database/AttributeDataset.htm</a></li></ul><h2 id="pedestrian-attribute-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH2-sc-1fu06k9-2 ffNRvO gCPbFb fvbkiW Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#pedestrian-attribute-recognition" color="auto.gray.8" aria-label="Pedestrian Attribute Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pedestrian Attribute Recognition</h2><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>A Richly Annotated Dataset for Pedestrian Attribute Recognition</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://rap.idealtest.org/" class="Link-sc-1brdqhf-0 cKRjba">http://rap.idealtest.org/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1603.07054" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1603.07054</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pedestrian Attribute Recognition At Far Distance</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: PEdesTrian Attribute (PETA)</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://mmlab.ie.cuhk.edu.hk/projects/PETA.html" class="Link-sc-1brdqhf-0 cKRjba">http://mmlab.ie.cuhk.edu.hk/projects/PETA.html</a></li><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Market-1501_Attribute</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vana77/Market-1501_Attribute" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vana77/Market-1501_Attribute</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://vana77.github.io" class="Link-sc-1brdqhf-0 cKRjba">https://vana77.github.io</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DukeMTMC-attribute</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vana77/DukeMTMC-attribute" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vana77/DukeMTMC-attribute</a></li><li>blog: <a target="_blank" rel="noopener noreferrer" href="https://vana77.github.io" class="Link-sc-1brdqhf-0 cKRjba">https://vana77.github.io</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Parse27k</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Pedestrian Attribute Recognition in Sequences</li><li>intro: &gt;27,000 annotated pedestrians, 10 attributes</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="https://www.vision.rwth-aachen.de/page/parse27k" class="Link-sc-1brdqhf-0 cKRjba">https://www.vision.rwth-aachen.de/page/parse27k</a></li><li>tools: <a target="_blank" rel="noopener noreferrer" href="https://github.com/psudowe/parse27k_tools" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/psudowe/parse27k_tools</a></li></ul><h1 id="tracking" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tracking" color="auto.gray.8" aria-label="Tracking permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tracking</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://detrac-db.rit.albany.edu/" class="Link-sc-1brdqhf-0 cKRjba">http://detrac-db.rit.albany.edu/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1511.04136" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1511.04136</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>DukeMTMC: Duke Multi-Target, Multi-Camera Tracking Project</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: DukeMTMC aims to accelerate advances in multi-target multi-camera tracking. It provides a tracking system that works within and across cameras, a new large scale HD video data set recorded by 8 synchronized cameras with more than 7,000 single camera trajectories and over 2,000 unique identities</li><li>homepage: <a target="_blank" rel="noopener noreferrer" href="http://vision.cs.duke.edu/DukeMTMC/" class="Link-sc-1brdqhf-0 cKRjba">http://vision.cs.duke.edu/DukeMTMC/</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>The WILDTRACK Seven-Camera HD Dataset</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://cvlab.epfl.ch/data/wildtrack" class="Link-sc-1brdqhf-0 cKRjba">https://cvlab.epfl.ch/data/wildtrack</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>GOT-10k: Generic Object Tracking Benchmark</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A large, high-diversity, one-shot database for generic object tracking in the wild</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="http://got-10k.aitestunion.com/" class="Link-sc-1brdqhf-0 cKRjba">http://got-10k.aitestunion.com/</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/got-10k/toolkit" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/got-10k/toolkit</a></li></ul><h1 id="color-classification" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#color-classification" color="auto.gray.8" aria-label="Color Classification permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Color Classification</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Vehicle Color Recognition on an Urban Road by Feature Context</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://mclab.eic.hust.edu.cn/~pchen/project.html" class="Link-sc-1brdqhf-0 cKRjba">http://mclab.eic.hust.edu.cn/~pchen/project.html</a></p><h1 id="license-plate-detection-and-recognition" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#license-plate-detection-and-recognition" color="auto.gray.8" aria-label="License Plate Detection and Recognition permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License Plate Detection and Recognition</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Application-Oriented License Plate (AVOP) Database</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://aolpr.ntust.edu.tw/lab/download.html" class="Link-sc-1brdqhf-0 cKRjba">http://aolpr.ntust.edu.tw/lab/download.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CCPD: Chinese City Parking Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>paper: <a target="_blank" rel="noopener noreferrer" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf" class="Link-sc-1brdqhf-0 cKRjba">http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/detectRecog/CCPD" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/detectRecog/CCPD</a></li><li>dataset: <a target="_blank" rel="noopener noreferrer" href="https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view" class="Link-sc-1brdqhf-0 cKRjba">https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view</a></li></ul><h1 id="face-anti-spoofing" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#face-anti-spoofing" color="auto.gray.8" aria-label="Face Anti-Spoofing permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Face Anti-Spoofing</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: ECCV 2020</li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2007.12342" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/2007.12342</a></li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Davidzhangyuanhan/CelebA-Spoof" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Davidzhangyuanhan/CelebA-Spoof</a></li></ul><h1 id="tools" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#tools" color="auto.gray.8" aria-label="Tools permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tools</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>VoTT: Visual Object Tagging Tool 1.5</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/Microsoft/VoTT" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/Microsoft/VoTT</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>LabelImg: a graphical image annotation tool and label object bounding boxes in images</strong></p><img src="https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo2.png" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tzutalin/labelImg" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/tzutalin/labelImg</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Pychet Labeller</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A python based annotation/labelling toolbox for images.
The program allows the user to annotate individual objects in images.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/sbargoti/pychetlabeller" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/sbargoti/pychetlabeller</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>ml-pyxis: Tool for reading and writing datasets of tensors (numpy.ndarray) with MessagePack and Lightning Memory-Mapped Database (LMDB).</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Tool for reading and writing datasets of tensors in a Lightning Memory-Mapped Database (LMDB).
Designed to manage machine learning datasets with fast reading speeds.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/vicolab/ml-pyxis" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/vicolab/ml-pyxis</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Open Image Dataset downloader</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/e-lab/crawl-dataset" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/e-lab/crawl-dataset</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BBox-Label-Tool</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A simple tool for labeling object bounding boxes in images</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/puzzledqs/BBox-Label-Tool" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/puzzledqs/BBox-Label-Tool</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Data Labeler for Video</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: A GUI tool for conveniently label the objects in video, using the powerful object tracking.</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com//hahnyuan/video_labeler" class="Link-sc-1brdqhf-0 cKRjba">https://github.com//hahnyuan/video_labeler</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Computer Vision Annotation Tool (CVAT)</strong></p><img src="https://raw.githubusercontent.com/opencv/cvat/master/cvat/apps/documentation/static/documentation/images/gif003.gif" class="image__Image-sc-1r30dtv-0 elBfYx"/><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: Computer Vision Annotation Tool (CVAT) is a web-based tool which helps to annotate video and images for Computer Vision algorithms</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/opencv/cvat" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/opencv/cvat</a></li></ul><h1 id="artist" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#artist" color="auto.gray.8" aria-label="Artist permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Artist</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>BAM! The Behance Artistic Media Dataset</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: 2.5M artwork urls, 393K attribute labels, 74K short image descriptions/captions</li><li>project page: <a target="_blank" rel="noopener noreferrer" href="https://bam-dataset.org/" class="Link-sc-1brdqhf-0 cKRjba">https://bam-dataset.org/</a></li><li>arxiv: <a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/1704.08614" class="Link-sc-1brdqhf-0 cKRjba">https://arxiv.org/abs/1704.08614</a></li></ul><h1 id="resources" class="Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 heading__StyledH1-sc-1fu06k9-1 ffNRvO gCPbFb fGjcEF Heading-sc-1cjoo9h-0 heading__StyledHeading-sc-1fu06k9-0 ffNRvO gCPbFb"><a href="#resources" color="auto.gray.8" aria-label="Resources permalink" class="Link-sc-1brdqhf-0 ekSqTm"><svg aria-hidden="true" role="img" class="octicon-link" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:middle;overflow:visible"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Resources</h1><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>CV Datasets on the web</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="http://www.cvpapers.com/datasets.html" class="Link-sc-1brdqhf-0 cKRjba">http://www.cvpapers.com/datasets.html</a></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Awesome Public Datasets</strong></p><ul class="list__List-sc-s5kxp2-0 dFVIUa"><li>intro: An awesome list of high-quality open datasets in public domains (on-going). By everyone, for everyone!</li><li>github: <a target="_blank" rel="noopener noreferrer" href="https://github.com/caesar0301/awesome-public-datasets" class="Link-sc-1brdqhf-0 cKRjba">https://github.com/caesar0301/awesome-public-datasets</a></li></ul><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><strong>Machine Learning Repository</strong></p><p class="paragraph__Paragraph-sc-17pab92-0 iNQqSl"><a target="_blank" rel="noopener noreferrer" href="https://archive.ics.uci.edu/ml/datasets.html" class="Link-sc-1brdqhf-0 cKRjba">https://archive.ics.uci.edu/ml/datasets.html</a></p><div class="Box-nv15kw-0 ksEcN"><div display="flex" class="Box-nv15kw-0 jsSpbO"><a href="https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets.md" class="Link-sc-1brdqhf-0 iLYDsn"><svg aria-hidden="true" role="img" class="StyledOcticon-uhnt7w-0 fafffn" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" style="display:inline-block;user-select:none;vertical-align:text-bottom;overflow:visible"><path fill-rule="evenodd" d="M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z"></path></svg>Edit this page</a><div><span font-size="1" color="auto.gray.7" class="Text-sc-1s3uzov-0 gHwtLv">Last updated on<!-- --> <b>12/30/2022</b></span></div></div></div></div></div></main></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id="></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){window.dataLayer && window.dataLayer.push(arguments);}
        gtag('js', new Date());

        
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/";window.___webpackCompilationHash="10c8b9c1f9dde870e591";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-2526e2a471eef3b9c3b2.js"],"app":["/app-f28009dab402ccf9360c.js"],"component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js":["/component---node-modules-gatsby-theme-primer-wiki-src-pages-404-js-bd1c4b7f67a97d4f99af.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-latest-query-js-6ed623c5d829c1a69525.js"],"component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js":["/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js"]};/*]]>*/</script><script src="/polyfill-2526e2a471eef3b9c3b2.js" nomodule=""></script><script src="/component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js-46274f1a3983fff8a36b.js" async=""></script><script src="/commons-c89ede6cb9a530ac5a37.js" async=""></script><script src="/app-f28009dab402ccf9360c.js" async=""></script><script src="/dc6a8720040df98778fe970bf6c000a41750d3ae-8fdfd959b24cacbf7cee.js" async=""></script><script src="/0e226fb0-1cb0709e5ed968a9c435.js" async=""></script><script src="/f0e45107-3309acb69b4ccd30ce0c.js" async=""></script><script src="/framework-6c63f85700e5678d2c2a.js" async=""></script><script src="/webpack-runtime-1fe3daf7582b39746d36.js" async=""></script></body></html>