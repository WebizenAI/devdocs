{"data":{"allMdx":{"nodes":[{"fields":{"slug":"/Host Service Requirements/Email Services/","title":"Email Services"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","title":"LD_PostOffice_SemanticMGR"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Host Service Requirements/Media Processing/","title":"Media Processing"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Non-HTTP(s) Protocols/GUNECO/","title":"GUNECO"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Non-HTTP(s) Protocols/IPFS/","title":"IPFS"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Non-HTTP(s) Protocols/WebRTC/","title":"WebRTC"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Non-HTTP(s) Protocols/WebSockets/","title":"WebSockets"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Non-HTTP(s) Protocols/WebTorrent/","title":"WebTorrent"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","title":"Sustainable Development Goals (ESG)"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Implementation V1/edge/Webizen Local App Functionality/","title":"Webizen Local App Functionality"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Implementation V1/vps/Server Functionality Summary (VPS)/","title":"Server Functionality Summary (VPS)"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","title":"Audio & Video Analysis"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Webizen 2.0/AI Capabilities/Image Analysis/","title":"Image Analysis"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Webizen 2.0/AI Capabilities/Text Analysis/","title":"Text Analysis"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Webizen 2.0/Mobile Apps/Android/","title":"Android"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Webizen 2.0/Mobile Apps/iOS/","title":"iOS"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","title":"Semantic Web - An Introduction"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","title":"Credentials & Contracts Manager"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","title":"File (Package) Manager"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","title":"Webizen Apps (V1)"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","title":"Webizen Manager"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","title":"GraphQL"},"frontmatter":{"draft":false},"rawBody":""},{"fields":{"slug":"/","title":"Webizen V1 Project Documentation"},"frontmatter":{"draft":false},"rawBody":"# Webizen V1 Project Documentation\nThe [Webizen 1.0](Implementation%20V1/Webizen%201.0.md) [Design Goals Overview](Implementation%20V1/App-design-sdk-v1/Design%20Goals/Design%20Goals%20Overview.md) documents alongside the [Implementation V1 Summary](Implementation%20V1/Implementation%20V1%20Summary.md), provides information about the objectives, which seek to use [GoLang Libraries](Implementation%20V1/GoLang%20Libraries.md) as a means to quickly produce a solution that provides basic functionality as required to make progress.\n\n**There are two packages required;**\na. An Implementation that can be installed and operated on a VPS:  [Server Functionality Summary (VPS|[VPS)](Implementation%20V1/vps/Server%20Functionality%20Summary%20(VPS)]].md)\nb. An Implementation that can be easily installed and operated on Home Computers: [Webizen Local App Functionality](Implementation%20V1/edge/Webizen%20Local%20App%20Functionality.md) \n\nThe project intends to improve upon the works already previously demonstrated as [GOLD](https://github.com/linkeddata/gold). \n\nThe Webizen 1.0 will be used to develop the [Webizen Alliance](Commercial/Webizen%20Alliance.md) socio-technical framework.  Therefore, the objectives of the Webizen 1.0 Software Development (Platform) Project; is specifically targeted at providing the underlying system functionality required to build these systems using Semantic(RDF)Web(HTML/JS/CSS) app development practices.\n\n### Webizen Project Documentation\n\n1. Webizen 1.0 \n3. Core Services\n4. Core Technologies\n5. Database Requirements\n6. Host Service Requirements\n7. Non-HTTP(s) Protocols\n8. Webizen 2.0\n9. Commercial\n\n\n#### DOCUMENT NOTES:\nSome content is produced through the use of information obtained from Wikipedia.  The links are provided to the articles, where this is the case.\n\n**TAGS** \n#intro #Summary #draft"},{"fields":{"slug":"/Commercial/Commercial Structure/","title":"Commercial Structure"},"frontmatter":{"draft":false},"rawBody":"The work on Webizen is currently being done by Timothy Holborn.  \n\nThe Webizen Project reflects an attempt to successfully accumplish 'next steps', for a project thats more than two decades old overall.   There are many steps or milestones still required, and its been a difficult journey overall / overtime. \n\nThe Webizen Project brings together works that have been done by many around the world, in a different sort of way.  The Works undertaken by Holborn have been managed in connection to the trading names 'sailing digital', 'media prophet', 'web civics', 'trust factory' and 'webizen'. \n\nThe objective is to establish a 'minimum viable ecosystem' to support incorporating a somewhat complex for-purpose structure, that is able to support human rights in ways not possible earlier. This objectives to achieve this 'mission' is outlined by the [Implementation V1 Summary](../Implementation%20V1/Implementation%20V1%20Summary.md) document.\n\nThere are MANY who've been instrumentally supportive in various ways over time (and there's been various people who've been extrodinarily destructive; and some, quite dangerous), which is most certainly on my mind.  \n\nWhat i have found historically, is that many attempts to create 'decentralised platforms' have come and gone over the decades.  Whenever projects start with some kind of centralised model, they never seem to be able to get to the part where they're then able to decentralise the platform and/or platform deployments, as to support the human rights of people that groups involved in commercialising products / platforms, want to have depending upon their works / platforms. \n\nAs such; great care has been taken, to seek to ensure that there are no barriers for doing it right, to do the right thing by people, who i hope will benefit greatly through the use of Webizen technology.\n\n#draft "},{"fields":{"slug":"/Commercial/Community of Practice/","title":"Community of Practice"},"frontmatter":{"draft":false},"rawBody":"What is a 'Community of Practice'?\n\nA Community of Practice is defined in [WikiPedia](https://en.wikipedia.org/wiki/Community_of_practice) in the following way;\n\nA **community of practice** (**CoP**) is a group of people who \"share a concern or a passion for something they do and learn how to do it better as they interact regularly\".[](https://en.wikipedia.org/wiki/Community_of_practice#cite_note-1) The concept was first proposed by [cognitive anthropologist](https://en.wikipedia.org/wiki/Cognitive_anthropology \"Cognitive anthropology\") [Jean Lave](https://en.wikipedia.org/wiki/Jean_Lave \"Jean Lave\") and educational theorist [Etienne Wenger](https://en.wikipedia.org/wiki/Etienne_Wenger \"Etienne Wenger\") in their 1991 book _Situated Learning_ ([](https://en.wikipedia.org/wiki/Community_of_practice#CITEREFLaveWenger1991)). Wenger then significantly expanded on the concept in his 1998 book _Communities of Practice_ ([](https://en.wikipedia.org/wiki/Community_of_practice#CITEREFWenger1998)).\n\nA CoP can evolve naturally because of the members' common interest in a particular domain or area, or it can be created deliberately with the goal of gaining knowledge related to a specific field. It is through the process of sharing information and experiences with the group that members learn from each other, and have an opportunity to develop personally and professionally ([](https://en.wikipedia.org/wiki/Community_of_practice#CITEREFLaveWenger1991)).\n\nCoPs can exist in physical settings, for example, a lunchroom at work, a field setting, a factory floor, or elsewhere in the environment, but members of CoPs do not have to be co-located. They form a \"virtual community of practice\" (VCoP) ([](https://en.wikipedia.org/wiki/Community_of_practice#CITEREFDub%C3%A9BourhisJacob2005)) when they collaborate online, such as within discussion boards, newsgroups, or the various chats on social media, such as #musochat centered on contemporary classical music performance ([](https://en.wikipedia.org/wiki/Community_of_practice#CITEREFSheridan2015)). A \"mobile community of practice\" (MCoP) ([](https://en.wikipedia.org/wiki/Community_of_practice#CITEREFKietzmannPlanggerEatonHeilgenberg2013)) is when members communicate with one another via mobile phones and participate in community work on the go.\n\nCommunities of practice are not new phenomena: this type of learning has existed for as long as people have been learning and sharing their experiences through storytelling. The idea is rooted in [American pragmatism](https://en.wikipedia.org/wiki/American_pragmatism \"American pragmatism\"), especially [C. S. Peirce](https://en.wikipedia.org/wiki/C._S._Peirce \"C. S. Peirce\")'s concept of the \"[community of inquiry](https://en.wikipedia.org/wiki/Community_of_inquiry \"Community of inquiry\")\" ([](https://en.wikipedia.org/wiki/Community_of_practice#CITEREFShields2003)), but also [John Dewey](https://en.wikipedia.org/wiki/John_Dewey \"John Dewey\")'s principle of learning through occupation ([](https://en.wikipedia.org/wiki/Community_of_practice#CITEREFWallace2007)).\n\n#socialfabric #ValuesFrameworks"},{"fields":{"slug":"/Commercial/Domains/","title":"Domains"},"frontmatter":{"draft":false},"rawBody":"The Webizen Project is currently supported via the ownership of the domains listed below;\n\nintent.eco\n\nlinkeddata.com.au\nlinkeddata.au\n\nwebizen.au\nwebizen.org\nwebizen.org.au\n\nwebcivics.org\nwebcivics.net\nwebcivics.com (CHECK)\n\nTrustfactory.com.au\n\n\n"},{"fields":{"slug":"/Commercial/Webizen Alliance/","title":"Webizen Alliance"},"frontmatter":{"draft":false},"rawBody":"The Webizen Alliance is difficult to define prior to sorting out the [Values Credentials](../Core%20Services/Safety%20Protocols/Values%20Credentials.md) frameworks.  as such, the construct of the framework is seeking to be applied upon an ability to have [Webizen 1.0](../Implementation%20V1/Webizen%201.0.md) operating, as to support the requirements needed by alliance members to support the charter.\n\nThe Webizen 1.0 environment will be used to develop social-protocols to support frameworks relating to the [Centricity](Centricity.md) considerations, as to also support the growth of Webizen as a [Community of Practice](Community%20of%20Practice.md) that is designed to support various values frameworks, as part of the underpinnings for how it is those involved are able to work with one another and in-turn also, address disputes and support their own needs to be safe and supported, in ways webizen hopes to provide a means to support in ways that other solutions (whilst technically possible) have failed to provide already.  \n\nCommercially; The Webizen Alliance seeks to support the growth of a valuable socioeconomic environment whereby many contributors are able to play an important role both in the development of the broader ecosystems, as well as the growth, support and maintainence of solutions for end-users.  \n\nThe effect of this alliance has multiple tiers;\n\n1. Network Level\n    Network level providers agree to the principals set-out and are thereby endorsed to provide 'host services' in regions that they exist in.  Part of the consideration herein; is that wrong-doings, are sought to be handled within the same juristiction as the customer (end-users) if that's what they elect to use (ie: they may elect to use foreign services, for some reason).\n\n2. Value Added Reseller (VAR)\n\tA VAR would generally be some sort of vendor who provides some sort of valued services to an end-user / consumer / owner of a webizen system.\n\n#socialfabric #ValuesFrameworks"},{"fields":{"slug":"/Core Services/Decentralised Ontologies/","title":"Decentralised Ontologies"},"frontmatter":{"draft":false},"rawBody":"The Context of the term Ontologies for this project, is about the use of RDF ontologies or moreover 'semantic web' parts.\n\nThere has been a historic and long-term problem with temporal resolution of 'web' (WWW) content.  This problem has also impacted the availability of 'ontologies', which are the structured vocabularies (semantics) that support the development and use of applications that are dependant upon those ontologies.  In-turn also, ontologies change overtime; whereas the HTTP URI may not. \n\nThen there is also the problem that the use of public (hosted somewhere) ontological resources (particularly in relation to 'commons' information) is often accessed via an API during runtime.  So, the implication becomes a form of API based survellience in relation to activities of humans. \n\nSo in summary; It is desirable to have,\n1. ontologies stored locally (or at least within a 'private network' env) \n2. An ability to manage 'versions' of an ontology that remain 'static'\n3. An ability to assert access control (Read/Write/Append); in a manner that takes into account social factors (ie: stewards vs. consumers / dependents)\n\nSo the purpose of decentralised ontologies (which is part of the Permissive Commons ecosystems) is to create an information systems framework that supports the ability for different authoritative actors / agents (inc. groups) to produce different ontologies and manage the authorship of them; whilst also enabling users to download a copy of those ontologies for use locally (with a semantic / AI agent); and therefore also, any applications that depend upon those ontologies being available, no-longer rely upon whether they're still availalbe on the HTTP URI where they were originally published; but rather, on the DLT (decentralised ledger technology) still being available somewhere.  "},{"fields":{"slug":"/Core Services/Permissive Commons/","title":"Permissive Commons"},"frontmatter":{"draft":false},"rawBody":"Permissive Commons is in-fact similar to the [Decentralised Ontologies](Decentralised%20Ontologies.md) however the scope of 'hyper-media' assets is broader than simply any RDF based assets; and the two are interoperable.\n\nThe Permissive Commons concept is like an advancement of 'open-data', whilst adding more functionality as a consequence of the abillity to use permission structures in relation to assets.\n\nThere are a few different 'usecases' relating to the use of 'permissive commons'. \n\n**The Public Commons**\nThe first is what would generally be considered 'open data' or information / artifacts of knowledge that pertain to and/or are counterparts to the essential knowledge artifacts required to support human rights.  \n\nThe historical context of what these sorts of assets 'look like'; include but are not limited to,\n\n1. Languages\n2. Encylopedias (knowledge about our biosphere)\n3. Liberal Arts related artifacts\n4. STEM related artifacts (including but not exclusive to statistics)\n5. Laws\n6. Works that are no-longer protected by Copyright Rights (public domain works)\n\nThe nature of how these assets are made available online has changed the 'media format' of the content in various ways.  This in-turn produces an array of new 'functionality' or artifact constituencies that did not exist in previous media (plural of medium) formats, as is illustrated in various types of formats both online and in physical and/or inter-personally communicated formats.   Historically, there has generally been a publisher of whatever the artifacts are; although, the fuller body of knowledge about that particular topic or thing, may have artifacts create / published by many entities. \n\nMore recently, an incredibly significant dependence has developed upon the use of a few websites only to make available 'commons' information.  \n\nIn almost all circumstances, the abillity to add more information about something to 'commons' resources - requires some sort of permission or indeed also, attribution - even if only to protect from the potential implications of wrong-doers... \n\nIn other circumstances, its quite important that there is a valid 'authority' associated to the publication of some kind of commons resource; an example could be the legislative information about a particular law made by a government / parliament in some particular juristiction; where the ability to read is made public, and the application of the 'law' is intended to be defined upon all people; but that the creation of that law, requires a particular process involving specified persons.\n\nIn other situations; for example, the development of a software project using 'github' or similar; or the collaborative development of a document or spreadsheet using an application like 'google docs', the permission structures may be different; indeed, it may be restricted to a particular group or components may be restricted to a particular group (ie: the development of a new version). SO, whilst these sorts of examples are still 'commons' between those people involved; it is also 'permissioned' in a particular sort of way.  This then brings about a consideration about how new systems may in-turn offer solutions for problems that haven't been able to be addressed earlier; such as,\n\n- The abilty to associate compensation to 'knowledge' works, whether that be via a licensing model that applies the same costs equally to all types of 'consumers' or whether it differentiates between different types of consumers, based on particular characteristics, etc. \n\n- The ability to track a process of compensating a person for useful works in a way, that can support the ability to employ 'micropayments' via some sort of defined model; to provide fair compensation for work that a person has done, as is defined electronically; and then, once that fair-compensation amount has been renumerated, the license / permissions are changed.  The point being, that people who do work for the good of humanity should be paid fairly for their useful contributions; but that also, they should not be paid in perpetuity as to seek rents, in a way that would harm the ability to support 'freedom of thought'.  \n\n**Technical Description**\n\nPermissive commons uses a variety of different DLT (Decentralised Ledger Technologies) protocols, noting different protocols better suit different types of applications / use-cases. \n\nEach 'entity' has its own 'graph' (pointed graph); whereby elements of that graph can be downloaded to parse the machine-readable format of the data-structures for use by the local agent (or a linked agent within a users personal or 'trusted' network).\n\nConcept or Entity Centric \"Containers\"\n\nAn example is to describe a particular type of Plant; i'll pick a Melaleuca.\nhttps://en.wikipedia.org/wiki/Melaleuca\n\nTherein - the 'permissive commons' container(s) for that 'thing' would include all sorts of information that describes all the different bits and pieces of information known about a mellaleuca; including, both microscopic & macroscopic information; and the ability to store 'identification' information for use by a AI processor (ie: to identify any melaleuca in photos, etc.).\n\nThis then employs the ontological structures to support AI processing for the information about whatever the topic is; and, users are then able to produce new content, that references via URIs the concepts that they're communicating in a manner that is linked to the graph about that particular topic, concept or thing.\n\nThe storage of information about each 'concept' is intended to support provonance, and any alterations provide links to previous versions; alongside an ability to form semantic inferencing notations about any meaningful relations between the concepts / topics noted earlier as may be effected by updates. \n\n[Temporal Semantics](Temporal%20Semantics.md)\nFor example; is a news-article was written about a particular topic in reliance of a particular 'fact' that was believed to be 'true' at the time; but later, new information shows that the old assumption was in-fact incorrect.  The permissive commons engine would support the ability to contextualise the information both from a perspective of when that article or piece of content was written; and, as the manifest underlying assumptions or representations - whether it be about the circumstances of that time, or any changes that have happened since - can in-turn be contextualised for observers (users), temporallly.\n\n[Verifiable Claims & Credentials](Verifiable%20Claims%20&%20Credentials.md)\n\nThe verifiable claims element, has various applications / implications; including, the ability to support provonance in relation to who did what; and in-turn, any terms that may relate to those contributions.  \n\n#Privacy #socialfabric #ValuesFrameworks #SafetyProtocols"},{"fields":{"slug":"/Core Services/Temporal Semantics/","title":"Temporal Semantics"},"frontmatter":{"draft":false},"rawBody":"The concept of 'Temporal Semantics' is critical to the entire ecosystem. \n\nPart of the objective is to address [Social Attack Vectors](Safety%20Protocols/Social%20Factors/Social%20Attack%20Vectors.md) that employ temporal factors as part of the attack method; alongside, an array of happier purposes for providing support for temporal semantics.\n\n#socialfabric #ValuesFrameworks #SafetyProtocols #RuleOfLaw "},{"fields":{"slug":"/Core Services/Verifiable Claims & Credentials/","title":"Verifiable Claims & Credentials"},"frontmatter":{"draft":false},"rawBody":"Verifiable Claims work has developed and morphed into what they call 'verifiable credentials'.  \n\nThe purpose of 'verifiable claims' is to create a cryptographically signed (tamper evident) container that can contain 'data' that can be relied upon as an instrument of 'fact' (regardless of whether or not the claims in it, are factual).  IE: The fact is, i have this 'verifiable claim' that was generated in relation to XYZ or XYZ circumstance, it clearly states 'these things' or provides 'xyz' information from \"Alice\" or 'bob' or 'acme .inc' which may be used as a instrument for electronic contracts.\n\n\n\n#socialfabric #ValuesFrameworks #SafetyProtocols #RuleOfLaw "},{"fields":{"slug":"/Host Service Requirements/Domain Hosting/","title":"Domain Hosting"},"frontmatter":{"draft":false},"rawBody":"# Domain Hosting\nThe VPS Host should support the ability to host a users domain. \n\n"},{"fields":{"slug":"/Host Service Requirements/Website Host/","title":"Website Host"},"frontmatter":{"draft":false},"rawBody":"The \"Web Host\" [Server Functionality Summary (VPS|[VPS)](../Implementation%20V1/vps/Server%20Functionality%20Summary%20(VPS)]].md) is intended to provide information about the VPS (virtual Private Server) side functionality required to support local 'webizen' servers.\n\nThe Webizen Environment will provide a means to support public permissions.  Information that is provided as public, can in-turn be published publically.  The WebHost should provide the ability to use static-site-generators that are able to employ the public-assets provided by webizen users, to selectively publish (and structure) content online, that can in-turn be supported with [Domain Hosting](Domain%20Hosting.md).\n\n\n\n\n\n"},{"fields":{"slug":"/Implementation V1/GoLang Libraries/","title":"GoLang Libraries"},"frontmatter":{"draft":false},"rawBody":"\n\nNOTES FOR LATER\n\nSEARCH:\nEverything: https://github.com/dominikh/everything \n\nsnowman\nA static site generator for SPARQL backends.\nhttps://github.com/glaciers-in-archives/snowman"},{"fields":{"slug":"/Implementation V1/Implementation V1 Summary/","title":"Implementation V1 Summary"},"frontmatter":{"draft":false},"rawBody":"Motivation\n\n\n\n### Server Objective\n\nThe server (VPS) side objective is to reduce the entry level cost for operating a private 'webizen network' as much as is reasonably possible.  However, the better description is more complex.\n\n*If you're not the customer, you're the product...*\n\nThe deployment models will provide both the open-source packages as well as a hosted service, and commercially via the developmentof the [Webizen Alliance](../Commercial/Webizen%20Alliance.md) other hosted-services are encouraged to be developed internationally.  The early versions of these works are expected to lack an array of functionality that is hoped to be achieved in-time.  This will have impacts in many areas.\n\nThere is a cost associated with the use of resources, whether it be in relation to the availability, use and operation of technology (nb: [Biosphere Ontologies](../Core%20Services/Webizen%20Socio-Economics/Biosphere%20Ontologies.md)) or the fact that people doing work to help make good thigs happen, and part of what we're trying to achieve requires us to make [Best Efforts](../Core%20Services/Safety%20Protocols/Social%20Factors/Best%20Efforts.md) to be good agents and work towards [Ending Digital Slavery](../Core%20Services/Safety%20Protocols/Social%20Factors/Ending%20Digital%20Slavery.md), protecting [Freedom of Thought](../Core%20Services/Safety%20Protocols/Social%20Factors/Freedom%20of%20Thought.md) and support various other values.  As such, the means to provide a 'free VPS services' is impossible.  It could be funded by other people, but somehow the costs will need to be covered, and the work on \n[Currencies](../Core%20Services/Webizen%20Socio-Economics/Currencies.md) will be working towards better illustrating how these sorts of realities actually work.\n\nIf someone does years of unpaid work towards some sort of good objective, the implication is not that the useful derivatives of that persons work is free - there are costs, regardless of whether or not they're attributed in some way good, fair and reasonable.  Whilst the history of these practices, and the probability that these practices will pragmatically be inexorable (unadvoidable) in the near-future, the objective is to address this and other problems well, in the soonest possible timeframe.\n\nIn consideration also; there are often '*two (or more) competing realities*' that relate to complex choices.  For example; some may consider that users don't need their own domains, which means i might simply use some of the domains i've established to support the project; but what if i can't afford to provide those services anymore?  does that mean their systems will no-longer work? \n\nSo this concept of seeking to ensure [No Golden Handcuffs](../Core%20Services/Safety%20Protocols/Social%20Factors/No%20Golden%20Handcuffs.md), is in-turn bonded to various other [Values Credentials](../Core%20Services/Safety%20Protocols/Values%20Credentials.md).\n\n\n\n"},{"fields":{"slug":"/Implementation V1/Webizen 1.0/","title":"Webizen 1.0"},"frontmatter":{"draft":false},"rawBody":"Webizen 1.0 is a 'hello world' platform.  The details of the objectives are described in the [Implementation V1 Summary](Implementation%20V1%20Summary.md) document. \n\nWebizen 1.0 is intended to provide an ability to either sign-up to a webizen provider or create your own on your own VPS, and then using an installer package for your Windows/OSX/Linux desktop/laptop machines, with an installation helper, set-up a 'Webizen' Environment that provides the user an ability to make, use and maintain their own AI environment, that is able to be used in various ways through the use of apps designed using HTML5 (HTML/JS/CSS) that operate locally, or online on other devices by authentication into the users Webizen AI environment.\n\nWhilst future work seeks to radically enhance the AI capabilities, the webizen 1.0 environment objectives are to simply produce and deliver the basic networking and software runtime environment to produce and run software that is based upon the 'webizen specs', that will evolve overtime to support a variety of additional capabilities, that are sought to extend the capabilities.\n\nWebizen is a Propriatery Methodology, that supports the development and use of information systems in a particular way.  It seeks to employ, support and make available open-source technologies; with a view to augmenting the approach to support various objectives that seek to be done for the betterment of mankind; whilst also tackling difficult issues, that require an evaluated approach that is different to the way things have historically been made to work. \n\n"},{"fields":{"slug":"/Non-HTTP(s) Protocols/DAT/","title":"DAT"},"frontmatter":{"draft":false},"rawBody":"https://dat-ecosystem.org/ \n\nDAT protocol is now called HyperCore Protocol. \n\n(Unable to find go library - TBA)"},{"fields":{"slug":"/Non-HTTP(s) Protocols/GIT/","title":"GIT"},"frontmatter":{"draft":false},"rawBody":"https://github.com/go-git/go-git"},{"fields":{"slug":"/Non-HTTP(s) Protocols/Lightning Network/","title":"Lightning Network"},"frontmatter":{"draft":false},"rawBody":"The Lightning network provides an interface for performing transactions that are in-turn supported by the bitcoin protocol.  \n\nhttps://github.com/lightningnetwork/lnd\n\n"},{"fields":{"slug":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","title":"Non-HTTP(s) Protocols (& DLTs)"},"frontmatter":{"draft":false},"rawBody":"\nNon-HTTP(s) protocols that run over internet are various, some are 'blockchains' many are not; yet broadly, they're called Decentralised Ledger Tables.\n\nSome examples include;\n- [DAT](DAT.md)\n- [IPFS](IPFS.md)\n- [WebTorrent](WebTorrent.md)\n- [Lightning Network](Lightning%20Network.md)\n- [GUNECO](GUNECO.md)\n\nThese sorts of protocols are helpful for [Decentralised Ontologies](../Core%20Services/Decentralised%20Ontologies.md), [Permissive Commons](../Core%20Services/Permissive%20Commons.md), [Verifiable Claims & Credentials](../Core%20Services/Verifiable%20Claims%20&%20Credentials.md) and in-turn also support for various forms of [Currencies](../Core%20Services/Webizen%20Socio-Economics/Currencies.md) (including Micro-Payments).\n\nOthers, are still useful and important; but not DLTs, for example,\n\n- [GIT](GIT.md) as is used for GitHub / GitLab, etc. \n- [WebRTC](WebRTC.md) As is used for Live Audio/Video Streaming\n- [WebSockets](WebSockets.md) As is used for real-time notifications\n- [Email Services](../Host%20Service%20Requirements/Email%20Services.md) As does somewhat need to be supported somehow in the ecosystem.\n\nand there are many others. \n\nThe methods are not seeking to suggest that any particular protocol (ie: web3 stuff) shouldn't be supported; but rather, that the design of these webizen systems requires HTTP(s) support. \n\nHTTP based protocols are also alot faster than many DLT solutions (depending on networking issues, etc.).\n\n"},{"fields":{"slug":"/Webizen 2.0/AI Related Links & Notes/","title":"LOD-a-lot"},"frontmatter":{"draft":false},"rawBody":"# LOD-a-lot\n\nAn indexed and ready-to-consume crawl of a large portion of the LOD Cloud.\n\n[](http://lod-a-lot.lod.labs.vu.nl/#aboutLOD-a-lot)\nhttp://lod-a-lot.lod.labs.vu.nl/\n\nLOD Laundromat\nhttps://lodlaundromat.org/\n\nMD to Ontology\nhttps://ozekik.github.io/markdown-ld/\nhttps://github.com/ozekik/markdown-ld \n\nOntology to Documentation\nhttp://pylode.surroundaustralia.com/\n\nEasy RDF Converter\n\nhttps://www.easyrdf.org/converter"},{"fields":{"slug":"/Webizen 2.0/Webizen 2.0/","title":"Webizen 2.0"},"frontmatter":{"draft":false},"rawBody":"The Webizen 2.0 Project, is sought to be built using the Webizen 1.0 technology stack.  \n\nThe Webizen 2.0 components will include a physical hardware solution: [Webizen Pro Summary](Webizen%20Pro%20Summary.md).\n\nIt will also include works to radically improve the AI capabilities of the Webizen AI OS Platform."},{"fields":{"slug":"/Webizen 2.0/Webizen AI OS Platform/","title":"Webizen AI OS Platform"},"frontmatter":{"draft":false},"rawBody":"The Webizen AI OS Platform is a new type of operating system environment.  \n\nThe [Webizen Pro Summary](Webizen%20Pro%20Summary.md) describes a Hardware device that is powered by the Webizen AI OS Platform, and the apps that developers produce to work on it.  However there are also an array of  solutions that are sought to be achieved, which are intended to provide solutions that are cheaper to operate than a self-hosted wordpress site or similar. \n\n\n\n\n"},{"fields":{"slug":"/Webizen 2.0/Webizen Pro Summary/","title":"Webizen Pro Summary"},"frontmatter":{"draft":false},"rawBody":"The concept of Webizen Pro: is that it is a specific Hardware / Software Specification, that is designed to provide 'Artificial Intelligence' (term used broadly) capabilities that are designed to operate in a particular way.  In-effect, this provides some insights / description of the concept described of as 'webizen', that in-turn provides far greater value than any of its parts alone.\n\n\n"},{"fields":{"slug":"/Core Services/Safety Protocols/Safety Protocols/","title":"Safety Protocols"},"frontmatter":{"draft":false},"rawBody":"There is a balance between absolute security and secrecy / privacy vs. social needs, to uphold basic values.  The method considered most feasible to address this difficult balance, is emboided by the concept of creating Safety protocols.\n\nThe Safety Protocols requirement will end-up being defined in-full to be interoperable terms with by [The Webizen Charter](Social%20Factors/The%20Webizen%20Charter.md). \n\n#socialfabric #ValuesFrameworks #SafetyProtocols"},{"fields":{"slug":"/Core Services/Safety Protocols/Values Credentials/","title":"Values Credentials"},"frontmatter":{"draft":false},"rawBody":"Values Credentials are electronic instruments that can be used to support various types of relationships with other entities, agents, transactions and things. Values credentials are a type of\n[Verifiable Claims & Credentials](../Verifiable%20Claims%20&%20Credentials.md).\n\nThe term 'values' is intended to denote the concept of a persons (human being or guardian) 'human values' that need to be provided the apparatus to provide support for those values.\n\nThese tools, are considered to be of instrumental importance for the development of webizen as 'peace infrastructure' both as a project, and as a toolset for the production of other projects.\n\nThe Values Credentials, provide support for how other agents are able to be managed.  Rather than the concept of 'opt-in' or 'consent', the concept of 'values credentials' relates to 'human agency', and the ability and desirability for human beings as foundationally important active natural agents, to make wilful choices about how it is they want to organise their life; and in-turn also, how it is they're able to track and improve their own quality of being, and the growth of themselves as a person.  \n\nUsers are able to use values credentials, to help them curate how they interact with other agents; and can even provide support for excluding themselves from other agents who do not share some particular common-value that is of great importance to that person / agent.\n\nImportantly also, these systems are designed to support [Relationships (Social|[../Temporal Semantics](Social%20Factors/Relationships%20(Social)]].md), [Credentials & Contracts Manager](../../Implementation%20V1/App-design-sdk-v1/Core%20Apps/Credentials%20&%20Contracts%20Manager.md), [Agent Directory](../../Implementation%20V1/App-design-sdk-v1/Core%20Apps/Agent%20Directory.md)\n\n### Types of Values Credentials\n\n#### Human Rights related (universal values)\n\n- Universal Declaration of Human Rights\n- Code of Conduct for Law Enforcement Officials\n- Abolition of Forced Labour Convention, 1957 (No. 105)\n- Basic Principles and Guidelines on the Right to a Remedy and Reparation for Victims of Gross Violations of International Human Rights Law and Serious Violations of International Humanitarian Law\n- Basic Principles for the Treatment of Prisoners\n- Basic Principles on the Independence of the Judiciary\n- Basic Principles on the Role of Lawyers\n- Basic Principles on the Use of Force and Firearms by Law Enforcement Officials\n- Body of Principles for the Protection of All Persons under Any Form of Detention or Imprisonment\n- Convention against Discrimination in Education\n- Convention for the Suppression of the Traffic in Persons and of the Exploitation of the Prostitution of Others\n- Convention on Consent to Marriage, Minimum Age for Marriage and Registration of Marriages\n- Convention on the Elimination of All Forms of Discrimination against Women New York, 18 December 1979\n- Convention on the Non-Applicability of Statutory Limitations to War Crimes and Crimes Against Humanity\n- Convention on the Prevention and Punishment of the Crime of Genocide\n- Convention on the Reduction of Statelessness\n- Convention relating to the Status of Stateless Persons\n- Convention on the Rights of the Child\n- Convention relating to the Status of Refugees\n- Declaration of Basic Principles of Justice for Victims of Crime and Abuse of Power\n- Declaration of Commitment on HIV/AIDS\n- Declaration on Race and Racial Prejudice\n- Declaration on Social Progress and Development\n- Declaration on the Elimination of All Forms of Intolerance and of Discrimination Based on Religion or Belief\n- Declaration on the Elimination of Violence against Women\n- Declaration on the Granting of Independence to Colonial Countries and Peoples\n- Declaration on the Human Rights of Individuals who are not nationals of the country in which they live\n- Declaration on the Protection of All Persons from Being Subjected to Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment\n- Declaration on the Protection of all Persons from Enforced Disappearance\n- Declaration on the Protection of Women and Children in Emergency and Armed Conflict\n- Declaration on the Right and Responsibility of Individuals, Groups and Organs of Society to Promote and Protect Universally Recognized Human Rights and Fundamental Freedoms\n- Declaration on the Right of Peoples to Peace\n- Geneva Convention relative to the Protection of Civilian Persons in Time of War (2nd part)\n- Declaration on the Right to Development\n- Declaration on the Rights of Disabled Persons\n- Declaration on the Rights of Mentally Retarded Persons\n- Declaration on the Rights of Persons Belonging to National or Ethnic, Religious and Linguistic Minorities\n- Declaration on the Use of Scientific and Technological Progress in the Interests of Peace and for the Benefit of Mankind\n- Discrimination (Employment and Occupation) Convention, 1958 (No. 111)\n- Employment Policy Convention, 1964 (No. 122)\n- Equal Remuneration Convention, 1951 (No. 100)\n- Forced Labour Convention, 1930 (No. 29)\n- Freedom of Association and Protection of the Right to Organize Convention, 1948 (No. 87)\n- General Assembly resolution 1803 (XVII) of 14 December 1962, \"Permanent sovereignty over natural resources\"\n- Geneva Convention relative to the Treatment of Prisoners of War\n- Guidelines for Action on Children in the Criminal Justice System\n- Guidelines on the Role of Prosecutors\n- Indigenous and Tribal Peoples Convention, 1989 (No. 169)\n- International Convention against the Recruitment, Use, Financing and Training of Mercenaries\n- International Convention for the Protection of All Persons from Enforced Disappearance\n- International Convention on the Elimination of All Forms of Racial Discrimination\n- International Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families\n- International Covenant on Civil and Political Rights\n- Optional Protocol to the International Covenant on Civil and Political Rights\n- International Covenant on Economic, Social and Cultural Rights\n- Minimum Age Convention, 1973 (No. 138)\n- Optional Protocol to the Convention against Torture and other Cruel, Inhuman or Degrading Treatment or Punishment\n- Principles on the Effective Investigation and Documentation of Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment\n- Optional Protocol to the Convention on the Elimination of All Forms of Discrimination against Women\n- Optional Protocol to the Convention on the Rights of the Child on the sale of children, child prostitution and child pornography\n- Principles for the protection of persons with mental illness and the improvement of mental health care\n- Principles of international co-operation in the detection, arrest, extradition and punishment of persons guilty of war crimes and crimes against humanity\n- Principles of Medical Ethics relevant to the Role of Health Personnel, particularly Physicians, in the Protection of Prisoners and Detainees against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment\n- Principles on the Effective Prevention and Investigation of Extra-legal, Arbitrary and Summary Executions\n- Principles relating to the Status of National Institutions (The Paris Principles)\n- Protocol Additional to the Geneva Conventions of 12 August 1949, and relating to the Protection of Victims of International Armed Conflicts (Protocol 1)\n- Protocol Additional to the Geneva Conventions of 12 August 1949, and Relating to the Protection of Victims of Non-International Armed Conflicts (Protocol II)\n- Protocol against the Smuggling of Migrants by Land, Sea and Air, supplementing the United Nations Convention against Transnational Organized Crime\n- Protocol amending the Slavery Convention signed at Geneva on 25 September 1926\n- Protocol Instituting a Conciliation and Good Offices Commission to be responsible for seeking a settlement of any disputes which may arise between States Parties to the Convention against Discrimination in Education\n- Protocol of 2014 to the Forced Labour Convention, 1930\n- Protocol relating to the Status of Refugees\n- Protocol to Prevent, Suppress and Punish Trafficking in Persons Especially Women and Children, supplementing the United Nations Convention against Transnational Organized Crime\n- Recommendation on Consent to Marriage, Minimum Age for Marriage and Registration of Marriages\n- Right to Organise and Collective Bargaining Convention, 1949 (No. 98)\n- Rome Statute of the International Criminal Court\n- Safeguards guaranteeing protection of the rights of those facing the death penalty\n- Second Optional Protocol to the International Covenant on Civil and Political Rights, aiming at the abolition of the death penalty\n- Slavery Convention\n- Supplementary Convention on the Abolition of Slavery, the Slave Trade, and Institutions and Practices Similar to Slavery\n- Standard Rules on the Equalization of Opportunities for Persons with Disabilities\n- Statute of the International Criminal Tribunal for the Prosecution of Persons Responsible for Genocide and Other Serious Violations of International Humanitarian Law Committed in the Territory of Rwanda and Rwandan Citizens Responsible for Genocide and Other Such Violations Committed in the Territory of Neighbouring States, between 1 January 1994 and 31 December 1994\n- Statute of the International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of the Former Yugoslavia since 1991 (International Tribunal for the Former Yugoslavia)\n- United Nations Guidelines for the Prevention of Juvenile Delinquency (The Riyadh Guidelines)\n- United Nations Rules for the Protection of Juveniles Deprived of their Liberty\n- United Nations Millennium Declaration\n- United Nations Standard Minimum Rules for the Administration of Juvenile Justice (The Beijing Rules)\n- Vienna Declaration and Programme of Action\n- United Nations Principles for Older Persons\n- United Nations Rules for the Treatment of Women Prisoners and Non-custodial Measures for Women Offenders (the Bangkok Rules)\n- United Nations Standard Minimum Rules for Non-custodial Measures (The Tokyo Rules)\n- Universal Declaration on Cultural Diversity\n- Universal Declaration on the Eradication of Hunger and Malnutrition\n- Universal Declaration on the Human Genome and Human Rights\n- Worst Forms of Child Labour Convention, 1999 (No. 182)\n- Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment\n- Convention on the Elimination of All Forms of Discrimination against Women New York, 18 December 1979\n- Convention on the Rights of Persons with Disabilities\n- Convention on the Rights of the Child\n- International Convention for the Protection of All Persons from Enforced Disappearance\n- International Convention on the Elimination of All Forms of Racial Discrimination\n- International Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families\n- International Covenant on Civil and Political Rights\n- International Covenant on Economic, Social and Cultural Rights\n- Optional Protocol to the Convention against Torture and other Cruel, Inhuman or Degrading Treatment or Punishment\n- Optional Protocol to the Convention on the Elimination of All Forms of Discrimination against Women\n- Optional Protocol to the Convention on the Rights of Persons with Disabilities\n- Optional Protocol to the Convention on the Rights of the Child on a communications procedure\n- Optional Protocol to the Convention on the Rights of the Child on the involvement of children in armed conflict\n- Optional Protocol to the Convention on the Rights of the Child on the sale of children, child prostitution and child pornography\n- Optional Protocol to the International Covenant on Civil and Political Rights\n- Optional Protocol to the International Covenant on Economic, Social and Cultural Rights\n- Second Optional Protocol to the International Covenant on Civil and Political Rights, aiming at the abolition of the death penalty\n- Universal Declaration of Human Rights\n- Universal Declaration of Human Rights\n- Sustainable Development Goals\n\n#### Citizenship Related Values\n- Choice of Law (juristiction)\n- Local Laws\n\t- Economic\n\t- Social\n\t- Criminal\n\n#### Professional Charters\n- International code of ethics for occupational health professionals\n- DECLARATION OF GENEVA\n- International Code of Ethics for Professional Accountants\n- THE ICN CODE OF ETHICS FOR NURSES\n- International Code of Ethics for Midwives\n- International Bar Association: International Code of Ethics\n- Information Technology and Moral Values\n- ACM Code of Ethics\n- ITPA\n- AMA Code of Ethics\n- Medical Board Code of Ethics\n- Contributor Covenant\n- Law Society of NSW\n- Queensland Law Society\n- Law Council of Australia\n- APS Code of Conduct\n\n\n#socialfabric #ValuesFrameworks #SafetyProtocols"},{"fields":{"slug":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","title":"Biosphere Ontologies"},"frontmatter":{"draft":false},"rawBody":"![Biosphere Systems from WikiPedia](https://upload.wikimedia.org/wikipedia/commons/b/b9/Biosphere_system.png)\n\nThe Work on [Biosphere](https://en.wikipedia.org/wiki/Biosphere) Ontologies is interactive with [Currencies](Currencies.md) and in-turn also [Values Credentials](../Safety%20Protocols/Values%20Credentials.md) and [Safety Protocols](../Safety%20Protocols/Safety%20Protocols.md).  \n\nAll members of Humanity are part of our biosphere.  Whilst there has been a significant push towards the creation of a 'carbon economy', there are various other factors related to our means to support the development of systems that support rejuvative repercussive effects, in our biosphere.\n\nSo the concept of works in this area, is to produce ecosystem solutions that provide the option for users to make use of various kinds of biosphere ontology features and functions, to get a better grasp on their relationship with our biosphere.  \n\nThis area of work will seek to take into account various different fields of biosphere related metrix.\n\n## Energy Lifecycle\n\n['Cradle-to-grave'](https://www.eea.europa.eu/help/glossary/eea-glossary/cradle-to-grave)assessment considers impacts at each stage of a product's life-cycle, from the time natural resources are extracted from the ground and processed through each subsequent stage of manufacturing, transportation, product use, and ultimately, disposal. \n\n[Energy Life-Cycle Cost Analysis (ELCCA)](ELCCA)) is **a decision-making tool that compares owning and operating costs for energy using systems in new and remodeled facilities**. The ELCCA provides a method for the owner to evaluate different energy using systems and select the most cost-effective.\n\n## Total cost of ownership\n[**Total cost of ownership** (**TCO**)](**TCO**)) is a financial estimate intended to help buyers and owners determine the direct and [indirect costs](https://en.wikipedia.org/wiki/Indirect_costs \"Indirect costs\") of a product or service. It is a [management accounting](https://en.wikipedia.org/wiki/Management_accounting \"Management accounting\") concept that can be used in [full cost accounting](https://en.wikipedia.org/wiki/Full_cost_accounting \"Full cost accounting\") or even [ecological economics](https://en.wikipedia.org/wiki/Ecological_economics \"Ecological economics\") where it includes [social costs](https://en.wikipedia.org/wiki/Social_cost \"Social cost\").\n\nFor manufacturing, as TCO is typically compared with doing business overseas, it goes beyond the initial manufacturing cycle time and cost to make parts. TCO includes a variety of cost of doing business items, for example, ship and re-ship, and opportunity costs, while it also considers incentives developed for an alternative approach. Incentives and other variables include tax credits, common language, expedited delivery, and customer-oriented supplier visits."},{"fields":{"slug":"/Core Services/Webizen Socio-Economics/Centricity/","title":"Centricity"},"frontmatter":{"draft":false},"rawBody":"\nThe Concept of 'centricity' refers to structuring a way of thinking about the socially-interactive semantics layers.  This conceptual framework seeks to provide support for the values frameworks the webizen environments are built to support.  In-effect, providing some of the foundational considerations or 'tenants' to a form of moral framework, that helps to inform design requirements.\n\n### Human Centric\nThe development of any tools, whether it be companies or things; is intended to support the needs of humanity and/or members of our human family, and through us, our Social Spheres([SocioSphere Ontologies](SocioSphere%20Ontologies.md)) and the use of resources, our role in our Biosphere ([Biosphere Ontologies](Biosphere%20Ontologies.md)).  No other form of agent can be sent to prison.  The responsibility for what occurs in our world is fundamentally held by Human Beings.  \n\nWhilst there are always complexities involved in almost any circumstance and indeed also the evaluation of that circumstance by others; the fundamental role of technology is to provide useful tools for human beings, to improve and manage their relationships with other agents; both as an active natural agent, and as an observer.  \n\n### Agent Centric\n\nThe term Agent refers to things that do stuff. A well known sub-class is Person, representing people. Other kinds of agents include Organizations, Groups, software and other 'things'.\n\nIt is noted that in some juristictions the term 'person' may refer to a natural person (human being) or a 'legal personality' (Incorporated Entity (such as a company) / legal person). \n\nIn legal terms, an agent is defined to mean;\n*One who agrees and is authorized to act on behalf of another, a principal, to legally bind an individual in particular business transactions with third parties pursuant to an agency relationship.*\n\nSource: https://legal-dictionary.thefreedictionary.com/agent\n\nAs such, the term 'agent' is intended to provide a distinction between the entity, person or 'actor' (ie: software / \"Ai Agent\") that is performing some sort of task or activity; and the role in which that agent is undertaking that task.  \n\nWhilst there are many senarios where the concept of Agent Centric semantics apply, one example is;\n\n*a natural person in their own private capacity may hold some particular belief or moral position about something, circumstance, etc.  If they are acting on behalf of an employer or other entity; they may be required to carry out tasks in a particular way that may not be of their own choosing.*  \n\n### Entity Centric\n\nEntity Centric refers to legal entities, whether they be individual natural persons or legal persons (incorporated entities, government entities, etc.).  \n\nThis is interactive with both agent and human centric semantics; whilst seeking to support a structural framework around defining how systems are developed to support a means to associate the activities, rights and responsibilities of an entity as does exist in our offline socio-economic world, to be supported in being able to maintain the same principles based structure online, in webizen environments. \n\n\n"},{"fields":{"slug":"/Core Services/Webizen Socio-Economics/Currencies/","title":"Currencies"},"frontmatter":{"draft":false},"rawBody":"The Webizen Stack intends to produce support for an array of different 'currencies', whereby the term itself isn't simply or solely about money, but also various other factors relating to a broader semantic concept of socio-economic and biosphere interactions, in particular (there are others).\n\nSupport for Micropayments is incredibly important, yet the modelling for how micropayments are achieved also needs to take into consideration the cost of energy and other supply / distribution chain related costing frameworks. \n\nSimilar yet differently, are various forms of 'social currencies', which on the present-day web are fairly simple; such as how many views or likes or 'friends' an 'account' has; yet, it may be a bot that has been set-up for socially narfarious purposes.\n\nIn other parts of our social fabric - there's issues where it is not easy for people to declare and communicate that they've been going down some difficult path of trying to talk about an important topic with integrity; and that they've been harmed for doing so during a time where others didn't understand, and when others start to learn about the realities of whatever it is they were talking about earlier (getting harmed); then, there's no benefit - others take over, whilst their 'social reputation' is still damaged for having spoken about it earlier when the statements were unpopular.  Conversely also; people who make popular statements, that are later found false.\n\nSome of these considerations are explored further in the document about [Social Attack Vectors](../Safety%20Protocols/Social%20Factors/Social%20Attack%20Vectors.md) alongside the objective outline about [Ending Digital Slavery](../Safety%20Protocols/Social%20Factors/Ending%20Digital%20Slavery.md).\n\nAnother area of interest is about 'learning' or 'knowledge' currencies', whereby the activities that people are able to be shown to have both done and understood; creates some sort of acknowledgement benefit, that may help them better qualify themselves for particular kinds of activities (ie: work).\n\nThere are many different types of 'currencies' and part of the webizen project / process, is to explore how they may be made to work - for the betterment of human kind. \n\n\n\n### Considerations about AI & Human Dignity (inc. privacy)\n\nWhilst many are fearful of a 'social credit score' or similar; the reality is that these systems already exist, its just that they're operating in a way that's very poor / morally improveraged. \n\nWhether or not 'consumers' are aware of the vast amount of information about people that is collected by alternative systems, website and web-related services; the difference between exposing and modelling how these sorts of things could (or 'should') work, is actually far better than setting it aside - which doesn't mean others aren't aware of these metrics, rather, that the 'data subject' (the target) is unaware of it.   So the objective is to turn that around, so that the people who the statistics or (social) currencies relate to - are more aware of it, than any other.\n\n\n"},{"fields":{"slug":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","title":"SocioSphere Ontologies"},"frontmatter":{"draft":false},"rawBody":"The Concept of Sociosphere Ontologies is similar and interactive with [Biosphere Ontologies](Biosphere%20Ontologies.md), in that the social behaviours of what people do (or fail to do) or neglect, have impacts upon our sociology. \n\nPresently, many electronically interactive socio-economic systems prioritise the value of claims over whether or not they've been categorised appropriately or indeed also, if intimated to be 'facts', whether or not they're actually true.   \n\nThere are many interactive concepts and implications that delineate the relationships between activities and inferences; that is the causal relationships between natural agents, and observers; and these natural systems have [Relationships (Social|[../Temporal Semantics](../Safety%20Protocols/Social%20Factors/Relationships%20(Social)]].md)."},{"fields":{"slug":"/Core Technologies/AUTH/Authentication Fabric/","title":"Authentication Fabric"},"frontmatter":{"draft":false},"rawBody":"(incomplete - to be updated)\nThe Authentication Fabric is complicated and involves many parts, that are intended to work as an ecosystem.  \n\nThe effect of the Authentication Fabric is intended to result in a curatable, highly secure & accountability 'fabric' with attribute based access control; that becomes managable via the query interface (alongside more manual interfaces). \n\nThere's a couple of different layers involved in this Authentication Fabric.\n\n### Authentication of Devices\nA variety of host-fingerprinting information is employed and thereafter supported via [WebID](../Webizen%20App%20Spec/SemWebSpecs/W3C%20Specifications/WebID.md) [WebID-TLS](../Webizen%20App%20Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS.md), [WebID-TLS](../Webizen%20App%20Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS.md) and similar.  \n\nNB: https://github.com/jaypipes/ghw\n \n### Network Authentication tooling\nThe \"Network\" Authentication fabric makes use of IPv4/IPv6 (preferred) tooling, alongside DNS, TLS, [Verifiable Claims & Credentials](../../Core%20Services/Verifiable%20Claims%20&%20Credentials.md) and [WebID-OIDC](../Webizen%20App%20Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC.md)\n\n### Agent Authentication Tooling\nAgent based authentication links with the semantic authentication chain, alongside [WebID-OIDC](../Webizen%20App%20Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC.md) and [Verifiable Claims & Credentials](../../Core%20Services/Verifiable%20Claims%20&%20Credentials.md) alongside tools such as [WebAuthn](../Webizen%20App%20Spec/WebSpec/WebPlatformTools/WebAuthn.md). \n\nNOTE\nThis document is incomplete.  There's a bunch more that ends-up going into the ecosystem that ends-up supporting a form of safety / security 'fabric' that (afaik) is unlike others.\n\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","title":"Webizen App Spec 1.0"},"frontmatter":{"draft":false},"rawBody":"The Webizen Application Specifications seek to use [HTML SPECS](WebSpec/HTML%20SPECS.md) in a manner that is supported by a [Web Of Data](https://www.w3.org/2013/data/) Approach that is in-turn extended to support the use of non-web-based protocols, for various express purposes.\n\n"},{"fields":{"slug":"/Database requirements/Database Alternatives/CayleyGraph/","title":"CayleyGraph"},"frontmatter":{"draft":false},"rawBody":"Cayley is an open-source database for [Linked Data](https://www.w3.org/standards/semanticweb/data). It is inspired by the graph database behind Google's [Knowledge Graph](https://en.wikipedia.org/wiki/Knowledge_Graph) (formerly [Freebase](https://en.wikipedia.org/wiki/Freebase_(database))).\nhttps://cayley.io/\n- https://cayley.io/\n- https://github.com/cayleygraph\n- https://github.com/cayleygraph/quad\n"},{"fields":{"slug":"/Database requirements/Database Alternatives/akutan/","title":"Akutan"},"frontmatter":{"draft":false},"rawBody":"https://github.com/eBay/akutan\nAkutan is a distributed knowledge graph store, sometimes called an RDF store or a triple store. Knowledge graphs are suitable for modeling data that is highly interconnected by many types of relationships, like encyclopedic information about the world. A knowledge graph store enables rich queries on its data, which can be used to power real-time interfaces, to complement machine learning applications, and to make sense of new, unstructured information in the context of the existing knowledge."},{"fields":{"slug":"/Database requirements/Database methods/GraphQL/","title":"GraphQL"},"frontmatter":{"draft":false},"rawBody":"Most JSON (or JSON-LD) related app developers use and/or understand GraphQL (not sparql). "},{"fields":{"slug":"/Database requirements/Database methods/Sparql/","title":"Sparql"},"frontmatter":{"draft":false},"rawBody":" [Sparql Family](../../Core%20Technologies/Webizen%20App%20Spec/SemWebSpecs/Sparql/Sparql%20Family.md) is the intended native method for the platform."},{"fields":{"slug":"/Host Service Requirements/Media Processing/ffmpeg/","title":"Ffmpeg"},"frontmatter":{"draft":false},"rawBody":"https://ffmpeg.org/\n\nhttps://pkg.go.dev/search?q=ffmpeg\n"},{"fields":{"slug":"/Host Service Requirements/Media Processing/opencv/","title":"Opencv"},"frontmatter":{"draft":false},"rawBody":"https://gocv.io/\n\nhttps://opencv.org/"},{"fields":{"slug":"/ICT Stack/General References/List of Protocols ISO model/","title":"List of Protocols ISO Model"},"frontmatter":{"draft":false},"rawBody":"## Layer 1 ([physical layer](https://en.wikipedia.org/wiki/Physical_layer))\n\n-   Telephone network [modems](https://en.wikipedia.org/wiki/Modems \"Modems\")\n-   [IrDA](https://en.wikipedia.org/wiki/Infrared_Data_Association \"Infrared Data Association\") physical layer\n-   [USB](https://en.wikipedia.org/wiki/USB \"USB\") physical layer\n-   [EIA](https://en.wikipedia.org/wiki/Electronic_Industries_Alliance \"Electronic Industries Alliance\") [RS-232](https://en.wikipedia.org/wiki/RS-232 \"RS-232\"), [EIA-422](https://en.wikipedia.org/wiki/EIA-422 \"EIA-422\"), [EIA-423](https://en.wikipedia.org/wiki/RS-423 \"RS-423\"), [RS-449](https://en.wikipedia.org/wiki/RS-449 \"RS-449\"), [RS-485](https://en.wikipedia.org/wiki/RS-485 \"RS-485\")\n-   [Ethernet physical layer](https://en.wikipedia.org/wiki/Ethernet_physical_layer \"Ethernet physical layer\") [10BASE-T](https://en.wikipedia.org/wiki/10BASE-T \"10BASE-T\"), [10BASE2](https://en.wikipedia.org/wiki/10BASE2 \"10BASE2\"), [10BASE5](https://en.wikipedia.org/wiki/10BASE5 \"10BASE5\"), [100BASE-TX](https://en.wikipedia.org/wiki/100BASE-TX \"100BASE-TX\"), [100BASE-FX](https://en.wikipedia.org/wiki/100BASE-FX \"100BASE-FX\"), [1000BASE-T](https://en.wikipedia.org/wiki/1000BASE-T \"1000BASE-T\"), [1000BASE-SX](https://en.wikipedia.org/wiki/1000BASE-SX \"1000BASE-SX\") and other varieties\n-   Varieties of [802.11](https://en.wikipedia.org/wiki/802.11 \"802.11\") [Wi-Fi](https://en.wikipedia.org/wiki/Wi-Fi \"Wi-Fi\") physical layers\n-   [DSL](https://en.wikipedia.org/wiki/Digital_subscriber_line \"Digital subscriber line\")\n-   [ISDN](https://en.wikipedia.org/wiki/Integrated_Services_Digital_Network \"Integrated Services Digital Network\")\n-   T1 and other [T-carrier](https://en.wikipedia.org/wiki/T-carrier \"T-carrier\") links, and E1 and other [E-carrier](https://en.wikipedia.org/wiki/E-carrier \"E-carrier\") links\n-   [ITU](https://en.wikipedia.org/wiki/International_Telecommunication_Union \"International Telecommunication Union\") Recommendations: see [ITU-T](https://en.wikipedia.org/wiki/ITU-T \"ITU-T\")\n-   [IEEE 1394 interfaces](https://en.wikipedia.org/wiki/IEEE_1394_interface \"IEEE 1394 interface\")\n-   [TransferJet](https://en.wikipedia.org/wiki/TransferJet \"TransferJet\")\n-   [Etherloop](https://en.wikipedia.org/wiki/Etherloop \"Etherloop\")\n-   [ARINC 818](https://en.wikipedia.org/wiki/ARINC_818 \"ARINC 818\") Avionics Digital Video Bus\n-   [G.hn](https://en.wikipedia.org/wiki/G.hn \"G.hn\")/[G.9960](https://en.wikipedia.org/wiki/G.9960 \"G.9960\") physical layer\n-   [CAN bus](https://en.wikipedia.org/wiki/CAN_bus \"CAN bus\") (controller area network) physical layer\n-   [Mobile Industry Processor Interface](https://en.wikipedia.org/wiki/Mobile_Industry_Processor_Interface \"Mobile Industry Processor Interface\") physical layer\n-   [Infrared](https://en.wikipedia.org/wiki/Infrared \"Infrared\")\n-   Frame Relay\n-   FO Fiber optics\n-   [X.25](https://en.wikipedia.org/wiki/X.25 \"X.25\")\n\n## Layer 2 ([data link layer](https://en.wikipedia.org/wiki/Data_link_layer))\n\n-   [ARCnet](https://en.wikipedia.org/wiki/ARCnet \"ARCnet\") Attached Resource Computer NETwork\n-   [ARP](https://en.wikipedia.org/wiki/Address_Resolution_Protocol \"Address Resolution Protocol\") Address Resolution Protocol\n-   [ATM](https://en.wikipedia.org/wiki/Asynchronous_Transfer_Mode \"Asynchronous Transfer Mode\") Asynchronous Transfer Mode\n-   [CHAP](https://en.wikipedia.org/wiki/Challenge-Handshake_Authentication_Protocol \"Challenge-Handshake Authentication Protocol\") Challenge Handshake Authentication Protocol\n-   [CDP](https://en.wikipedia.org/wiki/Cisco_Discovery_Protocol \"Cisco Discovery Protocol\") Cisco Discovery Protocol\n-   DCAP Data Link Switching Client Access Protocol\n-   [Distributed Multi-Link Trunking](https://en.wikipedia.org/wiki/Distributed_Multi-Link_Trunking \"Distributed Multi-Link Trunking\")\n-   [Distributed Split Multi-Link Trunking](https://en.wikipedia.org/wiki/Distributed_Split_Multi-Link_Trunking \"Distributed Split Multi-Link Trunking\")\n-   DTP [Dynamic Trunking Protocol](https://en.wikipedia.org/wiki/Dynamic_Trunking_Protocol \"Dynamic Trunking Protocol\")\n-   [Econet](https://en.wikipedia.org/wiki/Econet \"Econet\")\n-   [Ethernet](https://en.wikipedia.org/wiki/Ethernet \"Ethernet\")\n-   [FDDI](https://en.wikipedia.org/wiki/Fiber_distributed_data_interface \"Fiber distributed data interface\") Fiber Distributed Data Interface\n-   [Frame Relay](https://en.wikipedia.org/wiki/Frame_Relay \"Frame Relay\")\n-   [ITU-T](https://en.wikipedia.org/wiki/ITU-T \"ITU-T\") [G.hn](https://en.wikipedia.org/wiki/G.hn \"G.hn\")\n-   [HDLC](https://en.wikipedia.org/wiki/High-Level_Data_Link_Control \"High-Level Data Link Control\") High-Level Data Link Control\n-   [IEEE 802.11](https://en.wikipedia.org/wiki/IEEE_802.11 \"IEEE 802.11\") WiFi\n-   [IEEE 802.16](https://en.wikipedia.org/wiki/IEEE_802.16 \"IEEE 802.16\") WiMAX\n-   [LACP](https://en.wikipedia.org/wiki/Link_Aggregation_Control_Protocol \"Link Aggregation Control Protocol\") Link Aggregation Control Protocol\n-   [LattisNet](https://en.wikipedia.org/wiki/LattisNet \"LattisNet\")\n-   [LocalTalk](https://en.wikipedia.org/wiki/LocalTalk \"LocalTalk\")\n-   [L2F](https://en.wikipedia.org/wiki/L2F \"L2F\") Layer 2 Forwarding Protocol\n-   [L2TP](https://en.wikipedia.org/wiki/L2TP \"L2TP\") Layer 2 Tunneling Protocol\n-   [LLDP](https://en.wikipedia.org/wiki/Link_Layer_Discovery_Protocol \"Link Layer Discovery Protocol\") Link Layer Discovery Protocol\n-   [LLDP-MED](https://en.wikipedia.org/wiki/LLDP-MED \"LLDP-MED\") Link Layer Discovery Protocol - Media Endpoint Discovery\n-   [MAC](https://en.wikipedia.org/wiki/Media_Access_Control \"Media Access Control\") Media Access Control\n-   [Q.710](https://en.wikipedia.org/wiki/Message_Transfer_Part \"Message Transfer Part\") Simplified [Message Transfer Part](https://en.wikipedia.org/wiki/Message_Transfer_Part \"Message Transfer Part\")\n-   [Multi-link trunking](https://en.wikipedia.org/wiki/Multi-link_trunking \"Multi-link trunking\") Protocol\n-   [NDP](https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol \"Neighbor Discovery Protocol\") Neighbor Discovery Protocol\n-   [PAgP](https://en.wikipedia.org/wiki/Port_Aggregation_Protocol \"Port Aggregation Protocol\") - Cisco Systems proprietary link aggregation protocol\n-   [PPP](https://en.wikipedia.org/wiki/Point-to-Point_Protocol \"Point-to-Point Protocol\") Point-to-Point Protocol\n-   [PPTP](https://en.wikipedia.org/wiki/Point-to-point_tunneling_protocol \"Point-to-point tunneling protocol\") Point-to-Point Tunneling Protocol\n-   [PAP](https://en.wikipedia.org/wiki/Password_Authentication_Protocol \"Password Authentication Protocol\") Password Authentication Protocol\n-   [RPR](https://en.wikipedia.org/wiki/Resilient_Packet_Ring \"Resilient Packet Ring\") IEEE 802.17 Resilient Packet Ring\n-   [SLIP](https://en.wikipedia.org/wiki/Serial_Line_Internet_Protocol \"Serial Line Internet Protocol\") Serial Line Internet Protocol (obsolete)\n-   [StarLAN](https://en.wikipedia.org/wiki/StarLAN \"StarLAN\")\n-   Space Data Link Protocol, one of the norms for Space Data Link from the [Consultative Committee for Space Data Systems](https://en.wikipedia.org/wiki/Consultative_Committee_for_Space_Data_Systems \"Consultative Committee for Space Data Systems\")\n-   [STP](https://en.wikipedia.org/wiki/Spanning_Tree_Protocol \"Spanning Tree Protocol\") Spanning Tree Protocol\n-   [Split multi-link trunking](https://en.wikipedia.org/wiki/Split_multi-link_trunking \"Split multi-link trunking\") Protocol\n-   [Token Ring](https://en.wikipedia.org/wiki/Token_Ring \"Token Ring\") a protocol developed by IBM; the name can also be used to describe the [token passing](https://en.wikipedia.org/wiki/Token_passing \"Token passing\") ring logical topology that it popularized.\n-   [Virtual Extended Network (VEN)](VEN))&action=edit&redlink=1 \"Virtual Extended Network (VEN) (page does not exist)\") a protocol developed by iQuila.\n-   [VTP](https://en.wikipedia.org/wiki/VTP \"VTP\") VLAN Trunking Protocol\n-   [VLAN](https://en.wikipedia.org/wiki/VLAN \"VLAN\") Virtual Local Area Network\n\n## Network Topology\n\n-   [Asynchronous Transfer Mode](https://en.wikipedia.org/wiki/Asynchronous_Transfer_Mode \"Asynchronous Transfer Mode\") (ATM)\n-   [IS-IS](https://en.wikipedia.org/wiki/IS-IS \"IS-IS\"), Intermediate System - Intermediate System (OSI)\n-   [SPB](https://en.wikipedia.org/wiki/Shortest_Path_Bridging \"Shortest Path Bridging\") Shortest Path Bridging\n-   [MTP](https://en.wikipedia.org/wiki/Message_Transfer_Part \"Message Transfer Part\") Message Transfer Part\n-   [NSP](https://en.wikipedia.org/wiki/Signalling_Connection_Control_Part \"Signalling Connection Control Part\") Network Service Part\n-   [TRILL](https://en.wikipedia.org/wiki/TRILL \"TRILL\") (TRansparent Interconnection of Lots of Links)\n\n## Layer 2.5\n\n-   [ARP](https://en.wikipedia.org/wiki/Address_Resolution_Protocol \"Address Resolution Protocol\") Address Resolution Protocol\n-   [MPLS](https://en.wikipedia.org/wiki/Multiprotocol_Label_Switching \"Multiprotocol Label Switching\") Multiprotocol Label Switching\n-   [PPPoE](https://en.wikipedia.org/wiki/Point-to-Point_Protocol_over_Ethernet \"Point-to-Point Protocol over Ethernet\") Point-to-Point Protocol over Ethernet\n-   [TIPC](https://en.wikipedia.org/wiki/Transparent_Inter-process_Communication \"Transparent Inter-process Communication\") Transparent Inter-process Communication\n\n## Layer 3 ([Network Layer](https://en.wikipedia.org/wiki/Network_layer))\n\n-   [CLNP](https://en.wikipedia.org/wiki/CLNP \"CLNP\") Connectionless Networking Protocol\n-   [IPX](https://en.wikipedia.org/wiki/IPX \"IPX\") Internetwork Packet Exchange\n-   [NAT](https://en.wikipedia.org/wiki/Network_address_translation \"Network address translation\") Network Address Translation\n-   [Routed-SMLT](https://en.wikipedia.org/wiki/R-SMLT \"R-SMLT\")\n-   [SCCP](https://en.wikipedia.org/wiki/Signalling_Connection_Control_Part \"Signalling Connection Control Part\") Signalling Connection Control Part\n-   AppleTalk DDP\n-   [HSRP](https://en.wikipedia.org/wiki/Hot_Standby_Router_Protocol \"Hot Standby Router Protocol\") Hot Standby Router protocol\n-   [VRRP](https://en.wikipedia.org/wiki/Virtual_Router_Redundancy_Protocol \"Virtual Router Redundancy Protocol\") Virtual Router Redundancy Protocol\n-   IP [Internet Protocol](https://en.wikipedia.org/wiki/Internet_Protocol \"Internet Protocol\")\n-   [ICMP](https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol \"Internet Control Message Protocol\") Internet Control Message Protocol\n-   [ARP](https://en.wikipedia.org/wiki/Address_Resolution_Protocol \"Address Resolution Protocol\") Address Resolution Protocol\n-   RIP [Routing Information Protocol](https://en.wikipedia.org/wiki/Routing_Information_Protocol \"Routing Information Protocol\") (v1 and v2)\n-   OSPF [Open Shortest Path First](https://en.wikipedia.org/wiki/Open_Shortest_Path_First \"Open Shortest Path First\") (v1 and v2)\n-   IPSEC [IPsec](https://en.wikipedia.org/wiki/IPsec \"IPsec\")\n\n## Layer 3+4 (Protocol Suites)\n\n-   [AppleTalk](https://en.wikipedia.org/wiki/AppleTalk \"AppleTalk\")\n-   [DECnet](https://en.wikipedia.org/wiki/DECnet \"DECnet\")\n-   [IPX/SPX](https://en.wikipedia.org/wiki/IPX/SPX \"IPX/SPX\")\n-   [Internet Protocol Suite](https://en.wikipedia.org/wiki/Internet_Protocol_Suite \"Internet Protocol Suite\")\n-   [Xerox Network Systems](https://en.wikipedia.org/wiki/Xerox_Network_Systems \"Xerox Network Systems\")\n\n## Layer 4 ([Transport Layer](https://en.wikipedia.org/wiki/Transport_layer))\n\n-   [AEP](https://en.wikipedia.org/wiki/AppleTalk_Echo_Protocol \"AppleTalk Echo Protocol\") AppleTalk Echo Protocol\n-   [AH](https://en.wikipedia.org/wiki/Authentication_Header \"Authentication Header\") Authentication Header over IP or IPSec\n-   [DCCP](https://en.wikipedia.org/wiki/Datagram_Congestion_Control_Protocol \"Datagram Congestion Control Protocol\") Datagram Congestion Control Protocol\n-   [ESP](https://en.wikipedia.org/wiki/Encapsulating_Security_Payload \"Encapsulating Security Payload\") Encapsulating Security Payload over IP or IPSec\n-   [FCP](https://en.wikipedia.org/wiki/Fibre_Channel_Protocol \"Fibre Channel Protocol\") Fibre Channel Protocol\n-   [NetBIOS](https://en.wikipedia.org/wiki/NetBIOS \"NetBIOS\") NetBIOS, File Sharing and Name Resolution\n-   [IL](https://en.wikipedia.org/wiki/IL_Protocol \"IL Protocol\") Originally developed as transport layer for [9P](https://en.wikipedia.org/wiki/9P_(protocol) \"9P (protocol)\")\n-   [iSCSI](https://en.wikipedia.org/wiki/ISCSI \"ISCSI\") Internet Small Computer System Interface\n-   [NBF](https://en.wikipedia.org/wiki/NetBIOS_Frames \"NetBIOS Frames\") NetBIOS Frames protocol\n-   [SCTP](https://en.wikipedia.org/wiki/Stream_Control_Transmission_Protocol \"Stream Control Transmission Protocol\") Stream Control Transmission Protocol\n-   [Sinec H1](https://en.wikipedia.org/wiki/Sinec_H1 \"Sinec H1\") for telecontrol\n-   [TUP](https://en.wikipedia.org/wiki/Telephone_User_Part \"Telephone User Part\"), Telephone User Part\n-   [SPX](https://en.wikipedia.org/wiki/IPX/SPX \"IPX/SPX\") Sequenced Packet Exchange\n-   [](https://en.wikipedia.org/wiki/AppleTalk#Name_Binding_Protocol%20%22AppleTalk%22) Name Binding Protocol {for AppleTalk}\n-   TCP [Transmission Control Protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol \"Transmission Control Protocol\")\n-   UDP [User Datagram Protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol \"User Datagram Protocol\")\n-   [QUIC](https://en.wikipedia.org/wiki/QUIC \"QUIC\")\n\n## Layer 5 ([Session Layer](https://en.wikipedia.org/wiki/Session_layer))\n\nThis layer, presentation Layer and application layer are combined in [TCP/IP model](https://en.wikipedia.org/wiki/TCP/IP_model \"TCP/IP model\").\n\n-   [9P](https://en.wikipedia.org/wiki/9P_(protocol) \"9P (protocol)\") Distributed file system protocol developed originally as part of [Plan 9](https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs \"Plan 9 from Bell Labs\")\n-   ADSP AppleTalk Data Stream Protocol\n-   ASP AppleTalk Session Protocol\n-   [H.245](https://en.wikipedia.org/wiki/H.245 \"H.245\") Call Control Protocol for Multimedia Communications\n-   [iSNS](https://en.wikipedia.org/wiki/ISNS \"ISNS\") Internet Storage Name Service\n-   [NetBIOS](https://en.wikipedia.org/wiki/NetBIOS \"NetBIOS\"), File Sharing and Name Resolution protocol - the basis of file sharing with Windows.\n-   [NetBEUI](https://en.wikipedia.org/wiki/NetBEUI \"NetBEUI\"), NetBIOS Enhanced User Interface\n-   [NCP](https://en.wikipedia.org/wiki/NetWare_Core_Protocol \"NetWare Core Protocol\") NetWare Core Protocol\n-   [PAP](http://mirror.informatimago.com/next/developer.apple.com/documentation/mac/NetworkingOT/NetworkingWOT-75.html) Printer Access Protocol\n-   [RPC](https://en.wikipedia.org/wiki/Remote_procedure_call \"Remote procedure call\") Remote Procedure Call\n-   [RTCP](https://en.wikipedia.org/wiki/RTP_Control_Protocol \"RTP Control Protocol\") RTP Control Protocol\n-   [SDP](https://en.wikipedia.org/wiki/Sockets_Direct_Protocol \"Sockets Direct Protocol\") Sockets Direct Protocol\n-   [SMB](https://en.wikipedia.org/wiki/Server_message_block \"Server message block\") Server Message Block\n-   [SMPP](https://en.wikipedia.org/wiki/Short_Message_Peer-to-Peer \"Short Message Peer-to-Peer\") Short Message Peer-to-Peer\n-   [SOCKS](https://en.wikipedia.org/wiki/SOCKS \"SOCKS\") \"SOCKetS\"\n-   [](https://en.wikipedia.org/wiki/AppleTalk#Zone_Information_Protocol%20%22AppleTalk%22) Zone Information Protocol {For AppleTalk}\n-   This layer provides session management capabilities between hosts. For example, if some host needs a password verification for access and if credentials are provided then for that session password verification does not happen again. This layer can assist in synchronization, dialog control and critical operation management (e.g., an online bank transaction).\n\n## Layer 6 ([Presentation Layer](https://en.wikipedia.org/wiki/Presentation_layer \"Presentation layer\"))\n\n-   [TLS](https://en.wikipedia.org/wiki/Transport_Layer_Security \"Transport Layer Security\") Transport Layer Security[2](2)(https://en.wikipedia.org/wiki/List_of_network_protocols_(OSI_model)#cite_note-:0-2)\n-   [SSL](https://en.wikipedia.org/wiki/Secure_Socket_Tunneling_Protocol \"Secure Socket Tunneling Protocol\") Secure Socket Tunneling[2](2)(https://en.wikipedia.org/wiki/List_of_network_protocols_(OSI_model)#cite_note-:0-2)\n-   [AFP](https://en.wikipedia.org/wiki/Apple_Filing_Protocol \"Apple Filing Protocol\") Apple Filing Protocol[2](2)(https://en.wikipedia.org/wiki/List_of_network_protocols_(OSI_model)#cite_note-:0-2)\n-   Independent Computing Architecture (ICA), the Citrix system core protocol\n-   Lightweight Presentation Protocol (LPP)[2](2)(https://en.wikipedia.org/wiki/List_of_network_protocols_(OSI_model)#cite_note-:0-2)\n-   NetWare Core Protocol (NCP)\n-   Network Data Representation (NDR)[2](2)(https://en.wikipedia.org/wiki/List_of_network_protocols_(OSI_model)#cite_note-:0-2)\n-   Tox, The Tox protocol is sometimes regarded as part of both the presentation and application layer\n-   eXternal Data Representation (XDR)[2](2)(https://en.wikipedia.org/wiki/List_of_network_protocols_(OSI_model)#cite_note-:0-2)\n-   X.25 Packet Assembler/Disassembler Protocol (PAD)\n\n## Layer 7 ([Application Layer](https://en.wikipedia.org/wiki/Application_layer \"Application layer\"))\n\nSee Link for a larger list of [application layer protocols](https://en.wikipedia.org/wiki/Category:Application_layer_protocols)\n\n-   [SOAP](https://en.wikipedia.org/wiki/Simple_Object_Access_Protocol \"Simple Object Access Protocol\"), Simple Object Access Protocol\n-   [Simple Service Discovery Protocol](https://en.wikipedia.org/wiki/Simple_Service_Discovery_Protocol \"Simple Service Discovery Protocol\"), A discovery protocol employed by UPnP\n-   [TCAP](https://en.wikipedia.org/wiki/Transaction_Capabilities_Application_Part \"Transaction Capabilities Application Part\"), Transaction Capabilities Application Part\n-   [Universal Plug and Play](https://en.wikipedia.org/wiki/Universal_Plug_and_Play \"Universal Plug and Play\")\n-   [DHCP](https://en.wikipedia.org/wiki/DHCP \"DHCP\") Dynamic Host Configuration Protocol\n-   [DNS](https://en.wikipedia.org/wiki/DNS \"DNS\") Domain Name System\n-   [BOOTP](https://en.wikipedia.org/wiki/BOOTP \"BOOTP\") Bootstrap Protocol\n-   [HTTP](https://en.wikipedia.org/wiki/HTTP \"HTTP\") Hyper Text Transfer Protocol\n-   [HTTPS](https://en.wikipedia.org/wiki/HTTPS \"HTTPS\")\n-   [NFS](https://en.wikipedia.org/wiki/Network_File_System \"Network File System\")\n-   [POP3](https://en.wikipedia.org/wiki/POP3 \"POP3\") Post Office Protocol\n-   [SMTP](https://en.wikipedia.org/wiki/SMTP \"SMTP\")\n-   [SNMP](https://en.wikipedia.org/wiki/SNMP \"SNMP\")\n-   [FTP](https://en.wikipedia.org/wiki/FTP \"FTP\")\n-   [NTP](https://en.wikipedia.org/wiki/Network_Time_Protocol \"Network Time Protocol\")\n-   [IRC](https://en.wikipedia.org/wiki/IRC \"IRC\")\n-   [Telnet](https://en.wikipedia.org/wiki/Telnet \"Telnet\") Tele Communication Protocol\n-   [SSH](https://en.wikipedia.org/wiki/SSH \"SSH\")\n-   [TFTP](https://en.wikipedia.org/wiki/TFTP \"TFTP\")\n-   [IMAP](https://en.wikipedia.org/wiki/IMAP \"IMAP\")\n-   [Gemini](https://en.wikipedia.org/wiki/Gemini_(protocol) \"Gemini (protocol)\")\n"},{"fields":{"slug":"/ICT Stack/Internet/Internet Stack/","title":"Internet Stack"},"frontmatter":{"draft":false},"rawBody":"The Webizen stack is built to use IPv6 whilst being designed to support backwards compatability for IPv4 services.  The Webizen Stack also supports traditional ICANN Domain Name System.  Whilst it is understood that other alternatives are emerging, and that there are serious problems with the consequences of how the existing DNS system has been managed historically, the hope is that these problems will be able to be addressed somehow, rather than investiture in alternatives that may end-up presenting other problems, that have the effect of not providing any material progress.  \n\n###  [THE ISO MODEL](https://en.wikipedia.org/wiki/OSI_model)  \nThe **Open Systems Interconnection model** (**OSI model**) is a [conceptual model](https://en.wikipedia.org/wiki/Conceptual_model \"Conceptual model\") that 'provides a common basis for the coordination of [ISO] standards development for the purpose of systems interconnection'.  In the OSI reference model, the communications between a computing system are split into seven different abstraction layers: Physical, Data Link, Network, Transport, Session, Presentation, and Application. \n| LayerType   | LayerLevel | LayerName                                                        | [](https://en.wikipedia.org/wiki/Protocol_data_unit)[Protocol data unit](https://en.wikipedia.org/wiki/Protocol_data_unit) (PDU) | [](https://en.wikipedia.org/wiki/OSI_model#cite_note-26)[](https://en.wikipedia.org/wiki/OSI_model#cite_note-26)                                                                                                                                           |\n| ----------- | ---------- | ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| Host Layer  | 7          | [Application](https://en.wikipedia.org/wiki/Application_layer)   | [Data](https://en.wikipedia.org/wiki/Data_(computing))                                                                           | [High-level protocols such as for resource sharing or remote file access, e.g.](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol).                                                            |\n| Host Layer  | 6          | [Presentation](https://en.wikipedia.org/wiki/Presentation_layer) | [Data](https://en.wikipedia.org/wiki/Data_(computing))                                                                           | Translation of data between a networking service and an application; including character encoding, data compression and encryption/decryption                                                                                                                            |\n| Host Layer  | 5          | [Session](https://en.wikipedia.org/wiki/Session_layer)           | [Data](https://en.wikipedia.org/wiki/Data_(computing))                                                                           | [Managing communication](https://en.wikipedia.org/wiki/Session_(computer_science)) [sessions](https://en.wikipedia.org/wiki/Session_(computer_science)), i.e., continuous exchange of information in the form of multiple back-and-forth transmissions between two nodes |\n| Host Layer  | 4          | [Transport](https://en.wikipedia.org/wiki/Transport_layer)       | Segment, Datagram                                                                                                                | Reliable transmission of data segments between points on a network, including segmentation, acknowledgement and multiplexing                                                                                                                                             |\n| Media Layer | 3          | [Network](https://en.wikipedia.org/wiki/Network_layer)           | [Packet](https://en.wikipedia.org/wiki/Network_packet)                                                                           | Structuring and managing a multi-node network, including addressing, routing and traffic control                                                                                                                                                                         |\n| Media Layer | 2          | [Data link](https://en.wikipedia.org/wiki/Data_link_layer)       | [Frame](https://en.wikipedia.org/wiki/Frame_(networking))                                                                        | Transmission of data frames between two nodes connected by a physical layer                                                                                                                                                                                              |\n| Media Layer | 1          | [Physical](https://en.wikipedia.org/wiki/Physical_layer)         | Bit, Symbol                                                                                                                      | Transmission and reception of raw bit streams over a physical medium                                                                                                                                                                                                     |\n\nA broader description of these elements is provided here: [List of Protocols ISO model](../General%20References/List%20of%20Protocols%20ISO%20model.md). \n\nIn practice; this model ends-up getting a bit mixed up, as work-arounds are defined to address various issues that exist that make it more difficult (or impossible) to achieve some sort of functionality via a more 'straight forward' approach (ie: the use of WireGuard / VPN technologies or the use of DNS over TLS or HTTPS). \n\nIPv6 Protocols\n\nThe [Domain Name System Security Extensions** (**DNSSEC**)](**DNSSEC**)) are a suite of extension specifications by the [Internet Engineering Task Force](https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force \"Internet Engineering Task Force\") (IETF) for securing data exchanged in the [Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System \"Domain Name System\") (DNS) in [Internet Protocol](https://en.wikipedia.org/wiki/Internet_Protocol \"Internet Protocol\") (IP) networks. The protocol provides [cryptographic authentication](https://en.wikipedia.org/wiki/Message_authentication \"Message authentication\") of data, authenticated denial of existence, and data integrity, but not availability or confidentiality. \n\n[**DNS-based Authentication of Named Entities** (**DANE**)](**DANE**)) is an [Internet security](https://en.wikipedia.org/wiki/Internet_security \"Internet security\") protocol to allow [X.509](https://en.wikipedia.org/wiki/X.509 \"X.509\") [digital certificates](https://en.wikipedia.org/wiki/Digital_certificates \"Digital certificates\"), commonly used for [Transport Layer Security](https://en.wikipedia.org/wiki/Transport_Layer_Security \"Transport Layer Security\") (TLS), to be bound to [domain names](https://en.wikipedia.org/wiki/Domain_name \"Domain name\") using [Domain Name System Security Extensions](https://en.wikipedia.org/wiki/Domain_Name_System_Security_Extensions \"Domain Name System Security Extensions\") (DNSSEC).[](https://en.wikipedia.org/wiki/DNS-based_Authentication_of_Named_Entities#cite_note-1)\n\n\nAbout [ICANN](https://www.icann.org/)\n\nThe [**Internet Corporation for Assigned Names and Numbers**](https://en.wikipedia.org/wiki/ICANN) (**ICANN** [/ˈaɪkæn/](https://en.wikipedia.org/wiki/Help:IPA/English \"Help:IPA/English\") [_EYE-kan_](https://en.wikipedia.org/wiki/Help:Pronunciation_respelling_key \"Help:Pronunciation respelling key\")) is an American [multistakeholder group](https://en.wikipedia.org/wiki/Multistakeholder_governance \"Multistakeholder governance\") and [nonprofit organization](https://en.wikipedia.org/wiki/Nonprofit_organization \"Nonprofit organization\") responsible for coordinating the maintenance and procedures of several [databases](https://en.wikipedia.org/wiki/Database \"Database\") related to the [namespaces](https://en.wikipedia.org/wiki/Namespace \"Namespace\") and numerical spaces of the [Internet](https://en.wikipedia.org/wiki/Internet \"Internet\"), ensuring the network's stable and secure operation.[](https://en.wikipedia.org/wiki/ICANN#cite_note-ICANN_Bylaws-2) ICANN performs the actual technical maintenance work of the Central Internet Address pools and [DNS root zone](https://en.wikipedia.org/wiki/DNS_root_zone \"DNS root zone\") registries pursuant to the [Internet Assigned Numbers Authority](https://en.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority \"Internet Assigned Numbers Authority\") (IANA) function contract. The contract regarding the IANA [stewardship](https://en.wikipedia.org/wiki/Stewardship \"Stewardship\") functions between ICANN and the [National Telecommunications and Information Administration](https://en.wikipedia.org/wiki/National_Telecommunications_and_Information_Administration \"National Telecommunications and Information Administration\") (NTIA) of the [United States Department of Commerce](https://en.wikipedia.org/wiki/United_States_Department_of_Commerce \"United States Department of Commerce\") ended on October 1, 2016, formally transitioning the functions to the global [multistakeholder community](https://en.wikipedia.org/wiki/Multistakeholder_governance \"Multistakeholder governance\").\n\n\nDNS Security\n\n[**DNS over HTTPS** (**DoH**)](**DoH**)) is a protocol for performing remote [Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System \"Domain Name System\") (DNS) resolution via the [HTTPS](https://en.wikipedia.org/wiki/HTTPS \"HTTPS\") protocol. A goal of the method is to increase user privacy and security by preventing eavesdropping and manipulation of DNS data by [man-in-the-middle attacks](https://en.wikipedia.org/wiki/Man-in-the-middle_attacks \"Man-in-the-middle attacks\")[](https://en.wikipedia.org/wiki/DNS_over_HTTPS#cite_note-register-1) by using the HTTPS protocol to [encrypt](https://en.wikipedia.org/wiki/Encrypt \"Encrypt\") the data between the DoH client and the DoH-based [DNS resolver](https://en.wikipedia.org/wiki/DNS_resolver \"DNS resolver\").\n\n[**DNS over TLS** (**DoT**)](**DoT**)) is a network [security protocol](https://en.wikipedia.org/wiki/Security_protocol \"Security protocol\") for encrypting and wrapping [Domain Name System](https://en.wikipedia.org/wiki/Domain_Name_System \"Domain Name System\") (DNS) queries and answers via the [Transport Layer Security](https://en.wikipedia.org/wiki/Transport_Layer_Security \"Transport Layer Security\") (TLS) protocol. The goal of the method is to increase user privacy and security by preventing eavesdropping and manipulation of DNS data via [man-in-the-middle attacks](https://en.wikipedia.org/wiki/Man-in-the-middle_attacks \"Man-in-the-middle attacks\"). The [well-known port number](https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers \"List of TCP and UDP port numbers\") for DoT is 853.\n\nWhile DNS-over-TLS is applicable to any DNS transaction, it was first standardized for use between stub or forwarding resolvers and recursive resolvers, in [RFC](https://en.wikipedia.org/wiki/RFC_(identifier) \"RFC (identifier)\") [7858](https://datatracker.ietf.org/doc/html/rfc7858) in May of 2016."},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Data Applications/","title":"Data Applications"},"frontmatter":{"draft":false},"rawBody":"There's an array of 'applications' that are more focused on performing 'data tasks' rather than providing an application interface or a management interface for a particular type of data.\n\n"},{"fields":{"slug":"/Implementation V1/Webizen-Connect/Social Media APIs/","title":"Social Media APIs"},"frontmatter":{"draft":false},"rawBody":"\n\nFacebook API: https://pkg.go.dev/github.com/huandu/facebook/v2\nTwitter API: https://pkg.go.dev/github.com/dghubble/go-twitter/twitter \n"},{"fields":{"slug":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","title":"Webizen-Connect (Summary)"},"frontmatter":{"draft":false},"rawBody":"Webizen Connect is a term that denotes a concept of Connecting, parsing and integrating APIs from other platforms (ie: web2 silos). \n\nAn Example is  [Social Media APIs](Social%20Media%20APIs.md).\n\n"},{"fields":{"slug":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","title":"AI Capabilities Objectives"},"frontmatter":{"draft":false},"rawBody":"\n| TYPE                       | FIELD                                                                     | Method |\n| -------------------------- | ------------------------------------------------------------------------- | ------ |\n| Deep Learning              | Adding sounds to silent movies                                            |        |\n|                            | Advanced Cryptography                                                     |        |\n| Artificial Neural Networks | Aerospace Engineering                                                     |        |\n| Artificial Neural Networks | Artificial Neural Network (ANN)                                           |        |\n|                            | Audio Processing                                                          |        |\n| Deep Learning              | Automatic Game Playing                                                    |        |\n| Deep Learning              | Automatic Handwriting Generation                                          |        |\n| Deep Learning              | Automatic Machine Translation                                             |        |\n| Machine Learning           | Automating Employee Access Control                                        |        |\n| Machine Learning           | Banking, Insurance & Finance (Fraud Protection, Algorithmic Trading, etc) |        |\n|                            | behavioral modelling                                                      |        |\n| Artificial Neural Networks | behavioural analytics (ie: social media)                                  |        |\n|                            | Biometric Analysis                                                        |        |\n|                            | Blockchain Analysis                                                       |        |\n|                            | Blockchain Mining                                                         |        |\n| Artificial Neural Networks | Character Recognition                                                     |        |\n|                            | Coding Assistant                                                          |        |\n| Deep Learning              | Colourisation of Black and White images                                   |        |\n|                            | Content Generation                                                        |        |\n| Artificial Neural Networks | Convolutional Neural Networks                                             |        |\n| Deep Learning              | Deep Dreaming                                                             |        |\n| Artificial Neural Networks | Deep Learning                                                             |        |\n| Artificial Neural Networks | Defence (inc. cybersecurity)                                              |        |\n| Deep Learning              | Demographic and Election Predictions                                      |        |\n| Deep Learning              | Detecting Developmental Delay in Children                                 |        |\n| Deep Learning              | Entertainment                                                             |        |\n| Artificial Neural Networks | Facial Recognition                                                        |        |\n| Deep Learning              | Fraud Detection                                                           |        |\n| Deep Learning              | Healthcare                                                                |        |\n|                            | Image Object Removal                                                      |        |\n| Machine Learning           | Image Recognition                                                         |        |\n| Artificial Neural Networks | Language Generation and Multi-document Summarization                      |        |\n| Deep Learning              | Language Translations                                                     |        |\n| Machine Learning           | LifeSciences                                                              |        |\n| Artificial Neural Networks | Machine Translation                                                       |        |\n|                            | Media Generation (Image, 3d Image, 3d Design, Code, written content, etc) |        |\n| Artificial Neural Networks | Named Entity Recognition (NER)                                            |        |\n| Deep Learning              | Natural Language Processing                                               |        |\n|                            | Network Analysis                                                          |        |\n| Deep Learning              | News Aggregation and Fraud News Detection                                 |        |\n| Artificial Neural Networks | Paraphrase Detection                                                      |        |\n| Artificial Neural Networks | Part-of-Speech Tagging                                                    |        |\n| Deep Learning              | Personalisations                                                          |        |\n| Deep Learning              | Photo Descriptions                                                        |        |\n| Deep Learning              | Pixel Restoration                                                         |        |\n| Machine Learning           | Predict Potential Heart Failure                                           |        |\n| Machine Learning           | Product Recommendations                                                   |        |\n|                            | Psychometric Analysis                                                     |        |\n| Machine Learning           | Regulating Healthcare Efficiency and Medical Services                     |        |\n|                            | Search Optimisation                                                       |        |\n|                            | Security Alarms                                                           |        |\n| Deep Learning              | Self Driving Cars                                                         |        |\n| Artificial Neural Networks | Semantic Parsing and Question Answering                                   |        |\n| Machine Learning           | Sentiment Analysis                                                        |        |\n| Artificial Neural Networks | Signature Verification and Handwriting Analysis                           |        |\n| Machine Learning           | Social Media Features                                                     |        |\n| Artificial Neural Networks | Speech Recognition                                                        |        |\n|                            | Speech Synthesis                                                          |        |\n| Artificial Neural Networks | Spell Checking                                                            |        |\n| Artificial Neural Networks | Stock Market Prediction                                                   |        |\n|                            | synthetic voice generation                                                |        |\n| Artificial Neural Networks | Text Classification and Categorization                                    |        |\n|                            | Threat Detection                                                          |        |\n|                            | Video Analysis                                                            |        |\n|                            | Video Processing                                                          |        |\n| Deep Learning              | Virtual Assistants                                                        |        |\n| Deep Learning              | Visual Recognition                                                        |        |\n| Artificial Neural Networks | Weather Forecasting                                                       |        |\n|                            | Predictive Analysis                                                       |        |\n\n\nTBC"},{"fields":{"slug":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","title":"General Mobile Architecture"},"frontmatter":{"draft":false},"rawBody":"One of the Objectives of webizen is to provide an alternative to these 'digital identity' wallets, as the concept of 'identity' is very different to what is displayed by these projects. \n\nAnother of the objectives; is to provide an 'ai agent' that is an animated thing, that can interact with its owner via text (nlp) or voice or via sensors, interactively.   So, the 'webizen' is like your helper - yet, its not defined exactly upon users, its just defined within a genre of 'artificial minds' that means its not humanoid, its not like 'skynet' take over the world kinda AI; nor it is able to be made 'evil'.  It needs to fit into the protocols and services that make-up how the webizen systems work.  Yet in-order to power a personal webizen agent; there's a bunch of stuff that needs to happen before that's able to be made possible.  In-effect, this is part of what the [Webizen 1.0](../../Implementation%20V1/Webizen%201.0.md) works seek to forge the foundations to support, in future. \n\nIn-Order for the Webizen Mobile environment to be made able to work properly, it'll need to support wallet like functionality alongside various distributed information protocols, to offload processing requirements to other 'trusted' devices within the 'webizen' private and/or permissioned network. \n\nSee links for More information about the specifics for [Android](Android.md) and [iOS](iOS.md).\n\n"},{"fields":{"slug":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","title":"Web Of Things (IoT)"},"frontmatter":{"draft":false},"rawBody":"The Web of Things architecture is instrumental to the Webizen pro architecture.\n\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/","title":"about"},"frontmatter":{"draft":false},"rawBody":"---\nid: 148\ntitle: about\ndate: '2017-05-12T09:00:40+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://openseason.online/?page_id=2'\ninline_featured_image:\n    - '0'\n---\n\nWebizen.net.au is an compilation of works that have been built upon a basis of principles that seek to apply the use of technology, for rule of law in a manner that is ‘human centric’. This means that law is a system of democracy should be designed to support the interests of the people who live in that society.\n\nLaw should also resource those who work on behalf of the public, to support the needs of their roles to do the job in a ‘fit and proper’ manner, lawfully.\n\nthe resources provided here, are being republished from a multitude of former websites and documents alongside other information repositories and the things i remember well, which do in-turn have usefully associated public links.\n\nThe purpose of publishing all this work online is to provide a means to help others improved their insights, tooling, addressability, exposure and available informatics; to capture, employ, present and improve our societies accessible methods to assist; in a rethink of – good guys vs. bad guys.\n\n> no person is above the law and all should have access to it, for the purposes of legal remedy, using the best technological tools, that science can provide. This obligation is encrypted in the [UDHR](http://www.un.org/en/documents/udhr/) and is consistent with both the [charter of the commonwealth](http://thecommonwealth.org/our-charter) and the goals outlined by the [United Nations SDGs](https://sustainabledevelopment.un.org).\n\n…sadly, we can do alot better as a society, today, all too often today: its the criminals who benefit most, from our insecurity and indecision to fix the problems known about, by all too many.\n\nThe problem is, as many will figure out or already know; that the decision to make changes is *mearly* a first step. It takes years of dedicated work to figure out how best to solve a problem as is the case in any field of expertise.\n\nThe works published on this site have been produced with the aim of helping you, make your rule of law, meaningful to you. We may communicate in an [infosphere](https://en.wikipedia.org/wiki/Infosphere) that we know to be ‘cyber space’ but we live in the natural world.\n\nOur natural world is made-up of places; and in the place i live, the objective always is to ensure *[It does exactly what it says on the tin](https://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin)*. My writing is not intended to suggest other systems of government shouldn’t reasonably consider how these works could work for them; but i’m Aussie, and my nationhood and means to contribute to the dignity of life here; is as important to me as it is to others, elsewhere.\n\nI hope considerations are acknowledged to have been made, for our differences. This in-turn can be put into machine-readable formats, as i’ve described throughout this site; and we’re able to ensure we are better able to consider our differences and the means through which [art](https://en.wikipedia.org/wiki/Art) is our natural world."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/embed-link/","title":"Embed Link"},"frontmatter":{"draft":false},"rawBody":"---\nid: 7\ntitle: 'Embed Link'\ndate: '2018-09-21T06:10:53+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/embed-link/'\n---\n\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/posts/","title":"Posts"},"frontmatter":{"draft":false},"rawBody":"---\nid: 454\ntitle: Posts\ndate: '2018-09-25T14:13:38+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=454'\ninline_featured_image:\n    - '0'\n---\n\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/privacy-policy/","title":"Privacy Policy"},"frontmatter":{"draft":false},"rawBody":"---\nid: 3\ntitle: 'Privacy Policy'\ndate: '2018-09-21T05:58:52+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=3'\ninline_featured_image:\n    - '0'\n---\n\n## This Privacy Policy is Draft.\n\nThe intention of the website is to publish information; and the website is does not seek to engage with viewers of the website materials in any interactive way; such as via making available a log-in function, the ability to comment or any other form of engagement. In-part this is due to the complex nature in which GDPR and other similar ‘privacy’ related considerations makes delivering this functionality more difficult than is otherwise needed, in-order to write some thoughts and to post those thought online for the benefit of others.\n\nThis draft Policy statement will be updated as the website itself, is updated overtime.\n\n## Who we are\n\nOur website address is: https://www.webizen.net.au.\n\n## What personal data we collect and why we collect it\n\n### Comments\n\nThe Website is making best endeavors to turn off the ability for people to make comments, the intention thereby being that we will not allow users to engage with the website by way of comments, and therefore store no data.\n\nIF visitors find a way to leave comments on the site, wordpress natively collect the data shown in the comments form, and also the visitor’s IP address and browser user agent string to help spam detection.\n\nAn anonymized string created from your email address (also called a hash) may then be provided to the Gravatar service to see if you are using it. The Gravatar service privacy policy is available here: https://automattic.com/privacy/. After approval of your comment, your profile picture is visible to the public in the context of your comment.\n\n### Media\n\nThe website does not allow users to upload media. If you upload images to the website, you should avoid uploading images with embedded location data (EXIF GPS) included.\n\nVisitors to the website can download and extract any location data from images on the website.\n\n### Contact forms\n\n### Cookies\n\nIF my disabling comments is circumvented somehow, and you leave a comment on our site you may opt-in to saving your name, email address and website in cookies. These are for your convenience so that you do not have to fill in your details again when you leave another comment. These cookies will last for one year.\n\nIf you have an account and you log in to this site, we will set a temporary cookie to determine if your browser accepts cookies. This cookie contains no personal data and is discarded when you close your browser.\n\nWhen you log in, we will also set up several cookies to save your login information and your screen display choices. Login cookies last for two days, and screen options cookies last for a year. If you select “Remember Me”, your login will persist for two weeks. If you log out of your account, the login cookies will be removed.\n\nYou are unable to edit or publish an article; if this is nonetheless circumvented somehow, an additional cookie will be saved in your browser. This cookie includes no personal data and simply indicates the post ID of the article you just edited. It expires after 1 day.\n\n### Embedded content from other websites\n\nArticles on this site may include embedded content (e.g. videos, images, articles, etc.). Embedded content from other websites behaves in the exact same way as if the visitor has visited the other website.\n\nThese websites may collect data about you, use cookies, embed additional third-party tracking, and monitor your interaction with that embedded content, including tracking your interaction with the embedded content if you have an account and are logged in to that website.\n\n### Analytics\n\nThe website has some anonymised analytics systems to help identify whether anyone has read anything on the website. There are two ways this is being explored, one is by setting all the anonymised features on google analytics, another is via the web-hosting provider which can log basic information as is standard functionality provided on web-hosting providers.\n\nThe web-host functionality is currently turned off, and the google analytics plugin is being configured to keep my life simple by seeking to act in a manner accordance with law in the simplest possible way, given my attempt seeks to do what is similar to making available a form of electronic hypermedia book; without having to host that material on a major website elsewhere.\n\nFurther exploration will be done to ensure no impropriety is accidentally caused as to support rent-seekers in any way; relating to the draft publication of my works online.\n\n## Our considerations with respect to your data.\n\nwe would prefer to see a world where you own your data, in a knowledge banking industry. For whatever reason, this has not occurred and does not appear to be supported by those who are financially and legally otherwise capable of changing the situation.\n\nThis website doesn’t want to have any of your data. This website makes no meaningful attempts to get any of your data.\n\nYour data is your problem, made use of by others who have nothing to do with me. IF you have a problem about the use of your data, i suggest you review, rewrite, produce and go to market with solutions that make you the primary beneficiary of your data; and take that to people with more power over you, more money than me; to do something about it.\n\nIf you have a problem about the use of data, do something about it.\n\n## What rights you have over your data\n\nIf you have found some otherwise unwanted way of leaving any data on this website, it will be deleted as soon as it can be identified.\n\nThis does not include any data we are obliged to keep for legal, or security purposes.\n\n## Where we send your data\n\nVisitor engagements may be checked through an automated spam detection service.\n\n## Your contact information\n\n### Contact information can be found via Whois services."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/","title":"Resource Library"},"frontmatter":{"draft":false},"rawBody":"---\nid: 298\ntitle: 'Resource Library'\ndate: '2018-09-21T19:57:44+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=298'\ninline_featured_image:\n    - '0'\n---\n\nOver the past many years an array of research and development activities have been undertaken within the field of semantic web related ecosystems and the means to make use of these technologies for decentralised and/or federated, intelligent information (knowledge) systems design.\n\nThis resource library (under development) attempts to help short-cut the discovery process of this catalogue of work.\n\n1. [LINK LIBRARY](https://www.webizen.net.au/resource-library/link-library/)\n2. [VIDEO LIBRARY](https://www.webizen.net.au/resource-library/video-library/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","title":"An Overview"},"frontmatter":{"draft":false},"rawBody":"---\nid: 451\ntitle: 'An Overview'\ndate: '2018-09-25T14:12:34+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=451'\ninline_featured_image:\n    - '0'\nsharing_disabled:\n    - '1'\n---\n\n###### *<span style=\"font-weight: 400;\">Artificium Corpora et Sapientiae Universitas </span>*\n\n> *<span style=\"font-weight: 400;\">Webizen.net.au is about how it is entirely possible, to build a knowledge banking industry that is designed to support, a non-linear knowledge curation framework. These systems produce ‘inforgs’, that support the electromagnetic operation of Human Experience</span>*\n> \n> An **inforg** is an informationally embodied organism, entity made up of information, that exists in the [infosphere](https://en.wikipedia.org/wiki/Infosphere \"Infosphere\"). These informationally embodied organisms are also called natural agents. source [wikipedia](https://en.wikipedia.org/wiki/Inforg)\n\n<span style=\"font-weight: 400;\">My core argument is that the present, cloud-computing, silo based architecture is distorting the reality of humanity as a whole, causing significant problems. I have held a strong belief about how it is these problems can be solved, and this website documents the outcomes formed as a result of around two decades of work done on producing (and contributing towards) the tooling for a ‘Human Centric Web’. My theory, has been built upon the underlying theory that our societies need ‘knowledge banking industries’. </span><span style=\"font-weight: 400;\"> </span>\n\nI started in 2000 with my idea for an information bank, and over the course of my journey, have had the opportunity to work with many world-leaders overtime, in what has been a remarkable journey.\n\n<span style=\"font-weight: 400;\">In attempting to define the purpose of these works, in latin; </span>\n\n- *<span style=\"font-weight: 400;\">Artificium</span>*<span style=\"font-weight: 400;\">’ is defined to mean craft, skill or technology, </span>\n- *<span style=\"font-weight: 400;\">corpora as t</span>*<span style=\"font-weight: 400;\">he plural form of </span>*<span style=\"font-weight: 400;\">corpus, </span>*<span style=\"font-weight: 400;\">derives from the Latin word for body, or a “body of people” as to have now evolved to the term ”corporation”</span><span style=\"font-weight: 400;\">, </span>\n- *<span style=\"font-weight: 400;\">Sapientiae</span>*<span style=\"font-weight: 400;\"> as to mean in plural wisdom; and </span>\n- *<span style=\"font-weight: 400;\">Universitas</span>*<span style=\"font-weight: 400;\"> the word latin word to mean “the whole, total, the universe, the world”.</span>\n\n<span style=\"font-weight: 400;\">The concept of </span>*<span style=\"font-weight: 400;\">corpus </span>*<span style=\"font-weight: 400;\">is used in relation to the concept of </span>*<span style=\"font-weight: 400;\">Persona Ficta</span>*<span style=\"font-weight: 400;\"> which means ‘legal personality’, or legal entities who are not human. </span>\n\n<span style=\"font-weight: 400;\">By extension, the application of law has formed a man-made concept of ‘artificial persons’ that are now attributed </span>*<span style=\"font-weight: 400;\">legal rights,</span>*<span style=\"font-weight: 400;\"> independent to its relationships with natural persons. </span>\n\n<span style=\"font-weight: 400;\">Natural legal persons are of course distinct to any ‘artificial person’ and whilst the original context was formed in the previous generation of (“printing era”) technology and tools; the application of considerations with respect to ‘artificial actors’ now also include ICT agents. </span>\n\n<span style=\"font-weight: 400;\">The purpose of a ‘human centric web’, through the active use of inforgs, is to adapt the use of technology, to progress humanity as a species, in service to our natural world.</span>\n\nThis site is designed to compile the work-product produced; alongside related links and information; in the hope it’ll be useful to others. The site contains libraries of links, videos, books that are available to learn about the global standards based software tooling and related software products; alongside an array of thoughts, business concepts and related theory produced by me.\n\nI hope the dynamic array of consumable hypermedia provides something that can be made useful by people of different fields. from the business and governance related content, to the ICT technical materials made available.\n\nThe scope of these works can more easily be deployed by the global giants already operating the bulk of the technology stack, albeit, without the required legal, economic or consumer interface requirements needed to bring it about.\n\nThis can all change very quickly and I suspect things are about to change.\n\n##### Website Contents\n\nThe Webizen.net.au site contains an array of materials. The ‘information architecture’ of this website has not been done as yet, nor has templating been reviewed in any serious way (i’ve just got something that works, for now).\n\nAll content should be considered draft, and some, employs ‘social encryption’ and can be reasonably considered ‘rubbish’ by those who lack kindness. This is in-part, an intentional practice method that is now being reviewed given the progressive state of works more broadly; and the shift in audience, this site is intended to be made useful to, for proper purpose.\n\nMy practice method has separated materials in the following way;\n\n**constituents**: the ‘constituents’ page lists, posts that have been harvested off my old websites alongside newly created ones. There is a search feature alongside other means to find content produced historically.\n\n**about**: contains more commercially accessible (non-technical) pages that make an attempt to describe the multifaceted nature of the body of work.\n\n**Technical Introductions:** is WIP (work in progress). it is designed to provide technologists, of various skills sets, the means to evaluate the communicated concepts in various forms of technical terms.\n\n**Resource Library**: is also WIP. Some technical work needs to be done (which will be done during the ‘templating’ stage); but this section, is currently designed to provide references to known 3rd party materials (ie: books, code, relevent 3rd party projects) alongside some of my own.\n\n##### SOCIAL ENCRYPTION\n\nAmongst the fields of work that i believe to be amongst my greatest achievements, is the design of a methodology i believe to be quantum resistant; whereby informatics can be secured through a practice method i call ‘social encryption’. This methodology is a multifaceted thing, that by practice theory; has been shown to deliver ‘as intended outcomes’. Put simply, there is a means and methodology to what may otherwise be considered madness.\n\nThis in-turn provides an AI, Quantum Computer and idiot proof; informatics schema, that provides a defensible position against misuse, whilst supporting the means through which SME (subject matter experts) can make sense of it.\n\nThe methodology employs what is in-essence considered to be a form of ‘web science’; that is, a branching methodology that employs psychology as much as it does linguistics, decentralised information systems design; and other qualities, somewhat systematically.\n\nWhen searching this site for something that relates to a field of endeavour you really truely care about; you’ll likely find the resource you need. If you don’t, you won’t. This includes the additional information provided on the site about social encryption. If you don’t understand enough to be able to find it, It won’t make sense to you, you won’t know what to do about it; and that is part of my informatics design demonstrating something, you won’t understand makes sense or how it could be made use of to solve problems.\n\nCoders often suggest the only intellectual property that counts is documentation and working code. In-turn they seek out investors, to pay those they believe more broadly to be irrelevant.\n\nMy work, in part, argues the point; in a far more inclusive way.\n\n###### Disclaimer\n\nThis site is new, raw and a “work in progress”. I have years of work that i need to publish and intend to do so here; but that takes more time than i’ve spent, and as such – this site is still, from a content perspective, incomplete. I also need to refactor my ‘code’ to make it more accessible to others. In-past, i’ve employed an array of techniques now made defunct. As anyone who really knows what its like to do this kinda work will know; The importance of ensuring global, open, patent pool protected standards was (is) extraordinary.\n\nIt is the case today that the tooling required, is available in that format. This wasn’t the case at all when i started; so, please be considerate of the journey."},{"fields":{"slug":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","title":"Best Efforts"},"frontmatter":{"draft":false},"rawBody":"The Concept of seeking to define a framework around Defining of Best Efforts\n\nThe concept of \"best efforts\" typically refers to an obligation or promise to use a reasonable level of care and diligence in achieving a particular result. It is generally understood to mean that a person will make a good faith effort to achieve the desired result, but does not guarantee that the result will be achieved. \"Best efforts\" does not require someone to go beyond what is reasonable or to sacrifice their own interests in order to achieve the desired result.\n\nIn contrast, behavior that is intentionally harmful, exploitative, or a willful breach of an agreement of fair dealings would involve a conscious choice to act in a way that is detrimental to others or that goes against the terms of an agreement. This type of behavior would not be considered to be making a \"best effort\" to achieve a desired result, as it would be motivated by selfish or malicious intentions rather than a desire to achieve a mutually beneficial outcome.\n\nTo expand on my interpretation, \"best efforts\" refers to an obligation or promise to use a reasonable level of care and diligence in achieving a particular result. This means that a person will make a good faith effort to achieve the desired outcome, but does not guarantee that the result will be achieved. \"Best efforts\" does not require someone to go beyond what is reasonable or to sacrifice their own interests in order to achieve the desired result. This means that if something goes wrong or the desired result is not achieved, it is not necessarily a breach of the obligation as long as the person has made a good faith effort to achieve the result.\n\nIn contrast, mistakes that are made intentionally are different because they involve a conscious choice to act in a certain way, rather than being an unintentional or unforeseen outcome. Intentional mistakes are typically made with the knowledge that they are likely to result in a negative outcome, and may be motivated by selfish or malicious intentions. These mistakes would not be considered to be made in good faith or as a \"best effort\" to achieve a desired result, as the person making the mistake is acting in a way that is not in line with the goal or purpose of the activity.\n\nSource: Chat.OpenAi.com"},{"fields":{"slug":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","title":"Ending Digital Slavery"},"frontmatter":{"draft":false},"rawBody":"\nWhen asking ChatAI about Ending digital Slavery, it provided the following feedback;\n\nThere are a few different economic models that might be better suited to addressing the issue of unpaid or underpaid work, especially when it comes to knowledge workers. One approach could be to adopt a model that places a greater emphasis on workers' rights and the fair distribution of wealth. This could involve implementing policies that aim to reduce income inequality and provide a safety net for workers who are not being fairly compensated for their labor. Additionally, there could be a focus on providing training and education opportunities to help workers develop the skills they need to negotiate for better pay and working conditions. This could be coupled with stronger labor protections and enforcement mechanisms to ensure that workers are not exploited.\n\nThere are a number of behaviors that should be avoided in order to end digital slavery, including:\n\n1.  Exploitative working conditions: Employers should ensure that workers are not subjected to excessive or unreasonable workloads, and that they are provided with fair pay and benefits.\n    \n2.  Forced labor: Employers should not use threats, intimidation, or coercion to force workers to perform labor against their will.\n    \n3.  Lack of worker protections: Employers should provide workers with adequate protections, including safety equipment and training, to prevent accidents or injuries on the job.\n    \n4.  Discrimination: Employers should not discriminate against workers on the basis of their race, religion, gender, nationality, or any other protected characteristic.\n    \n5.  Harassment: Employers should have policies in place to prevent and address harassment, including sexual harassment, in the workplace.\n    \n6.  Lack of transparency: Employers should be transparent about their labor practices and should allow workers to freely communicate their concerns and report any issues without fear of retribution.\n\nThe principles for ending digital slavery can also be applied to open-source projects done online cooperatively with others. For example, it is important to ensure that contributors to an open-source project are not subjected to exploitation or forced labor. This means that they should not be asked to work excessive hours or be paid unfairly for their contributions.\n\nIt is also important to ensure that contributors to an open-source project are treated fairly and with respect, and that there is no discrimination or harassment. This may involve having clear policies in place to address these issues and providing a safe and inclusive environment for all contributors.\n\nTransparency is also important in open-source projects. Contributors should be aware of the terms and conditions under which they are working, and should be able to freely communicate their concerns and report any issues without fear of retribution.\n\nOverall, it is important to recognize that the contributions of open-source volunteers are valuable and should be treated with respect and fairness. By following these principles, open-source projects can be more inclusive and sustainable in the long term.\n\n"},{"fields":{"slug":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","title":"Freedom of Thought"},"frontmatter":{"draft":false},"rawBody":"Freedom of thought is the freedom of an individual to hold or consider a fact, opinion, or belief without interference or coercion. It is a fundamental human right that is protected by various national and international laws and agreements, including the Universal Declaration of Human Rights. Freedom of thought allows people to form and express their own beliefs and opinions, and to think and speak freely without fear of reprisal or punishment. It is an essential element of a free and democratic society, as it enables individuals to engage in critical thinking, debate, and the free exchange of ideas. Freedom of thought is often closely connected to other fundamental rights such as freedom of speech, freedom of religion, and freedom of the press.\n\nThere are a few key measures that are typically required to protect freedom of thought:\n\n1.  Legal protections: Freedom of thought is often protected by national and international laws and agreements that prohibit interference with an individual's right to hold and express their own beliefs and opinions.\n    \n2.  Political will: Governments and other authorities must be committed to protecting freedom of thought and be willing to take action to defend it when it is threatened.\n    \n3.  A robust civil society: A strong and active civil society, including media, NGOs, and other organizations, can help to promote and defend freedom of thought by advocating for the rights of individuals and holding authorities accountable.\n    \n4.  An informed and educated public: An informed and educated public is more likely to support and defend freedom of thought, as they are better equipped to understand the importance of this right and to recognize when it is being threatened.\n    \n5.  A culture of respect and tolerance: A culture that values diversity of thought and encourages respectful dialogue and debate can also help to protect freedom of thought.\n\nThere are a number of behaviors that can threaten freedom of thought, including:\n\n1.  Censorship: Governments or other authorities may try to suppress certain ideas or beliefs by censoring information or punishing those who express certain viewpoints.\n    \n2.  Intimidation: Threats, harassment, or violence against those who hold or express certain beliefs can deter people from thinking or speaking freely.\n    \n3.  Propaganda: The use of misleading or biased information to manipulate public opinion or shape people's beliefs can undermine freedom of thought.\n    \n4.  Groupthink: Pressure to conform to the beliefs or opinions of a particular group can discourage independent thought and limit the diversity of ideas and perspectives.\n    \n5.  Lack of access to information: If people do not have access to a wide range of viewpoints and sources of information, they may be limited in their ability to think freely and form their own opinions.\n\n\n#socialfabric #ValuesFrameworks #SafetyProtocols"},{"fields":{"slug":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","title":"No Golden Handcuffs"},"frontmatter":{"draft":false},"rawBody":"The Concept of [Golden Handcuffs](https://en.wikipedia.org/wiki/Golden_handcuffs), is about a means to seek rents from users by creating a situation where they're unable to exist or have meaningful support of their own human rights or have negative impacts if they decide to stop depending upon the 'things' you specifically provide them. "},{"fields":{"slug":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","title":"Relationships (Social)"},"frontmatter":{"draft":false},"rawBody":"The 'relationship management' safety protocol system is designed in consideration of human rights related principals, in addition to various social / temporal factors. \n\nOften, the way 'connections' work online is that the way current relationships are categorised or classified is rudimentry and overy simplified.  Then the way information is able to be managed if there is a change in that relationship status, is also over simplified - often, its friend, not-friend and/or block; and if someone is blocked, then anything that happened in past, is all gone too! \n\nSo these over-simplified systems are susceptable to abuse; in all sorts of different ways, that are consequentially very difficult to manage.   \n\nThe principal concept relating to the management of social relationships, is built upon the use of the [Values Credentials](../Values%20Credentials.md) and the rules defined are generally (although there are some exceptions that are intended to be supported via other safety protocols) temporally applied; and if / when the rules change; and in-turn also, the circumstances change, then the patterns relating to the discovery, interaction and use of materials associated with the relationship also changes.\n\n\n#socialfabric #ValuesFrameworks #SafetyProtocols"},{"fields":{"slug":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","title":"Social Attack Vectors"},"frontmatter":{"draft":false},"rawBody":"There are various forms of 'Social Attack' Vectors / Actions / Activities, that have existed for sometime but haven't been - better addressed - in-part due to the sophistocated nature of what needs to occur to bring about a more comprehensive 'ecosystems solution' to address bad behaviour / bad actors.\n\nThere are various 'classes' of bad actors which are not the same as one-another.  Often they're interactive and often it is unclear which 'category' or 'agent' is best associated with which category.  \n\nDue to various forms of vulnerability and the lack of functionality provided to support [Values Credentials](../Values%20Credentials.md) - that is, the ability to support a means where people self-declare what their values frameworks are; and both, what should be expected of them and in-turn also, the relationship.\n\nSo, this document seeks to illustrate various types of 'attack vectors' and in-turn support the means to consider how it is this environment appropriates a mechanism to provide better 'moral security' and/or alerts and accountability measures that are intended to act in-turn to better support healthy (safety) [Relationships (Social|[Social)](Relationships%20(Social)]].md),  etc.\n\nTopics: Dishonesty, Usury, Exploitation, Abuse, Malfesence, Social Harms, Deceptive & Misleading Behaviours, Wrongs. \n\nThere are a number of social attack vectors or behaviors that can negatively impact knowledge workers who are working online. Some examples include:\n\n1.  Character assassination: Attacking someone's character online, often through spreading false or malicious information about them, can damage their reputation and credibility, and can make it more difficult for them to work effectively.\n    \n2.  Gaslighting: Manipulating someone into doubting their own perceptions or memories, often through the use of manipulation and deception, can undermine their confidence and make it more difficult for them to speak up or advocate for themselves.\n    \n3.  Bullying: Using aggressive or abusive behavior to intimidate or dominate others can create a toxic work environment and make it more difficult for people to do their jobs effectively.\n    \n4.  Exclusion: Excluding someone from important conversations or decisions, or versioning them out of documentation that they were involved in creating, can undermine their contributions and make it more difficult for them to work effectively.\n    \n5.  Harassment: Harassment, including sexual harassment, can create a hostile work environment and make it more difficult for people to do their jobs effectively.\n    \n6.  Discrimination: Discrimination on the basis of race, religion, gender, nationality, or any other protected characteristic can create an unfair and hostile work environment, and can make it more difficult for people to succeed in their careers.\n\nIt is important to recognise that these social attack vectors can have serious negative impacts on knowledge workers, and can make it more difficult for them to do their jobs effectively. They can also create a toxic work environment that is harmful to the well-being of workers.\n\nTo protect against these types of social attacks, it is important to have clear policies in place to address harassment, discrimination, and other forms of abuse, and to have processes in place for addressing and resolving conflicts. It is also important to create a culture of respect and inclusion, and to encourage open and honest communication. By taking these steps, it is possible to create a more positive and supportive work environment for knowledge workers.\n\nIf a target of social attacks is unable to communicate the nature of their circumstances due to law enforcement, legal and/or court processes, or due to threats or fears for their safety, it is important for their employer or organization to have policies and practices in place to protect them. This may involve providing them with additional support and resources, such as legal assistance or counseling services, to help them navigate the situation.\n\nIt may also be necessary to take steps to protect the privacy of the individual and any other persons who may be involved in the situation. This may involve limiting the dissemination of information about the situation and taking steps to ensure that any information that is shared is done so in a way that is consistent with relevant privacy laws and regulations.\n\nIn some cases, it may be necessary to involve law enforcement or other authorities to address the situation and protect the safety of the individual and any other persons who may be involved.\n\nOverall, it is important to have a well-defined process in place for addressing and resolving situations where an individual is unable to communicate the nature of their circumstances due to legal or safety considerations, and to provide the necessary support and resources to ensure that the individual is protected.\n\nIf law enforcement fails to protect the safety of individuals involved in a situation, it may be necessary to explore other options for addressing the issue. This could involve seeking assistance from other authorities or organizations, such as human rights organizations or NGOs, that may be able to provide additional support and resources.\n\nIn some cases, it may be necessary to pursue legal action to hold law enforcement or other authorities accountable for their failure to protect the safety of the individuals involved. This could involve filing a complaint or lawsuit seeking damages or other remedies.\n\nIt is also important to consider the safety and well-being of the individuals involved and to take steps to protect them, even if law enforcement is not able to provide the necessary protection. This may involve seeking assistance from friends, family, or other trusted sources, or seeking shelter or other forms of support to ensure the safety of the individuals involved.\n\nOverall, it is important to have a plan in place for addressing situations where law enforcement fails to protect the safety of individuals, and to have the necessary resources and support available to ensure the safety and well-being of those involved.\n\nIf a person being attacked has reported the matter to law enforcement but the situation has not yet been resolved and they are subsequently attacked online publicly and/or socially, it is important for those managing online community environments to take steps to protect their human rights. This may involve:\n\n1.  Providing support and resources: It is important to provide the person with any necessary support and resources, such as legal assistance or counseling services, to help them navigate the situation.\n    \n2.  Ensuring their safety: It is important to take steps to ensure the safety of the person being attacked, such as by providing them with access to shelter or other forms of support.\n    \n3.  Protecting their privacy: It is important to respect the privacy of the person being attack and to take steps to protect their personal information, especially if they are unable to publish all of the evidence relating to the matter online due to legal or safety considerations.\n    \n4.  Providing a safe and inclusive environment: It is important to create a safe and inclusive environment for all members of the online community, and to have policies in place to address harassment, discrimination, and other forms of abuse.\n    \n5.  Encouraging open and honest communication: It is important to encourage open and honest communication and to create a culture of respect and inclusion, where people feel safe and supported in sharing their experiences and concerns.\n    \n\nBy taking these steps, it is possible to protect the human rights of individuals who are being attack online and to create a more positive and supportive online community environment.\n\nIt is important to note that in order to effectively protect the human rights of individuals who are being attacked online, it may be necessary to involve law enforcement or other authorities, depending on the nature of the attack and the risks to the individual's safety. It may also be necessary to seek assistance from other organizations or individuals who have expertise in addressing online attacks and can provide additional support and resources.\n\nIt is also important to be aware of relevant laws and regulations that may apply in situations where individuals are being attack online, and to take steps to ensure that the rights of the individuals involved are being protected.\n\nOverall, it is important to take a proactive approach to addressing online attacks and to have the necessary policies and resources in place to protect the human rights of those involved. This may involve providing support and resources to individuals who are being attack, creating a safe and inclusive online community environment, and working with law enforcement and other authorities to address the situation.\n\n### Commercially Motivated Attacks\nIf the underlying purpose of an attack is commercial or related to financial crimes, and the primary purpose is to obtain an advantage and/or harm the victim in a way that will prevent them from being compensated for their work and cause them injury, it is important for communities involved in work-related activities online to take steps to protect human rights and prevent harm to individuals. This may involve:\n\n1.  Having policies in place to address financial crimes and other forms of abuse: It is important to have policies in place that clearly define what types of behavior are not acceptable and provide guidance on how to report and address incidents of financial crimes or other forms of abuse.\n    \n2.  Providing support and resources to victims: It is important to provide victims of financial crimes or other forms of abuse with the necessary support and resources, such as legal assistance or counseling services, to help them navigate the situation and recover from any harm they may have suffered.\n    \n3.  Working with law enforcement and other authorities: It may be necessary to involve law enforcement or other authorities to address financial crimes and other forms of abuse, and to hold perpetrators accountable for their actions.\n    \n4.  Encouraging open and honest communication: It is important to create a culture of transparency and encourage open and honest communication within the community, as this can help to identify and address issues related to financial crimes or other forms of abuse.\n    \n\nBy taking these steps, it is possible to protect the human rights of individuals and prevent harm caused by those seeking to abuse others for financially gainful purposes.\n\nIn addition to the steps I listed earlier, there are a few other things that communities involved in work-related activities online can do to protect human rights and prevent harm caused by those seeking to abuse others for financially gainful purposes:\n\n1.  Educate members of the community: It is important to educate members of the community about financial crimes and other forms of abuse, and to provide them with information about how to recognize and report these types of incidents.\n    \n2.  Promote ethical behavior: It is important to promote ethical behavior and to encourage community members to act with integrity and respect for the rights of others. This may involve establishing codes of conduct or ethical guidelines for community members to follow.\n    \n3.  Foster a culture of inclusivity and respect: It is important to create a culture of inclusivity and respect within the community, and to encourage open and respectful dialogue and debate. This can help to create a more positive and supportive environment and reduce the risk of harm to individuals.\n    \n4.  Monitor and address issues as they arise: It is important to regularly monitor the community for signs of financial crimes or other forms of abuse, and to take prompt action to address any issues that are identified.\n    \n\nBy taking these steps, communities can create a more positive and supportive environment for all members, and can help to protect the human rights of individuals who may be at risk of abuse.\n\nIf a case or circumstance like this cannot be openly discussed publicly due to privacy, legal, or safety reasons, it can be difficult to provide support and assistance to those involved. In these situations, it may be necessary to take a more discreet approach, and to work with trusted individuals or organizations who can provide the necessary support and resources.\n\nOne option may be to seek assistance from organizations or individuals who have expertise in working with victims of financial crimes or other forms of abuse, and who can provide confidential support and resources. This could include legal assistance, counseling services, or other types of support that are tailored to the needs of the individuals involved.\n\nIt may also be necessary to work with law enforcement or other authorities to address the situation, while taking steps to protect the privacy and safety of those involved. This could involve sharing information on a confidential basis, or taking other measures to ensure the safety of the individuals involved.\n\nOverall, it is important to be mindful of the privacy, legal, and safety considerations involved in these types of cases, and to work with trusted individuals or organizations to provide the necessary support and resources to those who may be at risk of harm.\n\nIf a person is excluded, socially assaulted, and subjected to other broader social, mental, and economic harms as a result of an attack, and the circumstances of the attack cannot be discussed publicly due to privacy and safety issues, it can be difficult to provide support and assistance to the person who has been harmed. In these situations, it may be necessary to take a more discreet approach, and to work with trusted individuals or organizations who can provide the necessary support and resources.\n\nOne option may be to seek assistance from organizations or individuals who have expertise in working with victims of social, mental, and economic abuse, and who can provide confidential support and resources. This could include legal assistance, counseling services, or other types of support that are tailored to the needs of the individual.\n\nIt may also be necessary to work with law enforcement or other authorities to address the situation, while taking steps to protect the privacy and safety of the person who has been harmed. This could involve sharing information on a confidential basis, or taking other measures to ensure the safety of the individual.\n\nOverall, it is important to be mindful of the privacy and safety considerations involved in these types of cases, and to work with trusted individuals or organizations to provide the necessary support and resources to those who may be at risk of harm.\n\nThere are a number of compensation and other measures that could be considered in order to support a code of conduct or community of practice that acts to most benefit people who do the right thing and act in accordance with and support of human rights instruments. Some options may include:\n\n1.  Financial compensation: In cases where individuals have suffered financial harm as a result of being subjected to social attack vectors, it may be appropriate to provide financial compensation to help them recover from their losses.\n    \n2.  Legal remedies: In cases where the actions of others have violated the rights of individuals, it may be appropriate to seek legal remedies, such as damages or injunctions, to hold those responsible accountable for their actions and to provide compensation to the individuals who have been harmed.\n    \n3.  Counseling and support services: It may be necessary to provide counseling and other support services to individuals who have been subjected to social attack vectors in order to help them cope with the emotional and mental trauma they may have experienced.\n    \n4.  Education and training: Providing education and training on human rights and ethical conduct can help to promote a culture of respect and inclusion, and can encourage individuals to act in accordance with these principles.\n    \n5.  Policies and procedures: Having clear policies and procedures in place to address social attack vectors and other forms of abuse can help to prevent harm to individuals and create a more positive and supportive community environment.\n    \n\nBy considering these and other measures, it is possible to support a code of conduct or community of practice that acts to most benefit those who do the right thing and act in support of human rights, rather than benefiting those who engage in social attack vectors for financial or social gain.\n\nIt is also important to have mechanisms in place for addressing and resolving conflicts that may arise within a community of practice, and for holding individuals accountable for their actions. This may involve having processes in place for reporting and investigating incidents of abuse or misconduct, and for taking disciplinary action as appropriate.\n\nIt may also be helpful to have resources available for individuals who have been subjected to social attack vectors or other forms of abuse, such as legal assistance or counseling services, to help them navigate the situation and recover from any harm they may have suffered.\n\nOverall, it is important to create a culture of respect and inclusion within a community of practice, and to have policies and procedures in place to address and prevent social attack vectors and other forms of abuse. By taking these steps, it is possible to create a more positive and supportive environment for all members of the community and to ensure that the rights of individuals are respected and protected.\n\nA semantic web ontology is a formal representation of a set of concepts and relationships within a specific domain of knowledge, using a standardized vocabulary and logical rules. In the context of managing social attack vectors, an ontology could be used to represent concepts such as:\n\n-   Social attack vector: A specific type of behavior or tactic that is used to harm or exploit others, either individually or as part of a group.\n    \n-   Human rights: The fundamental rights and freedoms that are inherent to all human beings, and which are protected under international law.\n    \n-   Financial crimes: Illegal activities that involve the misuse of financial resources or systems for personal or financial gain.\n    \n-   Counseling services: Professional services provided by trained individuals to help individuals cope with and resolve emotional, mental, or behavioral problems.\n    \n-   Legal remedies: Measures that can be taken to address and resolve legal issues, such as damages or injunctions.\n    \n\nBy defining these concepts and their relationships within an ontology, it is possible to create a structured and logical representation of the principles for managing social attack vectors, and to use this representation to support decision-making and other processes related to this domain.\n\n\n``` \n@prefix : <http://webizen.org/ns/social/sav.ttl> .\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n\n:SocialAttackVector rdf:type rdfs:Class ;\n                   rdfs:label \"Social attack vector\" ;\n                   rdfs:comment \"A specific type of behavior or tactic that is used to harm or exploit others, either individually or as part of a group.\" .\n\n:HumanRights rdf:type rdfs:Class ;\n             rdfs:label \"Human rights\" ;\n             rdfs:comment \"The fundamental rights and freedoms that are inherent to all human beings, and which are protected under international law.\" .\n\n:FinancialCrimes rdf:type rdfs:Class ;\n                rdfs:label \"Financial crimes\" ;\n                rdfs:comment \"Illegal activities that involve the misuse of financial resources or systems for personal or financial gain.\" .\n\n:CounselingServices rdf:type rdfs:Class ;\n                   rdfs:label \"Counseling services\" ;\n                   rdfs:comment \"Professional services provided by trained individuals to help individuals cope with and resolve emotional, mental, or behavioral problems.\" .\n\n:LegalRemedies rdf:type rdfs:Class ;\n              rdfs:label \"Legal remedies\" ;\n              rdfs:comment \"Measures that can be taken to address and resolve legal issues, such as damages or injunctions.\" .\n\n:ManagingSocialAttackVectors rdf:type rdfs:Class ;\n                            rdfs:label \"Managing social attack vectors\" ;\n                            rdfs:comment \"The principles and practices for addressing and preventing social attack vectors and other forms of abuse.\" .\n\n:FinancialCompensation rdf:type rdfs:Class ;\n                     rdfs:label \"Financial compensation\" ;\n                     rdfs:comment \"Compensation provided to individuals to help them recover from financial harm caused by social attack vectors or other forms of abuse.\" .\n\n:EducationAndTraining rdf:type rdfs:Class ;\n                     rdfs:label \"Education and training\" ;\n                     rdfs:comment \"Education and training on human rights and ethical conduct, to promote a culture of respect and inclusion and encourage individuals to act in accordance with these principles.\" .\n\n:PoliciesAndProcedures rdf:type rdfs:Class ;\n                      rdfs:label \"Policies and procedures\" ;\n                      rdfs:comment \"Clear policies and procedures to address and prevent social attack vectors and other forms of abuse, and to create a more positive and supportive community environment.\" .\n\n:ConflictResolution rdf:type rdfs:Class ;\n                   rdfs:label \"Conflict resolution\" ;\n                   rdfs:comment \"Mechanisms for addressing and resolving conflicts that may arise within a community of practice.\" .\n\n:Accountability rdf:type rdfs:Class ;\n                rdfs:label \"Accountability\" ;\n                rdfs:comment \"Holding individuals accountable for their actions and taking disciplinary action as appropriate.\" .\n\n:ManagingSocialAttackVectors .\n:FinancialCompensation rdfs:subClassOf :ManagingSocialAttackVectors .\n:LegalRemedies rdfs:subClassOf :ManagingSocialAttackVectors .\n:CounselingServices rdfs:subClassOf :ManagingSocialAttackVectors .\n:EducationAndTraining rdfs:subClassOf :ManagingSocialAttackVectors .\n:PoliciesAndProcedures rdfs:subClassOf :ManagingSocialAttackVectors .\n:ConflictResolution rdfs:subClassOf :ManagingSocialAttackVectors .\n:Accountability rdfs:subClassOf :ManagingSocialAttackVectors .\n\n:SocialAttackVector rdfs:subClassOf :ManagingSocialAttackVectors .\n:FinancialCrimes rdfs:subClassOf :ManagingSocialAttackVectors .\n\n:ManagingSocialAttackVectors rdfs:subClassOf :HumanRights .\n:SocialAttackVector rdfs:subClassOf :HumanRights .\n:FinancialCrimes rdfs:subClassOf :HumanRights .\n\n```\n\n#socialfabric #ValuesFrameworks #SafetyProtocols"},{"fields":{"slug":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","title":"The Webizen Charter"},"frontmatter":{"draft":false},"rawBody":"The Webizen Charter is the web-civics version of the 3 laws of robotics or the Universal Declaration of Human Rights (UDHR).\n\nIts yet to be formally defined, yet basically, its about promoting good (even if not understood at the time) whilst discouraging / punishing bad behaviours.  The structure of it, seeks to support the semantics of what people involved 'believe' rather than being dictatorial; and, It seeks to support rule of law.  \n\nThere are also philophical elements, yet these constituencies are often emboided in some form via existing [Values Credentials](../Values%20Credentials.md). As such, it forms a fabric to support both; network wide scale terms, in addition to personal terms; that relate more specifically to the user and/or their dependants (ie: their kids, or elderly persons for whom they are lawfully responsible to make just decisions for).\n\nThe Artificial Intelligence related terms; also consider the following aspects,\n\n1. That only Human Beings can be sent to prison.  As such the 'human centric' nature of the constructs, seek to support 'root cause analysis' and the provision of appropriately defined electronic evidence for persons, irraspective of their financial capacity or mental state; to seek lawful remedy via a court of law through the use of evidence as is collected about their lives.\n\n\ta. It is not upto a 'computer' to determine finally a 'just outcome' rather, the facility is provided socially via courts of law.  Systems are produced to support the requirements of courts of law.\n\t\n\tb. That the right to self-determination (as is required and a requirement for [Freedom of Thought](Freedom%20of%20Thought.md) ) depends upon an ability for persons to have some sense of situation awareness that is provided to them - on a basis that may reasonably be considered [Best Efforts](Best%20Efforts.md) as to distinguish a circumstance where a person engaged in behaviour that was an intentional or willful breach, vs. other circumstances that should influence the determinations of fault & other 'common' responsibilities.\n\t\n\tc. That the environment expressly supports the means for persons to both learn about; and support / agree / take-on the personal responsibility, to uphold particular values.  If somone engages with another who has not taken-on those values, then to some-degree they do so knowingly. \n\t\n2. The Entity Centric approaches consider the role of legal entities (legal personalities / incorporated groups) as is distinct yet inclusive with, natural persons (human beings / citizens / etc).  In this 'legal entity' group inclusive process, built upon the human centric capacities (1) there is still responsibility for what individual people do; either directly, or via an agent (ie: software, etc); but it takes into account 'ethics' vs. 'morals' in which, \n   \n   a. an individual may express (and have recorded) their personal view, which defines a moral factor in relation to their own being; yet,\n\t\n\tb. a group (ie: board of directors) may vote, and the most amount of votes upon a decision is what ends-up becoming the outcome; which becomes an ethical statement of the group.\n\t\n\tAn ethical decision made by a group (corpus) may be found to be wrong; but then still, the responsibility lies upon those who voted for whatever it was that supported wrongs, rather than those who voted against it. \n\n\tThis in-turn; supports the semantics between a person (natural person) and a legal entity (legal person); and their actions, as to distinguish who it is, that should end-up facing a court of law with the evidence provided via technology, to answer for the consequence of their actions; if a matter ends-up developing into a situation where that is deemed necessary by a party.\n\n3. Humans always stewards of our 'Tools'. \n   This principal relates to the use of software and other sorts of tools, as to support the values outlined by (1) and (2).  \n\n#socialfabric #ValuesFrameworks #SafetyProtocols"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","title":"HTML SPECS"},"frontmatter":{"draft":false},"rawBody":"The Objective is to use HTML5 based presentation specifications.  The Full specifications are available via the [W3C HTML5 Specification](https://html.spec.whatwg.org/multipage/) \n\nThe Engine should also support [HTML+RDFa](https://www.w3.org/TR/html-rdfa/). \n\nThis W3C link provides a more comprehensive outline of [web-design and applications](https://www.w3.org/standards/webdesign/) .\n\n"},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","title":"Agent Directory"},"frontmatter":{"draft":false},"rawBody":"The 'agent directory' is like a mix between an addressbook and 'active directory' (ldap).  \n\n### Class: foaf:Agent\n\n_Agent_ - An agent (eg. person, group, software or physical artifact).\n\nThe app provides a means to manage relationships relating to agents.  \n\nAgents will include;\n- People (human beings)\n- Companies / Brands\n- Projects\n- Devices\n- Software & Software Agents (ie: including websites)\n- Things\n\nBased upon the access control methods; users will use the 'agent directory' much like an addressbook. "},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","title":"Design Goals Overview"},"frontmatter":{"draft":false},"rawBody":"The objective of this project is to create a runtime environment for the use of RWW / solid like applications that are built using HTML5 (HTML/CSS/JS) on personal computers.  The focus of the first stage is to deliver an ecosystem that operates on laptop & desktop machines, with the support of a low-cost server (or server-side service) that provides networking support and related features built upon the tailscale / headscale networking packages.\n\nThe use of obsidian is experimental and these product / functionality specifications are likely to be updated overtime. \n\n**HIGH LEVEL OBJECTIVES**\n\nThe high-level objective, is to create some initial open-source software packages, that provide an ability for users to either download a server-runtime that can be set-up on a VPS to support private networking support with 'client' applications that are installable on Windows & OSX in particular (also linux, but not the priority); or, that the user is able to use the 'webizen' / webcivics install that is set-up to support users.\n\nUsers then download the 'client' apps, and set them up on their local machine (ie: OSX/Windows/linux) which in-turn provides them a folder they can drop HTML5 apps written for that 'webizen server' that uses WebID-TLS (from the local host rww server) and optionally also WebID-OIDC (via the remote host / 'control server').  \n\nThe client apps are able to be written using basic HTML5 tools; and the server provides the basic 'hooks' needed to perform various functions  (described later).\n\nThe instances use semantic web technologies to create decentralised 'social' application environments.  ACLs are managed by the user, via WebID related ACLs; which are in-turn built upon the namespace (entity centric).  Local apps do not provide any data to 3rd party systems unless expressly requested to do so.  This mechanism is sought to be achieved using 'verifiable credentials' (or openbadges) related methods (alongside WAC).   The more advanced aspects of how these systems should be implemented requires an operating environment that has some apps functioning on it, in-order to further develop the semantic logic processes. \n\nIn-order to achieve that objective quickly; a method has been identified to use (and update accordingly) existing go libraries - as to achieve the objective of building a basic environment that is able to be used by people who are not necessarily software engineers (ie: might have various types of skills that are important for progress); and, until there is something that operates in a manner that exemplifies how these systems are intending to create a different sort of ecosystem, its hard to explain. \n\nNOTE:  One of the first projects sought to be developed (after the basics are done) is a 'work platform' or app / ecosystem environment. The purpose of this 'app' / ecosystem environment; is to support decentralised development of projects, including the means to attribute resource value (project costs) which may in-turn articulate to future income (roi / compensation); using dual-licensing and other complex methods, as to support the competing interests of seeking to end-up providing open and freely available software for the betterment of mankind; whilst ensuring the method employed to do so, does not breach human rights / engender 'digital slavery'.  This project is about getting long-term works to a next milestone; and one / if that is achieved, then there is still a long way to go.\n\nOUTCOME\nThe outcomes of this project seek to achieve the following objectives;\n1. Users should be able to 'sign-up' online; and,\n\ta. create an WebID-OIDC Address; and,\n\tb. connect a domainname (or are alternatively provided one)\n\tc. be provided a basic webspace (html5) that can use local graph resources.\n3. Install 'clients' that create a networking environment that supports HTTPS & WebID-TLS using that domainname.\n4. Create and/or download/install 'webizen v1 apps' (rww/solid based) onto their 'home' webizen server/client.\n\nOnce these objectives are achieved, it is considered that users will be able to 'push' resources (different acls, etc.) to different nodes (ie: from their local machine, to the 'host' that's supporting some sort of 'public webpage', blog, etc.); and that this is achieved via the 'apps' that are created on top of this 'webizen v1 platform' stack.\n\nDevelopment Tools Summary\n\nPrimary Codebase\nIn-order to develop something that works quickly, it has been found that a number of  critical functionality requirements is able to be supported by existing GoLang projects.  Alternatives have not been found using nodejs libraries or otherwise.\n\nThese libraries include; but are not limited to,\n\nClient / Server Side Packages\n\nHeadscale (https://github.com/juanfont/headscale) \nHeadscale is an opensource implementation of the TailScale Control Server.  It provides an ability for users to set-up an account via OIDC (openID connect) and initiate a private network based upon the tailscale network technologies; that can then be routed both privately and publically, using an 'exit node'\n\nTailscale (https://github.com/tailscale/tailscale) \nTailscale provides both client and server packages for networking.  It provides a private network that supports TLS & IPv4/IPv6; enabling routing through firewalls (like a vpn).  It is controlled by a 'control server', headscale is an open-source project working to build one.\n\nCaddy Server (https://github.com/caddyserver/caddy)\nCaddy Server is an extensible web-server written in Go.  It is integrated with Tailscale.\n\nRWW Server\nIn the project now commonly known as solid, however earlier known as RWW (or crosscloud) there was a server implementation in GOlang. \nGOLD ( https://github.com/linkeddata/gold )\n\nAnother library (unknown level of development) called helix (https://github.com/deiu/helix) also exists. \n\n**Other libraries**\nThere's an array of other libraries that may be usefully employed and/or considered from an architectual point of view for future releases.  This does include; in-particular,\n- Graph Database libraries (NB: https://github.com/cayleygraph/cayley)\n- Credentials / Verifiable Credentials (nb: https://pkg.go.dev/github.com/hyperledger/aries-framework-go/pkg/doc/verifiable )\n\nThis is a non-exhaustive list and more information / research about the options is being compiled into this documentation environment."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","title":"Knowledge Capital"},"frontmatter":{"draft":false},"rawBody":"---\nid: 325\ntitle: 'Knowledge Capital'\ndate: '2012-10-17T06:30:07+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=1'\npermalink: /knowledge-capital/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Digital Economy'\n---\n\nWhat is knowledge capital, who creates it, values it and uses it for economic benefit…\n\nThis document attempts to consider a means to communicate on high-level terms; the fundamental pillars of democratic participatory, economically secure works, in virtual environments. These philosophical or “higher principles”, are applied to a concept of establishing a “knowledge banking” platform, that uses semantic internet technologies for the purpose of providing accounts, with permissions, for a myriad of content and identity services.\n\nThe first steps outline the need for digital identity as a cornerstone services offering to digital economy, and any related participatory schemes. How identity and dignity through the application of “permissions systems” controlled via law and the individual; should be established to provide the capacity for “fullest potential”. The concept, has been applied using an adaptive model of “knowledge banking”, applied as a conduit from our economic climate, to this new “virtual” digital economy and related systems.\n\nThe hope of this model, it that ‘economic value’ by participants can be attributed on a basis of contribution (content); which is economically rationalized as “knowledge capital”. The means described, suggests this is done via the application of digital “knowledge banking” accounts, capable of interfacing with “linked-data”, from a myriad of sources. The purpose of these accounts, as a business systems model, is to mirrors the utility of financial banking institutions; for storage, transaction and related services for intellectual capital on a permissions basis.\n\nA “knowledge capital” account is controlled by account holders on a transportable basis. It is envisaged that several “knowledge capital” providers will become established; where consumers of this service, will have choices about how they intend to interact with these services, and service providers. If the consumer, decides to migrate from one account provider – to another – this is made possible, through the use of appropriate standards. Over the past decade, these standards have been developed, and established.\n\nThe “utility”, “infrastructure” or “institutional” provider of these accounts; will likely provide “lock-in” methods; through the provision of specific proprietary services; placed upon the “top” of the account, in a non-destructive manner..\n\nSuch services may not be available to other providers for reasons of tactical intellectual capital – commercialization. However more broadly, it is envisaged that this will not be the case. standard types of services, would include hosting, application interfaces, styles, locations, support services, consulting services, etc. Overall, even with the use of open-standards, a competitive market is envisaged to be capable of development through the application of IPR concepts, and contractual precedent.\n\nOpen-source is still seemingly mandatory as a base-level economic protection; that needs to be put-in-place, to protect knowledge capital. Similarly, an institutional model is also required when applied to the concepts of digital economics. The basis used in consideration of these concepts; have been that of philosophy, cultural universalities and the social values underpinning democracy.\n\nThere is an evolutionary-role that must be played by governance, and the free market – defining how these services are to operate; and the constraints, that must be put-in-place, pursuant to the fundamental interests of our communities.\n\nIt seems that there are commonalities throughout; on a fundamental basis, identity – of any entity – online, providing the people (and organizations) the right of secured communications; which, in internet terms is equal to identity, and the civil right to allow someone to have an identity, to communicate and to participate in commerce; in a lawful manner, and in the best interests of the individual as required to reach their fullest potential; as it is in the interests of the community that we, as people, are offered the opportunity to reach our fullest potential as contributors to the broader community. Given that the driving force to most environments is finance; then the right to be remunerated for works on a fair basis; or as otherwise agreed by the parties as applicable, on a lawful basis – seems to be a fundamental pillar of any successful digital economy.\n\nSo, how is this different?\n\nThe fundamental difference between this proposal; and the current systems used on the internet, is that the application framework being inclusive of this “knowledge banking” platform, would “link” data, rather that duplicate, copy and license data without specific transactional bi-directional agreements. ,\n\nthe proposed systems seek to create a new format of internet – that can work in parallel to the existing infrastructure, rather than replacing it. The way in which this is suggested to occur, is via the delineation between IPv4 and IPv6. If someone counted the total number of grains of Sand on the planet; the number would need to be multiplied by a significant factor to equal the total number of available IPv6 addresses, in the name-space. In-line with other related technologies, systems on this new “internet framework” could provide inclusive valuation of contributed materials (“content”) whilst recognizing the concept of “knowledge capital” or data (based representations of) objects.\n\nAnother question – that seems necessary for discussion is whether inferred ‘ownership’ of knowledge capital on digital networks is a human right. Is the knowledge used, on internet; a tool of trade for which an individual or organization should not be subject to unlawful acquisition, or exploitation.\n\nThe term “digital economy” underlies a concept of trading intangible assets; as opposed to “economy” which prior to internet, most significantly defined trade of physical assets. In today’s age; the level of trade on digital platforms is continually increasing at a dramatic rate of knots. The problem is; a vast majority of content continues to suffer from “duplication”, without license; where the term license, defines the concept of materially trading “intellectual capital”.\n\nWhere economy continues to develop; into a digital era, the means for common-law protections, such as from theft, requires new means, a new method. The “knowledge capital” banking platform proposed, delivers this capability.\n\n**BANKING AND KNOWLEDGE CAPITAL**\n\nWhat is “Knowledge Capital”? The concept surrounds the intrinsic value of information, in trade.\n\n“If a man empties his purse into his head no one can take it away from him. An investment in knowledge always pays the best interest.”\n\n[Benjamin Franklin](http://www.quotationspage.com/quotes/Benjamin_Franklin/) US author, diplomat, inventor, physicist, politician, &amp; printer (1706 – 1790)\n\n**THE ESTABLISHMENT OF BANKING**\n\n**The worlds first bank**, *is known to be the Monte di Pietà – “founded in 1539, after Emperor Charles V issued an edict to expel the Jews devoted to usury. Medieval Christians were prohibited by the church from lending money at a profit, although this injunction was often ignored in the commercial centers of the peninsula. In response to the 1539 edict, some aristocratic Neapolitans (Aurelio Paparo, Gian Domenico di Lega, and Leonardo Palma) created a non-profit organization to lend money.”*\n\n*The third oldest bank is considered to be Stockholms Banco founded in 1657 by Johan Palmstruch which began printing banknotes in 1661. It was to be the precursor to the Sveriges Riksbank, the central bank of Sweden.*\n\n*Johan Palmstruch had made two failed proposals for the creation of a banking institution in the 1650s before his third proposal, with the addition of a promise to pay half of the bank’s profits to the crown, was accepted. King Charles X Gustav thus signed two charters on November 30, 1656 to create an exchange bank and a loans bank. The first of these (which opened in July 1657) took deposits for a fee (and accruing no interest) with the account owner later able to withdraw the money as cash or to write cheques. The second (which opened at the beginning of 1659) provided loans, financed by the bank owners and secured against property. These two departments were combined in Stockholms Banko with Palmstruch as general manager.*\n\n*The* ***Bank of England****, formally the Governor and Company of the Bank of England, is the central bank of theUnited Kingdom and the model on which most modern central banks have been based. Established in 1694, it is the second oldest central bank in the world. After England’s crushing defeat by France, the dominant naval power, in naval engagements culminating in the 1690 Battle of Beachy Head, became the catalyst for England’s rebuilding itself as a global power. England had no choice but to build a powerful navy if it were to regain global power. No public funds were available, and the credit of William III’s government was so low in London that it was impossible for it to borrow the £1,200,000 (at 8 per cent) that the government wanted.*\n\n*In order to induce subscription to the loan, the subscribers were to be incorporated by the name of the Governor and Company of the Bank of England. The bank was given exclusive possession of the government’s balances, and was the only limited-liability corporation allowed to issue bank-notes. The lenders would give the government cash (bullion) and also issue notes against the government bonds, which can be lent again. The £1.2m was raised in 12 days; half of this was used to rebuild the Navy.*\n\n*As a side-effect, the huge industrial effort needed, from establishing iron-works to make more nails to agriculture feeding the quadrupled strength of the Royal Navy, started to transform the economy. This helped the new Kingdom of Great Britain – England and Scotland were formally united in 1707 – to become powerful. The power of the navy made Britain the dominant world power in the late eighteenth and early nineteenth centuries.*\n\nThe Wikipedia entry for “knowledge capital”; states the following,\n\n***Knowledge capital*** *is a concept which asserts that ideas have intrinsic value which can be shared and leveraged within and between organizations. Knowledge capital connotes that sharing skills and information is a means of sharing power. Knowledge capital is the know how that results from the experience, information, Knowledge, learning, and skills of the employees or individuals of an organization or group. Of all the factors of production, knowledge capital creates the longest lasting competitive advantage. It may consist entirely of technical information (as in chemical and electronics industries) or may reside in the actual experience or skills acquired by the individuals (as in construction and steel industries). Knowledge capital is an essential component of human capital. Knowledge capital at large can be a strong vision, strategic information on market and business model, networks, talent, supply chain, innovation and creativity. There is also knowledge liability, the unknown concerning future business models, lack of knowledge on product-service, on human potential, on governance and supply chain. The balance between knowledge capital and knowledge liability equals knowledge equity. Knowledge equity plus emotional equity equals immaterial value of the company (goodwill).*\n\nIn summary; knowledge capital – in a “digital economy”, is similar to a banknote.\n\n**A DIGITAL ECONOMY**\n\nGovernance environments; around the world, have switched their focus to the development, of what has been labelled in political circles, a, or, the “digital economy”. The Digital Economy,\n\nas defined in Wikipedia states: A **Digital Economy** refers to an economy that is based on digital technologies.\n\nThe digital economy is also sometimes called the *Internet Economy*, the *New Economy*, or *Web Economy*.\n\nThe basis to this economy is digital communications; the concept that “information”, has value; and intrinsically, people who contribute information, do so on some form of licensing terms, whether acknowledged in writing or otherwise. The fundamental principle of economics; is that transactions (or trade) can be attributed to value acknowledgement; the intrinsic value to transactions.\n\nPlatforms such as “Facebook” freely license all material provided to their platforms. In today’s age; people are expected to communicate using internet technology. Socially, people become alienated in some-part, if they choose not to participate with internet technologies and products. Perhaps more importantly; more and more businesses are forced to undertake more sophisticated methods to protect their “knowledge capital” interests. Our civil laws; have not been allowed to develop in-line with this growth of technology; in a way compliant with our sociological concepts of democracy, as a best methods practice for governance.\n\nIt is noted that in many corporations, their ability to even open-up “good-faith” discussions about potential opportunities, now requires agreements of non-disclosure which are only enforceable via courts, and legal funding; a method necessary for protection by corporate entities, but makes recourse by individuals most-often financially unfeasible.\n\nPrior to the introduction of internet; our communities or ability to communicate was no-where near that of today. The social developmental influences of this change, is global.\n\nHowever, equally – it is difficult to imagine an inventor, or “ideas person”, or owner of a collection of assets; having to be so very fearful of external entities “stealing” their work; or indeed, being accused of stealing someone’s work on an infringing basis.\n\nI don’t think it is reasonable to assume, that if a person typed out a business plan, printed it – and handed it to specified people; in seeking assistance – those recipients, in similar terms today’s recipients – would so readily exploit that information without consideration for the content (or knowledge capital) originator; nor, do i think it is reasonable to suggest that entire business systems – such as those by telecommunications companies – would be so heavily invested in this capacity – as a methodology for growth, return on investment and viability.\n\nThis produces a situation that is simply undemocratic and an adversary to common-law principles; including but not exclusive to, the rights of the people to have access to lawful remedy and not be; on an enterprise level, exploited by the evolutionary capacities of enterprise, to practically develop methodologies for “denial of justice”, upon the people.\n\nIn terms of the “digital economy”, this aims to change the seemingly has unreasonably accessibility or participatory standards; the need of mountains of “financial capital”, as a prerequisite to securely seeking investment and/or purchase agreements for valued or useful contributions, be it in a service or product orientated way. This can directly affect the manner in how the common-person, can economically participate, with and into, the information economy.\n\n**WHY HASN’T THE PROBLEM BEEN FIXED IN PAST?**\n\nIt may seem easy to fix; however, the solutions are a double edged sword,\n\nMany successful internet companies base their valuations on the ability to easily acquire “non-exclusive” intellectual property rights. These assets most often incorporate increasingly substantive personal information; which is more generally translated to marketing value, where a company is able to exploit information provided; to deliver effective marketing and/or advertising services to and from customers. Good examples of these types of agreements are found in Facebook, for content contributed onto internet applications. Perhaps less fortunately, it is the people – who are more-often engaging in such terms, generally with a complete lack of awareness – as companies leverage these terms, to accumulate value without further consideration to terms such as “moral rights”, otherwise exhibited; and no choices of alternative methods.\n\n**THE CONSIDERATION OF CHANGE**\n\nThere are two facets critically interwoven into the issue of “knowledge capital” that should be core considerations for change.\n\n1. Global understandings of fundamental human rights.\n2. The application of human rights – into an environment of capitalism; and virtual goods and services – or, in effect – the ability to integrate the fundamental “pillars” of “democracy” (as an example) , into the “digital economy”.\n\nThe United Nations Human Rights Council unanimously backed the notion recently; that, “internet access and online freedom of expression is a basic human right”.\n\nWhat does this mean?\n\n- How does the internet currently support human rights?\n- What types of these rights might be applicable?\n- Who is the UN and what does that and this – have to do with sovereignty?\n\nIn the UN’s Universal Declaration of Human Rights\n\nArticle 1: Speaks of dignity.\n\nWikipedia states: Dignity\n\nis a term used in moral, ethical and political discussions to signify that a being has an innate right to respect and ethical treatment. It is an extension of the Enlightenment-era concepts of inherent, inalienable rights.\n\n– Article 2, speaks of protection from discrimination in all its forms; “Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status.”\n\nArticle 3, speaks of right to life\n\n- , liberty and security of person; Wikipedia starts to explain that liberty is “the ability of individuals to have agency (control over their own actions)” whilst security of person\n- entry inscribes; “In general, the right to the security of one’s person is associated with liberty and includes the right, if one is imprisoned unlawfully, to the remedy of hebeas corpus.\n\nArticle 4 speaks of slavery, servitude,\n\nA poignant factor for consideration is whether, by allowing the means to systemically disenfranchising individuals from capital-worth, in affiliation to their works, discoveries and other “knowledge capital”; without agreement; could be perceived to be allowing a form of slavery; where individuals may contribute and/or focus on works for an unspecified time, only to have these works capitalized upon without consideration to a receiving, often incorporated party.\n\nArticle 5 torture, cruel or inhumane, degrading treatment or punishment.\n\nPursuant to the point, in affiliation to article 4; where no reasonable means for recourse is available, is this not a form of torture, punishment or an otherwise inhumane act of piracy?\n\nArticle 8, effective remedy for violation of fundamental rights;\n\nWikipedia commences the subject of fundamental rights by stating; “**Fundamental rights** are a generally regarded set of entitlements in the context of a legal system, wherein such system is itself said to be based upon this same set of *basic, fundamental,* or *inalienable* entitlements or “rights”\n\nThe entry goes on to list some universally recognised rights, as fundamental,\n\n- Right to self-determination\n- Right to liberty\n- Right to due process of law\n- Right to freedom of movement\n- Right to freedom of thought\n- Right to freedom of religion\n- Right to freedom of expression\n- Right to peaceable assembly\n- Right to freedom of association\n- Right to marry\n\nArticle 12, arbitrary interference with privacy, family, home or correspondence nor attacks upon honour or reputation.\n\n“In clinical psychology, **arbitrary inference** is a type of cognitive bias in which a person quickly draws a conclusion without the requisite evidence.”\n\nHonour\n\n“is an abstract concept entailing a perceived quality of worthiness and respectability that affects both the social standing and the self-evaluation of an individual or corporate body such as a family, school, regiment or nation. Accordingly, individuals (or corporate bodies) are assigned worth and stature based on the harmony of their actions with a specific code of honour, and the moral code of the society at large.”\n\n**Reputation**\n\nof a social entity (a person, a group of people, an organisation) is an opinion about that entity, typically a result of social evaluation on a set of criteria. It is important in education, business, and online communities.\n\nReputation may be considered as a component of identity as defined by others.\n\nReputation is known to be an ubiquitous, spontaneous and highly efficient mechanism of social control in natural societies. It is a subject of study in social, management and technological sciences. Its influence ranges from competitive settings, like markets, to cooperative ones, like firms, organisations, institutions and communities. Furthermore, reputation acts on different levels of agency, individual and supra-individual. At the supra-individual level, it concerns groups, communities, collectives and abstract social entities (such as firms, corporations, organizations, countries, cultures and even civilizations). It affects phenomena of different scales, from everyday life to relationships between nations. Reputation is a fundamental instrument of social order, based upon distributed, spontaneous social control.\n\n- Article 17, the right to own property alone as well as in association with others, and that no-one should be arbitrarily deprived of their property.\n\n**Property**\n\nis any physical or intangible entity that is owned by a person or jointly by a group of people or a legal entity like a corporation. Depending on the nature of the property, an owner of property has the right to consume, sell, rent, mortgage, transfer, exchange or destroy it, or to exclude others from doing these things.\n\nThere are many articles within this document, of which i’ve only provided an extract. It is hoped that a basic understanding of the concept of knowledge capital as associated property to a contributing individual; is worthwhile cause; to seek to protect, as a formative means to enforce the rights of identity, dignity and other such human rights as prescribed by the united nations, overall. It is believed that these documents have a direct implications upon the fair-use, of intellectual properties – where a digital economy, is put in-place as a means of communication, economic growth and economic participation by the people. The problem therein; is that without a means for forming a digital identity and associating works to these digital identities; the rights otherwise available in a non-digital-economy, are unavailable; and therefore, incapable of acceptance by governance parties.\n\nThe United Nations Convention on the Rights of the Child goes on to describe similar principles, which are equally relevant – with special consideration to the rights of children – and internet use; but also, in terms of the rights of guardians or parents of that child, as an extension to the rights of the child themselves.\n\nThe terms outlined in these documents seem to provide a basic, comprehensive understanding governing principles for which value should be capable of attributing, to digital content; as a means, to enforcing “human rights”.\n\n**DEMOCRACY AND CAPITALISM**\n\nWikipedia defines **Capitalism** as an economic system that is based on private ownership of the means of production and the creation of goods or services for profit. Other elements central to capitalism include competitive markets, wage labor and capital accumulation. The Wikipedia entry for Capital Accumulation\n\nexplains;\n\nThe **accumulation of capital** is the gathering or amassing of objects of value; the increase in wealth through concentration; or the creation of wealth. Capital is money or a financial asset invested for the purpose of making more money (whether in the form of profit, rent, interest, royalties, capital gain or some other kind of return).\n\nThis entry does not currently contain information about “Knowledge Capital”\n\nKnowledge Capital is undoubtedly a concept which applies to the use and users of internet – especially where internet is used for some-form of commercial purpose, or attempt thereof. When considering how this applies to human rights – people have the right to be paid for their work. but how can this be defined, and what does it have to do with democracy?\n\nIn the “AusCivics” project, a project aiming to teach kids about democracy; I worked on methods to effectively understand and communicate the core-values, required to support a democracy. Already developed within this program, was a principle called “the 5 pillars of Australian democracy\n\n”. Within this outline, there is the notion of “shared values”.\n\nShared values, in this model include;\n\nCare, Compassion, Fair Go;\n\n- Doing Your Best;\n- Freedom;\n- Honesty, Integrity, and Trustworthiness;\n- Remembering;\n- Respect;\n- Responsibility;\n- Understanding,\n- Tolerance, and Inclusion;\n- Informed Judgment;\n- Health, Wellbeing, and Safety.\n\nWhether these “shared values” have been best communicated for the specified purpose of any region, is largely irrelevant – overall – it describes portions that value citizens within a democracy – and that we need a form of “shared values”, in-order to enact “the rule of law”, within a land; governed by way of a democracy.\n\nWikipedia states democracy\n\nto be; **Democracy** is a form of government in which all eligible citizens have an equal say in the decisions that affect their lives. Democracy allows people to participate equally—either directly or through elected representatives—in the proposal, development, and creation of laws.\n\n**Summary**\n\nThe adherence to; and creation of, “the rule of law”, in a digital economy, operated by a state or governance jurisdiction based upon the principles of democracy; or rather, the “rule of the law”, enforced by the state as prescribed by the rules “made by the people”. If this determination is true – it seems to most simply be in the peoples interests – to deliver tools that assist in the management of compliance, and participation – pursuant to “the rule of law”, in the interests of enabling all individuals the capacity to develop to their “fullest potential”.\n\nIn terms of internet; this means, a new form of digitally enabled “bank” is required, to assist people with use of “knowledge capital”, for the interests of the people, the economy, and those charged with the duty of governing democracy on behalf of the people.\n\nI have created this strategy, where my works commenced in 2000 specifically in considering how to create this type of “banking service”."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups"},"frontmatter":{"draft":false},"rawBody":"---\nid: 52\ntitle: 'Projects, Customers and Invoicing &#8211; Web-Services for Startups'\ndate: '2012-11-19T07:13:47+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=52'\npermalink: /projects-customers-and-invoicing-web-services-for-startups/\ninline_featured_image:\n    - '0'\ncategories:\n    - Start-ups\n    - Tutorials\n---\n\nSo,\n\nThere’s this whole ecosystem developing around how to interact with projects, customers whilst figuring out how to track payments / costs, etc.\n\nBattling away with getting rid of the shoebox method for managing reciepts; whilst considering how to improve the methods in managing customers, projects and invoicing.\n\nI’m in Apple Land – with a Mac, and IPAD. Still setting-up the Communications platform – to it’s optimal configuration – i’ve been setting-up methods to enhance the way i engage customers – and manage projects.\n\nI started out by setting-up [Freshbooks](http://www.freshbooks.com/ \"Freshbooks\"). [Freshbooks](http://www.freshbooks.com/ \"Freshbooks\") is an online invoicing system. It’s rather cheap (cost me less than most accounting packages would otherwise), and connects to my iPAD – with a few nifty functions such as being able to attach or take a photo of a receipt and assign it to a customer – as well as a “stopwatch” (which is more designed for iphones) that means i can sit-down, start the stop-watch, do something on a project – then stop the stopwatch, and track my hours.\n\nI can also put in expenses, manage project costs, add customers and a host of other invoice related functions – accessible both via an easy to use online interface – as well as on my iPad (or iphone).\n\nPerhaps one of the best bits – which i’m just starting to explore – is the ability to send an invoice; and, the ability for a customer to pay that invoice online (when appropriate banking details have been set-up).\n\nFreshbooks made it much easier – saved me heaps of time in admin – sorting out project costs, quotes, estimates, etc.\n\nThere are a few different types of systems, and solutions – but i really did like freshbooks – and have settled on it – mostly, due to the cost and application design.\n\nBut – i didn’t stop there.\n\nafter becoming a little frustrated by more complex project management processes i’m trying to streamline (for a growing business) – i then found [CapsuleCRM](http://capsulecrm.com/ \"Capsule CRM\").\n\n[CapsuleCRM](http://capsulecrm.com/ \"Capsule CRM\") integrates with Freshbooks. Making it much easier to figure stuff administrative out…\n\nAs well as integrating with the Invoicing systems – it also connects to [Linkedin](http://www.linkedin.com/ \"Linkedin\"), [Facebook](http://www.facebook.com/ \"Facebook\") and [Twitter](http://twitter.com/ \"Twitter\"). I’m not a twitter user – i feel as though i’ve got enough going on already – But depending on what type of business you’ve got – i can see how these functions can and are really useful.\n\n[ CapsuleCRM](http://capsulecrm.com/ \"Capsule CRM\") for a relatively low-cost – enables the management of teams, multitude of projects, sales pipelines, and all that “crm” type stuff which is generally difficult to set-up, let alone manage overall.\n\nAgain – Much like the options available when considering an invoicing system – there are a multitude of options out there for CRM too…\n\nThe other interesting platform is [Teambox](http://teambox.com \"Teambox\"). [TeamBox](http://teambox.com \"TeamBox\") is a relatively new start-up business. I got in-contact with them, as i set-up an account soon-after they were established. I was really impressed by the team – giving me useful support, and advise – in a really time-effective manner.\n\nTheir platform has developed quite a lot since they first started. I haven’t used it for a while, mostly because i haven’t needed to… I’ve logged into my account again, as i write this blog-entry… The interface has developed substancially, since their early days. I can link the account to my [dropbox](https://www.dropbox.com/ \"Dropbox\") account – for sharing files. It’s also got a helpdesk, and a few other functions. I can link it to facebook and a bunch of other apps. I’m thinking i’ll ask them about linking their platform into CapsuleCRM and/or Freshbooks – as that would provide a complete project lifecycle management platform – we’ll see how it goes i guess…\n\nOh – now [Dropbox](https://www.dropbox.com/ \"DropBox\") – I think i’ll cover that in a different section, more focused on communications generally; but the simple bit is as follows,\n\n[Dropbox](https://www.dropbox.com/ \"Dropbox\") is an online storage provider – it links to your devices, stores files, and makes it easy to share files on a permission based processes controlled by the account holder.\n\nOne of the most important things to look-at, when considering any of these “web-services” is an exit strategy – what happens if you want to use something else – how do you get your information out of these systems, and into something else that is useful.\n\nThese types of problems can be figured out through a growth-planning process, where your software choices are put into a lifecycle management process.\n\nBeyond simple lifecycle management – scaling cost is also an important thing to consider. If your thinking about hiring staff or collaborating with others – how can that be done? what’s needed – how is your “[knowledge capital](http://www.mediaprophet.net/?p=1 \"Knowledge Capital\")” effectively managed, throughout your businesses sales cycle.\n\nAnother good-thing about these “web-services” is that a great-deal of the more complex problems – have been thought out for you, and a well-designed site – will ask simple questions, for functional outcomes.\n\nIt’s worthwhile having a look-around at your options – seeing what can be done, and thinking about the cost/benefit to the available options, most appropriate for you and your business."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","title":"Website Templates"},"frontmatter":{"draft":false},"rawBody":"---\nid: 47\ntitle: 'Website Templates'\ndate: '2012-11-19T02:12:54+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=47'\npermalink: /templates/\ninline_featured_image:\n    - '0'\ncategories:\n    - Tutorials\n    - 'Web Templates'\n---\n\nA common misunderstanding is that a website developer builds websites, cheaply from scratch. What clients often do not understand, is that these website solutions are based upon easily accessible parts – which have taken hundreds or thousands of hours to code from scratch – most-often by highly skilled individuals.\n\nIn my page about [websites](http://www.mediaprophet.net/?page_id=16 \"Websites\") i’ve shown a number of “opensource” alternatives, allowing customers to easily create a basic content management system – based web-solution.\n\nOnce a solution has been identified to build the basic platform – the next problem becomes the “look and feel” of the website – the template.\n\nAlthough it is possible to generate a new-template from scratch – it’s often not what small businesses need to start-off. A website, is much like a shop – in that once you’ve got something that works, people – the business owners – generally think about how they’d like things to work over a period of time. This form of “organic growth”, is a natural learning curve that is part of the evolving identity any new business, or new venture undertakes.\n\nTo help with this; website templates are invaluable resources.\n\nMost Content Management Systems (WordPress, Drupal, etc.) all have a range of freely available templates – available for download either in the system, or alternatively – from the site where the CMS is developed and managed.\n\nBeyond these freely available templates; people around the world, develop “premium templates” which are generally cost effective – and relatively easy to set-up – with relatively advanced functionality.\n\n[Themeforest](http://themeforest.net/) and [Elegant Themes](http://www.elegantthemes.com/ \"Elegant Themes\") are great sites if your looking for wordpress templates. This is one of many for wordpress templates, and a personal favourite. A Quick Google Search for [wordpress templates](http://www.google.com/search?q=wordpress+templates&oq=wordpress+templates \"Google - WordPress Templates\") or [Drupal Templates](http://www.google.com.au/#hl=en&output=search&sclient=psy-ab&q=drupal+templates&oq=drupal+templates \"Google WordPress Templates\") will resource a host of different websites offering premium themes for a range of website solutions.\n\nAlthough i haven’t gone into it too much – other systems, such as [joomla](www.joomla.org \"Joomla\") as well as e-commerce systems, such as [magento](http://www.magentocommerce.com/ \"Magento\") and and [Open Cart](http://www.opencart.com/ \"OpenCart\") also offer themes or template solutions.\n\nHowever: When setting-up a website, it’s important to think about all the different aspects involved in running a business, most of which apply both to “Bricks and Morter” businesses, in addition to “digital” or virtual shop-front businesses.\n\nIt’s an important part of any business to consider the Total Cost of Ownership – which should take into account business risks, such as Fraud, Chargebacks, SPAM and other unwanted digital experiences – which can really take-up alot of time, money and energy sorting out problems which are best avoided.\n\nOur recommendation is – unless a business is big enough to justify hiring someone good – to look after the technology full-time – try to make these sorts of potential issues, someone else’s problem. Nonetheless, this is just another element of the overall strategy process involved in setting-up a business."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","title":"Tools of Trade"},"frontmatter":{"draft":false},"rawBody":"---\nid: 49\ntitle: 'Tools of Trade'\ndate: '2012-11-19T06:52:21+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=49'\npermalink: /tools-of-trade/\ninline_featured_image:\n    - '0'\ncategories:\n    - Start-ups\n    - Tutorials\n---\n\nWhen starting a new “digital business” – It’s important to get the right “tools of trade”, ensuring you understand the technology from a user-case perspective – and become proficient in undertaking the tasks your expecting your customers to undertake as part of finding you, with the digital resources your developing.\n\nIf it’s too difficult for you, that’s a good indicator it’s going to be difficult for customers – which at a minimum, is going to reduce your “traffic”, and therefore also – your sales potential.\n\nTools of Trade in any digital business, should include devices that provide you access to the resources you’re planning to provide your customers, in the method your expecting your customers to access your offerings.\n\nOther tools of trade include Accounting, Customer Relationship Management and other internal resources – to help your business or project, best manage timelines and expectations both internally, as well as externally.\n\nTim, of MediaProphet, formerly used Windows PC equipment for almost two decades. Over this time, computers were built – set-up, and used for a host of different activities. In more recent time, a transition has been made to Apple Equipment. Although more expensive in terms of hardware cost – i’m continually amazed by the lowered cost of operation, both in terms of the cost of software – as well as the usability of the system and it’s immunity to the vast majority of viruses and other nasties out there on the net.\n\nOn Workstations (and laptops) the Operating System – Called “OSX” is based on Unix or Linux – a different type of programming to windows, that means viruses and such written for windows – just don’t work on the operating system…\n\nPerhaps more importantly – The “brave new world” is now very much into “internet connected” devices. Internet Devices are envisaged to take-over the way most people do most things – computer based – when not typing a letter, writing or designing a proposal – or those other more “power-user” type applications.\n\nTherein; The Apple Environment, has a complete eco-system – connecting TV’s, with iPAD’s and iPhones – with Computers, Back-up storage devices, etc.\n\nAlthough these types of things can be done much “better” (or rather, with more flexibility) using alternatives – apple just makes it work for the layman.\n\nOn the flip-side – PC’s, Windows and increasingly – Google – do offer a range of very good solutions – which do, make-up most of the total user-base out there in the market.\n\nConsidering the solutions is an important step to setting-up a digital presence.\n\nWe can help you figure out something that might work for your organisation, and we’ve developed a few other outlines to think about – how to get yourself off the ground and into sales."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","title":"Web-Services &#8211; Marketing Tools"},"frontmatter":{"draft":false},"rawBody":"---\nid: 61\ntitle: 'Web-Services &#8211; Marketing Tools'\ndate: '2012-11-19T08:52:02+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=61'\npermalink: /web-services-marketing-tools/\ninline_featured_image:\n    - '0'\ncategories:\n    - Start-ups\n    - Tutorials\n---\n\nIn one of my other [posts](http://www.mediaprophet.net/?p=52 \"Projects, Customers and Invoicing – Web-Services for Startups\") – i’ve describe a bunch of web-services, used more for internal and/or project management – but how about marketing?\n\nMarketing a business, a group, an offering – is where it all starts. So, how can web-services help?\n\nI’ve already discussed [CapsuleCRM](http://capsulecrm.com/ \"Capsule CRM\") – a basic CRM Platform. However, this is only one-part of the broader opportunities. It’s difficult to identify which solutions suite a customers requirements – without having a good-look at the business, the business owner – and their needs.\n\nWherever you see an business orientated webservice – you’ll see a link to [Mail Chimp](http://mailchimp.com/ \"Mail Chimp\"). Mailchimp – helps with sending email marketing messages to users.\n\nAnother really important thing – is figuring out how to leverage forms.\n\nA Form is part of a webpage, where users fill-out details to make an enquiry, pay for something – all that kinda thing. One good option is JotForm It’s free to test-out, and integrates with FreshBooks, FaceBook, MailChimp and a bunch of other Web Services.\n\nPerhaps more essentially; Businesses can really benefit from setting-up links, pages and exposure for your business across the social network sites and other related places on the net – as appropriate to your website.\n\nSome of the basic sites to consider include; Facebook, Linkedin, Google+, Twitter, YouTube and other similar sites.\n\nEach industry has specific types of sites that support specific industries. These sites can include Ebay and other sites that help business owners sell products; other industries, such as travel, accommodation and professional services have specific solutions that help to resource customers and make sales.\n\nMost of these services need Professional Media Services to be facilitated by appropriate professionals – for your business. Whether you need strategy, photography, video production, graphic design, copyrighting or some-other form of professional services – to get your content in-order – it’s a worthwhile consideration, prior to setting-up these marketing-tools for your business."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","title":"Creating a Presence &#8211; Online"},"frontmatter":{"draft":false},"rawBody":"---\nid: 326\ntitle: 'Creating a Presence &#8211; Online'\ndate: '2012-11-28T04:07:38+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=56'\npermalink: /creating-a-presence-online/\ninline_featured_image:\n    - '0'\ncategories:\n    - Branding\n    - Consulting\n    - 'Digital Economy'\n    - 'Mind Mapping'\n    - Presence\n    - Start-ups\n---\n\nCreating an online presence may sound easy – but in reality – it’s time-consuming.\n\nThere are some basics that you’ll need – like a website. Honestly, if your setting-up a business nowadays – it’s better to try to manage some of these things yourself, whilst getting someone to help you, do it.\n\nThe internet has evolved, it’s a simple fact. In user-terms, the biggest growth market is likely to be the elderly; as most kids already use the internet – as natives – and adults increasingly grew-up with it. Yet, the elderly, our elders – are slowely becoming more and more isolated as their telephone and fax machine (if they’ve got one) continues to become as outdated as their love of writing letters, and sitting down with their family – to look at family photo’s, printed at the local chemist.\n\nThere’s a moral to that story; the tools available to set-up and manage websites, is becoming easier, and easier to use, modify and promote.\n\nA good technical person, acting with honesty, should be able to point you in the right direction – and help you set-up your site. The reality is, WordPress and other related tools – many of which i’ve discussed on this site; do most of the work for you. It’s most often, almost as difficult as using microsoft word – probably easier than word-perfect (for those who remember those days) and certainly much easier than putting some tape into a machine, to start a computer – or two floppy disks, for that matter…\n\nThe difference really is; that although it’s easier, more strait-forward – you still need a helping hand to get onto the right path to begin with – as well as, someone to hold your hand when you get into trouble.\n\nSo; assuming you’ve decided that you’ll set-up your own presence. The first place to start, is probably with a [mindmap](http://www.mediaprophet.net/?p=70 \"MindMapping – Setting-up a business – Identity\"). Once you figure out what kinda thing you want to say, and what headings / subheadings you want to use (otherwise known as “[information architecture](http://en.wikipedia.org/wiki/Information_architecture \"What is Information Architecture\")“) then your ready to think about making some pages on a website.\n\nWebsites nowadays link into all of these (still emerging) social-web portals. These portals, described [here](http://www.mediaprophet.net/?page_id=32 \"Social Networks\"). all want different information, in-order to maximise your potential in those environments. There are different philosophies around different sites, and how they should be used. Generally, start with whatever your most comfortable with, make a plan, and go step-by-step.\n\nOne of the most important things really, is content.\n\nYears ago, i / we (can’t remember) coined a term or rule – “the three c’s” – Content, Cash and Customers.\n\nIf you’ve got enough to get two of these, then it’s often viable to obtain the other. With only one, then you have to think about how to get the second – and if you have none – then well, Good Luck!!!\n\nContent, in essence – is intellectual input, formatted into some-sort of “digital” presence; which can be presented to customers.\n\nCash, is financial resource – the ability to present to a customer.\n\nCustomers – people who want to pay for your services. They’ll pay cash, if the content is right. If you don’t have the right Content, but you have cash – then you can always use that cash, to develop the right content – and secure the customers.\n\nCreating an online presence, is more than just a presence explaining what the business does. People like to explore, and a presence is a type of portal in itself, moulding all these different forms of threads, in all there different areas – into a coherent “space” or “network” that can attract customers, attract engagement and help generate more cash, more content, and more customers."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","title":"Events"},"frontmatter":{"draft":false},"rawBody":"---\nid: 328\ntitle: Events\ndate: '2012-11-28T04:31:22+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=84'\npermalink: /events/\ninline_featured_image:\n    - '0'\ncategories:\n    - Branding\n    - Consulting\n    - Presence\n    - Start-ups\n    - Tutorials\n---\n\nWell, You might think – what does events have to do with anything – but have a good think about it…\n\nWhen you get your first customer group all put together, wouldn’t it be nice to put on a nice event for them. If you can’t produce one – perhaps send them to one, or participate in an existing one – like a tradeshow or something…\n\nEvents is part of professional representation – of the business. Have you had a think about how you could do this?\n\nAn event done well, can be the best form of advertising for your business. An event done poorly, can be worse than receiving nails for your businesses coffin, voluntarily provided from your patrons.\n\nThe “secret” to events; is about good preparation, pre-production and management. I was recently challenged to consider how many events i had done.\n\nAs i never considered myself an “event manager”, i never really thought about before. It almost seemed like a distractions. Yet, in the face of volunteering to a community event – with so many contributors – but some massive problems due to the leadership of the event (an inexperience) i had to think about it. Perhaps a hundred, perhaps many more. It’s not really what i think about overall….\n\nOrganising an event, is kinda like setting-up a dinner party or a teenagers house party; or even, a company meeting… Within each of these events, people take different roles. It’s important to identify what you do have experience in doing, and what you do not. People, with events, are generally rather helpful. If their not helpful, don’t bother (well, in my experience anyhow).\n\nPeople who know events, know both that events hardly ever run without a problem of somekind. this is not to suggest they are all at risk of falling over, but rather – even events professionally planned – will have some sort of “hiccup” once it’s started. The difference really is; what doesn’t go wrong, because it was planned properly.\n\nAs always, templates is always a good-place to start when your thinking about how to set-up an event. It could be a digital event, it could be a wedding, it could be just about anything – if it’s too complicated to remember in your head, and have it sorted – a list, a template, can help.\n\nBeyond the bigger problem of attempting to manage the delivery of the event – the next part, which is somewhat more professional in nature – is return on investment.\n\nThis is where the art of being an event manager comes into play."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","title":"Logo&#8217;s, Style Guides and Artwork"},"frontmatter":{"draft":false},"rawBody":"---\nid: 327\ntitle: 'Logo&#8217;s, Style Guides and Artwork'\ndate: '2012-11-28T03:46:14+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=74'\npermalink: /logos-style-guides-and-artwork/\ninline_featured_image:\n    - '0'\ncategories:\n    - Artwork\n    - Branding\n    - Start-ups\n    - 'Web Templates'\n---\n\nWhat’s a Logo – well that’s an easy question, which is somewhat pointless answering.\n\nWhat’s a Style-Guide? Well, that’s more complicated.\n\nA [Style Guide](http://en.wikipedia.org/wiki/Style_guide) is a document or set of specifications outlining how to set-up the “look and feel” of documents, publications, websites and other materials produced by your organisation. A Style Guide can either be set-up by a graphic design professional – or developed overtime.\n\nThere are many templates to figure out what types of things are necessary for a style guide. a quick[ google-search](https://www.google.com.au/search?q=styleguide+template \"Style Guide Templates\") for the topic will come-up with some options.\n\nOne of the more annoying facets of producing a style guide; is the standardised formats in which the logos and related artwork needs to be produced.\n\nThere are a range of different shapes, sizes, and on digital – pixel density measures which are appropriated to logos – when applied to publishing environments.\n\nThis is much like advertorial guidelines, for submitting advertising artworks into 3rd party publications.\n\nlogo’s need to be simple. If their not simple, that is a style in itself, but equally – there is still a need to help a user – simplify the logo, so that it can be used in a versatile and transportable manner.\n\nMany hours can be spent in developing a logo; and related basic corporate identity artwork. in a start-up, if you do not have the skills to do this yourself, or perhaps – not the time – there are a range of 3rd parties who provide basic services, which can easily get you off the ground.\n\n[logospire](http://www.logospire.com/ \"LogoSpire\") creates logos for people, cheaply. A quick search [on google](https://www.google.com.au/search?q=related:www.logospire.com/ \"Search related sites LogoSpire\") – will find a bunch of similar sites… Sometimes, the quickest path – may not be the best, but the easiest."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","title":"MindMapping &#8211; Setting-up a business &#8211; Identity"},"frontmatter":{"draft":false},"rawBody":"---\nid: 70\ntitle: 'MindMapping &#8211; Setting-up a business &#8211; Identity'\ndate: '2012-11-28T02:41:32+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=70'\npermalink: /mindmapping-setting-up-a-business-identity/\ninline_featured_image:\n    - '0'\ncategories:\n    - Branding\n    - 'Digital Economy'\n    - 'Mind Mapping'\n    - Start-ups\n    - Tutorials\n---\n\nWhen starting a new business, the need to figure out identity is an important factor.\n\nAll businesses end-up with their own identity. The Australian Corporations Act, states that a company – is it’s own legal entity in the eyes of the law. In most cases, an ABN is sufficient to start a business – stepping up the process to a business name, which later down the track – if the business is successful – turns into a company.\n\nConsidering the businesses objectives is always a good starting point when considering how to provide a new business or company, identity.\n\n– What is it that the business seeks to achieve?\n\n– What does it provide that is valued to current and potential customers?\n\n– How will the business scale?\n\n– How does the business – make money…\n\nI often [mindmap](https://www.google.com.au/search?q=mindmapping \"Mind Mapping Tools\") out the solutions i’m developing, and trying to structure my thought around it. There are often a multitude of reasons why someone decides to go start a business. These underlying reasons change the nature of the start-up, the purpose of the business and the aspects in which the founder of the business, focuses upon developing the business.\n\nIn many cases; it’s simply an idea of generating revenue. People decide it’s a “good idea” to start a business, so they decide they’ll have a “crack” at it. In These cases, i often find it almost more important to explore the underlying personal reasons for setting-up the business – what is the purpose of doing this action – why a business?\n\nWhat are the critical things the business is expected to provide its founders – and what is the business designed to provide an audience – of customers. There is obviously a difference between the needs of a business operator, and a business. By exploring these needs, both personal and professional alike – a business model is more easily established around the skills and needs of its proprietors.\n\nIn other cases; a group, or individual has identified a “business opportunity”. This business opportunity is often a far more calculated start-up process, where the businesses establishment has less to do with the personal identities of the founders – and more to do with establishing a path for successful commercialisation of a product or services, for which the founders of the business have some sort of unique relationship to; as required, to facilitate the undertaking of setting-up a business.\n\nIn both cases, [mindmapping](https://www.google.com.au/search?q=mindmapping \"Mind Mapping Tools\") is a powerful tool to explore and visualise the very different, alternate and interwoven aspects to a business. To get a 15,000 foot view on the needs of setting up a business, providing it identity – and ensuring your work with it, encourages the growth – to fullest potential – of both the new business, and the participants involved in establishing it.\n\nIf your looking for some other perspectives on mindmapping – check-out the [youtube search.](http://www.youtube.com/results?search_query=mind+mapping \"YouTube Search Mind Mapping\")"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","title":"Stock Content"},"frontmatter":{"draft":false},"rawBody":"---\nid: 82\ntitle: 'Stock Content'\ndate: '2012-11-28T04:17:48+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=82'\npermalink: /stock-content/\ninline_featured_image:\n    - '0'\ncategories:\n    - Artwork\n    - Presence\n    - Start-ups\n    - Tutorials\n    - 'Web Templates'\n---\n\nWhat a day it was, when i found out about stock content…\n\nStock Content doesn’t appear to be on wikipedia… In simple-terms, it is the name given to pre-produced content (image, motion image, sound, etc.) and owned by production library and licensed to customers for use in their own productions, on a range of licensing terms; rather than, the production of content for exclusive use within a specifically produced product, services or greater work.\n\nThe purpose of Stock Content – is to acquire content, more cheaply than producing it yourself.\n\nWikipedia has a few entries on the subject, providing listings of providers of stock content – such as “[Stock Footage](http://en.wikipedia.org/wiki/Stock_footage \"Stock Footage - Wikipedia\")“, “[production music](http://en.wikipedia.org/wiki/Production_music \"WikiPedia Production Music\")” and “[stock photography](http://en.wikipedia.org/wiki/Stock_photography \"Stock Photography\")“.\n\nIn some instances; a user can download the images with existing watermarks, and use them to figure out what their going to do with the item their trying to produce – prior to making purchases.\n\nThere is a world of content out there. Sometimes, it’s just wiser to acknowledge someone else’s good-work, rather than trying to do too much yourself.\n\nThat said – i think we’ve all seen some of those images that are completely over-used. I guess, try to find something a little unique and contextual… just a suggestion…"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","title":"Advanced Functions &#8211; Facebook Pages"},"frontmatter":{"draft":false},"rawBody":"---\nid: 105\ntitle: 'Advanced Functions &#8211; Facebook Pages'\ndate: '2012-12-16T02:44:02+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=105'\npermalink: /advanced-functions-facebook-pages/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Facebook Pages'\n    - 'Social Networks'\n    - Start-ups\n    - Tutorials\n    - 'Web Apps'\n    - 'Web Templates'\n---\n\nOK,\n\nI’ve made a post already about [setting up a facebook page](http://www.mediaprophet.net/?p=103 \"Facebook Pages\"). If you’ve done that, and am interested in customising the page a little – then here you are.\n\nLike most web publishing nowadays, its more about content than technology. (somewhat) gone are the days when it takes tens of thousands to build a customised Content Management System – or perhaps more readily – have someone sell you one they’ve already made for lots of bucks…\n\nNowadays, the problem is almost more sophisticated – it’s about the more traditional skills – telling stories, taking pictures, putting together videos, creating identity – formatting content, in a uniquely compelling way to engage your communities.\n\nSo,\n\nWhen thinking about page-tabs – how can this web-environment be developed to better assist your business, brand or cause?\n\nWe’ll i’ve been searching for solutions. The main topic here, will be about page tabs. In a page, there are a few boxes – where new content can be added to your page. The Page-Tab function, brings more than simply a new page – it is a place for applications, that can interact with the community using your page.\n\nFacebook has a function where people can search for applications. In that area, there are a bunch of applications already; but that’s not all,\n\nA Search [on Google](https://www.google.com/search?q=facebook+pages+apps \"Facebook Pages Apps\") will find a bunch more. Some interesting sites i found, were [TabSite](http://www.tabsite.com/ \"TabSite\") and [TabFusion](http://www.tabfusion.com/ \"Tab Fusion\") Mind, i also found[ this site ](http://www.hongkiat.com/blog/apps-tools-to-customize-facebook-pages/ \"Hongkiat\")which provided a bunch of links.\n\nInterestingly; Oracle has a [site](http://www.involver.com/applications/ \"Oracle Involver\") with some great apps…\n\n[Pam’s site](http://www.pammarketingnut.com/2012/03/15-must-know-tips-to-rock-your-new-facebook-timeline-business-page/ \"Pam Marketing Nut\") gives some more info about pages for business. It’s good information, but there is heaps out there about it. I suggest you have a good look. Depending on the size of your existing audience, i’d suggest you go out there and experiment a bit before spending too much (if any) money.\n\nHope that’s helpful."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","title":"Facebook Pages"},"frontmatter":{"draft":false},"rawBody":"---\nid: 103\ntitle: 'Facebook Pages'\ndate: '2012-12-16T01:21:32+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=103'\npermalink: /facebook-pages/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Facebook Pages'\n    - 'Social Networks'\n    - Tutorials\n    - 'Web Apps'\n---\n\nOk. First-up, what is a facebook page, one might ask…. Perhaps even so, tell me about facebook?\n\nFacebook, and others like it – are massive data-aggregators. Their business models relay on people using the site frequently, whilst making connections to other sites and online services. This in-turn, offers facebook an unthinkable archive of data about people, and everything connected to them. From personal notes, through to friendships, through to locations, interests, etc.\n\nThis in-turn is theoretical “gold” to advertisers (and intelligence agencies) which helps them make enough money to pay for the service and make a profit.\n\non the up-side – there’s a lot of stuff they give away freely, on the basis that with a few dollars – someone may pay for advertising. regardless of money, there are literally billions of people on the network. It is a “walled garden” of users, with very different rules to the broader internet – such as high-degrees of authentication and identity assessment, and as such, is a critical environment for marketing to users.\n\nThe Page set-up process is relatively simple. To begin with, you’ll need to be a user – and ideally, link (incite) all your friends to create accounts, and become “friends”; as this provides your page the ability to gain an initial network of “friends” or “likes” (the start of a digital community). Once you’ve got yourself your own personal account on facebook, it’s simply a matter of going to the pages link – https://www.facebook.com/pages/ or “[LINK](https://www.facebook.com/pages/ \"Facebook Pages Link\")” to start creating your page.\n\nNow, before you start – you’ll need two images – one which is a square image, at low-resolution – to use as a logo. It’s important this image isn’t too detailed, it really must be a logo-orientated graphic, else it’ll look “crap”. The other image you’ll need, is a 16:9 ish image, which is used as a header or banner for your site. This can be more detailed, and overall – its very much a branding graphic.\n\nOnce you’ve gone through these basic links – you’ve got your facebook page up. A bunch of buttons will appear, asking you to invite your friends, etc.\n\nYou’ll be able to goto settings, and change different aspects of your site, including who can manage it.\n\nThere’s a bunch of youtube links about, Try searching google for [videos about facebook pages](https://www.google.com/search?tbm=vid&q=facebook+page \"Facebook Page - Google Video Search\")"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet"},"frontmatter":{"draft":false},"rawBody":"---\nid: 134\ntitle: 'Contemplation of the ITU Dubai Meeting and the Future of the Internet'\ndate: '2012-12-19T09:08:36+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=134'\npermalink: /contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Digital Economy'\n---\n\nSo,\n\nThroughout the world, a leading subject of technology news has been about the recent ITU meetings in Dubai. Throughout the world, a vast array of concepts have been affiliated with this meeting. Purposes of national policy, or that of economic benefit. Perhaps for the first time, the world has convened to discuss how policy may be orchestrated on this global network of communications, as this opportunity certainly was not available upon the inception of the internet; a fact many may suggest awarded the technology the basic capability for use, development and the ubiquity it has enjoyed now for more than 20 years. It is indeed debatable, that if the leaders of our world were forced to convene to discuss whether or not a concept like internet, should be allowed to exist – the probability it would exist is questionable at best, let alone how and what this revolutionary technology would have been allowed to do, deliver and communicate, within any form of governance structure.\n\nTherein; a reasonable guess, is that these battles hard fought, won, lost and continuing – between the founders of the internet, the geeks, the computer dudes – and the lawyers, military and commercial / banking interests; a mine-field of emerging importance and issues.\n\nOn one side, we have America. A founder of the internet, a patent territory of Intellectual Property Law, a Military Force – with significant investments into Internet and a far-reaching Venture Capital Association, Media and Computer Technology Empire – collectively manufacturing some of the largest Internet portals in the world, collecting data about people, facilitating use and development of internet in no small measure.\n\nOn the other, it looks as though we have the United Nations, A United International front of organisations all committed to joint works, collective agreements, Human Rights and international policy, throughout a world of disparate languages, peoples and interests.\n\nThe UN is the Collective Force, responsible for revolutionary agreements, such as [The Universal Declaration of Human Rights](http://www.un.org/en/documents/udhr/index.shtml \"UN Universal Declaration of Human Rights\"), first adopted on the 10th of December 1948, and others such as the [Convention on the Rights of the Child,](http://www2.ohchr.org/english/law/crc.htm \"CROC\") in 1989/90 developing what is now the basis for many legal policy precedent surrounding family law, refugee and war-crimes law and treaty, often also created and adopted via the United Nations. Although the united nations has provided little contribution in itself, toward the evolution of Internet and Information Communications Technologies, it is certainly a forum of high-esteem internationally, where considerations made are taken seriously overall.\n\nTherein; reality starts to bite upon the very nature, of this highly complex area of subject matter expertise, conflicts of interest, economic development and policy and indeed, factors involved in global stability and what is or is not allowed in terms of vigorous negotiations between states and/or other parties including but not exclusive to, acts of war.\n\nIn simple terms, much of the worlds communications infrastructure is in the hands of the American Forces, it’s allies and interests. This concept goes far beyond the simple aspect of infrastructure itself, but also into commercial models, businesses, business models, consumer brands and all things similar. In terms of international competition, there is some but not alot; Russia, for example, developed an alternative to GPS called [GLONASS](http://en.wikipedia.org/wiki/GLONASS \"GLONASS\"), now integrated into a variety of devices, although not internet based – highlighting the desire to reduce dependance on the US for all forms of Communications Technology, if and as the needs may need in future, not just for russia – but for all.\n\nThe scope is enormous. It is important to remember, that in the territory of Television, America has one version – used in some countries, whilst the world moreover – as different standards, which leads to an opportunity of co-habitation and content licensing overall.\n\nDifferent Countries have different principles of law and governance. Whilst some might complain that we’re all not just the same, it is commonly understood that the lessons of one region, can often transpire opportunities for other regions struggling with issues, within a differentiated structure of governance and law. The spirit of acceptance and collaboration between the many great nations, not only unifies us as a species, but also enhances our opportunities and diminishes threats, that would otherwise exist if we were all depending upon the same set of principles, values, system of law or indeed the many other variables of sociology that exist throughout the globe. In terms of internet, this is perhaps the greatest challenge the global community has ever faced. The system of internet relay’s upon a common set of standards and principles, which are in-turn translated into technology. although seeking an ideal, whether realistic or otherwise; the question becomes, who has the best understanding to forge this system of perfection, and should it be left to the people, the state or indeed some form of global representative group. In todays age, it appears this means of control, a questionable aspect in itself, is managed more broadly by america. It is simple to see where the benefit lies to america, and any close allies in maintaining the status quo, or so it may seem – to the uninitiated.\n\nOne of the many remarkable factors of EU Technology policy, underpinning a great-deal of the belief structure both of the ITU and the UN, is that of its [open-source](http://en.wikipedia.org/wiki/Open-source \"Open Source\") policies. If developers wish their great idea to be used by government throughout the EU, portions of it must be open-source. This is a security measure, more than anything else – however it is important to note, the difficulties a number of companies from the US have had with these policies overall. Indeed it may been considered, that technology development has, at times, been stymied by the policies held to account, by europeans. Whilst US companies may complain, it is simply not the case that a great deal of their software, web-business infrastructure is made available on open-source terms. Indeed, to example the difficulties many policing organisations throughout the world have had significant difficulty in tracking down those behaving illegally, due to the very difficult nature of acquiring appropriate information, as required by local law and law-enforcement agencies, on individuals breaching law through the use of online environments, such as facebook, myspace and others. difficulties have been discussed with me, to be more of a local law-enforcement nature, such as suggestions that youth had a suicide pact, or online bullying – aspects in Australia, that were not of national significance overall. However Australia, as a nation has enjoyed a highly communicable and trusted relationship with the US, which although explaining difficulties therein; may also highlight issues for other regions and their people, if the nature of the relationship was not equally or virtuously positive overall.\n\nThere is little disputing the fact that USA, through its own heritage and the continuation of that heritage, has a range of interests, beliefs, of great importance on a national and international stage. However reasonably, the question could be asked – does that mean we, as a global community, should be made dependant upon the interests of this region, it’s policies and its procedures. In western countries this question is less complicated, than those in other regions, with differentiated languages and beliefs. The mistakes of the crusades and/or the development of the british commonwealth may well be consequential in the coming debate, overtime.\n\nUnderlying all of this, there are some fundamental developments to the internet, which spawns the opportunity for discussion. The Internet works using a numbering system, called “IP” or “Internet Protocol”, which has proliferated throughout the globe under an auspice entitled “V4”. IPv4 provides the numbers, which in digital terms relate to whether or not the participant or device, is connected to the network. Founders of the internet, have discussed openly, that it was in-fact a lab-experiment, gone global. That the design of this fundamental pillar for internet, was not ready for global uptake. Vint Cerf, also goes on-to discuss how he gained an opportunity to work with an American Military organisation to develop a more appropriate platform for their use, and through that work a new model emerged, IPv6.\n\nThere are a number of critical differences between the two standards. The first, is the number of numbers available. The total number, of numbers (called the “ip address pool”) for IPv4 is limited, and running out. The number of available, IPv4 numbers when no-longer available – will result in an inability to issue new numbers for purposes such as secure communications, or indeed access. IPv6, has by a significant multiple – more numbers. There are theoretically so many numbers, that every unique object throughout the world could be provided a unique number – an identifier, and there would still be more left-over, for several generations.\n\nThe second, and perhaps more important aspect – is the inclusion of security, into the numbering system and the information transport system it provides.\n\nTraditionally, certificates were required from an authorised 3rd party, who held “the master key”. For obvious reasons, most of the companies holding those “master keys” were US Based. Although methods are available to use and/or generate alternative keys, consumer browsers would alert users, that the website was not secure, creating upset and difficulty for wide-spread commercial use. In IPv6, a system called “[DNSSEC](http://en.wikipedia.org/wiki/Domain_Name_System_Security_Extensions \"DNS SEC\")” is included. DNS SEC, in-turn, reduces the dependancy upon these certificate providers – and in-turn, provides an opportunity for broader commercial engagement over what is being labelled, a “digital economy”.\n\nAnd this; i believe, is where the problem is greatest – this concept, of a digital economy. Technology is never a problem, it is the utility of technology where problems emerge.\n\nAs more and more people, use digital technology as a key constituant to trade, and economic participation – security becomes a necessary aspect overall. The alternative, would be like an open-door policy to your local banking institution, and the vaults in which they hold their customers assets.\n\nWhereas one constituant of the consideration, relates to how the licensing and terms of use apply to fundamental aspects of internet technology; the other, is pursuant to the laws in which its users are most responsible. Is it more important that users do not breach US law, or are not capable of representing their disputes to a US law Court; or, is it whether such a would-be defendant, is able to be held accountable to local law. Then what about travellers?\n\nTodays internet provides little protection for the rights of individuals throughout the world. Equally, most people are able to exploit otherwise enforced law, such as copyright law, without regard. Institutional business models and profits have even emerged through the commercial exploitation of these factors, for which few if any are held accountable overall. In many of the submissions and discussions relating to this, and related matters the factor of whether or not the internet is “free” emerges. upon examination, it would be fair to consider what in america, if anything is actually “Free”, and as such – how do we pay for these things, and what rights do we have as consumers, whether locally or abroad.\n\nArguably, after the dot-com crash, the fight began – in the information age, knowledge is power and control of communications transfer and storage, is of immense power. We wait to see how this journey continues.\n\nIt is our belief, that an emergence of “information banking” will occur; but only through the discussion, of how the knowledge capital of sovereign states and its people, can at times be of value to GDP. It seems most likely, however, that a company like google will be capable of knowing about these types of changes, prior to agreement – enabling it to vend products, that in-themselves, may void the opportunity of united, democratic inter-governmental process; prior to the lifecycle process needed to forge the most opportune path, not otherwise defined within America."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","title":"WiX"},"frontmatter":{"draft":false},"rawBody":"---\nid: 329\ntitle: WiX\ndate: '2013-01-01T17:41:20+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=139'\npermalink: /wix/\ninline_featured_image:\n    - '0'\ncategories:\n    - Start-ups\n    - 'Web Apps'\n    - 'Web Templates'\n    - 'website solutions'\n---\n\nHi,\n\nthis is just a quick one. I found [“Wix”](http://www.wix.com/ \"wix\") and although i know there’s a bunch of these ones out there, as previously discussed; this one looks kinda simple, and easy to set-up."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","title":"Setting-up Twitter"},"frontmatter":{"draft":false},"rawBody":"---\nid: 330\ntitle: 'Setting-up Twitter'\ndate: '2013-06-07T04:26:11+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'http://www.mediaprophet.net/?p=189'\npermalink: /setting-up-twitter/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Social Networks'\n    - Tutorials\n    - Twitter\n---\n\nMorning folks,\n\nSo i’m on this journey setting-up an organisations digital infrastructure, and it’s time to make a few posts about Social Media use.\n\nSENARIO\n\nThe senario is an organisation, with a website and a bunch of media utilities which may or may not have been translated to digital. Common social-media networks include facebook, google+, youtube, p-interest, linkedin and vimeo but there are many others. These services fit into a description geeks call “web-services”, where the system not only has an application for users, but also an application for other applications – that allow these services to feed knowledge between each-other.\n\nThe question comes around how twitter fits into this social-media landscape and lifecycle.\n\nI think the simple way of looking at it, is that twitter is kinda like an internet based SMS Service – people can subscribe to. The system, being internet based – provides a means for “twittering” links, quick messages – overall, calls to action.\n\nTwitter can link into facebook. There is a facebook blog note about how to do it [here](https://www.facebook.com/blog/blog.php?post=123006872130 \"Linking Facebook pages to twitter\") but there are also videos on youtube  \n<iframe allowfullscreen=\"\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"http://www.youtube.com/embed/7FPQCTVpkbQ\" width=\"560\"></iframe>  \nThe benefit of linking twitter into facebook, is that rather than posting to twitter as well as facebook – posts into your facebook page, will automatically be posted on twitter too…\n\nSo, this is an easy way to get content streams happening on your twitter account. The next step, is figuring out about the “following” aspects.\n\nTwitter creates internal social networks. these networks are not linked to friendships in other programs, such as facebook or google+. their different, use different rules and are managed in a different way.\n\nAccounts you are following (which may represent either people, or organisations) can be grouped using lists. You might create a list, called “media” or one called “politics”, “environment”, etc.\n\nwhether you create lists or otherwise (lists are suggested) all you need to do is search for people, or things – and press the “follow” button. if you find an account you think might be following others you’d be interested in following too; then you can click, within their profiles; on their “following” link – and browse who that user is following; putting them into your own lists, if you like.\n\nLists can also be followed; so if you follow a particular party; you might find they’ve got a list of people they follow, like senators – or parliamentarians. rather than selecting each of those users, you can just select the list and follow that.\n\nthis is a simple method to set-up twitter. Have Fun!!"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","title":"Decentralized Web Conference 2016"},"frontmatter":{"draft":false},"rawBody":"---\nid: 393\ntitle: 'Decentralized Web Conference 2016'\ndate: '2016-06-09T14:28:25+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=393'\npermalink: /decentralized-web-2016/\ninline_featured_image:\n    - '0'\ncategories:\n    - Conferences\n    - 'Video Library'\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min yotu-limit-max   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMQ2JtejBWU1pfdm9faW5NXy1OdmJ5Znp4eXRRdjYxQXEiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2RiZDE5OCIsIm5leHQiOiIiLCJwcmV2IjoiIn0=\" data-showdesc=\"on\" data-total=\"1\" data-yotu=\"63ac7c3e00d4d\" id=\"yotuwp-63ac7c3dbd198\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> Decentralized Web Summit - Live From The Internet Archive </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3e00d4d\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> We are bringing together a diverse group of Web architects, activists, engineers, archivists, scholars, journalists, and other stakeholders to explore the technology required to build a Decentralized Web and its impact.  \n  \nSpeaker include: Mitchell Baker, Vint Cerf, Cory Doctorow, Brewster Kahle, Tim Berners-Lee  \n  \nJoin the conversation by Tweeting: #DWebSummit  \n  \nLive Streaming &amp; Video Production by: http://www.argushd.com </div> </div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![Decentralized Web Summit - Live From The Internet Archive](https://i.ytimg.com/vi/Yth7O6yeZRE/sddefault.jpg)</div> </div>### Decentralized Web Summit - Live From The Internet Archive\n    \n    <div class=\"yotu-video-description\">We are bringing together a diverse group of Web architects, activists, engineers, archivists, scholars, journalists, and other stakeholders to explore the technology required to build a Decentralized Web and its impact.  \n      \n    Speaker include: Mitchell Baker, Vint Cerf, Cory Doctorow, Brewster Kahle, Tim Berners-Lee  \n      \n    Join the conversation by Tweeting: #DWebSummit  \n      \n    Live Streaming &amp; Video Production by: http://www.argushd.com</div> ](#Yth7O6yeZRE \"Decentralized Web Summit - Live From The Internet Archive\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Decentralized Web Summit Day 2](https://i.ytimg.com/vi/PfWgin3JlAU/sddefault.jpg)</div> </div>### Decentralized Web Summit Day 2\n    \n    <div class=\"yotu-video-description\">Lightning Talks and Workshops  \n    The day after the Decentralized Web Summit, we will be hosting a Decentralized Web meetup, streamed live here.   \n      \n    SESSION A: Lightning Talks and Workshops  \n    Lightning talks in the Greatroom. Facilitated by Primavera De Filippi. Each presenter will give a 5-minute overview, followed by 5 minutes of Q &amp; A.   \n      \n    For a full days schedule, please visit: http://www.decentralizedweb.net/schedule/#meetup  \n      \n    Video Production &amp; Live Streaming Services provided by: http://argushd.com</div> ](#PfWgin3JlAU \"Decentralized Web Summit Day 2\")\n\n</div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div> </div> </div>For more information see: https://2016.decentralizedweb.net/"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","title":"OpenRefine"},"frontmatter":{"draft":false},"rawBody":"---\nid: 74\ntitle: OpenRefine\ndate: '2017-07-21T16:56:12+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=74'\npermalink: /74-2/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/openrefine'\ncategories:\n    - 'Data Cleaning &amp; Conversion'\n    - 'Product and Service Reviews'\n    - RDF\n---\n\nOpenRefine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; and extending it with web services and external data.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/videoseries?list=PL737054C67FCC0741\" width=\"560\"></iframe>\n\nImportantly, OpenRefine also includes an array of [plugins](http://openrefine.org/download.html) that helps users sort out data, then update it and work with data in an RDF format"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","title":"Advanced Search &#038; Discovery Tips"},"frontmatter":{"draft":false},"rawBody":"---\nid: 151\ntitle: 'Advanced Search &#038; Discovery Tips'\ndate: '2017-07-21T15:23:16+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=18'\npermalink: /advanced-search-discovery-tips/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/advanced_search___discovery_tips'\ncategories:\n    - Discovery\n    - 'Using Google'\n---\n\nIt’s amazing what you can find simply by searching the web. Advanced techniques for searching the web have an array of tools and tips, that we’ll try to cover here. In the first instance, i’ll highlight some of the more advanced capabilities of Google and how to use it.\n\n### Google Custom Search Engine\n\nGoogle [Custom Search Engine](https://cse.google.com) is a tool that can be used to create parameters around a type of search using the [linked data](https://openseason.online/2017/07/21/introduction-to-linked-data/) markup that’s often embedded for SEO purposes using [Schema.org](http://schema.org/) or by specifying a particular type of syntax that restricts the search to a particular zone. For example, if you are searching for information about a person; then in your advanced options within your custom search you’d add the schema concept [Person](http://schema.org/Person).\n\nor if you wanted to find information within a particular zone, for instance on Australian Websites then you’d put in the \\*.au. Not all websites use schema.org mark-up and if they do, they don’t necessarily do it well. So this may restrict the results you get if you get more specific than web developers have otherwise considered.\n\n### Advanced Search\n\nGoogle knows how to search more than simply entering a word or sentence. Here are a few examples.\n\nThe first important consideration is the use of “to declare particular words”. For example, if we search “[telecommunications law](http://lmgtfy.com/?q=%22Telecommunications+Law%22)” we’ll get one set of results. If you wanted to find a particular document, then you might also add filetype: (then extension) for example: [“telecommunications law” filetype:pdf](http://lmgtfy.com/?q=%22Telecommunications+Law%22+filetype%3Apdf)\n\nOther examples have been listed [here](https://support.google.com/websearch/answer/2466433?hl=en) noting that these methods can also be used with Google [Advanced Search](https://www.google.com/advanced_search).\n\nMaking things even more interesting; once you’ve figured all that out; you’re then able to set-up an [Alert](https://www.google.com/alerts) which will do the search and provide you any new results into your inbox when they become available to search, on a frequency you choose."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","title":"Downloading My Data from Social Networks"},"frontmatter":{"draft":false},"rawBody":"---\nid: 150\ntitle: 'Downloading My Data from Social Networks'\ndate: '2017-07-21T14:46:54+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=14'\npermalink: /downloading-my-data-from-social-networks/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/downloading_my_data_from_social_networks'\ncategories:\n    - 'Acquiring Data'\n---\n\nAlmost anyone on the web today has accounts on social-network websites. These websites offer a rich source of data for users, but it’s often difficult to get unless you know how.\n\n### FACEBOOK\n\nFacebook has a help link that can be found in [google](http://lmgtfy.com/?q=how+to+download+my+facebook+data). the current information about how to download your data can be found [here](https://www.facebook.com/help/131112897028467).\n\n### TWITTER\n\nTwitter is very similar to Facebook. The current link via google can be found [here](http://lmgtfy.com/?q=how+to+download+my+twitter+data)\n\n### LinkedIn\n\nSame as above. [see link](http://lmgtfy.com/?q=download+my+linkedin+data)\n\nEvery other social-network site i am aware of has a similar means in which it makes it easy to download your data into an archive format of some sort. Once you have the data, you then need to figure out how to make it available in a useable format. We’ll cover that in a seperate post."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","title":"Google Tracking Data (geolocation)"},"frontmatter":{"draft":false},"rawBody":"---\nid: 152\ntitle: 'Google Tracking Data (geolocation)'\ndate: '2017-07-21T15:36:35+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=22'\npermalink: /google-tracking/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/google_tracking_data_geolocation'\ncategories:\n    - 'Acquiring Data'\n    - 'Using Google'\n---\n\nHave you ever got a traffic infringement and couldn’t figure out what it was about? Have you wanted to look back at what you did some day in the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-a5adf503-2bb1-de71-a3ee-3f695523f562\" itemid=\"http://data.wordlift.io/wl0293/entity/past\">past</span>, and wished you had a record of some form?\n\nWell, if you’re not too sensitive about <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-ytb0we4fv8h4r7p1bkev59iemyem4uct\" itemid=\"http://data.wordlift.io/wl0293/entity/tracking_data\">tracking systems</span> <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-0b21c576-68df-1f14-0a0b-82fc5c207c09\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>; you might find [google maps timeline](https://www.google.com/maps/timeline) <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-e08c5d32-9838-050a-5d3d-dec4d7b372a8\" itemid=\"http://data.wordlift.io/wl0293/entity/software_feature\">feature</span> really very useful.\n\nWhilst you need to [turn it on](https://support.google.com/maps/answer/6258979), the means to have the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-60c25b3e-c927-1411-f471-046044408271\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> about where you were at a particular time in the past; might be more helpful in the future than you might otherwise know.\n\nThe google maps system uses your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-8191cbb2-d042-e694-7bb3-4bfb34cf059c\" itemid=\"http://data.wordlift.io/wl0293/entity/geolocation\">Geolocation</span> information from your phone that is otherwise tracked by <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-d03b3f10-0d3a-5186-6d38-94ee500d8188\" itemid=\"http://data.wordlift.io/wl0293/entity/telephone_company\">telecommunications companies</span> and apps on your phone. <span class=\"textannotation disambiguated wl-organization\" id=\"urn:enhancement-4eadd5dd-a3a6-7afc-4a31-c76dab959d8b\" itemid=\"http://data.wordlift.io/wl0293/entity/google\">Google</span> <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-ed78579d-94d6-bc0e-1bb7-db5ae23b548e\" itemid=\"http://data.wordlift.io/wl0293/entity/map\">Maps</span> <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-998915d5-8baf-b0a8-1d2b-9f2f2497684c\" itemid=\"http://data.wordlift.io/wl0293/entity/timeline\">timeline</span> is a way you can have access to that same <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-ee121c24-d035-03bc-98da-4fda33f7b354\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>.\n\nBetter yet; it’s [downloadable](https://takeout.google.com/settings/takeout/custom/location_history).\n\nIt’s important you don’t use this functionality on an account that is not yours and that you have no right / permission, to use or obtain. Whilst considerations about <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-ca641fb1-6799-ca90-677f-7b284e7fc802\" itemid=\"http://data.wordlift.io/wl0293/entity/privacy\">privacy</span> and <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-ccb1c7d8-c8fc-eb76-db8f-e26c9efe3048\" itemid=\"http://data.wordlift.io/wl0293/entity/surveillance\">surveillance</span> is different in different jurisdictions, it’s generally not a good thing to do to another person unless they’ve explicitly asked you to do so."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","title":"Introduction to Linked Data"},"frontmatter":{"draft":false},"rawBody":"---\nid: 149\ntitle: 'Introduction to Linked Data'\ndate: '2017-07-21T14:20:29+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=9'\npermalink: /introduction-to-linked-data/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/introduction_to_linked_data'\ncategories:\n    - RDF\n    - 'Technical Introductions'\n---\n\n# What is <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-16v2koc9fcuj5lwn7helj9zpo9t4iqv4\" itemid=\"http://data.wordlift.io/wl0293/entity/linked_data\">Linked Data</span>?\n\n[Linked Data ](https://www.w3.org/DesignIssues/LinkedData.html) is a way of authoring a hypertext <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-py1vhg5rmizqvkpsoppasnbr57uh45vn\" itemid=\"http://data.wordlift.io/wl0293/entity/document\">document</span> in a way that makes the concepts and relations described in the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-wddomh6twonz5937lfj4vn57go0d4f4u\" itemid=\"http://data.wordlift.io/wl0293/entity/document\">document</span> <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-hwb69s7v821zpra9fa1m7apreq5agcxg\" itemid=\"http://data.wordlift.io/wl0293/entity/machine-readable_data\">machine readable</span>.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/videoseries?list=PLCbmz0VSZ_vqZemkHpzb7-GWmzcUdGgfP\" width=\"560\"></iframe>\n\n## How is it used on the web?\n\nThe easiest way to see Linked-Data on the web is to use a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-v41hx6r7o69lyltajhewoi87d6g5y3yl\" itemid=\"http://data.wordlift.io/wl0293/entity/tool\">tool</span> that helps you see the information embedded into the web pages you use. These <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-lkb9k2m2jt9t5lnjgkl1iqj59t098w2r\" itemid=\"http://data.wordlift.io/wl0293/entity/tool\">tools</span> show you how machines can read the information embedded in web-pages, which can be used for many purposes; including referencing the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-tprujk1lg743cndmgi21lhrbhklhj7e5\" itemid=\"http://data.wordlift.io/wl0293/entity/machine-readable_data\">machine-readable data</span> in those pages in <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-kjybknl5djrua1p4gfmcg62wjdzzn3nd\" itemid=\"http://data.wordlift.io/wl0293/entity/application_software\">applications</span> you might want to develop that can utilise this data, from all the webpages you reference in your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-yox8wgo494sngzhrf18v2fcg3zb65vdk\" itemid=\"http://data.wordlift.io/wl0293/entity/application_software\">application</span> to create what is called a ‘graph’ of information about any particular subject.\n\nSome of the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-9gtop1z4mljv6puw5iarbnxp1dnu2428\" itemid=\"http://data.wordlift.io/wl0293/entity/tool\">tools</span> to see the data in webpages includes the [OpenLink Data Sniffer](http://osds.openlinksw.com/) alongside specialist <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-qxszfcx61zhkyjjhb1ukm1p0ib0t06qi\" itemid=\"http://data.wordlift.io/wl0293/entity/tool\">tools</span>, such as [Google’s Structured Data Testing Tool](https://search.google.com/structured-data/testing-tool).\n\nMost major websites use <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-pcsazxjw5gkcuu6w0erg55lleei7fvxf\" itemid=\"http://data.wordlift.io/wl0293/entity/linker_computing\">linked</span>data; but they’re not all using the same languages. Linked-Data languages are called ontologies. An ontology is a specially defined language or machine-readable dictionary, that’s been designed for some specified purpose. Websites such as <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-0cjhn8edxdm6cntqa1gosmkvzcu9zym6\" itemid=\"http://data.wordlift.io/wl0293/entity/wikipedia\">wikipedia</span> is made available as <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-5zaduzzk3p9pl0ctbz20wdx9akagec2d\" itemid=\"http://data.wordlift.io/wl0293/entity/linker_computing\">linked</span>-data via [<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-qv8gk7a9y2sq6hbncaihk0n7ycsb7g6z\" itemid=\"http://data.wordlift.io/wl0293/entity/wikidata\">wikidata</span>](http://wikidata.org).\n\nFor a more comprehensive list of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-ldgcgmimc51qywpn7tg5u69qk0quns5j\" itemid=\"http://data.wordlift.io/wl0293/entity/linker_computing\">linked</span>-data ontologies the website [LOD (linked open data) Cloud](http://lod-cloud.net/) provides a navigable <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-gg22ie60vewn2sbj0x31affrbcax3s7e\" itemid=\"http://data.wordlift.io/wl0293/entity/tool\">tool</span> to find various ontologies for almost any purpose. Other <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-gwugtbfa9h5lddf3so4hxjcetafince0\" itemid=\"http://data.wordlift.io/wl0293/entity/tool\">tools</span> like [LinDA](http://linda.epu.ntua.gr/) can be used to <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-qis5i0xeg6dyuyn732evy8t9mpphemr5\" itemid=\"http://data.wordlift.io/wl0293/entity/search_engine_technology\">search</span> for particular terms and find different ontologies that define those terms in a machine-readable way. The most popular ontology for <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-09hy3ctfe6ekdpn32lx46nr8eaphy41r\" itemid=\"http://data.wordlift.io/wl0293/entity/search_engine_technology\">search</span>-engine optimisation is called schema.org and is continually developed via a [github issues list](https://github.com/schemaorg/schemaorg/issues/) and related [W3C Community CG](https://www.w3.org/community/schemaorg/)."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","title":"Introduction to Maltego"},"frontmatter":{"draft":false},"rawBody":"---\nid: 153\ntitle: 'Introduction to Maltego'\ndate: '2017-07-23T11:21:42+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=145'\npermalink: /introduction-to-maltego/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/introduction_to_maltego'\ncategories:\n    - 'Acquiring Data'\n    - 'Product and Service Reviews'\n    - Tools\n    - Uncategorized\n---\n\nWhat is <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-c358392a-dd38-f998-7f37-bb0dac6c3bc2\" itemid=\"http://data.wordlift.io/wl0293/entity/maltego\">Maltego</span>?\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/kZWTMvORjCA\" width=\"560\"></iframe>  \n<span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-b27742e1-5037-6617-446d-0861029d51b4\" itemid=\"http://data.wordlift.io/wl0293/entity/maltego\">Maltego</span> is a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-7ad9a5c8-805c-e957-acf2-2bb8b4ed0236\" itemid=\"http://data.wordlift.io/wl0293/entity/tool\">tool</span> that’s [available](https://www.paterva.com) on dual licensing enabling commercial use or freely, providing a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-4cb73e1e-5970-772b-1b24-488bc5ca484b\" itemid=\"http://data.wordlift.io/wl0293/entity/tool\">tool</span> that’s used to investigate relationships using data, then map, store and print reports from those investigative views that makes it far more difficult for others to ignore.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/JTZfkQzMaPc\" width=\"560\"></iframe>\n\nI first stumbled across it when reviewing the information provided by Facebook to professional users, such as app providers, and the means in which that data subsequently allows them to facilitate advanced behavioural analysis as an international commercial entity\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/g6sOi9FOz2o\" width=\"560\"></iframe>\n\n(it’s generally rather difficult for a person to participate in society without <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-0af217cf-0ff0-cb13-50eb-688e3680b1f6\" itemid=\"http://data.wordlift.io/wl0293/entity/social\">social</span> media accounts; the above video gives some insight into what that costs).\n\nThe commercial version of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-f3dbf4b8-9787-132e-97f1-93a09930684e\" itemid=\"http://data.wordlift.io/wl0293/entity/maltego\">Maltego</span> offers more features and <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-c0d54382-1721-d34f-25e2-b7a694230d77\" itemid=\"http://data.wordlift.io/wl0293/entity/plug-in_computing\">plugins</span> that are otherwise not available in the community edition. One example is the [Social Links](https://mtg-bi.com/) framework that provides enhancements for <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-39755064-7973-d29d-4da6-02742db1ab04\" itemid=\"http://data.wordlift.io/wl0293/entity/social\">social</span>-network analysis than is otherwise provided ‘out of the box’ by the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-e4e51125-d705-f976-dd03-2c678ad8054f\" itemid=\"http://data.wordlift.io/wl0293/entity/maltego\">Maltego</span> community edition. Whilst SocialLinks is only one example, their videos can be found [here](https://www.youtube.com/channel/UCcFD986JKvXXU88w0SRfvow/videos)."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","title":"Web-Persistence"},"frontmatter":{"draft":false},"rawBody":"---\nid: 84\ntitle: Web-Persistence\ndate: '2017-07-23T09:35:08+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=84'\npermalink: /web-persistence/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/web-persistence'\ncategories:\n    - 'WWW Education'\n---\n\n<span style=\"font-weight: 400;\">I’m not quite sure what to title this section. Many speak of this concept as <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-yt9vbrswgsttwq9izj3ppeetg3ufdw1x\" itemid=\"http://data.wordlift.io/wl0293/entity/digital_identity\">digital identity</span> <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-70fb9b4d-e234-3c83-bb9a-b8fc05b5f0af\" itemid=\"http://data.wordlift.io/wl0293/entity/persistence_computer_science\">persistence</span>, yet often it’s not the person that is subject to ‘web <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-97834349-f33c-1316-e95d-33fd6e233637\" itemid=\"http://data.wordlift.io/wl0293/entity/persistence_computer_science\">persistence</span>’ but rather the machine or home <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-84e8965c-02d3-2477-3b8e-1fe66348d180\" itemid=\"http://data.wordlift.io/wl0293/entity/network_address\">network address</span> that provides persistent information about the characteristics of a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-342eb926-d76a-b96d-3acc-531d7119272c\" itemid=\"http://data.wordlift.io/wl0293/entity/user_computing\">user</span>; regardless of who the actual <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-98481615-a778-26fe-dace-3db49977a552\" itemid=\"http://data.wordlift.io/wl0293/entity/user_computing\">user</span> is. </span>\n\n<span style=\"font-weight: 400;\">This can end-up with an array of unfortunate situations. A father, mother or other adult in a household who enjoys adult material; may unwittingly alter the website advertising being provided to others in the household who <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-c1392b23-0e25-df3f-1824-a3c3fbf33c6a\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> the same <span class=\"textannotation disambiguated wl-organization\" id=\"urn:enhancement-9c868d97-d0b5-5832-1ace-52a7053aac62\" itemid=\"http://data.wordlift.io/wl0293/entity/internet_access\">internet connection</span> (children included!). Families who share machines and the accounts set-up in those machines may create web-experiences that pollinate in different ways irrespective of the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-8c3845e1-b106-df2e-bb19-3ed5afc4901c\" itemid=\"http://data.wordlift.io/wl0293/entity/user_computing\">user</span> at the time. </span>\n\n<span style=\"font-weight: 400;\">These issues pertain to what i’ll call ‘web <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-5e1d5167-2650-54a4-12a5-5782282549de\" itemid=\"http://data.wordlift.io/wl0293/entity/persistence_computer_science\">persistence</span>’ in describing the circumstance that the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-bcb62d82-8db8-62bb-d77f-55d36240676a\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> of internet is tracked by operators of the internet who work with whatever information they can get, altering the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-834a4c1f-c066-01e5-2416-eefb03154c66\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> of internet on that machine, in that <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-a77ba906-49ef-2a69-3738-7487dd9be40c\" itemid=\"http://data.wordlift.io/wl0293/entity/location_geography\">location</span>, from whatever account on the machine the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-b720bf55-4128-e737-3312-795d90c9c615\" itemid=\"http://data.wordlift.io/wl0293/entity/user_computing\">user</span> is using; in an effort sought by them to make money through your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-ee74ec76-efa5-1c9a-bcaa-81f0796f5f2d\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> of internet and they do this through the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-dae334dc-ccbd-c502-b83d-8d5a9fdd2f54\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-46c245d3-48fe-2e68-296b-9ed95cd13ba9\" itemid=\"http://data.wordlift.io/wl0293/entity/identifier\">identifiers</span>, and ‘scraps’ of information left-over from previous uses in relation to those <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-23713751-b8f0-04fb-801e-e97b1f530cbe\" itemid=\"http://data.wordlift.io/wl0293/entity/identifier\">identifiers</span>. The systems that collect this information is not simply the website you intend to go to, but also the services that websites uses as part of providing the functionality delivered by the sites you visit. When thinking about this from a security point of view, the term that is used is ‘vectors’. The concept being ‘attack vectors’ or ‘security vectors’ or other forms of ‘vectors’ that can be used to trace, track and identify.</span>\n\nAn easy way to understand the different ways this may occur is by considering the [OSI Model](https://en.wikipedia.org/wiki/OSI_model).\n\nEach Machine has a [MAC ADDRESS](https://en.wikipedia.org/wiki/MAC_address), which in-turn connects to a network and is provided an [IP ADDRESS](https://en.wikipedia.org/wiki/IP_address). From there, your machine forms a [fingerprint](https://en.wikipedia.org/wiki/Device_fingerprint).\n\nParts of this digital <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-gz84f8wzobuxmmcp2du83s0jzyy63lay\" itemid=\"http://data.wordlift.io/wl0293/entity/fingerprint\">fingerprint</span> includes your [User Account](https://en.wikipedia.org/wiki/User_(computing)), the [host IP Address](http://lmgtfy.com/?q=ip+address) used by the network you are able to be sent webpages from publicly and the [information stored by your browser](https://en.wikipedia.org/wiki/Internet_privacy#Risks_to_Internet_privacy), such as cookies, or login information within your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-534de5d7-6061-f2e1-aa10-29d4dc87e35b\" itemid=\"http://data.wordlift.io/wl0293/entity/web_browser\">browser</span> that websites may use to infer you were using the internet in a particular way; regardless of whether it was you at your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-8t7v1s2dg34tp74r8l26vzu00phk37ko\" itemid=\"http://data.wordlift.io/wl0293/entity/computer_keyboard\">keyboard</span> or someone else."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","title":"What is Open Source Intelligence?"},"frontmatter":{"draft":false},"rawBody":"---\nid: 133\ntitle: 'What is Open Source Intelligence?'\ndate: '2017-07-23T10:43:08+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=133'\npermalink: /what-is-osint/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/what_is_open_source_intelligence_'\ninline_featured_image:\n    - '0'\ncategories:\n    - ideas\n    - Tools\n---\n\n> <div class=\"mod\" data-md=\"61\"><div class=\"_oDd\" data-hveid=\"45\"><span class=\"_Tgc\">Open Source Intelligence (**OSINT**</span>) is a term used to refer to the data collected from publicly available sources to be used in an intelligence context. In the intelligence community, the term “open” refers to overt, publicly available sources (as opposed to covert or clandestine sources).”</div></div>\n\n<div class=\"mod\" data-md=\"61\"><div data-hveid=\"45\">Source: [OSINT WikiPedia](https://en.wikipedia.org/wiki/Open-source_intelligence)</div></div><div class=\"mod\" data-md=\"61\"><div data-hveid=\"45\"></div><div data-hveid=\"45\">In simple terms, Open Source Intelligence is the use of publicly available datasources and the data sources made available to you (legitimately) to communicate an issue using data that you’re inspired enough to undertake the exhaustive task of collecting, collating and representing that issue in a manner that may be taken more seriously by law enforcement officials, lawyers, medical clinicians or other parties you provide that information to.</div><div data-hveid=\"45\"></div><div data-hveid=\"45\">Indeed, so long as it’s public information and the information you have rightful use to use; and that information is true and correct, with appropriate consideration is unlikely anyone is able to stop or punish you for publishing it on a website, mark it up with the appropriate tools and create an outcome that’s likely to be top search result for those mentioned in the output.</div><div data-hveid=\"45\"></div></div>[IMHO](https://en.wiktionary.org/wiki/IMHO) it’s important not to make problems those of children or others. Where others fail to do their job, the means made available by OSINT techniques can clarify circumstances in ways that may be reviewed by others as to resolve problems. It’s important not to be a problem whilst using these techniques."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","title":"The WayBack Machine"},"frontmatter":{"draft":false},"rawBody":"---\nid: 154\ntitle: 'The WayBack Machine'\ndate: '2017-07-27T11:36:06+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=153'\npermalink: /the-wayback-machine/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/the_wayback_machine'\ncategories:\n    - Uncategorized\n---\n\n[The WayBack Machine](https://archive.org/web/) in an <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-b6800efc-2843-2b6b-ff0f-099a00caa556\" itemid=\"http://data.wordlift.io/wl0293/entity/archive\">archive</span> of more than 299 billion webpages. From the very first version of [Google](https://web.archive.org/web/19981111183552/http://google.stanford.edu:80/) or [wikipedia](https://web.archive.org/web/20010727112808/http://www.wikipedia.org:80/) to through to collections of the ‘[pioneers](https://web.archive.org/web/20011204181425/http://web.archive.org:80/collections/pioneers.html)‘ and the means to look-up most websites historically, to resource content that may have otherwise changed or is no-longer available on the web.\n\nA [Plugin for Chrome](https://chrome.google.com/webstore/detail/wayback-machine/fpnmgdkabkmnadcjpehmlllkndpkmiak) is also available which helps to easily find the latest copies of webpages that may have been removed or taken down."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","title":"Choice of Law"},"frontmatter":{"draft":false},"rawBody":"---\nid: 165\ntitle: 'Choice of Law'\ndate: '2017-12-26T22:47:45+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=165'\npermalink: /choice-of-law/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/choice_of_law'\ncategories:\n    - 'Web and the Law'\n---\n\n<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-alj3bnfkkxlpzuanqh8lqsbzetbbrtks\" itemid=\"http://data.wordlift.it/wl0353/entity/choice_of_law\">Choice of law</span> is the method provided to govern the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-jz48x0otiwezvw80lurs7egfg1bmfgon\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> of websites by way of international law. Each area (ie: territory, <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-qw0944k1lmkd0cbyydvuulvdhxes80lf\" itemid=\"http://data.wordlift.it/wl0353/entity/sovereign_state\">state</span> and country) has different laws and as the web developed databases to store peoples <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-q0q1qly98qcqq4p8zgtrwrmg1ymk6fp9\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>; it was deemed to be too difficult to support the intepretation of law as it applies to individuals.\n\nEach website therefore uses ‘<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-98twkaq9oeoz2daqyi5fv8s2p9zg822k\" itemid=\"http://data.wordlift.it/wl0353/entity/choice_of_law\">choice of law</span>‘ as a means to govern the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-3e78vp5kffmci76ob0rroctewlxxqkmb\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> of their products (and your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-6nv7ehe5m4671a3l7kspr2wyhhpjln5m\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>).\n\n<iframe height=\"480\" loading=\"lazy\" src=\"https://www.google.com/maps/d/embed?mid=1bHmB8_f7ASRHm97TwhZmmEQnTKU\" width=\"640\"></iframe>  \nThe embedded map shows ‘<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-44n35j19e7y4tj284yj9txikuc61fats\" itemid=\"http://data.wordlift.it/wl0353/entity/choice_of_law\">choice of law</span>‘ as it applies to some of the more popular products and <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-b0kavq6a22o879ap9mqjh3rw1x9th94o\" itemid=\"http://data.wordlift.it/wl0353/entity/service_economics\">services</span> provided on the Web.\n\nThe examples provided above (as mapped in 2016) show where the ‘governing law’ is applied for the software based <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-p6vzxdh4btb98bohb66vbc1ccwryzq1a\" itemid=\"http://data.wordlift.it/wl0353/entity/service_economics\">services</span> provided world-wide. The implications are enumerate. Put simply, the intepretation of many local laws, including Telecommunications and intellectual property; is governed by default on the intepretation of laws in the territory for which ‘<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-c9xh8skn4mcadnhmt7ng6ch88a8r2q4c\" itemid=\"http://data.wordlift.it/wl0353/entity/choice_of_law\">choice of law</span>‘ is claimed by way of the ‘terms of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-tqybeal2blxpxj4az3saf6g92rirhp6w\" itemid=\"http://data.wordlift.it/wl0353/entity/service_economics\">service</span>‘ or agreement made when electing to <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-y8nrppizecmvykv20tu7kjdtd598lfbm\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> the website.\n\nWhilst Governments and Enterprise may enter into agreements to vary the terms in which they <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-04yz732dleahdiextcolvrs6uo7p8evh\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> the products and <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-259aoc0gbhfp3wzjdilntngm2vdwelrb\" itemid=\"http://data.wordlift.it/wl0353/entity/service_economics\">services</span> provided by these organisations; individuals, or <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-p6ecaibghcixb5k6d562pc6ilqjzse1o\" itemid=\"http://data.wordlift.it/wl0353/entity/citizenship\">citizens</span> of sovereign countries around the world are not reasonably able to do so. Where disputes come about, it is expected that the user seek legal intervention by way of a court in the territory nominated by the ‘<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-b4m26yn25ncy8rxuh9mmjlfptr0vhkdg\" itemid=\"http://data.wordlift.it/wl0353/entity/choice_of_law\">choice of law</span>‘ contract; and indeed, if the user does not agree to this, then they should not ‘click the button’ which in-turn means, they should not <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-51rpedj3g8w50oj9s3wqlayl0rhs6rvl\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> the website. This becomes particularly difficult pragmatically when considering the implications of not using some of these sites and/or software <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-2km0pydpd2y5vm7ijraehckpy69m755u\" itemid=\"http://data.wordlift.it/wl0353/entity/service_economics\">services</span> (ie: mobile phone operating systems).\n\nFurthermore; governments do not hold the same expectations of legal responsibility over foreign nationals (‘legal aliens’) as they do for residential <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-zq7tyh4zexhixnwzkv8qqj97acp0kggw\" itemid=\"http://data.wordlift.it/wl0353/entity/citizenship\">citizens</span>. This is a complex area of Web Science that has largely been left without broad community engagement, discussion and consideration.\n\nTraditionally; software products, before the widespread <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-623wj2fdxvxtl5nazpa3lg6dckrnw896\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> of internet, maintained their own ‘<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-93xkvicwnqsnonxuk1ym4kp8lbt1kev2\" itemid=\"http://data.wordlift.it/wl0353/entity/choice_of_law\">choice of law</span>‘ in software licenses as to protect their products from wrongful exploitation and/or misuse. These forms of principles are still, many would consider, quite reasonable. Yet these principles have in-turn been applied to the accumulation of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-dophh258b7uqct7c6qsisls9q5jtai6v\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> that was previously stored by individuals (for example, on floppy disks) and is now stored by the website to which a license is granted as part of the usage agreement and its terms.\n\nAs <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-cqjqj8qbvc57pq3vmzdhetmyw89hn77o\" itemid=\"http://data.wordlift.it/wl0353/entity/internet\">Internet</span> related technologies continue to develop modern, dynamic &amp; ‘artificial intelligence’ empowered <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-mtkpol65fds7qgvaly3v8x6mp9n3vahk\" itemid=\"http://data.wordlift.it/wl0353/entity/service_economics\">services</span> through the utility of these <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-w3i70txdg783xbnutofgfr1xsfe6pm4n\" itemid=\"http://data.wordlift.it/wl0353/entity/service_economics\">services</span> one might consider that perhaps the application of a legal framework as initially designed to protect the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-i3f0b2zqk3kmhbb5q2u5ejvfnvfiyc7f\" itemid=\"http://data.wordlift.it/wl0353/entity/creative_work\">creative work</span> of the software vendor; may not be the most appropriate asymmetrical framework to apply to the ‘knowledge economy’ powered by humans in conjunction with the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-k782n35lf0qscfusa09vwck6e3usn1gk\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> of these globally integrated products and <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-nqkq6og832qkvufhnewi1cdp3wo5epfn\" itemid=\"http://data.wordlift.it/wl0353/entity/service_economics\">services</span>.\n\nMeanwhile; regardless of how a ‘consumer’ data is stored or considered by way of law interoperable with participating entities; it is still expected that citizens maintain adherence to their own choice of law, as a ‘natural legal entity’ / consumer.\n\nDISCLAIMER:\n\nThis article may contain errors. For specific legal advice is it advised interested parties seek professional advice from a legal professional."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","title":"Data Recovery: Laptop &#038; Computers"},"frontmatter":{"draft":false},"rawBody":"---\nid: 201\ntitle: 'Data Recovery: Laptop &#038; Computers'\ndate: '2017-12-28T03:41:10+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=201'\npermalink: /data-recovery-laptop-computers/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/data_recovery__laptop___computers'\ncategories:\n    - 'Acquiring Data'\n---\n\nData Recovery on Computers and Laptops can be a complex tasks, and in most cases quite time-consuming. In cases where physical hardware damage is the case of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-3f623f84-551c-3de2-b466-3626dc145241\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>-loss, the likelihood of getting the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-61995af6-3e81-5542-71fc-fc998a0a41ac\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> back – goes down…\n\nIn past experience, even the same type of drive produced via a different batch; the parts won’t work on the old drive. This is something to take into consideration if you or your organisation is storing important <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-31cc9cdc-d915-ac26-49e1-88af2c2c7e91\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>. When purchasing the storage devices (ie: IDE/<span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-59b86208-3fc1-4704-f6c6-d9865b00123c\" itemid=\"http://data.wordlift.it/wl0353/entity/serial_ata\">SATA</span> based <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-a93d14f0-fd6e-3090-6849-7d48da5df9a1\" itemid=\"http://data.wordlift.it/wl0353/entity/disk_storage\">drives</span> that are not Solid state) it might well be worth purchasing a spare or two, or ensuring a spare is available; to strip the daughter-board off the drive, with the same manufacture codes; as to retrieve lost <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-50486fee-1da6-64de-0936-43a89fdebe83\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> in the case the daughter-board dies…\n\nFurthermore; It is not advisable to create a stripped array over a multitude of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-80e3a870-594c-5ced-bba0-eb6ddda0818d\" itemid=\"http://data.wordlift.it/wl0353/entity/disk_storage\">disks</span>, if you at all value the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-adb232f9-52f6-3ffb-359a-b1a8d98921f2\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> you intend to store of that <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-cb5dbc0b-53a5-4c53-1717-2ac771ef13d5\" itemid=\"http://data.wordlift.io/wl0293/entity/data_storage_device\">storage device</span>.\n\nIf you’re just looking for an ultra-quick cache for content / <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-0443d93d-e39c-5812-4a63-7c1f89c8df89\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> you have stored elsewhere; then, yeah. just don’t trust it for long-term storage.\n\nPROCESS FOR RETRIEVING DATA FROM ‘COMPUTERS’.\n\nFor non-technical people who don’t know the difference between the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-8208e99a-e7f9-d440-d8bc-437aacca7652\" itemid=\"http://data.wordlift.io/wl0293/entity/data_storage_device\">storage device</span> and the ‘computer’; most computers have a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-31a62ff8-5351-d19b-f523-08d2cbe35e27\" itemid=\"http://data.wordlift.io/wl0293/entity/data_storage_device\">storage device</span> part that is able to be removed from a computer, even when the computer doesn’t work.\n\nMore common examples of where this happens; is where the drive is a little faulty, and kinda works, sometimes. Or where the power-supply or some other part in the computer stopped working, and the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-9fb0bbf7-9b2c-55bf-4d24-2f7e1675c1d7\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> is ‘trapped’.\n\nOther examples is where something bad has happened. You know there should be a record of it in the computer; but it’s not obvious, and, you want to check it out.\n\nSTEP 1: REMOVE THE STORAGE DEVICE\n\nIf you can’t remove the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-6415bf72-93fa-b4a7-1c73-165b9966f3d6\" itemid=\"http://data.wordlift.io/wl0293/entity/data_storage_device\">storage</span> device, you’re not going to be having much joy. Some newer computers have their <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-b8bb378a-2aec-697d-4ab5-d1406ffb942a\" itemid=\"http://data.wordlift.io/wl0293/entity/data_storage_device\">storage</span> devices fixed into the circuitry of the device and well; if it don’t work, you’re going to be in trouble.\n\nfor the majority of computers over the past 20+ years; they can be removed.\n\nWhat you don’t want to be doing, is writing anything to that <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-34fd9357-54a7-55a9-26d0-bc04b5d137c0\" itemid=\"http://data.wordlift.it/wl0353/entity/disk_storage\">disk</span>. that means, you don’t want to be turning it on or using it, until you’ve tried to get the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-b0f73e02-27b2-1843-3bff-541231ba4e2e\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> you want back.\n\nIf it’s simply a case of the computer dying, and you need to move the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-5124227a-ef72-c0fb-c71a-78e1abf5dac6\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> to your new computer; that’s easier.\n\nIn any-case; find some screwdrivers that are suitable and disassemble the computer to find the hard-drive. If you don’t know what your looking for,\n\na. search google for [‘hard drive’ images](https://www.google.com.au/search?q=Hard+drive&tbm=isch)\n\nb. get someone else to help you.\n\nSTEP 2: Plug the HDD into a new computer\n\nThe local computer shop has an array of cables and cradles that can help you plug your old hard-drive into a new computer. Another option is to get an ‘external case’ for your old hard-drive; if you want to keep it about.\n\nSTEP 3: Download <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-ad90a3d1-7af1-1986-8ad1-8ced813d4d6b\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>\n\nIf you’re simply going to copy the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-5ffb4cec-13db-abe1-2700-1f6249fcb5df\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> from ‘your old computer’ to ‘your new computer’, then that’s relatively straight forward. Browse the directories on the hard-drive and copy them across to your new computers hard drive.\n\nJob done.\n\nIf; you’ve lost <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-86a21176-8da4-3189-2744-c54e4934aef0\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>, the drive isn’t working so well or some other issue; it becomes useful to get another drive with the same amount of space on it, as the one you’re intending to get the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-82aaa801-cb43-b1f3-d001-41a055645ce8\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>-from, to use as a ‘working drive’ to copy all your files across to.\n\nSTEP 4: DATA RECOVERY\n\nSo, the first thing is; do not use the hard-drive you want to get <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-07ed78f0-9ecc-f244-0fbd-0fb5bfa6d873\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>-from, as the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-6d4ef44c-1f82-9ab7-1b2a-f506a7fbe31a\" itemid=\"http://data.wordlift.it/wl0353/entity/disk_storage\">disk</span> you use to turn on the computer, etc.\n\nIf you want to get <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-0258b219-6de2-1c92-4dd2-cc237d4dc35b\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>-back, use a different hdd and plug-in the drive you want to recover <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-eadfc947-2755-ef93-3c0f-5536177df7a5\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> from. It’s also useful to have a second drive, to put the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-891c7a97-a6a7-8061-fb29-6369c803a8f2\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> onto from the drive your recovering <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-4fd0d806-a5d1-d481-0835-b52254fcb828\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> from.\n\nGoto google; search [“data recovery software”](http://lmgtfy.com/?q=%22data+recovery+software%22) to find something that will work on the computer you’re using.\n\nRun the program, target the drive you want to recover from; and store the retrieved <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-c38e42d2-632b-74a1-fdca-2705eda38bc2\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> on the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-e5b06cf3-0287-a73f-3dea-838040ab6bdc\" itemid=\"http://data.wordlift.it/wl0353/entity/disk_storage\">disk</span> you’ve got to back it up onto."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","title":"Data Recovery &#038; Collection: Mobile Devices"},"frontmatter":{"draft":false},"rawBody":"---\nid: 189\ntitle: 'Data Recovery &#038; Collection: Mobile Devices'\ndate: '2017-12-28T03:10:09+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=189'\npermalink: /mobile-devices-data-recovery-collection/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/data_recovery___collection__mobile_devices'\ncategories:\n    - 'Acquiring Data'\n---\n\nHave you got a bunch of important messages on your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-gtmmb9rlz66kzvwxz99qds2qob2czbpf\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span> and you’re wondering how you can store this <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-ys24e1rcjkftbcfjd3dndlfthpjsvys6\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> for safe-keeping. Have you experienced an incident that has made you feel unsafe, and your wondering how to make a record of it to report it to your employer, school or police.\n\nif you type into google [‘templates incident report’](http://lmgtfy.com/?q=templates+%27incident+report%27) you’ll find a bunch of example documents that you can use to make something that suits your purposes.\n\nHowever; one of the problems might be that if you’re simply writing things out, perhaps the matter won’t be taken seriously… not what anyone wants.\n\nFor this reason, and many others, below is an outline of how to get <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-v4u5b12tmlenobcpevyf2o27ynu4l29h\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> out of your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-a6dar3843fw4wpqkn3ipr1y323uzl12c\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span>. We’ll also cover the process just in-case you’ve ‘accidentally’ deleted important <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-9gdb3lulzs5lumyqhyuobcaw6niu7d54\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> on your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-6dxhakg4xuehbimqa4d0l91eu5remopf\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span> already. whilst the method is not 100% successful, it’s a process worth trying out, just in case it makes your life easier.\n\nWe’ll just focus on Android and iOS. Whilst there’s a few other options out there; the majority of the case, it’ll be one or the other.\n\nData collection off most “<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-vw1ymiee2v3hr8rw56o78tggbf550okr\" itemid=\"http://data.wordlift.it/wl0353/entity/smartphone\">smart phones</span>“, is most-often handled by some-app that’s connected to it; whether that be facebook, gmail, twitter or several photo <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-zoipm0ox7pidm6gl4xpuf1dkp5v2b0qi\" itemid=\"http://data.wordlift.it/wl0353/entity/mobile_app\">apps</span>, etc. these systems all store the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-1jq2cp9wzkf072yr4dcho4i2e6vkc3b0\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> within their <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-wkrwvv8aktrzo4ek85rjjgflx99fhj59\" itemid=\"http://data.wordlift.it/wl0353/entity/mobile_app\">apps</span> and so, its alot more complicated to think about how to retrieve anything that may have been deleted within those <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-y2y7ig7kcihqh19qb07fqcpig2mvcrg6\" itemid=\"http://data.wordlift.it/wl0353/entity/mobile_app\">apps</span>; and indeed, the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-16ckvyzbjrocnfuvzbbnlqrjqrf6qmur\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> is stored on the ‘cloud service’, in which case – its’ better to figure out how to [download a copy](https://openseason.online/2017/07/21/downloading-my-data-from-social-networks/).\n\nHowever; Things like SMS’s &amp; Call Logs are a little different. these are generally not stored as part of a cloud-service and need to be retrieved from the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-4k6aj7w7qgv2atdpu2v49us144b4cxl0\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span>.\n\nPART 1: Lets start with a situation where the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-hk04jrrmfcilo2qhe61ldqhwnbs3ezhd\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> you want has been deleted;\n\nSTEP 1.\n\nTry not to use it and do not download anything to the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-lc4nnmzpy8iba0qj9mfb061k3sgxp7gf\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span> in an attempt to get that <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-n7i794d5xaaa1ojfzqyf6azlnkbzldht\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>.\n\nwhen a user tells the operating system managing the device to delete something, it’s generally not deleted. it’s just ‘marked’ for deletion and is no-longer available through the graphical user-interface of the computer, making it ‘deleted’ as far as most people would know. The space is then ‘freed-up’ which means the operating system knows that the area of the storage device used previously to store that <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-jc2qd5ctkn9yztmbc76eijs8l18phqlj\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>; can now be over-written with something else.\n\nWhilst the process of writing to the storage device does not necessarily write over that specific part, its not really very controllable. Sometimes <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-52nw11065odhr27io9dpjf8w8vqhb2dc\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> can remain for years; in other cases, it can be overwritten very, very quickly.\n\n‘<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-bg0u5ly904bczd67brt0t2adeen120dv\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> recovery’ applications that seek the user to download something to the same disk; aren’t the types of tools you want to use.\n\nSTEP 2.\n\nFind an application that works on a Laptop or Desktop Computer. A simple example of how to do this is to type into google [‘iPhone Data Recovery’](http://lmgtfy.com/?q=%27iPhone+data+recovery%27) or [‘Android data recovery’](http://lmgtfy.com/?q=%27android+data+recovery%27).\n\nFeatures you may want to look for;\n\n– What types of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-6pj3s3jv3afbtsv1h3quhva7718990y1\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> application supports retrieving.\n\n– What formats the application outputs the records.\n\nThe benefit of obtaining <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-gxbhr9qq3f9e9juf2lhn08cd293b1zz9\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> in a format such as CSV is that the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-m78vqdyss615ufjnzl7tn9eloiki46h5\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> is thereafter more easily consumed by analytics tools to have a better look into what’s been going on; or how to present that, to others seeking <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-uf3ay747l5b8dab99t3hv5vvvuzq5ymm\" itemid=\"http://data.wordlift.it/wl0353/entity/evidence\">evidence</span>.\n\nSTEP 3.\n\nPlug your mobile device into your computer &amp; download the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-m2j8crzqojrrisbezo0w7sj7ydh2gx02\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>.\n\nSTEP 4.\n\nMake a copy of the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-7grvyupan4rigyupzf5kakxstrxukx38\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> for back-up purposes, and do what you want with the working copy of the recovered <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-octpq40litrlc8df4y6zvmkwrx35rzhq\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>.\n\nPART 2: Data that is still on the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-0ctzrhf3uuc7rbcq66ybtu3k3tf7ooww\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span>, and you don’t need to worry about any deleted records.\n\nSo, if the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-5cmkwyyvezimiudx2o3wvdue2uf4xhao\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> is already on the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-saxbxh4e0zso39eplhk8p3kc20j6q97e\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span> and the whole ‘recovery’ process is unnecessary, then you’ll find a bunch of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-2bdh4gu0huu7m33lhjx0wjbmsx1ivejd\" itemid=\"http://data.wordlift.it/wl0353/entity/mobile_app\">apps</span> online that will work with your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-1q9w1rie4xpfosrtgnpcuueid1y1oksz\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span>, on your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-mezp3yzq9n00b6aqljfagkilaqlq0g42\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span>, to collect and upload your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-prjnullhl5egvwxiy0h9k4vay2nwcx8p\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> to a nominated location.\n\nImportantly; if, you need to make a point about something – an issue you might want to consider is that the ‘metadata’ stored in the files is more easily manipulated when you take that <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-xbkeqatbobqvb81l6026aobfhqnyb2ym\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> off the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-rzdlm61ekzqhzxqaecpsat0myrb6vb58\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span>. Whilst <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-dx9axein88tf2dleiticrd3f6j50ix4m\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>-records like call-logs remain on the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-f4zoj7ja3wp1aoodqu6s5ngzymjgeb6g\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span>; it’s far, far more difficult to manipulate these records. Therefore; in-terms of ‘<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-65lwz61piw5dpwgi3944hgc4ad1cczjl\" itemid=\"http://data.wordlift.it/wl0353/entity/evidence\">evidence</span> collection’, you might find taking a ‘screenshot’ of the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-84wx5wyvzxro2tto8yyow6v6u2putyqq\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> on the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-yagwr1qdrm8d76z8ffm4yakqexikjae4\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span> – to be an important part of your <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-r7wmhu0aililf4kydo607bjr53j9f6em\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>-collection process.\n\nSimilar to the above examples – search google for ‘snapshot android’ or ‘snapshot iOS’ and the method to do so can easily be found.\n\nPART 3: I’ve got voice-mail messages; and, the provider won’t give them to me.\n\nThe method i’ve found to obtain a copy; has been to <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-jvzlvm4ztr6nusrnqw6yaqwcbgjcc67i\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> an audio recorder <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-cuc5jvhtwk55ovnft575bmephgv1frcj\" itemid=\"http://data.wordlift.it/wl0353/entity/mobile_app\">app</span>, put the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-hwqdz6ez6ruu97zlhs4vve2aovl0bp3a\" itemid=\"http://data.wordlift.it/wl0353/entity/iphone\">phone</span> onto speakerphone, and whilst the audio-recorder is working; call the voicemail service and record the messages, including the information about when they were created, etc.\n\nOnce you have obtained these messages; <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-mcfk3kk1gho7q015jn5rjrpxuaztj1tj\" itemid=\"http://data.wordlift.io/wl0293/entity/use_law\">use</span> a audio editing application on a desktop or laptop computer and be sure to add the information about when the recording was made, etc.\n\n**Concluding remarks.**\n\nonce you have the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-41zwp54wf06r0ib81jdamfw6rpwhfy8n\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span> you need, you might find it helpful to log the records chronologically; and have a look at any available metadata that might be available to you, to further illustrate a clearer picture to those who need to know. Obviously, undertaking these sorts of tasks on innocent, unsuspecting 3rd parties without their knowledge is most likely, illegal, but moreover a gross breach of privacy and indeed trust. In some cases, it may be that someone needs help to do these sorts of tasks; in which case, it’s recommended that any would-be ‘good samaritan’ goes about doing it, on the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-d55o9iygtla7i52giiy1fnqwy81tzvma\" itemid=\"http://data.wordlift.io/wl0293/entity/data\">data</span>-owners equipment as to ensure, no lazy copies end-up floating about unnecessarily."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","title":"Basic Media Analysis &#8211; Part 1 (Audio)"},"frontmatter":{"draft":false},"rawBody":"---\nid: 214\ntitle: 'Basic Media Analysis &#8211; Part 1 (Audio)'\ndate: '2017-12-30T06:57:20+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=214'\npermalink: /media-analysis-part-1-audio/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/basic_media_analysis_-_part_1_audio'\ncategories:\n    - 'Acquiring Data'\n    - 'Technical Introductions'\n---\n\nWhen collecting materials, media files are long and often disused. The process of turning voice from audio files into something useful, such as a transcript, once required a person to manuallytranscribe the audio (a service that is still available) rather than their being an accessible and accurate method, to do so.\n\nMedia, tells a story that incorporates different information to what can otherwise be found solely via text or other forms of metadata. Whilst emotional intonations and other relevent capacities of audio analysis to machine readable formats is a constituent of what can be done, this guide will provide some basic examples of how to process Audio as to transcribe to text as to provide text based information that can be used for further analysis that will be covered in a different post.\n\nONLINE SERVICES &amp; TOOLS\n\nAfter a short amount of time searching for basic tools; three have been easily identified alongside the means in which to use YouTube to perform this action.\n\nYOUTUBE\n\nBy uploading media to YouTube, YouTube can transcribe the audio automatically. Searching google using terms like “Automatically transcribe audio using youTube” will easily pick it up.\n\nA number of online services exist to provide automatic Audio to Text. Many of them provide a free trial. A few examples include;\n\n[Sonix:](https://sonix.ai/) Sonic ([invite link](https://sonix.ai/invite/xkbrlak)) provides 30 minutes free.\n\n[Trint](https://trint.com/) also provides 30 minutes free.\n\n[SpokenOnline](http://spokenonline.io/) also provides 30 minutes free.\n\nLocal Desktop Alternatives include products provided by [Nuance](https://www.nuance.com) who has a long-history in the field, producing solutions for multiple sectors."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","title":"Basic Media Analysis &#8211; Part 2 (visual)"},"frontmatter":{"draft":false},"rawBody":"---\nid: 221\ntitle: 'Basic Media Analysis &#8211; Part 2 (visual)'\ndate: '2017-12-31T07:55:40+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=221'\npermalink: /media-analysis-part-2-visual/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/basic_media_analysis_-_part_2_visual'\ncategories:\n    - 'Acquiring Data'\n    - 'Technical Introductions'\n---\n\nThis is a basic introduction which will be followed-up in latter posts.\n\nEssentially, the analysis of video is for the most part, very similar to the analysis of images, as video is a stream of images.\n\nThe first element of analysing images / video, is that the files themselves contain an array of metadata in the file as part of the file creation process (depending on how its done). The ‘metadata’ contained within images can include;\n\n- Time/Date it was taken\n- Where it was taken (IE: GPS coordinates)\n- Information about the device it was taken with\n- Copyright information / information about who took the image\n- other embedded image metadata.\n\nThese elements of data doesn’t rely upon the image or video being of good quality. It’s simply data created as part of creating the file in the first place. [MetaPicz](http://metapicz.com/) is an online example of an online application/service that provides an example of this information. The next process is to analyse the content depicted in the image itself…\n\nThe first issue with analysing vision is ensuring the quality of the images is of sufficient quality as to identify, analyse and process the informatics available in the imagery. Where its required, correcting the vision is a useful first step. Beyond the usual processes, adjusting contrast, brightness and other standardised image processing methods; increasingly ‘<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-yvjwd9v2ukcc0noffeabbspupip51gc6\" itemid=\"http://data.wordlift.it/wl0353/entity/superresolution\">super-resolution</span>‘ processes are becoming available.\n\nOne process when using multiple still images is detailed here;\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/KknTmqGj744?rel=0\" width=\"560\"></iframe>  \nor alternatively, this guide on [petapixel](https://petapixel.com/2015/02/21/a-practical-guide-to-creating-superresolution-photos-with-photoshop/). Once these processes are done; others, involving the use of AI related processes, include those detailed [here.](https://github.com/tensorlayer/SRGAN)\n\nThe time-consuming ‘trick’ is, to go through a multitude of processes with an appropriate ‘treatment methodology’, involving the use of ‘master’ and derivative content stacks; that in-turn requires tooling, inclusive of appropriate equipment, to do so effectively.\n\nOnce the source-material has been processes as to get the best possible visual quality; the means to produce ‘entity graphs’, or further additional ‘structured data’ converting objects the vision to a structured dataset.\n\nOne of the basic differences between video and still images is a timecode. Ideally, storage of metadata/structured data in relation to video content, includes time-code information.\n\nOne of the seminal presentations with respect to the works in entity recognition in vision is the TedTalk presentation about [ImageNet](http://www.image-net.org)  \n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/40riCqvRoMs?rel=0\" width=\"560\"></iframe>\n\nIn an attempt to make things easier, i’ll try to break down modern image analysis into a couple of different categories.\n\n- Identification of ‘Things’\n- Identification of ‘Persons’ or ‘Faces’\n- Identification of ‘Emotions’ or ‘gestures’\n- Biometrics – The identification of a unique living organism\n\nThere’s an ’emerging’ array of services available to the public that have an array of similar capabilities, to which ends, this post will not explore, other than highlighting this emergent field of ‘knowledge banking’, which is producing a significant mass of information leveraging scale of organisations, as to enhance AI / classification and intepretation technologies. This is in-turn producing a core-asset for these organisations by way of providing API access, most-often on a fee-for-service basis, to users, as to enhance the services capability for enhanced analytics capabilities, SOME OF WHICH, they provide public access to by way of their online services.\n\nTo produce tooling that is truely ‘enhanced’ beyond traditional knowhow, it’s essential to DIY (“Do It Yourself”).\n\nThe easy way to outline services (in a simple way) is dot-points;\n\n- [ClarifAI](http://www.clarifai.com) is a service that identifies the objects\n- [Google cloud vision](https://cloud.google.com/vision/) (wp plugin: https://github.com/amirandalibi/perception )\n- [Amazon rekognition](https://aws.amazon.com/rekognition/) (Wp plugin: https://wordpress.org/plugins/wp-rekogni/\n- [cloud sight](http://cloudsight.ai)\n- [Kairos ](http://www.kairos.com)noting, they’ve got a good [comparison guide](http://www.kairos.com/blog/face-recognition-kairos-vs-microsoft-vs-google-vs-amazon-vs-opencv)\n- [Affectiva](https://developer.affectiva.com)\n- [Cognitec](http://www.cognitec.com)\n- [betaFaceAPI](https://www.betafaceapi.com/wpa/) – Designed for faces\n\nOnce the data has been retrieved, database the informatics provided by the tools (inclusive of time-code if video) ideally in an RDF format. The usefulness of RDF provides for enabling the metadata / structured data, discovered in media, to be part of the broader database that is the web."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)"},"frontmatter":{"draft":false},"rawBody":"---\nid: 212\ntitle: 'Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)'\ndate: '2018-01-01T23:12:36+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=212'\npermalink: /basic-media-analysis-part-3-text-metadata/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/basic_media_analysis_-_part_3_text___metadata'\ncategories:\n    - 'Acquiring Data'\n    - 'Technical Introductions'\n---\n\n## Metadata\n\n*<span style=\"font-size: 1rem;\">“a set of data that describes and gives information about other data.”</span>*\n\n<div aria-hidden=\"true\" class=\"xpdxpnd _xk vkc_np\" data-mh=\"1px\">wiki: https://en.wikipedia.org/wiki/Metadata</div><div aria-hidden=\"true\" class=\"xpdxpnd _xk vkc_np\" data-mh=\"1px\"></div><div aria-hidden=\"true\" class=\"xpdxpnd vkc_np vk_pl _Wi\" data-mh=\"-1\"> Too often people don’t think about metadata when considering what or how data can be organised and given context. In [part 1](https://openseason.online/media-analysis-part-1-audio/) and [part 2](https://openseason.online/media-analysis-part-2-visual/) of this series of posts, some tips regarding the production of metadata for other forms of media (audio and visual) has been addressed. The final part, is about text &amp; metadata, ending with some tips about processes that can now apply to all forms of media, to produce structured data, as the information stored in various media formats via these tips will now be formatted in a manner that can be processed in a similar way.</div><div aria-hidden=\"true\" data-mh=\"-1\"></div><div aria-hidden=\"true\" class=\"xpdxpnd vkc_np vk_pl _Wi\" data-mh=\"-1\">Whether it be documents, spreadsheets, PDFs or other types of ‘document’ files; the metadata can provide more important information about the file, than the file itself. When undertaking [data-recovery](https://openseason.online/data-recovery-laptop-computers/) sometimes the file-extension is incorrect and the way to figure out what sort of file it is, is via metadata. Here’s a list of tools for documents via [forensicswiki](http://www.forensicswiki.org/wiki/Document_Metadata_Extraction) and a broader series of tools can be found via a [google search](http://lmgtfy.com/?q=document+metadata+viewer).</div><div aria-hidden=\"true\" data-mh=\"-1\"></div><div aria-hidden=\"true\" data-mh=\"-1\">The next process may be to undertake produce a sentiment analysis of the document using a tool such as [depechemood](http://www.depechemood.eu/DepecheMood.html). Now, if you’ve been able to follow the processes provided in this series to deal with audio, visual and text structured documents, you should have a still basic, yet sophisticated series of metadata contexts associated with the content in your project. This can be added to a database, or for further, far broader enhancements to the utility of this data; it’s time to format the accessibility of it, as [Linked Data](https://openseason.online/introduction-to-linked-data/).</div>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","title":"Tim Berners Lee &#8211; Turing Lecture"},"frontmatter":{"draft":false},"rawBody":"---\nid: 396\ntitle: 'Tim Berners Lee &#8211; Turing Lecture'\ndate: '2018-05-29T14:30:34+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=396'\npermalink: /tim-berners-lee-turing-lecture/\ninline_featured_image:\n    - '0'\ncategories:\n    - Lectures\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min yotu-limit-max   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoidmlkZW9zIiwiaWQiOiJCYU1hNHU0RmlvNCIsInBhZ2luYXRpb24iOiJvbiIsInBhZ2l0eXBlIjoicGFnZXIiLCJjb2x1bW4iOiIzIiwicGVyX3BhZ2UiOiIxMiIsInRlbXBsYXRlIjoiZ3JpZCIsInRpdGxlIjoib24iLCJkZXNjcmlwdGlvbiI6Im9uIiwidGh1bWJyYXRpbyI6IjE2OSIsIm1ldGEiOiJvZmYiLCJtZXRhX2RhdGEiOiJvZmYiLCJtZXRhX3Bvc2l0aW9uIjoib2ZmIiwiZGF0ZV9mb3JtYXQiOiJvZmYiLCJtZXRhX2FsaWduIjoib2ZmIiwic3Vic2NyaWJlIjoib2ZmIiwiZHVyYXRpb24iOiJvZmYiLCJtZXRhX2ljb24iOiJvZmYiLCJuZXh0dGV4dCI6IiIsInByZXZ0ZXh0IjoiIiwibG9hZG1vcmV0ZXh0IjoiIiwicGxheWVyIjp7Im1vZGUiOiJsYXJnZSIsIndpZHRoIjoiNjAwIiwic2Nyb2xsaW5nIjoiMTAwIiwiYXV0b3BsYXkiOiJvZmYiLCJjb250cm9scyI6Im9uIiwibW9kZXN0YnJhbmRpbmciOiJvbiIsImxvb3AiOiJvZmYiLCJhdXRvbmV4dCI6Im9mZiIsInNob3dpbmZvIjoib24iLCJyZWwiOiJvbiIsInBsYXlpbmciOiJvZmYiLCJwbGF5aW5nX2Rlc2NyaXB0aW9uIjoib2ZmIiwidGh1bWJuYWlscyI6Im9mZiIsImNjX2xvYWRfcG9saWN5IjoiMSIsImNjX2xhbmdfcHJlZiI6IjEiLCJobCI6IiIsIml2X2xvYWRfcG9saWN5IjoiMSJ9LCJsYXN0X3RhYiI6ImFwaSIsInVzZV9hc19tb2RhbCI6Im9mZiIsIm1vZGFsX2lkIjoib2ZmIiwibGFzdF91cGRhdGUiOiIxNTM3Njc1NDU1Iiwic3R5bGluZyI6eyJwYWdlcl9sYXlvdXQiOiJkZWZhdWx0IiwiYnV0dG9uIjoiMSIsImJ1dHRvbl9jb2xvciI6IiIsImJ1dHRvbl9iZ19jb2xvciI6IiIsImJ1dHRvbl9jb2xvcl9ob3ZlciI6IiIsImJ1dHRvbl9iZ19jb2xvcl9ob3ZlciI6IiIsInZpZGVvX3N0eWxlIjoiIiwicGxheWljb25fY29sb3IiOiIiLCJob3Zlcl9pY29uIjoiIiwiZ2FsbGVyeV9iZyI6IiJ9LCJlZmZlY3RzIjp7InZpZGVvX2JveCI6IiIsImZsaXBfZWZmZWN0IjoiIn0sImdhbGxlcnlfaWQiOiI2M2FjN2MzZTM1NDM0In0=\" data-showdesc=\"on\" data-total=\"1\" data-yotu=\"63ac7c3e4740f\" id=\"yotuwp-63ac7c3e35434\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> Sir Tim Berners-Lee 2016 ACM A.M. Turing Lecture \"What is the World Wide Web &amp; what is its future?\" </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3e4740f\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> </div> </div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![Sir Tim Berners-Lee 2016 ACM A.M. Turing Lecture \"What is the World Wide Web & what is its future?\"](https://i.ytimg.com/vi/BaMa4u4Fio4/sddefault.jpg)</div> </div>### Sir Tim Berners-Lee 2016 ACM A.M. Turing Lecture \"What is the World Wide Web &amp; what is its future?\"\n    \n    <div class=\"yotu-video-description\">Sir Tim Berners-Lee of the Massachusetts Institute of Technology and the University of Oxford received the 2016 ACM A.M. Turing Award for inventing the World Wide Web, the first web browser, and the fundamental protocols and algorithms allowing the Web to scale. Considered one of the most influential computing innovations in history, the World Wide Web is the primary tool used by billions of people every day to communicate, access information, engage in commerce, and perform many other important activities.  \n      \n    Sir Tim delivered his Turing Award Lecture at the ACM Web Science Conference in Amsterdam on May 29, 2018, titled \"What is the World Wide Web and what is its future? What could it be, what should it be? What is the Web we want?\"   \n      \n    Background  \n    Berners-Lee is a graduate of Oxford University, where he received a first-class Bachelor of Arts degree in Physics. Berners-Lee is the 3Com Founders Professor of Engineering in the School of Engineering with a joint appointment in the Department of Electrical Engineering and Computer Science and the Computer Science and Artificial Intelligence Laboratory (CSAIL) at Massachusetts Institute of Technology (MIT), where he also heads the Decentralized Information Group (DIG). He is also a Fellow at Christ Church and a Professorial Research Fellow at the Department of Computer Science, University of Oxford.  \n      \n    Berners-Lee founded the World Wide Web Consortium (W3C) in 1994, where he continues to serve as Director. W3C is an international community that develops open standards to ensure the interoperability and long-term growth of the Web. In 2009, he established the Word Wide Web Foundation, which works to advance the Open Web as a public good and a basic human right. He is the President of the Open Data Institute (ODI) in London.  \n      \n    He has received many awards and honors, including the ACM Software System Award in 1995. Berners-Lee was knighted in 2004 and received the Order of Merit in 2007, becoming one of only 24 living members entitled to hold the honor. He is a Fellow of the Royal Society, and has received honorary degrees from a number of universities around the world, including Manchester, Harvard, and Yale. TIME magazine included him as one of the 100 Most Important People of the 20th Century.</div> ](#BaMa4u4Fio4 \"Sir Tim Berners-Lee 2016 ACM A.M. Turing Lecture \"What is the World Wide Web & what is its future?\"\")\n\n</div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div> </div> </div>More information: https://amturing.acm.org/award\\_winners/berners-lee\\_8087960.cfm"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","title":"Inferencing (introduction)"},"frontmatter":{"draft":false},"rawBody":"---\nid: 23\ntitle: 'Inferencing (introduction)'\ndate: '2018-09-21T17:46:58+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=23'\npermalink: /inferencing-introduction/\ncategories:\n    - AI\n---\n\nHuman inference (i.e. how humans draw conclusions) is traditionally studied within the field of [cognitive psychology](https://en.wikipedia.org/wiki/Cognitive_psychology \"Cognitive psychology\"); [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence \"Artificial intelligence\") researchers develop automated inference systems to emulate human inference. A constituent to the means through which semantic web technology supports ‘artificial intelligence’ functionality is by way of [Semantic Inferencing.](https://en.wikipedia.org/wiki/Inference#Semantic_web)\n\nSemantic Inferencing makes use of available structured data, formatted through the use of [ontologies](https://www.webizen.net.au/ontologies-intro/), to support the means through which assumed conclusions can be presented with a [probabilistic](https://en.wikipedia.org/wiki/Probability) degree of certainty.\n\nThe more data-points made available in connection to a specified form of query, the greater systems are able to improve the probability of query responses being correct. Semantic inferencing is an important constituent to the broader eco-system of ‘semantic web tools’."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","title":"Introduction to Ontologies"},"frontmatter":{"draft":false},"rawBody":"---\nid: 15\ntitle: 'Introduction to Ontologies'\ndate: '2018-09-21T16:59:17+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=15'\npermalink: /ontologies-intro/\ncategories:\n    - ontologies\n---\n\n> On the Semantic Web, vocabularies define the concepts and relationships (also referred to as “terms”) used to describe and represent an area of concern. Vocabularies are used to classify the terms that can be used in a particular application, characterize possible relationships, and define possible constraints on using those terms. In practice, vocabularies can be very complex (with several thousands of terms) or very simple (describing one or two concepts only).\n> \n> There is no clear division between what is referred to as “vocabularies” and “ontologies”. The trend is to use the word “ontology” for more complex, and possibly quite formal collection of terms, whereas “vocabulary” is used when such strict formalism is not necessarily used or only in a very loose sense. Vocabularies are the basic building blocks for [inference](https://www.w3.org/standards/semanticweb/inference) techniques on the Semantic Web.\n\nSource: https://www.w3.org/standards/semanticweb/ontology\n\nThe most commonly used Ontologies are [schemaorg](https://schema.org/docs/about.html) which is most notably used for powering search, alongside [Open Graph Protocol](http://ogp.me/), which is used to support the means through which web-content can be reposted on facebook.\n\nBeyond these two notable examples, an array of public ontologies can be found through sites such as [The Linked Open Data Cloud](https://lod-cloud.net/) site."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","title":"RWW &#038; some Solid history"},"frontmatter":{"draft":false},"rawBody":"---\nid: 259\ntitle: 'RWW &#038; some Solid history'\ndate: '2018-09-21T19:09:48+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=259'\npermalink: /rww-some-solid-history/\ncategories:\n    - 'RWW (inc. Solid)'\n---\n\nDated 2009, Tim Berners Lee wrote a document about ‘[read write linked data](https://www.w3.org/DesignIssues/ReadWriteLinkedData.html)‘ which is in-turn supplemented by the document he also authored about ‘[socially aware cloud storage](https://www.w3.org/DesignIssues/CloudStorage.html)‘. Together, these elements forge what is considered to be the world-wide-web standards based works, on a solution where people are able to store their own data online, in a manner that supports ‘linking’ between online data-sources across the web using the Semantic Web technology ecosystem.\n\nAn underlying storage standard has evolved to support the meaningful utility of these concepts, called [Linked Data Platform](https://www.w3.org/TR/ldp/).\n\nKey Academic thesis produced by [Andrei Sambra (2013)](https://www.webizen.net.au/?attachment_id=263), [Joe Presbrey (2014)](https://www.webizen.net.au/?attachment_id=262) and [Amy Guy (2017)](https://rhiaro.github.io/thesis/). The evolution of what was first called RWW (note: [W3 CG for RWW](https://www.w3.org/community/rww/)) was later ‘spun out’ as ‘[solid](https://solid.mit.edu/)‘ following a [donation by mastercard](https://www.csail.mit.edu/news/web-inventor-tim-berners-lees-next-project-platform-gives-users-control-their-data).\n\nNotably, one of the first applications produced to search RWW systems, to create an decentralised index of persons involved, was produced by Andrei and named ‘Webizen’; which is the source of inspiration for this sites name."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","title":"Semantic Web (An Intro)"},"frontmatter":{"draft":false},"rawBody":"---\nid: 14\ntitle: 'Semantic Web (An Intro)'\ndate: '2018-09-21T17:02:02+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=14'\npermalink: /semantic-web-an-intro/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'linked data'\n    - 'technology ecosystem'\n---\n\nAround the year 2000, a concept called the ‘[semantic web](https://en.wikipedia.org/wiki/Semantic_Web)‘ was brought together and has continued to evolve since. By 2007, the use of semantic web technology had grown to a point where tim berners-lee wrote an article about the ‘[Giant Global Graph](https://web.archive.org/web/20160713021037/http://dig.csail.mit.edu/breadcrumbs/node/215)‘ that had been forged through the use of it.\n\n<figure class=\"wp-caption alignnone\" style=\"width: 4936px\">![](https://www.w3.org/2001/04/roadmap/all.svg)<figcaption class=\"wp-caption-text\">W3C technology road map. In the end, all W3C activities are in service to the top-level goal of reaching the semantic Web’s full potential. Arrows indicate “how” things are implemented; following them in reverse indicates “why” they exist (or should) IEEE INTERNET COMPUTING http://computer.org/internet/ JULY • AUGUST 2001 PG: 13</figcaption></figure>SOURCE: http://jmvidal.cse.sc.edu/library/w4012.pdf\n\nHistorically, it is noted that a constituent of the origins for these technologies were indeed born out of [DARPA Agent Markup Language](https://en.wikipedia.org/wiki/DARPA_Agent_Markup_Language).\n\nThe ‘semantic web’ ecosystem of technologies has an array of different names and technical constituents which have developed overtime.\n\nCritically, semantic web employs the use of [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework) in an array of different [serialisation formats](https://en.wikipedia.org/wiki/Resource_Description_Framework#Serialization_formats). Almost any form of data can be converted into RDF.\n\nOnce data is stored in an RDF format, it can therefore be employed by systems that provide the means to query data structured in this format. The means through which this is done is most-often by way of a family of query language services denoted moreover as [sparql](https://en.wikipedia.org/wiki/SPARQL).\n\nSparql family solutions include (but are not limited to) [sparql-mm](http://marmotta.apache.org/kiwi/sparql-mm.html) that provides support for multimedia, [Sparql-FED](https://www.w3.org/TR/sparql11-federated-query/) that provides the means to query multiple end-points.\n\nSomewhere around 2009 a rebranding attempt for Semantic Web (“SemWeb”) &amp; RDF; to the term ‘[Linked Data](https://twitter.com/timoreilly/status/1266750154)‘ was started to be made. Whilst the implication and extensive nature of technology use is not well-known, it is indeed the case that the vast majority of web services contain RDF and are therefore constituent elements of the broader ‘semantic web’.\n\nOne of the ways this can be better understood is by reviewing the means through which [ontologies](https://www.webizen.net.au/ontologies-intro/) are currently used and/or installing relevent plugins that also provide the necessary tools, to make it easier to see the ‘web of (structured) data’."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","title":"Verifiable Claims (An Introduction)"},"frontmatter":{"draft":false},"rawBody":"---\nid: 11\ntitle: 'Verifiable Claims (An Introduction)'\ndate: '2018-09-21T16:32:22+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=11'\npermalink: /vc-intro/\ncategories:\n    - 'knowledge bank'\n    - 'technology ecosystem'\n    - 'verifiable claims'\n---\n\nAn important part of human ‘identity’ is the way claims are made about a person, and in relation to a person. Claims related ‘instruments’ are used throughout society, as to be relied upon in association to many interactions.\n\nThe W3 community group ‘[credentials](https://www.w3.org/community/credentials/)‘ was established to support works designed to deliver outcomes required in this area.\n\nPart of related works produced include the open-badge version 2 specification which can be found [here](https://www.imsglobal.org/sites/default/files/Badges/OBv2p0/index.html).\n\nThese works make use of RDF and URIs to support the development and use of claims made between an authority of some sort, and what may be called ‘the data subject’; For instance,\n\n**A BANK-CARD**  \nA person has a bankcard that supports their needs to make payments. The banking card is owned by the financial institution providing the financial instrument or ‘card’. The purpose of it being provided to the person, is to support their means to use the card to make use of their bank-account.\n\n#### **A BIRTH CERTIFICATE**\n\nA Birth Certificate is issued most-often, by a government. The ‘subject’ of that document is the person whom the certificate provides evidence about in relation to their birth. The information presented by a birth certificate includes statements about whether or not the person is over a certain age (ie: over 18 or over 21), where they were born / nationality, who their parents were, etc.\n\n#### **A Postage Stamp**\n\nA postage stamp is applied to a item that is sent through the post. The stamp, and related markings made by the postage service provider assists in verifying that envelope (and its contents) have been sent through the mail system at a particular point in time, etc.\n\n### SUMMARY\n\nRDF based ‘verifiable claims’ provide the means to employ 3rd party, claims made in relation to people; as a constituent of the semantics employed in running, processing and subsequently presenting a query."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","title":"Web of Things &#8211; an Introduction"},"frontmatter":{"draft":false},"rawBody":"---\nid: 291\ntitle: 'Web of Things &#8211; an Introduction'\ndate: '2018-09-21T19:33:15+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=291'\npermalink: /web-of-things-an-introduction/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Web of Things'\n---\n\n(DOCUMENT STATUS – QUICK DRAFT)\n\nWeb of Things introduces the use of Semantic Web tooling, applied to the use of Internet of Things (IoT).\n\nSome background can be found via the links below;\n\nLinks:\n\n```\n<pre id=\"body\">2009 - semantics <a href=\"https://www.w3.org/2009/03/xbrl/talks/intro2semweb-dsr.pdf\">https://www.w3.org/2009/03/xbrl/talks/intro2semweb-dsr.pdf</a>\nflyer - 2010 - <a href=\"https://webofthings.org/wot/2010/WoT_2010_cfp.pdf\">https://webofthings.org/wot/2010/WoT_2010_cfp.pdf</a>\n\n2010 - 24 January 2010 -<a href=\"https://www.w3.org/2010/Talks/0123-dsr-sofsem.pdf\">https://www.w3.org/2010/Talks/0123-dsr-sofsem.pdf</a>\n\n2010 - <a href=\"https://www.w3.org/2010/Talks/sofsem2010-raggett.pdf\">https://www.w3.org/2010/Talks/sofsem2010-raggett.pdf</a>\nSeptember 8–12, 2013 -\n<a href=\"http://www.ubicomp.org/ubicomp2013/adjunct/adjunct/p1487.pdf\">http://www.ubicomp.org/ubicomp2013/adjunct/adjunct/p1487.pdf</a>\n2014 - web of thoughts - <a href=\"https://www.w3.org/2014/10/29-dsr-wot.pdf\">https://www.w3.org/2014/10/29-dsr-wot.pdf</a>\n\nApril 2015 - <a href=\"https://hal.inria.fr/hal-01244735/document\">https://hal.inria.fr/hal-01244735/document</a>\nJuly 2015 (thesis) <a href=\"https://tel.archives-ouvertes.fr/tel-01178286/document\">https://tel.archives-ouvertes.fr/tel-01178286/document</a>\n2015 - <a href=\"https://ieeexplore.ieee.org/document/7111885/\">https://ieeexplore.ieee.org/document/7111885/</a>\n2015 - <a href=\"https://www.w3.org/2015/05/wot-framework.pdf\">https://www.w3.org/2015/05/wot-framework.pdf</a>\n\nMarch 2016 -\n<a href=\"https://www.iab.org/wp-content/IAB-uploads/2016/03/Raggett-Kanti-Datta.pdf\">https://www.iab.org/wp-content/IAB-uploads/2016/03/Raggett-Kanti-Datta.pdf</a>\n|\n<a href=\"https://www.ietf.org/proceedings/interim-2016-t2trg-02/slides/slides-interim-2016-t2trg-2-11.pdf\">https://www.ietf.org/proceedings/interim-2016-t2trg-02/slides/slides-interim-2016-t2trg-2-11.pdf</a>\n\nSeptember 2016 - <a href=\"https://www.w3.org/2016/09/IoTW/wot-intro.pdf\">https://www.w3.org/2016/09/IoTW/wot-intro.pdf</a>\n2017\n<a href=\"https://iotweek.blob.core.windows.net/slides2017/THEMATIC%20SESSIONS/Emerging%20IoT%20Researches%20and%20Technologies/Web%20of%20Things/D.%20Raggett-%20Countering%20Fragmentation.pdf\">https://iotweek.blob.core.windows.net/slides2017/THEMATIC%20SESSIONS/Emerging%20IoT%20Researches%20and%20Technologies/Web%20of%20Things/D.%20Raggett-%20Countering%20Fragmentation.pdf</a>\n\n\nWotCity 2017 <a href=\"https://wotcity.com/WoTCity-WhitePaper.pdf\">https://wotcity.com/WoTCity-WhitePaper.pdf</a>\n2018 - <a href=\"https://www.w3.org/2018/05/08-dsr-wot.pdf\">https://www.w3.org/2018/05/08-dsr-wot.pdf</a>\n```"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","title":"Credentials and Payments by Manu Sporny"},"frontmatter":{"draft":false},"rawBody":"---\nid: 377\ntitle: 'Credentials and Payments by Manu Sporny'\ndate: '2018-09-23T14:08:42+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=377'\npermalink: /credentials-and-payments-by-manu-sporny/\ninline_featured_image:\n    - '0'\ncategories:\n    - Credentials\n    - Payments\n    - 'Video Library'\n    - 'What is RDF?'\n---\n\nCredentials and Payments by Manu Sporny\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min yotu-limit-max   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMQ2JtejBWU1pfdnFJLVNmWjNWUm84YnM2akpGLURtcnMiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2U1OGIxMiIsIm5leHQiOiIiLCJwcmV2IjoiIn0=\" data-showdesc=\"on\" data-total=\"1\" data-yotu=\"63ac7c3e75688\" id=\"yotuwp-63ac7c3e58b12\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> Code for the Web - Linked Data and Web Payments </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3e75688\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> Web developers: Fund world-class students to build Web Payments and Linked Data publishing tools for the next generation Web. Fund the project here: http://www.indiegogo.com/projects/code-for-the-web-linked-data-and-web-payments </div> </div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![Code for the Web - Linked Data and Web Payments](https://i.ytimg.com/vi/bjbICcGanIg/sddefault.jpg)</div> </div>### Code for the Web - Linked Data and Web Payments\n    \n    <div class=\"yotu-video-description\">Web developers: Fund world-class students to build Web Payments and Linked Data publishing tools for the next generation Web. Fund the project here: http://www.indiegogo.com/projects/code-for-the-web-linked-data-and-web-payments</div> ](#bjbICcGanIg \"Code for the Web - Linked Data and Web Payments\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![What is Linked Data?](https://i.ytimg.com/vi/4x_xzT5eF5Q/hqdefault.jpg)</div> </div>### What is Linked Data?\n    \n    <div class=\"yotu-video-description\">A short non-technical introduction to Linked Data, Google's Knowledge Graph, and Facebook's Open Graph Protocol. If you have any questions, hit me up on Twitter: @manusporny or G+: Manu Sporny</div> ](#4x_xzT5eF5Q \"What is Linked Data?\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Universal Payment for the Web](https://i.ytimg.com/vi/gYdXZ3-U-oQ/sddefault.jpg)</div> </div>### Universal Payment for the Web\n    \n    <div class=\"yotu-video-description\">More info at - http://payswarm.com/ Follow on Twitter: @manusporny @payswarm - This video covers how new infrastructure for the Web is built and how PaySwarm will help support our culture of reward.</div> ](#gYdXZ3-U-oQ \"Universal Payment for the Web\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Credentials on the Web](https://i.ytimg.com/vi/eWtOg3vSzxI/sddefault.jpg)</div> </div>### Credentials on the Web\n    \n    <div class=\"yotu-video-description\">A quick introduction to verifiable credentials on the Web. For more information, see http://opencreds.org/ or follow @manusporny on Twitter for weekly announcements about the initiative.</div> ](#eWtOg3vSzxI \"Credentials on the Web\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Linked Data Signatures](https://i.ytimg.com/vi/QdUZaYeQblY/sddefault.jpg)</div> </div>### Linked Data Signatures\n    \n    <div class=\"yotu-video-description\">An overview of how digital signatures can be added to Linked Data to provide verifiable statements on the Web. For more information, see http://opencreds.org/specs/</div> ](#QdUZaYeQblY \"Linked Data Signatures\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![JSON-LD: Core Markup](https://i.ytimg.com/vi/UmvWk_TQ30A/sddefault.jpg)</div> </div>### JSON-LD: Core Markup\n    \n    <div class=\"yotu-video-description\">An overview of some of the core markup features of JSON-LD including types, aliasing, nesting, and internationalization support. See http://json-ld.org/ for more info, or follow @manusporny on Twitter.</div> ](#UmvWk_TQ30A \"JSON-LD: Core Markup\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Digital Bazaar Credential Handler Polyfill](https://i.ytimg.com/vi/qdbDu1oV0PI/sddefault.jpg)</div> </div>### Digital Bazaar Credential Handler Polyfill\n    \n    <div class=\"yotu-video-description\">A browser polyfill for the Credential Handler API created by Digital Bazaar.</div> ](#qdbDu1oV0PI \"Digital Bazaar Credential Handler Polyfill\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Tim Berners-Lee Demonstrates Solid (2018)](https://i.ytimg.com/vi/gZB6d-4klmU/sddefault.jpg)</div> </div>### Tim Berners-Lee Demonstrates Solid (2018)\n    \n    <div class=\"yotu-video-description\">Tim Berners-lee demonstrating Solid at DWeb Summit 2018.   \n    Video source: https://archive.org/details/dweb-8_2_18_LL_TALK-Solid-empoweringpeoplethroughchoice</div> ](#gZB6d-4klmU \"Tim Berners-Lee Demonstrates Solid (2018)\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![The future of your society, is in the hands and hearts of its people.](https://i.ytimg.com/vi/e9vROTibKiE/sddefault.jpg)</div> </div>### The future of your society, is in the hands and hearts of its people.\n    \n    <div class=\"yotu-video-description\">first published Christmas day 2016 Educational Video about the status of technology development relating to information sciences, computing and artificial intelligence.  \n      \n    This video highlights some of the aspects to the socio-technological development of our species and its interactions with the world. It's important to remember more than a billion humans have no access to electricity, and many more do not have basic computing skills and ubiquitous access, let alone the skills to contribute to this sphere of humanitarian development impacting everything. The % of humanity participating in these works and the underlying debate is a tiny fraction, of a single %, of society world-wide.</div> ](#e9vROTibKiE \"The future of your society, is in the hands and hearts of its people.\")\n\n</div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div> </div></div>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","title":"Decentralized Web Summit 2018"},"frontmatter":{"draft":false},"rawBody":"---\nid: 391\ntitle: 'Decentralized Web Summit 2018'\ndate: '2018-09-23T14:26:48+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=391'\npermalink: /decentralized-web-summit-2018/\ninline_featured_image:\n    - '0'\ncategories:\n    - Conferences\n    - 'Video Library'\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min yotu-limit-max   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMQ2JtejBWU1pfdnFIYVk1RGVTTkFEaGZ3clF6bzRCLTgiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2Y0MjJmNiIsIm5leHQiOiIiLCJwcmV2IjoiIn0=\" data-showdesc=\"on\" data-total=\"1\" data-yotu=\"63ac7c3f5b6af\" id=\"yotuwp-63ac7c3f422f6\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> Decentralized Web Summit 2018 - Opening Night Party </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3f5b6af\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> Close your eyes. If you had unlimited time and resources to create any new tool, what would it be?  \n  \nWhen Richard Hendricks, the fictional founder of a Silicon Valley start-up, was confronted with that question, his answer? A Decentralized Internet. An internet that would run on all the unused compute power in our phones. An internet that wouldn't spy on us, or market us, or be prone to hacking.   \n  \nIf decentralization is becoming a meme, one of the individuals responsible for that may be Mike Judge, co-creator of the HBO series Silicon Valley. Mike is the famed writer/director of Idiocracy, Office Space, and King of the Hill. Judge will sit down with master polemicist, Cory Doctorow, for a wide ranging conversation about technology, entertainment and of course...the decentralized internet that we all deserve.  \n  \nNOTE: The Internet Archive does not have the rights to stream the clips of HBO's Silicon Valley. You can watch the key moment here:  \n  \nhttp://www.youtube.com/watch?v=nJVW_DqLntg </div> </div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![Decentralized Web Summit 2018 - Opening Night Party](https://i.ytimg.com/vi/B-5HXDOveGM/sddefault.jpg)</div> </div>### Decentralized Web Summit 2018 - Opening Night Party\n    \n    <div class=\"yotu-video-description\">Close your eyes. If you had unlimited time and resources to create any new tool, what would it be?  \n      \n    When Richard Hendricks, the fictional founder of a Silicon Valley start-up, was confronted with that question, his answer? A Decentralized Internet. An internet that would run on all the unused compute power in our phones. An internet that wouldn't spy on us, or market us, or be prone to hacking.   \n      \n    If decentralization is becoming a meme, one of the individuals responsible for that may be Mike Judge, co-creator of the HBO series Silicon Valley. Mike is the famed writer/director of Idiocracy, Office Space, and King of the Hill. Judge will sit down with master polemicist, Cory Doctorow, for a wide ranging conversation about technology, entertainment and of course...the decentralized internet that we all deserve.  \n      \n    NOTE: The Internet Archive does not have the rights to stream the clips of HBO's Silicon Valley. You can watch the key moment here:  \n      \n    http://www.youtube.com/watch?v=nJVW_DqLntg</div> ](#B-5HXDOveGM \"Decentralized Web Summit 2018 - Opening Night Party\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Decentralized Web Summit 2018 — Day 1 — Hyper Room](https://i.ytimg.com/vi/tJqZ223R7dQ/sddefault.jpg)</div> </div>### Decentralized Web Summit 2018 — Day 1 — Hyper Room\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    10:30am – 12:00pm  \n    Lightning Talks: New Discoveries  \n    Jeremy Rand, Paul Lindner, Jim Pick, John Light, Mikey Williams  \n      \n    1:30pm – 3:00pm  \n    Lightning Talks: New Discoveries  \n    Brendan O’Brien, Dimitri De Jonghe, Primavera De Filippi, Mehdi Yahyanejad, John Light  \n      \n    3:30pm – 5:00pm  \n    Lightning Talks: New Discoveries  \n    Cecilia Maundu, Chris Chrysostom, Nighat Dad, Rob Kaye, Mark Kudlac, John Light</div> ](#tJqZ223R7dQ \"Decentralized Web Summit 2018 — Day 1 — Hyper Room\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Decentralized Web Summit 2018 — Day 1 — Hash Lounge](https://i.ytimg.com/vi/Zptu1Bngv3Q/sddefault.jpg)</div> </div>### Decentralized Web Summit 2018 — Day 1 — Hash Lounge\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    10:30am – 12:00pm  \n    Panel: Decentralizing Social Networks  \n    Moderators: Ross Schulman  \n    Chris Riley, Cindy Cohn, Caroline Sinders, Kendra Albert  \n      \n    1:30pm – 2:00pm  \n    Talk: The Web of Commons  \n    Karissa McKelvey  \n      \n    2:00pm – 3:00pm  \n    Talk: The Open Index Protocol and its ecosystem  \n    Chris Chrysostom, Devon Read James, Amy James, Davi Ortega  \n      \n    3:30pm – 4:00pm  \n    Tech Talk: What is the meaning of \"decentralization\"?  \n    Dominic Tarr  \n      \n    4:00pm – 5:00pm  \n    Panel: Decentralized Identity  \n    Moderators: Kaliya  \n    Kim Hamilton Duffy, Daniel Buchner, Eugeniu Rusu, Markus Sabadello</div> ](#Zptu1Bngv3Q \"Decentralized Web Summit 2018 — Day 1 — Hash Lounge\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Decentralized Web Summit 2018 — Day 1 — Log Lounge](https://i.ytimg.com/vi/eO6pYYWZBs8/sddefault.jpg)</div> </div>### Decentralized Web Summit 2018 — Day 1 — Log Lounge\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    10:30am – 11:30am  \n    Talks: IPFS Lightning Talks!  \n    Mike Goelzer, David Dias, Juan Benet, Mitra Ardron  \n      \n    11:30am – 12:00pm  \n    Workshop: A Journey Round Zeronet Tamas Kocsis  \n      \n    1:30pm – 2:00pm  \n    Talks: Deep dive into decentralizing the Internet Archive  \n    Mitra Ardron, Feross Aboukhadjeh, Arkadiy Kukarkin  \n      \n    2:00pm – 3:00pm  \n    Talks: Maidsafe: The SAFE Network: An Internet for the people by the people  \n    Nick Lambert, Vivekanand Rajkumar, David Irvine, Gabriel Viagnotti  \n      \n    3:30pm – 5:00pm  \n    Talks: The journey to Web 3 - the Builders, the Protocols, and the Challenges Ahead  \n    Peter Czaban, Jutta Steiner, Fredrik Harrysson, Mihai Ciucu, Patrick Nielsen, Andrei Grigorean, Dimitri De Jonghe, aaron kumavis</div> ](#eO6pYYWZBs8 \"Decentralized Web Summit 2018 — Day 1 — Log Lounge\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Front End Stage - Decentralized Web Summit 2018 — Day 1](https://i.ytimg.com/vi/tsz3ffrJDpw/sddefault.jpg)</div> </div>### Front End Stage - Decentralized Web Summit 2018 — Day 1\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    10:30am – 11:00am  \n    Talk: Mitchell Baker--\"Revitalizing the Web\"  \n    Mitchell Baker  \n      \n    11:00am – 12:00pm  \n    Panel: Stories from the Field—A View of the Internet from the Global South  \n    Cecilia Maundu, Nighat Dad, Nicolás Pace, Sarah Bowers  \n      \n    1:30pm – 2:15pm  \n    Talk: Vint Cerf--Continued thoughts on a self- archiving Web  \n    Vint Cerf  \n      \n    2:15pm – 3:00pm  \n    Panel: In a Decentralized World, Who Decides? Five Leaders Share their Governance Stories  \n    Greg McMullen, Zooko Wilcox, Danielle Robinson, Muneeb Ali, Amandine Le Pape  \n      \n    3:30pm – 4:15pm  \n    Talk: Emily Jacobi: Tools of Solidarity - Building Decentralized Tools in Remote Environments  \n    Emily Jacobi  \n      \n    4:15pm – 5:00pm  \n    Panel: Building the First Web—Lessons for a Decentralized World  \n    Brewster Kahle, Whitfield Diffie, Tim Berners-Lee, Mitchell Baker</div> ](#tsz3ffrJDpw \"Front End Stage - Decentralized Web Summit 2018 — Day 1\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Front End Stage – Decentralized Web Summit 2018 — Day 2](https://i.ytimg.com/vi/g7Ln3mREbrs/sddefault.jpg)</div> </div>### Front End Stage – Decentralized Web Summit 2018 — Day 2\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    9:00am – 9:45am  \n    Talk: Jennifer Granick—The End of the Internet As We Knew It, And What Happens Next  \n    Jennifer Granick  \n      \n    9:45am – 10:30am  \n    Talk: Brewster Kahle—A Game with Many Winners  \n    Brewster Kahle  \n      \n    11:00am – 11:45am  \n    Talk: Primavera De Filippi—Building the Decentralized Stack: The Missing Social Layer  \n    Primavera De Filippi  \n      \n    11:45am – 12:30pm  \n    Talk: Joseph Poon—The Abundance Game  \n    Joseph Poon  \n      \n    2:00pm – 2:45pm  \n    Talk: Juan Benet—DWeb Progress: Where have we been? What's next?  \n    Juan Benet  \n      \n    2:45-3:15 PM Decentralization by Elimination by Aya Miyaguchi and Albert Ni  \n    https://decentralizedwebsummit2018.sched.com/event/FmJV/talk-decentralization-by-elimination  \n      \n    3:30pm – 4:30pm  \n    Panel: Cryptocurrency and the Law  \n    Moderator: Althea Allen   \n    Peter Van Valkenburgh, Zooko Wilcox, Marvin Ammori, Ryan Shea</div> ](#g7Ln3mREbrs \"Front End Stage – Decentralized Web Summit 2018 — Day 2\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Front End Stage – Decentralized Web Summit 2018 — Day 2 02](https://i.ytimg.com/vi/mntWm0y49AA/sddefault.jpg)</div> </div>### Front End Stage – Decentralized Web Summit 2018 — Day 2 02\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    9:00am – 9:45am  \n    Talk: Jennifer Granick—The End of the Internet As We Knew It, And What Happens Next  \n    Jennifer Granick  \n      \n    9:45am – 10:30am  \n    Talk: Brewster Kahle—A Game with Many Winners  \n    Brewster Kahle  \n      \n    11:00am – 11:45am  \n    Talk: Primavera De Filippi—Building the Decentralized Stack: The Missing Social Layer  \n    Primavera De Filippi  \n      \n    11:45am – 12:30pm  \n    Talk: Joseph Poon—The Abundance Game  \n    Joseph Poon  \n      \n    2:00pm – 2:45pm  \n    Talk: Juan Benet—DWeb Progress: Where have we been? What's next?  \n    Juan Benet  \n      \n    2:45-3:15 PM Decentralization by Elimination by Aya Miyaguchi and Albert Ni  \n    https://decentralizedwebsummit2018.sched.com/event/FmJV/talk-decentralization-by-elimination  \n      \n    3:30pm – 4:30pm  \n    Panel: Cryptocurrency and the Law  \n    Moderator: Althea Allen   \n    Peter Van Valkenburgh, Zooko Wilcox, Marvin Ammori, Ryan Shea</div> ](#mntWm0y49AA \"Front End Stage – Decentralized Web Summit 2018 — Day 2 02\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Front End Stage – Decentralized Web Summit 2018 — Day 2 03](https://i.ytimg.com/vi/sDpu9I0mgf8/hqdefault.jpg)</div> </div>### Front End Stage – Decentralized Web Summit 2018 — Day 2 03\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    9:00am – 9:45am  \n    Talk: Jennifer Granick—The End of the Internet As We Knew It, And What Happens Next  \n    Jennifer Granick  \n      \n    9:45am – 10:30am  \n    Talk: Brewster Kahle—A Game with Many Winners  \n    Brewster Kahle  \n      \n    11:00am – 11:45am  \n    Talk: Primavera De Filippi—Building the Decentralized Stack: The Missing Social Layer  \n    Primavera De Filippi  \n      \n    11:45am – 12:30pm  \n    Talk: Joseph Poon—The Abundance Game  \n    Joseph Poon  \n      \n    2:00pm – 2:45pm  \n    Talk: Juan Benet—DWeb Progress: Where have we been? What's next?  \n    Juan Benet  \n      \n    2:45-3:15 PM Decentralization by Elimination by Aya Miyaguchi and Albert Ni  \n    https://decentralizedwebsummit2018.sched.com/event/FmJV/talk-decentralization-by-elimination  \n      \n    3:30pm – 4:30pm  \n    Panel: Cryptocurrency and the Law  \n    Moderator: Althea Allen   \n    Peter Van Valkenburgh, Zooko Wilcox, Marvin Ammori, Ryan Shea</div> ](#sDpu9I0mgf8 \"Front End Stage – Decentralized Web Summit 2018 — Day 2 03\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Hyper Room — Decentralized Web Summit 2018 — Day 2](https://i.ytimg.com/vi/g0WEAcUcsFw/sddefault.jpg)</div> </div>### Hyper Room — Decentralized Web Summit 2018 — Day 2\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    9:00am – 10:30am  \n    Lightning Talks: New Discoveries  \n    Daniel Buchner, Charles Lehner, Michelle Lee, Jehan Tremback, John Light  \n      \n    11:00am – 12:30pm  \n    Lightning Talks: New Discoveries  \n    John Light, Alberto Elias, Antonio Tenorio-Fornés, Peter Todd, Fennie Wang, Feross Aboukhadjeh, Jenny Ryan, John Kunze  \n      \n    2:00pm – 3:15pm  \n    Lightning Talks: New Discoveries Tantek, Althea Allen, John Light, Gordon Mohr  \n      \n    3:30pm – 4:30pm  \n    Lightning Talks: New Discoveries Jay Graber, John Light</div> ](#g0WEAcUcsFw \"Hyper Room — Decentralized Web Summit 2018 — Day 2\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Log Lounge — Decentralized Web Summit 2018 — Day 2](https://i.ytimg.com/vi/kW6e1GCpqpE/sddefault.jpg)</div> </div>### Log Lounge — Decentralized Web Summit 2018 — Day 2\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    9:00am – 9:30am  \n    Talk: Internet Infrastructure Challenges with Decentralized Web  \n    Marco Rodrigues  \n      \n    9:30am – 10:30am  \n    Talk: Better Algorithms for a Decentralized Web— the GUN Stack  \n    Mark Nadal, Adrien Marie, Priya Kuber, Martti Malmi  \n      \n    11:00am – 11:30am  \n    Talks: Blockstack: Building a Decentralized Internet &amp; the Decentralized Apps  \n    Justin Hunter, Patrick Stanley, Prabhaav Bhardwaj  \n      \n    11:30am – 11:45am  \n    Talk: Authentication &amp; Access Control: How to hide things when there everything is visible  \n    Mitra Ardron  \n      \n     11:45am – 12:00pm  \n    Talk: Fearless Cooperation: Giving eval() to your worst enemy for fun and profit  \n    Brian Warner  \n      \n    12:00pm – 12:30pm  \n    Where is the Web Closed? Empirical Evidence of Regional Blocking  \n    Anushah Hossain, Sadia Afroz  \n      \n    2:00pm – 2:30pm  \n    TALK: Solid: empowering people through choice  \n    Tim Berners-Lee, Ruben Verborgh  \n      \n    2:30pm – 3:15pm  \n    Talks: Building Peer to Peer Applications on Dat &amp; Beaker Browser  \n    Danielle Robinson, Tara Vancil, Jon-Kyle Mohr, noffle, Laurel Schwulst, Jim Pick, Mathias Buus, Joe Hand, Paul Frazee  \n      \n    3:30pm – 4:00pm  \n    Talks: libp2p Lightning Talks! Jeromy Johnson  \n      \n    4:00pm – 4:30pm  \n    Talks: Holo Lightning Talks! Sami Van Ness, Jean Russell</div> ](#kW6e1GCpqpE \"Log Lounge — Decentralized Web Summit 2018 — Day 2\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![HyperLounge - Afternoon - Decentralized Web Summit 2018 — Day 2](https://i.ytimg.com/vi/u-XtbUEiL38/sddefault.jpg)</div> </div>### HyperLounge - Afternoon - Decentralized Web Summit 2018 — Day 2\n    \n    <div class=\"yotu-video-description\"></div> ](#u-XtbUEiL38 \"HyperLounge - Afternoon - Decentralized Web Summit 2018 — Day 2\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Hash Lounge — Decentralized Web Summit 2018 — Day 2](https://i.ytimg.com/vi/KLPJRVKRjVc/sddefault.jpg)</div> </div>### Hash Lounge — Decentralized Web Summit 2018 — Day 2\n    \n    <div class=\"yotu-video-description\">ALL TIMES PDT (UTC−07:00)  \n      \n    9:00am – 10:30am  \n    Panel/Workshop: Data Stewardship on the Decentralized Web  \n    Danielle Robinson, Jefferson Bailey, Matt Zumwalt, Dawn Walker, Michelle Hertzfeld, Brendan O’Brien  \n      \n    11:00am – 12:30pm  \n    Panel: Can the DWeb combat Censorship, Prosecution, and Government Interference? Moderators: Cori Zarek  \n    Danny O'Brien, Dmitri Vitaliev, Mary Kay Magistad, Victoria Baranetsky, Emma Llansó  \n      \n    2:00pm – 3:15pm  \n    Panel/Workshop: How do we create sustainable funding for Open Source Projects?  \n    Moderators: Jenny Ryan  \n      \n    3:30pm – 4:00pm  \n    Talk: New Primitives for Cryptographic Protocols  \n    Bram Cohen</div> ](#KLPJRVKRjVc \"Hash Lounge — Decentralized Web Summit 2018 — Day 2\")\n\n</div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div> </div> </div>more information see https://decentralizedweb.net/"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","title":"Human Consciousness"},"frontmatter":{"draft":false},"rawBody":"---\nid: 399\ntitle: 'Human Consciousness'\ndate: '2018-09-23T14:57:03+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=399'\npermalink: /human-consciousness/\ninline_featured_image:\n    - '0'\ncategories:\n    - AI\n    - 'Video Library'\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMQ2JtejBWU1pfdm9UcFJLOS1vNVJrc0VSYWs0a09MNDAiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2Y2NDJkMyIsIm5leHQiOiJFQUFhQmxCVU9rTkJkdyIsInByZXYiOiIifQ==\" data-showdesc=\"on\" data-total=\"6\" data-yotu=\"63ac7c3f82f32\" id=\"yotuwp-63ac7c3f642d3\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> Your brain hallucinates your conscious reality | Anil Seth </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3f82f32\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> Visit http://TED.com to get our entire library of TED Talks, transcripts, translations, personalized talk recommendations and more.  \n  \nRight now, billions of neurons in your brain are working together to generate a conscious experience -- and not just any conscious experience, your experience of the world around you and of yourself within it. How does this happen? According to neuroscientist Anil Seth, we're all hallucinating all the time; when we agree about our hallucinations, we call it \"reality.\" Join Seth for a delightfully disorienting talk that may leave you questioning the very nature of your existence.  \n  \nThe TED Talks channel features the best talks and performances from the TED Conference, where the world's leading thinkers and doers give the talk of their lives in 18 minutes (or less). Look for talks on Technology, Entertainment and Design -- plus science, business, global issues, the arts and more. You're welcome to link to or embed these videos, forward them to others and share these ideas with people you know.   \n  \nFollow TED on Twitter: http://twitter.com/TEDTalks  \nLike TED on Facebook: http://facebook.com/TED  \nSubscribe to our channel: http://youtube.com/TED  \n  \nTED's videos may be used for non-commercial purposes under a Creative Commons License, Attribution–Non Commercial–No Derivatives (or the CC BY – NC – ND 4.0 International) and in accordance with our TED Talks Usage Policy (https://www.ted.com/about/our-organization/our-policies-terms/ted-talks-usage-policy). For more information on using TED for commercial purposes (e.g. employee learning, in a film or online course), please submit a Media Request at https://media-requests.ted.com </div> </div><div class=\"yotu-pagination yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">6</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![Your brain hallucinates your conscious reality | Anil Seth](https://i.ytimg.com/vi/lyu7v7nWzfo/sddefault.jpg)</div> </div>### Your brain hallucinates your conscious reality | Anil Seth\n    \n    <div class=\"yotu-video-description\">Visit http://TED.com to get our entire library of TED Talks, transcripts, translations, personalized talk recommendations and more.  \n      \n    Right now, billions of neurons in your brain are working together to generate a conscious experience -- and not just any conscious experience, your experience of the world around you and of yourself within it. How does this happen? According to neuroscientist Anil Seth, we're all hallucinating all the time; when we agree about our hallucinations, we call it \"reality.\" Join Seth for a delightfully disorienting talk that may leave you questioning the very nature of your existence.  \n      \n    The TED Talks channel features the best talks and performances from the TED Conference, where the world's leading thinkers and doers give the talk of their lives in 18 minutes (or less). Look for talks on Technology, Entertainment and Design -- plus science, business, global issues, the arts and more. You're welcome to link to or embed these videos, forward them to others and share these ideas with people you know.   \n      \n    Follow TED on Twitter: http://twitter.com/TEDTalks  \n    Like TED on Facebook: http://facebook.com/TED  \n    Subscribe to our channel: http://youtube.com/TED  \n      \n    TED's videos may be used for non-commercial purposes under a Creative Commons License, Attribution–Non Commercial–No Derivatives (or the CC BY – NC – ND 4.0 International) and in accordance with our TED Talks Usage Policy (https://www.ted.com/about/our-organization/our-policies-terms/ted-talks-usage-policy). For more information on using TED for commercial purposes (e.g. employee learning, in a film or online course), please submit a Media Request at https://media-requests.ted.com</div> ](#lyu7v7nWzfo \"Your brain hallucinates your conscious reality | Anil Seth\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![The brain dictionary](https://i.ytimg.com/vi/k61nJkx5aDQ/sddefault.jpg)</div> </div>### The brain dictionary\n    \n    <div class=\"yotu-video-description\">Where exactly are the words in your head? Scientists have created an interactive map showing which brain areas respond to hearing different words. The map reveals how language is spread throughout the cortex and across both hemispheres, showing groups of words clustered together by meaning. The beautiful interactive model allows us to explore the complex organisation of the enormous dictionaries in our heads.  \n      \n    Explore the brain model for yourself here: http://gallantlab.org/huth2016   \n      \n    Read the paper here: http://www.nature.com/doifinder/10.1038/nature17637  \n      \n    28th April 2016</div> ](#k61nJkx5aDQ \"The brain dictionary\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![The most important lesson from 83,000 brain scans | Daniel Amen | TEDxOrangeCoast](https://i.ytimg.com/vi/esPRsT-lmw8/sddefault.jpg)</div> </div>### The most important lesson from 83,000 brain scans | Daniel Amen | TEDxOrangeCoast\n    \n    <div class=\"yotu-video-description\">Never miss a talk! SUBSCRIBE to the TEDx channel: http://bit.ly/1FAg8hB  \n      \n    In the spirit of ideas worth spreading, TEDx is a program of local, self-organized events that bring people together to share a TED-like experience. At a TEDx event, TEDTalks video and live speakers combine to spark deep discussion and connection in a small group. These local, self-organized events are branded TEDx, where x = independently organized TED event. The TED Conference provides general guidance for the TEDx program, but individual TEDx events are self-organized.* (*Subject to certain rules and regulations)</div> ](#esPRsT-lmw8 \"The most important lesson from 83,000 brain scans | Daniel Amen | TEDxOrangeCoast\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Henry Stapp on Quantum Mechanics and Human Consciousness](https://i.ytimg.com/vi/ZYPjXz1MVv0/hqdefault.jpg)</div> </div>### Henry Stapp on Quantum Mechanics and Human Consciousness\n    \n    <div class=\"yotu-video-description\">The United Nations, New York - September 11, 2008  \n     Beyond the Mind-Body Problem: New Paradigms in the Science of Consciousness  \n      \n     An excerpt of Henry Stapp, PhD talking about how quantum mechanics can explain how intent can cause changes in the structure of the brain that can ultimately affect behavior.  \n      \n     For more information about the symposium, visit:   \n     http://www.nourfoundation.com/mind-body-problem</div> ](#ZYPjXz1MVv0 \"Henry Stapp on Quantum Mechanics and Human Consciousness\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Russell Brand Interviews Quantum Physicist Dr. John Hagelin (Part 13)](https://i.ytimg.com/vi/LRgu3V6Ex_A/sddefault.jpg)</div> </div>### Russell Brand Interviews Quantum Physicist Dr. John Hagelin (Part 13)\n    \n    <div class=\"yotu-video-description\">http://wwww.davidlynchfoundation.org/ In this video actor/comedian Russell Brand interviews renowned quantum physicist Dr. John Hagelin at the David Lynch Foundation's 3rd annual Change Begins Within gala in Los Angeles. They discuss the science behind the Transcendental Meditation technique, the Unified Field, and the importance of having a quantum physicist as a butler. Russell then welcomes film director David Lynch to the stage.  \n      \n    For more information on the David Lynch Foundation please visit http://www.davidlynchfoundation.org/  \n      \n    For more information on the Transcendental Meditation technique, scholarship information, and to contact your local teacher, please visit http://www.tm.org?leadsource=CRM1262</div> ](#LRgu3V6Ex_A \"Russell Brand Interviews Quantum Physicist Dr. John Hagelin (Part 13)\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Mind and the Wave Function Collapse, John Hagelin in conversation with Henry Stapp](https://i.ytimg.com/vi/hSv0NLSCEYo/sddefault.jpg)</div> </div>### Mind and the Wave Function Collapse, John Hagelin in conversation with Henry Stapp\n    \n    <div class=\"yotu-video-description\">In this conversation John Hagelin and Henry Stapp discuss the collapse of the wave function, the connection between mind and wave function, superposition, quantum observer, experience and objective reality and other quantum conundrums.   \n      \n    Henry Stapp, Ph.D. Quantum Physicist  \n      \n    Stapp received his Ph.D. in particle physics at the University of California, Berkeley, under the supervision of Nobel Laureates Emilio Segrè and Owen Chamberlain. Wolfgang Pauli visited Berkeley in the spring of 1958. He talked extensively with Stapp, and invited him to work with him in Zurich in the Fall. Stapp worked in Zurich with Pauli on fundamental problems until Pauli sudden unexpected death in December. In 1970 Werner Heisenberg invited Stapp to Munich, where the two conversed often on fundamental issues surrounding quantum mechanics. After returning to Berkeley wrote an influential article The Copenhagen Interpretation, published in the American Journal of Physics with Heisenberg’s comments appearing in an Appendix. Stapp has has made major contributions to analytic S-matrix theory, generalizations of Bell’s theorems, and understanding the quantum  \n    connection of mind to physical processes.  \n      \n    John Hagelin, Ph.D.  \n    President of the David Lynch Foundation  \n    President of the Global Union of Scientists for Peace  \n      \n    SAND14_John-HagelinJohn Hagelin, Ph.D., is a world-renowned quantum physicist, educator, public policy expert, and leading proponent of peace. Dr. Hagelin received his A.B. summa cum laude from Dartmouth College and his M.A. and Ph.D. from Harvard University, and conducted pioneering research at CERN (the European Center for Particle Physics) and SLAC (the Stanford Linear Accelerator Center). His scientific contributions in the fields of electroweak unification, grand unification, super-symmetry and cosmology include some of the most cited references in the physical sciences. He is also responsible for the development of a highly successful Grand Unified Field Theory based on the Superstring. But Dr. Hagelin is unique among scientists in being the first to apply this most advanced knowledge for the practical benefit of humankind. He has pioneered the use of Unified Field-based technologies proven to reduce crime, violence, terrorism, and war and to promote peace throughout society—technologies derived from the ancient Vedic science of consciousness.  \n      \n    http://www.hagelin.org</div> ](#hSv0NLSCEYo \"Mind and the Wave Function Collapse, John Hagelin in conversation with Henry Stapp\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Noam Chomsky \"Grammar, Mind and Body- A Personal View\"](https://i.ytimg.com/vi/wMQS3klG3N0/sddefault.jpg)</div> </div>### Noam Chomsky \"Grammar, Mind and Body- A Personal View\"\n    \n    <div class=\"yotu-video-description\">Noam Chomsky speaks about language and philosophy as part of the Dean's Lecture Series.</div> ](#wMQS3klG3N0 \"Noam Chomsky \"Grammar, Mind and Body- A Personal View\"\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![David Deutsch: A new way to explain explanation](https://i.ytimg.com/vi/folTvNDL08A/hqdefault.jpg)</div> </div>### David Deutsch: A new way to explain explanation\n    \n    <div class=\"yotu-video-description\">http://www.ted.com For tens of thousands of years our ancestors understood the world through myths, and the pace of change was glacial. The rise of scientific understanding transformed the world within a few centuries. Why? Physicist David Deutsch proposes a subtle answer.  \n      \n     TEDTalks is a daily video podcast of the best talks and performances from the TED Conference, where the world's leading thinkers and doers give the talk of their lives in 18 minutes. Featured speakers have included Al Gore on climate change, Philippe Starck on design, Jill Bolte Taylor on observing her own stroke, Nicholas Negroponte on One Laptop per Child, Jane Goodall on chimpanzees, Bill Gates on malaria and mosquitoes, Pattie Maes on the \"Sixth Sense\" wearable tech, and \"Lost\" producer JJ Abrams on the allure of mystery. TED stands for Technology, Entertainment, Design, and TEDTalks cover these topics as well as science, business, development and the arts. Closed captions and translated subtitles in a variety of languages are now available on TED.com, at http://www.ted.com/translate. Watch a highlight reel of the Top 10 TEDTalks at http://www.ted.com/index.php/talks/top10</div> ](#folTvNDL08A \"David Deutsch: A new way to explain explanation\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![The Secret of Life - Alan Watts](https://i.ytimg.com/vi/iZ8so-ld-l0/sddefault.jpg)</div> </div>### The Secret of Life - Alan Watts\n    \n    <div class=\"yotu-video-description\">Subscribe for more inspiring videos  \n    http://www.youtube.com/subscription_center?add_user=thejourneyofpurpose  \n    www.facebook.com/TJOPofficial  \n    www.instagram.com/TJOPofficial  \n      \n    Words by Alan Watts  \n    TJOP does not claim ownership over any footages or music presented in this video  \n    Please See credits at the end of presentation (music, speech, footages)  \n    Copyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"fair use\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use. All copyrighted materials contained herein belong to their respective copyright holders, I do not claim ownership over any of these materials. I realize no profit, monetary or otherwise, from the exhibition of this video.  \n      \n    Facebook: www.facebook.com/thejourneyofpurposeTJOP  \n    \"This is the real secret of life -- to be completely engaged with what you are doing in the here and now. And instead of calling it work, realize it is play.\"  \n    Alan Wilson Watts  \n      \n    Speech from:  \n    Alan Watts - Work as Play</div> ](#iZ8so-ld-l0 \"The Secret of Life - Alan Watts\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Joe Rogan - Mathematician on Trying to Measure Consciousness](https://i.ytimg.com/vi/GX10mR_N0Vs/sddefault.jpg)</div> </div>### Joe Rogan - Mathematician on Trying to Measure Consciousness\n    \n    <div class=\"yotu-video-description\">Taken from Joe Rogan Experience #1216:  \n    https://www.youtube.com/watch?v=GEw0ePZUMHA</div> ](#GX10mR_N0Vs \"Joe Rogan - Mathematician on Trying to Measure Consciousness\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![TEDx Brussels 2010 - Stuart Hameroff - Do we have a quantum Soul?](https://i.ytimg.com/vi/iIyEjh6ef_8/hqdefault.jpg)</div> </div>### TEDx Brussels 2010 - Stuart Hameroff - Do we have a quantum Soul?\n    \n    <div class=\"yotu-video-description\">Dr. Hameroff's research for 35 years has involved consciousness - how the pinkish gray meat between our ears produces the richness of experiential awareness. A clinical anesthesiologist, Hameroff has studied how anesthetic gas molecules selectively erase consciousness via delicate quantum effects on protein dynamics. Following a longstanding interest in the computational capacity of microtubules inside neurons, Hameroff teamed with the eminent British physicist Sir Roger Penrose to develop a controversial quantum theory of consciousness called orchestrated objective reduction (Orch OR) which connects brain processes to fundamental spacetime geometry. Recently Hameroff has explored the theoretical implications of Orch OR for consciousness to exist independent of the body, distributed in deeper, lower, faster scales in non-local, holographic spacetime, raising possible scientific approaches to the soul and spirituality.  \n      \n     @TEDxBrussels 2010</div> ](#iIyEjh6ef_8 \"TEDx Brussels 2010 - Stuart Hameroff - Do we have a quantum Soul?\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![CONSCIOUSNESS - A conversation with Deepak Chopra and Stuart Hameroff](https://i.ytimg.com/vi/erSd5xep30w/sddefault.jpg)</div> </div>### CONSCIOUSNESS - A conversation with Deepak Chopra and Stuart Hameroff\n    \n    <div class=\"yotu-video-description\">🙏🙏 The New Free Courses by The Chopra Well Guests 🙏🙏   \n    FREE Feminine Power Breakthrough Ebook 👉https://bit.ly/FreeFemininePowerEbook   \n    FREE Feminine Power Seminar 👉https://bit.ly/FreeFemininePowerCourse   \n    SPECIAL FREE COURSE : How to Unlock YOUR QUANTUM POWERS 👉https://bit.ly/UnlockQuantumPower   \n      \n    --   \n    Join me for @chopra's 21 days of free, guided meditation with @jbalvin: https://bit.ly/21DayWithDeepak  \n    From Human to #Metahuman - 🙏 Get the book @ http://bit.ly/METAHUMAN 🙏  \n    Description: Deepak Chopra and Stuart Hameroff take an in-depth dive into the science of consciousness.*  \n      \n    Stuart Hameroff, MD is a physician, Professor of Anesthesiology and Psychology, and Director of the Center for Consciousness Studies at the University of Arizona in Tucson. In medical school, Hameroff became interested in intelligent behavior of microtubules, protein lattices within brain neurons and other living cells. Hameroff developed theories of microtubules as self-organizing molecular computers, and teamed with Sir Roger Penrose on the controversial Penrose-Hameroff \"Orch OR\" model of consciousness. Based on quantum computing in brain microtubules, Orch OR connects brain activities to the most basic level of the universe -- fundamental spacetime geometry at the Planck scale. At that level, Penrose has proposed Platonic information guiding or influencing conscious choices and perceptions. Orch OR could be seen as providing a plausibility argument for non-locality and spirituality. Hameroff is also involved with clinical trials of transcranial ultrasound (TUS) for mood and cognitive dysfunction, and co-organizes the biennial interdisciplinary conference 'Toward a Science of Consciousness.'  \n      \n    ____  \n      \n    THE CHOPRA WELL is dedicated to inspiring, fun, and thought-provoking videos about healthy living, wellness, and spirituality. We are anchored by doctor and author Deepak Chopra, as well as family &amp; friends, who aim to provide you with tools for personal and social transformation. We deal with some \"serious\" topics and themes here, but we definitely don't take ourselves too seriously. We encourage you to watch a few videos, engage with us and fellow viewers by sharing comments, and subscribing to show your support for our channel. We have several new videos every week, so come back soon! Click here to subscribe: http://bit.ly/M5z254  \n      \n    Read our blogs here: http://intentblog.com/author/thechoprawell/  \n    Follow us on Twitter: http://twitter.com/thechoprawell  \n    Follow Deepak on Twitter: https://twitter.com/deepakchopra  \n    Follow us on Facebook: http://goo.gl/nclfA  \n    Add The Chopra Well to your Google+: http://bit.ly/Oryrwu  \n      \n    THE CHOPRA WELL is dedicated to inspiring, fun, and thought-provoking videos about healthy living, wellness, and spirituality. We are anchored by doctor and author Deepak Chopra, as well as family &amp; friends, who aim to provide you with tools for personal and social transformation. We deal with some \"serious\" topics and themes here, but we definitely don't take ourselves too seriously. We encourage you to watch a few videos, engage with us and fellow viewers by sharing comments, and subscribing to show your support for our channel. We have several new videos every week, so come back soon!</div> ](#erSd5xep30w \"CONSCIOUSNESS - A conversation with Deepak Chopra and Stuart Hameroff\")\n\n</div><div class=\"yotu-pagination yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">6</span>[Next](#)</div> </div></div>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","title":"Image Recgonition Video Playlist"},"frontmatter":{"draft":false},"rawBody":"---\nid: 384\ntitle: 'Image Recgonition Video Playlist'\ndate: '2018-09-23T14:13:36+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=384'\npermalink: /image-recgonition-video-playlist/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Acquiring Data'\n    - 'Computer Vision'\n    - 'Video Library'\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMQ2JtejBWU1pfdnIwRlVSRzFqdFEtcG9aT0JpdnNqb2wiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2ViMzc2MiIsIm5leHQiOiJFQUFhQmxCVU9rTkJkdyIsInByZXYiOiIifQ==\" data-showdesc=\"on\" data-total=\"2\" data-yotu=\"63ac7c3ee6de4\" id=\"yotuwp-63ac7c3eb3762\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> The Future of Augmented Reality </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3ee6de4\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> A conceptual view of how augmented reality may be used in the future.  \n  \nAlthough the video depicts the user with a smartphone, in the future it is more likely that the user will be wearing some form of augmented reality eyewear.  \n  \nOnce refined and adapted, augmented reality will play a key role in the way we all interact with the digital world. At the moment, there are a few challenges we face that must be overcome before this is possible…  \n  \n1. Develop socially acceptable eyewear - something that has the ‘Oakley Effect’ (looks cool)  \n  \n2. Increase the coverage and speed of our networks in order to sustain the throughput required to send and receive rich and dynamic virtual content  \n  \n3. Improve storage capacity and reduce battery weight so to accommodate the processing speeds and form factor required  \n  \nAbout Hidden Creative –  \nComplex systems can be very hard to translate into meaningful experiences, but that’s what we do best. We thrive on tackling expansive challenges and have developed the kind of agile structure necessary to deliver intricate solutions in typically short industry timescales.  \n  \nWorking at the cutting edge of technology, we collaborate with brands that are delivering future-shaping innovations on a global scale. By integrating closely with their internal teams, we provide a discreet layer of full service expertise that can be deployed wherever it’s required in the world.  \n  \nAcross our dedicated and passionate team you’ll find a unique combination of digital expertise and personal service that ensures we always go the extra mile to meet your goals.  \n  \n+44 0161 236 8181  \nstudio@hiddenltd.com  \n  \nhttp://www.hiddenltd.com </div> </div><div class=\"yotu-pagination yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">2</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![The Future of Augmented Reality](https://i.ytimg.com/vi/tnRJaHZH9lo/sddefault.jpg)</div> </div>### The Future of Augmented Reality\n    \n    <div class=\"yotu-video-description\">A conceptual view of how augmented reality may be used in the future.  \n      \n    Although the video depicts the user with a smartphone, in the future it is more likely that the user will be wearing some form of augmented reality eyewear.  \n      \n    Once refined and adapted, augmented reality will play a key role in the way we all interact with the digital world. At the moment, there are a few challenges we face that must be overcome before this is possible…  \n      \n    1. Develop socially acceptable eyewear - something that has the ‘Oakley Effect’ (looks cool)  \n      \n    2. Increase the coverage and speed of our networks in order to sustain the throughput required to send and receive rich and dynamic virtual content  \n      \n    3. Improve storage capacity and reduce battery weight so to accommodate the processing speeds and form factor required  \n      \n    About Hidden Creative –  \n    Complex systems can be very hard to translate into meaningful experiences, but that’s what we do best. We thrive on tackling expansive challenges and have developed the kind of agile structure necessary to deliver intricate solutions in typically short industry timescales.  \n      \n    Working at the cutting edge of technology, we collaborate with brands that are delivering future-shaping innovations on a global scale. By integrating closely with their internal teams, we provide a discreet layer of full service expertise that can be deployed wherever it’s required in the world.  \n      \n    Across our dedicated and passionate team you’ll find a unique combination of digital expertise and personal service that ensures we always go the extra mile to meet your goals.  \n      \n    +44 0161 236 8181  \n    studio@hiddenltd.com  \n      \n    http://www.hiddenltd.com</div> ](#tnRJaHZH9lo \"The Future of Augmented Reality\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Privacy visor glasses jam facial recognition systems to protect your privacy #DigInfo](https://i.ytimg.com/vi/LRj8whKmN1M/sddefault.jpg)</div> </div>### Privacy visor glasses jam facial recognition systems to protect your privacy #DigInfo\n    \n    <div class=\"yotu-video-description\">Privacy visor glasses jam facial recognition systems to protect your privacy  \n    (http://www.diginfo.tv/v/13-0050-r-en.php)  \n      \n    14/6/2013 NII Open House 2013  \n      \n    National Institute of Informatics  \n    Privacy Visor  \n      \n    DigInfo TV - http://www.diginfo.tv</div> ](#LRj8whKmN1M \"Privacy visor glasses jam facial recognition systems to protect your privacy #DigInfo\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Real Time Object Recognition using SURF and OpenCV EEL6562](https://i.ytimg.com/vi/ZXn69V-1kEM/sddefault.jpg)</div> </div>### Real Time Object Recognition using SURF and OpenCV EEL6562\n    \n    <div class=\"yotu-video-description\">Real Time Object Recognition using SURF and OpenCV EEL6562  \n      \n    Find the code, report, and presentation at:  \n    http://frankbergschneider.weebly.com/</div> ](#ZXn69V-1kEM \"Real Time Object Recognition using SURF and OpenCV EEL6562\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Object Detection and Recogition](https://i.ytimg.com/vi/tlC2O9T9jks/sddefault.jpg)</div> </div>### Object Detection and Recogition\n    \n    <div class=\"yotu-video-description\">http://www.willowgarage.com/blog/2010/09/20/scalable-object-recognition</div> ](#tlC2O9T9jks \"Object Detection and Recogition\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Private video]()</div> </div>### Private video\n    \n    <div class=\"yotu-video-description\">This video is private.</div> ](#mdhvRNYX0PI \"Private video\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![9. Constraints: Visual Object Recognition](https://i.ytimg.com/vi/gvmfbePC2pc/sddefault.jpg)</div> </div>### 9. Constraints: Visual Object Recognition\n    \n    <div class=\"yotu-video-description\">MIT 6.034 Artificial Intelligence, Fall 2010  \n    View the complete course: http://ocw.mit.edu/6-034F10  \n    Instructor: Patrick Winston  \n      \n    We consider how object recognition has evolved over the past 30 years. In alignment theory, 2-D projections are used to determine whether an additional picture is of the same object. To recognize faces, we use intermediate-sized features and correlation.  \n      \n    License: Creative Commons BY-NC-SA  \n    More information at http://ocw.mit.edu/terms  \n    More courses at http://ocw.mit.edu</div> ](#gvmfbePC2pc \"9. Constraints: Visual Object Recognition\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Lane tracking and vehicle tracking (rainy day)](https://i.ytimg.com/vi/JmxDIuCIIcg/hqdefault.jpg)</div> </div>### Lane tracking and vehicle tracking (rainy day)\n    \n    <div class=\"yotu-video-description\">http://marcosnietoblog.wordpress.com/  \n    Lane tracking for Lane Departure Warning Systems (LDWS) and vehicle detection and tracking for Safety Distance System (SDS)</div> ](#JmxDIuCIIcg \"Lane tracking and vehicle tracking (rainy day)\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Curved Lane Detection](https://i.ytimg.com/vi/VlH3OEhZnow/sddefault.jpg)</div> </div>### Curved Lane Detection\n    \n    <div class=\"yotu-video-description\">Curved Lane Detection using OpenCV  \n      \n    This algorithm not only detects lanes, curves as well as straight, but also predict the direction of upcoming curves.  \n      \n    It was developed by Chanhee Jang, Yeongji Park, and Jiye Yun, under the supervision of Prof. Young-keun Kim, for a final project for Mechatronics Capstone Design ll @ Handong University. (2014.12.20)  \n      \n    Original footage is taken by Hagseo Park and can be found in following link: http://www.youtube.com/watch?v=T6c0o7iR2u4</div> ](#VlH3OEhZnow \"Curved Lane Detection\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![RIVA IP Cameras - People Counting Supermarket](https://i.ytimg.com/vi/tL-cCQkl20Q/sddefault.jpg)</div> </div>### RIVA IP Cameras - People Counting Supermarket\n    \n    <div class=\"yotu-video-description\">RIVA IP Cameras with integrated video analytics</div> ](#tL-cCQkl20Q \"RIVA IP Cameras - People Counting Supermarket\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Deep Green & Augmented Reality Pool](https://i.ytimg.com/vi/AENJxqR0g48/hqdefault.jpg)</div> </div>### Deep Green &amp; Augmented Reality Pool\n    \n    <div class=\"yotu-video-description\">The RCVLab at Queen's University demonstrates Deep Green, a pool playing robot, and ARPool, an augmented reality system for teaching the science of pool.</div> ](#AENJxqR0g48 \"Deep Green & Augmented Reality Pool\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Deleted video]()</div> </div>### Deleted video\n    \n    <div class=\"yotu-video-description\">This video is unavailable.</div> ](#eob532iEpqk \"Deleted video\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Natural Vision: Wild flower recognition app](https://i.ytimg.com/vi/trk0GsL0oZg/sddefault.jpg)</div> </div>### Natural Vision: Wild flower recognition app\n    \n    <div class=\"yotu-video-description\">An app that lets you take a picture of a wild flower and use image recognition to tell you which species you're looking at.  \n      \n    For more information, send an email to daniel.nouri@gmail.com</div> ](#trk0GsL0oZg \"Natural Vision: Wild flower recognition app\")\n\n</div><div class=\"yotu-pagination yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">2</span>[Next](#)</div> </div></div>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","title":"Introduction to AI"},"frontmatter":{"draft":false},"rawBody":"---\nid: 389\ntitle: 'Introduction to AI'\ndate: '2018-09-23T14:20:28+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=389'\npermalink: /introduction-to-ai/\ninline_featured_image:\n    - '0'\ncategories:\n    - AI\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMSWUzY3YtbW9idTFlbDZZZ0dsQVhxaWZzaGZLWmpxdmIiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2YxYmE0OSIsIm5leHQiOiJFQUFhQmxCVU9rTkJkdyIsInByZXYiOiIifQ==\" data-showdesc=\"on\" data-total=\"3\" data-yotu=\"63ac7c3f3b44e\" id=\"yotuwp-63ac7c3f1ba49\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> Public Lecture with Google DeepMind's Demis Hassabis </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3f3b44e\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> Watch the founder of Google DeepMind's Demis Hassabis' lecture about the future and capabilities of artificial intelligence.  \n  \nThis video was filmed by IET TV.   \n  \nFor more information about the Hassabis talk visit the RTS website to read the event report. </div> </div><div class=\"yotu-pagination yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">3</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![Public Lecture with Google DeepMind's Demis Hassabis](https://i.ytimg.com/vi/0X-NdPtFKq0/sddefault.jpg)</div> </div>### Public Lecture with Google DeepMind's Demis Hassabis\n    \n    <div class=\"yotu-video-description\">Watch the founder of Google DeepMind's Demis Hassabis' lecture about the future and capabilities of artificial intelligence.  \n      \n    This video was filmed by IET TV.   \n      \n    For more information about the Hassabis talk visit the RTS website to read the event report.</div> ](#0X-NdPtFKq0 \"Public Lecture with Google DeepMind's Demis Hassabis\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Future directions of machine learning: Part 2](https://i.ytimg.com/vi/XAbLn66iHcQ/sddefault.jpg)</div> </div>### Future directions of machine learning: Part 2\n    \n    <div class=\"yotu-video-description\">On 22 May 2015, the Royal Society launched a new series of high-level conferences on major scientific and technical challenges of the next decade. 'Breakthrough Science and Technologies: Transforming our Future' conferences feature cutting-edge science from industry and academia and bring together leading experts from the wider scientific community, industry, government, funding bodies and charities.  \n      \n    The first conference was on the topic of machine learning and was organised by Dr Hermann Hauser KBE FREng FRS and Dr Robert Ghanea-Hercock.   \n      \n    This video session features:  \n      \n    - Professor Steve Furber CBE FREng FRS, “Building Brains” (00:00)  \n    - Dr Demis Hassabis, \"General learning algorithms” (1:25:55)  \n    - Simon Knowles, “Machines for intelligence” (28:58)  \n    - Professor Simon Benjamin, “Machine learning as a near-future applications of emerging quantum technologies” (56:05)  \n      \n    Read more about the conference on the Royal Society website: https://royalsociety.org/events/2015/05/breakthrough-science-technologies-machine-learning/</div> ](#XAbLn66iHcQ \"Future directions of machine learning: Part 2\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Shane Legg on a New Metric for Measuring Macine Intelligence](https://i.ytimg.com/vi/V6umr1OP8uo/hqdefault.jpg)</div> </div>### Shane Legg on a New Metric for Measuring Macine Intelligence\n    \n    <div class=\"yotu-video-description\"></div> ](#V6umr1OP8uo \"Shane Legg on a New Metric for Measuring Macine Intelligence\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Can A.I. Become More Human? // Gary Marcus, Geometric Intelligence (Hosted by FirstMark Capital)](https://i.ytimg.com/vi/PtHicmmblMk/sddefault.jpg)</div> </div>### Can A.I. Become More Human? // Gary Marcus, Geometric Intelligence (Hosted by FirstMark Capital)\n    \n    <div class=\"yotu-video-description\">Gary Marcus, CEO at Geometric Intelligence, presented at FirstMark's Data Driven NYC on January 19, 2016. Marcus' talk focused on ways A.I. is stuck.  \n      \n    Geometric Intelligence is redefining the boundaries of machine learning through innovative, patent-pending techniques that learn more efficiently from less data.  \n      \n    Data Driven NYC is a monthly event covering Big Data and data-driven products and startups, hosted by Matt Turck, partner at FirstMark.  \n      \n    FirstMark is an early stage venture capital firm based in New York City. Find out more about Data Driven NYC at http://datadrivennyc.com and FirstMark Capital at http://firstmarkcap.com.</div> ](#PtHicmmblMk \"Can A.I. Become More Human? // Gary Marcus, Geometric Intelligence (Hosted by FirstMark Capital)\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> !['Accelerated Understanding: Deep learning Intelligent Applications and GPUs](https://i.ytimg.com/vi/ZfNspUJt18I/sddefault.jpg)</div> </div>### 'Accelerated Understanding: Deep learning Intelligent Applications and GPUs\n    \n    <div class=\"yotu-video-description\"></div> ](#ZfNspUJt18I \"'Accelerated Understanding: Deep learning Intelligent Applications and GPUs\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Private video]()</div> </div>### Private video\n    \n    <div class=\"yotu-video-description\">This video is private.</div> ](#CJzoi9BEg9o \"Private video\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Extracting Knowledge from Text - Pedro Domingos](https://i.ytimg.com/vi/6ZJzfRdCZjc/sddefault.jpg)</div> </div>### Extracting Knowledge from Text - Pedro Domingos\n    \n    <div class=\"yotu-video-description\">Title: Extracting Knowledge from Text w/ Tractable Markov Logic &amp; Symmetry-Based Semantic Parsing  \n    Abstract: Building very large commonsense knowledge bases and  \n    reasoning with them is a long-standing dream of AI. Today that  \n    knowledge is available in text; all we have to do is extract it. Text,  \n    however, is extremely messy, noisy, ambiguous, incomplete, and  \n    variable. A formal representation of it needs to be both probabilistic  \n    and relational, either of which leads to intractable inference and  \n    therefore poor scalability. In the first part of this talk I will  \n    describe tractable Markov logic, a language that is restricted enough  \n    to be tractable yet expressive enough to represent much of the  \n    commonsense knowledge contained in text. Even then, transforming text  \n    into a formal representation of its meaning remains a difficult  \n    problem. There is no agreement on what the representation primitives  \n    should be, and labeled data in the form of sentence-meaning pairs for  \n    training a semantic parser is very hard to come by. In the second part  \n    of the talk I will propose a solution to both these problems, based on  \n    concepts from symmetry group theory. A symmetry of a sentence is a  \n    syntactic transformation that does not change its meaning. Learning a  \n    semantic parser for a language is discovering its symmetry group, and  \n    the meaning of a sentence is its orbit under the group (i.e., the set  \n    of all sentences it can be mapped to by composing symmetries).  \n    Preliminary experiments indicate that tractable Markov logic and  \n    symmetry-based semantic parsing can be powerful tools for scalably extracting knowledge from text.</div> ](#6ZJzfRdCZjc \"Extracting Knowledge from Text - Pedro Domingos\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![The Master Algorithm | Pedro Domingos | Talks at Google](https://i.ytimg.com/vi/B8J4uefCQMc/sddefault.jpg)</div> </div>### The Master Algorithm | Pedro Domingos | Talks at Google\n    \n    <div class=\"yotu-video-description\">Machine learning is the automation of discovery, and it is responsible for making our smartphones work, helping Netflix suggest movies for us to watch, and getting presidents elected. But there is a push to use machine learning to do even more—to cure cancer and AIDS and possibly solve every problem humanity has. Domingos is at the very forefront of the search for the Master Algorithm, a universal learner capable of deriving all knowledge—past, present and future—from data. In this book, he lifts the veil on the usually secretive machine learning industry and details the quest for the Master Algorithm, along with the revolutionary implications such a discovery will have on our society.  \n      \n    Pedro Domingos is a Professor of Computer Science and Engineering at the University of Washington, and he is the cofounder of the International Machine Learning Society.  \n      \n    https://books.google.com/books/about/The_Master_Algorithm.html?id=glUtrgEACAAJ  \n      \n    This Authors at Google talk was hosted by Boris Debic.  \n      \n    eBook  \n    https://play.google.com/store/books/details/Pedro_Domingos_The_Master_Algorithm?id=CPgqCgAAQBAJ</div> ](#B8J4uefCQMc \"The Master Algorithm | Pedro Domingos | Talks at Google\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Open and Exploratory Extraction of Relations and Common Sense from Large Text Corpora - Alan Akbik](https://i.ytimg.com/vi/Ce499F0M_rM/sddefault.jpg)</div> </div>### Open and Exploratory Extraction of Relations and Common Sense from Large Text Corpora - Alan Akbik\n    \n    <div class=\"yotu-video-description\">Alan Akbik November 10, 2014  \n    Title: Open and Exploratory Extraction of Relations (and Common Sense) from Large Text Corpora  \n      \n    Abstract: The use of deep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction (IE). Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this talk, I present an approach and a graphical tool that allows even novice users to quickly and easily define extraction patterns over dependency trees and directly execute them on a very large text corpus. This enables users to explore a corpus for structured information of interest in a highly interactive and data-guided fashion, and allows them to create extractors for those semantic relations they find interesting. I then present a project in which we use Information Extraction to automatically construct a very large common sense knowledge base. This knowledge base - dubbed \"The Weltmodell\" - contains common sense facts that pertain to proper noun concepts; an example of this is the concept \"coffee\", for which we know that it is typically drunk by a person or brought by a waiter. I show how we mine such information from very large amounts of text, how we quantify notions such as typicality and similarity, and discuss some ideas how such world knowledge can be used to address reasoning tasks.</div> ](#Ce499F0M_rM \"Open and Exploratory Extraction of Relations and Common Sense from Large Text Corpora - Alan Akbik\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Claudio Delli Bovi: Open Information Extraction: Where Are We Going?](https://i.ytimg.com/vi/EhOF_AbDwcE/sddefault.jpg)</div> </div>### Claudio Delli Bovi: Open Information Extraction: Where Are We Going?\n    \n    <div class=\"yotu-video-description\">Claudio Delli Bovi  \n      \n    Open Information Extraction: where are we going?  \n      \n    ABSTRACT: The Open Information Extraction (OIE) paradigm has received much attention in the NLP community over the last decade. Since the earliest days, most OIE approaches have been focusing on Web-scale corpora, which raises issues such as massive amounts of noise. Also, OIE systems can be very different in nature and develop their own type inventories, with no portable ontological structure. This talk steps back and explores both issues by presenting two substantially different approaches to the task: in the first we shift the target of a full-fledged OIE pipeline to a relatively small, dense corpus of definitional knowledge; in the second we try to make sense of different OIE outputs by merging them into a single, unified and fully disambiguated knowledge repository.</div> ](#EhOF_AbDwcE \"Claudio Delli Bovi: Open Information Extraction: Where Are We Going?\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Dave Snowden | Managing under conditions of uncertainty | State of the Net 2014](https://i.ytimg.com/vi/APB_mhpsQp8/sddefault.jpg)</div> </div>### Dave Snowden | Managing under conditions of uncertainty | State of the Net 2014\n    \n    <div class=\"yotu-video-description\">State of the Net 2014 - Trieste (Italy), June 12th-14th  \n      \n    Keynote speech  \n    Managing under conditions of uncertainty: lessons from the natural sciences  \n      \n    Dave Snowden, chief scientific officer at Cognitive Edge  \n    http://sotn.it/speakers/dave-snowden/  \n      \n      \n    State of the Net  \n    http://sotn.it  \n    http://twitter.com/stateofthenet  \n    http://facebook.com/stateofthenet  \n    \\#sotn14</div> ](#APB_mhpsQp8 \"Dave Snowden | Managing under conditions of uncertainty | State of the Net 2014\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Debate: \"Does AI Need More Innate Machinery?\" (Yann LeCun, Gary Marcus)](https://i.ytimg.com/vi/vdWPQ6iAkT4/sddefault.jpg)</div> </div>### Debate: \"Does AI Need More Innate Machinery?\" (Yann LeCun, Gary Marcus)\n    \n    <div class=\"yotu-video-description\">Debate between Yann LeCun and Gary Marcus at NYU, October 5 2017. Moderated by David Chalmers. Sponsored by the NYU Center for Mind, Brain and Consciousness.</div> ](#vdWPQ6iAkT4 \"Debate: \"Does AI Need More Innate Machinery?\" (Yann LeCun, Gary Marcus)\")\n\n</div><div class=\"yotu-pagination yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">3</span>[Next](#)</div> </div></div>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","title":"Introduction to Semantic Web"},"frontmatter":{"draft":false},"rawBody":"---\nid: 386\ntitle: 'Introduction to Semantic Web'\ndate: '2018-09-23T14:15:12+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=386'\npermalink: /introduction-to-semantic-web/\ninline_featured_image:\n    - '0'\ncategories:\n    - People\n    - 'Semantic Web'\n    - 'Video Library'\n    - 'WWW Education'\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMQ2JtejBWU1pfdnBWWHZObkkxbTZoYVllQ1g0Qk9ZYmYiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2VlZDAzYSIsIm5leHQiOiJFQUFhQmxCVU9rTkJkdyIsInByZXYiOiIifQ==\" data-showdesc=\"on\" data-total=\"2\" data-yotu=\"63ac7c3f1660d\" id=\"yotuwp-63ac7c3eed03a\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> [#OHM13] the secure social web - bblfish </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3f1660d\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> OHM2013. Observe, Hack, Make. A five day outdoor international camping festival for hackers and makers, and those with an inquisitive mind. https://program.ohm2013.org/ </div> </div><div class=\"yotu-pagination yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">2</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![[#OHM13] the secure social web - bblfish](https://i.ytimg.com/vi/qnhdv8_xLTM/sddefault.jpg)</div> </div>### \\[#OHM13\\] the secure social web - bblfish\n    \n    <div class=\"yotu-video-description\">OHM2013. Observe, Hack, Make. A five day outdoor international camping festival for hackers and makers, and those with an inquisitive mind. https://program.ohm2013.org/</div> ](#qnhdv8_xLTM \"[#OHM13] the secure social web - bblfish\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Intro to the Semantic Web](https://i.ytimg.com/vi/OGg8A2zfWKg/hqdefault.jpg)</div> </div>### Intro to the Semantic Web\n    \n    <div class=\"yotu-video-description\">A short introduction to the semantic web.</div> ](#OGg8A2zfWKg \"Intro to the Semantic Web\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![The Semantic Web of Data Tim Berners-Lee](https://i.ytimg.com/vi/HeUrEh-nqtU/hqdefault.jpg)</div> </div>### The Semantic Web of Data Tim Berners-Lee\n    \n    <div class=\"yotu-video-description\">The Semantic Web of Data  \n    Tim Berners-Lee</div> ](#HeUrEh-nqtU \"The Semantic Web of Data Tim Berners-Lee\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![The Semantic Web - An Overview](https://i.ytimg.com/vi/rhgUDGtT2EM/sddefault.jpg)</div> </div>### The Semantic Web - An Overview\n    \n    <div class=\"yotu-video-description\">Find out what the Semantic Web is all about and how it might be structured. How can we make computers smarter so we can get the information we need in context?   \n      \n     From the online Web Services class offered by Computer Careers at South Central College located in North Mankato, MN 56003 USA. http://cc.SouthCentral.edu</div> ](#rhgUDGtT2EM \"The Semantic Web - An Overview\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Tim Berners-Lee: The next Web of open, linked data](https://i.ytimg.com/vi/OM6XIICm_qo/hqdefault.jpg)</div> </div>### Tim Berners-Lee: The next Web of open, linked data\n    \n    <div class=\"yotu-video-description\">http://www.ted.com 20 years ago, Tim Berners-Lee invented the World Wide Web. For his next project, he's building a web for open, linked data that could do for numbers what the Web did for words, pictures, video: unlock our data and reframe the way we use it together.  \n      \n      \n     TEDTalks is a daily video podcast of the best talks and performances from the TED Conference, where the world's leading thinkers and doers give the talk of their lives in 18 minutes. TED stands for Technology, Entertainment, Design, and TEDTalks cover these topics as well as science, business, development and the arts. Watch the Top 10 TEDTalks on TED.com, at http://www.ted.com/index.php/talks/top10  \n      \n      \n     Follow us on Twitter  \n     http://www.twitter.com/tednews  \n      \n     Checkout our Facebook page for TED exclusives  \n     https://www.facebook.com/TED</div> ](#OM6XIICm_qo \"Tim Berners-Lee: The next Web of open, linked data\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![RDF - Connecting Software and People](https://i.ytimg.com/vi/Nk9TOx1sBUk/sddefault.jpg)</div> </div>### RDF - Connecting Software and People\n    \n    <div class=\"yotu-video-description\">An short 30 minute in depth introduction to the Semantic Web for people with some Software engineering background. Presenting the problem of the Mythical Man Month, this shows how one can use an OWL Ontology, to describe the relation between people and software bugs. Makes the case that the best way to get the Semantic Web going is by opening up a valuable database to a SPARQL end point.   \n     PDF presentation and more available at:  \n     http://blogs.sun.com/roller/page/bblfish/20060323  \n      \n    Sun Microsystems</div> ](#Nk9TOx1sBUk \"RDF - Connecting Software and People\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![What is Linked Data?](https://i.ytimg.com/vi/4x_xzT5eF5Q/hqdefault.jpg)</div> </div>### What is Linked Data?\n    \n    <div class=\"yotu-video-description\">A short non-technical introduction to Linked Data, Google's Knowledge Graph, and Facebook's Open Graph Protocol. If you have any questions, hit me up on Twitter: @manusporny or G+: Manu Sporny</div> ](#4x_xzT5eF5Q \"What is Linked Data?\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![What is an Ontology](https://i.ytimg.com/vi/jfUPLuPL3Ho/hqdefault.jpg)</div> </div>### What is an Ontology\n    \n    <div class=\"yotu-video-description\">Description of an ontology and its benefits. Please contact info@spryinc.com for more information.</div> ](#jfUPLuPL3Ho \"What is an Ontology\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Gov 2.0 Expo 2010:   Tim Berners-Lee, \"Open, Linked Data for a Global Community\"](https://i.ytimg.com/vi/ga1aSJXCFe0/hqdefault.jpg)</div> </div>### Gov 2.0 Expo 2010: Tim Berners-Lee, \"Open, Linked Data for a Global Community\"\n    \n    <div class=\"yotu-video-description\">Tim Berners-Lee (World Wide Web Consortium),   \n    \"Open, Linked Data for a Global Community\"</div> ](#ga1aSJXCFe0 \"Gov 2.0 Expo 2010:   Tim Berners-Lee, \"Open, Linked Data for a Global Community\"\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Chief Google Evangelist, Vinton G. Cerf speaking on The Future of the Internet at Jawahar Bhawan](https://i.ytimg.com/vi/hVTfmMnHhnI/hqdefault.jpg)</div> </div>### Chief Google Evangelist, Vinton G. Cerf speaking on The Future of the Internet at Jawahar Bhawan\n    \n    <div class=\"yotu-video-description\">Chief google Evangelist and VP, Vinton G. Cerf speaking at the Jawahar Bhawan on empowering 1.2 Billion Indians through building infrastructure and future of the internet. Question and Answers moderated by Congress Vice President, Rahul Gandhi.</div> ](#hVTfmMnHhnI \"Chief Google Evangelist, Vinton G. Cerf speaking on The Future of the Internet at Jawahar Bhawan\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Vinton G. Cerf answers questions on the credibility of information on the Internet - Full Video](https://i.ytimg.com/vi/nZS6lKd0vfw/hqdefault.jpg)</div> </div>### Vinton G. Cerf answers questions on the credibility of information on the Internet - Full Video\n    \n    <div class=\"yotu-video-description\">An extremely interesting discussion on Information and misinformation through different mediums. Teaching children to practice critical thinking. Asking questions on the origin on Information and the alignment-realignment of universities 30-40 years from now.</div> ](#nZS6lKd0vfw \"Vinton G. Cerf answers questions on the credibility of information on the Internet - Full Video\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Introducing the Knowledge Graph](https://i.ytimg.com/vi/mmQl6VGvX-c/sddefault.jpg)</div> </div>### Introducing the Knowledge Graph\n    \n    <div class=\"yotu-video-description\">Get an under the hood look at the next frontier in Search, from the team at Google behind the technology. The Knowledge Graph is a huge collection of the people, places and things in the world and how they're connected to one another. With this technology, Google can get you the best possible answers and help jump start your discovery.  \n      \n    Learn more at http://www.google.com/insidesearch/features/search/knowledge.html</div> ](#mmQl6VGvX-c \"Introducing the Knowledge Graph\")\n\n</div><div class=\"yotu-pagination yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">2</span>[Next](#)</div> </div></div>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","title":"Trust Factory 2017"},"frontmatter":{"draft":false},"rawBody":"---\nid: 379\ntitle: 'Trust Factory 2017'\ndate: '2018-09-23T14:09:53+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=379'\npermalink: /trust-factory-2017/\ninline_featured_image:\n    - '0'\ncategories:\n    - Conferences\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min yotu-limit-max   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMQ2JtejBWU1pfdnI5Vlc2Q2pPcXlZVmVkSHdfUWwyZmEiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2U3OTdmYyIsIm5leHQiOiIiLCJwcmV2IjoiIn0=\" data-showdesc=\"on\" data-total=\"1\" data-yotu=\"63ac7c3e93d3f\" id=\"yotuwp-63ac7c3e797fc\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> ABC - TrustFactory - WWW2017 </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3e93d3f\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> Speakers participating in the Trust Factory Event are interviewed for ABC news. </div> </div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![ABC - TrustFactory - WWW2017](https://i.ytimg.com/vi/nsRdAZ7eDww/hqdefault.jpg)</div> </div>### ABC - TrustFactory - WWW2017\n    \n    <div class=\"yotu-video-description\">Speakers participating in the Trust Factory Event are interviewed for ABC news.</div> ](#nsRdAZ7eDww \"ABC - TrustFactory - WWW2017\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![The future of your society, is in the hands and hearts of its people.](https://i.ytimg.com/vi/e9vROTibKiE/sddefault.jpg)</div> </div>### The future of your society, is in the hands and hearts of its people.\n    \n    <div class=\"yotu-video-description\">first published Christmas day 2016 Educational Video about the status of technology development relating to information sciences, computing and artificial intelligence.  \n      \n    This video highlights some of the aspects to the socio-technological development of our species and its interactions with the world. It's important to remember more than a billion humans have no access to electricity, and many more do not have basic computing skills and ubiquitous access, let alone the skills to contribute to this sphere of humanitarian development impacting everything. The % of humanity participating in these works and the underlying debate is a tiny fraction, of a single %, of society world-wide.</div> ](#e9vROTibKiE \"The future of your society, is in the hands and hearts of its people.\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![100 0877](https://i.ytimg.com/vi/RbgtUZ0gkoM/sddefault.jpg)</div> </div>### 100 0877\n    \n    <div class=\"yotu-video-description\"></div> ](#RbgtUZ0gkoM \"100 0877\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![10010878](https://i.ytimg.com/vi/lVZMmQzRivA/sddefault.jpg)</div> </div>### 10010878\n    \n    <div class=\"yotu-video-description\"></div> ](#lVZMmQzRivA \"10010878\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![10020878](https://i.ytimg.com/vi/7sarO4UvN5U/sddefault.jpg)</div> </div>### 10020878\n    \n    <div class=\"yotu-video-description\"></div> ](#7sarO4UvN5U \"10020878\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Trust Factory panel discussions](https://i.ytimg.com/vi/z5qBT_cYO0g/sddefault.jpg)</div> </div>### Trust Factory panel discussions\n    \n    <div class=\"yotu-video-description\"></div> ](#z5qBT_cYO0g \"Trust Factory panel discussions\")\n\n</div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div> </div></div>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","title":"What is Linked Data?"},"frontmatter":{"draft":false},"rawBody":"---\nid: 381\ntitle: 'What is Linked Data?'\ndate: '2018-09-23T14:11:12+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=381'\npermalink: /what-is-linked-data/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Video Library'\n    - 'What is RDF?'\n---\n\n<div class=\"yotu-playlist yotuwp yotu-limit-min yotu-limit-max   yotu-thumb-169  yotu-template-grid\" data-page=\"1\" data-player=\"large\" data-settings=\"eyJ0eXBlIjoicGxheWxpc3QiLCJpZCI6IlBMQ2JtejBWU1pfdnFaZW1rSHB6YjctR1dtemNVZEdnZlAiLCJwYWdpbmF0aW9uIjoib24iLCJwYWdpdHlwZSI6InBhZ2VyIiwiY29sdW1uIjoiMyIsInBlcl9wYWdlIjoiMTIiLCJ0ZW1wbGF0ZSI6ImdyaWQiLCJ0aXRsZSI6Im9uIiwiZGVzY3JpcHRpb24iOiJvbiIsInRodW1icmF0aW8iOiIxNjkiLCJtZXRhIjoib2ZmIiwibWV0YV9kYXRhIjoib2ZmIiwibWV0YV9wb3NpdGlvbiI6Im9mZiIsImRhdGVfZm9ybWF0Ijoib2ZmIiwibWV0YV9hbGlnbiI6Im9mZiIsInN1YnNjcmliZSI6Im9mZiIsImR1cmF0aW9uIjoib2ZmIiwibWV0YV9pY29uIjoib2ZmIiwibmV4dHRleHQiOiIiLCJwcmV2dGV4dCI6IiIsImxvYWRtb3JldGV4dCI6IiIsInBsYXllciI6eyJtb2RlIjoibGFyZ2UiLCJ3aWR0aCI6IjYwMCIsInNjcm9sbGluZyI6IjEwMCIsImF1dG9wbGF5Ijoib2ZmIiwiY29udHJvbHMiOiJvbiIsIm1vZGVzdGJyYW5kaW5nIjoib24iLCJsb29wIjoib2ZmIiwiYXV0b25leHQiOiJvZmYiLCJzaG93aW5mbyI6Im9uIiwicmVsIjoib24iLCJwbGF5aW5nIjoib2ZmIiwicGxheWluZ19kZXNjcmlwdGlvbiI6Im9mZiIsInRodW1ibmFpbHMiOiJvZmYiLCJjY19sb2FkX3BvbGljeSI6IjEiLCJjY19sYW5nX3ByZWYiOiIxIiwiaGwiOiIiLCJpdl9sb2FkX3BvbGljeSI6IjEifSwibGFzdF90YWIiOiJhcGkiLCJ1c2VfYXNfbW9kYWwiOiJvZmYiLCJtb2RhbF9pZCI6Im9mZiIsImxhc3RfdXBkYXRlIjoiMTUzNzY3NTQ1NSIsInN0eWxpbmciOnsicGFnZXJfbGF5b3V0IjoiZGVmYXVsdCIsImJ1dHRvbiI6IjEiLCJidXR0b25fY29sb3IiOiIiLCJidXR0b25fYmdfY29sb3IiOiIiLCJidXR0b25fY29sb3JfaG92ZXIiOiIiLCJidXR0b25fYmdfY29sb3JfaG92ZXIiOiIiLCJ2aWRlb19zdHlsZSI6IiIsInBsYXlpY29uX2NvbG9yIjoiIiwiaG92ZXJfaWNvbiI6IiIsImdhbGxlcnlfYmciOiIifSwiZWZmZWN0cyI6eyJ2aWRlb19ib3giOiIiLCJmbGlwX2VmZmVjdCI6IiJ9LCJnYWxsZXJ5X2lkIjoiNjNhYzdjM2U5NzA2MiIsIm5leHQiOiIiLCJwcmV2IjoiIn0=\" data-showdesc=\"on\" data-total=\"1\" data-yotu=\"63ac7c3eae521\" id=\"yotuwp-63ac7c3e97062\"><div><div class=\"yotu-wrapper-player\" style=\"width:600px\"><div class=\"yotu-playing\"> What is Linked Data? </div><div class=\"yotu-player\"><div class=\"yotu-video-placeholder\" id=\"yotu-player-63ac7c3eae521\"></div> </div><div class=\"yotu-playing-status\"></div><div class=\"yotu-playing-description\"> A short non-technical introduction to Linked Data, Google's Knowledge Graph, and Facebook's Open Graph Protocol. If you have any questions, hit me up on Twitter: @manusporny or G+: Manu Sporny </div> </div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-top\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div><div class=\"yotu-videos yotu-mode-grid yotu-column-3 yotu-player-mode-large\">- [<div class=\"yotu-video-thumb-wrp\"><div> ![What is Linked Data?](https://i.ytimg.com/vi/4x_xzT5eF5Q/hqdefault.jpg)</div> </div>### What is Linked Data?\n    \n    <div class=\"yotu-video-description\">A short non-technical introduction to Linked Data, Google's Knowledge Graph, and Facebook's Open Graph Protocol. If you have any questions, hit me up on Twitter: @manusporny or G+: Manu Sporny</div> ](#4x_xzT5eF5Q \"What is Linked Data?\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Jargon Free Linked Data](https://i.ytimg.com/vi/gd6ccBomspw/sddefault.jpg)</div> </div>### Jargon Free Linked Data\n    \n    <div class=\"yotu-video-description\">My talk with Bernadette Hyland from the 2013 Health Datapalooza in Washington, DC, compressed into a quick 21 minutes.</div> ](#gd6ccBomspw \"Jargon Free Linked Data\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![RDF Explained by Suess and me.mp4](https://i.ytimg.com/vi/VFYLu83WtX0/sddefault.jpg)</div> </div>### RDF Explained by Suess and me.mp4\n    \n    <div class=\"yotu-video-description\">The Resource Description Framework (RDF) as explained by Dr. Suess and me.</div> ](#VFYLu83WtX0 \"RDF Explained by Suess and me.mp4\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![RDF Explained by Suess and me.mp4](https://i.ytimg.com/vi/VFYLu83WtX0/sddefault.jpg)</div> </div>### RDF Explained by Suess and me.mp4\n    \n    <div class=\"yotu-video-description\">The Resource Description Framework (RDF) as explained by Dr. Suess and me.</div> ](#VFYLu83WtX0 \"RDF Explained by Suess and me.mp4\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Linked Data for Libraries, Archives and Museums](https://i.ytimg.com/vi/MnM3tHWAsSA/sddefault.jpg)</div> </div>### Linked Data for Libraries, Archives and Museums\n    \n    <div class=\"yotu-video-description\">New handbook: How to clean, link and publish your metadata  \n      \n    Discover the book at http://book.freeyourmetadata.org/  \n    Order on Amazon: http://www.amazon.com/Linked-Data-Libraries-Archives-Museums/dp/1856049647/</div> ](#MnM3tHWAsSA \"Linked Data for Libraries, Archives and Museums\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Why I LOVE 💖 Linked Data- And Maybe You Should Too](https://i.ytimg.com/vi/0LZHXBxG87k/sddefault.jpg)</div> </div>### Why I LOVE 💖 Linked Data- And Maybe You Should Too\n    \n    <div class=\"yotu-video-description\">Linked Data has a lot of benefits, mapping between entities, sharing of data across dispersed databases, and making ETL way easier if you are using the base standards from W3C. Come check out some of the other things I love about Linked Data, and why you might want to go and check it out for yourself.  \n      \n    Stay in touch:  \n    LinkedIn: https://www.linkedin.com/in/ashleighnfaith/   \n    Direct Message: isadatathing-at-gmail.com  \n      \n    Videos on linked data:  \n    https://youtu.be/l6amUpDMJ4s  \n    https://youtu.be/YCEeoyZkl-U  \n    https://youtu.be/l1q7JGRrTNs  \n    https://youtu.be/yg1Kboe-Y3E  \n    https://youtu.be/4x_xzT5eF5Q  \n      \n    More Resources:  \n    https://www.w3.org/standards/semanticweb/data  \n    https://www.ontotext.com/knowledgehub/fundamentals/linked-data-linked-open-data/  \n    https://medium.com/virtuoso-blog/linked-data-ontologies-and-knowledge-graphs-a3d0ad6d6f66  \n    https://www.researchgate.net/publication/312485719_Enterprise_Knowledge_Graphs_A_Backbone_of_Linked_Enterprise_Data  \n    https://link.springer.com/book/10.1007/978-3-319-45654-6  \n    https://www.wikidata.org/wiki/Wikidata:Main_Page  \n    https://www.nlm.nih.gov/research/umls/index.html  \n    https://lod-cloud.net/</div> ](#0LZHXBxG87k \"Why I LOVE 💖 Linked Data- And Maybe You Should Too\")\n- [<div class=\"yotu-video-thumb-wrp\"><div> ![Linked Open Data, what on earth is that?](https://i.ytimg.com/vi/mMR6JQ1M6qE/sddefault.jpg)</div> </div>### Linked Open Data, what on earth is that?\n    \n    <div class=\"yotu-video-description\">Linked Open Data? Tell me more, what's that? Learn from Winifred Lamb, the Fitzwilliam's Honorary Curator of Antiquities!   \n      \n    An AHRC funded animation for the Linking Islands of Data Project, led by Professor Daniel Pett and Professor Elton Barker (Open University) with the Pelagios Community, Getty Museum, Institute for the Study of the Ancient World, American Numismatics Society, Brown University, School of Advanced Study University of London. The animation was made by Ed Tracy of Too Tall Productions and the voice over by Dr Hannah Platts of Royal Holloway University of London. Photogrammetry 3D models created by Daniel Pett, rendering of Fitzwilliam gallery by Ed Tracy, Digital Periegesis mapping Pausanias application from University of Uppsala and Rainer Simon, numismatics from American Numismatics Society/Nomisma. All 3D models can be found on Sketchfab - https://sketchfab.com/fitzwilliammuseum/collections/gallery-21 and can be downloaded under CC-BY-NC license.  \n      \n    Part 2 on how to make data is here https://youtu.be/0m79yDb4AzE</div> ](#mMR6JQ1M6qE \"Linked Open Data, what on earth is that?\")\n\n</div><div class=\"yotu-pagination yotu-hide yotu-pager_layout-default yotu-pagination-bottom\">[Prev](#)<span class=\"yotu-pagination-current\">1</span> <span>of</span> <span class=\"yotu-pagination-total\">1</span>[Next](#)</div> </div></div>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","title":"Building an Economy based upon Knowledge Equity."},"frontmatter":{"draft":false},"rawBody":"---\nid: 465\ntitle: 'Building an Economy based upon Knowledge Equity.'\ndate: '2018-09-25T14:56:28+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=465'\npermalink: /building-an-economy-based-upon-knowledge-equity/\ninline_featured_image:\n    - '0'\ncategories:\n    - 'Digital Economy'\n    - Economics\n---\n\n<span style=\"font-weight: 400;\">As illustrated by the [OECD Knowledge Based Capital](http://oecd.org/sti/inno/newsourcesofgrowthknowledge-basedcapital.htm) is a well-known form of asset that is used to make-up a significant proportion of the valuations provided to corporations</span><span style=\"font-weight: 400;\">. Whilst the technology tools are now well-established as to support new models; the means through which the economically evaluate relationships between ‘the changing nature of work’ and the needs of those workers to participate in the ‘knowledge economy’ as natural persons; is known by technologists, to require an alternative means of information management that is designed to bring to market an operating model, that can be built upon. </span>\n\n<span style=\"font-weight: 400;\">This in turn builds upon the idea that the role of ‘knowledge based equity’ is becoming increasingly important as the 4th industrial era impacts natural persons.</span>\n\n<span style=\"font-weight: 400;\">The solution put forth is based upon work I have been developing over many years to establish a technically viable means to build a ‘knowledge’ banking platform. </span>\n\n<span style=\"font-weight: 400;\">Over the past 6 or so years, this journey led me to works on international standards required to evolve the technology solutions required to form a workable embodiment.</span>\n\n<span style=\"font-weight: 400;\">Long-term works, known as the ‘semantic web’ were found and it was through the extension of these works that the tooling was able to be evolved. Whilst I have contributed in various ways, it is only due to the similarity of the ideas and commitment made to them by those such as Tim Berners-Lee, inventor of the world-wide-web, and through international collaboration that the technical means to build systems based on technology standards has been achieved. </span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","title":"Social Encryption: An Introduction"},"frontmatter":{"draft":false},"rawBody":"---\nid: 463\ntitle: 'Social Encryption: An Introduction'\ndate: '2018-09-25T14:53:27+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=463'\npermalink: /social-encryption-an-introduction/\ninline_featured_image:\n    - '0'\ncategories:\n    - Presence\n    - 'Social Networks'\n---\n\n<span style=\"font-weight: 400;\">Human beings are far more complex than computers. </span>\n\n<span style=\"font-weight: 400;\">As social-organisms, making use of our real-world environment alongside other senses; we are very different to the things we’ve made, even though we’re now using them to make us ‘better’. </span>\n\n<span style=\"font-weight: 400;\">ICT systems are already ‘smarter’ than us in a number of ways. </span><span style=\"font-weight: 400;\">They’re able to process information in a way that no group of humans could ever achieve in a competitive timeframe. </span>\n\n<span style=\"font-weight: 400;\">ICT networks and sensors continue to make attempts to mimic an array of behaviours humans learn, that humans have, yet humans generally develop these skills over long periods of time and computers that do many new things are still new.</span>\n\n<span style=\"font-weight: 400;\">Amongst the most rudimentary of core assumptions humans make is that we are able to rely upon our capacity to form a shared comprehension of things we consider to be constituents of our ‘reality’.</span>\n\n> <span style=\"font-weight: 400;\">ICT is being used to both enhance and augment these capacities.</span>\n\n<span style=\"font-weight: 400;\">With sufficient evidence that any forum could be considered to share a level of consensus between those involved, Social Graph enabled ICT systems can be used to target specific groups and influence these outcomes. </span>\n\n<span style=\"font-weight: 400;\">The practice of doing so, becomes much easier where there is only one ‘system’; however, the ramifications of ‘data quality’ suffers greatly as a consequence.</span>\n\n<span style=\"font-weight: 400;\">The means through which humans are ‘programmed’, is different to the way online systems are developed – to mimic and support the needs and ‘best interests’ of their operators.</span>\n\n<span style=\"font-weight: 400;\">This is influenced by the socioeconomic frameworks to which Institutions are bound by law to maintain; critical characteristics, that are different to what it means for all of us to be human. </span>\n\n<span style=\"font-weight: 400;\">For example; computers can care less for children, other than as may be computed that they be provided additional stimulus; to warrant more ‘economic attention’.</span>\n\n<span style=\"font-weight: 400;\">By decentralising the web, the means to build social-encryption is considered to be amongst the most important underlying pillars required for socioeconomic growth. The consequence of decentralising data-custodianship, access and discovery for federated queries enabling rendered outputs by dynamic agents; is considered, to be able to dramatically improve data-quality.</span>\n\n<span style=\"font-weight: 400;\">The concept of ‘social encryption’ is about making use of a multitude of networked, yet independently managed, computer systems in a manner that involves a large number of human beings. </span>\n\n[<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/UMNFehJIi0E?rel=0\" width=\"560\"></iframe>](https://en.wikipedia.org/wiki/2012_Summer_Olympics_opening_ceremony) Q: How can we Verifiable the Claim that Tim Berners Lee appear at the 2012 Summer Olympics opening ceremony A: There are tens of thousands of people who were there, tens of millions watching live and now millions of links online, across the web, to improve the means available to verify whether or not this is true. The reality of these facts can only be changed by going through and causing every one of these agents to be made irrelevant, without being noticed for doing so.\n\n> <span style=\"font-weight: 400;\">The more people involved, who are using different, but linked systems; the more device/targets involved, making some forms of attack more difficult. </span>\n\n<span style=\"font-weight: 400;\">A common-property of all participants is time. Our computer systems don’t work very well unless tracking activity in relation to time. By forming the means to produce records that are distributed across a multitude of systems, with a multitude of participants contributing towards a unified informatics environment, it becomes possible to make use of humans to produce ‘social encryption’. </span>\n\n<span style=\"font-weight: 400;\">The impact of doing so is to improve the means to address threats otherwise posed by cryptography applied to singular systems, alongside issues posed by AI technologies; and the means to build a self-preserving, robust information management system that is more difficult to technologically attack.</span>\n\n<span style=\"font-weight: 400;\">Perhaps the most important consideration is how ICT can be used to protect against economic attacks. The problem is not about whether the data exists or not; more often than not, the means to use data to evaluate any situation does indeed exist somewhere. The problem is how we make use of data based on the standardised practices through which we may build infrastructure that performs the function of information management upon an idea of economic merit; build on old technology which now has other options. </span>\n\n<span style=\"font-weight: 400;\">The technology and tools required to make a change already power the largest organisations operating many of the worlds most important ICT systems today.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","title":"About W3C"},"frontmatter":{"draft":false},"rawBody":"---\nid: 510\ntitle: 'About W3C'\ndate: '2018-09-27T22:31:04+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://www.webizen.net.au/?p=510'\npermalink: /about-w3c/\ninline_featured_image:\n    - '0'\ncategories:\n    - standards\n    - W3C\n---\n\n<figure class=\"wp-caption alignnone\" style=\"width: 2681px\">![](https://www.w3.org/2007/09/map/main.jpg)<figcaption class=\"wp-caption-text\">History and Future of WWW</figcaption></figure><span style=\"font-weight: 400;\">W3C was established by Sir Tim Berners-Lee using a set of particular ideological decisions that he made to produce the web we know. He has communicated these decisions, including his future vision of the Web from a standards development, or rather “network theory” perspective at Annenberg Networks Network seminar</span><span style=\"font-weight: 400;\">.</span>\n\n<span style=\"font-weight: 400;\">On the 6th of September last year, Jeff Jaffe replied to a query I had in relation to philosophical concepts pertaining to the growth of W3C and how it is legally structured, with the following;</span>\n\n<span style=\"font-weight: 400;\">“W3C is not a legal entity. W3C staff exists at four research universities (MIT, ERCIM, Keio, and Beihang) and legally it is a series of contracts that binds the universities and member organisation to work together on web standards.” Through this mechanism, the [w3c patent policy](https://www.w3.org/Consortium/Patent-Policy-20170801/) is administered in relation to the role W3C plays, in [internet governance](https://en.wikipedia.org/wiki/Internet_governance).</span>\n\nW3C supports the growth of open-standards through a methodology that elects a particular field of endeavour, resourcing that field to support cooperative / collaborative group works, where technical solutions are designed in the form of ‘web standards’, and eventually overtime, these standards become released in a form supported by the meaningful use of the patent policy ecosystem structure. In more modern times, many of the projects taken-up by W3C start-off in a [Community Group](https://www.w3.org/community/). Overtime the work produced in a Community Group or CG, may develop into a scope of work that’s developed further through an [interest group](https://www.w3.org/Consortium/activities#Interest), and finally a [working group](https://www.w3.org/Consortium/activities#Working) which in-turn results in the publication of standards implemented and provided ‘patent pool support’ by participating [W3C Members](https://www.w3.org/Consortium/Member/List). W3C works are generally carried out online, with [mailing lists](https://lists.w3.org/), [code](http://github.com/w3c/) and even [strategy works](https://github.com/w3c/strategy/projects/2) available online. Increasingly W3 also maintains online resources for [Permanent Identifiers for the Web](https://w3id.org/).\n\n<span style=\"font-weight: 400;\">The role and meaningful utility of W3C has no known equal, within the sphere if its operationally realised purpose as has been developed since first established. Through the leadership (and somewhat onerous role) of Tim Berners-Lee; an array of semi-structured frameworks, backed by the international patent pools that power the web making possible the means to ensure free use of the global standards made by it (interoperable / cooperatively with others) are extensible in ways that have no similar alternative in the field of ICT ‘knowledge engineering’ ‘information management’ systems tooling. Whilst W3 works produce an array of constituent elements, it is principle custodianship of URI based layered technologies, built upon the underlying topological constituents of internet.</span>\n\n<span style=\"font-weight: 400;\">In works that moreover, seek to extend the beneficial development of the world via ICT, a remarkable influence is instrumentally built through the use and adaption of the ‘semantic web’ concept.</span>\n\n<figure class=\"wp-caption alignnone\" style=\"width: 4936px\">![](https://www.w3.org/2001/04/roadmap/all.svg)<figcaption class=\"wp-caption-text\">Circa 2001 – “W3C technology road map. In the end, all W3C activities are in service to the top-level goal of reaching the semantic Web’s full potential. Arrows indicate “how” things are implemented; following them in reverse indicates “why” they exist (or should)”. Source: http://jmvidal.cse.sc.edu/library/w4012.pdf</figcaption></figure><span style=\"font-weight: 400;\">Internet, has reached a level where access to it is debated as a human right, yet the complexities of these arguments extend to the meaningful use of it, beyond simply access to it. </span><span style=\"font-weight: 400;\">The means through which this can now be better addressed makes instrumental use of works produced by W3C. </span>\n\n<span style=\"font-weight: 400;\">The link between internet and economics is now, inextricably linked; and it is worth noting, Internet Society as another key tenant in international internet governance worldwide; and whilst </span><span style=\"font-weight: 400;\">i note, that it is, arguably impossible, to produce anything without some form of inherent, ideological definitions woven into a specification (whether that be done via conscious, subconscious or as a result of unintended decision making processes) W3C and its fellow internet governance constituents are instrumental in the provision of intellectual property rights that entitle others to use ICT technologies without being required to pay financial royalties to companies for use of the meaningful human rights that relate and depend upon, the ability for persons communicate, as is a foundational required by all. </span>\n\nFor more information see the [W3 Website](https://www.w3.org/Consortium/) and related [WikiPedia Article](https://en.wikipedia.org/wiki/World_Wide_Web_Consortium)."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","title":"Link Library"},"frontmatter":{"draft":false},"rawBody":"---\nid: 415\ntitle: 'Link Library'\ndate: '2018-09-25T08:20:25+10:00'\nauthor: ubiquitous\nlayout: revision\nguid: 'https://www.webizen.net.au/414-revision-v1/'\npermalink: '/?p=415'\n---\n\n\n[handong1587 – CV resources](https://handong1587.github.io/computer\\_vision/2015/09/12/cv-resources.html)\n\n[Quantum Algorithm Zoo](http://math.nist.gov/quantum/zoo/)\n\n[RedLink](https://redlink.co/)\n\n[Avalon Media System\\](http://www.avalonmediasystem.org/)\n\n[Mico-Project](http://www.mico-project.eu)\n\n\n[Apache Marmotta](http://marmotta.apache.org/)\n\n[Avalon Media System](http://www.avalonmediasystem.org/)\n[OMEKA](https://omeka.org/)\n\n[MPAT project](http://mpat.eu/)\n[SAMVERA](https://samvera.org/)\n\n\n\\[LDP\\](http://ldp)\n\n[Openlink Virtuoso\\](https://virtuoso.openlinksw.com/)\n\nMedia Platforms\n\n[Helixware](https://helixware.tv/)\n\n[OER PUB](https://oerpub.org/tools/)\n\n\n\\[RWW / Solid Apps\\](http://rww-apps)\n\n[Activity Tracker](https://github.com/linkeddata/solid-fitness)\n[App-set](https://github.com/linkeddata/solid-app-set)\n[Blogging Platform](https://github.com/deiu/solid-plume/)\n[File browser](https://github.com/linkeddata/warp)\n[Markdown Code Editor](https://github.com/melvincarvalho/markdown-editor)\n[HyperDocument Editor](https://github.com/linkeddata/dokieli)\n[Inbox](https://github.com/solid/solid-inbox/)\n[Meeting Scheduler](https://github.com/linkeddata/app-schedule)\n[note pad](https://github.com/timbl/pad)\n[Profile Editor](https://github.com/linkeddata/profile-editor)\n[RWW Contacts App](https://github.com/linkeddata/contacts)\n[Sign-up polyfill](https://github.com/solid/solid-signup)\n[Social Media: timeline](https://github.com/solid-social/timeline)\n[Spreadsheet](https://github.com/linkeddata/spreadsheet)\n[Twitter Tweet Reputation | Fact Checker](https://github.com/factsmission/twee-fi)\\\n[twitter’ like messenging system](https://github.com/linkeddata/cimba)\n\nGitHub\n\n\n[ClownFace](https://github.com/rdf-ext/clownface)\n\n[Food-Dashboard](https://github.com/ouisharelabs/food-dashboard)\n\n\n[Graphite PHP Linked Data Library](https://github.com/cgutteridge/Graphite)\n\n[hypergraphql](https://github.com/semantic-integration/hypergraphql)\n[LevelGraph](https://github.com/mcollina/levelgraph#navigator-api)\n\n[Linked Data Fragments Server](https://github.com/LinkedDataFragments/Server.js)\n\n[neosemantics](https://github.com/jbarrasa/neosemantics)\n[node-quadstore](https://github.com/beautifulinteractions/node-quadstore)\n\n[Ontobee](https://github.com/OntoZoo/ontobee)\n[Open Semantic Framework](https://github.com/structureddynamics/Open-Semantic-Framework-Installer)\n\n[RDF JavaScript Libraries](https://github.com/rdfjs)\n\n[rdf-tests](https://github.com/w3c/rdf-tests)\n\n[RDF2H](https://github.com/rdf2h)\n\n[rdfLib](https://github.com/linkeddata/rdflib.js)\n\n[SPARQL 1.1 parser for JavaScript](https://github.com/RubenVerborgh/SPARQL.js/)\n[static-ldp](https://github.com/trellis-ldp/static-ldp)\n\n[Value Flows](https://github.com/valueflows/valueflows)\n\n\n[Web of Things](https://github.com/w3c/wot)\n\nhealth\n\n[Snomed-CT – AU – National Clinical Terminology Service](https://www.healthterminologies.gov.au/)\\[\n\nMedia\n\n[Helixware](https://helixware.tv/)\n\n\nLearning Resources\n\n[Linked-Data Developer](http://linkeddatadeveloper.com/Projects/Home-Page/index.xhtml?view)\n\nLinked Data\n\nOntology Tools\n\n[DBPedia](https://dbpedia.org/)\n- [Linda](http://linda.epu.ntua.gr/)\n\n- [Linking Open Data – Cloud Diagram](http://lod-cloud.net/)\n- [MMI Ontology Registry and Repository](http://mmisw.org/)\n\n- [OpenLink Structured Data Sniffer](http://osds.openlinksw.com/)\n\n[Protege](http://protege.stanford.edu/)\n\n[Wiki-Data](https://www.wikidata.org/wiki/Wikidata:Main\\_Page)\nSchemaOrg\n\n[RDFaCE](https://wordpress.org/plugins/rdface/)\n\n\nRDF Tools\n\n\\- \\[Apache Jena\\](https://jena.apache.org/)\n\n- \\[Apache: Anything To Triples (any23)\\](https://any23.apache.org/)\n\n- \\[Google: Structured Data Testing Tool\\](https://search.google.com/structured-data/testing-tool)\\[(Edit)\\](https://www.webizen.net.au/wp-admin/post.php?action=edit&post=361) - \\[Json-LD Playground\\](http://json-ld.org/playground/)\n- \\[Ontology Driven RDF Access from Java\\](http://jastor.sourceforge.net/)\n\nOntologies\n\n\\- \\[Getty Vocabularies\\](http://www.getty.edu/research/tools/vocabularies/index.html)\n- \\[Snomed-CT – AU – National Clinical Terminology Service\\](https://www.healthterminologies.gov.au/)\n\n\nWordPressPlugin\n\n\\- \\[LH RDF\\](http://wordpress.org/extend/plugins/lh-rdf/)\n- \\[LH Relationships\\](https://wordpress.org/plugins/lh-relationships/)\n- \\[LH Tools\\](https://wordpress.org/plugins/lh-tools/)\n\n\n- \\[Pool Party Thesaurus\\](https://wordpress.org/plugins/poolparty-thesaurus/)\n\n- \\[Schema App WordPress Plugin\\](https://wordpress.org/plugins/schema-app-structured-data-for-schemaorg/) \n- \\[WordLift\\](http://wordlift.io/)\n- \\[WP Data Cube\\](https://wordpress.org/plugins/wp-data-cube/)\n- \\[WP-LDP\\](https://wordpress.org/plugins/wp-ldp/)\n- \\[WP-Linked-Data\\](https://wordpress.org/plugins/wp-linked-data/) \n "},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","title":"About The Author"},"frontmatter":{"draft":false},"rawBody":"---\nid: 767\ntitle: 'About The Author'\ndate: '2018-10-13T12:26:20+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=767'\ninline_featured_image:\n    - '0'\n---\n\n<div>(document status; draft)</div>I had an interest in computers since he was a child. Born in 1978, from the early 90’s my interest in computer led me to worked as a computer professional from the mid 90s (as a teenager) fixing / building computers, etc.\n\nWhilst i grew up sailing, learning piano and being supported in my interest in computers (grandpa gave me a vic 20, and later bought me my first, 386dlc, which i upgraded) my family were troubled, as is documented in the press.\n\nIt was through my interest in computing, and my talents elsewhere in life (‘finishing school, esk’) and the support of families in my community; that i was provided opportunities that shaped my world. My means to learn of kindness, and to make full use of those lessons in my elected behaviours towards others, responding to them and their needs. As a teenager person, many families were involved in my care and personal development, this was their choice; as did in-turn, provide a wealth of lessons about personhood, to me.\n\nIn 2000, after having sought to build a ‘computer that worked on my TV’, it was suggested to that i think about ‘online data storage’ and was inspired by the tails of a relatives work ([eccles](https://www.nobelprize.org/nobel_prizes/medicine/laureates/1963/eccles-bio.html)) on synapses. Combining those thoughts with other interests in behavioural sciences (family being prominent in pathology &amp; teaching); I wanted to ‘link’ documents rather than sending them.\n\nI could see the potential for an address book entry to automatically update, alongside the broader opportunities brought about. The first documents written about the idea, firstly called it the ‘crescent network’, followed by [iBank](http://sailingdigital.com/iBank.html).\n\nThis led to establishing my first start-up; which ended-up very broken.\n\nThe intersections between life, technology, economics and relationships were clearly described by the information systems that were defined; itself, to be an aggregation orientated solution, despite best attempts to do otherwise.\n\n> the battle to build a better [syphon…](https://simple.wikipedia.org/wiki/Syphon)\n\nI started again; which led to a journey that went on for about a decade. I needed to learn about all the facets involved in my design; and learn enough, to understand how to make a secure solution in the future, when it made sense to do so; whilst most importantly, learning about how to build a resilient leadership framework that worked in concert with our systems of law.\n\nI didn’t know much about consumer electronics MFGs, so i collaborated with a contact in Taiwan. I didn’t know much about data centres &amp; ISPs, so i worked for some of them. I worked on many ideas and start-ups learning as a living. I made best attempts to make my own start-ups, but the results were of those attempts led to new lessons about how broken our “rule of law”, particularly access to justice, really was. (as has overtime, now been so well illustrated)\n\nThe main area of excellence shown by me in my work, has been exhibited by works in the media industry where i had an involvement in some of the first VOD, IPTV, Live Streaming (via broadband satellite) systems; notably including my work on progressing my ‘knowledge banking ‘related work in media technology; whereby after delivering the first IPTV system (over 2006, DSL2) I identified the need to build global [‘hypermedia’ standards](https://www.webizen.net.au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/) for ‘tv’; and found TV-Anytime [CRID](https://tools.ietf.org/html/rfc4078) in early 2007 (aka “freeview”) , identifying the means to make use of it; and got involved (2007/8) in what became [HbbTV](https://www.hbbtv.org/resource-library/) (which is still not an entirely realised vision, despite [its growth](https://www.hbbtv.org/wp-content/uploads/2017/10/02_Paul-Gray_HbbTV-Symposium-2017-Television-Sets-Evolution-Power-Shifts-and-Outlook.pdf)) which in-turn led to an involvement setting-up the [DCI](https://en.wikipedia.org/wiki/Digital_Cinema_Initiatives) cinema industry migration. The sum-total of projects and industries would bring about quite a long-list; whilst noting my research inspired professional history included projects in fields as broad as to have included; medical fields, education, FMCG/Retail, environment, law, telecommunications, SMEs, energy; whilst not being limited to said fields.\n\nPut simply; I have a range of skills that have contributed towards works, locally and internationally led by many incredible people; that relates to ‘solutions architecture’ and is bedded upon a full-stack understanding of technology.\n\n[![](https://banner2.kisspng.com/20180606/qai/kisspng-osi-model-computer-network-conceptual-model-intern-5b1763b48cd6c5.4405539115282595085769.jpg)](https://en.wikipedia.org/wiki/OSI_model)\n\nIf something needs to be done, i can generally figure out how to do it. My views and considerations have been valued by others, globally, for sometime.\n\nWhilst noting, that over the past few years; its had some massive changes. those who have no basis for comprehension have often made really bad assumptions; and have then ‘run away’, which is in-part, how my formulation of ‘social encryption’ came about; not really comprehending the useful purpose of it in other fields across society as a whole. The problem often is, when contributing towards large-scale projects (without large-scale investment) is that it takes a number of years for things to migrate from an idea, to a ubiquitous outcome. No one person ‘invents’ anything wholly, and there are a large amount of [sheeple](https://en.wikipedia.org/wiki/Sheeple), which is known globally by those who actually do the work; to be a big problem; as it is the core resource leveraged by the [attention economy](https://en.wikipedia.org/wiki/Attention_economy), towards ends that are known to be bad.\n\n> If people want to be sheeple, so long as its their choice that’s ok by me. But if they’re engaging in acts that deny that choice being available to others, that’s a different situation, as it becomes something that can be used to wilfully harm [freedom of thought](https://en.wikipedia.org/wiki/Freedom_of_thought).\n\n###### The escalation of acceptable rates, of wrong-doings.\n\nI’d known of, and had accepted disagreeably in past; mistakes made by others that i knew to be wrong, but felt disempowered to do much more than i did at the time, about it. We are all human, no learning is achieved without making mistakes; and we can’t go back in time; we can only change what we do in the future, as is made possible through our journey of discovery and learning.\n\nI knew the issues i’d experienced, were minor when being considerate of those others lived. I’d worked on an array of projects responding to the challenges of how man impacts our environment. The ability to put my mind to the challenge of improving the way societal systems worked being an easy extension to my computer ‘systems architecture’ thinking; whilst learning about politics in the process of so doing. One was an environmental focused project, which progressed during my time helping it, another (of many) was within the field of [fuel-cells](https://www.horizonfuelcell.com/); which i felt, could be instrumental in shifting energy economics.\n\nYet, it was through the [education project](https://www.youtube.com/watch?v=e9TRCVWURMk&list=PLCbmz0VSZ_vp1ySFW8fMg1DNCdDne249U) that aimed to produce ‘community hubs’, that i’d spent alot of time helping the hewett brothers improve; whose [story of hardship](https://www.smh.com.au/opinion/two-decades-of-battling-bureaucracy-bears-fruit-20040512-gdiwmr.html) was then so foreign to me, that I had learned about what it takes to bring about radical change. Whilst still naive, one set of points that stood out; whilst transferring their historical works illustrating their journey; that i watched [Bill Kelty](https://en.wikipedia.org/wiki/Bill_Kelty) talk of their earlier work, in the following way…\n\n<div></div><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/6UgpxUMrhOk?rel=0&start=615\" width=\"560\"></iframe>\n\nThe problem for so many projects, was that the income / revenue models for establishing innovation projects; didn’t really exist. It was hard to figure out how to form a funding model for something that was intended to do good, without forming some sort of ‘rent seeking’ framework; as was operated by others. The means to form ‘industry transition pathways’ became clear; yet as my experiences taught me in other industries, this was far from easy to do.\n\nPeople may know what they’re doing isn’t the best thing they can do for others, for the environment, for the things that really matter; but what matters most to them, is their ability to economically participate and live with the safety only a job, only income, is able to provide.\n\nI had also found this to be different to the issues and the way they were responded to, internationally. Whilst working in the UAE where i provided valued assistance towards building a modern culture in a new city, where the means to provide the resources for an emergent modern society was prioritise in ways that were required to conform to local customs, whilst various forms of outreach and experimentation were encouraged;\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/B2-hw1-ZIeA?rel=0&start=615\" width=\"560\"></iframe>\n\nI learned the distinction between our local problems (and considerations about the relationship they may have to the treatment of our indigenous people); in dubai, a person with a good idea was provided the help they needed to deliver it. In Australia, a good idea was taken by others to deliver it in the way they believed was the best way to shape that idea for their needs.\n\nThis in-turn was a culturally identifiable characteristic, and problem. My work, on a knowledge banking system, offered a means to shift things; and i started to think about how that could be done by defining plans that linked international works in dubai; with ideas for how to produce an indigenous cultural framework that provided ownership and economic benefit for them, through the means to apply works by linking their heritage with electronic products and services (ie: a game console title) that could change the way our tourism industry worked; change the things kids, wanted to see here,\n\n<iframe height=\"370\" loading=\"lazy\" src=\"https://drive.google.com/file/d/1rKMzkKf6Xf6TUqPw0S9PurFMtrA_na0f/preview\" width=\"640\"></iframe>\n\nthat could change the way we viewed our nation, its people, and its history. To [improve the knowledge Australians have, about Australians.](https://www.youtube.com/user/Guneriboi/videos)\n\n##### Events in time: that change everything\n\n> *[Primum non nocere](https://en.wikipedia.org/wiki/Primum_non_nocere):* People throughout their lives will have things that happen, that change everything. They’re most often, deeply personal and we don’t, go into full details online; which in-turn, has the effect, of shaping artificial intelligence and the information available for people to make use of in their means to make judgements about how it is they want to interact with us.\n> \n> Whilst seminal agreements on human rights (ie: [UDHR](http://www.un.org/en/documents/udhr/)) are premised upon the needs of humanity, to form language, that universally support our means to live in communities that enshrine a persons right to ‘dignity’; it is simultaneously the case, that as organic, conscious beings,\n> \n> “<span style=\"font-size: 1.125rem;\"><span style=\"font-size: 1.125rem;\">the distinction between reality and our knowledge of reality, between reality and information, cannot be made” [Anton Zeilinger](https://www.nature.com/articles/438743a)</span></span>\n\n###### Circumstantial\n\n> *When fate slapped me,* <span style=\"color: #333333; font-size: 1rem;\">circa 2009-10, i was involved in transitioning the cinema industry to digital projection systems. This was frightening for projectionists, who often were unable to use computer systems; where they were required to know how to fix them; whilst without them, cinemas were left unclean; and an opportunity / risk for the industry, if the transition was exploited due to poor management. These sorts of ‘industry transition’ problems were not unusual; and a core part of what it was i did in projects over my career, learning. Everything sounds simple to begin with, expertise and the means to be an expert at anything, takes time. More time than many consider, and more time than those who seek to economically exploit situations are willing to consider or be persuaded to do otherwise, unless law, is meaningful.</span>\n\n<div>It was not a choice those affected by the changes were able to make; this had been the same for doctors who had been required to use internet; and the television, video hire, music and print industry, who were forced to think about broadband; and whilst we ensured our Australian Media industry had the best chance for growth and support of creative people throughout our nation; I was taught some unforgettable lessons that changed my life and my priorities.</div>I had wanted to wait, until the tooling had been made internationally; but my priorities changed as the sense of urgency was so horrifically demonstrated to me; But, <span style=\"font-size: 1rem;\">in 2010 i learned that the compromises </span>*exacted*<span style=\"font-size: 1rem;\"> in business to exploit people for profit; extended to the wilful abuse of the human rights of infant children by employed persons, negligently carrying out harmful acts with impunity. </span>\n\n##### What disgust looks like…\n\n<span style=\"font-size: 1rem;\">I was institutionally enforced that i be taught by low-paid, predominately middle aged women illustrating who simultaneously engaged me in their complaints about their own relationships in their own personal lives; how our system of government provided them power by way of their jobs, that were organisationally orchestrated to game our system of government, and the principles of [our rule of law](https://www.ruleoflaw.org.au/principles/), for their own economic means as they knowingly scarified the human rights of vulnerable children. These people, desperately in need of mental healthcare themselves; were shown to forcefully and systematically, exact upon others the circumstances of their </span>existence<span style=\"font-size: 1rem;\"> and behaviours, as was endorsed by information policies that limited factual information provided to/by government authorities to persons/courts; as be reasonably employed to exact a circumstance nation-wide that was reprehensibly intolerable. Their game was to manipulate the circumstances lived by children prior to any means to engage our system of law, as to [usurp](https://en.wikipedia.org/wiki/Usurper) [natural justice](https://en.wikipedia.org/wiki/Natural_justice) upon a basis, that none of them would be made accountable; and that irrespective of the changes they brought about in so doing; any person who truely cares for any child, cannot do anything about what they do; as they had organisationally been made all powerful, and any attempt to change their directives as actors provided industrial scale impunity; would only further harm the child and any [attachment relationships](https://en.wikipedia.org/wiki/Attachment_theory) left for the child, as a direct result of their negligent and horrific economic attacks upon them.</span>\n\nThe best way to avoid harm, was to stop participating, to stop providing them the core resource they needed to add numbers to their continual desire for more budget; and to be responsible in any acts as to promote the otherwise [internationally recognised rights of children](https://www.ohchr.org/en/professionalinterest/pages/crc.aspx); notwithstanding the failure of others to adhere to; and/or promote the same; and the way they monetised the use of the resources made available to them whilst knowingly doing otherwise as professionals.\n\n> <div>*I’d had enough.*</div>\n\n<div>Notwithstanding the severe nature of mental illness exhibited by the all too many; they hadn’t been encouraged to seek treatment, rather; they commercialised their malfeasance enabled by it; to knowingly harm kids and claim innocence &amp; impunity, for the instrumental role played by them as adults who as experts had every responsibility to do something about it; they failed to do so, and continued to take the money and turn up to work every day to continue to perpetuate the organisationally known failures upon children, systemically, profitably, and with an ever increasing sense of entitlement.</div><div></div><div>Whilst such forms of acts are not simply [biblically referred to](https://en.wikipedia.org/wiki/Massacre_of_the_Innocents), my Australian Government by 2010 had failed to consider meritoriously by law, [the rights of the child](https://www.ohchr.org/en/professionalinterest/pages/crc.aspx).and it became the fact that my role was instrumental to changing law.</div><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/CFqkxrYYH1A?rel=0\" width=\"560\"></iframe>\n\n<div>Clip Source: [Oranges and sunshine](https://www.imdb.com/title/tt1438216/)</div><div></div>As an Australian; the economic history of exploiting the human rights of children including infants, had been made horrifically clear to me. They did not teach the truth to us as children, they still do not do so when educating young boys; and these morally reprehensible and unconscionable acts are not only endorsed and organisationally supported, but the statement was in-effect made – ***what are you going to do about it***.\n\n> *As though, those in their low-paid jobs, abusing children, were all powerful. As members of an industry with a shared shameful secret that would be defended by our law-enforcement, intelligence, public service and parliamentarians; at all costs…*\n\nThe manner in which professionals abuse the rights of infants, with impunity, where if morally prosecuted is sought to be made acceptable by a few tears; as adult, professionals who had knowingly engaged in ‘gainful employment’, build upon an economic model built upon their ability to exploit children, for personal profit; was more than i could take whilst living and breathing.\n\nThe fact is that by targeting children, the economic drivers brought about to support jobs that perpetuate harms against children are extremely valuable to those who care not at all about anything to do with human rights. Human Rights, the rights of the child; was made to be irrelevant, cruelty to children was at that time – entirely legal, or so it was said at that time in Australia.\n\nYet i knew, this was not something i should take personally; it was a big problem, and the reality was there is no parliamentarian who would seek to be elected upon a basis that they promote, support or finance, child-abuse.\n\n###### *The Challenge made: What are you going go to do about it?*\n\nI’d worked in comm’s, I knew full well how much data existed. This situation was entirely unacceptable. I knew, there was no leader who could stand-up to their electorate and advise those who vote that abusing children, is ok. There was no point whatsoever in targeting the behaviour of any one operator, it was an institutionalised problem that was considered acceptable by the entire sector. They could be pressed to acknowledge it, and show shame, but did nothing about it; even though, they considered themselves, industry experts.\n\nThe information systems were designed to support it. Alternatives technically existed; to ensure, the ‘proper purpose’ was served well by technology, but they had not chosen to deliver this outcome; the business systems to do so, did not exist. The problem was so bad; that not even the statistics existed to warrant the means for academic and mental health researchers to study the problem or even acknowledge its existence on a basis of evidentiary facts.\n\n> *<span style=\"font-size: 1rem;\">This is still the case today. Indeed also, critical research within the fields of clinical psychology, sociology and health are still ‘to do’ list items. They do not know the life-cycle cost caused to kids, by adults.</span>*\n\n<div>> *They do not know how the repercussive effects impact relationships, health and the ability for children to grow &amp; thrive to fullest potential.*\n\n</div><div></div><div>> *If children, as adults, seek to have the matters addressed, they do not have clinical studies that advise others how to help them do so.*\n\n</div><div></div><div>*Historically, it was the case that many aboriginal women were raped. Whilst a fight by Australia’s Indigenous People, notably including the [tent embassy](https://en.wikipedia.org/wiki/Aboriginal_Tent_Embassy), led to changes where they were later deemed to be ‘humans’ rather than ‘fauna’ by law; and the [Australian Government later apologised noting the stolen generation](https://en.wikipedia.org/wiki/National_Sorry_Day), the problem is still left that many of these women have not taken the matter of their being raped to court. The problem for many, is that they were deemed by law to have been animals at the time, so the law relating to their being raped, given their race, it has been reported to me; to only be punishable by laws relating to [bestiality.](https://en.wikipedia.org/wiki/Legality_of_bestiality_by_country_or_territory#Commonwealth_of_Australia) The advancement of our society has changed rapidly over a short period of time throughs the use of technology; whilst our moral choices &amp; means, are poorly served.*</div>> “*The fact that there aren’t pressures and costs does not absolve people of their <span class=\"lG\">moral</span> responsibility. The primary custodian of one’s actions is oneself.*” April 2018 Noam Chomsky via email, to me.\n\n<div></div><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/Kr2K8mo-A5g?rel=0&start=4898\" width=\"560\"></iframe>\n\nThe climate; as was defined by our information systems in 2010 was designed to facilitate the means to support those employed by large organisations, to act with impunity had – extended to such an extent; that the biblically reprehensible acts known by all too many, had been profitably industrialised.\n\nThe country, our GDP, our means to attend to any issue was made poorer for it; It was shown to not be a concern worthy of doing anything about by those taking home a pay-check for perpetuating wrongs; as is all too easily considered by others, to denote success by comparison to those who live more poorly, whilst doing otherwise. The raw evaluation, money matters most.\n\nYet I emphatically disagreed. Made poorer for it, but depends on the modelling used to form any such form of assessment.\n\nI formed the fixed and unapologetic view, that the situation had to change.\n\nThat the manner through which the human rights of children were being commercially exploited for economic gain, was reprehensible; and whilst broader social-shifts started to occur, i started building international apparatus to show them exactly what i was going to do about it.\n\nThere’s 7.5Bn people on the planet, only ~25M in Australia; they question I decided to drive home was\n\n<div>What are they going to do about the situation they’ve made for themselves, when the use of technology brings forth an era, in the interests of our nation, via the global platform of vested interests internationally; that can and should be made, to forces them to tell the truth and be recognised for what it is exactly, they done.</div><div></div><div>Maybe their assessment of hierarchy and success will be then, contextualised; and given what they’ve done, what else can i do…</div>Whether this analysis is made by international supercomputers linked together via foreign jurisdictions powering Artificial Intelligence to assess their ‘worthiness’ and the vectors of their choices that apply to that; or whether, humility is brought about, and we take charge of our circumstances and power our GDP in a direction that could radically benefit our nation, our people, and our means to help others internationally. Putting a stop to the wholesale abuse of children is a fairly straight forward matter; it was time to do that.\n\n##### Tooling as webizen; defining the future as it occurred.\n\nMy focus became building the ‘knowledge banking’ platform systems to ensure any one engaging in acts that exploit and harm children, including infants; as a means to help them get paid that day on the basis that they are made able to do so in a void that is considered to be protected with no data, no ‘verifiable information’ that could otherwise be presented to a court of law; that this, in the interests of all citizens and their children; was going to change.\n\nSoon thereafter; it was the case that a commission from the governor general to teach kids about our ‘system of democracy’ walked in the door.\n\n<div></div><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/6lasZCwYijU?rel=0\" width=\"560\"></iframe>\n\nWhilst i went about working on the global standards required to ensure a royalty free, morally consistent means for life to manifest; through the means to improve the manifest use of our information systems, our government started deploying new departments that sought to do the same.\n\n<div></div><div>![](http://emilysquotes.com/wp-content/uploads/2014/01/EmilysQuotes.Com-world-dangerous-people-evil-ignorance-Threat-Albert-Einstein.jpg)</div><div></div>We have a system of democracy that promotes our means to live by ‘rule of law’, that is the purpose of those who truely seek to serve to protect us all. As it is the case that even the [charter of the commonwealth](http://thecommonwealth.org/our-charter) brought about an event where pen was put to paper (at an event where i organised for a constituent of the AusCivics materials, [the Australian Way](https://vimeo.com/30416090), to be played publicly); service has broadly been rendered to discretely denounce the acts of these truely poverty stricken, small minded relatively unskilled people known formerly to be busy, weaponising children for their personal profit.\n\n<div>> They can be named, but i’m not going to do that.\n\n</div><div></div><div>> I’ll build the systems needed to ensure others are able to do so fairly, honestly and accurately, in the best interests of society and moreover; the children for whom we are responsible.\n\n</div><div></div><div>> As it has been an ‘ecosystem’ problem; I believe, the same people will end-up on lists made by others, without me doing anything more than building a better information management system, for humanity.\n\n</div>Our technology, our information systems, must be built to support our needs.\n\n##### my Considerations in 2018: the journey continues\n\nThe means to make use of our technology to support humanity is more important than ever. If we build systems that should not be trusted to provide [knowledge](https://en.wikipedia.org/wiki/Knowledge) as to form opinions based on the full nature of facts; then the decisions we make as a consequence will exhibit distortions between the values we say we have, as individuals and as a community, vs. those that are shown by history to have been truthfully accurate.\n\nThis real-world representation of reality; and its distortions, made intentionally by the unconscionable conduct of those made able to benefit from it; influences our economy, our society and our means to manage our natural world unsustainable.\n\n> Who, in their right mind, wants to pay someone with mental health issues that lead them to employing those funds to knowingly abuse children as a means to assist their business (non-profit or otherwise) to retain and improve their economically rationalised business systems.\n> \n> If knowledge, as informatics, is put in the hands of people; whose going to stop the otherwise very complex statistical modelling from being done. Did they think their government would pass a law to prevent ‘freedom of thought’, did they think, as an industry, they’d be made able to demand the legalisation of ‘[fake news](https://docs.google.com/document/d/1OPghC4ra6QLhaHhW8QvPJRMKGEXT7KaZtG_7s5-UQrw/edit#)‘ to benefit them…\n\nIt’s time we ensure the [information management](http://info.cern.ch/Proposal.html) systems we use to power our democracy; eradicates these practices, and that means it’ll bring to bare responsibility upon those whose lives have economically benefited, in contrast to the lives of others who they’ve involved themselves with; due to their role and the way they were made able, to wilful abuse of children in communities.\n\nIt is now the case, some years later, that the people who are most driving change are those who (i’ve been working with freely) invented internet, world-wide-web, many of the technologies used internationally to support intelligence activities world-wide; and frankly, the game is largely over.\n\nWebizen; as a website, pulls together the resource produced by me alongside the resources available online, in book-stores, and those that can be further researched in a multitude of ways; to show, how the global standards are now in the process of being re-engineered; and now, its all about the ethics game, and the way others are going to act as to position the Australian Economy, in an [international knowledge based capital](http://www.oecd.org/sti/inno/newsourcesofgrowthknowledge-basedcapital.htm) centred, global economy; and the way the wilful behaviour of persons will influence, the future for all Australians.\n\nThe Information management systems we produce to participate in this global shift; should help every generations of our humanity live in a better world, now and into the future. Through the efforts of adults, to ensure the problems of adults are not made to be the problems of vulnerable children, we will define whether and how ‘fake news’ and the scores of other problems that are otherwise able to be addressed, using advanced ‘linked-data’ systems.\n\nThese systems are now fully disclosed on this site as to enable any capable actor the means to make them. It doesn’t matter if you want to process [audio](https://www.webizen.net.au/media-analysis-part-1-audio/), [video,](https://www.webizen.net.au/media-analysis-part-2-visual/) [text](https://www.webizen.net.au/basic-media-analysis-part-3-text-metadata/) or anything else; were able to domesticate the [pervasive surveillance](https://www.webizen.net.au/about/the-vision/a-technical-vision/) used in relation to our lives by others anyway; to do far more as adult members of society, in defining who are the ‘good guys’ and who is bad.\n\n*The [Knowledge Banking Architectural framework](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/) is designed to improve our economic ecology off the back of a reliable, social graph. If citizens need to take a matter to court, they should be able to make use of all the data, to do so.*\n\nAkin to the development of libraries and schools in the past; the means for educating persons is based upon their ability to garnish knowledge. This means, telling the [truth](https://en.wikipedia.org/wiki/Truth) is important. Their means to distinguish ‘fact from fiction’, is important to the development of their [nervous system](https://en.wikipedia.org/wiki/Development_of_the_nervous_system) as to define minds most capable fo finding true answers, to real-world issues, overtime.\n\n> When the everyone was convinced the world was flat, the person who first highlighted their thoughts of the world being round, and revolving around the sun; was likely not considered to be communicating a very popular idea.\n> \n> It is surely the case, that those selling tickets to the edge of the world would have been particularly displeased with this, now broadly accepted, point of view. Question is, where would we be without it…\n\n<div></div><div>For children; it is seemingly important for them, to be encouraged to think about solutions based on a true embodiment of ‘[situational awareness](https://en.wikipedia.org/wiki/Situation_awareness)‘; as those provided the best opportunities, will in future become our means, to attend to widespread global issues. This is not to say, they should be made to carry the burden in any way; but that is in-fact what happens, when we fail.</div><div></div><div>Today, we are building our world upon a basis of economic misuse of information management systems. We’re not employing technology, to solve real-world issues; we know this, and its really isolating to do anything about it.</div><div></div><div>This doesn’t need to be the case. We can all do more to change the narrative.</div><div></div><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/3DuampumYoc?rel=0\" width=\"560\"></iframe>\n\n<div>Source: http://survivingprogress.com/</div><div></div>##### Addressing the Sociological Challenges\n\nPeople are made to feel isolated, powerless and frighted to speak out.\n\nStatistics and other relevent verifiable data required to highlight real-world issues is all too often kept private or made available in unusable formats. The means to make use of ‘life data’ that is otherwise seperately stored by different specialised custodians, as to be made useless; is entirely possible and now those realities are on the global agenda by its leaders.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/kW6e1GCpqpE?rel=0&start=17916\" width=\"560\"></iframe>\n\n<div>(You’ll see me in that list shown at ~7:37:50, whilst, its kinda irrelevant.)</div><div></div>The means to make use of people, and the creative ideas they have; that groups of people use to solve real-world-issues issues’, is damaged as to nullify the benefits that by older texts, suggest should be provided to that person.\n\n<div><figure class=\"wp-caption alignnone\" style=\"width: 390px\">[![](https://infostory.files.wordpress.com/2014/09/copyright_clause.jpg)](https://en.wikipedia.org/wiki/Copyright_Clause)<figcaption class=\"wp-caption-text\">The Copyright Clause describes an enumerated power listed in the United States Constitution (Article I, Section 8, Clause 8)</figcaption></figure></div><div></div>*By the modern re:design of our economic systems, that are unintentionally causing harm, our means to address inexorable problems will change radically.*\n\n##### Reality Check &amp; the delivery of ideas, Facts &amp; resources\n\nAn International Knowledge Banking Industry has been the subject of design by me, over two decades, to solve these problems. The work has identified many who lead the world in different spheres to have similar ideas; which in-turn, brings forth a form of movement that is likely to rapidly change the world. I have had the good fortune to help them, build better solutions for humanity. This means, i’ve had the opportunity to be heard and contribute towards the works led by many global leaders, who are now delivering outcomes that are built upon the works of the few; in communities that i’ve been involved with, for quite sometime now. The ramifications for having actually done the work; is far more reliable than any bet, on any person, who said falsely that they have done so. This has been a journey of discovery and iterative testing across a multitude of fields; and it has been my journey.\n\n[Fiction](https://en.wikipedia.org/wiki/Fiction) may help us dream of solutions that are distinct to our experiences today; but it is the act of our minds and the extension of that via communications, that brings about the means to define real-world solutions.\n\nThis site attempts to ‘open-source’, like a ‘hypermedia book’, an embodiment of the works that lead to the real-world commercial solutions that mean we can change the fundamental nature through which all decisions are made. The field of study is not unlike others such as biomedical or nuclear research; the means to make technology does not in itself specify how it should be used.\n\nThe means to define your world, is really up to you. Yet, akin to the beauty of any art, the fact remains that if you try, you might find you get what you need.\n\n<div></div><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/oqMl5CRoFdk?rel=0\" width=\"560\"></iframe>\n\nWe need our economic systems to be built in a manner that reflects our moral values. This is not occurring due to the design of our information management systems. I’ve had the pleasure of being involved in defining, with those such as [microsoft](https://www.microsoft.com/en-us/microsoft-365/blog/2018/02/12/decentralized-digital-identities-and-blockchain-the-future-as-we-see-it/), and [many others](https://www.w3.org/2004/01/pp-impl/73816/status), how to deliver upon global [sustainable development goals](https://sustainabledevelopment.un.org/).\n\n> I hope the reason why i’ve done that, has now been made very clear.\n\n<div></div><div>I also hope, that [dignity](https://en.wikipedia.org/wiki/Dignity) be better equipped overtime than the apparent ‘misuse’ of [privacy](http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm) work done in past; as to suggest our means of personhood and the human rights considerations able to be made in utility of it; do not slide down a slippery slope of simply denoting humanity to be more simply considered through economic lens as [consumers](https://unctad.org/en/Pages/DITC/CompetitionLaw/UN-Guidelines-on-Consumer-Protection.aspx); of the outputs made available by artificial beings made by [homo sapiens](https://en.wikipedia.org/wiki/Homo_sapiens) (which means ‘wise man’ in latin) whether they be those defined by law as a means to rationalise a group of humans; or those made in [cyberspace](https://en.wikipedia.org/wiki/Cyberspace), that can take away all means for any decision to be made on the basis of reliable evidence.</div><div></div><div>It’s up to you, to get involved enough, to form value for the reasons why making change, is so very important. If you don’t value those reasons, neither will anyone else.</div><div></div><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/3vAvAo-pO2w?rel=0&start=4898\" width=\"560\"></iframe>\n\nAt the end of the day; the means to do amazing things, is not about selfishness in anyway; indeed its known to be quite costly to them personally, all too often.\n\nIn my journey, i’ve found this to be true for the members of our armed forces and law enforcement professionals; for our local community leaders and those who lead the world.\n\nIt is seemingly true, for anyone else who puts themselves in harms way as is brought about by the act of seeking change; as is done by those who stand for the principles, that the health of our communities, that of its people and the environments of our natural world depends upon; as to ensure people are not enslaved to the profiteers who care less for others; that the acts of those later defined by history to have been leaders has all too often been a difficult and onerous journey, filled with mistakes and the on-going challenge to fix them.\n\nToday, there is a movement world-wide, brought about by the insecurity privacy provides wrong-doers, protecting them, from accountability; to build better systems, to build electronic knowledge management systems; that will improve the way artificial intelligence is able to be made useful for humanity.\n\n<div></div><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/e9vROTibKiE?rel=0\" width=\"560\"></iframe>\n\nThe only way to solve the problems my lived experience has demonstrated to me; has been to freely and openly produce, publish and make available all the resources needed for others, to change the world, to the best of my ability.\n\nParts of these works make use of ‘[social encryption](https://www.webizen.net.au/social-encryption-an-introduction/)‘, yet, intrinsically, this is, in-turn, a sociologically refined informatics tool. A form of apparatus, tailored in the approach through which it’s been made use of, over the many years; to form a comprehensive global framework of tooling that has everything it now needs. Simple fact is, not everyone needs to know everything; yet, If you know how to find me; and think of something i’ve missed, let me know.\n\nAs i think i’ve said previously; the purpose of this site, is to ensure those who are willing and able to engage in what is likely to be amongst the most valuable fields of economic development, growth and expertise, as a core foundation to the [4th industrial revolution](https://en.wikipedia.org/wiki/Fourth_Industrial_Revolution); that this, should help, alongside the other resources available globally – to independently go about doing related work, that can help others. Whilst the site contains alot of my works, as republished assets (and i’ve not provided the ‘[priority dates](https://en.wikipedia.org/wiki/Priority_right)‘ purposefully); for the most-part, its more about the resources available; and my means to help people think about alternatives and how it is they can be made to work for them."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","title":"Applied Theory: Applications for a Human Centric Web"},"frontmatter":{"draft":false},"rawBody":"---\nid: 567\ntitle: 'Applied Theory: Applications for a Human Centric Web'\ndate: '2018-10-02T16:39:22+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=567'\ninline_featured_image:\n    - '0'\n---\n\nOver the past several years, an array of different use-cases / applications have been explored as to be considered in association to the use of the aforementioned ‘knowledge banking platform’. This section of the document provides some insights into this more extensive library of works."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","title":"Executive Summary"},"frontmatter":{"draft":false},"rawBody":"---\nid: 457\ntitle: 'Executive Summary'\ndate: '2018-09-25T14:31:25+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=457'\ninline_featured_image:\n    - '0'\nsharing_disabled:\n    - '1'\n---\n\nThis website, including this document is about how the world is going to change. Moreover, its about an array of considerations influencing how ICT may be positively influenced, to bring about change.\n\n> The means to build an economically viable commercial model is not the first question any seasoned executive should be asking about this idea.\n\nWhilst I have covered business systems / modelling related concepts elsewhere; this, is a hard thing to do; and the underlying question any executive should be asking, is why is it important to do so. It is important that any one *foolish enough* to have a *jolly good crack* at it, do so as an educated and informed investor; of their time, of their effort or of any other resource.\n\n> This proposal, this site, is not about a ‘widget’, its about the future of humanity, the future of world-economics and the way in which AI will be made good. How we can built controls for a knowledge economy.\n\nWithout going into the depths of history as does relate to [ICT and global governance](https://www.webizen.net.au/about/history/history-global-governance-ict-1/); Since the 1930s humanity has developed an array of information and communications technologies that sought to make radical improvements to the means of humanity to flourish and over recent decades, the impact of these works have radically accelerated.\n\nPersonal Safety, Wealth and Opportunity is now inextricably linked to the use of ICT both with respect to all individual persons as independent entities; but moreover, as is made to relate to them by way of 3rd parties and the decisions that are brought about in conjunction with the use of ICT operating with defined parameters.\n\nSince 2000, i’ve been working on how to bring about a means for natural persons, to store and make use of ICT in a way allows them to ‘own their data’ and bring forth new economic models for the global knowledge economy that are thought to be capable of amazing things.\n\n> My original ideas have not simply developed, but have also found to be supported by similar ideas that have been developed internationally since those early days. its been quite a journey…\n\nThe use of ICT and the dependency upon ICT has exploded over the last 20 years. Now, with modern tools, the means to form a proposed ‘information management’ structure, through which; natural actors may be provided AI tools that are rendered as a prosthesis to the information sphere, through which every human is now required to socioeconomically depend upon.\n\n> The same concept put another way; is that the way data is recorded in relation to us, forms impactful changes upon sense of consciousness, our health and general wellbeing.\n\nThe principles through which the ideas for a ‘human centric web’ has be forged, takes into consideration these forms of sociological considerations.\n\n> Whilst the direct economic considerations are made elsewhere in this site, i’m prioritising in this document; the instrumental purpose and implications of advanced technology and how responsibility relates to the use of these works.\n\nIn a very broad form, as i’ve endeavoured to make meaningful contributions towards broader global efforts to establish a new revision of the ‘[information management](http://info.cern.ch/Proposal.html)’ (and the evolution thereof) systems employed; as to solve an emergent ‘human identity’ problem that is otherwise unable to be solved, I’ve formed this outline (and related works provided on the site) of what I am talking about, when i highlight the opportunity to build a “knowledge banking industry”. The concept of corporations economically &amp; beneficially resourcing value based upon [knowledge based assets](http://www.oecd.org/sti/inno/newsourcesofgrowthknowledge-basedcapital.htm) but this does not extend well to humans, not as individuals, consequentially distortedly impacting economics.\n\n> People ain’t able to prove anything if they have no available evidence.\n> \n> This principle does not only apply to individual humans, but also nations. How do you know what happened, how do you know how to vote; if your use of data is made irrelevant by those operating you.\n\nThe solution framework defines a temporally traversable, non-linear, ICT employed notional environment (“[cyberspace](https://en.wikipedia.org/wiki/Cyberspace)”) that is designed to be used as an extension of natural [personhood](https://en.wikipedia.org/wiki/Personhood).\n\nIt achieves this, by making use of machine-readable structured data standards that provide the means for an online service operated in a manner that is similar to a financial bank-account; to manage all the data produced in relation to a persons ‘things’ by the person directly or indirectly; in addition to supporting the storage and access of information created by others in a manner that can be referred to in a machine-readable format, as to be ‘linked together’. These tools have been made and are being used to power the web today. It is the case that some new tools did need to be made over the past few years, whilst moreover, the bulk of the tooling required is now free to use.\n\nIn so doing; the specified use of these tools form a discretionary means to address our pervasive ‘[infosphere](https://en.wikipedia.org/wiki/Infosphere)’ that is therefore better-able to be relied upon, whilst maintaining the means to support AI in a manner that preserves rights such as personhood and the right to self-determination.\n\nWhilst many have been involved and in-turn have their own views; my views are my own. A ‘human centric web’ was a term coined by me, as was a ‘knowledge banking industry’ and related considerations. It may be the case others had similar ideas; whilst noting, works online help with provenance.\n\nThe ‘human centric web’, has also been explored through the lens of considerations made to be well-known concepts communicated by quantum physics professionals. Seemingly, and incredibly, this information management system can be considered by various means that corresponds to the ideas communicated by experts in that field, such as the form of “the role of the [observer](https://en.wikipedia.org/wiki/Observer_effect_(physics))”. Therein; the relationship between the ‘recorded state’ of ‘facts’; is only able to be recorded, to have different properties, due to the format through which an assessment is made. The implications of these considerations i continue to find befuddling and worthy of further research.\n\nAs may be more clearly made understandable to others; by redesigning the architecture of Tim Berners lee Original ‘[information management](http://info.cern.ch/Proposal.html)‘ proposal, to a modern information management system that is intended to vastly improve support for [natural persons](https://en.wikipedia.org/wiki/Natural_person) to meaningfully participate (communicate) as a consequence of new and distinct design principles.\n\nCurrently it is the case that information systems relied upon by society are maintained almost exclusively by artificial persons (companies); and this has the effect of disaffecting our means to improve data-hygiene.\n\n> How ICT can distinguish between fact vs. fiction; whilst maintaining support for privacy, dignity and different, [reasonable](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/), points of view.\n\nThe nature of the effects that can be translated into computer code; is referred to, by those who study quantum physics, through a concept known as an ‘interference pattern’. This can in-turn be modelled, as is otherwise employed in the interests of existing economic models exhibited by solutions that work today, such as those that exploit ‘[attention economy](https://en.wikipedia.org/wiki/Attention_economy)‘ factors.\n\n*Therefore; the way we store and curate information electronically, as to forms a sense of ‘complete’ articulated ‘facts’ and provenance for any assessment can be shown to be influenced by the ‘information systems’ design, that in-turn causes a form of ‘interference pattern’.*\n\nWhilst i’ve gone into this field of consideration in relation to [social informatics elsewhere](https://www.webizen.net.au/about/references/social-informatics-design-concept-and-principles/); The tools used by [corpus](https://en.wikipedia.org/wiki/Corporation#History) to define artefacts to be extraneous to needs or benefits of [Corpora](https://en.wiktionary.org/wiki/corpora) is in-part calculated by way of provenance and its relations as becomes assumed to pertains to all future states. These are calculable models where math can be employed to drive particular outcomes.\n\nDue to the circumstances through which ‘the role of the observer’ is not supported (ie: natural persons not being able to employ a ‘information system’ that provides them a complete and entire record of data, relating to any form of enquiry that involves others) the means through which calculations (and/or evaluations) are made; can therefore be compromised and malformed, whether intentionally, unintentionally, incrementally in relation to prior intentional and unintentional acts; or in isolation as an anomaly.\n\nThere is only one ‘principle’ means considered viable as a means to re-define the ‘vulnerabilities’ exhibited to be ‘at risk’ on mass in relation to these forms of attacks; and that is through the definition of a modern [infosphere](https://en.wikipedia.org/wiki/Infosphere) that is made to be ‘fit for purpose’, by attaching the ‘cryptography’ or security mechanisms to its primary stakeholders, humanity. Same sort of solution was define as to form the financial banking sector; same sort of thing needs to happen again – whilst somewhat different, due to the non-linear nature of it.\n\nThe human centric web concept is built upon the use of ICT tools to change the way ‘information management’ systems are defined to provide improved support for humans as data-stakeholders, and beneficiaries.\n\nThe ethical foundation for a ‘human centric web’ is premised upon the application of tooling, based upon a series of principles that are designed to ensure the primary custodian of a natural person, and acts that pertain to [moral grammar](https://en.wikipedia.org/wiki/Universal_grammar), is the natural person (“Individual Rights”).\n\nThe meaningful utility of it, is by way of ensuring the corpus, whether predefined or dynamic rendered, be provided means through which the job of maintaining ethical custodianship, over the determinist characteristics, that define the environment (“responsibilities”), are built to support a means to evaluate the implications of each and every natural persons decisions and involvement, in a manner that can be evaluated overtime. The way this is achieved is through informatics systems that provide a means through which, individual roles are employed in the storage and use of accountability systems; that form the basis through which the means to discern metrics of trust are able to be emitted.\n\nIt is incumbent upon all to ensure the beneficial owner of the recordings made of our communications, as are constituents to the form of moral grammar exhibited by all living persons, be supplied by technology to the individuals natural person to whom the properties pertains. The biggest reason why it is important is that the reliability of relations used to form provenance, and by way of studies, for anyone to make decisions, the accuracy and contextualisation of recorded information plays a most important role. As it is reasonable to ensure ‘not everyone needs to know everything’, personhood requires the means to ensure the person affected by the data about them, is able to be made aware and provided a 1st class informatics capability to engage in the decisions that relate to them; and to use their data, as it should otherwise be made available for use to support rule of law, in relation to them.\n\nTo form means to discern the meaningful benefit of custodianship by way of corpora; the linguistic systems must be made available to all (“open-data”), and must support the ability to target versions produced in past, as is linked to updates; as to ensure the means to refer and infer relations, is maintained.\n\nAs constituents of dynamic groups, our information management systems also need to ensure our individual needs to be provided the capacity to maintain and retain ‘tamper evidence’ in relation to all data records that affects us as individuals.\n\nTo ensure we are made capable of maintaining our responsibilities, and by extension, our rights; the design of our information management systems, in our modern environment; is inextricably linked to an informatics definition of all ICT systems designed to support delegation of responsibilities, as is required to support tamper evidence, requires open and global standards.\n\nThese formats help to ensure any technical solution built through the use of them is better able to be afforded to everyone and that ‘vendor lock-in’ becomes less problematic.\n\nThe actions of humans within an [infosphere](https://en.wikipedia.org/wiki/Infosphere), defined by humans, is, in reality, the only actors who maintain custodianship over the effect of its use on our natural world. yet in our increasingly augmented world, the choices that are made about how we make use of data, is shown to be artificially limiting the means available to us; to limit harms our environment and life in it.\n\nThis is a consequence of the design paradigm used by our existing information management systems, which in-turn bring together the pre-internet economic structures, to manage the newly available cyber-space.\n\nInformation management systems are most-often used to support the notation of actions by ‘persona ficta’ (corpa) with or without association to other actors. These structures are now being used to support a new class of artificium actors (AI); and the means through which, their effects upon our natural world are influenced by agents, that are not a constituent of it.\n\nThe choices made forms a basis through which; distinctions are made between humans as distinct active agents in these environments; and the means through which we classify all other forms of social machines and/or tools.\n\nNow therefore; I seek to illustrate,\n\n- the circumstances in which humanity has defined the current info sphere,\n- why it is that we must change that design; and,\n- how we may adaptively employ modern technology,\n\n> to forge a “human centric web”.\n\nThe intended purpose in which any derivatives relating to the use of the concepts portrayed on this site; is for the development of a a ‘knowledge banking’ industry, that performs the minimum function of providing support to the ‘[inforgs](https://en.wikipedia.org/wiki/Inforg)’ of natural persons. The term ‘inforg’ is distinct to ‘identity’, as is most-commonly used (and otherwise defined by a dictionary). Identity is most-often used in relation to personal attributes that identify ‘sameness’. for instance, a drivers license is a form of ‘identity’ that shows you are one of the many who have one. But a drivers license, isn’t controllable by you, its not illustrating all the variations of what it means to be you; it isn’t you, and nor are many of the other instruments used for ‘identity’, whilst the semantics produced in relation to the use of them; is really very helpful in an inforg.\n\nA Knowledge Bank, is considered as a new form of institutional provider that is yet to materialise, in a manner that would make it ‘fit for purpose’.\n\nThe purpose of a knowledge bank, as part of a specified new class of industry, is to provide knowledge and ‘information trust’ services by way of a market-based mechanism that can in-turn be domestically regulated. In-effect, we live in a world of [pervasive surveillance](https://www.webizen.net.au/about/the-vision/a-technical-vision/), which is unavoidable, so we need to ‘domesticate it’. Our devices in our home and on our person, continuously listen and feed whatever information they can get; to the vendor of the software running on them, alongside others who form agreements to get it.\n\nThis data, when linked together using common-place data-science techniques, is extremely personal; extremely sensitive and both socially linked and leveraged for commercial gains. When shifting the paradigm to put the human being involved in data collection, the primary agent; the institutional trust mechanisms involved in managing that infrastructure, are really important. It needs to support legal concepts, like [separation of powers](https://en.wikipedia.org/wiki/Separation_of_powers), whilst being designed in a way that is different to banks and ISPs. the data collected is not for their use; it is stored in these environments, for you. The sum-total volume of data made available to it, will know more about you, than you do; irrespective of how that data is used. Today, its poorly used, poorly able to be maintained &amp; checked for accuracy; and poorly accounted for as the underlying resource continually produced in relation to knowledge economy.\n\nA knowledge banking industry is a critical part of the legal (and technical) apparatus required to fix that problem.\n\nThrough the production of open-standards based, corpora of providers; who would be committed (and bound legally) to sustainably governing the development, support and operation of information storage and communications apparatus as to protect the interests of natural persons, is considered to be a critical pillar of how economies into the future will operate.\n\nIt is the case today in the real-world that ‘rule of law’ is furnished meaningful utility by way of rules that are applied in geographic and culturally grouped territories.\n\nIn Australia, we have subscribed to the idea that we be ‘[ruled by law](https://www.ruleoflaw.org.au/principles/)’, as to protect against ‘tyranny’, such as to be unreasonably subjected to any foreign form that would lead to us being ‘ruled by man’ rather than by law.\n\nbut today, that’s not how it works online. When ‘choice of law’ was applied to the distribution of software products originally; those products were not services that applied the same legal concept to the jurisdictional rights of use that related to the data collected about us and made, by us, attributed to ‘online data storage systems’ that bound both to the same ‘rule of law’.\n\n<iframe height=\"480\" loading=\"lazy\" src=\"https://www.google.com/maps/d/u/0/embed?mid=1bHmB8_f7ASRHm97TwhZmmEQnTKU\" width=\"640\"></iframe>  \nA knowledge banking industry fixes this problem by re:enabling the separation of application data (creative work by others) and user data (the things we once stored on a floppy disk).\n\nThe moral role of a ‘platform operator’; through which the definition of ‘dignity’ becomes reliant, is in taking-up the role of seeking to defend and serve the interests of humans through the operation of ‘knowledge banks’, as is considered to be distinct to all other forms of institutions known to date.\n\nMuch like the advent of the ISP industry, part of their challenge will be in working to define how it is made to work as an organisation; and how to protect its customers, its stakeholders from wilful exploitation.\n\nInternationally, the concept of a ‘knowledge bank’ was consider in a similar yet distinct form to be one of an ‘[information fiduciary](https://www.theatlantic.com/technology/archive/2016/10/information-fiduciary/502346/)’.\n\nIn consideration; the concept of ‘information’ is seemingly insufficient, as it does not necessarily relate easily to the ‘metadata’ (misnomer, moreover its simply ‘other’ data – but anyhow); nor does it easily relate to the AI and/or processing related outputs formed through the use of ‘information’ provided by consumers. A way to describe this distinction, is that an image may display a bunch of people in it in a place with a landscape; the information able to be converted into ‘structured data’ may include; facial recognition &amp; object analysis data alongside the informatics about the device it was taken on, amongst the many other things; but this is different, to ensuring people are able to obtain a copy of the image file itself and in isolation to its useful processing by systems, as to support the means in which it is use as part of a [knowledge graph](https://en.wikipedia.org/wiki/Ontology_(information_science)). <span style=\"font-size: 1rem;\">Broader considerations therein include how the utility of artificial agents (such as bots), to make use information storage, as a body of knowledge, available in such a way as to ensure personhood is central to its economic frameworks, and the proper purpose of any such organisation..</span>\n\n<span style=\"font-size: 1rem;\"> Therein, the purpose if put simply, is to provide the means via our technology to ensure we are the primary actors who are entitled to form the informatics that represent our sense of wisdom, as is universally effected in any case.</span>\n\nThe concept of a knowledge banking industry, in-turn, seeks to support citizenship and personhood; the means for it to do so, heavily rely upon the practices through which we classify what is ‘commons’ and what and how we produce related ‘open data’ machine-readable ontologically relevent, works.\n\nOpen-Data and the availability of it in a machine-readable and structured format, is an undertaking of enormous importance. The independence, of being entitled to make use of ‘open-data’; by way of linked-data software does in-turn relate technically, to the custodianship of trusted systems, the means through which privacy is made viable in its use; and, is thought to be imperative for any system that can be made to be able to be used; to attend to the needs of humans as the only natural world actors involved.\n\nIn the real-world, we don’t need to pay a licensing fee for looking at a tree. This is not necessarily going to be the same ethical or moral quality exhibited to us online; on a macroscopic or microscopic level, if we care-less.\n\nThe intended effect; caused by modernisation of our means to make use of this new ‘[infosphere](https://en.wikipedia.org/wiki/Infosphere)’ medium; is to ensure the means to traverse, query and provide support for modern ‘smart agents’, is built upon the basis of a ‘human centric web’.\n\nThis ‘human centric web’ concept is ‘something’ that is entirely different to current alternatives commonly used today; and now employed for the definition of spaces, self and our social environments.\n\nTo resource ethics &amp; fairness, a knowledge economy asserts a value that is put upon participants. It’s important the way systems perform important functions throughout an interconnected environment; is designed to work in a manner that is centred upon our needs as humans, and in the interests of our natural world.\n\nIn democracies such as Australia, the rules we care about are written down as laws. Humans are the intended beneficiaries of these laws. It is important we make best use of technology to support its intended purpose.\n\nThe results brought about by forming this new ‘information management’ systems approach, is that new economic models emerge; that in-turn power advanced research, energy efficiency and web-scale problem solving in ways that were entirely impossible before; in a similar, yet different way, to the changes brought about by WWW over the past twenty years and smart-phones over the last decade. Over the next decade, everything is going to change. The concept of a a knowledge banking industry is to ensure those changes are made to be ‘human centric’ that we can sustainably manage the implications of global change and through that, advance issues of global sustainability.\n\nToday, it is the case that our means to trust news, statistics, other people or much at all is poorly served by our online systems. A knowledge banking industry, is designed to instrumentally change that situation.\n\nThe technological tools have been produced over a period of more than 20 years. The economic models about how to make use of them, are still an area that requires a great deal of work. The value proposition looks great, and whilst the major international providers can quickly shift; there is no good forecast on how it is this shift is likely to be brought about and how it will work when it does. There is a scope of widespread consequential problems that emerge; and whilst some models suggest that it is easier to build global knowledge banks that power billions world-wide, i’m not sure what society would look like if that’s the way our financial banking sector (or any sector) worked. As an Australian (one of 25m in a world of 7.5bn) its hard to see how the interests of Australians could be made a priority; unless of course,\n\nAustralia builds capacity to deliver its capabilities as a leading production environment of new technology that is powered by knowledge banking industries; and that our people, get more involved in fixing the problems."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/history/","title":"History"},"frontmatter":{"draft":false},"rawBody":"---\nid: 527\ntitle: History\ndate: '2018-10-02T12:39:34+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=527'\ninline_featured_image:\n    - '0'\n---\n\n(placeholder)\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/_-6mhdjE1XE\" width=\"560\"></iframe>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","title":"Knowledge Banking: A Technical Architecture Summary"},"frontmatter":{"draft":false},"rawBody":"---\nid: 499\ntitle: 'Knowledge Banking: A Technical Architecture Summary'\ndate: '2018-09-27T17:41:34+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=499'\ninline_featured_image:\n    - '0'\n---\n\n<span style=\"font-weight: 400;\">The first thing that is most important to point out; is, that it’s very, very important not to oversimplify things. </span>\n\n<span style=\"font-weight: 400;\">The world is entirely invested in its use of ICT and to bring about a radical change, whilst examples of how this has happened in other areas can be shown; is nonetheless, extremely complicated. This is the reason why i’m working on putting this website together. I recon, i can help make it more accessible, irrespective fo the complexities more broadly.</span>\n\nFurthermore; i have some particular views which are often shared with some, elements are often disagreed with by some whilst agreed to by others; and frankly, these are my views not anyone else’s. If someone wants to go do something that’s a better design, go for it. My method, has sought to ensure the basic infrastructure does not lead to asserting the right of any commercial entity to demand royalty payments from humans to live (that’s the job of governments, and its called ‘tax’).\n\nA summary of my design methodologies (at present) are as follows;\n\n<span style=\"font-weight: 400;\">The way this thing works is by making use of machine-readable data formats for storing information. Data in its most basic form, for traditional computers, ends-up being processed in binary. Languages are built to change the way software is developed (so they’re not authoring code in a binary format); and some of those languages, offer the means to produce software where the human readable information produced, is also machine-readable; in a format, that enables interconnected computing systems to understand the meaning of what is communicated in the file or document. Other systems exist, that help convert and change the way a single object (for instance a picture) can be interpreted by computers. for instance, an image can be processed to identify any persons (and their faces), objects, location, time and an array of other information. Audio, can be processed to transcribe to text, and to analyse the type of content alongside other things. </span>  \n[![DIKW Pyramid](https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/DIKW_Pyramid.svg/256px-DIKW_Pyramid.svg.png)](https://commons.wikimedia.org/wiki/File:DIKW_Pyramid.svg \"By Longlivetheux [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0 )], from Wikimedia Commons\")  \n<span style=\"font-weight: 400;\"> Accumulatively, the underlying processing capabilities all flow back to the means through which data, as information, is processed to be able to be meaningful. Yet this in-turn brings about the next challenge, which is how to uplift [information](https://en.wikipedia.org/wiki/Information) to a juncture where it can reasonably be considered ‘[knowledge](https://en.wikipedia.org/wiki/Knowledge)‘, and the overall idea is that the person who is managing their own [inforg](https://en.wikipedia.org/wiki/Inforg) is the primary [agent](http://xmlns.com/foaf/spec/#term_Agent) using [semantics](https://en.wikipedia.org/wiki/Semantics_(computer_science)) to define what [wisdom](https://en.wikipedia.org/wiki/Wisdom) means to them, and for them, as is recorded [temporally](https://en.wikipedia.org/wiki/Temporal) overtime by computing systems networking and communicating information in relation to them, one way or another.</span>\n\n<span style=\"font-weight: 400;\">Given ICT is predominantly operated in an ‘institutionally centric’ mode today, most commonly, the term ‘identity’ is used in relation to systems that relate to people, whereby the intended intepretation of the term, for consumers, is that a person is assigned a binding service by a legal personality of some sort. Whilst it is the case that these services are indeed essential, the implication is moreover one of confusion.</span>\n\n<span style=\"font-weight: 400;\">In my ‘knowledge banking industry’ methodologies, the modelling used is distinct. Therein, whilst legal personalities are amongst the ‘things’ that are supported by a ‘human centric’ architecture framework; the framework itself considers, the distinct needs of humans by way of considerations made in relation to the term ‘[inforg](https://en.wikipedia.org/wiki/Inforg)‘, and the meaningful use of AI through services provided by knowledge banking providers (that are perhaps something similar to the ‘information fiduciary’ concepts, discussed elsewhere). </span>\n\n<span style=\"font-weight: 400;\">By providing the means for people to have control over how they’re generating their own semantics, as is made possible by being made able to beneficially own and operate data relating to them; including how links are rendered and processed with a plurality of agents in any query (overtime) on a dynamic basis; the needs of the consequential, decentralised ‘information management’ ecosystems, brings about an array of functional requirements </span>that can be addressed by incorporating the use of a few distinct constituents; designed to provide interoperable and synergistic, permissive and authoritatively enhanced; data-flows.\n\nThe Key constituent elements identified to be required; to bring this about are,\n\n1. [<span style=\"font-weight: 400;\">Commons (ie: open data)</span>](https://en.wikipedia.org/wiki/Commons)\n2. [<span style=\"font-weight: 400;\">Things (and the data generated by them)</span>](https://schema.org/Thing)\n3. <span style=\"font-weight: 400;\">‘Identity Instruments’ (somewhat described [here by IBM](https://www.ibm.com/blogs/blockchain/2017/06/digital-identity-interactions/) as otherwise known as ‘[verifiable claims](https://www.w3.org/TR/verifiable-claims-use-cases/)‘)</span>\n4. Content &amp; relationships produced by self and others.\n5. <span style=\"font-weight: 400;\">[AI](https://en.wikipedia.org/wiki/Artificial_intelligence) Systems (and there’s a lot of this stuff that’s too complicated to get into in this post).</span>\n\n<span style=\"font-weight: 400;\">**Commons**, or ‘open-data’ is about things like languages and any means to describe anything of the natural world. It is the knowledge of humanity and our natural world. It is our constructs of law and other critical infrastructure required for any information management system; to function. Emerging technology is developing the underlying considerations to what could and should be ‘commons’. One example can be considered in relation to the advancements of computer vision technology that has an array of applications from those relating to biomedical research and services, through to facial recognition and new types of interfaces that are required for head-mounted, augmented reality displays; alongside similar functional capabilities that are built into vehicles, CCTV management systems; alongside other media platform applications. Whilst the means to preserve the right to ‘make arrangements’ surrounding the use of a persons own biometric signatures should, reasonably (subject to law) be made the sole right of the person (or ‘data subject’); these sorts of attributable rules are dissimilar to considerations that might be made about the means through which persons are able to identify flora and fauna; alongside the means to identify microscopic diseases, or other means through which ‘commons’ attributions may be beneficially applied. As a related consideration, the means through which these systems work, is based upon the use of [URIs](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier). There are two main reasons why this is important. The first reason, is that a single RDF document may contain a multitude of URIs resources from public and private sources; made accessible to an authorised agent, over a multitude of protocols. An example of how this is achieved can be shown by reviewing the [DID](https://w3c-ccg.github.io/did-spec/) info. The second reason is about decentralising discovery and maintaining privacy by way of making use, of decentralised ledger technologies. The ramifications of doing so, means that a private (“secret”) query can be achieved, without having to process the communications event across the infosphere; but rather, leverage off the ‘knowledge banking infrastructure provider’, to support the means of an individual to maintain their interests, as is the purpose of the knowledge bank itself. </span>\n\nThe specified means through which this might be achieved (ie: a few examples) is discussed elsewhere in the site (or will soon be).\n\n<span style=\"font-weight: 400;\">**Things** are literally things;</span>\n\n- <span style=\"font-weight: 400;\"> Things we connect to internet – things that interact, with internet. </span>\n- Things we call Companies (legal personalities)\n\nthere are all sorts of things. Some forms of things are able to be integrated using the RDF based (noting DID’s afore mentioned above)[ Web of Things](https://www.w3.org/TR/wot-thing-description/) tooling. Other forms of things, like a persons involvement with a company, can better be supported through the use of ledger technology, which in-turn also makes use of RDF to associate the activities of persons in their role with the legal personality; or project (ie: the production of a product) otherwise administered in a new and innovative way using technology.\n\n<span style=\"font-weight: 400;\">**Identity Instruments** are electronic claims that are issued socially, most-often, between legal personalities as is intended to be applied to natural persons or other legal personalities. Examples include the various embodiments of ‘verifiable claims’ that can be referred to having been provided ‘trusted support’ as a representation of ‘fact’ from a birth, university or telecommunications record. Others can include the ownership details of a thing, such as a motor vehicle; and related ‘verifiable claims’ that may relate to the registration and insurance of that asset. Yet another form may be the means to identify that a person is the legal tenant or occupier of a particular property; and that they are therefore entitled to define how and what ‘virtual items’ are able to be put upon the vicinity of their property; in the same sort of way a local council or government body may do the same, to ensure no virtual entertainment objects are encouraged to be engaged with via mobile phones, on freeways or other dangerous places.</span>\n\n<span style=\"font-weight: 400;\">The collection of all these things in a socially connected web, alongside all the stuff people create all by themselves, and the derivatives of the things created in relation to the other stuff that’s all stored together, in a format that supports version control – is called an Inforg. The entire emboidment of information stored in a persons Inforg, in addition to the materials that are connected to it; form the means through which **AI** systems are then employed, and programmed, to form the informational representation of an organic entity, the human being whose ‘inforg’ it is, to whoever it is that is engaging with it, on a dynamic basis; that is designed to be principally designed to be curated, by its ‘data subject’.</span>\n\n<span style=\"font-weight: 400;\">The purpose of a ‘knowledge banking industry’ is to supply the apparatus required for humans, to build and manage their inforg. To use the information that is able to be stored in it (as a consequence of any such type of thing existing); and to thereafter, build their own rules about how it can be used by others; and as a consequence of doing so, </span><span style=\"font-weight: 400;\">define their own AI infrastructure, that works socially with others, on AI infrastructure, as their own form of design and definition of what it needs to do for them. These designs are not about supporting the means for people to break the law. Whilst the design intends to support the needs of law, as defined by the people (through means enhanced by having produced such form of apparatus); the broader consideration, is that it limits the otherwise significant use and manipulation of data in its variety of presented forms, as is otherwise exploited inexorably by agents, world-wide. </span>\n\n<span style=\"font-weight: 400;\">This ‘knowledge banking’ technical ecosystem; in turn provides improves means to make use of AI for the betterment of humanity and the natural world. The means to apply the use of data to enhance research in areas such as healthcare and social policy, is enhanced greatly should an industry of providers be established, as to anonymise research request results and to deliver to relevent agencies, a multitude of aggregate results; that are in-turn, collectively processed as has been provided through the comprehensive availability of ‘point data’ sourced from those willing to participate; which in turn forms a far more comprehensive means to contribute towards the availability of ‘commons’, that can be generated by way of the ‘knowledge banking infrastructure’, in a manner that preserves privacy. Yet the design paradigm has been technically designed by taking into account far more than simply computer science considerations. Whilst the broader field through which such concepts are thought about, is generally called ‘web science’, the field takes into account an array of considerations most importantly including those that relate to our systems of law, and the role of language in any undertaking of ‘ontological design’.</span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/aigR2UU4R20\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">The way the ‘[Privacy](http://www.oecd.org/sti/ieconomy/30yearsaftertheimpactoftheoecdprivacyguidelines.htm)‘ &amp; ‘Dignity’ (ie: see [UDHR](http://www.un.org/en/universal-declaration-human-rights/index.html)) issues that otherwise plague our societies can be solve by a knowledge banking industry; is that, by the institutional nature of the proposed ‘regulated’ industry, not operated by government; it does in-turn provide the means to preserve separation of powers. Due to the decoupled nature between the legal rules that apply to data, vs. the legal jurisdiction that applies tot he use of applications (like it was in the days before internet); the system of government that applies to the individual to whom the inforg is about, also applies to the use of their data; and any differences between jurisdictions (ie: USA law vs. AU law, in relation to a concept, such as ‘copyright’) can be resolved through the use of semantics. The problem today, is that our ‘information management systems’, provide a foundation that is not ‘fit for purpose’ as they are now currently operated today. This can change, if it is sufficiently deemed meritorious, to do so. The ramifications are therefore; how to economically communicate the benefits of doing so, which is yet another complex constituent of the broader ecosystems challenge. Technology has changed so rapidly, many of our elders still struggle. Yet without appropriate leadership, the future for those they care for (and those who will need to care for them) may not be so good. </span>\n\nThe reason why the problem is so hard to solve, is that the rudimentary nature through which the idea of changing (or offering an alternative) to the way ‘information management’ works socioeconomically; impacts everything. It is not simply a ‘widget’ or an ‘app’, it is a change that is as significant, as the introduction of the internet, or the banking industry itself, that in the 1600s decided, humans needed a means to independently be provided the beneficial and protected use of a bank account.\n\nIn 2014, Sir Tim Berners-Lee considered related works and considerations in his talk about forming a ‘magna carta’ for the web.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/rCplocVemjo\" width=\"560\"></iframe>  \nHis views are in-turn able to be considered in relation to the advancement of what the term ‘personhood’ has been an area of social policy development over the period of living memory.<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/0HJRkwBOkdw\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">Making matters more complicated, the way the internet is predominately made use of today; the differences between the considerations made by sovereign Jurisdictions, that does not easily preserve the broadly accepted principles of ‘rule of law’ or those such as have been asserted by the [Charter of the Commonwealth](http://thecommonwealth.org/our-charter).</span>\n\n<span style=\"font-weight: 400;\">Other methodologies considered in relation to the way these sorts of problems may be solved, include the idea of personal servers, as is notably pursued for good purpose by the[ freedom box foundation](https://freedomboxfoundation.org/). Whilst this work is indeed very important (and interoperable); the issue that they cannot solve include the means through which address-space governance is managed in relation to [IANA](https://www.iana.org/), and the means through which to ensure domestic law-enforcement requirements are supported; whilst privacy preserved, alongside the need to define a methodology that supports ‘[Separation of Powers](https://en.wikipedia.org/wiki/Separation_of_powers_in_Australia)‘.</span>\n\n<span style=\"font-weight: 400;\">The technical design paradigm employed does thereby respond to these complex issues by producing a methodology, not dissimilar to ISPs, where provider would have tens of thousands, if not hundreds of thousands or millions of members; whose analytical fingerprints can become eradicated when generating statistical reports involving so many data-subjects over a variety of data-points. </span>\n\n<span style=\"font-weight: 400;\">These systems are intended to be operated legally, where the default circumstance is that market-based providers can be elected by citizens and those citizens can change providers if they want to (which in the ISP industry, is termed ‘[Churning](https://whirlpool.net.au/wiki/what_is_rapid_transfer_or_churn)‘; that the industry is regulated, that it is the safest provider type to store information and/or that their role is to act as to protect the dignity of others. Using URI based information standards, it doesn’t matter where data is stored, applications permissively network.</span>\n\nMoreover; the benefits of building a institutional ‘knowledge banking industry’, is that the date-stamps and related electronic forensic tooling required by persons, to ensure they can ‘walk into a court room’ and have their data examined simply, as to seek meaningful utility of ‘rule of law’ is immeasurably assisted.\n\nAn old (perhaps incorporating a few errors) diagram is outlined below; i hope its somewhat illustrative…\n\n[![](https://www.webizen.net.au/wp-content/uploads/2018/09/Credential-enabled-Identity-5.svg)](https://www.webizen.net.au/knowledge-banking-a-technical-architecture-summary/credential-enabled-identity-5/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/references/","title":"References"},"frontmatter":{"draft":false},"rawBody":"---\nid: 531\ntitle: References\ndate: '2018-10-02T12:44:06+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=531'\ninline_featured_image:\n    - '0'\n---\n\nplace holder\n\n(contextual stuff, that’s important for intepretation of the rest of the materials)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","title":"The design of new medium"},"frontmatter":{"draft":false},"rawBody":"---\nid: 459\ntitle: 'The design of new medium'\ndate: '2018-09-25T14:33:09+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=459'\ninline_featured_image:\n    - '0'\n---\n\nThe invention of a ‘knowledge banking industry’ that builds the means to deliver a ‘human centric web’, leads to a ramification that is the creation of a new medium. This new medium, for many reasons, is considered to be a meritorious work of art that should lend meaningful support to humanity, by ensuring an alternative to the pre-existing international peers of information management solutions, is available.\n\n> ***media***\n> \n> noun\n> \n> 1. the main means of mass communication (broadcasting, publishing, and the Internet) regarded collectively.\n> 2. plural form of **<span class=\"SDZsVb\" data-term-for-update=\"medium\" data-ved=\"2ahUKEwj2nca3xdfdAhUShxoKHXLmDCIQgCswAHoECAUQCw\" role=\"link\" tabindex=\"0\">medium</span>**.\n> \n> ***medium***  \n> noun  \n> *1. an agency or means of doing something.*\n> \n> “using the latest technology as **a medium for** job creation”\n> \n> *2. The intervening substance through which sensory impressions are conveyed or physical forces are transmitted.*  \n> “radio communication needs no physical medium between the two stations”\n> \n> Source: Google.com.au\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/I_OGwrSTWT4?t=48m59s\" width=\"560\"><span class=\"mce_SELRES_start\" data-mce-type=\"bookmark\" style=\"display: inline-block; width: 0px; overflow: hidden; line-height: 0;\">﻿</span></iframe>\n\n<span style=\"font-weight: 400;\">Humanity has evolved through the meaningful use and development of technology that brought about radical changes moreover considered in terms of industrial evolutions. Over the 19th and 20th century these industrialised processes improved our media foundations, through which, we have empowered our means in continuing to strive to become better custodians for our natural world. Through communications media, we have developed remarkable means to do so, as our means to communicate with others, and to hear the creatively directed works of others, changed overtime in a plurality of ways. </span>\n\nInternet has brought about radical changes, over the course of only a few decades. These influences have become increasingly notable since the widespread introduction and use of smartphones. One of the most prominent examples includes the [Apple iPhone](https://en.wikipedia.org/wiki/IPhone), which since first released in 2007, is now reported to have cumulatively sold more than 1.2 Billion units by 2017 ([source](https://www.washingtonpost.com/news/the-switch/wp/2017/08/01/the-iphone-is-all-anyone-cares-about-when-it-comes-to-apple-earnings/)). The website [statista reports](https://www.statista.com/topics/779/mobile-internet/) *“According to January 2018 data, the global mobile population amounted to 3.7 billion unique users. As of February 2017, mobile devices accounted for 49.7 percent of web page views worldwide”,* whilst the news outlet [reuters reports](https://www.reuters.com/article/us-ericsson-mobility-report/ericsson-doubles-its-2023-forecast-for-iot-connections-idUSKBN1J80PP) that *“Ericsson has almost doubled its forecast for connected cellular Internet of Things (IoT) in 2023 to 3.5 billion” (devices)*.\n\n<span style=\"font-weight: 400;\">Our present era of socioeconomic development is rapidly changing and is more simply considered by some, to be “</span><span style=\"font-weight: 400;\">*the 4th industrial revolution*“. Humanity is now producing tools that enables the industrialisation of systems and methods; that support functional outcomes; such as, the means to bioengineer flora, fauna and other bio-organisms. </span>\n\n<span style=\"font-weight: 400;\">Our use of technology increasingly influences our world in many ways, including means through which we influence weather. We produce the means to operate artificial actors, and do so much more. As a species, through the use of our time as individuals and the way through which socioeconomic frameworks influence the capacities for us to support our needs, and to work with others; the material outcomes produced, shapes our world.</span>\n\n<span style=\"font-weight: 400;\">The current design methodology employed as has forged the operational characteristics of our ICT systems share a common set of characteristics of how our ‘information management systems’ work.</span>\n\n<span style=\"font-weight: 400;\"> Many of these systems continuing to make use of our natural world as a resources, as work done by natural persons is applied to the practical utility of how their efforts are articulated through the lens of legal personalities and the role they have, on defining what is supported to be done, and what is left without having been rendered a solution, that is technically otherwise able to be done. </span>\n\n<span style=\"font-weight: 400;\">We know the importance of ensuring we act sustainably, in the manner through which we do so. We know, and most-often do our best, to conform to the needs of our communities and to contribute meaningfully.</span>\n\n<span style=\"font-weight: 400;\">We are now manufacturing outcomes, in all sorts of new ways, that is built upon our use of ICT as a communications and recording medium. We depend upon the proper function of these systems, as others refer to the information made available as a consequence of them, to consequentially make decisions.</span>\n\n<span style=\"font-weight: 400;\">We are inextricably bound to the augmented world as is produced through the development of ICT mediums, and the means through which society is made able to function; now depend upon the functional characteristics of ICT, the way these systems provides us the means to do things, influencing how it is the work of human activity is applied to get things done, and how issues that unclear, are left unattended. </span>\n\n<span style=\"font-weight: 400;\">The ‘information management’ tooling presented through the more comprehensive notion of ICT; is built upon the means through which we produced tools that consume and manipulate the electromagnetic spectrum, alongside other chemical and biological processes (“the microscopic”). We have intentionally developed these capabilities as to manufacture outcomes in the real world, for natural people. This in-turn has brought about an array of intended, and unintended consequences. The ability for us to make use of these systems, is only made able through the specified design principles used, to apply the technological capacity to make systems, to the purpose for which those skills are employed and the means through which they’ve been made economically able, to be brought about. The means through which law interacts with these design decisions is significant, as those who work on making ‘things’, do so, most-often, on behalf of an employer and/or a party other than themselves who provides the resources to do so.</span>\n\n<span style=\"font-weight: 400;\">As we continue in our pursuits to learn about our world, and what it means to be human, we do so through the adaptive development of an employment scheme of arrangement that has evolved overtime, long before we developed such sophisticated online ‘artificial intelligence’ related information systems.</span>\n\n<span style=\"font-weight: 400;\">Through the application of this infrastructure approach; we continue to forge new instruments enabling us to make, and make use of, knowledge, in the manner rendered support through socioeconomic systems. But the needs of people and the societies to which they belong, is changing as a consequence of our rapidly evolving ICT tools. Until recently, it was the case that the principle and primary means through which records were produced, stored and provided to others; was by way of written records. As computing systems and databases were developed, these paper based records were transcribed, by data entry operators, to databases to assist organisations more rapidly find information that was primarily stored on paper record. Whilst paper records do not change, information stored in electronic records can be modified at any time. Whilst the use of paper records most-often required the creator of those documents to provide a copy of it to other participants relating to the production of the document, whether it be a health related record, a retail receipt or a contract between parties; electronic records often do not provide a similar level of support to the consumer of organisations, who are increasingly changing their practices to use databases. More modern systems now generate data ‘machine to machine’, whilst the style and functional support of almost all of the systems used by companies, provide far less functional support for consumers than they do for internal and commercial use, between businesses (such as advertiser, commercial partners and application developers by way of APIs)</span>\n\n<span style=\"font-weight: 400;\">The development of most important systems is, for the most part, produced by legal personalities in a manner that is focused on how those systems need to operate to serve their needs, alongside the needs of their consumers as required of them by law; and, as considered to be beneficial for their brand and economic growth. </span>\n\n<span style=\"font-weight: 400;\">As it is the case, that the way this ‘norm’ of ‘information systems’ have evolved overtime, as to definitively be the means through which information is produced, made available and operated today; as does currently focus on the needs of how it is, that our [legal personalities](https://en.wikipedia.org/wiki/Legal_person) are made able to operate it; and that they are reasonably preferentially considering their needs above our own, as natural persons, distortive effect are symptomatically prevalent at a time where these systems, are being connected to ‘artificial intelligence’ to intepretation based on what information is available, without being able to consider or algorithmically process, what is not available.</span>\n\n<span style=\"font-weight: 400;\">The implications of how our current mediums (information management systems) have been designed in a specific type of way, is that there is a volume of information that is available in support of some forms of issues; and characterisations, no-matter the form of relationship had with any particular organisation, is in some ways very well supported by the way systems work for those whose gainful employment relates to the use of them. Yet, when being considerate about the interactive nature between organisations and natural persons, brought about through the use of data, the far broader complexities are not well provided support. As much as our roles in society by way of participating in forms of gainful employment is important, our means to do so is premised upon our ability to ensure technology works best for us as citizens, through which any form of employment is thereby made possible. </span>\n\n<span style=\"font-weight: 400;\">Complicating matters relating to information management further; are considerations that are intrinsically linked to the economic framework, as to include (but not be exclusive to) the means through which international law (principally contract law, as does relate more specifically to consumers) interacts legally, on an international basis, that in-turn influences societies where some may relate to the ‘[rule of law](https://www.google.com/maps/d/u/0/edit?mid=1bHmB8_f7ASRHm97TwhZmmEQnTKU)‘ elected by the online system operator ([citizens](https://en.wikipedia.org/wiki/Citizenship)), and many others who depend upon it, may not ([legal alien](https://en.wikipedia.org/wiki/Alien_(law))). Law manifestly considers on a geographic territory basis intepretation, considerations and the means through which any geographic territory makes use of law to address important issues for those who are in the boundaries of their Jurisdiction territory; in-turn, leading to authorities building upon this, to form agreements with others internationally. Yet the texts and authored constituents of law, are not universally the same across the world; in fact, in so many ways, they are very, very different. This is not a problem that can simply be fixed by solving a problem in one legislative act, but rather, the means through which the eco-system of legislatives acts form a quilted framework in which to keep societies fair, just and equal. Whilst it is the case that very large companies and government can form their own agreements (particularly for law enforcement related purposes); access to justice, by vulnerable citizens cannot be easily attended to in this way. This in-turn leads to a circumstance where our present, medium, needs to change in-order to support the needs of our modern societies as has developed through the use of what we have already made; in a manner that has been so successful, it has led to new and emerging issues.</span>\n\nIn our pursuits to collate knowledge, that has driven us to build tools that we can use to improve the way we observe, study, calculate, modify and make use of the world and all things in it (*Universitas)* in new ways. Today, what was considered previously to be impossible, has brought about a medium through which our world and beyond, is now interconnected with an artificial realm. This medium acts through the objective use of data, through which; near instant, dynamic linguistic communications. now evolves with more capacity than humanity by itself could ever do. This new medium, has been designed to make use of knowledge, as we make new agents, that are natively designed to flourish in this new medium, more easily known as cyberspace.\n\nThe influence put upon our world, have been built through the use of knowledge, formed upon the exploration of classical physics; and moreover, the related fields of science, technology, engineering (arts) and mathematics. The influence of ‘art’ has always played a significant role; yet the means to form a system of measuring how this is the case, is not thought to exist.\n\n<span style=\"font-weight: 400;\">The developments of humankind and the impact we have upon our natural world, is influenced by the societal means through which, by works of art, we render opportunity. The means to do so depends upon the means of safety made available to those, who are inferred the means to pursue works that progress the useful arts and sciences. Through which the ubiquitous availability of derivatives, becomes available. If our information systems are poorly designed, the means to support *meaningful* progress is diminished.</span>\n\n<span style=\"font-weight: 400;\">The way these systems work, is inextricably linked to the way of our systems of economic distribution are made to function. Throughout the course of living memory, the changes have both been rapid and remarkable. From a time where distribution of material artefacts, such as gold, to the times where the modernisation of systems brought forth by a new ‘systems of information’ that does far more complex procedural considerations, we now modify the means of individuals to new economics of choice.</span>\n\nSo, when considering what it is to form modern medium, that takes into consideration all of the present framework opportunities; from the means to employ advanced artificial intelligence systems, the ability to ensure decentralised applications are now able to work across the planet via internet connectivity; the capacity to employ cryptography, and the ability to form new business systems through cryptographically trusted, decentralised ledgers; the means to build a better future is arguably no longer a technical challenge, but rather one of significant social consequence.\n\n<span style=\"font-weight: 400;\">In our current ‘information management’ systems; the means through which the notion of debt is employed; associates the </span>*<span style=\"font-weight: 400;\">capacity</span>*<span style=\"font-weight: 400;\"> of an individual to </span>*<span style=\"font-weight: 400;\">‘service</span>* *<span style=\"font-weight: 400;\">debt’</span>*<span style=\"font-weight: 400;\">, as to be afforded </span>*<span style=\"font-weight: 400;\">‘wealth’</span>*<span style=\"font-weight: 400;\">; which is in-turn able to be attributed to our natural world by metrics that include but are not limited to; consumption, energy use and socio-environmental impacts.</span>\n\n<span style=\"font-weight: 400;\">Sums can more simply be derived as a linear equation; as the manifestly compliant needs of an ‘informatics’ environment, that can otherwise be built, to the sufferance, of all or the opposite. Where systems fail; amongst those most vulnerable to the worst effects, are those who make works of ‘art’, which is all too commonplace today. </span><span style=\"font-weight: 400;\">The definition of art is; </span>*<span style=\"font-weight: 400;\">the expression or application of human creative skill and imagination.</span>*<span style=\"font-weight: 400;\"> The studies of works within within many subject matter fields, including quantum physics, presents a means through which we can better explain the concept of an interference pattern as to correlate the applied use of ICT; yet much like any form of ‘state of the art’ works, it is indeed easier to describe using existing examples, than making attempts to describe the application of the same concepts, to new, innovative applications; designed as a consequence of a persons ‘artistic’ capacities, complimenting a skillset that is otherwise denoted in the field of ‘[Liberal Arts](https://en.wikipedia.org/wiki/Liberal_arts_education)‘ or indeed [STEM](https://en.wikipedia.org/wiki/Science,_technology,_engineering,_and_mathematics). </span>\n\n<span style=\"font-weight: 400;\">These works of humanity are now, on a wholesale basis, being funnelled into large-scale operators who in-turn have enormous means, to process all the information provided to them, to produce highly advanced ‘artificial intelligence’ related services that are operated on a proprietary basis, due to their current global leadership as ‘silos’. As the use of internet continues to extend into our homes and our environments, the existential implications are beyond comprehension; and potentially, beyond means to form reasonable, legal remedy; for issues otherwise recorded, through the information systems of today.</span>\n\n<span style=\"font-weight: 400;\">The ability to correct these issues is brought about by investing in the development, availability and extension of an alternative marketplace offering, built upon a different ‘information management structures’. This is expected to be identifiably considered to be, an entirely new medium. It is expected to bring about, systemic impacts, that can result in the operation of </span>*<span style=\"font-weight: 400;\">corpus</span>*<span style=\"font-weight: 400;\"> through means built upon the capacity for individuals; as to improve their means to becoming informed to make decisions, and in-turn, be held to account for their decisions. </span>\n\n<span style=\"font-weight: 400;\">This is in-turn expected to impact the way other artefacts of the natural world become better understood, considered, cared-for and employed. It is expected to change the economics, of how we impact our world and what it is, people we consider leaders of society, are known to have done to be awarded such accolades.</span>\n\n<span style=\"font-weight: 400;\">The manifest of our reality, our world, is inextricably linked to the terms for which we define our innately unique and irreplaceable qualities; in information systems and how the dynamic qualities of those systems are applied to any and all communications &amp; systems, of value. </span>\n\n<span style=\"font-weight: 400;\">Whilst it is often reasonably understood that we as individuals may innately ‘know’ ‘good’ from ‘bad’, and that works such as the exploration of ‘moral grammar’; and related, contextually philosophical forms of material basis, through which discussion and process be met between humans; the capacity for us to have transferred a means, in which to support the use of agents that are artificial in nature, has simply not existed util recently. </span><span style=\"font-weight: 400;\">It is now the case, that the infosphere is an instrumental tool used to exist and modernise the world around us. </span>\n\nIf we want the outcome to be ‘human centric’, then its important we build systems to support the delivery of that outcome. <span style=\"font-weight: 400;\">It has always been the case that to understand our pathway forward, we must understand how it is that we have forged presences by way of provenience; as to discern what is real, and what it is we have immaterially created (fantasy). The structural nature of our current systems are failing to resolve widespread problems that are leading people to make decisions on the basis of fantasy, all too often.</span>\n\n<span style=\"font-weight: 400;\">It is incumbent upon us, i believe, to form means in which to preserve our natural world by way of the development of our societies; by way of the creations of </span>*<span style=\"font-weight: 400;\">ficta</span>*<span style=\"font-weight: 400;\">; which requires us to </span>[*<span style=\"font-weight: 400;\">Sig Animarum </span>*](https://www.google.com.au/search?&q=Sig+Animarum+translate)<span style=\"font-weight: 400;\">by</span>*<span style=\"font-weight: 400;\"> corpus,</span>*<span style=\"font-weight: 400;\"> as it presents the otherwise unmeasurable risk, that these systems may well be deployed as to form a manufactured </span>*<span style=\"font-weight: 400;\">sense</span>* *<span style=\"font-weight: 400;\">of</span>*<span style=\"font-weight: 400;\"> reality. </span>[*<span style=\"font-weight: 400;\">Artificium</span>*](https://en.wiktionary.org/wiki/artificial#English)<span style=\"font-weight: 400;\"> is now instrumental to the beneficial owners as to make </span>*<span style=\"font-weight: 400;\">itself</span>*<span style=\"font-weight: 400;\"> a reality that is now therefore distorted, as to cause incapacitation; and harm to us all. </span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","title":"The need to modernise socioeconomic infrastructure"},"frontmatter":{"draft":false},"rawBody":"---\nid: 496\ntitle: 'The need to modernise socioeconomic infrastructure'\ndate: '2018-09-27T16:56:28+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=496'\ninline_featured_image:\n    - '0'\n---\n\n<span style=\"font-weight: 400;\">As is programatically a constituent of the effects brought about by [the integral works of others](https://www.webizen.net.au/about/history/history-global-governance-ict-1/); notwithstanding the fields of endeavour as yet left unattended,</span>\n\n<span style=\"font-weight: 400;\">It is the case today, that in living memory, the concept of money has transformed systems based on the [gold standard](https://en.wikipedia.org/wiki/Gold_standard), to the binary notations produced by ICT. It is now the case that the complexities of [Fiat Money](https://en.wikipedia.org/wiki/Fiat_money) and [Fractional Reserve Banking](https://en.wikipedia.org/wiki/Fractional-reserve_banking) are a complex area of specialised knowledge which was to some-degree, made simple by the post by visualcapitalist.com in 2017 of “[All of the World’s Money and Markets in One Visualization](http://money.visualcapitalist.com/worlds-money-markets-one-visualization-2017/)” as to see the effect of these global, structural works.</span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/CI5CFQXJxcA\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">In consideration;</span>\n\n<span style=\"font-weight: 400;\">Post World War II, Australia notably set-about delivering socioeconomic frameworks to support a ‘middle class’. </span><span style=\"font-weight: 400;\">These social-policy structures incorporated such concepts as ‘[superannuation](https://en.wikipedia.org/wiki/Superannuation_in_Australia)’ (or ‘[private pensions](https://data.oecd.org/pension/private-pension-assets.htm)’), free healthcare ([medicare](https://en.wikipedia.org/wiki/Medicare_(Australia))), [Free Tertiary education](https://en.wikipedia.org/wiki/Tertiary_education_fees_in_Australia#Abolition_of_university_fees), a [40 hour work week](https://en.wikipedia.org/wiki/Eight-hour_day#Australia), and [legal aid](https://en.wikipedia.org/wiki/Legal_aid#Australia) and [trade union](https://en.wikipedia.org/wiki/Trade_union#Australia) related representation for work-related disputes, amongst many others. However these areas of policy started a trend of change, that may reasonably be considered in association to the growth of ICT as to have occurred throughout a similar timeline. </span>\n\nWhilst it is the case today, that ‘knowledge based capital’ is considered in accounting terms to have significant value in relation to the economics of legal personalities, as is highlighted in a related [OECD report.](http://www.oecd.org/sti/inno/newsourcesofgrowthknowledge-basedcapital.htm)\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/iaPCPuoaEPk\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">In effect, the universal take-up of ICT technology has brought about significant changes to the nature of ‘currency’. Before internet, most forms of work had physically visible implications. A tradesperson involved in construction or engineering, was able to show the efforts of their work. Those such as teachers and office worker undertook works where the economic association to their work, related to the number of hours they were working at a particular premises. Internet, has brought about radical changes that do not easily associate to these traditional forms of work; and intrinsically as a result, both the form through which ‘currency’ associates to new forms of work and indeed also thereafter; banking.</span>\n\nWhilst the OECD reports show the significance of ‘knowledge based capital’ as is associated to incorporated legal persons; the tooling available for natural persons to be provided the means to participate independently and/or have value associated to their works of a less tangible nature, is not as well developed as may be otherwise beneficially produced.\n\nIn one example highlighted by research undertaken by the Australian Superannuation firms AustralianSuper and Cbus Super, their research related [press-release document](https://www.cbussuper.com.au/about-us/news/media-release/research-shows-shortfall-in-superannuation) states\n\n> The research estimates that 2.3 million Australian workers now partially or entirely fall outside of superannuation coverage which equates to around $10 billion in missed superannuation payments each year.\n\n<span style=\"font-weight: 400;\">As the conditions for work have changed rapidly, and radically, the conditions through which other contributory constituents of support to natural persons are in-turn shown to be suffering from systemic issues. </span>\n\n<span style=\"font-weight: 400;\">Our local economy is now networked globally. The data used to support trade, is now socioeconomically rationalised in many cases, to not provide natural persons a comprehensive and trusted copy of the data that is created relating to them. Organisations whose economic structures are now built upon ‘knowledge based assets’ in-turn employing natural persons as a constituent of their broader use of the natural world, as a resource. </span>\n\n<span style=\"font-weight: 400;\">One of the many symptoms that is brought about by this, is that, in effect, natural persons who do not have a ‘safe place’ to store their data (akin to money, in a bank) – results in a circumstance where they do not personally have ‘evidence’ of their activities with others, in a trusted format. Whilst this can be considered to assist risk-management techniques for data-platform operators (legal personalities) for wrongs that disenfranchise natural persons, the means to ensure organisations are operating efficiently, and that investment decisions can be reliably made on the basis of good evidence – is also reduced.</span>\n\n<span style=\"font-weight: 400;\">The economic problems these ‘information management system’ design qualities present are not simple. </span>\n\n<span style=\"font-weight: 400;\">It is already the case that data (information -&gt; knowledge) is known to be an economically valuable resource, so the business problem is seemingly not about producing the means to demonstrate this underlying driver to broader considerations. Rather, the problem appears to be the inability for our society to recognise and act upon the means to recognise that the value of data is more importantly able to be assigned by law to natural persons, as to support governance structures of societies world-wide; than it is to simply ignore the needs of natural persons, and maintain a legislative framework through which the intended beneficiaries of data are almost exclusively legal personalities.</span>\n\nIn other words, by other references; some alive today remember the circumstances that both brought about and led to the production of the [United Nations UDHR Charter](http://www.un.org/en/universal-declaration-human-rights/index.html). Yet whilst many of the statements made by that document; can now be associated to an incredible volume of data produced in relation to the vast majority of humanity who live in advanced societies, the means through which they are now expected to do so is by way of being considered as a particular group of persons, which thereafter led to new language being formed and made use of by UN works, in terms of ‘[consumer protections](http://www.un.org/esa/sustdev/publications/consumption_en.pdf)‘.\n\n<span style=\"font-weight: 400;\">It’s difficult to identify and to translate, in economic terms, the benefits of having ‘high quality’ and ‘reliable’ data available, as a direct result of having produced platforms for natural persons to be organisationally supported as the beneficial owners of their data, which in turn may be used as a foundation through which they are then better able to support the expectations of their role in groups (legal personalities) to do tasks in our societies. The concept of how this might be done through the establishment of a framework such as was called an ‘information fiduciary’, in relation to US Works published in [The Atlantic (2016)](https://www.theatlantic.com/technology/archive/2016/10/information-fiduciary/502346/) yet, if any such field of work is going to more effectively consider what is in-effect, our modern forms of ‘currency’, investment is required, the ICT </span><span style=\"font-weight: 400;\">tooling needs to be brought about; and the changes through which we consider the nature of how it is our modern ‘[infosphere](https://en.wikipedia.org/wiki/Infosphere)‘ could be refactored and what new forms of [commons](https://en.wikipedia.org/wiki/Commons) infrastructure is imperative to our shared needs, and the progress of work otherwise identified from before the UDHR, and beyond, to help people understand what it is people need to do, to succeed in our modern world. </span>\n\n<span style=\"font-weight: 400;\">The fastest way to bring this about; is to form the economic means, through which, participating in the development of new and emergent economic models, beneficially supports them as they’re doing it. </span>\n\nIf this is going to be made able to occur; the means to do so must be constructed in a manner that is rendered meaningful support by (accessible) law."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/the-vision/","title":"The Vision"},"frontmatter":{"draft":false},"rawBody":"---\nid: 461\ntitle: 'The Vision'\ndate: '2018-09-25T14:46:01+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=461'\ninline_featured_image:\n    - '0'\n---\n\n> An [**inforg**](https://en.wikipedia.org/wiki/Inforg) is an informationally embodied organism, entity made up of information, that exists in the [infosphere](https://en.wikipedia.org/wiki/Infosphere \"Infosphere\"). These informationally embodied organisms are also called natural agents.\n> \n> <span style=\"color: #333333; font-size: 1rem;\">Imagine a world where our information technology is provided in an entirely different way, built upon new and different commercial footings designed to support your means to manage your inforg. </span>\n\n<span style=\"font-weight: 400;\">Imagine if all the core technology tools had been made to connect all existing software systems, and change the way data is stored and made use of to enable you to store all the information about you. </span>\n\n<span style=\"font-weight: 400;\">Imagine if all the things others needed to store about you could be securely linked to your data storage system. </span>\n\nImagine if the Artificial Intelligent Agent that interacts with your use of online tools, was defined by you; rather than the publisher or host of your content.\n\nCan imagine a world without banks? Now consider that we live in a world without knowledge banks.\n\n> Our society is able to change to use technology, to make knowledge banking industries today. If it chooses to…\n\n<span style=\"font-weight: 400;\"> The means to produce and deliver the legal and technical apparatus to define an evolutionary foundation for the knowledge economy is available today. </span>\n\n<span style=\"font-weight: 400;\">We can change the way we store and make use of data, in a manner that requires a ‘knowledge banking industry’ to work, that is built upon works that have technically been under development since around the turn of the millennium. </span><span style=\"font-weight: 400;\">These global standards based, software ecosystems, establish the opportunity to bring to market – solutions for citizens and their economies.</span>\n\n<span style=\"font-weight: 400;\">Humanity has lost control of the effects its information systems have upon it, and a knowledge banking industry is needed to take it back. The need for a knowledge banking industry is clear, present and needed urgently. </span>\n\n#### What is a Knowledge Bank?\n\n<span style=\"font-weight: 400;\">The technical apparatus provide the means to support a persons needs for data storage and curate the use of information and things, that their information systems powers. This is not unlike a web-hosting provider, although there are an array of distinct differences.</span>\n\n<span style=\"font-weight: 400;\">A knowledge bank provides the standardised means for humans to define the way artificial intelligence works for them. It does this using semantic web technologies. All types of applications and devices can be made to work with semantic web technologies, and it is the case today that many already do.</span>\n\nA Knowledge Bank acts as a fiduciary like a doctor or lawyer. Their job is not to have any beneficial ownership over ‘your inforg’ and as part of their job they need to protect you if anyone wants to get access to your data in any way.\n\nWhilst the only organisation who may legitimately have the ability to force access is the law-enforcement agencies relating to where you live, in your land ruled by law; your knowledge banking provider is designed to temper their requests and ensure they’re lawful.\n\nA knowledge bank provides the ability to ‘time-stamp’ and support your needs for more sophisticated enterprise services as an individual; such as cryptographic tools (access control), security and technical support, maintain back-ups, protect against exploitation online and support for your privacy.\n\nA knowledge bank provides you a place to store your private and personal information and blend the use of it, via advanced software, with all the other information that is being constantly produced by you, to improve outcomes for you, based on truth and the ability for you to collect the definitive evidence.\n\nA knowledge bank is a safe place for you to store your electronic financial and ‘identity’ instruments.\n\nA knowledge bank provides the means to identify what it is you do, and provide you the means to prove it. A knowledge bank, provides you the means to make use of the small amounts of information you have and produce, and network it with the knowledge that’s available from across the world to form better insights into the circumstances of your decisions, and how to improve.\n\nA knowledge bank, supports your means to transfer between providers based upon its use of software technology standards. Knowledge Banks provide the fundamental apparatus required to support advanced public statistics that help change public policies and enable your representatives to put the money where its needed to address the problems no one can solve by themselves.\n\nKnowledge banking, changes the media landscape by forming a new medium that is not built upon advertising funded messaging and discovery services.\n\nKnowledge Banking produces the knowledge layer to the web, to be human centric, for our human centric natural world.\n\nA knowledge banking industry also produces a ‘software as a utility’ industry in parallel. it does this by changing the way applications online work, by removing the means through which the application depends upon storing all of the consumers information to make their business work.\n\nA *knowledge banking* industry, is core infrastructure for our natural world in our modern age. Without one, our world will continue to distort uncontrollably."},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","title":"FOAF"},"frontmatter":{"draft":false},"rawBody":"http://xmlns.com/foaf/0.1/"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","title":"General Ontology Information"},"frontmatter":{"draft":false},"rawBody":"\nOntologies are defined using [RDF](../W3C%20Specifications/RDF.md). \n\nThe Development Folder is currently: https://github.com/WebCivics/ontologies/tree/2023 \n\nThe Objective is to work to ensure the ontologies refer to working URIs.  Some new ontology work may thereby be required, with the appropriate work to define the relations with pre-existing / historical ontologies / URIs.\n\nPart of what is sought to be achieved, is to decentralise the storage of URIs on protocols that are less suceptable to dead-links via [Decentralised Ontologies](../../../../Core%20Services/Decentralised%20Ontologies.md) efforts, which is in-turn part of the [Permissive Commons](../../../../Core%20Services/Permissive%20Commons.md) objectives more broadly.\n\nCORE Ontologies include;\n- [FOAF](FOAF.md)\n- [RDFS](RDFS.md)\n- [OWL](OWL.md)\n- [SOIC](SOIC.md)\n- [SKOS](SKOS.md)\n\nFacebook: https://ogp.me/\nSearch: https://schema.org/ \n\nLod Cloud: https://lod-cloud.net/\nWikiData: https://www.wikidata.org/wiki/Wikidata:Main_Page\nDbPedia: https://www.dbpedia.org/\n\nPrefix.CC search for ontologies ( https://prefix.cc/ )\n\nsolid ontologies: https://solidproject.org/developers/vocabularies/well-known/common\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","title":"OWL"},"frontmatter":{"draft":false},"rawBody":"https://www.w3.org/OWL/"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","title":"RDF Schema 1.1"},"frontmatter":{"draft":false},"rawBody":"Document Source:  https://www.w3.org/TR/rdf-schema/ \n\nRDF Schema 1.1\n==============\n\nW3C Recommendation 25 February 2014\n-----------------------------------\n\nThis version:\n\n<http://www.w3.org/TR/2014/REC-rdf-schema-20140225/>\n\nLatest published version:\n\n<http://www.w3.org/TR/rdf-schema/>\n\nPrevious version:\n\n<http://www.w3.org/TR/2014/PER-rdf-schema-20140109/>\n\nEditors:\n\n[Dan Brickley](http://danbri.org/), Google\n\nR.V. Guha, Google\n\nPrevious Editors:\n\nBrian McBride\n\nPlease check the [**errata**](http://www.w3.org/2014/rdf1.1-errata) for any errors or issues reported since publication.\n\nThis document is also available in this non-normative format: [diff w.r.t. 2004 Recommendation](https://www.w3.org/TR/rdf-schema/diff.html)\n\nThe English version of this specification is the only normative version. Non-normative [translations](http://www.w3.org/Consortium/Translation/) may also be available.\n\n[](http://www.w3.org/Consortium/Legal/ipr-notice#Copyright) © 2004-2014 [W3C](http://www.w3.org/)^®^ ([MIT](http://www.csail.mit.edu/), [ERCIM](http://www.ercim.eu/), [Keio](http://www.keio.ac.jp/), [Beihang](http://ev.buaa.edu.cn/)), All Rights Reserved. W3C [](http://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer), [](http://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks) and [document use](http://www.w3.org/Consortium/Legal/copyright-documents) rules apply.\n\n* * * * *\n\nAbstract\n--------\n\nRDF Schema provides a data-modelling vocabulary for RDF data. RDF Schema is an extension of the basic RDF vocabulary.\n\nStatus of This Document\n-----------------------\n\n*This section describes the status of this document at the time of its publication. Other documents may supersede this document. A list of current W3C publications and the latest revision of this technical report can be found in the [W3C technical reports index](http://www.w3.org/TR/) at http://www.w3.org/TR/.*\n\nThis document is an edited version of the 2004 RDF Schema Recommendation. The purpose of this revision is to make this document available as part of the RDF 1.1 document set. Changes are limited to errata, revised references, terminology updates, and adaptations to the introduction. The title of the document was changed from \"RDF Vocabulary Description Language 1.0: RDF Schema\" to \"RDF Schema 1.1\". The technical content of the document is unchanged. Details of the changes are listed in the [](https://www.w3.org/TR/rdf-schema/#PER-changes) section. Since the edits to this document do not constitute a technical change the Director decided no new implementation report was required.\n\nThis document was published by the [RDF Working Group](http://www.w3.org/2011/rdf-wg/) as a Recommendation. If you wish to make comments regarding this document, please send them to <public-rdf-comments@w3.org> ([subscribe](mailto:public-rdf-comments-request@w3.org?subject=subscribe), [archives](http://lists.w3.org/Archives/Public/public-rdf-comments/)). All comments are welcome.\n\nThis document has been reviewed by W3C Members, by software developers, and by other W3C groups and interested parties, and is endorsed by the Director as a W3C Recommendation. It is a stable document and may be used as reference material or cited from another document. W3C's role in making the Recommendation is to draw attention to the specification and to promote its widespread deployment. This enhances the functionality and interoperability of the Web.\n\nThis document was produced by a group operating under the [5 February 2004 W3C Patent Policy](http://www.w3.org/Consortium/Patent-Policy-20040205/). W3C maintains a [public list of any patent disclosures](http://www.w3.org/2004/01/pp-impl/46168/status) made in connection with the deliverables of the group; that page also includes instructions for disclosing a patent. An individual who has actual knowledge of a patent which the individual believes contains [](http://www.w3.org/Consortium/Patent-Policy-20040205/#def-essential) must disclose the information in accordance with [](http://www.w3.org/Consortium/Patent-Policy-20040205/#sec-Disclosure).\n\nTable of Contents\n-----------------\n\n-   [](https://www.w3.org/TR/rdf-schema/#ch_introduction)\n-   [](https://www.w3.org/TR/rdf-schema/#ch_classes)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_resource)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_class)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_literal)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_datatype)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_langstring)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_html)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_xmlliteral)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_property)\n-   [](https://www.w3.org/TR/rdf-schema/#ch_properties)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_range)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_domain)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_type)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_subclassof)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_subpropertyof)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_label)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_comment)\n-   [](https://www.w3.org/TR/rdf-schema/#ch_domainrange)\n-   [](https://www.w3.org/TR/rdf-schema/#ch_othervocab)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_containervocab)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_container)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_bag)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_seq)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_alt)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_containermembershipproperty)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_member)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_collectionvocab)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_list)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_first)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_rest)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_nil)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_reificationvocab)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_statement)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_subject)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_predicate)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_object)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_utilvocab)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_seealso)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_isdefinedby)\n        -   [](https://www.w3.org/TR/rdf-schema/#ch_value)\n-   [](https://www.w3.org/TR/rdf-schema/#ch_summary)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_sumclasses)\n    -   [](https://www.w3.org/TR/rdf-schema/#ch_sumproperties)\n-   [](https://www.w3.org/TR/rdf-schema/#ch_acknowledgements)\n-   [](https://www.w3.org/TR/rdf-schema/#PER-changes)\n-   [](https://www.w3.org/TR/rdf-schema/#references)\n    -   [](https://www.w3.org/TR/rdf-schema/#normative-references)\n    -   [](https://www.w3.org/TR/rdf-schema/#informative-references)\n\n1\\. Introduction\n----------------\n\nRDF Schema provides a data-modelling vocabulary for RDF data. It is complemented by several companion documents which describe the basic concepts and abstract syntax of RDF [](https://www.w3.org/TR/rdf-schema/#bib-RDF11-CONCEPTS)*], the formal semantics of RDF [](https://www.w3.org/TR/rdf-schema/#bib-RDF11-MT)*], and various concrete syntaxes for RDF, such as Turtle [](https://www.w3.org/TR/rdf-schema/#bib-TURTLE)*], TriG, [](https://www.w3.org/TR/rdf-schema/#bib-TRIG)*], and JSON-LD [](https://www.w3.org/TR/rdf-schema/#bib-JSON-LD)*]. The RDF Primer [](https://www.w3.org/TR/rdf-schema/#bib-RDF11-PRIMER)*] provides an informal introduction and examples of the use of the concepts specified in this document.\n\nThis document is intended to provide a clear specification of RDF Schema to those who find the formal semantics specification [](https://www.w3.org/TR/rdf-schema/#bib-RDF11-MT)*] daunting. Thus, this document duplicates material also specified in the RDF Semantics specification. Where there is disagreement between this document and the RDF Semantics specification, the RDF Semantics specification should be taken to be correct.\n\nRDF Schema is a [](http://www.w3.org/TR/rdf11-mt/#semantic-extensions-and-entailment-regimes) of RDF. It provides mechanisms for describing groups of related resources and the relationships between these resources. RDF Schema is written in RDF using the terms described in this document. These resources are used to determine characteristics of other resources, such as the [](https://www.w3.org/TR/rdf-schema/#ch_domain) and [](https://www.w3.org/TR/rdf-schema/#ch_range) of properties.\n\nThe RDF Schema class and property system is similar to the type systems of object-oriented programming languages such as Java. RDF Schema differs from many such systems in that instead of defining a class in terms of the properties its instances may have, RDF Schema describes properties in terms of the classes of resource to which they apply. This is the role of the [](https://www.w3.org/TR/rdf-schema/#ch_domain) and [](https://www.w3.org/TR/rdf-schema/#ch_range) mechanisms described in this specification. For example, we could define the `eg:author` property to have a domain of `eg:Document` and a range of `eg:Person`, whereas a classical object oriented system might typically define a class `eg:Book` with an attribute called `eg:author` of type `eg:Person`. Using the RDF approach, it is easy for others to subsequently define additional properties with a domain of eg:`Document` or a range of `eg:Person`. This can be done without the need to re-define the original description of these classes. One benefit of the RDF property-centric approach is that it allows anyone to extend the description of existing resources, one of the architectural principles of the Web [](https://www.w3.org/TR/rdf-schema/#bib-BERNERS-LEE98)*].\n\nThis specification does not attempt to enumerate all the possible forms of representing the meaning of RDF classes and properties. Instead, the RDF Schema strategy is to acknowledge that there are many techniques through which the meaning of classes and properties can be described. Richer vocabulary or 'ontology' languages such as OWL [](https://www.w3.org/TR/rdf-schema/#bib-OWL2-OVERVIEW)*], inference rule languages and other formalisms (for example temporal logics) will each contribute to our ability to capture meaningful generalizations about data in the Web.\n\nThe language defined in this specification consists of a collection of RDF resources that can be used to describe other RDF resources in application-specific RDF vocabularies. The core vocabulary is defined in a namespace informally called `rdfs` here. That namespace is identified by the IRI\n\n> `http://www.w3.org/2000/01/rdf-schema#`\n\nand is conventionally associated with the prefix `rdfs:`. This specification also uses the prefix `rdf:` to refer to the RDF namespace\n\n> `http://www.w3.org/1999/02/22-rdf-syntax-ns#`\n\nFor convenience and readability, this specification uses an abbreviated form to represent IRIs. A name of the form prefix:suffix should be interpreted as a IRI consisting of the IRI associated with the prefix concatenated with the suffix.\n\n2\\. Classes\n-----------\n\nResources may be divided into groups called classes. The members of a class are known as *instances* of the class. Classes are themselves resources. They are often identified by [](http://www.w3.org/TR/rdf11-concepts/#section-IRIs) and may be described using RDF properties. The `[](https://www.w3.org/TR/rdf-schema/#ch_type)` property may be used to state that a resource is an instance of a class.\n\nRDF distinguishes between a class and the set of its instances. Associated with each class is a set, called the class extension of the class, which is the set of the instances of the class. Two classes may have the same set of instances but be different classes. For example, the tax office may define the class of people living at the same address as the editor of this document. The Post Office may define the class of people whose address has the same zip code as the address of the author. It is possible for these classes to have exactly the same instances, yet to have different properties. Only one of the classes has the property that it was defined by the tax office, and only the other has the property that it was defined by the Post Office.\n\nA class may be a member of its own class extension and may be an instance of itself.\n\nThe group of resources that are RDF Schema classes is itself a class called [](https://www.w3.org/TR/rdf-schema/#ch_class).\n\nIf a class C is a *subclass* of a class C', then all instances of C will also be instances of C'. The [](https://www.w3.org/TR/rdf-schema/#ch_subclassof) property may be used to state that one class is a subclass of another. The term super-class is used as the inverse of subclass. If a class C' is a super-class of a class C, then all instances of C are also instances of C'.\n\nThe RDF Concepts and Abstract Syntax [](https://www.w3.org/TR/rdf-schema/#bib-RDF11-CONCEPTS)*] specification defines the RDF concept of an [](http://www.w3.org/TR/rdf11-concepts/#section-Datatypes). All datatypes are classes. The instances of a class that is a datatype are the members of the value space of the datatype.\n\n### 2.1 rdfs:Resource\n\nAll things described by RDF are called *resources*, and are instances of the class `rdfs:Resource`. This is the class of everything. All other classes are [](https://www.w3.org/TR/rdf-schema/#def-subclass) of this class. `rdfs:Resource` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_class).\n\n### 2.2 rdfs:Class\n\nThis is the class of resources that are RDF classes. `rdfs:Class` is an instance of `rdfs:Class.`\n\n### 2.3 rdfs:Literal\n\nThe class `rdfs:Literal` is the class of [](http://www.w3.org/TR/rdf11-concepts/#section-Graph-Literal) values such as strings and integers. Property values such as textual strings are examples of RDF literals.\n\n`rdfs:Literal` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_class). rdfs:Literal is a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of [](https://www.w3.org/TR/rdf-schema/#ch_resource).\n\n### 2.4 rdfs:Datatype\n\n`rdfs:Datatype` is the class of datatypes. All instances of `rdfs:Datatype` correspond to the [](http://www.w3.org/TR/rdf11-concepts/#section-Datatypes) described in the RDF Concepts specification [](https://www.w3.org/TR/rdf-schema/#bib-RDF11-CONCEPTS)*]. `rdfs:Datatype` is both an instance of and a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of [](https://www.w3.org/TR/rdf-schema/#ch_class). Each instance of `rdfs:Datatype` is a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of rdfs:Literal.\n\n### 2.5 rdf:langString\n\nThe class `rdf:langString` is the class of [](http://www.w3.org/TR/rdf11-concepts/#dfn-language-tagged-string). `rdf:langString` is an instance of `rdfs:Datatype` and a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of [](https://www.w3.org/TR/rdf-schema/#ch_literal).\n\n### 2.6 rdf:HTML\n\n*This section is non-normative.*\n\nThe class `rdf:HTML` is the class of [](http://www.w3.org/TR/rdf11-concepts/#section-html). `rdf:HTML` is an instance of `rdfs:Datatype` and a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of [](https://www.w3.org/TR/rdf-schema/#ch_literal).\n\n### 2.7 rdf:XMLLiteral\n\n*This section is non-normative.*\n\nThe class `rdf:XMLLiteral` is the class of [](http://www.w3.org/TR/rdf11-concepts/#section-XMLLiteral). `rdf:XMLLiteral` is an instance of `rdfs:Datatype` and a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of [](https://www.w3.org/TR/rdf-schema/#ch_literal).\n\n### 2.8 rdf:Property\n\n`rdf:Property` is the class of RDF properties. `rdf:Property` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_class).\n\n3\\. Properties\n--------------\n\nThe RDF Concepts and Abstract Syntax specification [](https://www.w3.org/TR/rdf-schema/#bib-RDF11-CONCEPTS)*] describes the concept of an RDF property as a relation between subject resources and object resources.\n\nThis specification defines the concept of subproperty. The [](https://www.w3.org/TR/rdf-schema/#ch_subclassof) property may be used to state that one property is a subproperty of another. If a property P is a subproperty of property P', then all pairs of resources which are related by P are also related by P'. The term super-property is often used as the inverse of subproperty. If a property P' is a super-property of a property P, then all pairs of resources which are related by P are also related by P'. This specification does not define a top property that is the super-property of all properties.\n\nNOTE\n\nThe basic facilities provided by [](https://www.w3.org/TR/rdf-schema/#ch_domain) and [](https://www.w3.org/TR/rdf-schema/#ch_range) do not provide any direct way to indicate property restrictions that are local to a class. Although it is possible to combine use [](https://www.w3.org/TR/rdf-schema/#ch_domain) and [](https://www.w3.org/TR/rdf-schema/#ch_range) with sub-property hierarchies, direct support for such declarations are provided by richer Web Ontology languages such as OWL [](https://www.w3.org/TR/rdf-schema/#bib-OWL2-OVERVIEW)*].\n\n### 3.1 rdfs:range\n\n`rdfs:range` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that is used to state that the values of a property are instances of one or more classes.\n\nThe triple\n\n> `P rdfs:range C`\n\nstates that P is an instance of the class [](https://www.w3.org/TR/rdf-schema/#ch_property), that C is an instance of the class [](https://www.w3.org/TR/rdf-schema/#ch_class) and that the resources denoted by the objects of triples whose predicate is P are instances of the class C.\n\nWhere P has more than one rdfs:range property, then the resources denoted by the objects of triples with predicate P are instances of all the classes stated by the `rdfs:range` properties.\n\nThe `rdfs:range` property can be applied to itself. The rdfs:range of `rdfs:range` is the class [](https://www.w3.org/TR/rdf-schema/#ch_class). This states that any resource that is the value of an `rdfs:range` property is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_class).\n\nThe `rdfs:range` property is applied to properties. This can be represented in RDF using the [](https://www.w3.org/TR/rdf-schema/#ch_domain) property. The [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdfs:range` is the class [](https://www.w3.org/TR/rdf-schema/#ch_property). This states that any resource with an `rdfs:range` property is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)`.\n\n### 3.2 rdfs:domain\n\n`rdfs:domain` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that is used to state that any resource that has a given property is an instance of one or more classes.\n\nA triple of the form:\n\n> `P rdfs:domain C`\n\nstates that P is an instance of the class `[](https://www.w3.org/TR/rdf-schema/#ch_property)`, that C is a instance of the class `[](https://www.w3.org/TR/rdf-schema/#ch_class)` and that the resources denoted by the subjects of triples whose predicate is P are instances of the class C.\n\nWhere a property P has more than one rdfs:domain property, then the resources denoted by subjects of triples with predicate P are instances of all the classes stated by the `rdfs:domain` properties.\n\nThe `rdfs:domain` property may be applied to itself. The rdfs:domain of `rdfs:domain` is the class [](https://www.w3.org/TR/rdf-schema/#ch_property). This states that any resource with an `rdfs:domain` property is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)`.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_range) of `rdfs:domain` is the class `[](https://www.w3.org/TR/rdf-schema/#ch_class)`. This states that any resource that is the value of an `rdfs:domain` property is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_class)`.\n\n### 3.3 rdf:type\n\n`rdf:type` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that is used to state that a resource is an instance of a class.\n\nA triple of the form:\n\n> `R rdf:type C`\n\nstates that C is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_class) and R is an instance of C.\n\nThe `[](https://www.w3.org/TR/rdf-schema/#ch_domain)` of `rdf:type` is [](https://www.w3.org/TR/rdf-schema/#ch_resource). The [](https://www.w3.org/TR/rdf-schema/#ch_range) of rdf:type is [](https://www.w3.org/TR/rdf-schema/#ch_class).\n\n### 3.4 rdfs:subClassOf\n\nThe property `rdfs:subClassOf` is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)` that is used to state that all the instances of one class are instances of another.\n\nA triple of the form:\n\n> `C1 rdfs:subClassOf C2`\n\nstates that C1 is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_class)`, C2 is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_class)` and C1 is a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of C2. The `rdfs:subClassOf` property is transitive.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdfs:subClassOf` is `[](https://www.w3.org/TR/rdf-schema/#ch_class)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of `rdfs:subClassOf` is [](https://www.w3.org/TR/rdf-schema/#ch_class).\n\n### 3.5 rdfs:subPropertyOf\n\nThe property `rdfs:subPropertyOf` is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)` that is used to state that all resources related by one property are also related by another.\n\nA triple of the form:\n\n> `P1 rdfs:subPropertyOf P2`\n\nstates that P1 is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)`, P2 is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)` and P1 is a [](https://www.w3.org/TR/rdf-schema/#def-subproperty) of P2. The `rdfs:subPropertyOf` property is transitive.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdfs:subPropertyOf` is `[](https://www.w3.org/TR/rdf-schema/#ch_property)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of rdfs:subPropertyOf is [](https://www.w3.org/TR/rdf-schema/#ch_property).\n\n### 3.6 rdfs:label\n\n`rdfs:label` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that may be used to provide a human-readable version of a resource's name.\n\nA triple of the form:\n\n> `R rdfs:label L`\n\nstates that L is a human readable label for R.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdfs:label` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of rdfs:label is [](https://www.w3.org/TR/rdf-schema/#ch_literal).\n\nMultilingual labels are supported using the [](http://www.w3.org/TR/rdf11-concepts/#section-Graph-Literal) facility of RDF literals.\n\n### 3.7 rdfs:comment\n\n`rdfs:comment` is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)` that may be used to provide a human-readable description of a resource.\n\nA triple of the form:\n\n> `R rdfs:comment L`\n\nstates that L is a human readable description of R.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdfs:comment` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of rdfs:comment is [](https://www.w3.org/TR/rdf-schema/#ch_literal).\n\nA textual comment helps clarify the meaning of RDF classes and properties. Such in-line documentation complements the use of both formal techniques (Ontology and rule languages) and informal (prose documentation, examples, test cases). A variety of documentation forms can be combined to indicate the intended meaning of the classes and properties described in an RDF vocabulary. Since RDF vocabularies are expressed as RDF graphs, vocabularies defined in other namespaces may be used to provide richer documentation.\n\nMultilingual documentation is supported through use of the [](http://www.w3.org/TR/rdf11-concepts/#section-Graph-Literal) facility of RDF literals.\n\n4\\. Using the Domain and Range vocabulary\n-----------------------------------------\n\n*This section is non-normative.*\n\nThis specification introduces an RDF vocabulary for describing the meaningful use of properties and classes in RDF data. For example, an RDF vocabulary might describe limitations on the types of values that are appropriate for some property, or on the classes to which it makes sense to ascribe such properties.\n\nRDF Schema provides a mechanism for describing this information, but does not say whether or how an application should use it. For example, while an RDF vocabulary can assert that an `author` property is used to indicate resources that are instances of the class `Person`, it does not say whether or how an application should act in processing that range information. Different applications will use this information in different ways. For example, data checking tools might use this to help discover errors in some data set, an interactive editor might suggest appropriate values, and a reasoning application might use it to infer additional information from instance data.\n\nRDF vocabularies can describe relationships between vocabulary items from multiple independently developed vocabularies. Since IRIs are used to identify classes and properties on the Web, it is possible to create new properties that have a `domain` or `range` whose value is a class defined in another namespace.\n\n5\\. Other vocabulary\n--------------------\n\nAdditional classes and properties, including constructs for representing containers and RDF statements, and for deploying RDF vocabulary descriptions in the World Wide Web, are defined in this section.\n\n### 5.1 Container Classes and Properties\n\n*This section is non-normative.*\n\nRDF containers are resources that are used to represent collections. The same resource may appear in a container more than once. Unlike containment in the physical world, a container may be contained in itself.\n\nThree different kinds of container are defined. Whilst the formal semantics [](https://www.w3.org/TR/rdf-schema/#bib-RDF11-MT)*] of all three classes of container are identical, different classes may be used to indicate informally further information. An rdf:Bag is used to indicate that the container is intended to be unordered. An rdf:Seq is used to indicate that the order indicated by the numerical order of the [](https://www.w3.org/TR/rdf-schema/#ch_containermembershipproperty) of the container is intended to be significant. An rdf:Alt container is used to indicate that typical processing of the container will be to select one of the members.\n\nJust as a hen house may have the property that it is made of wood, that does not mean that all the hens it contains are made of wood, a property of a container is not necessarily a property of all of its members.\n\nRDF containers are defined by the following classes and properties.\n\n#### 5.1.1 rdfs:Container\n\nThe `rdfs:Container` class is a super-class of the RDF Container classes, i.e. `[](https://www.w3.org/TR/rdf-schema/#ch_bag)`, `[](https://www.w3.org/TR/rdf-schema/#ch_seq)`, `[](https://www.w3.org/TR/rdf-schema/#ch_alt)`.\n\n#### 5.1.2 rdf:Bag\n\nThe `rdf:Bag` class is the class of RDF 'Bag' containers. It is a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of `[](https://www.w3.org/TR/rdf-schema/#ch_container)`. Whilst formally it is no different from an `[](https://www.w3.org/TR/rdf-schema/#ch_seq)` or an `[](https://www.w3.org/TR/rdf-schema/#ch_alt)`, the `rdf:Bag` class is used conventionally to indicate to a human reader that the container is intended to be unordered.\n\n#### 5.1.3 rdf:Seq\n\nThe `rdf:Seq` class is the class of RDF 'Sequence' containers. It is a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of `[](https://www.w3.org/TR/rdf-schema/#ch_container)`. Whilst formally it is no different from an `[](https://www.w3.org/TR/rdf-schema/#ch_bag)` or an `[](https://www.w3.org/TR/rdf-schema/#ch_alt)`, the `rdf:Seq` class is used conventionally to indicate to a human reader that the numerical ordering of the [](https://www.w3.org/TR/rdf-schema/#ch_containermembershipproperty) of the container is intended to be significant.\n\n#### 5.1.4 rdf:Alt\n\nThe `rdf:Alt` class is the class of RDF 'Alternative' containers. It is a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of `[](https://www.w3.org/TR/rdf-schema/#ch_container)`. Whilst formally it is no different from an `[](https://www.w3.org/TR/rdf-schema/#ch_seq)` or an `[](https://www.w3.org/TR/rdf-schema/#ch_bag)`, the `rdf:Alt` class is used conventionally to indicate to a human reader that typical processing will be to select one of the members of the container. The first member of the container, i.e. the value of the `[](https://www.w3.org/TR/rdf-schema/#ch_containermembershipproperty)` property, is the default choice.\n\n#### 5.1.5 rdfs:ContainerMembershipProperty\n\nThe `rdfs:ContainerMembershipProperty` class has as instances the properties `rdf:_1, rdf:_2, rdf:_3 ...` that are used to state that a resource is a member of a container. `rdfs:ContainerMembershipProperty` is a [](https://www.w3.org/TR/rdf-schema/#def-subclass) of [](https://www.w3.org/TR/rdf-schema/#ch_property). Each instance of `rdfs:ContainerMembershipProperty` is an [](https://www.w3.org/TR/rdf-schema/#ch_subpropertyof) the `[](https://www.w3.org/TR/rdf-schema/#ch_member)` property.\n\nGiven a container C, a triple of the form:\n\n> `C rdf:_nnn O`\n\nwhere `nnn` is the decimal representation of an integer greater than 0 with no leading zeros, states that O is a member of the container C.\n\nContainer membership properties may be applied to resources other than containers.\n\n#### 5.1.6 rdfs:member\n\n`rdfs:member` is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)` that is a super-property of all the container membership properties i.e. each container membership property has an [](https://www.w3.org/TR/rdf-schema/#ch_subpropertyof) relationship to the property `rdfs:member`.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdfs:member` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of `rdfs:member` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`.\n\n### 5.2 RDF Collections\n\n*This section is non-normative.*\n\nRDF containers are open in the sense that the core RDF specifications define no mechanism to state that there are no more members. The RDF Collection vocabulary of classes and properties can describe a closed collection, i.e. one that can have no more members.\n\nA collection is represented as a list of items, a representation that will be familiar to those with experience of Lisp and similar programming languages. There is a [](http://www.w3.org/TR/turtle/#collections) in the Turtle syntax specification for representing collections.\n\nNOTE\n\nRDFS does not require that there be only one first element of a list-like structure, or even that a list-like structure have a first element.\n\n#### 5.2.1 rdf:List\n\n`rdf:List` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_class) that can be used to build descriptions of lists and other list-like structures.\n\n#### 5.2.2 rdf:first\n\n`rdf:first` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that can be used to build descriptions of lists and other list-like structures.\n\nA triple of the form:\n\n> `L rdf:first O`\n\nstates that there is a first-element relationship between L and O.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdf:first` is `[](https://www.w3.org/TR/rdf-schema/#ch_list)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of `rdf:first` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`.\n\n#### 5.2.3 rdf:rest\n\n`rdf:rest` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that can be used to build descriptions of lists and other list-like structures.\n\nA triple of the form:\n\n> `L rdf:rest O`\n\nstates that there is a rest-of-list relationship between L and O.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdf:rest` is `[](https://www.w3.org/TR/rdf-schema/#ch_list)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of `rdf:rest` is `[](https://www.w3.org/TR/rdf-schema/#ch_list)`.\n\n#### 5.2.4 rdf:nil\n\nThe resource `rdf:nil` is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_list)` that can be used to represent an empty list or other list-like structure.\n\nA triple of the form:\n\n> `L rdf:rest rdf:nil`\n\nstates that L is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_list)` that has one item; that item can be indicated using the `[](https://www.w3.org/TR/rdf-schema/#ch_first)` property.\n\n### 5.3 Reification Vocabulary\n\n*This section is non-normative.*\n\n#### 5.3.1 rdf:Statement\n\n`rdf:Statement` is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_class)` It is intended to represent the class of RDF statements. An RDF statement is the statement made by a token of an RDF triple. The subject of an RDF statement is the instance of `[](https://www.w3.org/TR/rdf-schema/#ch_resource)` identified by the subject of the triple. The predicate of an RDF statement is the instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)` identified by the predicate of the triple. The object of an RDF statement is the instance of `[](https://www.w3.org/TR/rdf-schema/#ch_resource)` identified by the object of the triple. `rdf:Statement` is in the domain of the properties `[](https://www.w3.org/TR/rdf-schema/#ch_predicate)`, `[](https://www.w3.org/TR/rdf-schema/#ch_subject)` and `[](https://www.w3.org/TR/rdf-schema/#ch_object)`. Different individual `rdf:Statement` instances may have the same values for their `[](https://www.w3.org/TR/rdf-schema/#ch_predicate)`, `[](https://www.w3.org/TR/rdf-schema/#ch_subject)` and `[](https://www.w3.org/TR/rdf-schema/#ch_object)` properties.\n\n#### 5.3.2 rdf:subject\n\n`rdf:subject` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that is used to state the subject of a statement.\n\nA triple of the form:\n\n> `S rdf:subject R`\n\nstates that S is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_statement)` and that the subject of S is R.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdf:subject` is `[](https://www.w3.org/TR/rdf-schema/#ch_statement)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of `rdf:subject` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`.\n\n#### 5.3.3 rdf:predicate\n\nrdf:predicate is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that is used to state the predicate of a statement.\n\nA triple of the form:\n\n> `S rdf:predicate P`\n\nstates that S is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_statement)`, that P is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_property)` and that the predicate of S is P.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdf:predicate` is `[](https://www.w3.org/TR/rdf-schema/#ch_statement)` and the [](https://www.w3.org/TR/rdf-schema/#ch_range) is [](https://www.w3.org/TR/rdf-schema/#ch_resource).\n\n#### 5.3.4 rdf:object\n\nrdf:object is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that is used to state the object of a statement.\n\nA triple of the form:\n\n> `S rdf:object O`\n\nstates that S is an instance of `[](https://www.w3.org/TR/rdf-schema/#ch_statement)` and that the object of S is O.\n\nThe [](https://www.w3.org/TR/rdf-schema/#ch_domain) of `rdf:object` is `[](https://www.w3.org/TR/rdf-schema/#ch_statement)`. The [](https://www.w3.org/TR/rdf-schema/#ch_range) of `rdf:object` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`.\n\n### 5.4 Utility Properties\n\nThe following utility classes and properties are defined in the RDF core namespaces.\n\n#### 5.4.1 rdfs:seeAlso\n\n`rdfs:seeAlso` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that is used to indicate a resource that might provide additional information about the subject resource.\n\nA triple of the form:\n\n> `S rdfs:seeAlso O`\n\nstates that the resource O may provide additional information about S. It may be possible to retrieve representations of O from the Web, but this is not required. When such representations may be retrieved, no constraints are placed on the format of those representations.\n\nThe `[](https://www.w3.org/TR/rdf-schema/#ch_domain)` of `rdfs:seeAlso` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`. The `[](https://www.w3.org/TR/rdf-schema/#ch_range)` of `rdfs:seeAlso` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`.\n\n#### 5.4.2 rdfs:isDefinedBy\n\n`rdfs:isDefinedBy` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that is used to indicate a resource defining the subject resource. This property may be used to indicate an RDF vocabulary in which a resource is described.\n\nA triple of the form:\n\n> `S rdfs:isDefinedBy O`\n\nstates that the resource O defines S. It may be possible to retrieve representations of O from the Web, but this is not required. When such representations may be retrieved, no constraints are placed on the format of those representations. `rdfs:isDefinedBy` is a [](https://www.w3.org/TR/rdf-schema/#def-subproperty) of `[](https://www.w3.org/TR/rdf-schema/#ch_seealso)`.\n\nThe `[](https://www.w3.org/TR/rdf-schema/#ch_domain)` of `rdfs:isDefinedBy` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`. The `[](https://www.w3.org/TR/rdf-schema/#ch_range)` of `rdfs:isDefinedBy` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`.\n\n#### 5.4.3 rdf:value\n\n`rdf:value` is an instance of [](https://www.w3.org/TR/rdf-schema/#ch_property) that may be used in describing structured values.\n\nrdf:value has no meaning on its own. It is provided as a piece of vocabulary that may be used in idioms such as illustrated in example below:\n\nEXAMPLE 1\n\n<http://www.example.com/2002/04/products#item10245>\n    <http://www.example.org/terms/weight> [\n       rdf:value 2.4 ;\n       <http://www.example.org/terms/units> <http://www.example.org/units/kilograms>\n       ] .\n\nDespite the lack of formal specification of the meaning of this property, there is value in defining it to encourage the use of a common idiom in examples of this kind.\n\nThe `[](https://www.w3.org/TR/rdf-schema/#ch_domain)` of `rdf:value` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`. The `[](https://www.w3.org/TR/rdf-schema/#ch_range)` of `rdf:value` is `[](https://www.w3.org/TR/rdf-schema/#ch_resource)`.\n\n6\\. RDF Schema summary\n----------------------\n\n*This section is non-normative.*\n\nThe tables in this section provide an overview of the RDF Schema vocabulary.\n\n### 6.1 RDF classes\n\n| Class name | comment |\n| [](https://www.w3.org/TR/rdf-schema/#ch_resource) | The class resource, everything. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_literal) | The class of literal values, e.g. textual strings and integers. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_langstring) | The class of language-tagged string literal values. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_html) | The class of HTML literal values. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_xmlliteral) | The class of XML literal values. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_class) | The class of classes. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_property) | The class of RDF properties. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_datatype) | The class of RDF datatypes. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_statement) | The class of RDF statements. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_bag) | The class of unordered containers. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_seq) | The class of ordered containers. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_alt) | The class of containers of alternatives. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_container) | The class of RDF containers. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_containermembershipproperty) | The class of container membership properties, rdf:_1, rdf:_2, ..., all of which are sub-properties of 'member'. |\n| [](https://www.w3.org/TR/rdf-schema/#ch_list) | The class of RDF Lists. |\n\n### 6.2 RDF properties\n\n| Property name | comment | domain | range |\n| [](https://www.w3.org/TR/rdf-schema/#ch_type) | The subject is an instance of a class. | rdfs:Resource | rdfs:Class |\n| [](https://www.w3.org/TR/rdf-schema/#ch_subclassof) | The subject is a subclass of a class. | rdfs:Class | rdfs:Class |\n| [](https://www.w3.org/TR/rdf-schema/#ch_subpropertyof) | The subject is a subproperty of a property. | rdf:Property | rdf:Property |\n| [](https://www.w3.org/TR/rdf-schema/#ch_domain) | A domain of the subject property. | rdf:Property | rdfs:Class |\n| [](https://www.w3.org/TR/rdf-schema/#ch_range) | A range of the subject property. | rdf:Property | rdfs:Class |\n| [](https://www.w3.org/TR/rdf-schema/#ch_label) | A human-readable name for the subject. | rdfs:Resource | rdfs:Literal |\n| [](https://www.w3.org/TR/rdf-schema/#ch_comment) | A description of the subject resource. | rdfs:Resource | rdfs:Literal |\n| [](https://www.w3.org/TR/rdf-schema/#ch_member) | A member of the subject resource. | rdfs:Resource | rdfs:Resource |\n| [](https://www.w3.org/TR/rdf-schema/#ch_first) | The first item in the subject RDF list. | rdf:List | rdfs:Resource |\n| [](https://www.w3.org/TR/rdf-schema/#ch_rest) | The rest of the subject RDF list after the first item. | rdf:List | rdf:List |\n| [](https://www.w3.org/TR/rdf-schema/#ch_seealso) | Further information about the subject resource. | rdfs:Resource | rdfs:Resource |\n| [](https://www.w3.org/TR/rdf-schema/#ch_isdefinedby) | The definition of the subject resource. | rdfs:Resource | rdfs:Resource |\n| [](https://www.w3.org/TR/rdf-schema/#ch_value) | Idiomatic property used for structured values. | rdfs:Resource | rdfs:Resource |\n| [](https://www.w3.org/TR/rdf-schema/#ch_subject) | The subject of the subject RDF statement. | rdf:Statement | rdfs:Resource |\n| [](https://www.w3.org/TR/rdf-schema/#ch_predicate) | The predicate of the subject RDF statement. | rdf:Statement | rdfs:Resource |\n| [](https://www.w3.org/TR/rdf-schema/#ch_object) | The object of the subject RDF statement. | rdf:Statement | rdfs:Resource |\n\nIn addition to these classes and properties, RDF also uses properties called `rdf:_1`, `rdf:_2`, `rdf:_3`... etc., each of which is both a sub-property of `rdfs:member` and an instance of the class `rdfs:ContainerMembershipProperty`. There is also an instance of `rdf:List` called `rdf:nil` that is an empty `rdf:List`.\n\nA. Acknowledgments\n------------------\n\n*This section is non-normative.*\n\nThe RDF Schema design was originally produced by the RDF Schema Working Group (1997-2000). The current specification is largely an editorial clarification of that design, and has benefited greatly from the hard work of the [RDF Core Working Group](http://www.w3.org/2001/sw/RDFCore/) [](http://www.w3.org/2001/sw/RDFCore/#Membership), and from implementation feedback from many members of the [RDF Interest Group](http://www.w3.org/RDF/Interest/). In 2013-2014 Guus Schreiber edited this document on behalf of the [RDF Working Group](http://www.w3.org/2011/rdf-wg/) to bring it in line with the RDF 1.1 specifications.\n\nDavid Singer of IBM was the chair of the original RDF Schema group throughout most of the development of this specification; we thank David for his efforts and thank IBM for supporting him and us in this endeavor. Particular thanks are also due to Andrew Layman for his editorial work on early versions of this specification.\n\nThe original RDF Schema Working Group membership included:\n\nNick Arnett (Verity), Dan Brickley (ILRT / University of Bristol), Walter Chang (Adobe), Sailesh Chutani (Oracle), Ron Daniel (DATAFUSION), Charles Frankston (Microsoft), Joe Lapp (webMethods Inc.), Patrick Gannon (CommerceNet), RV Guha (Epinions, previously of Netscape Communications), Tom Hill (Apple Computer), Renato Iannella (DSTC), Sandeep Jain (Oracle), Kevin Jones, (InterMind), Emiko Kezuka (Digital Vision Laboratories), Ora Lassila (Nokia Research Center), Andrew Layman (Microsoft), John McCarthy (Lawrence Berkeley National Laboratory), Michael Mealling (Network Solutions), Norbert Mikula (DataChannel), Eric Miller (OCLC), Frank Olken (Lawrence Berkeley National Laboratory), Sri Raghavan (Digital/Compaq), Lisa Rein (webMethods Inc.), Tsuyoshi Sakata (Digital Vision Laboratories), Leon Shklar (Pencom Web Works), David Singer (IBM), Wei (William) Song (SISU), Neel Sundaresan (IBM), Ralph Swick (W3C), Naohiko Uramoto (IBM), Charles Wicksteed (Reuters Ltd.), Misha Wolf (Reuters Ltd.)\n\nB. Change since 2004 Recommendation\n-----------------------------------\n\n*This section is non-normative.*\n\nChanges for RDF 1.1 Recommendation\n\n-   No changes.\n\nChanges for RDF 1.1 Proposed Edited Recommendation\n\n-   Conversion to ReSpec, including formatting of examples and notes.\n-   References to RDF 1.0 documents where appropriate replaced by references to RDF 1.1 documents.\n-   Replaced the term \"URI Reference\" with the term \"IRI\".\n-   Removed discussion about distinction between plain and typed literals, as this distinction is absent in RDF 1.1 and has no technical bearing on RDF Schema.\n-   Removed the introductory paragraph of Sec. [](https://www.w3.org/TR/rdf-schema/#ch_reificationvocab), as this discussion is not related to the technical content and is irrelevant and confusing now.\n-   Update of affiliation of the editors.\n-   Added RDF WG to the Acknowledgements section.\n-   Renamed the document from \"RDF Vocabulary Description Language 1.0: RDF Schema\" to \"RDF Schema 1.1\", as the term Vocabulary Description Language has led to confusion.\n-   Three paragraphs of the Introduction were left out. These paragraphs described the things that RDF Schema does not do and are now much less relevant than in 2004.\n-   Added the datatypes `rdf:langString` and `rdf:HTML`.\n-   Removed Appendix \"RDF Schema in RDF/XML\". It was informative, but now out of date, in terms of content and in terms of syntax.\n-   Marked `rdf:HTML` and `rdf:XMLLiteral` as non-normative.\n-   Removed references to 2004 Primer from Secs. 5.1, 5.2 and 5.4.3. In the latter case the example referred to was moved into this document for readability purposes.\n\nC. References\n-------------\n\n### C.1 Normative references\n\n[JSON-LD]\n\nManu Sporny, Gregg Kellogg, Markus Lanthaler, Editors. *[JSON-LD 1.0](http://www.w3.org/TR/json-ld/)*. 16 January 2014. W3C Recommendation. URL: <http://www.w3.org/TR/json-ld/>\n\n[RDF11-CONCEPTS]\n\nRichard Cyganiak, David Wood, Markus Lanthaler. *[RDF 1.1 Concepts and Abstract Syntax.](http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/)* W3C Recommendation, 25 February 2014. URL: <http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/>. The latest edition is available at <http://www.w3.org/TR/rdf11-concepts/>\n\n[RDF11-MT]\n\nPatrick J. Hayes, Peter F. Patel-Schneider. *[RDF 1.1 Semantics.](http://www.w3.org/TR/2014/REC-rdf11-mt-20140225/)* W3C Recommendation, 25 February 2014. URL: <http://www.w3.org/TR/2014/REC-rdf11-mt-20140225/>. The latest edition is available at <http://www.w3.org/TR/rdf11-mt/>\n\n[TRIG]\n\nGavin Carothers, Andy Seaborne. *[TriG: RDF Dataset Language](http://www.w3.org/TR/2014/REC-trig-20140225/)*. W3C Recommendation, 25 February 2014. URL: <http://www.w3.org/TR/2014/REC-trig-20140225/>. The latest edition is available at <http://www.w3.org/TR/trig/>\n\n[TURTLE]\n\nEric Prud'hommeaux, Gavin Carothers. *[RDF 1.1 Turtle: Terse RDF Triple Language.](http://www.w3.org/TR/2014/REC-turtle-20140225/)* W3C Recommendation, 25 February 2014. URL: <http://www.w3.org/TR/2014/REC-turtle-20140225/>. The latest edition is available at <http://www.w3.org/TR/turtle/>\n\n### C.2 Informative references\n\n[BERNERS-LEE98]\n\nTim Berners-Lee. *[What the Semantic Web can represent](http://www.w3.org/DesignIssues/RDFnot.html)*. 1998. URI: <http://www.w3.org/DesignIssues/RDFnot.html>.\n\n[OWL2-OVERVIEW]\n\nW3C OWL Working Group. [*OWL 2 Web Ontology Language Document Overview (Second Edition)*](Second%20Edition)*). 11 December 2012. W3C Recommendation. URL: <http://www.w3.org/TR/owl2-overview/>\n\n[RDF11-PRIMER]\n\nGuus Schreiber, Yves Raimond. *[RDF 1.1 Primer](http://www.w3.org/TR/2014/NOTE-rdf11-primer-20140225/)*. W3C Working Group Note, 25 February 2014. The latest version is available at <http://www.w3.org/TR/rdf11-primer/>."},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","title":"SKOS"},"frontmatter":{"draft":false},"rawBody":"https://www.w3.org/TR/2008/WD-skos-reference-20080829/skos.html"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","title":"SOIC"},"frontmatter":{"draft":false},"rawBody":"http://rdfs.org/sioc/spec/"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","title":"Sitemap"},"frontmatter":{"draft":false},"rawBody":"Sitemap.rdf is an update to the Sitemap.xml file.\n\n```\n\n@prefix sitemap: <http://www.sitemaps.org/schemas/sitemap/0.9/> .\n@prefix xhtml: <http://www.w3.org/1999/xhtml/> .\n\n<http://example.com/sitemap.rdf> a sitemap:Sitemap ;\n  sitemap:url [\n    a sitemap:Url ;\n    sitemap:loc <http://example.com/> ;\n    sitemap:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime ;\n    sitemap:changefreq sitemap:weekly ;\n    sitemap:priority 1.0\n  ] .\n\n<http://example.com/sitemap.rdf> a sitemap:Sitemap ;\n  sitemap:url [\n    a sitemap:Url ;\n    sitemap:loc <http://example.com/about> ;\n    sitemap:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime ;\n    sitemap:changefreq sitemap:weekly ;\n    sitemap:priority 0.8\n  ] .\n\n@prefix sitemap: <http://www.sitemaps.org/schemas/sitemap/0.9/> .\n@prefix xhtml: <http://www.w3.org/1999/xhtml/> .\n\n<http://example.com/sitemap.rdf> a sitemap:Sitemap ;\n  sitemap:url [\n    a sitemap:Url ;\n    sitemap:loc <http://example.com/> ;\n    sitemap:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime ;\n    sitemap:changefreq sitemap:weekly ;\n    sitemap:priority 1.0 ;\n    sitemap:allow true\n  ] .\n\n<http://example.com/sitemap.rdf> a sitemap:Sitemap ;\n  sitemap:url [\n    a sitemap:Url ;\n    sitemap:loc <http://example.com/> ;\n    sitemap:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime ;\n    sitemap:changefreq sitemap:weekly ;\n    sitemap:priority 1.0 ;\n    sitemap:allow true\n  ] .\n\n<http://example.com/sitemap.rdf> a sitemap:Sitemap ;\n  sitemap:url [\n    a sitemap:Url ;\n    sitemap:loc <http://example.com/about> ;\n    sitemap:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime ;\n    sitemap:changefreq sitemap:weekly ;\n    sitemap:priority 0.8 ;\n    sitemap:allow true\n  ] .\n\n<http://example.com/sitemap.rdf> a sitemap:Sitemap ;\n  sitemap:url [\n    a sitemap:Url ;\n    sitemap:loc <http://example.com/private> ;\n    sitemap:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime ;\n    sitemap:changefreq sitemap:weekly ;\n    sitemap:priority 0.5 ;\n    sitemap:allow false\n  ] .\n\n\n\n```\n\nRobots.txt\n\n```\n\n@prefix robots: <http://www.robotstxt.org/ontology/robots#> .\n\n<http://example.com/robots.rdf> a robots:RobotsFile ;\n  robots:rule [\n    a robots:Rule ;\n    robots:userAgent \"Googlebot\" ;\n    robots:disallow \"/private\"\n  ] .\n\n<http://example.com/robots.rdf> a robots:RobotsFile ;\n  robots:rule [\n    a robots:Rule ;\n    robots:userAgent \"*\" ;\n    robots:allow \"/public\"\n  ] .\n\n```\n\nThis example defines a `robots.txt` file for a website with two rules. The first rule specifies that the Googlebot should not crawl the `/private` directory. The second rule specifies that all other user agents are allowed to crawl the `/public` directory.\n\nYou can add additional rules as needed by adding more `robots:rule` statements to the `robots:RobotsFile`. For example, you could add a rule that disallows all user agents from crawling a `/sensitive` directory, like this:\n\n```\n\n@prefix robots: <http://www.robotstxt.org/ontology/robots#> .\n\n<http://example.com/robots.rdf> a robots:RobotsFile ;\n  robots:rule [\n    a robots:Rule ;\n    robots:userAgent \"Googlebot\" ;\n    robots:disallow \"/private\"\n  ] .\n\n<http://example.com/robots.rdf> a robots:RobotsFile ;\n  robots:rule [\n    a robots:Rule ;\n    robots:userAgent \"*\" ;\n    robots:allow \"/public\"\n  ] .\n\n<http://example.com/robots.rdf> a robots:RobotsFile ;\n  robots:rule [\n    a robots:Rule ;\n    robots:userAgent \"*\" ;\n    robots:disallow \"/sensitive\"\n  ] .\n\n```\n\nSiteIndex\n\n```\n@prefix siteindex: <http://www.siteindex.org/ontology/siteindex#> .\n@prefix xhtml: <http://www.w3.org/1999/xhtml/> .\n\n<http://example.com/siteindex.rdf> a siteindex:SiteIndex ;\n  siteindex:url [\n    a siteindex:Url ;\n    siteindex:loc <http://example.com/> ;\n    siteindex:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime\n  ] .\n\n<http://example.com/siteindex.rdf> a siteindex:SiteIndex ;\n  siteindex:url [\n    a siteindex:Url ;\n    siteindex:loc <http://example.com/about> ;\n    siteindex:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime\n  ] .\n\n\n```\nYou can add additional URLs to the `siteindex` by adding more `siteindex:url` statements to the `siteindex:SiteIndex`. For example, you could add a page for contact information like this:\n\n```\n@prefix siteindex: <http://www.siteindex.org/ontology/siteindex#> .\n@prefix xhtml: <http://www.w3.org/1999/xhtml/> .\n\n<http://example.com/siteindex.rdf> a siteindex:SiteIndex ;\n  siteindex:url [\n    a siteindex:Url ;\n    siteindex:loc <http://example.com/> ;\n    siteindex:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime\n  ] .\n\n<http://example.com/siteindex.rdf> a siteindex:SiteIndex ;\n  siteindex:url [\n    a siteindex:Url ;\n    siteindex:loc <http://example.com/about> ;\n    siteindex:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime\n  ] .\n\n<http://example.com/siteindex.rdf> a siteindex:SiteIndex ;\n  siteindex:url [\n    a siteindex:Url ;\n    siteindex:loc <http://example.com/contact> ;\n    siteindex:lastmod \"2022-12-28T12:00:00+00:00\"^^xsd:dateTime\n  ] .\n```\n\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","title":"WebID-OIDC"},"frontmatter":{"draft":false},"rawBody":"\nOlder work was called [WebID-OIDC](https://github.com/solid/webid-oidc-spec) whilst the newer work appears to be called [solid-oidc](https://solidproject.org/TR/oidc). \n(the name change 'games' are annoying, imo)\n\nIn anycase; "},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","title":"WebID-RSA"},"frontmatter":{"draft":false},"rawBody":"Below is a copy of an outline provided on github; the [Sourcelink](https://github.com/solid/solid/blob/main/proposals/auth-webid-rsa.md) is provided below.\n\nWebID-RSA is somewhat similar to WebID-TLS, in that a public RSA key is\npublished in the WebID profile, and the user will sign a token with the\ncorresponding private key that matches the public key in the profile.\n\nThe client receives a secure token from the server, which it signs and then\nsends back to the server. The implementation of WebID-RSA is similar to [Digest\naccess authentication](https://tools.ietf.org/html/rfc2617) in HTTP, in that it\nreuses the same headers.\n\nHere is a step by step example that covers the authentication handshake.\n\nFirst, the client attempts to access a protected resource at\n`https://example.org/data/`.\n\nREQUEST:\n\n```\nGET /data/ HTTP/1.1\nHost: example.org\n```\n\nRESPONSE:\n\n```\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: WebID-RSA source=\"example.org\", nonce=\"securestring\"\n```\n\nNext, the client sets the username value to the user's WebID and signs the\n`SHA1` hash of the concatenated value of **source + username + nonce** before\nresending the request. The signature must use the `PKCS1v15` standard and it\nmust be `base64` encoded.\n\nIt is important that clients return the proper source value they received from\nthe server, in order to avoid man-in-the-middle attacks. Also note that the\nserver must send its own URI (**source**) together with the token, otherwise a\n[MitM](https://en.wikipedia.org/wiki/Man-in-the-middle_attack) can forward the\nclaim to the client; the server will also expect that clients return the same\nserver URI.\n\nREQUEST:\n\n```\nGET /data/ HTTP/1.1\nHost: example.org\nAuthorization: WebID-RSA source=\"example.org\",\n                         username=\"https://alice.example.org/card#me\",\n                         nonce=\"securestring\",\n                         sig=\"base64(sig(SHA1(SourceUsernameNonce)))\"\n```\n\nRESPONSE:\n\n```\nHTTP/1.1 200 OK\n```\n\nOne important advantage of WebID-RSA over WebID-TLS is that keys can be\ngenerated on the fly to sign and encrypt data. The way client certificate\nmanagement is currently implemented in browsers, it does not offer the means to\naccess keys inside certificates, for purposes other than authentication.\n\n***Proposed improvement:*** (not implemented yet) Instead of sending the WebID\nduring the response, the client could directly send the URI of the public key\nthat is need in order to verify the claim. For instance, Alice could list public\nkeys in her own profile, using fragment identifiers (e.g. `<#key1>`):\n\n```\n....\n<#me> cert:key <#key1>, <#key2> .\n\n<#key1> a cert:RSAPublicKey;\n        cert:modulus \"00cb24ed85d64d794b...\"^^xsd:hexBinary;\n        cert:exponent 65537  .\n```\n\nThe client would then send the following response:\n\n```\nGET /data/ HTTP/1.1\nHost: example.org\nAuthorization: WebID-RSA keyuri=\"https://alice.example.org/card#key1\",\n                         nonce=\"securestring\",\n                         sig=\"signatureOverUsernamePlusNonce\"\n```\n\nThe server would then be able to immediately identify and link the key that was\nused to sign the response to the user that owns it.\n\nsource: https://github.com/solid/solid/blob/main/proposals/auth-webid-rsa.md\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","title":"WebID-TLS"},"frontmatter":{"draft":false},"rawBody":"Specification Link is provided https://www.w3.org/2005/Incubator/webid/spec/tls/ \n\nIn summary;\n\nThe [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-key_store%20%22Key_Store%22) must have a [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-certificate%20%22Certificate%22) with a `Subject Alternative Name` URI entry. This URI must be one that dereferences to a [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid_profile%20%22WebID_Profile%22) whose graph contains a `cert:key` relation from the WebID to the public key published in the  . (see below [](https://www.w3.org/2005/Incubator/webid/spec/tls/#the-webid-profile-document))\n\nFor example, if a user Bob controls `https://bob.example/profile`, then his [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid%20%22WebID%22) can be `https://bob.example/profile#me`\n\nWhen creating a certificate it is very important to choose a user friendly Common Name (CN) for the user, that will allow him to distinguish between different certificates he may have, such as a personal or a business certificate, when selecting one from his browser. In the example below the CN is `Bob (personal)`. This name can then also be displayed by any server authenticating the user as a human friendly label. The [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid%20%22WebID%22) URL itself should not usually be used as a visible identifier for human users, rather it should be thought of as a hyperlink in an `<a href=\"https://...\">` anchor. That is the CN should be a label and the [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid%20%22WebID%22) a pointer.\n\nThe [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid_profile%20%22WebID_Profile%22) document _must_ be a [](https://www.w3.org/2005/Incubator/webid/spec/tls/#bib-WEBID)] document. It _must_ also expose the relation between the [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid%20%22WebID%22) URI and the [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-subject%20%22Subject%22)'s [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-public_key%20%22public_key%22)s using the [cert ontology](http://www.w3.org/ns/auth/cert#dfn-webid_profile \"WebID_Profile\"]] as well as the standard `xsd` datatypes.\n\n### Vocabulary\n\nRDF graphs are built using vocabularies defined by URIs, that can be placed in subject, predicate or object position. The definition of each URI should be found at the namespace of the URI. Here we detail the core cryptographic terms needed. The optional foaf vocabulary used to describe agents can be found at the [the foaf namespace vocabulary document](http://xmlns.com/foaf/0.1/).\n\nBelow is a short summary of the vocabulary elements to be used when conveying the relation between the [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-subject%20%22Subject%22) and his or her key, within a [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid_profile%20%22WebID_Profile%22) document. For more details please consult the [cert ontology document](http://www.w3.org/ns/auth/cert).\n\n[](http://www.w3.org/ns/auth/cert#key)\n\nUsed to associate a [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid%20%22WebID%22) URI with any PublicKey. A [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid_profile%20%22WebID_Profile%22) _must_ contain at least one PublicKey that is associated with the corresponding [](https://www.w3.org/2005/Incubator/webid/spec/tls/#dfn-webid%20%22WebID%22) URI.\n\n[](http://www.w3.org/ns/auth/cert#RSAPublicKey)\n\nRefers to the class of RSA Public Keys. A RSAPublicKey _must_ specify both a cert:modulus and a cert:exponent property. As the cert:modulus and cert:exponent relations both have as domain a cert:RSAPublicKey, the type of the key can be inferred by the use of those relations and need not be written out explicitly.\n\n[](http://www.w3.org/ns/auth/cert#modulus)\n\nUsed to relate an RSAPublic key to its modulus expressed as a hexBinary. An RSA key _must_ have one and only one modulus. The datatype of a modulus is xsd:hexBinary. The string representation of the hex:Binary _must_ not contain any whitespaces in between the hex numbers.\n\n[](http://www.w3.org/ns/auth/cert#exponent)\n\nUsed to relate an RSAPublic key to its exponent expressed as a decimal integer. An RSA key _must_ have one and only one exponent. The datatype of a modulus is xsd:integer.\n\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","title":"Sparql Family"},"frontmatter":{"draft":false},"rawBody":"Sparql 1.1\nhttps://www.w3.org/TR/sparql11-overview/\n\nSparql-FED\nhttps://www.w3.org/TR/2013/REC-sparql11-federated-query-20130321/\n\nSparql-MM\nhttps://github.com/tkurz/sparql-mm\n\nLinked Data Fragments: https://github.com/comunica/ \n\n\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","title":"Linked Data Fragments"},"frontmatter":{"draft":false},"rawBody":"https://linkeddatafragments.org/specification/linked-data-fragments/\n\nhttps://linkeddatafragments.org/specification/\n\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","title":"Linked Data Notifications"},"frontmatter":{"draft":false},"rawBody":"https://www.w3.org/TR/ldn/\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","title":"Linked Data Platform"},"frontmatter":{"draft":false},"rawBody":"https://www.w3.org/TR/ldp/"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","title":"Linked Media Fragments"},"frontmatter":{"draft":false},"rawBody":"https://www.w3.org/TR/media-frags/\n"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","title":"RDF"},"frontmatter":{"draft":false},"rawBody":"https://www.w3.org/RDF/"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","title":"Web Access Control (WAC)"},"frontmatter":{"draft":false},"rawBody":"https://www.w3.org/wiki/WebAccessControl"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","title":"Web Of Things"},"frontmatter":{"draft":false},"rawBody":"https://www.w3.org/WoT/documentation/"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","title":"WebID"},"frontmatter":{"draft":false},"rawBody":"Mentions WebID Protocol: https://www.w3.org/2005/Incubator/webid/\nWebID Specifications: https://www.w3.org/2005/Incubator/webid/spec/\n\nThe WebID-AUTH ecosystems include the following;\n- [WebID-OIDC](../SemWeb-AUTH/WebID-OIDC.md)\n- [WebID-TLS](../SemWeb-AUTH/WebID-TLS.md)\n- [WebID-RSA](../SemWeb-AUTH/WebID-RSA.md)"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","title":"WebAuthn"},"frontmatter":{"draft":false},"rawBody":"\n## WikiPedia description\n**Web Authentication** (**WebAuthn**) is a [web standard](https://en.wikipedia.org/wiki/Web_standard \"Web standard\") published by the [World Wide Web Consortium](https://en.wikipedia.org/wiki/World_Wide_Web_Consortium \"World Wide Web Consortium\") (W3C).[](https://en.wikipedia.org/wiki/WebAuthn#cite_note-W3C-WebAuthn-1)[](https://en.wikipedia.org/wiki/WebAuthn#cite_note-2)[](https://en.wikipedia.org/wiki/WebAuthn#cite_note-3) WebAuthn is a core component of the [FIDO2 Project](https://en.wikipedia.org/wiki/FIDO2_Project \"FIDO2 Project\") under the guidance of the [FIDO Alliance](https://en.wikipedia.org/wiki/FIDO_Alliance \"FIDO Alliance\").[](https://en.wikipedia.org/wiki/WebAuthn#cite_note-fido2-4) The goal of the project is to standardize an interface for authenticating users to web-based applications and services using [public-key cryptography](https://en.wikipedia.org/wiki/Public-key_cryptography \"Public-key cryptography\").\nLINK: https://en.wikipedia.org/wiki/WebAuthn \n\n\n## Specification Document\nThis specification defines an API enabling the creation and use of strong, attested, [](https://www.w3.org/TR/webauthn-2/#scope), public key-based credentials by [](https://www.w3.org/TR/webauthn-2/#web-application), for the purpose of strongly authenticating users. Conceptually, one or more [](https://www.w3.org/TR/webauthn-2/#public-key-credential), each [](https://www.w3.org/TR/webauthn-2/#scope) to a given [](https://www.w3.org/TR/webauthn-2/#webauthn-relying-party), are created by and [](https://www.w3.org/TR/webauthn-2/#bound-credential) to [](https://www.w3.org/TR/webauthn-2/#authenticator) as requested by the web application. The user agent mediates access to [](https://www.w3.org/TR/webauthn-2/#authenticator) and their [](https://www.w3.org/TR/webauthn-2/#public-key-credential) in order to preserve user privacy. [](https://www.w3.org/TR/webauthn-2/#authenticator) are responsible for ensuring that no operation is performed without [](https://www.w3.org/TR/webauthn-2/#user-consent). [](https://www.w3.org/TR/webauthn-2/#authenticator) provide cryptographic proof of their properties to [](https://www.w3.org/TR/webauthn-2/#relying-party) via [](https://www.w3.org/TR/webauthn-2/#attestation). This specification also describes the functional model for WebAuthn conformant [](https://www.w3.org/TR/webauthn-2/#authenticator), including their signature and [](https://www.w3.org/TR/webauthn-2/#attestation) functionality.\nLINK: https://www.w3.org/TR/webauthn-2/ "},{"fields":{"slug":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","title":"WebDav"},"frontmatter":{"draft":false},"rawBody":"Put simply, resources need to provide both a temporally static end-point and a 'latest' end-point; as to support temporal semantics.\n\nit is believed the best way to achieve this for HTTP(s) information is via WebDav\n\nI've found https://cs.opensource.google/go/x/net \n\nDescription of  [WebDav from WikiPedia](https://en.wikipedia.org/wiki/WebDAV)\n\n**WebDAV** (**Web Distributed Authoring and Versioning**) is a set of extensions to the [Hypertext Transfer Protocol](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol \"Hypertext Transfer Protocol\") (HTTP), which allows [user agents](https://en.wikipedia.org/wiki/User_agent \"User agent\") to collaboratively author contents _directly_ in an [HTTP web server](https://en.wikipedia.org/wiki/Web_server \"Web server\") by providing facilities for [concurrency control](https://en.wikipedia.org/wiki/Concurrency_control \"Concurrency control\") and [namespace operations](https://en.wikipedia.org/wiki/Namespace \"Namespace\"), thus allowing [Web](https://en.wikipedia.org/wiki/World_Wide_Web \"World Wide Web\") to be viewed as a _writeable, collaborative medium_ and not just a read-only medium.[](https://en.wikipedia.org/wiki/WebDAV#cite_note-FOOTNOTEWhiteheadGoland1999293-1) WebDAV is defined in [RFC](https://en.wikipedia.org/wiki/RFC_(identifier) \"RFC (identifier)\") [4918](https://datatracker.ietf.org/doc/html/rfc4918) by a [working group](https://en.wikipedia.org/wiki/Working_group \"Working group\") of the [Internet Engineering Task Force](https://en.wikipedia.org/wiki/Internet_Engineering_Task_Force \"Internet Engineering Task Force\") (IETF).[](https://en.wikipedia.org/wiki/WebDAV#cite_note-FOOTNOTEWhitehead199834-2)\n\nThe WebDAV protocol provides a framework for users to create, change and move documents on a [server](https://en.wikipedia.org/wiki/Server_(computing) \"Server (computing)\"). The most important features include the maintenance of properties about an author or modification date, [namespace](https://en.wikipedia.org/wiki/Namespace \"Namespace\") management, collections, and overwrite protection. Maintenance of properties includes such things as the creation, removal, and querying of file information. Namespace management deals with the ability to copy and move web pages within a server's namespace. Collections deal with the creation, removal, and listing of various resources. Lastly, overwrite protection handles aspects related to the locking of files. It takes advantage of existing technologies such as [Transport Layer Security](https://en.wikipedia.org/wiki/Transport_Layer_Security \"Transport Layer Security\"), [digest access authentication](https://en.wikipedia.org/wiki/Digest_access_authentication \"Digest access authentication\") or [XML](https://en.wikipedia.org/wiki/XML \"XML\") to satisfy those requirements.[](https://en.wikipedia.org/wiki/WebDAV#cite_note-FOOTNOTEWhiteheadGoland1999294-3)\n\nMany modern [operating systems](https://en.wikipedia.org/wiki/Operating_system \"Operating system\") provide built-in [client-side](https://en.wikipedia.org/wiki/Client-side \"Client-side\") support for WebDAV."},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","title":"Calendar"},"frontmatter":{"draft":false},"rawBody":"The Calendar app will help people manage and view what they do in relation to time."},{"fields":{"slug":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","title":"Timeline Interface"},"frontmatter":{"draft":false},"rawBody":"based on: https://timeline.knightlab.com/ \n\nThe idea is that people are able to create a faceted query (using structured langage) to create a timeline view of a topic or concept that they're interested in exploring, however this has various implications."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","title":"Digital Receipts"},"frontmatter":{"draft":false},"rawBody":"---\nid: 656\ntitle: 'Digital Receipts'\ndate: '2018-10-05T12:01:41+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=656'\ninline_featured_image:\n    - '0'\n---\n\nDigital Receipts is an area through which a vast amount of time has been delegated overtime. The material fact that industry as a whole believe thermally printed receipts, provided to customers, is the best that they can do; I find to be, offensive, and you should too. Work did need to be done on a [web-payments standards](https://www.w3.org/community/webpayments/), but that’s now well and truely developed; with [lots of big companies engaged](https://www.w3.org/2004/01/pp-impl/73816/status).\n\nThe problem is of course; even if they were to transfer to a machine-readable receipt system, where would ‘consumers’ store their digital receipts?\n\nThe answer to that problem, is a knowledge banking industry.\n\nThere are a variety of data-models that have been available for sometime, providing the means to define a ‘machine readable digital receipt’ model. these existing tools include [ArtsXML.](https://en.wikipedia.org/wiki/Association_for_Retail_Technology_Standards) The benefits of producing, storing and maintaining an achieve of digital receipts has so many beneficial uses, from the ability to manage warranty’s for product purchases, through to the ability to accounting / book-keeping benefits; and the ability to build new products, such as the means for people to donate products and services to those in need (such as homeless persons, on the street, who have specified needs).\n\nTherein; one way to explore the potential benefits is brought about by simply looking at food. The french based [OuiShare](https://www.ouishare.net/) Lab group, have produced some work on a ‘[food dashboard](https://github.com/WebCivics/food-dashboard)‘ some years ago. Through the links provided, the means to track food from farm (or factory) to consumption, is brought about.\n\nThis information can in-turn be used to improve customer choices, about the lived welfare of animals, they consume. To look at the overall energy cost of having been provided a product; or how the treatment of it, may alter the biological benefits provided to a person. The use of food-supply chain techniques can be used clinically, to identify what foods a person should not be eating due to their own unique biological condition, which may also have environmental factors; which in seeking to track it all down, can improve immeasurably, through the use of a digital receipt system.\n\nThe benefits of local industries can be made clearer; and market forces driving improved information to consumers, can assist with the development, delivery and proliferation of new products and services that improve health and wellbeing.\n\nIn other areas; the ability to ensure manufacturing practices are environmentally friendly; The ability to ensure solar panel purchases, take into consideration the means through which the [toxic waste products](https://www.forbes.com/sites/michaelshellenberger/2018/05/23/if-solar-panels-are-so-clean-why-do-they-produce-so-much-toxic-waste/#22909f7a121c) are processed as a constituent of the overall environmental benefit of the product.\n\nDigital receipts offer an array of remarkable benefits. Amongst all of this, is also the means to translate value to brand equity.\n\nThe ability to improve services surrounding the delivery of ‘digital receipts’ implicitly also supports a social-graph capability that provides support for ‘loyalty relationship’ management. If a consumer (customer) wants to share and engage; they can, its all baked into the way digital receipts can work.\n\nNew economic models can emerge through [DID’s and Ledgers](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/), as was noted about tracking coffee-cups; and in this new age of ‘open banking’, the means for financial institutions to improve their service offerings and differentiate themselves from their competitors may well come down to the semantic infrstructure they build, for industry specific requirements.\n\nWhether it be ensuring customers are returning clothes less often, or building a new clothing manufacturing industry based off the back of [measurements](https://www.webizen.net.au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/)\n\nThe problem that’s been holding it all up, is the ability to store this valuable, personal and private information somewhere secure, like a knowledge banking industry."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","title":"Fake News: Considerations → Principles → The Institution of Socio &#8211; Economic Values"},"frontmatter":{"draft":false},"rawBody":"---\nid: 609\ntitle: 'Fake News: Considerations → Principles → The Institution of Socio &#8211; Economic Values'\ndate: '2018-10-04T11:23:37+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=609'\ninline_featured_image:\n    - '0'\n---\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/KUhpK-wtldA?rel=0\" width=\"560\"></iframe>  \nAs I first published in [‘design solutions for fake news’ google doc](https://docs.google.com/document/d/1OPghC4ra6QLhaHhW8QvPJRMKGEXT7KaZtG_7s5-UQrw/).\n\n*<span style=\"font-weight: 400;\">A Perspective by Eben Moglen</span>* *<span style=\"font-weight: 400;\">from re:publica 2012 is otherwise considered with respect to the implications upon [freedom of thought](https://www.webizen.net.au/about/executive-summary/preserving-the-freedom-to-think/).</span>*\n\n<span style=\"font-weight: 400;\">The problem of ‘fake news’ may be solved in many ways. One way involves mass censorship of articles that do not come from major sources, but may not result in news that is any more ‘true’. Another way may be to shift the way we use the web, but that may not help us be more connected. Machine-readable documents are changing our world. </span>\n\n<span style=\"font-weight: 400;\">It is important that we distill ‘human values’ in assembly with ‘means for commerce’.</span> <span style=\"font-weight: 400;\">As we leave the former world of </span>**broadcast services**<span style=\"font-weight: 400;\"> where the considerations of propaganda were far better understood; to more modern services that serve not millions, but billions of humans across the planet, the principles we forged as communities seem to need to be re-established. We have the precedents of Humans Rights</span><span style=\"font-weight: 400;\">, but do not know how to apply them in a world where the ‘[choice of law](https://drive.google.com/open?id=1bHmB8_f7ASRHm97TwhZmmEQnTKU&usp=sharing)’</span><span style=\"font-weight: 400;\"> for the websites we use to communicate, may deem us to be [legal aliens](https://en.wikipedia.org/wiki/Alien_(law))</span><span style=\"font-weight: 400;\">. Traditionally these problems were solved via the application of [Liberal Arts](https://en.wikipedia.org/wiki/Liberal_arts_education)</span><span style=\"font-weight: 400;\">, however through the advent of the web, the more modern context becomes that of [Web Science](http://www.webscience.org/)</span><span style=\"font-weight: 400;\"> incorporating the role of ‘[philosophical engineering](https://www.w3.org/2007/09/map/main.jpg)’</span><span style=\"font-weight: 400;\"> (and therein the considerations of liberal arts via computer scientists).</span>\n\n<span style=\"font-weight: 400;\">So what are our principle, what are our shared values? And how do we build a ‘web we want’ that makes our world a better place both now, and into the future? </span>\n\n<span style=\"font-weight: 400;\">It seems many throughout the world have [suffered mental health issues](https://www.google.com.au/search?q=trump+therapy)</span><span style=\"font-weight: 400;\"> as a result of the recent election result in the USA. A moment in time where seemingly billions of people have simultaneously highlighted a perceived issue where the results of a populous exacting their democratic rights resulted in global issues that pertained to the outcome being a significant surprise. So perhaps the baseline question becomes; how will our web better provide the means in which to provide us (humans) a more accurate understanding of world-events and circumstances felt by humans, via our ‘world wide web’.</span>\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","title":"Healthy Living Economy"},"frontmatter":{"draft":false},"rawBody":"---\nid: 572\ntitle: 'Healthy Living Economy'\ndate: '2018-10-02T16:51:04+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=572'\ninline_featured_image:\n    - '0'\n---\n\n<span style=\"font-weight: 400;\">An opportunity to build an application that provides opportunities to test new technologies, in a manner that is socially beneficial.</span>\n\n**Objective**\n\n<span style=\"font-weight: 400;\">The Healthy Living. Mobile and Web Applications are designed to aid with discovery of opportunities for people to get involved with something that’ll keep them physically and socially active.</span>\n\n**Discovery**\n\n- <span style=\"font-weight: 400;\">The applications will aid engagement by Sporting related clubs of all forms to aid connectivity with individuals. </span>\n- <span style=\"font-weight: 400;\">The application will also support connecting people with similar sporting interests, as to enable arrangement of ad-hoc groups to undertake sports free from organisation of a sporting club specifically.</span>\n\n##### **A Healthy Persona**\n\n- <span style=\"font-weight: 400;\">The Applications will offer ‘badges’. These ‘badges’ are digital objects that can be displayed in relation to a persons online persona, linked to their social media experiences. </span>\n- <span style=\"font-weight: 400;\">The application, in providing discovery between sporting clubs and individuals, will seek to support individual rewards and social-marketing (referrals, etc.) as to enhance the number of participants engaged in sporting activities</span>\n- <span style=\"font-weight: 400;\">Users will be able to profile their sporting abilities, roles, professional memberships and insurances.</span>\n\n#### **Goals**\n\n##### <span style=\"font-weight: 400;\">GOAL 1 – Healthy People</span>\n\n<span style=\"font-weight: 400;\">To enhance the number of participants improving their health, via some form of regular physical activity.</span>\n\n###### <span style=\"font-weight: 400;\">HOW?</span>\n\n<span style=\"font-weight: 400;\">By enabling a means for people to identify how active they are, it becomes part of the social-value chain surrounding the way in which they treat themselves, and therefore; what information others have when considering them as people.</span>\n\n##### <span style=\"font-weight: 400;\">GOAL 2 – Improve Membership for Professional Sporting Clubs</span>\n\n<span style=\"font-weight: 400;\">Memberships with Sporting groups generally, is believed to be declining. Anecdotal evidence suggests that fitness instruction and gymnasium memberships are developing; in a manner dissimilar to the trends exhibited throughout other sporting groups.</span>\n\n###### <span style=\"font-weight: 400;\">HOW?</span>\n\n<span style=\"font-weight: 400;\">The applications will support an array of discovery mechanisms, that is targeted at aiding groups in attracting new members through their existing new-members events, etc. </span>\n\n#### **Solution**\n\n<span style=\"font-weight: 400;\">The Healthy Living. Family of applications, will create a social-networking hub around physical activity, connecting to leading Social-Media websites, promoting physical activity, providing an array of advanced social-media services to encourage engagement between citizens and health benefiting activities, as supported by Healthy Living. </span>\n\n<span style=\"font-weight: 400;\">Healthy Living. in-turn seeks to reinstate its brand, leading the way in how clubs and community groups connect with the broader community, seeking to discover and connect – with new opportunities for physical &amp; social activities. Through improving the way supporters of Healthy Living. engage using social-media management, through the use of the Healthy Living. suite of digital products, it is hoped the brand will grow and prosper.</span>\n\n<span style=\"font-weight: 400;\">The solution includes an array of analytics solutions that assist in tracking the activity of members, identifying the link between the brand ‘Healthy Living.’ and physical activity.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","title":"HyperMedia Solutions &#8211; Adapting HbbTV V2"},"frontmatter":{"draft":false},"rawBody":"---\nid: 574\ntitle: 'HyperMedia Solutions &#8211; Adapting HbbTV V2'\ndate: '2018-10-02T16:52:30+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=574'\ninline_featured_image:\n    - '0'\n---\n\n<span style=\"font-weight: 400;\">HbbTV is a standard for Interactive TV. HbbTV v2 methods in-turn support additional functionality designed to support sociological factors surrounding TV Viewing. HyperMedia is a solution that builds upon HbbTV v2 (or similar) to build multi-platform content packages that also incorporate the use of Linked-Data or RDF. </span>\n\n<span style=\"font-weight: 400;\">HbbTV v2 has recently been released. Most markets are currently served using previous versions of HbbTV. By combining the additional functionality of Linked-Data and Personal DataSpaces, enormous capabilities enhancements to target and personalise TV experiences may be achieved. Therein, the opportunity to create interactive content packages (ICP’s) that are designed for Multi-Device, interactive TV experiences, may be considered as a constituent of the R&amp;D cycle for delivering linked-data enabled TV services. Within this proposed topology, viewer data for HbbTV-ICP is stored by the user. The extension of existing standards principally involves the application of existing W3C Standards, as to support the interaction between entities contributing data to personalised presentation experiences. Underlying permissioning, data-storage and related systems are in-turn supported by standardisation efforts in enumerate linked-data related W3C Groups. </span>\n\n<span style=\"font-weight: 400;\">The creation of Interactive Content Packages (ie: HbbTV-ICP) standards is envisaged to aid the development of production tools that enables produced packages to be ingested by TV (media distribution) provider’s systems, which may in-turn be produced in a manner that so long as they support the standardised methods for producing packages, would be compatible with other lifecycle objects due to the nature of technology standards use, for technology production. These systems are different from other available systems, as the methodology would support the storage of information pertaining to or the property of a legal entity, by that legal entity, through specifically tailored services designed to support linked-data interactions, and socially aware cloud-storage. Legal entities are able to in-turn use their data to support Localised Advertising, Programming that interacts with user data, Profile and User-Preferences without necessarily sharing data to 3rd parties. Actions that share data with 3rd parties, is done on a knowing basis by the ‘owner’ of that data.</span>\n\n## **COMMERCIAL DRIVERS**\n\n<span style=\"font-weight: 400;\">Free (Media) Services are supported by the mix between available content (or services) that people like; and the availability of advertising people are willing to interact with, on a specified basis (ie: view or click). </span>\n\n### **COMPELLING ADVERTISING OPPORTUNITIES**\n\n<span style=\"font-weight: 400;\">In one embodiment; The platform provides a media experience designed for screen-media, that interacts with personal internet devices (ie: mobiles / tables) for social and interactive experiences that relate to media screened on a primary device (ie: tv). Advertisers make network offers, that can be evaluated by the users dataspace, to identify the best offer, which the user may in-turn provide feedback about whether that be in providing information to ask not to see advertisements from that brand (which is not shared with the brand); or, interacting with the offer (which is shared with the offer provider (ie: ad-agency or brand). </span>\n\n<span style=\"font-weight: 400;\">At a basic layer; these systems enable users to store a history of what they watched with embedded links to media and data, such as the organisations that media relates to, or an ingredients list, or cast &amp; crew information (etc); for on-demand viewing and/or interaction. </span>\n\n<span style=\"font-weight: 400;\">These forms of Interactive Content Spaces, powered by RDF, increases advertising opportunities remarkably. Overtime, as the technologies scale in nature; opportunities to extend capabilities, from identifying the Clothing people are wearing in media, through to the soundtracks, artists and other embodied concepts within an Interactive Content Package, may be explored by the producer of a content package in a manner that is supported by standards and therefore, may in-turn be widely distributed; which in-terms of advertising, supports an array of new business models not previously applied to supporting the delivery of media. </span>\n\n### **COMPELLING CONTENT OPPORTUNITIES**\n\n<span style=\"font-weight: 400;\">The dataspaces capability enables anyone to upload content to their own data-spaces that can be made available to channels with an array of business rules applied via various technology standards. </span>\n\n<span style=\"font-weight: 400;\">Content Packages are envisaged to offer an array of incredible new opportunities. Presentations will be able to use data from users, without necessarily providing that data to anyone else, other than whoever is watching the content on the device it is being displayed on. As the content packages are produced by content providers, no-matter whether the content package primarily relates to content or advertising, any and all advertising embodied within the content package would be managed by the content package provider. Content can be produced by any participant, and in-turn provided and monetized on a decentralised basis, supporting media participants via digital agreements made possible by standards technologies. </span>\n\n## **HyperMedia USE CASES**\n\n<span style=\"font-weight: 400;\">These use-cases iteratively highlight the scope of opportunity at scale for the platform to operate, how it operates and what it can do once the platform storing data on behalf of users is scaled to critical mass where these dataspaces are providing online data storage for users data in a variety of formats and circumstances including but not exclusive to;</span>\n\n- <span style=\"font-weight: 400;\">Device Metadata (sensor data such as GPS information from phones, healthcare info from on-body sensors, internet location info, etc.)</span>\n- <span style=\"font-weight: 400;\">Media (photos, videos, audio, etc.)</span>\n- <span style=\"font-weight: 400;\">Interaction info (Web-usage history, social relationships, event bookings, etc.)</span>\n- <span style=\"font-weight: 400;\">Consumption behaviours (digital receipt info, previous viewing behaviours, app licenses, etc.)</span>\n- <span style=\"font-weight: 400;\">Semantic Analysis (analyses of concepts in documents authored, analysis of faces, places and objects in image / video, phonetics in audio, collated analysis across multiple resources, etc.)</span>\n\n<span style=\"font-weight: 400;\">In early stages less information is available to the user and whilst this data and related service infrastructure exists on the network, where data is not stored by users specifically, the data does not become available for user-centric applications. All of the above functionality exists today. The issue that we’re dealing with, is who, where, and why that data is stored for it’s primary purpose; and the conditions in how that data is made available to others, for what purpose. The only condition pursuant to law with regard to mandatory access; is considered to be with regard to the needs of citizens pursuant to the rules of democracy, the rule of law, and how government with particular regard law-enforcement and system of courts, separation of powers, etc.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","title":"Measurements App"},"frontmatter":{"draft":false},"rawBody":"---\nid: 580\ntitle: 'Measurements App'\ndate: '2018-10-02T16:56:12+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=580'\ninline_featured_image:\n    - '0'\n---\n\n#### [![](https://www.webizen.net.au/wp-content/uploads/2018/10/measurement-chart-1024x714.jpg)](https://www.webizen.net.au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/measurement-chart/)\n\n#### Overview\n\n<span style=\"font-weight: 400;\">The measurements application provides an easy to use interface for people to upload, store and use their personal measurements on the web and through their phone. </span>\n\n<span style=\"font-weight: 400;\">The measurements stored by the app are accessible online and via a users phone enabling the user to easily find out whether items being purchased by the user will fit when purchasing clothing, underwear, headwear, eyewear, footwear and other things people wear on their person. By uploading the information and having it stored online in a secure account all a user has to do to is click a button to scan a tag or authorise an online website to access the measurements information, which will then automatically allow the website or via the mobile app – notify the user about whether or not the item will fit the customer or intended recipient of the purchase.</span>\n\n<span style=\"font-weight: 400;\">The application makes it easier and more convenient and time efficient for customers and retailers alike through this innovative way of assisting people in making purchases via online retailers and traditional stores.</span>\n\n<span style=\"font-weight: 400;\">The information produced by the measurements app is also shareable with others on a permissions basis. The ability for someone to share their measurements enables others to shop on behalf of the user, without being worried about whether or not purchases will fit. </span>\n\n<span style=\"font-weight: 400;\">The customers for the application are retailers who purchase a subscription from the site. </span>\n\n<span style=\"font-weight: 400;\">The Application helps retailers convert their design-sizes and any material related aspects (such as shrinkage, etc.) into a format that is easily converted between one measurement system to another digitally. Premium versions of the application will also provide anonymised information about the user-base. This in-turn helps designers produce stock quantities suitable for the market through qualitative information made available.</span>\n\n<span style=\"font-weight: 400;\">It is considered that the Measurements app will have health and safety benefits for consumers, as small things such as uncomfortable underwear or shoes that do not fit properly can result in long-term health problems and helmets that may result in future damage in the case of an accident.. </span>\n\n<span style=\"font-weight: 400;\">It is also considered that the measurement app will assist retailers in many ways, including but not exclusive to aiding others purchase clothing as a gift, enhancing loyalty relationships, improved customer experience with less need for trying clothing on prior to purchase; and improved confidence about purchasing items online. additional benefits include the capacity for the application to support the development of new industries as the measurements of individuals is already done for members, which means the capacity for individuals to order custom clothing becomes that much easier.</span>\n\n**USER EXPERIENCE – concepts**\n\n**Consumers**\n\n<span style=\"font-weight: 400;\">A user is expected to either obtain a measuring tape to measure themselves or to obtain the services of someone else to measure them. The application can accept measurements in various forms from millimeter to centimeter to inches and other units, automatically converting these units online to suit different merchandising measurement systems.</span>\n\n<span style=\"font-weight: 400;\">The Measurements application will be available both on mobile and for desktop computers. An interactive guide will help people take their measurements, help them easily and privately share their measurements information and remind the user when they measurements haven’t been done for a while and may have changed. </span>\n\n<span style=\"font-weight: 400;\">The application will show the user both when they last measured themselves; as well as what measurements they’ve completed and what measurements they’re yet to do.. </span>\n\n**Retailers – Online**\n\n<span style=\"font-weight: 400;\">Online retailers will be provided with an array of tools including an API specification, plugins for major content management systems platforms and online tools to help them understand how often the application is being used in addition to being able to provide loyalty discounts to members who use the application and provide approval to the retailer for marketing purposes. </span>\n\n**Retailers – Stores and Manufacturers**\n\n<span style=\"font-weight: 400;\">Stores and Manufacturers will be provided a set of tools that helps their products easily interact with the measurement app to obtain sizing information and make recommendations to consumers about which products fit best, and whether the sizing is optimal, suitable or unsuitable for that consumer.</span>\n\n**VAR – Industry Solutions**\n\n<span style=\"font-weight: 400;\">Our solution uses W3C Credentialing Technology alongside 2D barcodes that can be generated online through our API and related software products. We’re actively looking for partners who are interested in integrating with our technology to help their customers provide consumers the best possible retailing experience. </span>\n\n**Complexity of Measurements.**\n\n<span style=\"font-weight: 400;\">Whilst shopping for apparel may seem relatively simple, many will be surprised to know that it is actually quite complicated when it comes to obtaining all the right measurements for a person; in addition to translating that the various sizing and styling available in the marketplace. each area of the body has an array of different measurements that relate to whether clothes fit, whether they fit well or whether they’re unsuitable. </span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","title":"Re:Animation"},"frontmatter":{"draft":false},"rawBody":"---\nid: 569\ntitle: 'Re:Animation'\ndate: '2018-10-02T16:46:41+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=569'\ninline_featured_image:\n    - '0'\n---\n\n<span style=\"font-weight: 400;\">I proposed the development of an interactive experience that would allow people are able to ask leading minds questions through interactive experiences that augment the means in which we communicate. </span>\n\n<span style=\"font-weight: 400;\">The project produces an AI experience that is dynamically generated, interactively with the living being who are made capable of improving the representations via standards based technologies, as to provide a time-capsule of their lives, mannerisms, views and considerations, for use into the foreseeable future.</span>\n\n<span style=\"font-weight: 400;\">This is a research project for the development of Human Centric Web Technology. </span>\n\n<span style=\"font-weight: 400;\">The constructed environment will produce demand for the production of human-centric advances in Science, Technology Engineering and Mathematics to improve means for virtualized individuals to respond, act and interact with others virtually. The method produces an interactive feedback loop between the human subject and the A.I. capability, which will challenge views of self and technology, in seeking to represent behavioural heuristics (ie: mannerisms), perceptual consciousness and identity.</span>\n\n<span style=\"font-weight: 400;\">By engaging in the project; world leaders (humans) will be able to participate in the development of both platform and interaction research and technology design, in a manner that allows definition, collaboration and community development. </span>\n\n<span style=\"font-weight: 400;\">The platform project will provide Human Subjects analytics to be consumed and considered in relation to how the world views the interactions with them. Human Subjects are able to produce (and selectively share) new interfaces produced using Web Standards. This feedback loop, empowers developers to add value to how these representational knowledge-discovery and management techniques are developed on both a personal and communicative basis by allowing both human viewers and human subjects to interoperable customise their experiences. The questions asked by human viewers provide analytics to human subjects, who in-turn use this information to define what to make available for human viewers.</span>\n\n<span style=\"font-weight: 400;\">The development of this artificial world will explore the array of technical decisions being made; such as, </span>\n\n- <span style=\"font-weight: 400;\">When someone lives or when they die, who are the trustees? How is the decision made?</span>\n- <span style=\"font-weight: 400;\">What are the interoperable established sense of ethics, shared-values and individual-views?</span>\n- <span style=\"font-weight: 400;\">What Problems will be discussed within an environment of limitless, personalised audiences</span>\n- <span style=\"font-weight: 400;\">How will schema support interactive knowledge-sharing? </span>\n- <span style=\"font-weight: 400;\">How can the evolution of the representation be appropriately stored and retrieved? </span>\n\n<span style=\"font-weight: 400;\">Through working with those who invented the foundations to this new epoch</span><span style=\"font-weight: 400;\"> and whom are critical to the world-around us; how do they seek to enable new forms of collaboration for the living. Humanity is now capable of providing an advanced learning environment, empowered by multidimensional communication technology at relatively low-cost. </span>\n\n<span style=\"font-weight: 400;\">This platform produces a visualisation tool for defining the w(eb)orld we want. It provides an accessible platform for research and development by enabling a new means of communication and knowledge-storage on the web, by those who invented it. Through engaging those humans who were primarily involved in producing the technology our world depends upon, in a personalised manner, an opportunity is provided to those humans to declare intent, belief and considerations with a view to allowing influence through an immortal representations; that are primarily defined by the minds of those for whom the representations appear to be considered by; whose decisions today, will decide how others may influence their world overtime.</span>\n\n<span style=\"font-weight: 400;\">HOW I PROPOSED TO DO IT</span>\n\n<span style=\"font-weight: 400;\">I propose the use of Read-Write-Web based technology (ie: LDP, et.al.) in conjunction with CV</span><span style=\"font-weight: 400;\"> technology to produce in the first instance – ‘talking heads’ – whereby it appears the person talking is actually talking to the viewer. </span>\n\n<span style=\"font-weight: 400;\">LDP and related A.I. techniques allow the Human Subject to upload, process and articulate an array of knowledge formed in their lives as is processed by advanced machine learning techniques that allow the known-knowledge of the human subject to be made machine-readable as a personal dataset that is controlled by the human. </span>\n\n<span style=\"font-weight: 400;\">The Human Subject is then able to produce their own A.I. related commands. Due to the Subjects being leaders in computer-science related fields; it is expected that they’ll know how to go about this, and in-turn it is hoped they may produce a library of A.I. functions that may support the future development of an algorithm marketplace for others to use in relation to the development of personalised A.I.</span>\n\n<span style=\"font-weight: 400;\">An analytics platform will be produced to manage shared-data and provide an interface for advanced computational tasking, where human subjects identify problems they’re unable to process individually. </span>\n\n<span style=\"font-weight: 400;\">Human Viewers are provided an authenticated solution for engaging with Human Subjects. These platforms allow sharing of basic information mandatorily, alongside selective rulesets for how other information is shared between ‘humans’ or any associated ‘trustees’ for deceased humans. </span>\n\n<span style=\"font-weight: 400;\">A collaboration of the proposed ‘human subject’ early participants is desirable, as to obtain their views and assistance in establishing the underlying framework for project development. </span>\n\n<span style=\"font-weight: 400;\">The objective is to provide means in which those who know, can communicate more easily to others what it is they believe is important for them, and for others.</span>\n\n- <span style=\"font-weight: 400;\">A Basic Vision Mapping Demo: </span>[<span style=\"font-weight: 400;\">https://www.youtube.com/watch?v=ohmajJTcpNk</span>](https://www.youtube.com/watch?v=ohmajJTcpNk)\n- <span style=\"font-weight: 400;\">High Quality Facial Animation: </span>[<span style=\"font-weight: 400;\">https://www.youtube.com/watch?v=eOjzC\\_NPCv8</span>](https://www.youtube.com/watch?v=eOjzC_NPCv8)\n- <span style=\"font-weight: 400;\">Modelling Raw-Audio: </span>[<span style=\"font-weight: 400;\">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</span>](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)\n\n<span style=\"font-weight: 400;\">The lack of details in this document about technology capability considerations should not be interpreted as not being considerate of what i can see, in terms of the possibilities, it seems ridiculous for me to describe the capabilities of the most advanced technologies to the intended audience of this document. </span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","title":"Assisting those who Enforce the Law"},"frontmatter":{"draft":false},"rawBody":"---\nid: 668\ntitle: 'Assisting those who Enforce the Law'\ndate: '2018-10-05T13:19:04+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=668'\ninline_featured_image:\n    - '0'\n---\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"360\" loading=\"lazy\" src=\"https://player.vimeo.com/video/30416090\" width=\"640\"></iframe>\n\n[The Australian Way](https://vimeo.com/30416090) from [AusCivics](https://vimeo.com/auscivics)\n\nIt is important, as a society, that we look to the *law, not as a form of tyranny, but as a source of peace in our daily lives.*\n\nThere are already in place an array of telecommunications related laws that support the means for a law-enforcement agency to gain access to data relating to the commission of crime. Whilst these laws need to be amended and updated overtime, the lens through which they’ve been produced today; does not take into account, the opportunities brought about by a ‘knowledge banking industry’.\n\nAs it is the case today; a great deal of the software we use is made available by way of licensing terms, based upon international law, through which a foreign choice of law is elected for the governance of disputes of any kind. These terms do not only apply to the applications being provided, but moreover, the users information stored on those systems and related services.\n\nThrough the establishment of a knowledge banking industry, a great deal can be changed. The means to protect the privacy of users is enhanced greatly; whilst the means to ensure the industry can act as a guardian and advocacy instrument for rule of law, is in-turn brought about.\n\nIt is currently the case that a great deal of information exists about persons already, but this is most-often not easily available to Law-enforcement agencies when it could be made most-useful to them, in investigating a crime.\n\nFurthermore; it is the role of the law-enforcement agency relating to the jurisdiction of the person, that is able to be made most-important, through a knowledge banking industry. Whilst alternatives do exist via centralised methods, often requiring radical tools to assist them, when no available alternative otherwise exists; such as has recently been [declared locally by the Department of Home Affairs](https://www.homeaffairs.gov.au/about/national-security/five-country-ministerial-2018); the means to change the nature through which these sorts of decisions need to be considered, can be brought to bare through the establishment of a knowledge banking industry; that helps to ensure the protection of our sovereignty as a nation, is considerate of its people; as the fundamental stakeholders to which their role is duty bound by instruments such as the [charter of the commonwealth](http://thecommonwealth.org/our-charter), amongst the many.\n\nEqually and most importantly; Those who work as law-enforcement professionals are indeed human too. They require for themselves, their own inforgs, their own ‘knowledge banking accounts’ and due to the role they play in our society it would be considered particularly important to them, their data is safe. This is the same for any [politically exposed person](https://en.wikipedia.org/wiki/Politically_exposed_person) alongside the many others. The means to ensure accountability within these systems, are in-turn a fundamental tenant of how any such system would need to be made to operate. The principle consideration made, when making a query with Noam Chomsky with respect to personhood, was responded by him in the following way\n\n> the fact that there are pressures and costs does not absolve people of their moral responsibility. The primary custodian of one’s actions is oneself. 3rd of April 2018.\n\nWhilst i debate the considerations of how it is our systems currently work as to disseminate ‘fake news’ amongst its many attributable problems; and that upon the basis of persons having been provided false and misleading information or otherwise placed in a position where their actions are based upon false beliefs made to be engendered upon them by others,\n\nThat unless we address these issues on a fundamental basis, the means to ensure the primary custodianship of ones actions to be ones self, is in-turn diminished and therein were the considerations made of personhood and AI."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","title":"Consumer Protections"},"frontmatter":{"draft":false},"rawBody":"---\nid: 642\ntitle: 'Consumer Protections'\ndate: '2018-10-05T10:19:41+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=642'\ninline_featured_image:\n    - '0'\n---\n\nIn order to make a knowledge banking industry work, there are an array of qualities that need to be considered as to ensure the beneficial owner of the ‘inforg’ stored on it, is the person to whom that inforg relates. Some of the considered needs are noted below, whilst this is not in any way shape or form a complex and comprehensive overview of requirements; as illustrated by this document, at this stage.\n\nA Knowledge Banking provider would conceptually store thousands if not millions of accounts. Part of the benefit of a knowledge banking industry, is the ability to provide pseudo-anonymous data-services in relation to personal data. This in-turn is assisted by decentralising discovery, through the use of decentralised ledgers, DIDs and similar. In a manner that’s very similar to DNS the practical method in-turn enables use-cases such as the ability for computer vision to be used with biometric signatures, such as facial recognition systems; where AR Head mounted displays, such as is exampled by Magic Leap or google glass; allow its user to have ‘web of data’ based apps.\n\nIn this way, the contextual environment of a person who is identified by software, can be provided the means to privately be asked what information they want to provide back to the agent (the head mounted display glasses, for instance) making a request for information. As the knowledge banking provider is not making use of a users personal and public address space, the discovery process is able to poll to identify which provider can provide information; and when the provider is identified, they can internally identify what information they want to provide back to the agent making the request.\n\nIn the context of a business conference, this might mean providing business card details. In the context of a social app, such as a dating app, this might simply be a persons first name (that might float above their head, in augmented reality space).\n\nThe second thing that’s very important is the ability to ‘churn’ between providers; or the ability to change providers. This requirement in-turn is indeed quite complex. Software Standards are instrumental to the means through which this is made possible; but the principle need for knowledge banking – account holders – to migrate their accounts (and not break the semantics, in a privacy-preserving model) requires agreements to be in-place that entitle users to be able to move providers. Semantics data-structures in a decentralised information management system, is far more complex than simply migrating funds from one financial institution to another.\n\nAdditionally; the means to ensure ‘consumers’ have an array of ontological tools that entitle them to make legal declarations by way of data, that knowledge banking providers are obligated to preserve, is also important.\n\nSome of the original considerations around this was formed in the context of [RASPS](https://lists.w3.org/Archives/Public/public-webpayments/2014Jul/0043.html) or the ability to make declarations that state the data service provided by the person includes semantic information that declares information such as,\n\n- Reuse: can the information being provided be reused.\n- Accessibility: How can this data be transformed for use by the recipient.\n- Security: Is it important for the recipient of this data to encrypt it, and keep it secure.\n- Privacy: What, who and how should this data be used by the recipient.\n- Sovereignty: what ‘choice of law’ is defined in relation to the use of this data."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","title":"Knowledge Banking: Legal Structures"},"frontmatter":{"draft":false},"rawBody":"---\nid: 641\ntitle: 'Knowledge Banking: Legal Structures'\ndate: '2018-10-05T12:36:16+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=641'\ninline_featured_image:\n    - '0'\n---\n\nSo, if you want to make something happen, the situation changes from one that’s about being an innovator; to one that’s about being an entrepreneur.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/B4ZSGQW0UMI?rel=0\" width=\"560\"></iframe>\n\nInternationally there’s alot of work going on around ‘co-ops’, ‘[information fiduciaries](https://www.theatlantic.com/technology/archive/2016/10/information-fiduciary/502346/)‘ and related concepts. One of the very real problems; is that a knowledge bank, by aggregating so much data about a person, whether its linked-together permissively from an array of different locations; or whether, its all stored together by a single provider; either way, from a ‘graph’ point of view, its a ‘[honey pot](https://en.wikipedia.org/wiki/Honeypot_(computing))‘ and the means to figure out how to ensure seniors don’t give away all their personal, private and sensitive information for some simple offer, such as a free tank of fuel for their car; are indeed difficult problems to solve.\n\nFor instance; in Australia, the means through which human rights translates to Australians is at best, poor.. The [Australian Corporations Act](http://www5.austlii.edu.au/au/legis/cth/consol_act/ca2001172/) binds directors of a company to a series of responsibilities, but these do poorly by themselves provide protection for directors (or companies) from changes to the companies operation that may in-turn exploit the data stored by them.\n\nOne means to attend to this issue lies in the means to define the constitution for the company in such a way as to prohibit the company from ever doing so.\n\nBut the constitution can be changed by voting stakeholders of the company. One emergent means of resolving this issue is by way of co-op like structures, such as the [crowd-sourced equity financing models](https://asic.gov.au/regulatory-resources/financial-services/crowd-sourced-funding/) offered in Australia, noting there are similar sorts of mechanisms overseas.\n\nThe principle consideration is that the organised structure of the ‘knowledge banking provider’ as a legal entity can in-turn be set-up as a ‘social entrepreneurship vehicle’. In so doing, the objectives would include;\n\n- Providing ROI to investors on commercial terms\n- Providing protection to account holders and board members and executives, bound to legal responsibilities for the management of the company involved.\n\nIt is considered that overtime, if an industry develops, the ability for regulatory frameworks to be brought about can and will improve the security of persons.\n\nPerhaps most importantly; is the means through which the operation of the organisation involved in the storage and use of personal and private data (information / knowledge) be jurisdictionally made the same as the owners of that data, on a default basis. Whilst people may elect to obtain services elsewhere, this does in-turn complicate matters as is already the case today."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","title":"Knowledge Economics &#8211; Services"},"frontmatter":{"draft":false},"rawBody":"---\nid: 646\ntitle: 'Knowledge Economics &#8211; Services'\ndate: '2018-10-05T11:26:56+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=646'\ninline_featured_image:\n    - '0'\n---\n\nPart of the problem that has been ‘blamed’ for why the development of internet has been done in such a way, that online data storage as developed from the turn of the millennium failed to provide users a place to beneficially own their data; was that the world wide web was moreover built upon two primary forms of revenue models, for consumer services. These models are defined as;\n\n- Advertising revenue\n- Surveillance capitalism (“Big Data”)\n\nAdvertising revenue based economic modelling brought about the effect of seeking to engender benefit from an [attention economy](https://en.wikipedia.org/wiki/Attention_economy), through which display and link based advertisers became the customers of a website, whilst the website users were consumed as the product offered to advertisers.\n\nSurveillance capitalism somewhat increased the efficacy of these systems as to define new [knowledge based capital](http://www.oecd.org/sti/inno/newsourcesofgrowthknowledge-basedcapital.htm) metrics relating to the amount of information the website was able to obtain about the people engaging their online products (apps) and services (website / data-services); and how instrumental, or ‘sticky’ those people were, as a means to inflate valuations.\n\nOne of the emerging economic models relates in-turn to the development of artificial intelligence and the vast amounts of ‘training data’ required to bring about high-quality software agents. These models are in-turn making use of humans to assist them in developing their economic value, in new ways. One example is communicated in relation to the Captcha, by [techradar](https://www.techradar.com/au/news/captcha-if-you-can-how-youve-been-training-ai-for-years-without-realising-it) and this note on [medium](https://medium.com/@thenextcorner/you-are-helping-google-ai-image-recognition-b24d89372b7e), whilst there are otherwise lots of examples…\n\nSimultaneously, what is occurring is a vast change in the means through which payment systems work, which is demonstrated well by the economic barriers blockchain technology has managed to overcome; in addition to the relatively well-known emergent narrative that surrounds the ‘sharing economy’ and the various forms of internet enabled ‘gig work’ (see: [World Economic Forum](https://www.weforum.org/agenda/2017/12/when-is-sharing-not-really-sharing/)). as noted elsewhere, the problems that are now emergent is impacting our entire economy. As Superannuation contributions (private pensions) are disaffected as is noted by [CBUS Super](https://www.cbussuper.com.au/about-us/news/media-release/research-shows-shortfall-in-superannuation) and a ‘[Creative Economy Deficit](http://www.abc.net.au/news/programs/national-press-club/2018-08-15/national-press-club:-russel-howcroft/10123782)‘ is brought about as a consequence, which in-turn impacts the means for persons to get home-loans, alongside many other things; the problem overall becomes, that we have not engineered a modern form of economic framework for the ‘world-wide-web’ (whilst not exclusively bound to it at a protocol level); and we need new economic apparatus, that can offer an array of new financial service industry products and services; that may in-turn, allow people to store their information by paying about as much as they otherwise do, to open a bank-account and have a banking instrument (bank card) sent to them.\n\nThe opportunity for knowledge banking providers is in being able to provide valued services for a negligible percentage of economic benefit obtained by their customers, the humans opening and managing their ‘knowledge banking accounts’. Thereafter, I feel its important to provide a few examples,\n\n1\\. Translating micro-payments, to usable currency.\n\nThere are an array of new abilities brought about through the development of the [web-payments](https://www.w3.org/community/webpayments/) standards and related works.\n\n![](https://docs.google.com/drawings/d/e/2PACX-1vR8ji8Ksg7IXlQKg0TDZqVywMOQl_gHpB8jgK-U7xpXUUVlw33UM9Kesl9Oc9L0AkrHYoJnQuqyTY4Z/pub?w=960&h=720)\n\nIn a world where the ‘information management systems’ are not tied to a specified app (such as uber, airbnb, etc.) the means for someone to make a small contribution to something, some project, that might be run off the back of a ledger built for the project that enhances the works of ‘[dynamic equity](https://slicingpie.com/the-grunt-fund-calculator/)‘ to form a means through which, a ‘software as a utility’ market may emerge, alongside many other opportunities; the problem becomes,\n\nhow does the person doing the work collect all these ‘micro-payments’ and put them all through the proper ‘clearing house’ related processes as to convert the ‘tokenisation’ of economic instruments, into usable currency.\n\nHow and what is needed for a person doing a bunch of different jobs, from spending an hour on something that’s worth alot of money; to spending hours (or years) on something that’s in its early stages and is currently worth nothing (or moreover, less than nothing, as there is a cost to do something that is obtained by others, freely).\n\nSo, this means one of the ‘knowledge banking providers’ revenue models can be all about collecting all these different economic instruments that are mapped across the web; and converting them all into usable money.\n\nIn-turn, this is also another example of how the [KYC/AML](https://en.wikipedia.org/wiki/Know_your_customer) related issues are dealt with by way of a knowledge banking industry.\n\n2\\. New Economic Products &amp; services.\n\nAs canvassed briefly in the page that noted some [historical considerations](https://www.webizen.net.au/about/history/history-global-governance-ict-1/), sadly, the new-age ‘union movement’ for knowledge workers has just not happened (yet). ideally, its a peaceful one and the least we can hope for, is that the issue of fake-news is addressed as to not otherwise continue to transgress. Way back when ‘workers rights’ were all the rage, in the years before those elderly people with their self-managed superannuation funds, built with many houses all funded with the support of housing mortgages; back when they were earning the money to enable them to do that now,\n\nthey were working in traditional trades. The ability for those involved in building and many other traditional trades to show what it is they’ve been doing overtime as to be paid; was a whole-lot easier than the problems ‘knowledge workers’ suffer from today. Builders and such could point to ‘property law’ and make a express point; but people who build ‘business systems’ and all sorts of things that end-up online, can’t really do that.\n\nKnowledge workers need different sorts of legal services to be provided to them to protect them from exploitation; sorts of legal services not available today. Insurance products need to be able to assess what it is a person is actually doing, which is really quite difficult when the recipient of the services is claiming they never benefited at all or never received anything. Health insurance is made difficult, when the statements made often contain factual errors that are both potentially damaging to the patients health, and kept secret from them. The more data a person has; in a ‘socially aware’ environment that interlinks with others, as to form a fabric of ‘[social encryption](https://www.webizen.net.au/social-encryption-an-introduction/)‘ the more difficult it is to lie and whilst this is mearly an alternative, based on an assessment that assisting any party to break the law, breaches the principles of ‘rule of law’ for which our societies have built upon,\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/dRFrXGkYjhM?rel=0\" width=\"560\"></iframe>  \nnow therefore the means to employ these systems, change the ‘risk modelling’ and provide nuanced services for information workers becomes instrumentally valuable to all concerned; and the means to engage those to whom the services, as it is otherwise stated, are designed to serve, can be improved dramatically, improving the economic performance of any such products.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/QkPBCYKuQKw?rel=0\" width=\"560\"></iframe>  \nYet another example; is built into the consideration of how it is our superannuation system currently works; where the people have made significant investments but their investments does not translate to a vote.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/RbgtUZ0gkoM?rel=0\" width=\"560\"></iframe>  \nIn the above video, the view put forward by [Andrew McLeod](https://www.youtube.com/watch?v=DLjlq8X3Pwk) was that if the people of society want to change the way the world is now operated, one of the best ways to do that is to unite the directions provided to the board members of the worlds largest companies, to the will of the people; and to forget about trying to change the way government works, change the way the organisations providing government revenue, work. change the way the internet relates to any and all forms of work, through making use of personal data, as property.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/VUMvObLKaaw?rel=0\" width=\"560\"></iframe>  \nOvertime, problems have always emerged between parties, but increasingly today these ‘agreements’ are online and stored solely by the provider of the agreement, via their website. Changing the way all of these systems work, changes the way our society is made able to work; and whilst not all may want to make use of a ‘knowledge banking industry’, without one, there is no option.\n\nArtificial intelligence is able to automate the discovery and procedural requirements of translating data and information; to knowledge that has been customised for use in some specified way. The hope is, on just terms.\n\nAn array of other examples are provided in other areas of this site (link to be provided); as is the case otherwise, the means to employ data-systems to support new economic revenues through commercial products and services (ie; digital receipts) is a big part of what can happen, through a knowledge banking industry. Most people don’t mind sharing their data for specified purposes, its just that the person people want to specify terms for the use of their ‘things’, their ‘data’, is them. If someone demands to use or take another persons physical property, there’s a law protecting them from that."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","title":"Preserving The Freedom to Think"},"frontmatter":{"draft":false},"rawBody":"---\nid: 627\ntitle: 'Preserving The Freedom to Think'\ndate: '2018-10-04T13:46:47+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=627'\ninline_featured_image:\n    - '0'\n---\n\n<span style=\"font-size: 1rem;\">Our socioeconomic frameworks and the means preserve the ‘freedom to think’, or ‘freedom of thought’, is influenced by ‘artificial intelligence’ systems. </span>\n\n<span style=\"font-size: 1rem;\">It is influenced in ways that range from issues illustrated by ‘autocorrect’, through to the economically derived methodologies through which information is preferentially presented to us, or ‘hyper personalisation’. It is also influenced both by what we choose to expressly provide, and what we do not provide information about. Whether it be the nature of interpersonal love shared between two people; or the many other, deeply personal attributes, AI is both shaped by us and what it knows (or does not know) about us; to in-turn, be employed by systems that shape us. </span>\n\nIt is increasingly through out ICT systems that a ‘selection’ of materials are produced; often, in relation to some set of specified purpose.\n\nIn-turn, this often subjectively produced selection of ‘data points’ are brought about, in relation to the author of those works (and/or their relations) alongside the influences of linguistics &amp; circumstances, in which any such material was made able to be produced and the information and knowledge relied upon for its production; as is a poor description of a knowledge graph.\n\nThe use of technological tools is becoming increasingly instrumental. There are qualitative differences between different types of tools that are made available; and, the functional properties built into them.\n\nThe ability to discern fact from fiction increasingly relates to the information sources relied upon to form an opinion; alongside the revenue model employed in relation to those sources, where some sources can in many cases be made known to be less than accurate, some subjective, alongside others that may be too complex and/or specialised.\n\nThe means through which we ‘think’, is for the most-part built upon our ‘learned’ or ‘lived experiences’.\n\n> “the distinction between reality and our knowledge of reality, between reality and information, cannot be made” [Anton Zeilinger](https://www.nature.com/articles/438743a)\n\nWhere we are made to consume information that is entirely unreliable, as apposed to nature; our view of the world (or of any particular subject) becomes distorted. Our freedom to think, becomes taxed. This is a core constituent to the operating economic models exploited online and the effects of it are not easily contained. Part of the drivers are described by [attention economy](https://en.wikipedia.org/wiki/Attention_economy) concepts; whilst thats only a small part of it.\n\nSemantics in the real world are dynamic and both entity and temporally unique. Our information systems do not support the nuances brought about by the exchange between online socioeconomics, and our natural world.\n\nThe trend of how data influences our world is increasing as the proliferation of internet connected devices, and the use of information management systems, increasingly relates to our environments. These systems are currently operated in a manner that is designed to most benefit the provider of ‘things’.\n\nThis can have implications on what should be said near them, even in private, and what should be expected of them; as the ‘operators’ are most-often influenced by commercial opportunity and lawful requirements put upon them; as does relate to them, as a corporate legal entity from somewhere in the world.\n\nBefore these changes, before[ pervasive surveillance](https://www.webizen.net.au/about/the-vision/a-technical-vision/), the means to impart knowledge as to communicate knowledge between people was freely able to be done, without significant cost in most circumstances. A childs parents did not need to pay anyone to teach their children about plants, animals, places and knowledge relating to being human, to living in our world and being able to best make use of it.\n\nThis was most-certainly improved through books, schooling and other resources; but not quite in the same way that now impacts all of us, in our internet connected world.\n\nThe means to retain and enhance our ‘freedom to think’, requires our society to think about how it is we want to deploy ICT. It needs ‘fit for purpose’ tooling that’s been design in such a way, that holds *values* to the core of its designs.\n\nDo we want a world, where we are better able to serve the needs of our loved-ones by restricting the knowledge we impart to others as to help them, as is now the case with all too many employment contracts today. do<span style=\"font-size: 1rem;\"> we want those, we depend upon, to be bound to contractual agreements for gainful employment as lawyers, public service workers, medical clinicians, law-enforcement officers or otherwise; that restrict their freedom to think.</span>\n\n> <span style=\"font-size: 1rem;\">or do we need to build something else, that can preserve freedom, to think.</span>\n\nDo we want to build economic frameworks that places unnecessary burdens upon humans, as to be entitled to exist? or,\n\nCan we extend human rights, modernise the theory, and ensure its employed, deployed and made available to all those whose **lives** are *linked* **online**.\n\nIn 2012, [Eben Moglen](https://en.wikipedia.org/wiki/Eben_Moglen) presented a talk about the problems as he considered them at the time. Where Moglen speaks of a ‘giant global graph’, he is talking about the use of *linked*-data in relation to the means through which it was being used in 2012; which has continued to radically develop since his talk.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/sKOk4Y4inVY?rel=0\" width=\"560\"></iframe>\n\nThe applied methodology for using these technologies were never, the only option; whilst it is still the case today, that a knowledge banking industry, does not exist, the considerations made in its design put the concept of ensuring the means for ‘self determination’ is kept ‘front and centre’ as to provide a means whereby the ontological definitions of self; is produced, by self. The devices that are connected to our environments, are controlled by us; not them, by default. It changes the economic models to eradicate the problem that there is still currently no available alternative in the market-sphere.\n\nThe means through which an inforg may be used to personalise the interactions between a person and every other agent (via ‘AI’); provides a means to support the needs of human beings to preserve (and enhance) their right to [self-determination](https://en.wikipedia.org/wiki/Self-determination).\n\nWhilst our society does certainly have still, many choices, some of those choices are considered to offer better outcomes. A conversation debating these issues was brought about in 2017 through my work; which included, in preparation, producing this (relatively short) clip.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/e9vROTibKiE?rel=0\" width=\"560\"></iframe>\n\nThe issues that is becoming increasingly clear, present; and in desperate need of an immediate response, have developed over sometime, its now more than 20 years old. The introduction of ‘online data storage’ to consumers was the commencement of a set of globally applied social decisions made by the few.\n\nToday, consequentially; It is not so much a computer-science based technical problem; any more than work on other areas is, for instance, the means for a group of people to design and build a new house; the tools and resources exist to do it, but its not done; until its been done.\n\nA knowledge banking industry; is considered by me, to be the best way to solve the problem. Indeed, *all things sound good in theory*, yet, the means to ensure something like an knowledge fiduciary, as an extension of our role as citizens through which our representatives make laws on behalf of us, to keep us safe. It is indeed through the application of new tooling that is brought about by a ‘knowledge banking industry’ that civilian engagement in systems of democracy can be radically enhanced.\n\nThe tooling required to ensure our ability to work cooperatively, can bring about new and improved democratic practices; that can be built into our ICT infrastructure as to make use of the same data systems, to form radically different systems than those depended upon today.\n\nDoing, building a decentralised model as is required to address the broader issues; in-turn, provide far better means, to preserve our needs, our freedom to think.\n\nThe use of Artificial intelligence can be used in many, many ways and reality ***does not necessarily need to be presented,*** to you in any way. The hope is,\n\nthat the tools for you to make decisions about what you do with it, are not simply maintained by you; but moreover, built as instrumental parts of our future, as independent persons who live in a natural world made to be the priority over all other things. That STEAM continues to be made most important; and that our freedom to think, freedom of thought, is not sold.\n\n> STEAM = science, technology, engineering ART and mathematics. Because the world with out art is eh.\n> \n> — Timothy Holborn (@SailingDigital) [February 7, 2017](https://twitter.com/SailingDigital/status/828918333365817344?ref_src=twsrc%5Etfw)\n\n<script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","title":"History: Global Governance and ICT."},"frontmatter":{"draft":false},"rawBody":"---\nid: 518\ntitle: 'History: Global Governance and ICT.'\ndate: '2018-10-02T12:24:09+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=518'\ninline_featured_image:\n    - '0'\n---\n\nThe development of today electronic Information Management Systems stem from the works produced for war, and the means through which these works continued, thereafter.\n\n> “Progress, far from consisting in change, depends on retentiveness. When change is absolute there remains no being to improve and no direction is set for possible improvement: and when experience is not retained, as among savages, infancy is perpetual. Those who cannot remember the past are condemned to repeat it.”\n> \n> *Source: [p. 284 of Reason in Common Sense. The Life of Reason, by; George Santayana](https://en.wikipedia.org/wiki/The_Life_of_Reason)*\n\n<span style=\"font-weight: 400;\">As we continue to build upon the </span>*<span style=\"font-weight: 400;\">prior art</span>*<span style=\"font-weight: 400;\"> forged to make the tools and instruments we use of today, many may reasonably consider it to be important, to garnish a level of understanding about relevant history.</span>\n\n<span style=\"font-weight: 400;\"> This can in-turn be broken down into a number of constituents, starting, perhaps most importantly, with a means to illustrate the circumstances of political will and manifest outcomes brought about by the express works of societal leaders whose works formed some of the more critical historical steps, that have been brought about. My consideration being, that by reviewing the socio-environmental landscape, it can assist those considering current challenges, exhibited consequentially, and support means to evaluate how these systems have manifested themselves today and what could be considered, if we’re seeking to rectify known issues.</span>\n\n<span style=\"font-weight: 400;\"> We all strive to improve safety and quality of life as experienced by ourselves and others. </span>\n\n<span style=\"font-weight: 400;\">The means to make use of the collective assets of human knowledge and our natural world, is critical to our means to meet our societal challenges. Communications technologies provides critical infrastructure for</span><span style=\"font-weight: 400;\"> global cooperative development of solutions and our means to solve shared problems (and evolve new ones). </span>\n\n<span style=\"font-weight: 400;\">At the start of the 20th century one of the greatest challenges humanity faced, was the means to forge and maintain a reliable basis for trusted claims to be maintained and made use of. Whilst the moral purpose to preserved a shared ethical sense of dignity for acts; inclusive to how those acts became considered in historical terms; the need for supportive apparatus remain to be consistent and truely needed, as to make distinct good from bad, lawful to unlawful, and information as a class or category associated to knowledge.</span>\n\n<span style=\"font-weight: 400;\">During the time of the first and second world-war, our sciences were used as a means to cause destruction, and catastrophic damage, to societies and our environmental domiciles. </span>\n\n<span style=\"font-weight: 400;\">Security of personhood became universally sought. </span><span style=\"font-weight: 400;\">World leaders responded by formed an international environment for diplomacy by way of the [United Nations](http://www.un.org/en/sections/history/history-united-nations/) (“UN”). Through works harboured by United Nations, a definition of universal human rights was established and agreed to by nations across the world.</span>\n\n<span style=\"font-weight: 400;\">These works on rights, forged a recording of universal shared values for all people throughout the world, make available in plain language, for use by the human race. </span>\n\n<span style=\"font-weight: 400;\">The consequence of these societal governance works led to providing the legal support for means through which the cooperative frameworks of mankind, to foster a new age of innovation and prosperity was born.</span>\n\n<span style=\"font-weight: 400;\">In effect, it was the realisation of a body of work that could be considered to have first made use of universal moral grammar. </span>\n\n<span style=\"font-weight: 400;\">In the post world-war II era, it was through the use of empathy, that humanity world-wide, first forged social tools to better equip </span>*<span style=\"font-weight: 400;\">corpus</span>*<span style=\"font-weight: 400;\"> a means to process the various embodiments of ‘rule of law’, as to engender the means to ratify an underlying sense of the ‘spirit’ of law, that could be shared across jurisdictions. To make use of common principles, in circumstances of disagreement, and diplomacy. </span>\n\n<span style=\"font-weight: 400;\">These acts are amongst the most important, as to have “set the stage”, through which humanity was provided means to thrive; and to collectively benefit, from the creative works made by persons of disparate cultural and socioeconomic groups. These principles for peaceful economic growth were instrumental in ensuring means to promote and foster significant advancements of humanity, as a whole.</span>\n\n<span style=\"font-weight: 400;\">Therein and Thereafter; amongst the greatest influencing factors is international trade,</span>\n\n<span style=\"font-weight: 400;\">A constituent of works was founded upon forming appropriate principles for international trade and social alliances, as they pertain to the foundations of nationhood that tied the efforts of any person in any location in our natural world a means through which they may engage economically with others. In-turn, this occurred in an era where both communications and transportation technologies evolved rapidly (notably including the advent of the [shipping container](https://en.wikipedia.org/wiki/Containerization)). This in-turn brought about the need to ensure delegate authorities, of provisional fiduciary responsibility relating to spatial regions (jurisdictions); maintained their social sense of sovereignty, and means to maintain ‘lands ruled by law’. Through this, humanity continued to garnished improved means for respect, consideration and tolerance of issues relating to cultural diversity, as mankind developed its modern, global economy. </span>\n\n<span style=\"font-weight: 400;\">In consideration; it is my view that it was through the seminal instruments brought together by way of the united nations, that our forum of international dialogue and means for cooperative advancement of shared interests; was brought about politically, as to engender the prosperous grounds for shared experiences. Through the fruits of these works, the sharing of knowledge and opportunity, freedoms of thought, movement and association became increasingly a field of political endeavour. Moreover, the means for a large proportion of humanity to participate in these affairs is something most still want to serve and protect. </span><span style=\"font-weight: 400;\"> </span>\n\n<span style=\"font-weight: 400;\">The primary resource of human creativity; has been applied upon a foundation of principles defined by groups such as the UN. This in-turn, has provided the necessary basis of support required for the advancement of technology through Science, Technology Engineering and Mathematics (or [S.T.E.A.M](https://en.wikipedia.org/wiki/STEAM_fields) to to include art or creativity) that delivers our means to define what is our shared manifestation of our present shared and containerised sense of reality. Both as individuals, and as members of various groups, we have applied the use of science to discover, produce and make use of tools that employ the electromagnetic spectrum as to make temporal notations of our natural world. We have formed a series of artificial tools and structures, used to manage how we communicate, live together, share interests, maintain life in peace.</span>\n\n<span style=\"font-weight: 400;\">In 2045, The Atlantic published an article</span> <span style=\"font-weight: 400;\">by [Vannevar Bush](https://en.wikipedia.org/wiki/Vannevar_Bush) titled ‘[As We May Think](https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/)’. Dr. Vannevar Bush had coordinated the activities of some six thousand leading American scientists in the application of science for warfare; and his article (and related works) are considered to have been instrumental to the means through which incentive were defined for scientists when the fighting had ceased. </span>\n\n> <span style=\"font-weight: 400;\">Dr. Bush urged men of science to turn to the task of making more accessible our bewildering store of knowledge.</span>\n> \n> *“Consider a future device … in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory”. “For years inventions have extended man’s physical powers rather than the powers of his mind.*<span style=\"font-weight: 400;\">” the editor remarks. </span>\n\n<span style=\"font-weight: 400;\">This record was later a constituent of the remarks made by [Ted Nelson](https://en.wikipedia.org/wiki/Ted_Nelson) in 1965 as he recorded and presented his views on “[A File Structure for The Complex, The Changing and the Indeterminate](http://csis.pace.edu/~marchese/CS835/Lec3/nelson.pdf)”</span><span style=\"font-weight: 400;\"> , in which he declares,</span>\n\n> *<span style=\"font-weight: 400;\">“THE KINDS OF FILE structures required if we are to use the computer for personal files and as an adjunct to creativity are wholly different in character from those customary in business and scientific data processing. They need to provide the capacity for intricate and idiosyncratic arrangements, total modifiability, undecided alternatives, and thorough internal documentation.”</span>*\n\n<span style=\"font-weight: 400;\">The (creative) works of Ted Nelson went on to declare a conceptual frameworks in which he presented his ideas of a Evolutionary File Structure (ELF), as a form of electronic list packaging framework for documents; in which his design extended by way of a “Personalized Retrieval, Indexing, and Documentation Evolutionary” System (PRIDE). At this time Ted Nelson, perhaps more importantly, went about defining a method in which to produce a system for the storage of knowledge on computing systems and in so doing, defined the terms </span>*<span style=\"font-weight: 400;\">HyperText</span>*<span style=\"font-weight: 400;\"> and </span>*<span style=\"font-weight: 400;\">HyperMedia</span>*<span style=\"font-weight: 400;\"> introducing the word “hypertext” as a be defined method to address the the problem he described in the following way;</span>\n\n> <span style=\"font-weight: 400;\"> “</span>*<span style=\"font-weight: 400;\">a body of written or pictorial material interconnected in such a complex way that it could not conveniently be presented or represented on paper. It may contain summaries, or maps of its contents and their interrelations; it may contain annotations, additions and footnotes from scholars who have examined it. Let me suggest that such an object and system, properly designed and administered, could have great potential for education, increasing the student’s range of choices, his sense of freedom, his motivation, and his intellectual grasp.” </span>*\n\n<span style=\"font-weight: 400;\">Ted Nelson went on to</span> <span style=\"font-weight: 400;\">illustrate his concept as to include </span>*<span style=\"font-weight: 400;\">HyperFilm</span>*<span style=\"font-weight: 400;\"> as one form of </span>*<span style=\"font-weight: 400;\">[HyperMedia](https://en.wikipedia.org/wiki/Hypermedia).</span>*\n\n<span style=\"font-weight: 400;\">Between 1965 and the 1980s computing technology continued to advance and become more accessible as an international communications networks became a part of life in many homes; and the proliferation of related appliances became more accessible for individuals; which in-turn promoted rapid growth in institutions and the means for which information and media technologies extended to the ‘desktops’ of professionals, replacing typewriters, calculators, classical inspection, observation and information management tools.</span>\n\n<span style=\"font-weight: 400;\">The work by [Robert Elliot Kahn &amp; Vinton Gray Cerf on internet protocol](https://www.internetsociety.org/wp-content/uploads/2017/09/ISOC-History-of-the-Internet_1997.pdf) (TCP/IP) considered to have started from 1975 emerged as to make available Internet from the mid-1980s and was made available to connect computers throughout the world over telecommunications systems providing a universally compatible means to communicate information between computing systems world wide. </span>\n\n<span style=\"font-weight: 400;\">[Tim Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee), working with CERN [submitted a proposal](http://info.cern.ch/Proposal.html) for an information management system to his boss, Mike Sendall In March 1989. Sendall wrote on the proposal ‘Vague, but exciting’, and allowed Berners-Lee to continue.</span><span style=\"font-weight: 400;\"> </span>\n\n> *<span style=\"font-weight: 400;\">The proposal concerned the management of general information about accelerators and experiments at CERN. It discusses the problems of loss of information about complex evolving systems and derives a solution based on a distributed hypertext system.</span>*\n> \n> source: https://www.w3.org/History/1989/proposal.html\n\n<span style=\"font-weight: 400;\">Tim Berners-Lee was afforded the opportunity to make these works of art available to the world, royalty free, and the use of his works exploded rapidly. </span>\n\n<span style=\"font-weight: 400;\">In 1992 Cerf and Kahn formed the [Internet Society](https://en.wikipedia.org/wiki/Internet_Society) to foster and guide the growth and development of ‘The Internet’. </span>\n\n<span style=\"font-weight: 400;\">At around the same time Tim Berners-Lee, in an effort to form a means in which to manage the growth of work founded upon his invention/s ‘The World Wide Web’, in a manner that maintained the means for the environment created as a result be produced by means that ensured the tools required to participate and contribute were ‘free from royalty’ encumbrances for all who wanted to use it; In October 1994, Tim Berners-Lee founded the[ World Wide Web Consortium](https://www.w3.org/wiki/The_history_of_the_Web) (W3C) at the Massachusetts Institute of Technology, Laboratory for Computer Science \\[MIT/LCS\\] in collaboration with CERN, with support from DARPA and the European Commission. In April 1995, INRIA (Institut National de Recherche en Informatique et Automatique) became the first European W3C host, followed by Keio University of Japan (Shonan Fujisawa Campus) in Asia in 1996</span>\n\n<span style=\"font-weight: 400;\">The World Wide Web Consortium (W3C) has developed to become an international community where Member organizations, a full-time staff, and the public work together to develop Web standards. W3C’s mission is to lead the Web to its full potential W3C’s mission is to lead the Web to its full potential.</span><span style=\"font-weight: 400;\"> and a W3C Patent Policy, governs the handling of patents in the process of producing Web standards. The goal of this policy is to assure that Recommendations produced under this policy can be implemented on a Royalty-Free (RF) basis. </span>\n\n<span style=\"font-weight: 400;\">These works (and many others) have been instrumental to the foundation of Internet Governance</span><span style=\"font-weight: 400;\"> humanity depends upon today. Yet, in consideration, it is the case that these remarkable changes have occurred in a period of time that is within the living history of those who today, remember the war.</span>\n\nWhilst those people do not necessarily know how to use our modern technology, those who do weren’t alive when the world was making critical decisions post-war. The lack of comprehension of our world before modern science and the benefits of those works, as is the case for example with modern medicine.\n\nIt was the case that the world decided to build the means to trust in each-other. This was the foundation through which, we now live with “fake news”. The concepts of war are not anywhere near as new as the medium through which modern warfare is moreover carried out in utility of technology, on many levels.\n\nThe means through which decisions were made by those who are elders of today, was based on personal relationships and systems that did not rely upon database technologies or the design of web-systems, as is used to define people today. As those critical, global decisions were being made; it was mostly printed products, that assisted communities who were more-often interpersonally known to each-other, that formed the means through which decisions of importance; were made. Today that is not so much the case.\n\nHistory does help tell a story about how it has become that whilst legal personalities of all forms now rely upon the use of database related, information technologies; that the systems to support people, still haven’t been made. It is considered that the reason why this is the case, is due to how it all came about.\n\nWe can of course change these circumstances, as others changed theirs and those of their community, before our time was brought about to be challenged to do the same.\n\nThere are a few other related, yet distinct considerations with respect to economics and banking that i’ll cover in a seperate article, that will be linked-here – when its been uploaded."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","title":"Personal Augmentation of AI"},"frontmatter":{"draft":false},"rawBody":"---\nid: 636\ntitle: 'Personal Augmentation of AI'\ndate: '2018-10-04T18:59:14+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=636'\ninline_featured_image:\n    - '0'\n---\n\nArtificial Intelligence, or AI, is a complex set of interrelated concepts that in-turn bring about the means to programmatically define how software agents are made to work. Whilst computational power or energy can be applied to solve problems made otherwise impossible; The principle means employ to employ ‘artificial intelligence’ services in a Human Centric Web, powered by a knowledge banking industry, is by way of personally defined decisions.\n\nIn this way, the means through which RDF related semantics is applied helps to shape the personalisation of relationships, overtime, with other agents.\n\nIn effect, what is brought about is the means for AI agents, to be an extension of self, like a prosthetic instrument. The decisions people make in applications today, can be translated to modal considerations made by those persons at the time; in a manner that can both retain the resolution of decisions made earlier, in addition to providing the means through which personally define changes can occur that in-turn changes the way a persons AI agent behaves.\n\nThis in-turn also supports the means to alter the resolution of queries, for example, to decide whether a person can identify the exact location of a person, or the approximate location of a person. Whether and what is able to be identified through the use of Ai related technologies with biometric signatures. there are many parts to a persons ‘inforg’ and all these parts are able to be interoperable, defined, in-terms of what is able to be ‘augmented’ as is provided to any agent, individually, via ‘knowledge banking’ infrastructure.\n\nThe solution to AI, is to build a human centric web, to make it an extension of self. This is a not insignificant part, of what is brought about by these works.\n\n(this is a quick draft – more on this to be provided later)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","title":"Semantic Inferencing"},"frontmatter":{"draft":false},"rawBody":"---\nid: 707\ntitle: 'Semantic Inferencing'\ndate: '2018-10-06T16:43:53+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=707'\ninline_featured_image:\n    - '0'\n---\n\nPlaceholder page.\n\n\n```html\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/videoseries?list=PLCbmz0VSZ_vqWAb5NHkaEvsT9RA8--QkY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n```\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","title":"Web of Things (IoT+LD)"},"frontmatter":{"draft":false},"rawBody":"---\nid: 631\ntitle: 'Web of Things (IoT+LD)'\ndate: '2018-10-04T14:11:31+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=631'\ninline_featured_image:\n    - '0'\n---\n\nThe ‘Web of Things’ is a term used to describe approaches to software development that allow real-world objects to be part of the semantic web.\n\nInstrumentally, Web of Things related works incorporate the use of [linked-data](https://www.webizen.net.au/what-is-linked-data/) &amp; whilst most of the documentation relates in-turn to the use of HTTP URIs, [DIDs](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/) also work; which in-turn briefly considers means through which passive objects (such as coffee cups) can be tagged to become part of the ‘web of things’ sphere of knowledge (“infosphere”), alongside the means through which ‘sensing’ (such as [computer vision](https://www.webizen.net.au/image-recgonition-video-playlist/) tools) and related semantics, whether made to be part of the ‘[commons](https://www.webizen.net.au/about/references/the-need-for-decentralised-open-linked-data/)‘ or [otherwise;](https://www.webizen.net.au/media-analysis-part-2-visual/) Part of the challenge is going to be, how to technically manage an ‘internet of things’ Future.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/4QTAtFaIiyc?rel=0\" width=\"560\"></iframe>\n\n- How will these systems work for environments with those known to them, alongside others.\n- How will these systems be made interoperable with other systems?\n- How will apps and the means to use these systems, be made scalable?\n- How will ‘things’ as tools, be made best use of to speed development?\n- Who is the beneficial owner to things? Who must have access to them?\n- Upon what basis are we made to rely upon ‘things’ that interweave into our world?\n- Will we need a subscription or can we still buy products?\n\nTraditional “API” based approaches to the development of IoT can not scale in the same way a ‘Web of Things’ approach brings about through means that are otherwise impossible for almost any other alternative.\n\nImportant links about Web of Things can be found in the [links section](https://www.webizen.net.au/resource-library/link-library/), alongside other posts such as the quick intro to Web of Things [post](https://www.webizen.net.au/web-of-things-an-introduction/).\n\nAmongst the other considerations that are notably added herein; is a consideration that i’ve only recently considered with respect to the choices made by the [solid team](https://solid.mit.edu/), in electing to develop a [nodejs based solid-server](https://github.com/solid/node-solid-server). Should it be required that people will require a local server to preserve the means through which they operate their environments (ie: homes, cars, boats, etc.) this nodejs environment being built; would suit those needs well, although it is also considered that it would in-turn benefit greatly, through the additional services able to be offered by way of a knowledge banking industry."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","title":"An introduction to Credentials."},"frontmatter":{"draft":false},"rawBody":"---\nid: 544\ntitle: 'An introduction to Credentials.'\ndate: '2018-10-02T15:46:25+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=544'\ninline_featured_image:\n    - '0'\n---\n\n[![](https://www.webizen.net.au/wp-content/uploads/2018/10/Screen-Shot-2018-10-02-at-3.38.10-pm-1024x595.png)](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/screen-shot-2018-10-02-at-3-38-10-pm/)\n\n<span style=\"font-weight: 400;\">A </span>***credential***<span style=\"font-weight: 400;\"> is a qualification, achievement, quality, or piece of information about an entity’s background, typically used to indicate suitability. A credential could include information such as a name, home address, government ID, professional license, or university degree. The use of credentials to demonstrate capability, membership, status, and minimum legal requirements is a practice as old as society itself. The potential use cases are innumerable, but common important examples are represented here in the domains of: </span>\n\n- **Education**<span style=\"font-weight: 400;\"> – where academic credentials and co-curricular activities are recognized and exchanged among learners, institutions, employers or consumers</span>\n- **Workplace**<span style=\"font-weight: 400;\"> – where an applicant’s certified skill or license is a condition for employment, professional development and promotability </span>\n- **Civil Society**<span style=\"font-weight: 400;\"> – where access to social benefits and contracts may be based on verifiable conditions such as marital status</span>\n- **Payments** <span style=\"font-weight: 400;\">– where the legal right to purchase a product depends on the verifiable age or location of the buyer</span>\n- **Identity** <span style=\"font-weight: 400;\">– where an entity presents credentials to prove their identity or qualification</span>\n- **Ownership** – where an entity presents proof of ownership of a particular asset or a right to perform specific operations against a resource. Examples are ownership of securities that entitles the entity to dividend payments or the authority to transact on an account.\n\nCredentials is the subject of work carried out via W3C where a [Credentials Community Group was established](https://www.w3.org/community/credentials/2014/08/06/call-for-participation-in-credentials-community-group/), and continues to develop linked-data compatible solutions, notably including [Decentralised IDentifiers](https://www.w3.org/TR/verifiable-claims-data-model/), which supply the means for 3rd party verifiable claims, or credentials, to be protocol independent.\n\n##### **Commerce and Credentials**\n\n<span style=\"font-weight: 400;\">The intended design paradigm for credentials is intended to supply the means to resolve a solution, by way of technological embodiment, how it is that socio economic participation may in-turn form economic relations. In this way it is envisaged that the operation of a group may be entirely made manageable online. The form of how these claims are rendered in some examples, such as those used by[ IMS Global](https://www.imsglobal.org/sites/default/files/Badges/OBv2p0/index.html), make use of linked-data, which means the machine-readable information embedded in them can be used.</span>\n\n[![](https://www.webizen.net.au/wp-content/uploads/2018/10/Screen-Shot-2018-10-02-at-3.37.47-pm-1024x582.png)](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/screen-shot-2018-10-02-at-3-37-47-pm/)\n\n<span style=\"font-weight: 400;\">Increasingly; the means in which to interact with government is [being made available via API](http://pipka.org/2015/09/23/government-as-an-api-how-to-change-the-system/).</span><span style=\"font-weight: 400;\"> In-so-doing, the means in which a ‘legal personality’ may be entirely managed by way of a series of online services, made useful by cryptographically enhanced ledgers; makes possible the means in which to form comprehensive means in which to manage the interactions of humans and their </span>*<span style=\"font-weight: 400;\">‘creative works’</span>*<span style=\"font-weight: 400;\">, with one or more </span>*<span style=\"font-weight: 400;\">legal personality</span>*<span style=\"font-weight: 400;\"> (‘persona ficta’); and that in so doing, new ‘emergent’ opportunities (and challenges) are then able to evolve. </span>\n\n<span style=\"font-weight: 400;\">Whilst I personally have a particular regard for the production of new formats that help to improve social and philanthropic works of public benefit; in addition to the means in which to render service for the needs of individuals, in my works to realise these tools; now therefore, an array of enterprise is now made possible in a manner that was not possible before. The simple foundation to all such things is the means in which to design the foundations in which the electronic extension to personhood; be made by design, to be of primary custodianship to that same person. </span>\n\n<span style=\"font-weight: 400;\">The means in which a ‘corporation’ (group) may thereafter form significantly improved management systems flows from this foundation as the formation and management of ‘their own’ ‘proprietary inforg’; can then built upon environments that performs an array of self-archiving functions, rendered in a form consistent with the use of ‘[CoolURIs](https://www.w3.org/TR/cooluris/)’,</span><span style=\"font-weight: 400;\"> where human knowledge and systems of societal governance in a fault-tolerant, broader infosphere; makes use of decentralised semantic notations and the pragmatics of inter-networking relations as pertains to all systems of human governance.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","title":"Making the distinction between ‘privacy’ and ‘dignity’."},"frontmatter":{"draft":false},"rawBody":"---\nid: 554\ntitle: 'Making the distinction between ‘privacy’ and ‘dignity’.'\ndate: '2018-10-02T15:51:05+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=554'\ninline_featured_image:\n    - '0'\n---\n\n*<span style=\"font-weight: 400;\">Dignity Enhancing Knowledge Management Systems Development. </span>*\n\n<span style=\"font-weight: 400;\">privacy</span>\n\n<span style=\"font-weight: 400;\">ˈprɪvəsi,ˈprʌɪvəsi/</span>\n\n*<span style=\"font-weight: 400;\">noun</span>*\n\n1. <span style=\"font-weight: 400;\">a state in which one is not observed or disturbed by other people.</span>\n2. <span style=\"font-weight: 400;\">“she returned to the privacy of her own home”</span>\n\n| 1. *<span style=\"font-weight: 400;\">synonyms:</span>* | - [<span style=\"font-weight: 400;\">seclusion</span>](https://www.google.com.au/search?num=100&safe=off&q=define+seclusion&forcedict=seclusion&sa=X&ved=0ahUKEwiS-YbnhvbTAhWCFZQKHQ5bCaUQ_SoIKjAA)<span style=\"font-weight: 400;\">, privateness, </span>[<span style=\"font-weight: 400;\">solitude</span>](https://www.google.com.au/search?num=100&safe=off&q=define+solitude&forcedict=solitude&sa=X&ved=0ahUKEwiS-YbnhvbTAhWCFZQKHQ5bCaUQ_SoIKzAA)<span style=\"font-weight: 400;\">, </span>[<span style=\"font-weight: 400;\">isolation</span>](https://www.google.com.au/search?num=100&safe=off&q=define+isolation&forcedict=isolation&sa=X&ved=0ahUKEwiS-YbnhvbTAhWCFZQKHQ5bCaUQ_SoILDAA)<span style=\"font-weight: 400;\">, </span>[<span style=\"font-weight: 400;\">retirement</span>](https://www.google.com.au/search?num=100&safe=off&q=define+retirement&forcedict=retirement&sa=X&ved=0ahUKEwiS-YbnhvbTAhWCFZQKHQ5bCaUQ_SoILTAA)<span style=\"font-weight: 400;\">, </span>[<span style=\"font-weight: 400;\">peace</span>](https://www.google.com.au/search?num=100&safe=off&q=define+peace&forcedict=peace&sa=X&ved=0ahUKEwiS-YbnhvbTAhWCFZQKHQ5bCaUQ_SoILjAA)<span style=\"font-weight: 400;\">, peace and quiet, peacefulness, </span>[<span style=\"font-weight: 400;\">quietness</span>](https://www.google.com.au/search?num=100&safe=off&q=define+quietness&forcedict=quietness&sa=X&ved=0ahUKEwiS-YbnhvbTAhWCFZQKHQ5bCaUQ_SoILzAA)<span style=\"font-weight: 400;\">, lack of disturbance, lack of interruption, freedom from interference; </span><span style=\"font-weight: 400;\">More</span> |\n|---|---|\n\n- <span style=\"font-weight: 400;\">the state of being free from public attention.</span>\n- <span style=\"font-weight: 400;\">“a law to restrict newspapers’ freedom to invade people’s privacy”</span>\n\n<span style=\"font-weight: 400;\">dignity</span>\n\n<span style=\"font-weight: 400;\">ˈdɪɡnɪti/</span>\n\n*<span style=\"font-weight: 400;\">noun</span>*\n\n1. <span style=\"font-weight: 400;\">the state or quality of being worthy of honour or respect.</span>\n2. <span style=\"font-weight: 400;\">“the dignity of labour”</span>\n    - <span style=\"font-weight: 400;\">a composed or serious manner or style.</span>\n    - <span style=\"font-weight: 400;\">“he bowed with great dignity”</span>\n\n<span style=\"font-weight: 400;\">The distinction between privacy and dignity is in some ways considered from the point of view of a particularly personal form of act; as is the nature of defining an agent that in-turn defines its custodian.</span>\n\n<span style=\"font-weight: 400;\">In this way, the consideration that privacy should restrict from the person / subject, knowledge of how this interference pattern may cause a direct effect to them; or that they are not entitled to be furnished means in which to respond to what it is they may therefore be knowingly subjected to processes of which they are made allowances to know; or that an event that may factually been proven to have caused serious injury due to the actions (or lack thereof) of others; as to cause beneficial grounds in which any acts in breach of law be rendered by effect impunity; is now therefore whilst too often considered meritorious upon the basis of privacy – adjunct to the interests of dignity; as to form poisonous grounds in which bad actors are rendered meaningful service. Now therefore; the principle frame in which dignity may pertain; not simply includes the consideration of confidentiality (or privacy) but also the means in which to consider the acts of others as to be valued and respected in a manner that is based upon the means in which we act as to treat each-other ethically.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","title":"Roles &#8211; Entity Analysis"},"frontmatter":{"draft":false},"rawBody":"---\nid: 533\ntitle: 'Roles &#8211; Entity Analysis'\ndate: '2018-10-02T12:55:24+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=533'\ninline_featured_image:\n    - '0'\n---\n\nThe manner in which any ‘information management’ system is managed, relates to different types of roles of actors; whereby relationships between the activities of an agent and/or actor carrying out that role; needs to be put in context by way of an ‘entity analysis’, evaluation model, [modality](https://en.wikipedia.org/wiki/Modality) or [graph](https://en.wikipedia.org/wiki/Graph_theory).\n\nIt is important to note that jurisdictional considerations have an influence on the definition, role and rights of each of these forms of ‘active actors’, in a manner that forms what is considered to be similar to an [interference pattern](https://en.wikipedia.org/wiki/Wave_interference), as may be programatically queried, evaluated and/or made use of.\n\n##### **Human Actors:** \n\n- <span style=\"font-weight: 400;\">A human actor is the only</span><span style=\"font-weight: 400;\"> current natural-world actor. </span>\n- <span style=\"font-weight: 400;\">A real-world actor may be subject to legal penalties in a manner that is distinct to the role of any other legal actor.</span>\n- <span style=\"font-weight: 400;\">Human actors are rendered means in which to socioeconomically participate by way of the socio-geopolitical mechanic of personhood; in which citizenship makes distinct those whom a series of interpretive definitions apply in a particular form of manifest; vs. those considered to be ‘alien’ to any-such geographically defined environment.</span>\n\n##### **Persona Ficta;** *<span style=\"font-weight: 400;\"> ‘[legal person](https://en.wikipedia.org/wiki/Legal_person)‘ or ‘legal personality’</span>*\n\n- *<span style=\"font-weight: 400;\">legal personalities, incorporating a group of persons as to form a group personality </span>*<span style=\"font-weight: 400;\">which is rendered means of existence by way of shared responsibilities and whom by such means may own tangible legal rights whether ‘artificial’ (copyright), commons subject (law) or natural (land). </span>*<span style=\"font-weight: 400;\"> </span>*\n- <span style=\"font-weight: 400;\">Is operated by way of human actors whose responsibilities and custodianship may change from time to time.</span>\n\n##### **[Persona](https://en.wiktionary.org/wiki/persona) [artificium](https://en.wiktionary.org/wiki/artificium)**\n\n*<span style=\"font-weight: 400;\">An Artificial personalities that is brought about by the work of natural persons, to be an artificial and native actor within the [infosphere](https://en.wikipedia.org/wiki/Infosphere) whose acts influence the natural world; such as, a program, an applied machine learning algorithm and other forms of artificial intelligence.</span>*\n\n- <span style=\"font-weight: 400;\">A work of art produced by one or more persons which by its operation brings cause to influence the acts of others in a manner that may or may not be intended by its custodians.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","title":"Social Informatics Design Considerations"},"frontmatter":{"draft":false},"rawBody":"---\nid: 670\ntitle: 'Social Informatics Design Considerations'\ndate: '2018-10-05T15:49:51+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=670'\ninline_featured_image:\n    - '0'\n---\n\n### Introduction\n\nI consider this document, to be my philosophically aligned, playground. I illustrate my frustrations, i illustrate the environment where solutions exist, noting, it is logically impossible to disassociate a persons consciousness from their ability to communicate and explore concepts of the world around them.\n\nFor any person to form a view and to communicate that view as to be understood by others, depends upon an array of functional properties that are applied to computing systems, using math and physics.\n\nSocial informatics systems interfere with the means for persons to communicate. Most of the time, this is considered in terms of the benefits.\n\n> World was a much smaller place before the advent of the telephone.\n\n<span style=\"font-size: 1rem;\">The practice of forming communications events requires each human entity participating to be conscious, to have, what we call, </span>consciousness<span style=\"font-size: 1rem;\">. </span>\n\n> something that’s existed long before the telephone.\n\n<span style=\"font-size: 1rem;\">The application of social informatics design principles attends to the relational interference patterns that emerge both personally, and socially, as a result of informatics design as applied to ICT.</span>\n\n<span style=\"font-size: 1rem;\">An intended distinction is made between a ‘human centric’ approach as an alternative to the others that are operating today. The purpose of a human centric approach focuses upon the material impact (“interference pattern” relating to human consciousness) as is experienced by humans; whereby the informatics design principles centres upon the needs of humans, as the natural world stakeholders actively involved in the system, rather than any other alternative. In-order for this field of design principles to be made ‘fit for purpose’, it does indeed depend upon the proper use of it, in the real world.</span>\n\n### Web 2.0 vs. a Human Centric Informatics model\n\nInformatics modelling is known to exhibit ‘interference patterns’ that lead to effect on the natural world. Web 2.0 informatics models are known to have an array of significant short-comings, Irrespective of the (short-term) gains made by the artificial actors involved in the informatics systems processing model platforms; the means to describe considerations involved in remodelling informatics systems to be ‘human centric’ is complex and multifaceted.\n\nTo address the modal requirements involved, an array of different fields of study is conversantly required to be brought together.\n\nThis is achieved through conditioned responses (somewhat referred to as ‘[classical conditioning](https://en.wikipedia.org/wiki/Classical_conditioning)‘) as has factually related to lived experience, ‘intelligence’; and as a constituent of considerations therein,\n\n> the developmental comprehension of languages.\n\nIt is presently the case that the convergence of language or linguistic use does not always provide the same meaning between languages. Whilst there are easy examples,\n\n<figure aria-describedby=\"caption-attachment-718\" class=\"wp-caption aligncenter\" id=\"attachment_718\" style=\"width: 525px\">[![](https://www.webizen.net.au/wp-content/uploads/2018/10/Screen-Shot-2018-10-07-at-2.31.15-pm-1024x247.png)](https://translate.google.com/#en/id/water)<figcaption class=\"wp-caption-text\" id=\"caption-attachment-718\">https://translate.google.com/#en/id/water</figcaption></figure>the greater problems emerge when complex concepts are provided shorthand notation that do in-turn take quite some time to translate and support the growth of comprehension to the recipient; which is in-turn an experience that forms a body of experiences to the person imparting the knowledge.\n\nWhilst our networked computing systems of today are far more “intelligent” than humans in this way; as is demonstrated by typing any question into google, which will rapidly draw a world of knowledge in a way that is impossible for any human agent to do; the emergence of these social machines in a web 2.0 construct is indeed causing problems.\n\n<figure aria-describedby=\"caption-attachment-719\" class=\"wp-caption aligncenter\" id=\"attachment_719\" style=\"width: 525px\">![](https://www.webizen.net.au/wp-content/uploads/2018/10/22426309_1963344347247232_6591348862593366590_o-1-3-768x1024.jpg)<figcaption class=\"wp-caption-text\" id=\"caption-attachment-719\">The book Social Machines can be found in the [books](https://www.webizen.net.au/resource-library/book-library/) section of the resources library provided by this site.</figcaption></figure>#### Interpretational modality\n\nUniversally, the design requirement to include considerations about how persons communicate their views, as related to the use of languages and the conditioned behavioural responses formed as a constituent of the apparatus that forms consciousness; needs to be applied by professionals both;\n\na. the means through which their thoughts about *how it is,* their modality of consciousness works; and,\n\nb. the means they elect to employ any modality, to communicate (to others) at any given time.\n\nThis in-turn illustrates in a somewhat plain way, the means through which persons are made able to create their own interference patterns, as to make use of the freedom of choice they have biologically, as is concussively disaffected by poor informatics systems design and any social reliance systems build upon the failings of these systems.\n\nA nuance therein, is that the discussion about *the implications their discoveries,* and *their own means to find understanding of themselves* is in-turn inextricably linked to *their own form of consciousness*.\n\nThe repercussive effect of *parallel conversations*, about how their works has applied to them and how the product of their work, can be applied to other ‘things’.\n\nSeemingly, in-turn, considerations in different ways, are made by the interpretations by experts; who are known as to have served humanity, and an array of eloquent nuances that are difficult for social machines to non-declaratively comprehend. This intricate relationship produced by humans, in communications that form bridges between the social-sciences, math, physics &amp; information sciences; becomes a core field of emergent human expertise, as an emergent field of specialisation, instrumental for society.\n\nThe field itself, speaks to the role of ethics in the development of medium. This is not unlike the realm of ethics sought to be depended upon by others in fields of significant social reliance, whether it be the job of a person carrying out their field of expertise as a judge, law enforcement officer, doctor, engineer or technologist; there is an innate sense of ‘social contract’, that our current ICT systems as have forged our Web 2.0 environment have not met.\n\n### Informatics Design 101\n\n> now therefore; here’s a quick intro…\n\n[![](https://www.webizen.net.au/wp-content/uploads/2018/10/Screen-Shot-2018-10-05-at-1.26.19-pm-1024x641.png)](https://www.webizen.net.au/about/references/social-informatics-design-concept-and-principles/screen-shot-2018-10-05-at-1-26-19-pm/)\n\n> *Informatics* is a branch of information engineering. It involves the practice of information processing and the engineering of information systems, and as an academic field it is an applied form of information science. source: [wikipedia](https://en.wikipedia.org/wiki/Informatics)\n\n<span style=\"font-weight: 400;\">These works aim to confer the means to consider various forms of information systems embodiments, for our socially connected, prosthetic extension of persons via ICT. This is perhaps the most convoluted summary of complex concepts that i still feel an attempt to parse, is worthwhile.</span>\n\n#### The Emergence of Naturalised Interfaces &amp; the prosthetic ICT realm\n\nTechnology is rapidly evolving and considerations now need to include the influences Web of Things (or Internet of Things) will bring about, alongside other technological influences such as Augmented Reality Head Mounted display technologies (such as magic leap) where the vision of the future, is often considered in dystopian terms…\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"360\" loading=\"lazy\" src=\"https://player.vimeo.com/video/166807261?color=c9ff23\" width=\"640\"></iframe>\n\n[HYPER-REALITY](https://vimeo.com/166807261) from [Keiichi Matsuda](https://vimeo.com/chocobaby)\n\n#### Social systems informatics design\n\n<span style=\"font-weight: 400;\">The knowledge banking industry approach seeks to augment the way information systems work, at a rudimentary level. As highlighted throughout this site, It seeks to achieve this by way of a decentralised system of [inforgs](https://en.wikipedia.org/wiki/Inforg); in which the use of ontological forms, or semantic sentences, forms a frameworks of tools to form a temporally notated knowledge-sphere, of personal agency; capable of recording the interference causality of interactions between self, artificial and social agents may furnish improved agency of self.</span>\n\n> [\\#Language](https://twitter.com/hashtag/Language?src=hash&ref_src=twsrc%5Etfw) &amp; sentences are old concepts. [\\#RDF](https://twitter.com/hashtag/RDF?src=hash&ref_src=twsrc%5Etfw) brings this power to a [\\#SemanticWeb](https://twitter.com/hashtag/SemanticWeb?src=hash&ref_src=twsrc%5Etfw) via [\\#LinkedData](https://twitter.com/hashtag/LinkedData?src=hash&ref_src=twsrc%5Etfw) principles: <https://t.co/eLKacNgciC>. [\\#AI](https://twitter.com/hashtag/AI?src=hash&ref_src=twsrc%5Etfw) [pic.twitter.com/hHIUkPw1cP](https://t.co/hHIUkPw1cP)\n> \n> — Kingsley Uyi Idehen (@kidehen) [June 29, 2017](https://twitter.com/kidehen/status/880537550972010496?ref_src=twsrc%5Etfw)\n\n<script async=\"\" charset=\"utf-8\" src=\"https://platform.twitter.com/widgets.js\"></script>\n\nAn RDF sentence, is a machine-readable set of statements that can be used by an ICT agent to process information in a manner specified by the use of ‘rdf sentences’ produced overtime, to enable machine-readable (artificial intelligence related tooling) statements to be made by humans about how the information structures stored by a person, should be used for that person.\n\n<span style=\"font-weight: 400;\">Therein; the means of entitling natural persons to the primary custodianship of their own *cyber-self*; that is, the informatics apparatus and modelling used to relate to them socially; improves support for conditional moral grammar, to be incorporated into the use of artificial intelligence technology with ICT.</span>\n\n<span style=\"font-weight: 400;\"> Through the development of a shared capacity to define AI Agents for the management of self; the outcome is expected to be a means through which we are able to uniformly alters the conditions in which decisions are made.</span>\n\n> In so doing; improve support for preserving the basis for human rights\n\nThe introduction of smart-phones changed the way people lived their lives, the ramifications of this body of works; if employed, is far greater.\n\nPERSONHOOD\n\n<span style=\"font-weight: 400;\">In the fields of sociology, psychology and philosophy, the relationships between [Agency](https://en.wikipedia.org/wiki/Agency_(sociology)), [personhood](https://en.wikipedia.org/wiki/Personhood), [identity,](https://en.wikipedia.org/wiki/Identity_(philosophy)) [Metaphysics ](https://en.wikipedia.org/wiki/Metaphysics) have been an areas of exploration for a long time. This in-turn made-up part of what are known elements of a [liberal arts education](https://en.wikipedia.org/wiki/Liberal_arts_education).</span>\n\n<figure class=\"wp-caption alignnone\" style=\"width: 3625px\">[![](https://upload.wikimedia.org/wikipedia/commons/4/49/Hortus_Deliciarum%2C_Die_Philosophie_mit_den_sieben_freien_K%C3%BCnsten.JPG)](https://commons.wikimedia.org/wiki/File:Hortus_Deliciarum,_Die_Philosophie_mit_den_sieben_freien_K%C3%BCnsten.JPG)<figcaption class=\"wp-caption-text\">Herrad of Landsberg \\[Public domain\\], via Wikimedia Commons</figcaption></figure><span style=\"font-weight: 400;\">Whilst this has been a field of study, as it does linguistically form relationships in the natural world, for sometime, the means to inform its intended rationale when appended to what is currently employed by ICT in our modern environment is different.</span>\n\n- <span style=\"font-weight: 400;\"> [Agency](https://en.wikipedia.org/wiki/Agency_(sociology)) sameAs/similarTo [FOAF Agent](http://xmlns.com/foaf/spec/#term_Agent)</span>\n- <span style=\"font-weight: 400;\">[personhood](https://en.wikipedia.org/wiki/Personhood) sameAs/similarTo [Legal Person](https://en.wikipedia.org/wiki/Legal_person)</span>\n- <span style=\"font-weight: 400;\">[identity](https://en.wikipedia.org/wiki/Identity_(philosophy)) sameAs/similarTo [digital identity](https://en.wikipedia.org/wiki/Digital_identity)</span>\n- <span style=\"font-weight: 400;\">[Metaphysics](https://en.wikipedia.org/wiki/Metaphysics) sameAs/similarTo [metadata](https://en.wikipedia.org/wiki/Metadata)</span>\n\n<span style=\"font-weight: 400;\">The modelling of linguistic reuse, as is currently observably normalised by information management systems designs today; are engendering causal impact through both intentional and unintentional design effects. Implicitly, the distortions </span><span style=\"font-weight: 400;\">that are caused as a result of these issues, are in-turn put upon natural persons in a manner that becomes part of an interference pattern.</span>\n\nIn computer science terms, this is called a [graph](https://en.wikipedia.org/wiki/Graph_(abstract_data_type)) and is more-often used in a plural agent form, as an engineered temporal [social graph](https://en.wikipedia.org/wiki/Social_graph).\n\n<figure class=\"wp-caption alignnone\" style=\"width: 454px\">[![](http://www.physicsclassroom.com/Class/light/u12l3a1.gif)](https://www.physicsclassroom.com/class/light/Lesson-3/Anatomy-of-a-Two-Point-Source-Interference-Pattern)<figcaption class=\"wp-caption-text\">The concept of an interference pattern as may be made use of by computer algorithms can be considered in relation to the application of the same form of scientific expression in relation to light. Whereby in a ‘social graph’ the interaction between two agents does in-turn cause an interference pattern, although involving far more than simply two agents at any stage in time; over a temporally relevent period, as is otherwise consistent with the animated display of the wave demonstration.</figcaption></figure>#### Web 2.0 Economics – The Attention Economy\n\n<span style=\"font-weight: 400;\">As is brought about by applied science, these informatics models are most readily applied in a manner that is designed to sell advertising, whilst not exclusively the case. Therein, the informatics design methodology is set-up to exploit the liberal arts to drive an [attention economy](https://en.wikipedia.org/wiki/Attention_economy) to enhance engagement. Engagement, in current advertising models, drives revenue, which builds sites.</span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/50R21mblLb0?rel=0\" width=\"560\"></iframe>\n\nThe psychological effect brings about circumstances through which it intentionally causes an absence of rational relations, of otherwise by medium, accessibility and mutual recognition of intended views, between the distinct spheres, or unified rational points of view, constituting the distinct persons or identities, involved at each end-point of an end-to-end communications event. Whilst technology <span style=\"font-weight: 400;\">circumstances renders a means to maintain responsibility, this has in-turn been made by operators, to have been made mute.</span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/tNsbKtckzcM?rel=0\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">Building upon these pre-existing structures is making matters worse and there is no technical (meaning ICT) reason, why the organisations building these platforms have not been able to build alternative solutions (such as a knowledge banking industry) that would address the issues. </span>\n\n<span style=\"font-weight: 400;\"> The ramifications for how software ‘web-native’ ‘active agents’ now build upon these systems, leveraging ‘machine learning’ / ‘artificial intelligence’ technologies; to apply concepts in formula, to ‘win the game’; as they are programmed to do by persons working for the organisations that define them.</span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/Ewj4ql6pu9w?rel=0\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">Artificial Intelligence is in-effect the creation of an ‘active artificial agent’ or ‘web native’ person; controlled by the formerly most powerful artificial agents, corporations, who did not had the capacity to act without a human expressly acting on behalf of it. ‘web natives’ are different, geared for generating profitability for the corporations who operate them, they influence reality.</span>\n\n<span style=\"font-weight: 400;\"> This is new and as influential as any other former tool used for mass effect. It was formally the case that issues pertaining to the actions performed by natural persons, on behalf of companies, could lead to prosecuting the specific humans as responsible parties; the same is not the case for AI agents.</span>\n\n#### A Defined rationale for a new and different informatics structures.\n\n<span style=\"font-weight: 400;\">By modifying the modal structure in which all artificial actors are designed to act; as to bind the responsibility of acts to human actors, as is achieved in a human centric web; the foundation of having a defined environment whereby the definition of an ‘artificial actor’ is fundamentally changed. </span>\n\n> <span style=\"font-weight: 400;\">The dynamic responses of an artificial agent is defined, prosthetically, by way of an inforg, by a human person. Whilst information structures are socially exchange; the AI agent facilitating these exchanges are all different, unique.</span>\n\n<span style=\"font-weight: 400;\"> In-so-doing; the means to serve the needs of our natural world and nominated natural actors who are responsible for them (akin to children or companies via directorships) becomes more reasonably possible. The nuances of these relationships can be define, refined and provided real-world support in ways that web 2.0 based solutions have been found simply incapable of doing.</span>\n\n##### Why making a Human Centric infosphere is so important\n\n<span style=\"font-weight: 400;\">Imagine if the info-sphere was inextricably and pervasively bound to the temporal and intrinsically granulated; delegate authority, of human actors – whom collectively maintain the authoritative custodianship of our ever more complex natural world. Imagine the difference is systems were designed to represent human persons, rather than the otherwise, overly simplified economic valuation models suggested to be in the interests of corporations</span>\n\n> *<span style=\"font-weight: 400;\">Even a government is made incapable of forming means to comprehend their own irrelevance, where informatics circumstances render the personhood of its members, to be made, mute.</span>*\n\n<span style=\"font-weight: 400;\">Any group of natural persons, including those who make-up the operating institutional framework of government; whose role it is to work on behalf of the people, is in-turn made-up of people, whose views are subjectively engendered upon the basis of the information they rely upon, to do their jobs. Through the unreasonably narrowed form of informatics use, in an otherwise pervasively applied informatics environment, systemic implications brought about by ‘fake news’ and the difficulties natural persons have in providing evidence of facts, otherwise stored in online systems, acts in concert to undermine personhood. If we do not maintain the beneficial ownership of our personhood; we do not have the means to do good in any role, for any group. </span>\n\n*<span style=\"font-weight: 400;\">I considered, a long-ago, this to be some form of “conjoint, metaphysical neurosis”, as our socioeconomic systems sought to preserve a meaningful ‘sense’ of being, and of purpose; through which, the temporally faceted qualities of having been observed, as an observer; was thought by me, to require a different ‘information management systems’ design framework. </span>*\n\nOvertime i’ve found the concerns identified so many years ago, to be shared.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/LcuzNfPA584?rel=0\" width=\"560\"></iframe>\n\n#### Challenges that need to be overcome\n\n*<span style=\"font-weight: 400;\">Overtime, irrespective of our need to form a</span><span style=\"font-weight: 400;\"> ‘human centric web’, the opposite has occurred. </span>*<span style=\"font-weight: 400;\">There are multiple embodiments of how to form of </span>*<span style=\"font-weight: 400;\">methodological use of ontology, semantics and </span>*<span style=\"font-weight: 400;\">the nature through which the intended input of humans is interfered with by artificial agents, for programmatically defined purposes, on a medium that is dynamically evolving ‘autonomous’ machines and artificial super intelligence. The lens through which this is now done relates specifically, to economics. Many believe ‘bitcoin’ will be the solution, but i think they’re missing the point.</span>\n\n<figure class=\"wp-caption alignnone\" style=\"width: 1070px\">[![](https://www.webizen.net.au/wp-content/uploads/2019/02/all-the-worlds-money-infographic.png)](http://www.visualcapitalist.com/worlds-money-markets-one-visualization-2017/)<figcaption class=\"wp-caption-text\">All the worlds money in one visualisation by Visual Capitalist 2017</figcaption></figure><span style=\"font-weight: 400;\"> Our means of societal governance has made the use of computing systems inexorably tied to our means of maintaining personal safety, social security and socio-economic participation. </span><span style=\"font-size: 1rem;\">In a cashless society, where food is made available through the use of computing systems; in environments of significant population density dependently operated by systems controlled by computers; forging social communities of humans, who are increasingly unable to communicate or seek assistance of others they know, without computers; the sustainability of our natural world is being defined through the use of ICT. </span><span style=\"font-weight: 400;\">Therein, whilst there is an emergent discussion about ‘augmented reality’ in the context of new ‘head mounted displays’, in reality, our lives, our societies, our realities – have already *augmented* by ICT.</span>\n\n#### The (un)intentional Augmentation of Truth: ICT intepretation of natural justice.\n\n> In [English law](https://en.wikipedia.org/wiki/English_law \"English law\"), [Natural Justice](https://en.wikipedia.org/wiki/Natural_justice), **natural justice** is technical terminology for the rule against bias and the right to a fair hearing. This must be coupled with [Evidence](https://en.wikipedia.org/wiki/Evidence), broadly construed, is anything presented in support of an assertion, as it is the case In law, that rules of evidence govern the types of evidence that are admissible in a legal proceeding. Where natural persons do not have the beneficial use of the storage medium in which evidence is stored, natural justice is made to be incapacitated; and is now-therefore, poorer for it.\n\n##### Data Quality\n\n<span style=\"font-weight: 400;\">If an employee of a company looks you up in their database and it says something about you that is inconsistent with your statements; who do they believe? does it need to be correct, complete or shared with you? </span>\n\nno. it does not. Law as is applied to the development and use of informatics systems have not been designed to support those natural justice related requirements. The systems in-place are for use by employees ([agents](https://en.wikipedia.org/wiki/Law_of_agency)) who are sought to make decisions about you through the use of the systems designed for them to use, for the benefit of the [legal personality](https://en.wikipedia.org/wiki/Legal_person) who provides the employee income. This in-turn can lead to data-errors that range from false-statements, mistakes, misrepresentations and worse. As the data-systems are not designed to immediately and by default provide both participants an electronic copy of the materials relied upon by the fiduciary, distortions become an effect of informatics systems design.\n\n###### Considerations, of an agent\n\n<span style=\"font-weight: 400;\">As we have successfully forged a pervasive augmented concepts of corpus over a very short period of time, whilst failing to understand how to serve the underlying purpose and meaning of the word that was defined to be meaningful </span>*<span style=\"font-weight: 400;\">way back</span>*<span style=\"font-weight: 400;\"> in roman times. Yet smart-phones are only a little over a decade old; technology has moved so rapidly, when society needs more time.</span>\n\n<span style=\"font-weight: 400;\">We have now, consequentially, put in-control derivatives of these *ancient concepts* ([corpus](https://en.wikipedia.org/wiki/Corporation#History)) as the community through the *[lens](https://en.wikipedia.org/wiki/Point_of_view_(philosophy))* of ‘</span>[*<span style=\"font-weight: 400;\">persona ficta</span>*](https://en.wikipedia.org/wiki/Legal_person)<span style=\"font-weight: 400;\">’ preserve on our behalf, our needs in our natural world.</span>\n\n<span style=\"font-weight: 400;\">We have built an incredibly capable [infosphere](https://en.wikipedia.org/wiki/Infosphere), yet it has been made to [usurp](https://en.wikipedia.org/wiki/Usurper), ‘wisdom of man’, or ‘[wise man](https://en.wikipedia.org/wiki/Homo_sapiens)‘, as to engender collectively subservient in exchange for ensuring they are </span>*<span style=\"font-weight: 400;\">not reasonable subject to penalties for failures,</span>*<span style=\"font-weight: 400;\"> in any way that may provide adequate remedy to the disaffected. </span>\n\n> Most of the most successful Web 2.0 businesses were built by first breaking the law, building an enormous international business; and then working with law-makers to make, their business models, legal.\n\n<span style=\"font-weight: 400;\">Whilst it is certainly the case that the former industrial era controls are a poor-fit to our modern society; the result brought about by our engineered solutions for information systems as has been implemented by design, as are now constituents of our socioeconomic, architectural features; exhaustively illustrates contextually; as a ‘common-service method’ for societal operations infrastructure (world-wide), whilst threaten all life with inexorable risks. </span>\n\n<span style=\"font-weight: 400;\">These are choices that have been made; they are not innate flaws of ICT as a form of technology, that can be used to resolved the architectural challenges, should the choice to do so be made.</span>\n\n<span style=\"font-weight: 400;\">The problem that any solution depends upon today; is in its means to illustrate the reason why solving that ‘particular’ problem, is so very important. </span><span style=\"font-weight: 400;\">Certainly building entirely new systems needs to be justified in serious terms. </span>\n\n<span style=\"font-weight: 400;\">The means in which to form an information management structure that is centred upon humans, rather than companies, making the human actor personally responsible for their choices is a fairly radical idea.</span>\n\n#### Engineering Informatics support for Personhood\n\n<span style=\"font-weight: 400;\"> To form a means of comprehending the nature of how perceived reality is formed; is considered often, to be contrary to the interests of existing stakeholders, to make information available to ‘them’ (most often meaning, consumers). </span>\n\n> <span style=\"font-weight: 400;\">Yet upon what basis do they form an understanding of why decisions were made; and what it is that can, or should, be done to make the necessary adjustments.</span>\n\n<span style=\"font-weight: 400;\">The changes that are seemingly required, i conceptualised as a new form of information and knowledge management system, that makes use of a ‘human centric’ approach, as is defined by present technologies. The purpose being, to build an informatics system that is guided by an interference pattern caused by individual human actors purposeful decision making practices; as the primitive constituent, of any other formative derivatives that may therefore ‘be evaluated’ (re: [inferencing](https://www.webizen.net.au/inferencing-introduction/)) as to form a record of comprehension; and its relations to patterns of diffusion and cognitive dissidence, as is made presentably measurable overtime. </span>\n\n<span style=\"font-weight: 400;\">In this way the information system develops a means in which to forge a comprehension of the intent, as is stored as part of a graph. </span><span style=\"font-size: 1rem;\">Overtime, this is consumed by agents to process and </span>evaluate<span style=\"font-size: 1rem;\"> characteristics.</span>\n\n<span style=\"font-weight: 400;\">The resolution of available data-points for operational data-metrics is continuously developing in resolution; as it does, the means to evaluate vectors of ‘communicable error rates’, are increasingly made able to be mapped (and/or manipulated) overtime. Machine evaluation of ‘lineage of acts’, carried out </span><span style=\"font-weight: 400;\">between the ‘junctures of intended purpose’ and ‘applied evidentiary outcomes’, improves the means to act in anyway. By bringing to market a knowledge banking industry based on supporting individuals, the means to ensure these business processes are efficient, is bought about.</span>\n\nPut simply, it is currently the case that one group of actors (legal personalities) have a bunch of data, another group (natural persons) don’t.\n\n> Decisions that need to be made by responsible persons, needs reliable data.\n\nDecisions to address issues that affect humans, are far more difficult than resolving issues that affect legal personalities; due simply, to the information management systems design framework used, and maintained. There is a substantive [competitive advantage](https://en.wikipedia.org/wiki/Competitive_advantage) brought about by solving this issue.\n\n<span style=\"font-weight: 400;\">Therein, considerations of how these [modals](https://en.wikipedia.org/wiki/Modal_logic) may be consistent with the means of artificium notation; through the use of creative works such as [Sophomore’s dream](https://en.wikipedia.org/wiki/Sophomore%27s_dream)</span><span style=\"font-weight: 400;\"> may in-turn provide a means to form the interpretative values for artificial actors with improved resolution.</span>\n\n<figure class=\"wp-caption alignnone\" style=\"width: 665px\">![](https://upload.wikimedia.org/wikipedia/commons/4/4e/Sophdream.png)<figcaption class=\"wp-caption-text\">Sophomore’s dream. Algorithms powers Artificial intelligence. How can you define how algorithms, define you.</figcaption></figure><span style=\"font-weight: 400;\">Another important consideration is the nature in which articles, actors and/or agents exist (classically) in a </span>*<span style=\"font-weight: 400;\">temporally linear field array,</span>*<span style=\"font-weight: 400;\"> to the preclusively of all others, as to manifest a form of a ledger; or series of (inter related) lists. </span>\n\n#### The algorithmic society\n\nI firmly believe the use of algorithms impacts our natural world in calculable ways. T<span style=\"font-size: 1rem;\">he application physics, quantum physics, information theory, informatics and its role in social and human consciousness; through the means provided by fields such as those, to communicate; that in-turn forms an interference pattern between choices that are made, and outcomes, is now defined by algorithms. We are saturated in data, whilst knowledge equity, is now considered rarely, to be a valuable asset of a legal person. </span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/5b5BDoddOLA?rel=0\" width=\"560\"></iframe>\n\n#### Is our Consciousness due to a bio electro-chemical *Quantum* Processes\n\nThe field of quantum physics in particular, is an area considered to be beyond the realm of comprehension. There are many contributors, from authoritative sources whose studies and considerations have been used to build machines. The application of these works, as algorithms, to manipulate social-graph informatics infrastructure is well within the field of available science today.\n\nYet the field of consideration has moreover been made in relation to the manufacture of things, rather than the consideration of how it may play a role in consciousness, or moreover; what it means to use science, to make our worlds, a better, more sustainable place. For ourselves, and others.\n\n<span style=\"font-weight: 400;\">It is expected that the advent of quantum computing will disruptively influence the otherwise binary serviced means of analytics and analysis. The means through which some qualities of a ‘</span>*<span style=\"font-weight: 400;\">quantum infosphere</span>*<span style=\"font-weight: 400;\">’ can seemingly be achieved, is through the development of tools that makes use of *personal ontologies* via *temporally aligned notation*, within an *socially interwoven informatics,* computational environment.</span>\n\n###### segmentation of informatics structures\n\n<span style=\"font-weight: 400;\">The use of a computational environment built upon a means through which a ‘knowledge banking industry’ provisions personal information spheres, seeks to instrumentally influence the informatics topologies (graph architecture).</span>\n\n> <span style=\"font-weight: 400;\">Inforgs are able to be employed by agents for the purpose of evaluating complex interference patterns brought about by the acts, arts and interactions of humanity with the natural world and the artificium universitas in which humanity preserves custodianship. </span>\n\n<span style=\"font-weight: 400;\">This theorem, should be technically possible today through the application of implicitly defined, information theory, in relation to the use of APIs provided by existing stakeholders (ie: leading social-network-silos). </span>\n\n> <span style=\"font-weight: 400;\">Whether it be for targeted advertising or otherwise. How these systems actually work, is now considered to be proprietary ‘knowledge equity’ of the relevent providers.</span>\n\nGiven it is the case that these international organisations cannot manually process everything; the role of artificial intelligence is increasing exponentially.\n\n<span style=\"font-weight: 400;\">Machine Learning and other forms of Artificial Intelligence Agents form an artificial ‘sense’ of ‘cognitive’ function through the practice of consuming very large amounts of information made available to it, as to form an evaluation framework that, whilst often related to probability, forms the means to engender a yes/no answer as to present a result. </span><span style=\"font-weight: 400;\">It is important for the *useful purpose* of these agents; to make distinct that which is consistent with an informatics embodiment of our “natural world” and/or ‘truth’. </span><span style=\"font-weight: 400;\">The intangible constructs of informatics system forms; cause impacts in the real world. </span>\n\n> <span style=\"font-weight: 400;\">In effect, the information systems design methodology becomes instrumental in the means and methods used to shape our world.</span>\n\n<span style=\"font-weight: 400;\">The means in which rules are applied, renders an inexorable influence upon the governance of the health of our environment. </span>\n\n<span style=\"font-weight: 400;\">In nature, the manner in which risks are managed is by ensuring autonomy of individual organisms, in a manner that does not preserve significant factors of risk across the species should a problem exist within an isolated organism or group; as is currently applied artificially, by the highly centralised nature of business systems logic, as is applied globally through current applications. </span>\n\n<span style=\"font-weight: 400;\">Through the practice of converting the means in which information is managed to become ‘human centric’; the acts of humans as individuals are brought into the equations used by others for other purposes. </span>\n\n> <span style=\"font-weight: 400;\">We are furnished the means to reconsider virtue and relevance for areas of science used to define a structure for humanity as the seminal actors for all environments, as may otherwise evolve as to serve the best interests of </span>*<span style=\"font-weight: 400;\">persona artificum &amp; ficta</span>*<span style=\"font-weight: 400;\">.</span>\n> \n> A knowledge banking industry puts humanity in control of artificial intelligence related systems.\n\n##### Bringing Human Experience into the Dynamics\n\n<span style=\"font-weight: 400;\">In my work, i have considered the use of cryptography, linked-data related technologies in a manner that is paired with considerations of </span>*<span style=\"font-weight: 400;\">the</span>* *<span style=\"font-weight: 400;\">dynamics,</span>*<span style=\"font-weight: 400;\"> as explained by way of quantum physics theory; which i don’t really claim to understand, other than noting the algorithms do exist and can be applied online to social-systems. </span>\n\n<span style=\"font-weight: 400;\">The inspiration for having started these works back in 2000, was brought about in relation to considerations of how neural synapses worked; which was a field of accolade studied by my grandfathers cousin, [john carew eccles](https://www.nobelprize.org/prizes/medicine/1963/eccles/biographical/). Interestingly, it has later been found this work has been [cited by works](https://plato.stanford.edu/entries/qt-consciousness/#3.2) consistent with the form of views produced overtime. </span>\n\nThe hypothesis now-therefore becomes;\n\n> is it the case that the design of our infosphere, or our information management systems, can be shown to cause harm to the natural world as a consequence of its design and the unmet challenges that can be altered; through the application of a knowledge banking industry. Is it the case, that the giant global graph, can do that today?\n\nIs it the case, that by ‘bringing the observer into the equastion’ (the human) by enabling their means to store and manage the use of AI on their behalf, as is used by a dynamic agent; that the way societal choices manifest can, in-turn, radically change the way we consider the meaningful use of concepts such as “moral grammer”.\n\n> Does human consciousness, have something to do with quantum physics; and if it does, should the algorithms that apply the use of those natural process related faculties; be identified and made ‘commons’.\n> \n> #### Moral Grammar\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/Kr2K8mo-A5g?rel=0&start=4898\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">A ‘human centric web’ seeks to provide adherence to the principle that natural person be engendering support for the inscribed mechanics, of their temporal conscious existence in the natural world.</span>\n\n> The continual development of ICT and the records produced by it will deliver exponentially growth of information stored about our lives to seemingly limitless levels of dynamic resolution.\n> \n> Truely, there are increasingly fewer secrets.\n\n##### Rationalising the storage of pervasive surveillance\n\n<span style=\"font-weight: 400;\">The information management systems design methodology, influences both realms of a data-subjects role in the natural and artificial realms of human socioeconomic existence; and the formative environment in which consciousness becomes dynamically aligned. </span>\n\n<span style=\"font-weight: 400;\">When considering the socio-economic influences in geopolitical (law) terms, whilst treating separately; work embodied as proprietary rights (corpus), commons (natural world) and personal works; as is required for governance of the individual personhood and of all persons within a dynamic environment; we are now recorded and employed by ICT that supports non-linear enquiry.</span>\n\nThe illustrated concept of time-travel isn’t about jumping into a machine; rather, the concept that extends from our ability to now use old media, old records, to spend some time in the present, engaging in the past.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/2rDoS7XErcw?rel=0\" width=\"560\"></iframe>\n\n##### Your temporal &amp; machine readable record\n\nThe notes on an AI based [re:animation of persons](https://www.webizen.net.au/about/applied-theory-applications-for-a-human-centric-web/reanimation/), to some-degree discusses this concept in a more accessible manner; noting, the resolution of records stored today pales in comparison to what is going to be created as [Web of Things](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/) ( or Internet of Things) will do to society, in exponential ways.\n\n> <span style=\"font-weight: 400;\">What is the ‘role of the observer’, and who is defined as the observer; who is defined as the legal actor, and who is defined as a stakeholder.</span>\n\n<span style=\"font-weight: 400;\">Is it reasonable to expected that our information systems be defined as to yield new means in which the consequential narrative of defining an interpretative value of ‘truth’; be better equipped through the media format made use of to do so? </span>\n\n<span style=\"font-weight: 400;\">Can these known problems be expressed by way of physics and other fields of science? </span>\n\n<span style=\"font-weight: 400;\">Do false statement, and their </span>*<span style=\"font-weight: 400;\">interference patterns</span>*<span style=\"font-weight: 400;\">, algorithmically deliver programmable derivatives that can be relied upon; to be safe, as intended, as if; proper, or reliable; if it were the case, that it was the intended purpose.</span>\n\n##### **The importance of relations &amp; the role of quantum physics in Consciousness.**\n\n*<span style=\"font-weight: 400;\">John Bell showed that the quantum predictions for entanglement are in conflict with local realism. From that ‘natural’ point of view any property we observe is</span>*\n\n*<span style=\"font-weight: 400;\"> (a) evidence of elements of reality out there; and </span>*\n\n*<span style=\"font-weight: 400;\">(b) independent of any actions taken at distant locations simultaneously with the measurement.</span>*<span style=\"font-weight: 400;\"> </span>\n\n*<span style=\"font-weight: 400;\">“We have learned in the history of physics that it is important not to make distinctions that have no basis — such as the pre-newtonian distinction between the laws on Earth and those that govern the motion of heavenly bodies. I suggest that in a similar way, the distinction between reality and our knowledge of reality, between reality and information, cannot be made. There is no way to refer to reality without using the information we have about it. Maybe this suggests that reality and information are two sides of the same coin, that they are in a deep sense indistinguishable. If that is true, then what can be said in a given situation must, in some way, define, or at least put serious limitations on what can exist.”</span>*\n\nSource: [Message from the Quantum](https://www.nature.com/articles/438743a)\n\n<span style=\"font-weight: 400;\">By considering the notations of </span>*<span style=\"font-weight: 400;\"> Stapp who </span><span style=\"font-weight: 400;\">favors the idea that quantum wave functions collapse only when they interact with consciousness as a consequence of “orthodox” quantum mechanics. “In order to make quantum mechanics work, you’ve got to bring the human agent into the equations of quantum mechanics which are designed to explain human experience.”</span>*\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/ZYPjXz1MVv0?rel=0\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">it is problematic that the means through which these works have been created has absolutely nothing to do with linked-data or the means through which these works may be applied to artificial intelligence, semantic web powered, decentralised systems; so, the context needs to be adapted.</span>\n\n<span style=\"font-weight: 400;\">Yet; The illustrated problem when applied to that specified purpose; may provide some reason as to why the </span>*<span style=\"font-weight: 400;\">corpa</span>*<span style=\"font-weight: 400;\"> &amp; </span>*<span style=\"font-weight: 400;\">persona ficta </span>*<span style=\"font-weight: 400;\">we have defined </span>*<span style=\"font-weight: 400;\">artificium</span>*<span style=\"font-weight: 400;\"> (artificially); as to still now dislocate the human to the infosphere now maintained by way of </span>*<span style=\"font-weight: 400;\">artificium agency; and, </span>*<span style=\"font-weight: 400;\">the means in which the records pertaining to us are continuing to be maintained; is now therefore able to be shown to be causing </span>*<span style=\"font-weight: 400;\">distortions </span>*<span style=\"font-weight: 400;\">to our shared reality,</span><span style=\"font-weight: 400;\"> due to the design.</span>\n\nthese distortions have seemingly been purposefully applied to make limits.\n\n#### The Making of Quantum Physics, to be Confusing\n\nThe statements made by ‘experts’ and available press, are at best confusing.\n\nThe publication by MIT technology review “[Chinese satellite uses quantum cryptography for secure video conference between continents](https://www.technologyreview.com/s/610106/chinese-satellite-uses-quantum-cryptography-for-secure-video-conference-between-continents/)” highlights the use of quantum physics related concepts, for use on communications technology. A linked and related article; [First Object Teleported from Earth to Orbit](https://www.technologyreview.com/s/608252/first-object-teleported-from-earth-to-orbit/) states talks about the use of entanglement. “This occurs when two quantum objects, such as photons, form at the same instant and point in space and so share the same existence. In technical terms, they are described by the same wave function” meanwhile talks made available by Google “[The Quantum Conspiracy: What Popularizers of QM Don’t Want You to Know](https://www.youtube.com/watch?v=dEaecUuEqfc)” (long and convoluted video, noting i disagree in some areas, personally) seems to start-out by suggesting what has been discussed by the MIT technology review articles, couldn’t possibly occur, yet over the course of the hour long video continues to present an array of mathematical expressions, that seemingly shifts the linguistic methods in which a concept is discussed, to a format that is painful to go through and watch; whilst most useful, for software development purposes.\n\n*I find the existing materials confusing and seemingly conflated by marketing related linguistic methods, to be perceived to be a more specialised field than is reasonably true otherwise.*\n\nPeople independently make choices every moment in the real-world, why is it that the ability to make choices, be subjugated due to the creation of a new communications medium where ‘experts’ define how it works.\n\nIn a broader environment where many presentations relate to a person seeking new forms of income; and that all specialised industries, have got a bunch of people in it with their own problems, as we all do; and so therein, given the concept of ‘observing something’ is debated even by the experts;\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/ZacggH9wB7Y?rel=0\" width=\"560\"></iframe>\n\n###### can quantum physics equastion be employed to affect the human condition?\n\nthe question becomes reinforced once again; about whether information science can be used to produce improved measurement apparatus (and means to make use of), somehow via ICT, to influence consciousness…\n\nAn article published by BBC titled ‘[organisms might be quantum machines](http://www.bbc.com/earth/story/20160715-organisms-might-be-quantum-machines)‘, alongside another more recently title [the strange link between the human mind and quantum physics](http://www.bbc.com/earth/story/20170215-the-strange-link-between-the-human-mind-and-quantum-physics) continues to consider the concept in various ways (notably including considerations about [lithium](https://en.wikipedia.org/wiki/Lithium_(medication)) – perhaps that’s how those Chinese scientists got the jobs to be involved in making the technology discussed in the MIT articles above, who knows, perhaps it’s recommended for those interested in finding out, to go about trying it); but no so far as the applied considerations relate to the function of google, facebook, linkedin and similar (other than showing the presentation of these works to their software developers); or the means (or implications) of making something different, when considering the role of physics as does intersect between informatics / information systems, perception and the development of what it is we call ‘consciousness’, thought, and our role through personhood / agency; of being made able to make good decisions.\n\n###### The debate about the Application of[ Quantum Algorithms](http://math.nist.gov/quantum/zoo/) on the Social Graph\n\n<span style=\"font-weight: 400;\">So, it seems there’s enough debate about whether and how quantum physics plays a role in consciousness, which in-turn supports the information theory premise through which, consideration is made about the role of entities and definitions of capacity; as to form interference pattern, in relation to that entity. That </span><span style=\"font-weight: 400;\">these interference patterns may well algorithmically, in-turn, form the material basis through which change and influence is applied to that entity. </span>\n\nOne of the many references that exist, include the page hosted by stanford university on [Quantum Approaches to Consciousness](https://plato.stanford.edu/entries/qt-consciousness/#3.2) (in-part involving my grandfathers cousin, [eccles](https://www.nobelprize.org/nobel_prizes/medicine/laureates/1963/eccles-bio.html), whose work, first inspired my own back in 2000) whereby the relationships brought about by the work on how consciousness neurologically works and the relationships informatics are influenced by that; to formatively distinct, work on quantum physics; does indeed, in some way or another, relate to each other – whilst the scope of impacts, are unknown.\n\n*<span style=\"font-weight: 400;\">therein it is noted, as it is otherwise noted above; In an interview of 2006, Stapp (2006) specifies some ontological features of his approach with respect to Whitehead’s process thinking, where actual occasions rather than matter or mind are fundamental elements of reality. They are conceived as based on a processual rather than a substantial ontology. Stapp relates the fundamentally processual nature of actual occasions to both the physical act of state reduction and the correlated psychological intentional act.</span>*\n\nBut the implication is, that if this does stimulate changes on a biological basis, it is not dissimilar in effect, to any other form of implemented constraint.\n\n##### The battle to be a beneficial owners of knowledge equity\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/aigR2UU4R20?rel=0\" width=\"560\"></iframe>\n\n*<span style=\"font-weight: 400;\">At a level at which conscious mental states and material brain states are distinguished, each conscious experience, according to Stapp (1999, p. 153), has as its physical counterpart a quantum state reduction actualizing “the pattern of activity that is sometimes called the neural correlate of that conscious experience”.”</span>*\n\n<span style=\"font-weight: 400;\">Therein, the nature of existence and the characteristics of ‘interference patterns’ were first studied by way of Youngs Interference Experiment,</span><span style=\"font-weight: 400;\"> in which a source of light (wave) or movement of a wave (particles), when passing through a barrier that incorporates two slits; leads to a phenomena of the formation of multiple waves on the opposing side of the barrier. </span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/Jqm4f55soJQ?rel=0\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">Through the use of this illustration, the means in which to better understand the nature in which all interactions cause an effect which is not simply able in anyway to be isolated but rather maintains a relationship to quantum entanglement and [wave-function collapse](https://en.wikipedia.org/wiki/Wave_function_collapse)</span><span style=\"font-weight: 400;\"> through which the tooling to form a measurement apparatus that in-turn provides answers to the question first raised in 1935 “[Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?](http://cds.cern.ch/record/1060284/files/PhysRev.48.696.pdf)”</span><span style=\"font-weight: 400;\"> in which the constituency of causality by association to temporal ontological notation, can to form means in which to analyse a complex, static, and isolated circumstance, pertaining to a subject; as simply as to analyse the interaction of two actors in isolation to all others. </span>\n\n<span style=\"font-weight: 400;\">This modal environment is thought to provide assistance in forming a model for understanding the infinitely more complex version of reality; that can be made known, through the study of the effects interactions with others have upon everything. </span>\n\n##### Extension of custodianship of what becomes our history\n\nThe memory of computers when adapted to store in high resolution large volumes of information about the lifecycle of any actor, far outstrips human capacities. yet these mediums are dynamic in nature, the data can be easily changed; and there are currently few custodians. The means to make use of that medium, brings with it, a new range of considerations relating quantum theory to information theory, and the application of those works with respect to artificial intelligence systems.\n\n<span style=\"font-weight: 400;\">To preserve a human centric means for informatics architecture, storage and permissive capacities; through which agents, are defined to query and interact. The quandary, ‘ripple effect’, is in-turn caused by the design of the informatics system; and the current design, does not well support humanity.</span>\n\nSo the reason why its really very important to build a ‘human centric web’ and moreover, a ‘knowledge banking industry’; may be a problem that can be shown in relation to the use of physics, and stuff we don’t understand; that says in-turn, its amongst the most important things we need to do if we want to progress as a species and improve the way we define our ‘interference patterns’ with the world around us and all natural life, in our natural world.\n\n![](https://i.pinimg.com/originals/40/b6/14/40b614606c955a16220a180057f56579.jpg)\n\n<span style=\"font-weight: 400;\">By defining a new, modern modal medium; records made use of in the infosphere, are made up of the informatics constructs defined in personal inforgs; which render </span>*<span style=\"font-weight: 400;\">personable definitions</span>*<span style=\"font-weight: 400;\"> of </span>*<span style=\"font-weight: 400;\">ontological design</span>*<span style=\"font-weight: 400;\"> – alongside the means in which the broader realm of the infosphere is supported as to become better equipped. </span><span style=\"font-weight: 400;\">This in-turn, empowers us as humans to make use of the complex, repercussions effect, through which, in a temporal frame of influence and context; The manifestations of what it is to be made able, to socially consider identity, causality, and in relation to conscious human actors as is temporally managed by way of electronic records; the forms of information management systems structures, better suited to the needs of AI and the natural world it depends upon and is designed otherwise, to serve.</span>\n\nYet moreover; Its far more useful to get stuck into the practical elements of how to build a profitable knowledge banking industry.\n\nAt the end of the day, economics, is presently made to be the main priority.\n\nYet irrespective of how well our internet elders, run some of the largest and most pervasive companies in the world; they simply don’t solve the problems.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/GV0A82TCrf0?rel=0\" width=\"560\"></iframe>  \nThe way, our history, will be manifestly put back in the hands of the people, is through the development of a human centric web, a knowledge banking industry; an international, socio-economic system, like the formation of a banking sector of relevance, in making of an age for *our algorithmic society*."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","title":"Socio-economic relations | A conceptual model"},"frontmatter":{"draft":false},"rawBody":"---\nid: 535\ntitle: 'Socio-economic relations | A conceptual model'\ndate: '2018-10-02T13:14:05+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=535'\ninline_featured_image:\n    - '0'\n---\n\n[![](https://www.webizen.net.au/wp-content/uploads/2018/10/infosphere_actor_objects.svg)](https://www.webizen.net.au/about/references/socioeconomic-relations-p1/infosphere_actor_objects/)\n\n(DRAFT)\n\n<span style=\"font-weight: 400;\">The above image attempts to stimulate the mind of readers, means to reconsider information management environments as info spheres, where designs can be made; to make humans the primary knowledge fiduciary, as can in-turn provided infrastructure support for diversity and shared benefit. </span>\n\n<span style=\"font-weight: 400;\">In so doing; relations form the informatics heuristics, relating decisions made by groups who are in-turn legally obligated to operate some form of ledger to provide support for legal personalities (legally recognised groups) and derivatives, as orchestrated by humans, in so doing. </span>\n\n<span style=\"font-weight: 400;\">In this image; The relations between natural persons (“natural agents”), companies (Legally recognised groups), artificial actors (software agents), commons and things (property such as devices) are provided the means to make use of ‘commons informatics’ which provides the imperative function of supporting the means for ontological declarations to be made and made use of; in relation to informatics artefacts communicated with any further sense of proprietary custodianship. </span>\n\n<span style=\"font-weight: 400;\">When conjoint; the intent is to make a [modal](https://en.wikipedia.org/wiki/Modal_logic) environment; designed to be used as a basis for the growth of a knowledge economy. </span><span style=\"font-weight: 400;\">In this way, it is considered to be an engineering feat that is ‘fit for purpose’; for </span>\n\n- <span style=\"font-weight: 400;\">the continuance of geopolitical &amp; socioeconomic operations of commerce, by way of the multi-modal form of fiduciary instruments; and, </span>\n- <span style=\"font-weight: 400;\">The management of artificial intelligence by way of forming the frameworks for which it functions as an extension to natural personhood; and, </span>\n- <span style=\"font-weight: 400;\">The means in which to establish non-linear information management system for the operation of personal inforgs</span><span style=\"font-weight: 400;\"> as to make possible a broader interoperable environment of information spheres.</span>\n\n<span style=\"font-weight: 400;\">The intention of this design is to forge apparatus required to restructure the constituent, electronic embodiments and influencing ‘information systems’ relating to human ‘consciousness’. The purpose being, to define the manner through which meaningful custodianship and electronic use be therefore reunited with the human person (aka “data subject”); otherwise employed </span><span style=\"font-weight: 400;\">through interactions with other agents; the structural redesign practice method, is purposefully applied to provide meaningful capacity of maintaining personal custodianship of the inforg and its use in provisioning a dynamic electronic information storage, knowledge engine; as is otherwise relied upon by the ‘data subject’.</span>\n\n<span style=\"font-weight: 400;\">Considered important also; is the means through which this can be achieved through the use of royalty free technologies. Without the meaningful utility of ‘royalty free technologies’, which are in-turn brought about through patent-pools and internet governance related infrastructure; the ability to support human kind, to be the beneficial owners of information about their being, in a portable format, would have been made impossible. Yet as this work has been done, the means to</span><span style=\"font-weight: 400;\"> produce and consume, in relation to the use of internet infrastructure, defined schema; as to define new schema, for the ontological maintenance of the “cyber self”, is in-turn brought about. </span>\n\n<span style=\"font-weight: 400;\">The present embodiment has been architecturally illustrated in various forms on this site; in a lightly coupled alliance with others worldwide such as to have included the practical and material support of those; such as to include many groups of actors working on human custodianship; involving most of the worlds most prominent incorporated groups of contributors alongside individual contributions made on a voluntary basis. These works, now make it possible to advent, a new forum of discovery and enlightenment; in a manner that has no equal. These works are founded upon the meaningful use of these collective works; attending to the principles of the World Wide Web in so doing, whilst not dependent upon HTTP Specifically. </span><span style=\"font-weight: 400;\">The manner in which these systems bring to effect the means in which to electronically render a topological environment in which the communications may be provided the support of inference, is by way of machine readable ontologies as made possible by Resource Description Framework (“RDF”) or as is otherwise known, linked-data; which is URI dependent overall.</span>\n\n<span style=\"font-weight: 400;\">Linked-data provides the means in which a vocabulary may be produced as to form in natural language, machine-readable ontologies; which are made-use of to structure the information environment stored in an inforg. This provides the means to furnish support for machine-learning and the assertion of rules to make use of otherwise unstructured semantics.</span>\n\n<span style=\"font-weight: 400;\">Each actor forms and manifest their own formative ontological structures, in which to make useful to others, the means to refer and denote </span>*<span style=\"font-weight: 400;\">exchange values</span>*<span style=\"font-weight: 400;\"> between two forms, derivatives and the means to assert permissions as is used to formulate defined statements and principles in a machine-readable format; that is in-turn used by other actor / agent; in a dynamic temporally notated format; that makes use of the information that has been made available in the past, alongside the means in which to traverse into the present and by linguistics or other language; define the sphere of informatics in a conjoint manner with others; in consideration of, ‘moral grammar’, social contract, citizenship and socioeconomic rights and responsibilities of participation. </span>\n\n<span style=\"font-weight: 400;\">Through these ontologically defined inforgs, broader inferences are able to be considered as Humanity becomes by derivative of graph architectures, the custodians for the information environment in which </span>*<span style=\"font-weight: 400;\">truth</span>*<span style=\"font-weight: 400;\"> is without equivalence. </span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","title":"The need for decentralised Open (Linked) Data"},"frontmatter":{"draft":false},"rawBody":"---\nid: 564\ntitle: 'The need for decentralised Open (Linked) Data'\ndate: '2018-10-02T16:37:27+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=564'\ninline_featured_image:\n    - '0'\n---\n\n### (DRAFT)\n\nthere are an array of considerations relating to how it is that resources such as those produced by wikimedia, be decentralised, in a manner that supports version-control (cooluris) alongside other characteristics.\n\nSome of these requirements include;\n\n#### **Geography (“Space”)**\n\n<span style=\"font-weight: 400;\">Geographic spaces in-turn pertain also to law; in which some predicates include the means in which to temporally define means for association to private space.</span>\n\n<span style=\"font-weight: 400;\">The means in which to make addressable rights as they pertain in a temporal format to places is a matter of importance with particular regard to the utility of modern interfaces such as ‘Augmented reality’ head mounted eyeglasses or smart phones, in addition to IoT applications; and the means in which to preserve privacy as is defined by rules that may be applied to geographic space. </span>\n\n<span style=\"font-weight: 400;\">Whilst the means in which any space may first be made addressable is a constituent of the civics info sphere; the broader notation of proprietary rights is intended to be performed by act between various forms of persona ficta and persons; in which space may be applied conditional rules as to preserve privacy, protections from electronic torts —&gt; and to ensure kids are not encouraged to play virtual games on freeways.</span>\n\n#### **Language**\n\n<span style=\"font-weight: 400;\">The basis to almost all systems of societal interaction is language. Language is not simply verbal (phonetic) or text (written words) but is also inclusive to gestures and other body language, paintings, song-lines, dress / garments, alongside other symbols, ‘social concept’, communicable mediums and related embodiments. These constituents of cultural epidemiology have traditionally been formed by way of geographic region; whilst more recently through the utility of the world wide web in particular; may pertain to groups whose origins are native to the infosphere.</span>\n\n#### **History**\n\n<span style=\"font-weight: 400;\">The history of environments may have complex multiplicity; whilst the analysis and interpretation of records provide some basis for inferred relations considered to be fact; whilst the conditional logic as is inferred by way of a reductionist format by way of the correlation of invoked new influences; may bring cause to form new insights otherwise made less capable of consideration. </span>\n\n#### **Law**\n\n<span style=\"font-weight: 400;\">Law is amongst the more important areas in which the employ of ontological design may meet challenges of how to afford the translation of legal definition as made distinct by region; may in-turn form means for dynamic international agreements in which a purposeful common-meaning may be obtained. </span>\n\n<span style=\"font-weight: 400;\">Law is subject to changes overtime and the means in which to refer the manifest nature of the laws that were relevant at any particular time; also, provides context that may otherwise be less considered in the semantic relations of records used without means of agents made capable to determine semantic inferences.</span>\n\n#### **Algorithmic Derivatives (“machine-learning” artifacts)**\n\n<span style=\"font-weight: 400;\">A deterministic mechanism is defined in which the natural world forms an interference pattern as it is transferred into an infosphere environment as to be made use of by agents; in a manner that may be regarded to be rendered governance, custodianship, operation and management by the express and sole responsibility of human actors in which all acts in relation to others form a constituent of a broader infosphere of accountable, temporally traversable inferencing frameworks empowered by machine readable informatics.</span>\n\n<span style=\"font-weight: 400;\">Irrespective of the form in which these machine-readable embodiments be made necessarily articulated; the intent of so resourcing a means in which the natural world is by considerations of ‘commons’ made accessible; is for the express purpose of a trusted network of privately operated ‘inferencing’ services to facilitate interpolar communications events between the dynamic inforg of human actors in utility of the manifest notations made by said actor; and the means in which dynamic representations be furnished in a temporally relevant environment overtime; that may in-turn be referenced to in a non-linear manner as to improve our broader capacities as humans to undertake provenance evaluation of any query and related inference related knowledge processing task/s. The means in which this can only be made possible; is if the means in which to obtain the relevant ‘machine learning’ derivative outcome be made available privately to the agent elected by the human subject with no such right or privilege as to circumvent the human rights of the individual; as to seek to usurp any human moral right as it pertains to dignity in any way that may be considered a form of ‘golden handcuff’, modern slavery, forced labour, or similar; as defined contextually by means of the infosphere; through the application of governance, with open-linked-data.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","title":"Domesticating Pervasive Surveillance"},"frontmatter":{"draft":false},"rawBody":"---\nid: 750\ntitle: 'Domesticating Pervasive Surveillance'\ndate: '2018-10-08T12:16:33+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=750'\ninline_featured_image:\n    - '0'\n---\n\n(DOCUMENT STATUS: DRAFT)\n\nPervasive Surveillance is here and its not going away. The technical vision is built upon the use of URIs and machine-readable documents, often referred to as [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework) or [Linked Data](https://en.wikipedia.org/wiki/Linked_data). These technologies are used for sophisticated purposes today by the worlds most powerful organisations (and governments)\n\nThe tooling is built upon international standards have have robust international patent-pool protections, in consideration of intellectual property frameworks internationally and the legal means bring integrity into the game.\n\nThese tools are able to be swapped out for other parts should one part fail.\n\nThese steps delivering architectural resilience, is delivered through means that leverages an international community who have been dedicated to a consistently similar vision for a number of decades. Their very smart people.\n\nThe means to communicate in the real-world is immutable. The means to furnish the right to communicate online should also be immutable; whilst it is certainly also the case, that law does relate always to acts that include communication. If a person says they’re going to harm another person, those around them would intervene; this is similarly reasonably the case online, yet, the qualities of how this is done to the betterment of humanity, is complicated.\n\n<span style=\"font-weight: 400;\">The technology ecosystem centres upon the needs of human actors. It enables the linked-storage and use of files across internet, to ensure people are made able to personally make use of ‘smart agents’ (AI), dynamically overtime, which supports the act of communicating on behalf of the person. </span>\n\n<span style=\"font-weight: 400;\">The consequence of the information systems design is to deliver a means to query data temporally (ie: accessibility to past queries, and the continual generation of new ‘present’ queries) as to provide a unique response based on a persons definitions of how the ‘AI Agent’ should respond to that specified query as a unique event. The intended purpose of this is to deliver the means for a persons to make full-use of ICT, taking into consideration the intimacy formed with a persons ‘inforg’, as is used in relation to them, by them and for them. </span>\n\n<span style=\"font-weight: 400;\">The ‘inferencing rules’ are defined by the person rather than being defined by alternative methods, such as advertising revenue focused enterprise owned and operated AI. The</span> distinction made here; can be considered through the lens of Autocorrect; an AI agent, is reading the documents to gain a sense of what is being written by a human person (a form of surveillance); and the AI agent is owned by the software company whose interest is in improving its commercial re:use potential. Where Autocorrect ‘mistakes’ are made; two things happen, firstly a correction is read by the AI agent to assist it in learning, as to improve its value to the owner / operator of it. Secondly, more communication is required to fix a problem that occurred as a result of that mistake. This in-turn generates more income for the service providers involved. yet the consequence to the end-user, the human, may be that they’re personhood is tarnished / damaged in some way due to the mistake made by AI that is deemed to reasonably and acceptably occur.\n\nSimilarly; agents on behalf of companies can write false statements in computer systems about a human ‘data subject’. where this is unknown and unable to be corrected; the re-use of that ‘data’ / ‘information’ can lead to harms to the ‘data subject’ without any repercussive effects on the document author. These behaviours do indeed cause harm which can only be prevented by changing the method and systems design of information management; as does relate otherwise, to the operation of the ‘pervasive surveillance’ environment that is otherwise unavoidably impacting the lives of humans .\n\n> <span style=\"font-weight: 400;\">By changing the way data is managed and information is processed, as to form a constituent embodiments of knowledge; everything else changes. It’s about putting *wisdom,* into the *minds* of humanity.</span>\n\n<span style=\"font-weight: 400;\">As data is collected, augmented, and made use of by others using online systems, this in-turn has an impact on the ‘data subject’. where the data quality is poor, the impacts on the data subject can be very serious. Yet the data that is most important to us, is stored by others and we often do not even know what it is those systems have been updated to say about us nor how to easily correct errors.</span>\n\nMany years ago, works commenced on building our systems for banking, unions, medicare and more recently the ISP industry. <span style=\"font-weight: 400;\">If we are to create a solution to this problem; consider the legislative environment required to ensure the data that is already produced and networked about you, (but stored by other companies), imagine if you could be provided services that enabled that information to be stored by you in your ‘knowledge banking accounts’. Imagine if those systems were built in a way, that meant anyone could use them, irrespective of any disabilities or impairments. </span>\n\n> **What would Stephen Hawking want?**  Imagine if the high-resolution neural interface supporting the means for a person with severe disability to interact with the world, was able to be owned by them, and not anyone else. That they were able to maintain some sense of privacy for their thoughts, as part of how we think about human rights, as we forge designs.\n\n<span style=\"font-weight: 400;\">Think about the fact that all of the data produced *constantly* about you and your life is currently stored somewhere. What if all of that could change and that an expectation be set that if it’s got something to do with your activities in the world, you need a copy of it. you don’t need to know the details of what every person might say about you to other people in private; but, if data is going to affect your life – you need to be able to make use of it for self defence.</span>\n\n<span style=\"font-weight: 400;\">Imagine how you would benefit in your life, if the data about you was yours. </span>\n\n<span style=\"font-weight: 400;\">How could it affect your means, to socioeconomically participate. </span><span style=\"font-weight: 400;\">Imagine how much you would need any such ICT system, to be trustworthy; designed to support your health and wellbeing. To operate an advanced security team.</span>\n\n<span style=\"font-weight: 400;\">Imagine how the sophistication of modern information technology could be used by you; if all the information produced about you, about what you do, who you interact with – all that data already being stored somewhere; was made available to you in a way, that allowed you to define how artificial intelligence works for you. That you could ensure your doctor was able to make use of all of that information as part of your healthcare, looking for preventative solutions rather than reactive treatments. Imagine if the work done by you was able to be connected to work done by others, and if you helped in any way, you could economically participate in the benefits of the outcomes brought about. </span>\n\n<span style=\"font-weight: 400;\"> Imagine how all that data, when stored securely, by (a) provider(s) who are legally better able to prove the information produced is not false; and that through the means to network information in documents a chorus of persons are able to construct shared statements of fact. I</span><span style=\"font-weight: 400;\">magine how such a system could be used to protect your needs as required by you.</span>\n\n#### Making Pervasive Surveillance work for humanity\n\n<span style=\"font-weight: 400;\">We live in a world today that is pervasively surveilled, and that’s not going away. With all this technology, how has your ‘intelligence’ become advanced by artificial intelligence. how does it help you make contextually smarter decisions and how does bad (false / wrong poor quality) information affect you? How does all the things summarised as ‘fake news’, </span><span style=\"font-weight: 400;\">provide a more intelligent means for you to make more good decisions overtime, than was otherwise needed before any of the technology existed at all. How is technology used to consume you, and how is it used to make the natural world a better place. </span>\n\n> <span style=\"font-weight: 400;\">How is technology made useful to you, to protect you, if you become a victim of crime, to stop that from ever happening, or to prove that it did…</span>\n\n<span style=\"font-weight: 400;\"> To ensure the statements made about you are at least known to you, and that any errors are corrected, through the use of the information stored by you in your knowledge bank. </span><span style=\"font-weight: 400;\">Then, if disputes arise, the completeness of information able to be processed by systems run by a court; can more accurately, fairly and cheaply communicate the issues in a court of law as to seek lawful remedy.</span>\n\n<span style=\"font-weight: 400;\">Imagine what might change if the systems that serve to deliver pervasive surveillance throughout society, that we rely upon in our societies, changed the way it made use of permissions to make you the most important person, for the information that is otherwise stored about you. that your decisions about how that information is used, is considered by law and is built around your ability to make the vast majority of decisions for you.</span>\n\n<span style=\"font-weight: 400;\">To ensure the views built and depended upon about you; is produced on the basis of your direct involvement, to help ensure the most accurate information available to the world at any time about any matter concerning you, or that if the information is inaccurate, the reason for that problem is you. </span>\n\n> <span style=\"font-weight: 400;\">Imagine a world that decided to redesign and adaption of our information management systems, to maintains our shared moral fortitudes and the means to support ‘verifiable claims’; and the reference-able terms any honest claim, is built upon.</span>\n\n##### fixing the misuse of ‘security’\n\n<span style=\"font-weight: 400;\">In our world as is operated without a ‘knowledge banking industry’, our views, and consequentially our world, is increasingly subjected to distortions; our news, is impacted by the complexities of what is simply called ‘[fake news](https://docs.google.com/document/d/1OPghC4ra6QLhaHhW8QvPJRMKGEXT7KaZtG_7s5-UQrw/edit)‘, even though, our world is increasingly saturated in data.</span>\n\n<span style=\"font-weight: 400;\">It is due to the structure of ICT systems; or more specifically, our information management systems, that is critical to our means, as individuals and as members of any group, that the design of the way ‘truth’ and trust; is supported by the communications infrastructure that now replaces the ‘print era’ tools used formally. Whilst it is currently the case that [consumers](http://www.un.org/esa/sustdev/publications/consumption_en.pdf) have virtually no means to solely and beneficially store data products produced about them, this does not need to be the case. </span>\n\n<span style=\"font-weight: 400;\">Technically, it can be made to work, somewhat easily.</span>\n\n<span style=\"font-weight: 400;\">If through the proper guidance and consideration, a knowledge banking industry was established; how would it yield influence upon our world? </span>\n\n<span style=\"font-weight: 400;\">How did the advent of our banking system, influence our society? </span>\n\n<span style=\"font-weight: 400;\">The advent of the seminal concept to our system of democracy, ‘rule of law’, how has the advent of ISPs influenced “rule of law” in our societies?</span>\n\n<span style=\"font-weight: 400;\">How could a knowledge banking industry, built upon international ‘free to use’ internet technology (including web) standards influence the world. </span><span style=\"font-weight: 400;\">Using data, how could we build applications to power economic yield through more modern means, to re-evaluate assumptions made due to the manner through which ‘information management’ now already impacts the world.</span>\n\n<span style=\"font-weight: 400;\">It is expected that significant benefit can be realised, by ensuring the application of data is made to be an extension of self; and that through infrastructure that supports this underlying principle, the means to cooperatively manage groups (ie: [persona ficta](https://en.wikipedia.org/wiki/Legal_person)), group works and decision making, can be provided an immeasurable level of technologically driven, socioeconomic support. In-effect, the person who should benefits most from the surveillance of themselves, should be that person, unless they’re breaking the law and harming others. </span><span style=\"font-size: 1rem;\">If this is not technically made to be the case by design, then the person is made to be a commodity, which breaks the needs of any ‘democracy’.</span>\n\n<span style=\"font-weight: 400;\">This underpinning concept; The vision, of how to improved economic infrastructure to supply, for the express benefit of natural persons, our means to live in societies ruled by law; that are made participatory with us; requires infrastructure that ensures we are entitled to make use of ‘information management’ infrastructure services as independent constituents. This in-turn sets the stage for participation with broader group, distinguished by the cooperative, jurisdictional, application of law and lawfully provided apparatus. </span>\n\n<span style=\"font-weight: 400;\">This legal infrastructure is in-turn integral to any and all, stakeholder tenancy. If we seek to be known as a stakeholder in our system of democracy, our information systems must be produced as to support it. If we want to innovate, to make the world a better place, we need to be able to make use of knowledge, or research, of quality, qualified works done by others as to contribute by way of innovation and the truer nature of invention that has made us what we are today. We cannot do that, where our information systems provide economic support, for the distribution of ‘fake news’, false knowledge (which is still ‘information’); and whilst the means to enhance our words as individuals is not in any way similarly capable, to the means furnished through Internet Technologies, the least we could hope is that the use of ‘autocorrect’ does not mislead others as to our intended words; the best we can hope, is that our intended communications are not only able to be understood by few; but by many, should that be something we want to occur.</span>\n\n<span style=\"font-weight: 400;\">The aim of our [4th industrial evolution](https://en.wikipedia.org/wiki/Fourth_Industrial_Revolution) is to positively impact quality of life in our natural world. We are assumed to be the principle beneficiaries and decision makers, as members of our human species.</span>\n\n<span style=\"font-weight: 400;\">It is through us, that the health and sustainability of our natural world can be thoughtfully, and one might consider positively – impacted. That whilst it is the case we now have technology to augment the definition of lifeforms by DNA and otherwise; that our means to make moral choices, appropriate choices, that this capacity and burden, put upon the few, is built upon the means through which they are able to make use of our information systems. Whilst the term ‘Artificial Intelligence’ is muddy, when considering the broad nature of the tooling used in relation to the concept of ‘AI’, it is the case that most of these systems are designed to consume vast quantities of information from available sources online. </span>\n\nBut think, about how many things that relates to what it is to be human and ‘lived experience’ that one would never want to so comprehensively publish online, as to ensure the algorithmic training processes for AI; are able to take those sorts of things into account, for the beneficial use of the legal personalities who own and operate them for profit.\n\n<span style=\"font-weight: 400;\">Irrespective of the importance and implications of what it means to the world, should we produce a human centric web, powered by a ‘knowledge banking industry’; It remains the case that we must consider, in economic terms, how it is these sorts of goals, might be pragmatically achieved and what, if we do not, the implications of those choices are irrespective of what decisions are made, and what is in-turn, set-aside.</span>\n\n<span style=\"font-weight: 400;\"> </span><span style=\"font-weight: 400;\">The way in which an international knowledge banking industry could improve society is not yet broadly understood from this economic standpoint. The vision of a ‘human centric web’ has been produced, and later communicated, as to be contingent upon a design philosophy that is built upon [SOLID](https://en.wikipedia.org/wiki/SOLID), seminal, principles. Through these means the tooling has been produced through both the disconnected, and cooperatively collected; myriad of works produced by many, overtime, whose shared, principle related visionary considerations has now brought about an opportunity for the rest of humanity to decide what they want to do, if they want to define their own [inforg](https://en.wikipedia.org/wiki/Inforg) and how the systems that manage it are made to be used by others both in life and historically thereafter. This site, attempts to provide information about the tools to present, to any one who is interested, how it is, that a knowledge banking industry **can** be made.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","title":"Handong1587"},"frontmatter":{"draft":false},"rawBody":"This github blog theme is forked from [zJiaJun](https://github.com/zJiaJun).\n\nI use this repo to organise interesting papers, projects, websites, blogs and my reading/study notes.\n\nFeel free to send pull requests or open issues :-)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","title":"Does Anonymity exist?"},"frontmatter":{"draft":false},"rawBody":"---\nid: 127\ntitle: 'Does Anonymity exist?'\ndate: '2017-07-23T10:08:49+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=127'\npermalink: /does-anonymity-exist/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/does_anonymity_exist_'\ncategories:\n    - Uncategorized\n    - 'WWW Education'\n---\n\n<span style=\"font-weight: 400;\">The easiest way to answer this question is; in effect, no. 99% of all issues pertaining to a circumstance where it is said that the information doesn’t exist, is actually a problem of who has access to that information and the cost of obtaining it, rather than an honest circumstance where the data actually does not exist to substantiate a circumstance. </span>\n\n<span style=\"font-weight: 400;\">Whilst it took many years to convince leading, high-value organisations that the internet was a useful and worthwhile investment; their investments now, similarly to their investments in past, instigate controls over the internet that make it very difficult to genuinely do anything without leaving traces of those actions on the internet somewhere. </span>\n\n<span style=\"font-weight: 400;\">The bigger problem; is that this information is not available to the majority of victims who have been harmed by the unlawful behaviours of others; and in many circumstances, it’s illegal to collect that information for the purposes of participation in [<span class=\"textannotation disambiguated wl-thing\" id=\"urn:enhancement-d72dec6f-5aef-65de-95ac-c54eaa482dbb\" itemid=\"http://data.wordlift.io/wl0293/entity/rule_of_law\">rule of law</span>](https://en.wikipedia.org/wiki/Rule_of_law) fully; as a subset of the guiding principles that operate our society. </span>\n\n<span style=\"font-weight: 400;\">These problems are thereafter not technical in nature; but rather, socio-political. If public servants are found to be doing the wrong thing; that would cost the government, if they were easily able to provide that information to a court of law in a manner required by that court to effectively evaluate a circumstance. If powerful married men with families want to engage in sex with those who are not their wives, and their wives at times do the same; then whilst the ‘data’ may exist, it’s not available, regardless of the subsequent harm an acrimonious relationship may cause children. </span>\n\n<span style=\"font-weight: 400;\">Most organisations use sophisticated computing systems to manage their accounting, stock-management and related business records; yet we are still provided thermally printed paper receipts that fade in sunlight. </span>\n\n<span style=\"font-weight: 400;\">Mobile phones continuously track the whereabouts (and speed of travel) of it as a device; but this is not available for the purpose of dealing with traffic infringements. New vehicles can tell whether someone is wearing a seatbelt; but a special device is needed to get that information.</span>\n\n<span style=\"font-weight: 400;\">Our web-usage is continuously tracked, the websites we use can figure out when we sleep (due to lack of activity on mobile devices, et.al.) and whilst these things all form part of what is used for crimes that pertain to significant financial loss of government entities, it is more often than not suggested ‘not to exist’, and save particularly ‘special circumstances’ are not made available to a citizen seeking lawful remedy. </span>\n\n<span style=\"font-weight: 400;\">Whilst it is true that some, particularly skilled, dedicated and well-financed individuals can form circumstances in which their actions are made ‘anonymous’ or unable to be identified; this is simply not reality for the vast majority living in our modern ‘connected’ age. </span>\n\n<span style=\"font-weight: 400;\">So, whereas whether living in a democracy or otherwise; we seek ‘lawful remedy’ the question becomes how exactly it is that we go about achieving this, when we may be discouraged by others to do so. </span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","title":"An introduction to Virtual Machines."},"frontmatter":{"draft":false},"rawBody":"---\nid: 107\ntitle: 'An introduction to Virtual Machines.'\ndate: '2017-07-23T09:43:51+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=107'\npermalink: /an-introduction-to-virtual-machines/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/an_introduction_to_virtual_machines'\ncategories:\n    - 'Advanced Computing'\n---\n\n<span style=\"font-weight: 400;\">A virtual machine (<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-sh4yb3wo3la2fp58h4s08eiwfgyhzgck\" itemid=\"http://data.wordlift.io/wl0293/entity/vm_operating_system\">VM</span>) is an <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-el2vjcwhlvpjemh4dn0djhls75ze87n3\" itemid=\"http://data.wordlift.io/wl0293/entity/emulator\">emulation</span> of a computer system through the use of specialised software on a host <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-1bvp5brxari25fi9yf1rrpwyf2a0tnnb\" itemid=\"http://data.wordlift.io/wl0293/entity/computing\">computing</span> system. Virtual <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-kp2naj1xaxf9vlv6uefovjbshjr9eli2\" itemid=\"http://data.wordlift.io/wl0293/entity/machine\">Machines</span> (<span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-x54tt6khmxituuvao0rjep5k1cjttfwl\" itemid=\"http://data.wordlift.io/wl0293/entity/vm_operating_system\">VM</span>’s) are used throughout the internet for hosting systems, websites and other resources for an array of purposes including the means to scale a solution from using limited <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-nqshxxiw2nsann3c7o856lei201tvei2\" itemid=\"http://data.wordlift.io/wl0293/entity/computer_hardware\">hardware</span> resources as a small site or solution; through to managing the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-gn22qc83xapbkwr5s9cqagpwhuiq9kwg\" itemid=\"http://data.wordlift.io/wl0293/entity/computer_hardware\">hardware</span> requirements for that solution as it grows. Other uses of <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-daq1riy60rkeitoo4k8po2utmfx10gz3\" itemid=\"http://data.wordlift.io/wl0293/entity/vm_operating_system\">VM</span>’s include developers who want to test and/or develop websites, technology professionals who need to test particular forms of software, figure out or manage security risks such as malware; and an array of other purposes that make the use of VMs very, very popular. </span>\n\n<span style=\"font-weight: 400;\">On a less sophisticated basis; <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-k1ln4zd39rngqacnc5bd3j40024b2wpi\" itemid=\"http://data.wordlift.io/wl0293/entity/vm_operating_system\">VM</span>’s offer the means to run any type of operating system, as an <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-a62apjb2xj4cavogdjutwulof7v7dxej\" itemid=\"http://data.wordlift.io/wl0293/entity/application_software\">application</span> on most computers or laptops, so it doesn’t matter if you have a mac or a pc; you can run whatever operating system you want in a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-ibyetmfx8icl2js0842z5xxdubmhi7mi\" itemid=\"http://data.wordlift.io/wl0293/entity/vm_operating_system\">VM</span> and it will load on your machine when you want to use it, and can be turned-of whenever you want to turn it off – without leaving problems on your host-machine. Because the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-rlziwd6ppsrb9icz7yb8jxvqaxanf1kb\" itemid=\"http://data.wordlift.io/wl0293/entity/virtual_machine\">Virtual Machine</span> is an independent environment, from the operating system right through to any and all <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-4264hxdqots8jey3da5uq5kj4eiamox7\" itemid=\"http://data.wordlift.io/wl0293/entity/application_software\">applications</span> that run within it, whatever you do in that environment is stored within the virtual machine rather than in your normal <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-ttsqpdkpw981r1izvh5ultrbn6hzag9g\" itemid=\"http://data.wordlift.io/wl0293/entity/computing\">computing</span> environment. It’s also possible to put a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-sagz7t1tg2se5bco2hq6uxmulf0hx9e9\" itemid=\"http://data.wordlift.io/wl0293/entity/vm_operating_system\">VM</span> on a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-sk4mk829lyshpvagcis7pakjom8pfghy\" itemid=\"http://data.wordlift.io/wl0293/entity/usb_flash_drive\">USB key</span> and load it on other machines, or share the work you’ve done in the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-dvtmq1bd9x79gx2zr6n63f2r5zpod6g6\" itemid=\"http://data.wordlift.io/wl0293/entity/vm_operating_system\">VM</span> by simply copying the <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-lp4ftviq1hqgiu5q2mvqinzwbng7swwr\" itemid=\"http://data.wordlift.io/wl0293/entity/vm_operating_system\">VM</span> on a <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-bj9in9i6791qdpbxeajowz7qr1kskxqd\" itemid=\"http://data.wordlift.io/wl0293/entity/usb_flash_drive\">USB Key</span> and giving it to someone (with the relevant details) for them to review of store safely for you.</span>\n\n<span style=\"font-weight: 400;\">A commonly used <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-4rcqfj6thvmsj18m62jxeya0pd6l087h\" itemid=\"http://data.wordlift.io/wl0293/entity/application_software\">application</span> for creating <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-xdvngzja3tlwkr7mxe9gqzby2068c2ul\" itemid=\"http://data.wordlift.io/wl0293/entity/virtual_machine\">Virtual Machines</span> is [VirtualBox](https://www.virtualbox.org/). </span>\n\nVirtual Machines can be used to create a cleaner computing environment that can be used for some sort of specific purpose that you don’t want to be stored on your every day computing environment. In this way, virtual machines are an effective means to deal with other [web-persistence](https://openseason.online/2017/07/23/web-persistence/) issues, ideally also alongside the use of a [VPN](https://en.wikipedia.org/wiki/Virtual_private_network)."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","title":"Openlink Virtuoso"},"frontmatter":{"draft":false},"rawBody":"---\nid: 78\ntitle: 'Openlink Virtuoso'\ndate: '2017-07-21T17:13:18+10:00'\nauthor: ubiquitous\nlayout: post\nguid: 'https://openseason.online/?p=78'\npermalink: /openlink-virtuoso/\nentity_url:\n    - 'http://data.wordlift.it/wl0353/post/openlink_virtuoso'\ncategories:\n    - 'Product and Service Reviews'\n---\n\nOpenlink Virtuoso is a universal server product that offers advanced support for RDF, access control systems and the integration of multiple datasources into a format that can be queried.\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"485\" loading=\"lazy\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"//www.slideshare.net/slideshow/embed_code/key/suMtskxfdYXgsk\" style=\"border: 1px solid #CCC; border-width: 1px; margin-bottom: 5px; max-width: 100%;\" width=\"595\"> </iframe>\n\n<div style=\"margin-bottom: 5px;\"> **[OpenLink Virtuoso – Management &amp; Decision Makers Overview](//www.slideshare.net/kidehen/openlink-virtuoso-management-des \"OpenLink Virtuoso - Management & Decision Makers Overview\")**  from **[Kingsley Uyi Idehen](https://www.slideshare.net/kidehen)**</div>Some of the more powerful features of this server product is the means in which it is able to map RDBMS sources into RDF queryable formats, leading to an ability to link a multitude of data-sources into a singular query interface\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/videoseries?list=PLCbmz0VSZ_vqsYV4B_oXulV3ncJbFj0Hb\" width=\"560\"></iframe>"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","title":"UDHR"},"frontmatter":{"draft":false},"rawBody":"\nAn Example of the UN [Universal Declaration of Human Rights](https://www.un.org/en/about-us/universal-declaration-of-human-rights) in RDF, is below.\n\nLinks to the Files are per below\n\nTurtle: https://github.com/WebCivics/ontologies/blob/2023/ttl/un/udhr.ttl\nJson-LD: https://github.com/WebCivics/ontologies/blob/2023/jsonld/un/udhr.jsonld\n\nThis is work in progress, there are many others that are on the to do list.\n\n```\n@prefix udhr: <http://www.udhr.org/ontology/udhr#> .\n\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n\n@prefix xhtml: <http://www.w3.org/1999/xhtml/> .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasPreamble [\n\n    a udhr:Preamble ;\n\n    udhr:hasText \"\"\"\n\n      Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world,\n\n      Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people,\n\n      Whereas it is essential, if man is not to be compelled to have recourse, as a last resort, to rebellion against tyranny and oppression, that human rights should be protected by the rule of law,\n\n      Whereas it is essential to promote the development of friendly relations between nations,\n\n      Whereas the peoples of the United Nations have in the Charter reaffirmed their faith in fundamental human rights, in the dignity and worth of the human person and in the equal rights of men and women and have determined to promote social progress and better standards of life in larger freedom,\n\n      Whereas Member States have pledged themselves to achieve, in co-operation with the United Nations, the promotion of universal respect for and observance of human rights and fundamental freedoms,\n\n      Whereas a common understanding of these rights and freedoms is of the greatest importance for the full realization of this pledge,\n\n      Now, Therefore THE GENERAL ASSEMBLY proclaims THIS UNIVERSAL DECLARATION OF HUMAN RIGHTS as a common standard of achievement for all peoples and all nations, to the end that every individual and every organ of society, keeping this Declaration constantly in mind, shall strive by teaching and education to promote respect for these rights and freedoms and by progressive measures, national and international, to secure their universal and effective recognition and observance, both among the peoples of Member States themselves and among the peoples of territories under their jurisdiction.\n\n    \"\"\"@en ;\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"1\"^^xsd:integer ;\n\n    rdfs:label \"Article 1. All human beings are born free and equal in dignity and rights.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status. Furthermore, no distinction shall be made on the basis of the political, jurisdictional or international status of the country or territory to which a person belongs, whether it be independent, trust, non-self-governing or under any other limitation of sovereignty.\n\n    \"\"\"@en ;\n\n  \n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"2\"^^xsd:integer ;\n\n    rdfs:label \"Article 2. Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status. Furthermore, no distinction shall be made on the basis of the political, jurisdictional or international status of the country or territory to which a person belongs, whether it be independent, trust, non-self-governing or under any other limitation of sovereignty.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status. Furthermore, no distinction shall be made on the basis of the political, jurisdictional or international status of the country or territory to which a person belongs, whether it be independent, trust, non-self-governing or under any other limitation of sovereignty.\n\n    \"\"\"@en ;\n\n   ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"3\"^^xsd:integer ;\n\n    rdfs:label \"Article 3. Everyone has the right to life, liberty and security of person.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone has the right to life, liberty and security of person.\n\n    \"\"\"@en ;\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"4\"^^xsd:integer ;\n\n    rdfs:label \"Article 4. No one shall be held in slavery or servitude; slavery and the slave trade shall be prohibited in all their forms.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      No one shall be held in slavery or servitude; slavery and the slave trade shall be prohibited in all their forms.\n\n    \"\"\"@en ;\n\n  \n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"5\"^^xsd:integer ;\n\n    rdfs:label \"Article 5. No one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      No one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment.\n\n    \"\"\"@en ;\n\n    ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n  udhr:articleNumber \"6\"^^xsd:integer ;    rdfs:label \"Article 6. Everyone has the right to recognition everywhere as a person before the law.\"@en ;    udhr:hasText \"\"\"\n\n      Everyone has the right to recognition everywhere as a person before the law. \"\"\"@en ;\n\n  \n\n  ] .\n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"7\"^^xsd:integer ;\n\n    rdfs:label \"Article 7. All are equal before the law and are entitled without any discrimination to equal protection of the law. All are entitled to equal protection against any discrimination in violation of this Declaration and against any incitement to such discrimination.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      All are equal before the law and are entitled without any discrimination to equal protection of the law. All are entitled to equal protection against any discrimination in violation of this Declaration and against any incitement to such discrimination.\n\n    \"\"\"@en ;\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"8\"^^xsd:integer ;\n\n    rdfs:label \"Article 8. Everyone has the right to an effective remedy by the competent national tribunals for acts violating the fundamental rights granted him by the constitution or by law.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone has the right to an effective remedy by the competent national tribunals for acts violating the fundamental rights granted him by the constitution or by law.\n\n    \"\"\"@en ;\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"9\"^^xsd:integer ;\n\n    rdfs:label \"Article 9. No one shall be subjected to arbitrary arrest, detention or exile.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      No one shall be subjected to arbitrary arrest, detention or exile.\n\n    \"\"\"@en ;\n\n  ] .\n\n  \n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"10\"^^xsd:integer ;\n\n    rdfs:label \"Article 10. Everyone is entitled in full equality to a fair and public hearing by an independent and impartial tribunal, in the determination of his rights and obligations and of any criminal charge against him.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone is entitled in full equality to a fair and public hearing by an independent and impartial tribunal, in the determination of his rights and obligations and of any criminal charge against him.\n\n    \"\"\"@en ;\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"11\"^^xsd:integer ;\n\n    rdfs:label \"Article 11. Everyone charged with a penal offence has the right to be presumed innocent until proved guilty according to law in a public trial at which he has had all the guarantees necessary for his defence.\"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone charged with a penal offence has the right to be presumed innocent until proved guilty according to law in a public trial at which he has had all the guarantees necessary for his defence.\n\n      \"\"\"@en\n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 2\" ;      udhr:hasText \"\"\"\n\n        No one shall be held guilty of any penal offence on account of any act or omission which did not constitute a penal offence, under national or international law, at the time when it was committed. Nor shall a heavier penalty be imposed than the one that was applicable at the time the penal offence was committed.\n\n      \"\"\"@en  \n\n    ]\n\n  ] .\n\n  \n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"12\"^^xsd:integer ;\n\n    rdfs:label \"Article 12. No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation. Everyone has the right to the protection of the law against such interference or attacks.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation. Everyone has the right to the protection of the law against such interference or attacks.\n\n    \"\"\"@en\n\n  ] .\n\n  \n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"13\"^^xsd:integer ;\n\n    rdfs:label \"Article 13. (1) Everyone has the right to freedom of movement and residence within the borders of each state. (2) Everyone has the right to leave any country, including his own, and to return to his country.\"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to freedom of movement and residence within the borders of each state.\n\n      \"\"\"@en\n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 2\" ;      udhr:hasText \"\"\"\n\n        Everyone has the right to leave any country, including his own, and to return to his country.\n\n      \"\"\"@en\n\n      ]\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"14\"^^xsd:integer ;\n\n    rdfs:label \"Article 14. (1) Everyone has the right to seek and to enjoy in other countries asylum from persecution. (2) This right may not be invoked in the case of prosecutions genuinely arising from non-political crimes or from acts contrary to the purposes and principles of the United Nations.\"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to seek and to enjoy in other countries asylum from persecution.\n\n      \"\"\"@en\n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 2\" ;      udhr:hasText \"\"\"\n\n        This right may not be invoked in the case of prosecutions genuinely arising from non-political crimes or from acts contrary to the purposes and principles of the United Nations.\n\n      \"\"\"@en ;  \n\n      ]\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"15\"^^xsd:integer ;\n\n    rdfs:label \"Article 15. (1) Everyone has the right to a nationality. (2) No one shall be arbitrarily deprived of his nationality nor denied the right to change his nationality.\"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to a nationality.\n\n      \"\"\"@en\n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 2\" ;      udhr:hasText \"\"\"\n\n        No one shall be arbitrarily deprived of his nationality nor denied the right to change his nationality.\n\n      \"\"\"@en\n\n    ]\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"16\"^^xsd:integer ;\n\n    rdfs:label \"Article 16. (1) Men and women of full age, without any limitation due to race, nationality or religion, have the right to marry and to found a family. They are entitled to equal rights as to marriage, during marriage and at its dissolution. (2) Marriage shall be entered into only with the free and full consent of the intending spouses. (3) The family is the natural and fundamental group unit of society and is entitled to protection by society and the State.\"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Men and women of full age, without any limitation due to race, nationality or religion, have the right to marry and to found a family. They are entitled to equal rights as to marriage, during marriage and at its dissolution.\n\n      \"\"\"@en  \n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 2\" ;      udhr:hasText \"\"\"\n\n        Marriage shall be entered into only with the free and full consent of the intending spouses.\n\n      \"\"\"@en  ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 4\" ;      udhr:hasText \"\"\"\n\n       The family is the natural and fundamental group unit of society and is entitled to protection by society and the State.\n\n      \"\"\"@en\n\n    ]\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"17\"^^xsd:integer ;\n\n    rdfs:label \"Article 17. (1) Everyone has the right to own property alone as well as in association with others. (2) No one shall be arbitrarily deprived of his property.\"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to own property alone as well as in association with others.\n\n      \"\"\"@en\n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 2\" ;      udhr:hasText \"\"\"\n\n        No one shall be arbitrarily deprived of his property.\n\n      \"\"\"@en\n\n    ]\n\n  \n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"18\"^^xsd:integer ;\n\n    rdfs:label \"Article 18. Everyone has the right to freedom of thought, conscience and religion; this right includes freedom to change his religion or belief, and freedom, either alone or in community with others and in public or private, to manifest his religion or belief in teaching, practice, worship and observance.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone has the right to freedom of thought, conscience and religion; this right includes freedom to change his religion or belief, and freedom, either alone or in community with others and in public or private, to manifest his religion or belief in teaching, practice, worship and observance.\n\n    \"\"\"@en\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"19\"^^xsd:integer ;\n\n    rdfs:label \"Article 19. Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.\n\n    \"\"\"@en\n\n  ] .\n\n  \n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"20\"^^xsd:integer ;\n\n    rdfs:label \"Article 20. (1) Everyone has the right to freedom of peaceful assembly and association. (2) No one may be compelled to belong to an association.\" @en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to freedom of peaceful assembly and association.\n\n      \"\"\"@en\n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 2\" ;      udhr:hasText \"\"\"\n\n        No one may be compelled to belong to an association.\n\n      \"\"\"@en\n\n    ]\n\n  ] .\n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"21\"^^xsd:integer ;\n\n    rdfs:label \"Article 21. (1) Everyone has the right to take part in the government of his country, directly or through freely chosen representatives. (2)  Everyone has the right of equal access to public service in his country. (3)  The will of the people shall be the basis of the authority of government; this will shall be expressed in periodic and genuine elections which shall be by universal and equal suffrage and shall be held by secret vote or by equivalent free voting procedures.\"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to take part in the government of his country, directly or through freely chosen representatives.\n\n      \"\"\"@en\n\n    ] ;\n\n  \n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 2\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right of equal access to public service in his country.\n\n      \"\"\"@en\n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 3\" ;      udhr:hasText \"\"\"\n\n        The will of the people shall be the basis of the authority of government; this will shall be expressed in periodic and genuine elections which shall be by universal and equal suffrage and shall be held by secret vote or by equivalent free voting procedures.\n\n      \"\"\"@en\n\n    ]\n\n  \n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"22\"^^xsd:integer ;\n\n    rdfs:label \"Article 22. Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality.\n\n    \"\"\"@en\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"23\"^^xsd:integer ;\n\n    rdfs:label \"Article 23. (1) Everyone has the right to work, to free choice of employment, to just and favourable conditions of work and to protection against unemployment. (2)  Everyone, without any discrimination, has the right to equal pay for equal work. (3)  Everyone who works has the right to just and favourable remuneration ensuring for himself and his family an existence worthy of human dignity, and supplemented, if necessary, by other means of social protection. (4)  Everyone has the right to form and to join trade unions for the protection of his interests.Everyone has the right to take part in the government of his country, directly or through freely chosen representatives. \"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to work, to free choice of employment, to just and favourable conditions of work and to protection against unemployment.\n\n      \"\"\"@en\n\n    ] ;\n\n  \n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 2\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone, without any discrimination, has the right to equal pay for equal work.\n\n      \"\"\"@en\n\n    ] ;\n\n  \n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 3\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone who works has the right to just and favourable remuneration ensuring for himself and his family an existence worthy of human dignity, and supplemented, if necessary, by other means of social protection.\n\n      \"\"\"@en\n\n    ] ;\n\n    udhr:hasParagraph [      a udhr:Paragraph ;      rdfs:label \"Paragraph 4\" ;      udhr:hasText \"\"\"\n\n         Everyone has the right to form and to join trade unions for the protection of his interests.Everyone has the right to take part in the government of his country, directly or through freely chosen representatives.\n\n      \"\"\"@en\n\n      ]\n\n  \n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"24\"^^xsd:integer ;\n\n    rdfs:label \"Article 24. Everyone has the right to rest and leisure, including reasonable limitation of working hours and periodic holidays with pay.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone has the right to rest and leisure, including reasonable limitation of working hours and periodic holidays with pay.\n\n    \"\"\"@en ;\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"25\"^^xsd:integer ;\n\n    rdfs:label \"Article 25. (1) Everyone has the right to a standard of living adequate for the health and well-being of himself and of his family, including food, clothing, housing and medical care and necessary social services, and the right to security in the event of unemployment, sickness, disability, widowhood, old age or other lack of livelihood in circumstances beyond his control. (2)  Motherhood and childhood are entitled to special care and assistance. All children, whether born in or out of wedlock, shall enjoy the same social protection. \"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to a standard of living adequate for the health and well-being of himself and of his family, including food, clothing, housing and medical care and necessary social services, and the right to security in the event of unemployment, sickness, disability, widowhood, old age or other lack of livelihood in circumstances beyond his control.\n\n      \"\"\"@en ;\n\n    ] ;\n\n  \n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 2\" ;\n\n      udhr:hasText \"\"\"\n\n         Motherhood and childhood are entitled to special care and assistance. All children, whether born in or out of wedlock, shall enjoy the same social protection.\n\n      \"\"\"@en ;\n\n    ]\n\n  \n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"26\"^^xsd:integer ;\n\n    rdfs:label \"Article 26. (1) Everyone has the right to education. Education shall be free, at least in the elementary and fundamental stages. Elementary education shall be compulsory. Technical and professional education shall be made generally available and higher education shall be equally accessible to all on the basis of merit. (2)  Education shall be directed to the full development of the human personality and to the strengthening of respect for human rights and fundamental freedoms. It shall promote understanding, tolerance and friendship among all nations, racial or religious groups, and shall further the activities of the United Nations for the maintenance of peace. (3)  Parents have a prior right to choose the kind of education that shall be given to their children. \"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right to education. Education shall be free, at least in the elementary and fundamental stages. Elementary education shall be compulsory. Technical and professional education shall be made generally available and higher education shall be equally accessible to all on the basis of merit.\n\n      \"\"\"@en ;\n\n    ] ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 2\" ;\n\n      udhr:hasText \"\"\"\n\n        Education shall be directed to the full development of the human personality and to the strengthening of respect for human rights and fundamental freedoms. It shall promote understanding, tolerance and friendship among all nations, racial or religious groups, and shall further the activities of the United Nations for the maintenance of peace.\n\n      \"\"\"@en ;\n\n    ] ;\n\n  \n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 3\" ;\n\n      udhr:hasText \"\"\"\n\n         Parents have a prior right to choose the kind of education that shall be given to their children.\n\n      \"\"\"@en ;\n\n    ]\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"27\"^^xsd:integer ;\n\n    rdfs:label \"Article 27. (1) Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits. (2)  Everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author. \"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits.\n\n      \"\"\"@en ;\n\n    ] ;\n\n  \n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 2\" ;\n\n      udhr:hasText \"\"\"\n\n         Everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author.\n\n      \"\"\"@en ;\n\n    ]\n\n  \n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"28\"^^xsd:integer ;\n\n    rdfs:label \"Article 28. Everyone is entitled to a social and international order in which the rights and freedoms set forth in this Declaration can be fully realized.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Everyone is entitled to a social and international order in which the rights and freedoms set forth in this Declaration can be fully realized.\n\n    \"\"\"@en ;\n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"29\"^^xsd:integer ;\n\n    rdfs:label \"Article 29. (1) Everyone has duties to the community in which alone the free and full development of his personality is possible.  (2)  In the exercise of his rights and freedoms, everyone shall be subject only to such limitations as are determined by law solely for the purpose of securing due recognition and respect for the rights and freedoms of others and of meeting the just requirements of morality, public order and the general welfare in a democratic society.  (3)  These rights and freedoms may in no case be exercised contrary to the purposes and principles of the United Nations. \"@en ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 1\" ;\n\n      udhr:hasText \"\"\"\n\n        Everyone has duties to the community in which alone the free and full development of his personality is possible.\n\n      \"\"\"@en ;\n\n    ] ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 2\" ;\n\n      udhr:hasText \"\"\"\n\n        In the exercise of his rights and freedoms, everyone shall be subject only to such limitations as are determined by law solely for the purpose of securing due recognition and respect for the rights and freedoms of others and of meeting the just requirements of morality, public order and the general welfare in a democratic society.\n\n      \"\"\"@en ;\n\n    ] ;\n\n    udhr:hasParagraph [\n\n      a udhr:Paragraph ;\n\n      rdfs:label \"Paragraph 3\" ;\n\n      udhr:hasText \"\"\"\n\n          These rights and freedoms may in no case be exercised contrary to the purposes and principles of the United Nations.\n\n      \"\"\"@en ;\n\n    ]\n\n  \n\n  ] .\n\n  \n\n<http://www.udhr.org/> a udhr:Declaration ;\n\n  udhr:hasArticle [\n\n    a udhr:Article ;\n\n    udhr:articleNumber \"30\"^^xsd:integer ;\n\n    rdfs:label \"Article 30. Nothing in this Declaration may be interpreted as implying for any State, group or person any right to engage in any activity or to perform any act aimed at the destruction of any of the rights and freedoms set forth herein.\"@en ;\n\n    udhr:hasText \"\"\"\n\n      Nothing in this Declaration may be interpreted as implying for any State, group or person any right to engage in any activity or to perform any act aimed at the destruction of any of the rights and freedoms set forth herein.\n\n    \"\"\"@en ;\n\n  ] .\n```"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","title":"DataTypesOntology (DTO) Core"},"frontmatter":{"draft":false},"rawBody":"# DataTypesOntology (DTO) Core\n\n`<http://webizen.org/ontologies/dto/0.1/>`\n\n```\n@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs:  <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix owl:  <http://www.w3.org/2002/07/owl#> .\n@prefix vs:  <http://www.w3.org/2003/06/sw-vocab-status/ns#> .\n@prefix foaf:  <http://xmlns.com/foaf/0.1/> .\n@prefix wot:  <http://xmlns.com/wot/0.1/> .\n@prefix dc:  <http://purl.org/dc/elements/1.1/> .\n@prefix dto:  <http://webizen.org/ontologies/dto/0.1/> .\n\nfoaf:   rdf:type        owl:Ontology ;\n        dc:title        \"Data Types Ontology (DTO) vocabulary\" ;\n        dc:description  \"Data Types Ontology (DTO) RDF vocabulary, described using W3C RDF Schema and the Web Ontology Language.\" .\n```\n\nThe Data Types Ontology (DTO) RDF vocabulary, described using W3C RDF Schema and the Web Ontology Language.\n\n[The original specification](http://xmlns.com/foaf/spec/) is licensed under [CC BY-1.0](http://creativecommons.org/licenses/by/1.0/).\n\n(TO DO - COMPLETELY REWORK ONTOLOGY TO SUPPORT DataTypes)\n\n#### Contents\n\n- [Agent](#agent)\n- [Person](#person)\n- [name](#name)\n- [title](#title)\n- [img](#img)\n- [depiction](#depiction)\n- [family name](#family-name)\n- [given name](#given-name)\n- [knows](#knows)\n- [based near](#based-near)\n- [age](#age)\n- [made](#made)\n- [primary topic](#primary-topic)\n- [Project](#project)\n- [Organization](#organization)\n- [Group](#group)\n- [member](#member)\n- [Document](#document)\n- [Image](#image)\n\n## Agent\n\n`foaf:Agent`\n\n```\nfoaf:Agent  rdf:type         rdfs:Class ;\n        vs:term_status       \"stable\" ;\n        rdfs:label           \"Agent\" ;\n        rdfs:comment         \"An agent (eg. person, group, software or physical artifact).\" ;\n        rdf:type             owl:Class .\n```\n\nThe Agent class is the class of agents; things that do stuff. A well known sub-class is Person, representing people. Other kinds of agents include Organization and Group.\n\nThe Agent class is useful in a few places in FOAF where Person would have been overly specific. For example, the IM chat ID properties such as jabberID are typically associated with people, but sometimes belong to software bots.\n\n### equivalent class\n\n`owl:equivalentClass`\n\n- Agent `<http://purl.org/dc/terms/Agent>`\n\n## Person\n\n`foaf:Person`\n\n```\nfoaf:Person  rdf:type        rdfs:Class ;\n        rdfs:label           \"Person\" ;\n        rdfs:comment         \"A person.\" ;\n        vs:term_status       \"stable\" ;\n        rdf:type             owl:Class ;\n        rdfs:isDefinedBy     foaf: .\n```\n\nThe Person class represents people. Something is a Person if it is a person. We don't nitpic about whether they're alive, dead, real, or imaginary. The Person class is a sub-class of the Agent class, since all people are considered 'agents' in FOAF.\n\n### sub-class of\n\n`rdfs:subClassOf`\n\n- [Agent](#agent) `foaf:Agent`\n\n### equivalent classes\n\n`owl:equivalentClass`\n\n- Person `<http://schema.org/Person>`\n- Person `<http://www.w3.org/2000/10/swap/pim/contact#Person>`\n\n### disjoint with\n\n`owl:disjointWith`\n\n- [Organization](#organization) `foaf:Organization`\n- [Project](#project) `foaf:Project`\n\n## name\n\n`foaf:name`\n\n```\nfoaf:name  rdf:type         rdf:Property ;\n        vs:term_status      \"testing\" ;\n        rdfs:label          \"name\" ;\n        rdfs:comment        \"A name for some thing.\" ;\n        rdf:type            owl:DatatypeProperty ;\n        rdfs:isDefinedBy    foaf: .\n```\n\nFOAF provides some other naming constructs. While foaf:name does not explicitly represent name substructure (family vs given etc.) it does provide a basic level of interoperability. See the issue tracker for status of work on this issue.\n\nThe name property, like all RDF properties with a range of rdfs:Literal, may be used with XMLLiteral datatyped values (multiple names are acceptable whether they are in the same langauge or not). XMLLiteral usage is not yet widely adopted. Feedback on this aspect of the FOAF design is particularly welcomed.\n\n### sub-property of\n\n`rdfs:subPropertyOf`\n\n- Label `rdfs:label`\n\n### domain\n\n`rdfs:domain`\n\n- Thing `owl:Thing`\n\n### range\n\n`rdfs:range`\n\n- Literal `rdfs:Literal`\n\n## title\n\n`foaf:title`\n\n```\nfoaf:title  rdf:type      rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"title\" ;\n        rdfs:comment      \"Title (Mr, Mrs, Ms, Dr. etc)\" ;\n        rdf:type          owl:DatatypeProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\nThe approriate values for title are not formally constrained, and will vary across community and context. Values such as 'Mr', 'Mrs', 'Ms', 'Dr' etc. are expected.\n\n## img\n\n`foaf:img`\n\n```\nfoaf:img  rdf:type          rdf:Property ;\n        vs:term_status      \"testing\" ;\n        rdfs:label          \"image\" ;\n        rdfs:comment        \"An image that can be used to represent some thing (ie. those depictions which are particularly representative of something, eg. one's photo on a homepage).\" ;\n        rdf:type            owl:ObjectProperty ;\n        rdfs:isDefinedBy    foaf: .\n```\n\nThe img property relates a Person to a Image that represents them. Unlike its super-property depiction, we only use img when an image is particularly representative of some person. The analogy is with the image(s) that might appear on someone's homepage, rather than happen to appear somewhere in their photo album.\n\nUnlike the more general depiction property (and its inverse, depicts), the img property is only used with representations of people (ie. instances of Person). So you can't use it to find pictures of cats, dogs etc. The basic idea is to have a term whose use is more restricted than depiction so we can have a useful way of picking out a reasonable image to represent someone. FOAF defines img as a sub-property of depiction, which means that the latter relationship is implied whenever two things are related by the former.\n\nNote that img does not have any restrictions on the dimensions, colour depth, format etc of the Image it references.\n\nTerminology: note that img is a property (ie. relationship), and that code:Image is a similarly named class (ie. category, a type of thing). It might have been more helpful to call img 'mugshot' or similar; instead it is named by analogy to the HTML IMG element.\n\n### sub-property of\n\n`rdfs:subPropertyOf`\n\n- [Depiction](#depiction) `foaf:depiction`\n\n### domain\n\n`rdfs:domain`\n\n- [Person](#person) `foaf:Person`\n\n### range\n\n`rdfs:range`\n\n- [Image](#image) `foaf:Image`\n\n## depiction\n\n`foaf:depiction`\n\n```\nfoaf:depiction  rdf:type  rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"depiction\" ;\n        rdfs:comment      \"A depiction of some thing.\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe depiction property is a relationship between a thing and an Image that depicts it. As such it is an inverse of the depicts relationship.\n\nA common use of depiction (and depicts) is to indicate the contents of a digital image, for example the people or objects represented in an online photo gallery.\n\nExtensions to this basic idea include 'Co-Depiction' (social networks as evidenced in photos), as well as richer photo metadata through the mechanism of using SVG paths to indicate the regions of an image which depict some particular thing. See 'Annotating Images With SVG' for tools and details.\n\nThe basic notion of 'depiction' could also be extended to deal with multimedia content (video clips, audio), or refined to deal with corner cases, such as pictures of pictures etc.\n\nThe depiction property is a super-property of the more specific property img, which is used more sparingly. You stand in a depiction relation to any Image that depicts you, whereas img is typically used to indicate a few images that are particularly representative.\n\n### domain\n\n`rdfs:domain`\n\n- Thing `owl:Thing`\n\n### range\n\n`rdfs:range`\n\n- [Image](#image) `foaf:Image`\n\n### inverse of\n\n`owl:inverseOf`\n\n- [depicts](#depicts) `foaf:depicts`\n\n## family name\n\n`foaf:familyName`\n\n```\nfoaf:familyName  rdf:type  rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"familyName\" ;\n        rdfs:comment      \"The family name of some person.\" ;\n        rdf:type          owl:DatatypeProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe familyName property is provided (alongside givenName) for use when describing parts of people's names. Although these concepts do not capture the full range of personal naming styles found world-wide, they are commonly used and have some value.\n\nThere is also a simple name property.\n\nSupport is also provided for the more archaic and culturally varying terminology of firstName and lastName.\n\nSee the issue tracker for design discussions, status and ongoing work on rationalising the FOAF naming machinery.\n\n### domain\n\n`rdfs:domain`\n\n- [Person](#person) `foaf:Person`\n\n### range\n\n`rdfs:range`\n\n- Literal `rdfs:Literal`\n\n## given name\n\n`foaf:givenName`\n\n```\nfoaf:givenName  rdf:type  rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"Given name\" ;\n        rdfs:comment      \"The given name of some person.\" ;\n        rdf:type          owl:DatatypeProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe givenName property is provided (alongside familyName) for use when describing parts of people's names. Although these concepts do not capture the full range of personal naming styles found world-wide, they are commonly used and have some value.\n\nThere is also a simple name property.\n\nSupport is also provided for the more archaic and culturally varying terminology of firstName and lastName.\n\nSee the issue tracker for design discussions, status and ongoing work on rationalising the FOAF naming machinery.\n\n## knows\n\n`foaf:knows`\n\n```\nfoaf:knows  rdf:type      rdf:Property ;\n        vs:term_status    \"stable\" ;\n        rdfs:label        \"knows\" ;\n        rdfs:comment      \"A person known by this person (indicating some level of reciprocated interaction between the parties).\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe knows property relates a Person to another Person that he or she knows.\n\nWe take a broad view of 'knows', but do require some form of reciprocated interaction (ie. stalkers need not apply). Since social attitudes and conventions on this topic vary greatly between communities, counties and cultures, it is not appropriate for FOAF to be overly-specific here.\n\nIf someone knows a person, it would be usual for the relation to be reciprocated. However this doesn't mean that there is any obligation for either party to publish FOAF describing this relationship. A knows relationship does not imply friendship, endorsement, or that a face-to-face meeting has taken place: phone, fax, email, and smoke signals are all perfectly acceptable ways of communicating with people you know.\n\nYou probably know hundreds of people, yet might only list a few in your public FOAF file. That's OK. Or you might list them all. It is perfectly fine to have a FOAF file and not list anyone else in it at all. This illustrates the Semantic Web principle of partial description: RDF documents rarely describe the entire picture. There is always more to be said, more information living elsewhere in the Web (or in our heads...).\n\nSince knows is vague by design, it may be suprising that it has uses. Typically these involve combining other RDF properties. For example, an application might look at properties of each weblog that was made by someone you \"knows\". Or check the newsfeed of the online photo archive for each of these people, to show you recent photos taken by people you know.\n\nTo provide additional levels of representation beyond mere 'knows', FOAF applications can do several things.\n\nThey can use more precise relationships than knows to relate people to people. The original FOAF design included two of these ('knowsWell','friend') which we removed because they were somewhat awkward to actually use, bringing an inappopriate air of precision to an intrinsically vague concept. Other extensions have been proposed, including Eric Vitiello's Relationship module for FOAF.\n\nIn addition to using more specialised inter-personal relationship types (eg rel:acquaintanceOf etc) it is often just as good to use RDF descriptions of the states of affairs which imply particular kinds of relationship. So for example, two people who have the same value for their workplaceHomepage property are typically colleagues. We don't (currently) clutter FOAF up with these extra relationships, but the facts can be written in FOAF nevertheless. Similarly, if there exists a Document that has two people listed as its makers, then they are probably collaborators of some kind. Or if two people appear in 100s of digital photos together, there's a good chance they're friends and/or colleagues.\n\nSo FOAF is quite pluralistic in its approach to representing relationships between people. FOAF is built on top of a general purpose machine language for representing relationships (ie. RDF), so is quite capable of representing any kinds of relationship we care to add. The problems are generally social rather than technical; deciding on appropriate ways of describing these interconnections is a subtle art.\n\nPerhaps the most important use of knows is, alongside the rdfs:seeAlso property, to connect FOAF files together. Taken alone, a FOAF file is somewhat dull. But linked in with 1000s of other FOAF files it becomes more interesting, with each FOAF file saying a little more about people, places, documents, things... By mentioning other people (via knows or other relationships), and by providing an rdfs:seeAlso link to their FOAF file, you can make it easy for FOAF indexing tools ('scutters') to find your FOAF and the FOAF of the people you've mentioned. And the FOAF of the people they mention, and so on. This makes it possible to build FOAF aggregators without the need for a centrally managed directory of FOAF files...\n\n### domain\n\n`rdfs:domain`\n\n- [Person](#person) `foaf:Person`\n\n### range\n\n`rdfs:range`\n\n- [Person](#person) `foaf:Person`\n\n## based near\n\n`foaf:based_near`\n\n```\nfoaf:based_near  rdf:type  rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"based near\" ;\n        rdfs:comment      \"A location that something is based near, for some broadly human notion of near.\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe based_near relationship relates two \"spatial things\" (anything that can be somewhere), the latter typically described using the geo:lat / geo:long geo-positioning vocabulary (See GeoInfo in the W3C semweb wiki for details). This allows us to say describe the typical latitute and longitude of, say, a Person (people are spatial things - they can be places) without implying that a precise location has been given.\n\nWe do not say much about what 'near' means in this context; it is a 'rough and ready' concept. For a more precise treatment, see GeoOnion vocab design discussions, which are aiming to produce a more sophisticated vocabulary for such purposes.\n\nFOAF files often make use of the contact:nearestAirport property. This illustrates the distinction between FOAF documents (which may make claims using any RDF vocabulary) and the core FOAF vocabulary defined by this specification. For further reading on the use of nearestAirport see UsingContactNearestAirport in the FOAF wiki.\n\n### domain\n\n`rdfs:domain`\n\n- Spatial Thing `<http://www.w3.org/2003/01/geo/wgs84_pos#SpatialThing>`\n\n### range\n\n`rdfs:range`\n\n- Spatial Thing `<http://www.w3.org/2003/01/geo/wgs84_pos#SpatialThing>`\n\n## age\n\n`foaf:age`\n\n```\nfoaf:age  rdf:type        rdf:Property ;\n        vs:term_status    \"unstable\" ;\n        rdfs:label        \"age\" ;\n        rdfs:comment      \"The age in years of some agent.\" ;\n        rdf:type          owl:FunctionalProperty ;\n        rdf:type          owl:DatatypeProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe age property is a relationship between a Agent and an integer string representing their age in years. See also birthday.\n\n### domain\n\n`rdfs:domain`\n\n- [Agent](#agent) `foaf:Agent`\n\n### range\n\n`rdfs:range`\n\n- Literal `rdfs:Literal`\n\n## made\n\n`foaf:made`\n\n```\nfoaf:made  rdf:type       rdf:Property ;\n        vs:term_status    \"stable\" ;\n        rdfs:label        \"made\" ;\n        rdfs:comment      \"Something that was made by this agent.\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe made property relates a Agent to something made by it. As such it is an inverse of the maker property, which relates a thing to something that made it. See made for more details on the relationship between these FOAF terms and related Dublin Core vocabulary.\n\n### domain\n\n`rdfs:domain`\n\n- [Agent](#agent) `foaf:Agent`\n\n### range\n\n`rdfs:range`\n\n- Thing `owl:Thing`\n\n### inverse of\n\n`owl:inverseOf`\n\n- [maker](#maker) `foaf:maker`\n\n## primary topic\n\n`foaf:primaryTopic`\n\n```\nfoaf:primaryTopic  rdf:type  rdf:Property ;\n        vs:term_status    \"stable\" ;\n        rdfs:label        \"primary topic\" ;\n        rdfs:comment      \"The primary topic of some page or document.\" ;\n        rdf:type          owl:FunctionalProperty ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe primaryTopic property relates a document to the main thing that the document is about.\n\nThe primaryTopic property is functional: for any document it applies to, it can have at most one value. This is useful, as it allows for data merging. In many cases it may be difficult for third parties to determine the primary topic of a document, but in a useful number of cases (eg. descriptions of movies, restaurants, politicians, ...) it should be reasonably obvious. Documents are very often the most authoritative source of information about their own primary topics, although this cannot be guaranteed since documents cannot be assumed to be accurate, honest etc.\n\nIt is an inverse of the isPrimaryTopicOf property, which relates a thing to a document primarily about that thing. The choice between these two properties is purely pragmatic. When describing documents, we use primaryTopic former to point to the things they're about. When describing things (people etc.), it is useful to be able to directly cite documents which have those things as their main topic - so we use isPrimaryTopicOf. In this way, Web sites such as Wikipedia or NNDB can provide indirect identification for the things they have descriptions of.\n\n### domain\n\n`rdfs:domain`\n\n- [Document](#document) `foaf:Document`\n\n### range\n\n`rdfs:range`\n\n- Thing `owl:Thing`\n\n### inverse of\n\n`owl:inverseOf`\n\n- [is primary topic of](#is-primary-topic-of) `foaf:isPrimaryTopicOf`\n\n## Project\n\n`foaf:Project`\n\n```\nfoaf:Project  rdf:type    rdfs:Class ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"Project\" ;\n        rdfs:comment      \"A project (a collective endeavour of some kind).\" ;\n        rdf:type          owl:Class ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe Project class represents the class of things that are 'projects'. These may be formal or informal, collective or individual. It is often useful to indicate the homepage of a Project.\n\n### disjoint with\n\n`owl:disjointWith`\n\n- [Person](#person) `foaf:Person`\n- [Document](#document) `foaf:Document`\n\n## Organization\n\n`foaf:Organization`\n\n```\nfoaf:Organization  rdf:type  rdfs:Class ;\n        rdfs:label        \"Organization\" ;\n        rdfs:comment      \"An organization.\" ;\n        vs:term_status    \"stable\" ;\n        rdf:type          owl:Class ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe Organization class represents a kind of Agent corresponding to social instititutions such as companies, societies etc.\n\n### sub-class of\n\n`rdfs:subClassOf`\n\n- [Agent](#agent) `foaf:Agent`\n\n### disjoint with\n\n`owl:disjointWith`\n\n- [Person](#person) `foaf:Person`\n- [Document](#document) `foaf:Document`\n\n## Group\n\n`foaf:Group`\n\n```\nfoaf:Group  rdf:type     rdfs:Class ;\n        vs:term_status   \"stable\" ;\n        rdfs:label       \"Group\" ;\n        rdfs:comment     \"A class of Agents.\" ;\n        rdf:type         owl:Class .\n```\n\nThe Group class represents a collection of individual agents (and may itself play the role of a Agent, ie. something that can perform actions).\n\nThis concept is intentionally quite broad, covering informal and ad-hoc groups, long-lived communities, organizational groups within a workplace, etc. Some such groups may have associated characteristics which could be captured in RDF (perhaps a homepage, name, mailing list etc.).\n\nWhile a Group has the characteristics of a Agent, it is also associated with a number of other Agents (typically people) who constitute the Group. FOAF provides a mechanism, the membershipClass property, which relates a Group to a sub-class of the class Agent who are members of the group. This is a little complicated, but allows us to make group membership rules explicit.\n\nThe markup (shown below) for defining a group is both complex and powerful. It allows group membership rules to match against any RDF-describable characteristics of the potential group members. As FOAF and similar vocabularies become more expressive in their ability to describe individuals, the Group mechanism for categorising them into groups also becomes more powerful.\n\nWhile the formal description of membership criteria for a Group may be complex, the basic mechanism for saying that someone is in a Group is very simple. We simply use a member property of the Group to indicate the agents that are members of the group. For example:\n\n```xml\n<foaf:Group>\n <foaf:name>ILRT staff</foaf:name>\n <foaf:member>\n  <foaf:Person>\n   <foaf:name>Martin Poulter</foaf:name>\n   <foaf:homepage rdf:resource=\"http://www.ilrt.bris.ac.uk/aboutus/staff/staffprofile/?search=plmlp\"/>\n   <foaf:workplaceHomepage rdf:resource=\"http://www.ilrt.bris.ac.uk/\"/>\n  </foaf:Person>\n </foaf:member>\n</foaf:Group>\n```\n\nBehind the scenes, further RDF statements can be used to express the rules for being a member of this group. End-users of FOAF need not pay attention to these details.\n\nHere is an example. We define a Group representing those people who are ILRT staff members (ILRT is a department at the University of Bristol). The membershipClass property connects the group (conceived of as a social entity and agent in its own right) with the class definition for those people who constitute it. In this case, the rule is that all group members are in the ILRTStaffPerson class, which is in turn populated by all those things that are a Person and which have a workplaceHomepage of http://www.ilrt.bris.ac.uk/. This is typical: FOAF groups are created by specifying a sub-class of Agent (in fact usually this will be a sub-class of Person), and giving criteria for which things fall in or out of the sub-class. For this, we use the owl:onProperty and owl:hasValue properties, indicating the property/value pairs which must be true of matching agents.\n\n```xml\n<!-- here we see a FOAF group described.\n     each foaf group may be associated with an OWL definition\n     specifying the class of agents that constitute the group's membership -->\n<foaf:Group>\n <foaf:name>ILRT staff</foaf:name>\n <foaf:membershipClass>\n    <owl:Class rdf:about=\"http://ilrt.example.com/groups#ILRTStaffPerson\">\n     <rdfs:subClassOf rdf:resource=\"http://xmlns.com/foaf/0.1/Person\"/>\n     <rdfs:subClassOf>\n       <owl:Restriction>\n         <owl:onProperty rdf:resource=\"http://xmlns.com/foaf/0.1/workplaceHomepage\"/>\n         <owl:hasValue rdf:resource=\"http://www.ilrt.bris.ac.uk/\"/>\n       </owl:Restriction>\n     </rdfs:subClassOf>\n   </owl:Class>\n </foaf:membershipClass>\n</foaf:Group>\n```\n\nNote that while these example OWL rules for being in the eg:ILRTStaffPerson class are based on a Person having a particular workplaceHomepage, this places no obligations on the authors of actual FOAF documents to include this information. If the information is included, then generic OWL tools may infer that some person is an eg:ILRTStaffPerson. To go the extra step and infer that some eg:ILRTStaffPerson is a member of the group whose name is \"ILRT staff\", tools will need some knowledge of the way FOAF deals with groups. In other words, generic OWL technology gets us most of the way, but the full Group machinery requires extra work for implimentors.\n\nThe current design names the relationship as pointing from the group, to the member. This is convenient when writing XML/RDF that encloses the members within markup that describes the group. Alternate representations of the same content are allowed in RDF, so you can write claims about the Person and the Group without having to nest either description inside the other. For (brief) example:\n\n```xml\n<foaf:Group>\n <foaf:member rdf:nodeID=\"martin\"/>\n <!-- more about the group here -->\n</foaf:Group>\n<foaf:Person rdf:nodeID=\"martin\">\n  <!-- more about martin here -->\n</foaf:Person>\n```\n\nThere is a FOAF issue tracker associated with this FOAF term. A design goal is to make the most of W3C's OWL language for representing group-membership criteria, while also making it easy to leverage existing groups and datasets available online (eg. buddylists, mailing list membership lists etc). Feedback on the current design is solicited! Should we consider using SPARQL queries instead, for example?\n\n### sub-class of\n\n`rdfs:subClassOf`\n\n- [Agent](#agent) `foaf:Agent`\n\n## member\n\n`foaf:member`\n\n```\nfoaf:member  rdf:type     rdf:Property ;\n        vs:term_status    \"stable\" ;\n        rdfs:label        \"member\" ;\n        rdfs:comment      \"Indicates a member of a Group\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe member property relates a Group to a Agent that is a member of that group.\n\nSee Group for details and examples.\n\n### domain\n\n`rdfs:domain`\n\n- [Group](#group) `foaf:Group`\n\n### range\n\n`rdfs:range`\n\n- [Agent](#agent) `foaf:Agent`\n\n## Document\n\n`foaf:Document`\n\n```\nfoaf:Document  rdf:type      rdfs:Class ;\n        rdfs:label           \"Document\" ;\n        rdfs:comment         \"A document.\" ;\n        vs:term_status       \"stable\" ;\n        rdf:type             owl:Class ;\n        rdfs:isDefinedBy     foaf: .\n```\n\nThe Document class represents those things which are, broadly conceived, 'documents'.\n\nThe Image class is a sub-class of Document, since all images are documents.\n\n### equivalent class\n\n`owl:equivalentClass`\n\n- Creative Work `<http://schema.org/CreativeWork>`\n\n### disjoint with\n\n`owl:disjointWith`\n\n- [Organization](#organization) `foaf:Organization`\n- [Project](#project) `foaf:Project`\n\n## Image\n\n`foaf:Image`\n\n```\nfoaf:Image  rdf:type         rdfs:Class ;\n        vs:term_status       \"stable\" ;\n        rdfs:label           \"Image\" ;\n        rdfs:comment         \"An image.\" ;\n        rdf:type             owl:Class ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe class Image is a sub-class of Document corresponding to those documents which are images.\n\nDigital images (such as JPEG, PNG, GIF bitmaps, SVG diagrams etc.) are examples of Image.\n\n### sub-class of\n\n`rdfs:subClassOf`\n\n- [Document](#document) `foaf:Document`\n\n### equivalent class\n\n`owl:equivalentClass`\n\n- Image Object `<http://schema.org/ImageObject>`"},{"fields":{"slug":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","title":"Friend of a Friend (FOAF) Core"},"frontmatter":{"draft":false},"rawBody":"# Friend of a Friend (FOAF) Core\n\n`<http://xmlns.com/foaf/0.1/>`\n\n```\n@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs:  <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix owl:  <http://www.w3.org/2002/07/owl#> .\n@prefix vs:  <http://www.w3.org/2003/06/sw-vocab-status/ns#> .\n@prefix foaf:  <http://xmlns.com/foaf/0.1/> .\n@prefix wot:  <http://xmlns.com/wot/0.1/> .\n@prefix dc:  <http://purl.org/dc/elements/1.1/> .\n\nfoaf:   rdf:type        owl:Ontology ;\n        dc:title        \"Friend of a Friend (FOAF) vocabulary\" ;\n        dc:description  \"The Friend of a Friend (FOAF) RDF vocabulary, described using W3C RDF Schema and the Web Ontology Language.\" .\n```\n\nThe Friend of a Friend (FOAF) RDF vocabulary, described using W3C RDF Schema and the Web Ontology Language.\n\n[The original specification](http://xmlns.com/foaf/spec/) is licensed under [CC BY-1.0](http://creativecommons.org/licenses/by/1.0/).\n\n#### Contents\n\n- [Agent](#agent)\n- [Person](#person)\n- [name](#name)\n- [title](#title)\n- [img](#img)\n- [depiction](#depiction)\n- [family name](#family-name)\n- [given name](#given-name)\n- [knows](#knows)\n- [based near](#based-near)\n- [age](#age)\n- [made](#made)\n- [primary topic](#primary-topic)\n- [Project](#project)\n- [Organization](#organization)\n- [Group](#group)\n- [member](#member)\n- [Document](#document)\n- [Image](#image)\n\n## Agent\n\n`foaf:Agent`\n\n```\nfoaf:Agent  rdf:type         rdfs:Class ;\n        vs:term_status       \"stable\" ;\n        rdfs:label           \"Agent\" ;\n        rdfs:comment         \"An agent (eg. person, group, software or physical artifact).\" ;\n        rdf:type             owl:Class .\n```\n\nThe Agent class is the class of agents; things that do stuff. A well known sub-class is Person, representing people. Other kinds of agents include Organization and Group.\n\nThe Agent class is useful in a few places in FOAF where Person would have been overly specific. For example, the IM chat ID properties such as jabberID are typically associated with people, but sometimes belong to software bots.\n\n### equivalent class\n\n`owl:equivalentClass`\n\n- Agent `<http://purl.org/dc/terms/Agent>`\n\n## Person\n\n`foaf:Person`\n\n```\nfoaf:Person  rdf:type        rdfs:Class ;\n        rdfs:label           \"Person\" ;\n        rdfs:comment         \"A person.\" ;\n        vs:term_status       \"stable\" ;\n        rdf:type             owl:Class ;\n        rdfs:isDefinedBy     foaf: .\n```\n\nThe Person class represents people. Something is a Person if it is a person. We don't nitpic about whether they're alive, dead, real, or imaginary. The Person class is a sub-class of the Agent class, since all people are considered 'agents' in FOAF.\n\n### sub-class of\n\n`rdfs:subClassOf`\n\n- [Agent](#agent) `foaf:Agent`\n\n### equivalent classes\n\n`owl:equivalentClass`\n\n- Person `<http://schema.org/Person>`\n- Person `<http://www.w3.org/2000/10/swap/pim/contact#Person>`\n\n### disjoint with\n\n`owl:disjointWith`\n\n- [Organization](#organization) `foaf:Organization`\n- [Project](#project) `foaf:Project`\n\n## name\n\n`foaf:name`\n\n```\nfoaf:name  rdf:type         rdf:Property ;\n        vs:term_status      \"testing\" ;\n        rdfs:label          \"name\" ;\n        rdfs:comment        \"A name for some thing.\" ;\n        rdf:type            owl:DatatypeProperty ;\n        rdfs:isDefinedBy    foaf: .\n```\n\nFOAF provides some other naming constructs. While foaf:name does not explicitly represent name substructure (family vs given etc.) it does provide a basic level of interoperability. See the issue tracker for status of work on this issue.\n\nThe name property, like all RDF properties with a range of rdfs:Literal, may be used with XMLLiteral datatyped values (multiple names are acceptable whether they are in the same langauge or not). XMLLiteral usage is not yet widely adopted. Feedback on this aspect of the FOAF design is particularly welcomed.\n\n### sub-property of\n\n`rdfs:subPropertyOf`\n\n- Label `rdfs:label`\n\n### domain\n\n`rdfs:domain`\n\n- Thing `owl:Thing`\n\n### range\n\n`rdfs:range`\n\n- Literal `rdfs:Literal`\n\n## title\n\n`foaf:title`\n\n```\nfoaf:title  rdf:type      rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"title\" ;\n        rdfs:comment      \"Title (Mr, Mrs, Ms, Dr. etc)\" ;\n        rdf:type          owl:DatatypeProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\nThe approriate values for title are not formally constrained, and will vary across community and context. Values such as 'Mr', 'Mrs', 'Ms', 'Dr' etc. are expected.\n\n## img\n\n`foaf:img`\n\n```\nfoaf:img  rdf:type          rdf:Property ;\n        vs:term_status      \"testing\" ;\n        rdfs:label          \"image\" ;\n        rdfs:comment        \"An image that can be used to represent some thing (ie. those depictions which are particularly representative of something, eg. one's photo on a homepage).\" ;\n        rdf:type            owl:ObjectProperty ;\n        rdfs:isDefinedBy    foaf: .\n```\n\nThe img property relates a Person to a Image that represents them. Unlike its super-property depiction, we only use img when an image is particularly representative of some person. The analogy is with the image(s) that might appear on someone's homepage, rather than happen to appear somewhere in their photo album.\n\nUnlike the more general depiction property (and its inverse, depicts), the img property is only used with representations of people (ie. instances of Person). So you can't use it to find pictures of cats, dogs etc. The basic idea is to have a term whose use is more restricted than depiction so we can have a useful way of picking out a reasonable image to represent someone. FOAF defines img as a sub-property of depiction, which means that the latter relationship is implied whenever two things are related by the former.\n\nNote that img does not have any restrictions on the dimensions, colour depth, format etc of the Image it references.\n\nTerminology: note that img is a property (ie. relationship), and that code:Image is a similarly named class (ie. category, a type of thing). It might have been more helpful to call img 'mugshot' or similar; instead it is named by analogy to the HTML IMG element.\n\n### sub-property of\n\n`rdfs:subPropertyOf`\n\n- [Depiction](#depiction) `foaf:depiction`\n\n### domain\n\n`rdfs:domain`\n\n- [Person](#person) `foaf:Person`\n\n### range\n\n`rdfs:range`\n\n- [Image](#image) `foaf:Image`\n\n## depiction\n\n`foaf:depiction`\n\n```\nfoaf:depiction  rdf:type  rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"depiction\" ;\n        rdfs:comment      \"A depiction of some thing.\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe depiction property is a relationship between a thing and an Image that depicts it. As such it is an inverse of the depicts relationship.\n\nA common use of depiction (and depicts) is to indicate the contents of a digital image, for example the people or objects represented in an online photo gallery.\n\nExtensions to this basic idea include 'Co-Depiction' (social networks as evidenced in photos), as well as richer photo metadata through the mechanism of using SVG paths to indicate the regions of an image which depict some particular thing. See 'Annotating Images With SVG' for tools and details.\n\nThe basic notion of 'depiction' could also be extended to deal with multimedia content (video clips, audio), or refined to deal with corner cases, such as pictures of pictures etc.\n\nThe depiction property is a super-property of the more specific property img, which is used more sparingly. You stand in a depiction relation to any Image that depicts you, whereas img is typically used to indicate a few images that are particularly representative.\n\n### domain\n\n`rdfs:domain`\n\n- Thing `owl:Thing`\n\n### range\n\n`rdfs:range`\n\n- [Image](#image) `foaf:Image`\n\n### inverse of\n\n`owl:inverseOf`\n\n- [depicts](#depicts) `foaf:depicts`\n\n## family name\n\n`foaf:familyName`\n\n```\nfoaf:familyName  rdf:type  rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"familyName\" ;\n        rdfs:comment      \"The family name of some person.\" ;\n        rdf:type          owl:DatatypeProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe familyName property is provided (alongside givenName) for use when describing parts of people's names. Although these concepts do not capture the full range of personal naming styles found world-wide, they are commonly used and have some value.\n\nThere is also a simple name property.\n\nSupport is also provided for the more archaic and culturally varying terminology of firstName and lastName.\n\nSee the issue tracker for design discussions, status and ongoing work on rationalising the FOAF naming machinery.\n\n### domain\n\n`rdfs:domain`\n\n- [Person](#person) `foaf:Person`\n\n### range\n\n`rdfs:range`\n\n- Literal `rdfs:Literal`\n\n## given name\n\n`foaf:givenName`\n\n```\nfoaf:givenName  rdf:type  rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"Given name\" ;\n        rdfs:comment      \"The given name of some person.\" ;\n        rdf:type          owl:DatatypeProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe givenName property is provided (alongside familyName) for use when describing parts of people's names. Although these concepts do not capture the full range of personal naming styles found world-wide, they are commonly used and have some value.\n\nThere is also a simple name property.\n\nSupport is also provided for the more archaic and culturally varying terminology of firstName and lastName.\n\nSee the issue tracker for design discussions, status and ongoing work on rationalising the FOAF naming machinery.\n\n## knows\n\n`foaf:knows`\n\n```\nfoaf:knows  rdf:type      rdf:Property ;\n        vs:term_status    \"stable\" ;\n        rdfs:label        \"knows\" ;\n        rdfs:comment      \"A person known by this person (indicating some level of reciprocated interaction between the parties).\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe knows property relates a Person to another Person that he or she knows.\n\nWe take a broad view of 'knows', but do require some form of reciprocated interaction (ie. stalkers need not apply). Since social attitudes and conventions on this topic vary greatly between communities, counties and cultures, it is not appropriate for FOAF to be overly-specific here.\n\nIf someone knows a person, it would be usual for the relation to be reciprocated. However this doesn't mean that there is any obligation for either party to publish FOAF describing this relationship. A knows relationship does not imply friendship, endorsement, or that a face-to-face meeting has taken place: phone, fax, email, and smoke signals are all perfectly acceptable ways of communicating with people you know.\n\nYou probably know hundreds of people, yet might only list a few in your public FOAF file. That's OK. Or you might list them all. It is perfectly fine to have a FOAF file and not list anyone else in it at all. This illustrates the Semantic Web principle of partial description: RDF documents rarely describe the entire picture. There is always more to be said, more information living elsewhere in the Web (or in our heads...).\n\nSince knows is vague by design, it may be suprising that it has uses. Typically these involve combining other RDF properties. For example, an application might look at properties of each weblog that was made by someone you \"knows\". Or check the newsfeed of the online photo archive for each of these people, to show you recent photos taken by people you know.\n\nTo provide additional levels of representation beyond mere 'knows', FOAF applications can do several things.\n\nThey can use more precise relationships than knows to relate people to people. The original FOAF design included two of these ('knowsWell','friend') which we removed because they were somewhat awkward to actually use, bringing an inappopriate air of precision to an intrinsically vague concept. Other extensions have been proposed, including Eric Vitiello's Relationship module for FOAF.\n\nIn addition to using more specialised inter-personal relationship types (eg rel:acquaintanceOf etc) it is often just as good to use RDF descriptions of the states of affairs which imply particular kinds of relationship. So for example, two people who have the same value for their workplaceHomepage property are typically colleagues. We don't (currently) clutter FOAF up with these extra relationships, but the facts can be written in FOAF nevertheless. Similarly, if there exists a Document that has two people listed as its makers, then they are probably collaborators of some kind. Or if two people appear in 100s of digital photos together, there's a good chance they're friends and/or colleagues.\n\nSo FOAF is quite pluralistic in its approach to representing relationships between people. FOAF is built on top of a general purpose machine language for representing relationships (ie. RDF), so is quite capable of representing any kinds of relationship we care to add. The problems are generally social rather than technical; deciding on appropriate ways of describing these interconnections is a subtle art.\n\nPerhaps the most important use of knows is, alongside the rdfs:seeAlso property, to connect FOAF files together. Taken alone, a FOAF file is somewhat dull. But linked in with 1000s of other FOAF files it becomes more interesting, with each FOAF file saying a little more about people, places, documents, things... By mentioning other people (via knows or other relationships), and by providing an rdfs:seeAlso link to their FOAF file, you can make it easy for FOAF indexing tools ('scutters') to find your FOAF and the FOAF of the people you've mentioned. And the FOAF of the people they mention, and so on. This makes it possible to build FOAF aggregators without the need for a centrally managed directory of FOAF files...\n\n### domain\n\n`rdfs:domain`\n\n- [Person](#person) `foaf:Person`\n\n### range\n\n`rdfs:range`\n\n- [Person](#person) `foaf:Person`\n\n## based near\n\n`foaf:based_near`\n\n```\nfoaf:based_near  rdf:type  rdf:Property ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"based near\" ;\n        rdfs:comment      \"A location that something is based near, for some broadly human notion of near.\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe based_near relationship relates two \"spatial things\" (anything that can be somewhere), the latter typically described using the geo:lat / geo:long geo-positioning vocabulary (See GeoInfo in the W3C semweb wiki for details). This allows us to say describe the typical latitute and longitude of, say, a Person (people are spatial things - they can be places) without implying that a precise location has been given.\n\nWe do not say much about what 'near' means in this context; it is a 'rough and ready' concept. For a more precise treatment, see GeoOnion vocab design discussions, which are aiming to produce a more sophisticated vocabulary for such purposes.\n\nFOAF files often make use of the contact:nearestAirport property. This illustrates the distinction between FOAF documents (which may make claims using any RDF vocabulary) and the core FOAF vocabulary defined by this specification. For further reading on the use of nearestAirport see UsingContactNearestAirport in the FOAF wiki.\n\n### domain\n\n`rdfs:domain`\n\n- Spatial Thing `<http://www.w3.org/2003/01/geo/wgs84_pos#SpatialThing>`\n\n### range\n\n`rdfs:range`\n\n- Spatial Thing `<http://www.w3.org/2003/01/geo/wgs84_pos#SpatialThing>`\n\n## age\n\n`foaf:age`\n\n```\nfoaf:age  rdf:type        rdf:Property ;\n        vs:term_status    \"unstable\" ;\n        rdfs:label        \"age\" ;\n        rdfs:comment      \"The age in years of some agent.\" ;\n        rdf:type          owl:FunctionalProperty ;\n        rdf:type          owl:DatatypeProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe age property is a relationship between a Agent and an integer string representing their age in years. See also birthday.\n\n### domain\n\n`rdfs:domain`\n\n- [Agent](#agent) `foaf:Agent`\n\n### range\n\n`rdfs:range`\n\n- Literal `rdfs:Literal`\n\n## made\n\n`foaf:made`\n\n```\nfoaf:made  rdf:type       rdf:Property ;\n        vs:term_status    \"stable\" ;\n        rdfs:label        \"made\" ;\n        rdfs:comment      \"Something that was made by this agent.\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe made property relates a Agent to something made by it. As such it is an inverse of the maker property, which relates a thing to something that made it. See made for more details on the relationship between these FOAF terms and related Dublin Core vocabulary.\n\n### domain\n\n`rdfs:domain`\n\n- [Agent](#agent) `foaf:Agent`\n\n### range\n\n`rdfs:range`\n\n- Thing `owl:Thing`\n\n### inverse of\n\n`owl:inverseOf`\n\n- [maker](#maker) `foaf:maker`\n\n## primary topic\n\n`foaf:primaryTopic`\n\n```\nfoaf:primaryTopic  rdf:type  rdf:Property ;\n        vs:term_status    \"stable\" ;\n        rdfs:label        \"primary topic\" ;\n        rdfs:comment      \"The primary topic of some page or document.\" ;\n        rdf:type          owl:FunctionalProperty ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe primaryTopic property relates a document to the main thing that the document is about.\n\nThe primaryTopic property is functional: for any document it applies to, it can have at most one value. This is useful, as it allows for data merging. In many cases it may be difficult for third parties to determine the primary topic of a document, but in a useful number of cases (eg. descriptions of movies, restaurants, politicians, ...) it should be reasonably obvious. Documents are very often the most authoritative source of information about their own primary topics, although this cannot be guaranteed since documents cannot be assumed to be accurate, honest etc.\n\nIt is an inverse of the isPrimaryTopicOf property, which relates a thing to a document primarily about that thing. The choice between these two properties is purely pragmatic. When describing documents, we use primaryTopic former to point to the things they're about. When describing things (people etc.), it is useful to be able to directly cite documents which have those things as their main topic - so we use isPrimaryTopicOf. In this way, Web sites such as Wikipedia or NNDB can provide indirect identification for the things they have descriptions of.\n\n### domain\n\n`rdfs:domain`\n\n- [Document](#document) `foaf:Document`\n\n### range\n\n`rdfs:range`\n\n- Thing `owl:Thing`\n\n### inverse of\n\n`owl:inverseOf`\n\n- [is primary topic of](#is-primary-topic-of) `foaf:isPrimaryTopicOf`\n\n## Project\n\n`foaf:Project`\n\n```\nfoaf:Project  rdf:type    rdfs:Class ;\n        vs:term_status    \"testing\" ;\n        rdfs:label        \"Project\" ;\n        rdfs:comment      \"A project (a collective endeavour of some kind).\" ;\n        rdf:type          owl:Class ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe Project class represents the class of things that are 'projects'. These may be formal or informal, collective or individual. It is often useful to indicate the homepage of a Project.\n\n### disjoint with\n\n`owl:disjointWith`\n\n- [Person](#person) `foaf:Person`\n- [Document](#document) `foaf:Document`\n\n## Organization\n\n`foaf:Organization`\n\n```\nfoaf:Organization  rdf:type  rdfs:Class ;\n        rdfs:label        \"Organization\" ;\n        rdfs:comment      \"An organization.\" ;\n        vs:term_status    \"stable\" ;\n        rdf:type          owl:Class ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe Organization class represents a kind of Agent corresponding to social instititutions such as companies, societies etc.\n\n### sub-class of\n\n`rdfs:subClassOf`\n\n- [Agent](#agent) `foaf:Agent`\n\n### disjoint with\n\n`owl:disjointWith`\n\n- [Person](#person) `foaf:Person`\n- [Document](#document) `foaf:Document`\n\n## Group\n\n`foaf:Group`\n\n```\nfoaf:Group  rdf:type     rdfs:Class ;\n        vs:term_status   \"stable\" ;\n        rdfs:label       \"Group\" ;\n        rdfs:comment     \"A class of Agents.\" ;\n        rdf:type         owl:Class .\n```\n\nThe Group class represents a collection of individual agents (and may itself play the role of a Agent, ie. something that can perform actions).\n\nThis concept is intentionally quite broad, covering informal and ad-hoc groups, long-lived communities, organizational groups within a workplace, etc. Some such groups may have associated characteristics which could be captured in RDF (perhaps a homepage, name, mailing list etc.).\n\nWhile a Group has the characteristics of a Agent, it is also associated with a number of other Agents (typically people) who constitute the Group. FOAF provides a mechanism, the membershipClass property, which relates a Group to a sub-class of the class Agent who are members of the group. This is a little complicated, but allows us to make group membership rules explicit.\n\nThe markup (shown below) for defining a group is both complex and powerful. It allows group membership rules to match against any RDF-describable characteristics of the potential group members. As FOAF and similar vocabularies become more expressive in their ability to describe individuals, the Group mechanism for categorising them into groups also becomes more powerful.\n\nWhile the formal description of membership criteria for a Group may be complex, the basic mechanism for saying that someone is in a Group is very simple. We simply use a member property of the Group to indicate the agents that are members of the group. For example:\n\n```xml\n<foaf:Group>\n <foaf:name>ILRT staff</foaf:name>\n <foaf:member>\n  <foaf:Person>\n   <foaf:name>Martin Poulter</foaf:name>\n   <foaf:homepage rdf:resource=\"http://www.ilrt.bris.ac.uk/aboutus/staff/staffprofile/?search=plmlp\"/>\n   <foaf:workplaceHomepage rdf:resource=\"http://www.ilrt.bris.ac.uk/\"/>\n  </foaf:Person>\n </foaf:member>\n</foaf:Group>\n```\n\nBehind the scenes, further RDF statements can be used to express the rules for being a member of this group. End-users of FOAF need not pay attention to these details.\n\nHere is an example. We define a Group representing those people who are ILRT staff members (ILRT is a department at the University of Bristol). The membershipClass property connects the group (conceived of as a social entity and agent in its own right) with the class definition for those people who constitute it. In this case, the rule is that all group members are in the ILRTStaffPerson class, which is in turn populated by all those things that are a Person and which have a workplaceHomepage of http://www.ilrt.bris.ac.uk/. This is typical: FOAF groups are created by specifying a sub-class of Agent (in fact usually this will be a sub-class of Person), and giving criteria for which things fall in or out of the sub-class. For this, we use the owl:onProperty and owl:hasValue properties, indicating the property/value pairs which must be true of matching agents.\n\n```xml\n<!-- here we see a FOAF group described.\n     each foaf group may be associated with an OWL definition\n     specifying the class of agents that constitute the group's membership -->\n<foaf:Group>\n <foaf:name>ILRT staff</foaf:name>\n <foaf:membershipClass>\n    <owl:Class rdf:about=\"http://ilrt.example.com/groups#ILRTStaffPerson\">\n     <rdfs:subClassOf rdf:resource=\"http://xmlns.com/foaf/0.1/Person\"/>\n     <rdfs:subClassOf>\n       <owl:Restriction>\n         <owl:onProperty rdf:resource=\"http://xmlns.com/foaf/0.1/workplaceHomepage\"/>\n         <owl:hasValue rdf:resource=\"http://www.ilrt.bris.ac.uk/\"/>\n       </owl:Restriction>\n     </rdfs:subClassOf>\n   </owl:Class>\n </foaf:membershipClass>\n</foaf:Group>\n```\n\nNote that while these example OWL rules for being in the eg:ILRTStaffPerson class are based on a Person having a particular workplaceHomepage, this places no obligations on the authors of actual FOAF documents to include this information. If the information is included, then generic OWL tools may infer that some person is an eg:ILRTStaffPerson. To go the extra step and infer that some eg:ILRTStaffPerson is a member of the group whose name is \"ILRT staff\", tools will need some knowledge of the way FOAF deals with groups. In other words, generic OWL technology gets us most of the way, but the full Group machinery requires extra work for implimentors.\n\nThe current design names the relationship as pointing from the group, to the member. This is convenient when writing XML/RDF that encloses the members within markup that describes the group. Alternate representations of the same content are allowed in RDF, so you can write claims about the Person and the Group without having to nest either description inside the other. For (brief) example:\n\n```xml\n<foaf:Group>\n <foaf:member rdf:nodeID=\"martin\"/>\n <!-- more about the group here -->\n</foaf:Group>\n<foaf:Person rdf:nodeID=\"martin\">\n  <!-- more about martin here -->\n</foaf:Person>\n```\n\nThere is a FOAF issue tracker associated with this FOAF term. A design goal is to make the most of W3C's OWL language for representing group-membership criteria, while also making it easy to leverage existing groups and datasets available online (eg. buddylists, mailing list membership lists etc). Feedback on the current design is solicited! Should we consider using SPARQL queries instead, for example?\n\n### sub-class of\n\n`rdfs:subClassOf`\n\n- [Agent](#agent) `foaf:Agent`\n\n## member\n\n`foaf:member`\n\n```\nfoaf:member  rdf:type     rdf:Property ;\n        vs:term_status    \"stable\" ;\n        rdfs:label        \"member\" ;\n        rdfs:comment      \"Indicates a member of a Group\" ;\n        rdf:type          owl:ObjectProperty ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe member property relates a Group to a Agent that is a member of that group.\n\nSee Group for details and examples.\n\n### domain\n\n`rdfs:domain`\n\n- [Group](#group) `foaf:Group`\n\n### range\n\n`rdfs:range`\n\n- [Agent](#agent) `foaf:Agent`\n\n## Document\n\n`foaf:Document`\n\n```\nfoaf:Document  rdf:type      rdfs:Class ;\n        rdfs:label           \"Document\" ;\n        rdfs:comment         \"A document.\" ;\n        vs:term_status       \"stable\" ;\n        rdf:type             owl:Class ;\n        rdfs:isDefinedBy     foaf: .\n```\n\nThe Document class represents those things which are, broadly conceived, 'documents'.\n\nThe Image class is a sub-class of Document, since all images are documents.\n\n### equivalent class\n\n`owl:equivalentClass`\n\n- Creative Work `<http://schema.org/CreativeWork>`\n\n### disjoint with\n\n`owl:disjointWith`\n\n- [Organization](#organization) `foaf:Organization`\n- [Project](#project) `foaf:Project`\n\n## Image\n\n`foaf:Image`\n\n```\nfoaf:Image  rdf:type         rdfs:Class ;\n        vs:term_status       \"stable\" ;\n        rdfs:label           \"Image\" ;\n        rdfs:comment         \"An image.\" ;\n        rdf:type             owl:Class ;\n        rdfs:isDefinedBy  foaf: .\n```\n\nThe class Image is a sub-class of Document corresponding to those documents which are images.\n\nDigital images (such as JPEG, PNG, GIF bitmaps, SVG diagrams etc.) are examples of Image.\n\n### sub-class of\n\n`rdfs:subClassOf`\n\n- [Document](#document) `foaf:Document`\n\n### equivalent class\n\n`owl:equivalentClass`\n\n- Image Object `<http://schema.org/ImageObject>`"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations-%e2%86%92-principles-%e2%86%92-the-institution-of-socio-economic-values/solutions-to-fakenews-linked-data-ontologies-and-verifiable-claims/","title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims"},"frontmatter":{"draft":false},"rawBody":"---\nid: 610\ntitle: 'Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims'\ndate: '2018-10-04T10:21:31+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=610'\ninline_featured_image:\n    - '0'\n---\n\n[**Linked-Data**](https://www.webizen.net.au/what-is-linked-data/)<span style=\"font-weight: 400;\"> is a technology that produces machine and human readable information that is embedded in webpages. Linked-Data powers many of the online experiences we use today, with a vast array of the web made available in these machine-readable formats. The scope of linked-data use, even within the public sphere, is rather [enormous](http://lod-cloud.net/)</span><span style=\"font-weight: 400;\">.</span>\n\n<span style=\"font-weight: 400;\">Right now, most websites are using ‘linked data’ to ensure their news is being presented correctly on [Facebook](http://ogp.me/#)</span><span style=\"font-weight: 400;\"> and via search, which is primarily supported via </span>[<span style=\"font-weight: 400;\">Schema.org</span>](https://schema.org/docs/about.html) <span style=\"font-weight: 400;\">.</span>\n\n**The first problem is:**<span style=\"font-weight: 400;\"> that these ontologies do not support concepts such as [genres](https://en.wikipedia.org/wiki/Genre)</span><span style=\"font-weight: 400;\">. This means in-turn that rather than ‘news’ becoming [classified](https://en.wikipedia.org/wiki/List_of_genres)</span><span style=\"font-weight: 400;\">, as it would in any ordinary library or newspaper, the way in which ‘news’ is presented in a machine-readable format is particularly narrow and without (machine readable) context. </span>\n\n<span style=\"font-weight: 400;\">This means, in-turn, that the ability for content publishers to self-identify whether their article is an ‘advertorial’, ‘factual’, ‘satire’, ‘entertainment’ or other form of creative work – is not currently available in a machine-readable context. </span>\n\n<span style=\"font-weight: 400;\">This is kind of similar to the lack of ‘emotions’ provided by ‘social network silos’</span><span style=\"font-weight: 400;\"> to understand ‘sentiment analysis’ (EG: research links [1](http://people.sabanciuniv.edu/berrin/share/LDA/Stanford-NLP-Course-termproject-ssoriajr-kanej.pdf) and [2](https://blogs.msdn.microsoft.com/jennifer/2016/06/28/sentiment-analysis-on-social-network-data-twitter-facebook-etc/)) </span><span style=\"font-weight: 400;\"> through semantic tooling that offer means to profile environments</span><span style=\"font-weight: 400;\"> and offer tooling for organisations. Whilst Facebook offers the means to moderate particular words for its pages product</span><span style=\"font-weight: 400;\"> this functionality is not currently available to humans (account holders). </span>\n\n<span style=\"font-weight: 400;\">The mixture of a lack of available markup language for classifying posts, alongside the technical capabilities available to</span>*<span style=\"font-weight: 400;\"> ‘persona ficta’</span>*<span style=\"font-weight: 400;\"> in a manner that is not similarly available to Humans, contributes towards the lack of ‘human centric’ functionality these platforms currently exhibit. </span>\n\n**Bad Actors and Fact-Checking**\n\n<span style=\"font-weight: 400;\">In dealing with </span>**the second problem** <span style=\"font-weight: 400;\">(In association to the use of Linked-Data), the means in which to verify claims is available through the application of ‘credentials’</span><span style=\"font-weight: 400;\"> or [Verifiable Claims](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/)</span><span style=\"font-weight: 400;\"> which in-turn relates to the Open Badges Spec</span><span style=\"font-weight: 400;\">.</span>\n\n<span style=\"font-weight: 400;\">These solutions allow an actor to gain verification from 3rd parties to provide their audience greater confidence that the claims represented by their articles. Whether it is the means to “fact check” words, ensure images have not been ‘photoshopped</span><span style=\"font-weight: 400;\">’ or other ‘verification tasks’, one or more reputable sources could use verifiable claims to in-turn support end-user (reader / human) to gain confidence in what has been published. Pragmatically, this can either be done locally or via the web through 3rd parties through the use of Linked-Data. For more information, get involved in W3C</span><span style=\"font-weight: 400;\">, you’ll find almost every significant organisation involved with Web Technology debating how to build standard to define the web we want</span><span style=\"font-weight: 400;\">.</span>\n\n<span style=\"font-weight: 400;\">  \n</span><span style=\"font-weight: 400;\">General (re: Linked Data)</span>\n\n<span style=\"font-weight: 400;\">If you would like to review the machine-readable markup embedded in the web you enjoy today, one of the means to do so is via the [Openlink Data Sniffer](http://osds.openlinksw.com/)</span><span style=\"font-weight: 400;\"> An innovative concept for representing information was produced by Ted Nelson</span><span style=\"font-weight: 400;\"> via his Xanadu Concept</span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/En_2T7KH6RA?rel=0\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">Advancements in Computing Technology may make it difficult to trust media-sources</span><span style=\"font-weight: 400;\"> in an environment that seemingly has difficulty understanding the human-centric foundations to our world; and, where the issues highlighted by many, including Eben Moglen</span><span style=\"font-weight: 400;\">, continue to grow. Regardless of the technical means we have to analyse content (ie: [redlink demo](https://my.redlink.io/#/apps/DEMO/playground)) </span><span style=\"font-weight: 400;\">, it will always be important that we consider virtues such as [kindness](https://lists.w3.org/Archives/Public/public-schemaorg/2016Nov/0063.html)</span><span style=\"font-weight: 400;\">; and, it is important that those who represent us, in seeking solutions for [vulnerable people](https://lists.w3.org/Archives/Public/public-credentials/2016Feb/0069.html) us, put these sorts of issues</span><span style=\"font-weight: 400;\"> on the agenda in which “fake news” has become yet another example (or symptom) of a much broader problem (imho).</span>\n\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/aiFIu_z4dM8?rel=0\" width=\"560\"></iframe>\n\n<span style=\"font-weight: 400;\">A simple (additional) example of how a ‘graph database’ works as illustrated works such as this DbPedia related example (see: [visualdataweb.org](http://www.visualdataweb.org/) original example broken).</span><span style=\"font-weight: 400;\"> The production of “web 3.0”</span><span style=\"font-weight: 400;\"> is remarkably different (see: [startups to smartups](http://jeffsayre.com/2010/09/13/web-3-0-powering-startups-to-become-smartups/)) to former versions due to the volume of pre-existing web-users. Whilst studies have shown that humans are not really that different</span><span style=\"font-weight: 400;\">, the challenge becomes how to fund the development costs of works that are not commercially focused (ie: in the interests of </span>*<span style=\"font-weight: 400;\">‘persona ficta’</span>*<span style=\"font-weight: 400;\">) in the short-term, and to challenge issues such as ‘fake news’ or indeed also even, how to find a ‘Toilets’</span><span style=\"font-weight: 400;\">. </span>\n\n<figure aria-describedby=\"caption-attachment-613\" class=\"wp-caption aligncenter\" id=\"attachment_613\" style=\"width: 525px\">[![](https://www.webizen.net.au/wp-content/uploads/2018/10/Screen-Shot-2016-08-30-at-6.46.45-PM-1024x705.png)](https://www.webizen.net.au/about/applied-theory-applications-for-a-human-centric-web/solutions-to-fakenews-linked-data-ontologies-and-verifiable-claims/screen-shot-2016-08-30-at-6-46-45-pm/)<figcaption class=\"wp-caption-text\" id=\"caption-attachment-613\">Searching for a public Toilet (google search, 30 August 2016)</figcaption></figure><span style=\"font-weight: 400;\">As ‘human centric’ needs continue to be unsupported via the web or indeed also, the emerging [intelligent assistants](https://en.wikipedia.org/wiki/Intelligent_personal_assistant)</span><span style=\"font-weight: 400;\"> working upon the same datasets; the problem technologists have broadly produced becomes that of a world produced for things that ‘sell’, without support for things we value. Whether it be support for how to help vulnerable people, receipts that don’t fade (ie: not thermal, but rather machine-readable), civic services, the means to use data to uphold ‘rule of law’, vote and participate in civics or the array of other examples in which we have the technology, but not the accessible application in which to apply the use of our technology to social/human needs. </span>\n\n<span style=\"font-weight: 400;\">Indeed the works we produce and contribute on the web are for the most-part provided not simply freely, but at our own cost. The things that are ‘human’ are less important and indeed, poorly supported. </span>\n\n<span style=\"font-weight: 400;\">This is the bigger issue. We need to define means to distil the concept of ‘dignity’ on the web. Apps such as Facebook often have GPS history from our phones; does that mean the world should use that data to identify who broke into a house? If it is said you broke a speed limit in your vehicle when the GPS records show you were somewhere else, how should that help you? </span>\n\nnote; this article is based upon an earlier post re: [solutions to fake news.](https://docs.google.com/document/d/1OPghC4ra6QLhaHhW8QvPJRMKGEXT7KaZtG_7s5-UQrw/edit#)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","title":"HYPERMEDIA PACKAGES"},"frontmatter":{"draft":false},"rawBody":"---\nid: 576\ntitle: 'HYPERMEDIA PACKAGES'\ndate: '2018-10-02T16:53:13+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=576'\ninline_featured_image:\n    - '0'\n---\n\n<span style=\"font-weight: 400;\">I describe HyperMedia as a media package that incorporates linked-data, with audio, visual and HTML5 assets.</span>\n\n<span style=\"font-weight: 400;\">A HyperMedia Package incorporates these assets in a manner that supports an interactive and/or social experiences.</span>\n\n<span style=\"font-weight: 400;\">The technologies described below enable an array of new business models that improve Audience participation whilst supporting privacy by adapting the means in which data is stored as to support data-storage accounts where entities store and make available, subject to permissions, data, to specified parties. This paper applies these technologies to the utility of delivering solutions for Media specifically.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE"},"frontmatter":{"draft":false},"rawBody":"---\nid: 578\ntitle: 'USER STORIES: INTERACTIVE VIEWING EXPERIENCE'\ndate: '2018-10-02T16:54:34+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=578'\ninline_featured_image:\n    - '0'\n---\n\n#### **Socially Watching a Film**\n\n<span style=\"font-weight: 400;\">They search for a film they want to watch, and it identifies the film. Person selects the film (for instance, in this case, it’s in the channel 7 directory – in other cases, it might be with ABC, c31, etc.) The send the film to the TV, film starts. Midway through your partner or child sits to watch it with you. they want to know whether that actor was in some other film, they go to their FreeTV App, and it recognises your in the same place asking if you’re watching the same thing. press the button, say yes. then search available information about the people involved in the film (IE: IMDB) </span><span style=\"font-weight: 400;\">The person who did that search can then save that actor as someone they’re interested in. Film was never interrupted on TV.</span>\n\n##### **USE CASE: Watching a Cooking Show**\n\n<span style=\"font-weight: 400;\">Person searches for the particular meal they want to make that night for a dinner party. They search for ‘slow-cooked lamb shoulder’, and find a jamie oliver episode provided by a commercial FTA provider. They’ve got a list of the food they’ve purchased recently from their digital receipt information. The program has a TVC that outlines the end-product created by the food, and its decided – time to go shopping.</span>\n\n<span style=\"font-weight: 400;\">A button is pressed, and the information about ingredients is selected. The application has a field that asks who’s coming for dinner – the user selects the people coming for dinner, and their dietary requirements are checked for allergies and any food preferences. The app calculates portion sizes based upon the number of people who will be eating, and the shopping list is almost ready. They’re able to tick off the food that’s already in the kitchen, then add the remaining ingredients. A file is created on the user’s data-space that includes information about the application they used to create the meal, and a credential relating to the agent who supplied that application and on behalf of whom. </span>\n\n**FUNCTIONAL CONCEPT 1**\n\n- <span style=\"font-weight: 400;\">jamie and his distribution partners have a deal with a particular shopping network, so if it’s kosha, that’s ok, but otherwise there’s preferences about where to get it</span>\n- <span style=\"font-weight: 400;\">Jamie also has another deal, that’s not as good, with other outlets. </span>\n\n<span style=\"font-weight: 400;\">ie: The user opens their ‘coles app’, and makes the selection. the app also thinks the milk has gone off, and the user can select other things they need which may have been collated over time (ie: toilet paper low, ran out of dishwashing detergent, etc.) They can get the ingredients delivered or go shopping.</span>\n\n**FUNCTIONAL CONCEPT 2**\n\n- <span style=\"font-weight: 400;\">The recipe is Jamie’s Intellectual property. He cares about his food. When the user goes to a store that offer digital receipt functionality to data-spaces, if that purchase relates specifically to the production of the meal, fulfillment of ingredients to make it, the shop has in their system an attribution method that allows a percentage of valid food products to goto jamie.</span>\n\n<span style=\"font-weight: 400;\">Ingredients have been purchased and the user can watch the TV or flick through the cooking guide whilst preparing the dish. This can happen on their device, on their tv, with both, etc.</span>\n\n<span style=\"font-weight: 400;\">The ‘interactive content package’ enables this entire experience to be programmed, packaged and distributed globally. differentiators between markets / regions, can be managed by agents, distributors and local partners without needing to change the format of the content package.</span>\n\n##### **USE CASE: Buying a Motor Vehicle**\n\n<span style=\"font-weight: 400;\">Joe wants to purchase a new vehicle. He really wants a 2004 XC90, thinking that it suits his needs and represents good value. He wants to watch a review, searches for one – finds an old TVC made in britain, made available locally through the a local program that’s been syndicated by a commercial broadcaster who has purchased the media as part of their on-demand offerings. </span>\n\n<span style=\"font-weight: 400;\">The presentation page has a presales button, which the user selects on his phone. He can see how many were sold in the market, average cost of insurance, average KM’s, average price based on condition and how many are available for sale. He watches the video and plans to go have a look at a few. He finds a dealer who is offering a vehicle he likes and speaks to their sales agent Frank. Joe asks the Frank to Take the Vehicle for a Test-Drive. </span>\n\n<span style=\"font-weight: 400;\">Frank would like to know whether Joe has the capacity to purchase the vehicle or whether, Joe’s simply interested in going for a test-drive with no-capacity to make a transaction. Frank has a family and it’s important he spends his time on sales opportunities. Frank does not have alot of time to waste on ‘tyre kickers’. Joe has a look at the information about the car on his phone that his obtained when he found the vehicle. </span>\n\n<span style=\"font-weight: 400;\">The car has been in the lot for too long, and Frank thinks this is a ‘hot lead’ but wants to qualify the opportunity. He asks to share some more details and if the information provided stacks up, then joe should borrow the car. Joe presses a button on his phone, and Frank gets a ‘green light’ indicator that shows that joe has a license and has the financial capacity to purchase the car. </span>\n\n<span style=\"font-weight: 400;\">— &gt; Whilst it is none of Franks business; Joe has a linked-credential that denotes the intention of his parents to purchase a vehicle for him to a particular value, which in-turn contributed towards getting the ‘green light’ he needed, before going to find a car he likes.</span>\n\n<span style=\"font-weight: 400;\">Frank is happy to provide Joe access to the car, to take for a test-drive. Frank and Joe issues credentials for the purpose of the test-drive, that support insuring the Joe in case he has an accident; whilst also supporting Frank, in case Joe doesn’t come back with the car.</span>\n\n<span style=\"font-weight: 400;\">Joe takes the car for a test-drive and notices that there are some mechanical issues with the car. He enters the information in his record that relates to the vehicle, and the application on his phone provides an estimation of the cost to fix the problem in addition to any information about whether by law, Frank needs to fix that problem before he sells it to Joe. </span>\n\n<span style=\"font-weight: 400;\">Joe Returns. Frank and Joe talk about the price, which results in Frank finishing the sales-opportunity by issuing Joe an Offer that is attached to the record stored in relation to his phone application. Joe informs frank he’ll be back, frank limits the offer to a few days hoping to close a deal before the time his commissions need to be finalized for the month; Joe goes to have a look at other vehicles.</span>\n\n<span style=\"font-weight: 400;\">If the sale goes through, then the lead was generated by the program on TV and the experience provided by that program. It is possible that they’re then able to ‘clip the ticket’, which may result in an improvement around their advertising on TV, a direct cash-payment, or other means.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","title":"credentials and custodianship"},"frontmatter":{"draft":false},"rawBody":"---\nid: 556\ntitle: 'credentials and custodianship'\ndate: '2018-10-02T16:34:19+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=556'\ninline_featured_image:\n    - '0'\n---\n\n<span style=\"font-weight: 400;\">Historically, systems of government and economy have forged means through which, by way of financial instruments, exchange is made possible by terms in which may furnish protections to participants, by law.</span>\n\n<span style=\"font-weight: 400;\">The instruments used as an embodiment of value included different forms of stamp (such as a [postmark](https://en.wikipedia.org/wiki/Postmark#History)), the practice of minting tokens (such as [currency](https://en.wikipedia.org/wiki/Mint_(facility))), that may or may not be worth their weight in gold; and various other forms of controlled ‘tokens’, ‘creative embodiments’ such as ‘certificates’, passports and other more modern forms of legal instrument such as electronic cards and licenses. These legal instruments are most-often owned by the issuer of the credential; who provides it to the holder of the credential whilst maintaining the ability to cancel it and/or protect the instrument from misuse. </span>\n\n<span style=\"font-weight: 400;\">In this way, the definition of personhood is instrumentally provided authoritative notations that form relations between self and external groups whose participatory governance role, provides assurance, and the means to maintain a form of insurance, as is sought by others, in relation to their fiduciary responsibilities pertaining to self and any role of [agent](https://en.wikipedia.org/wiki/Law_of_agency) they may be facilitating at a time of trade.</span>\n\n<span style=\"font-weight: 400;\">Internet has brought about circumstances where former means, have been outmoded. ICT systems have been adapted to make use of our international systems of knowledge economy; whilst still failing to render meaningfully responsive service, to an array of unmet challenges.</span>\n\n<span style=\"font-weight: 400;\">Some of these challenges include the means in which the notation of human reputation, contribution and socioeconomic participation is made; in systems of international ‘high-frequency’ trade. Where new instruments that render meaningful service to a ‘beneficial owner’, become moreover attributable to ‘surveillance’ and ‘attention’ related information exchanges by way of their agents, with few otherwise technically available options, having been explored or made easily available. </span>\n\n<span style=\"font-weight: 400;\">Today, many social network service (SNS) platform providers have a persistent electronic relationship with users worldwide; whilst there continues to be a built-upon *fallacy,* that the exchange of providing access to the predetermined and asymmetrically defined, universal environment; without financial charge (other than purchase of equipment and the on-going payment for subscribing to make use of the environment) is the only alternative in an environment that is now inextricably depended upon, for socioeconomic participation. Whilst these systems make use of data to benefit their commercial purpose, it remains the case that a significant variety of alternative use-cases are left without support.</span>\n\n<span style=\"font-weight: 400;\">Complicating matters further; As the means in which the information orchestration corpus is now operated; necessarily providing any and all knowledge to be transmitted, stored and made use of by them; artificial agents, owned and operated by them, designed to serve their needs, influence how it all works.</span>\n\n<span style=\"font-weight: 400;\">This situation is in-turn built upon, the implicit (rather than explicit) promissory ‘message’ that their environment – is indeed ‘fit for purpose’ for the beneficial use of personhood as to seek means in which to be made independently free, from all encumbrances be otherwise entitled to be attributed. In so doing, as we battle with issues such as fake news and ‘autocorrect’ issues (but not exclusive to it), our online systems furnish us with the right to define by way of express instruction; the means in which, ‘for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries’.</span>\n\n<span style=\"font-weight: 400;\">Technologically, it is the case, that the capabilities of the systems used and depended upon; are more capable of rendering a definition of what any such ‘discovery’ may be considered to have been beneficially ‘invented’ prior to any such human actor being made capable to undertake the necessary process as to complete their writings in independence for the purpose of seeking claim for any-such right. </span>\n\n<span style=\"font-weight: 400;\">Making matters more complicated; is the implication of legal means in which these ‘services’ are made useful by context to systems of law and diplomacy and inferred, lesser expressly considerations as they pertain to geopolitical stability and means in which to foster the works first documented by the means of international works undertaken in relation to the united nations, and the Organisation for Economic Co-operation and Development (“OECD”).</span>\n\n<span style=\"font-weight: 400;\">Through the use of credentials the means in which to associate group attributions as to make useful personhood; is now therefore performed. The issues of whether and/or how, ‘autocorrect’ or other artificial intelligence related services influenced communications and available ‘knowledge’, is being inextricably shaped by the operating characteristics of these systems. As it is the case, that autocorrect issues; and the practical process of fixing them, assists the operator to improve their artificial intelligence service; and that moreover, the principle driver of revenue is advertising as obtained through views and clicks, our world is being shaped through these narrow lens, world-wide; and the issue of ‘credentials’ or ‘verifiable claims’ has never, consequentially, been more important..</span>\n\n##### Verifiable claims and Economics\n\n<span style=\"font-weight: 400;\">In our knowledge based economy the means in qualification or endorsement as to serve the interests of group/s, of various forms; makes use of “official” documents, keys or insignia in which participants in groups, furnish individuals with ‘instruments’, designed to denote a skill, competency, privileged and/or membership, as required for privileged participatory rights in some way shape or form. Similarly ‘proofs’ be by so means provided a means to warrant that a particular creative work is considered to not have formerly been embodied. [Credentials](https://www.webizen.net.au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/) form a means in which to present a claim, that is considered to be verified by a relevant authority who shares custodianship over the use of that claim. </span>\n\n<span style=\"font-weight: 400;\">Yet when considering the nuanced characteristics in which ‘electronic commons’ for the social benefits, as formerly facilitated in many ways, such as libraries, language and the irrelevance of extending these concept to a new electronic domain, the means to author, review, maintain, archive and make accessible; both old and new forms of commons, is needed to ensure access to articles of the natural world is both free and a shared responsibility. </span>\n\n<span style=\"font-weight: 400;\">In the production of these tools, services and bodies of work, that furnishes the means in which humanity is made able to learn; the practice of transferring notations into our infosphere is now also used for the contextual purpose of making available persistent accessibility to agents. </span>\n\n<span style=\"font-weight: 400;\">The context of which ‘electronic commons’ extends the ‘knowledge’ of microscopic and macroscopic natural world as a series of electronic embodiments of objects to universally make use of; As such, civics custodianship frameworks are required to address considerations as to the manner in which the underlying artefact of our natural world, be maintained and made accessible to all. </span>\n\n<span style=\"font-weight: 400;\">Through the use of ledger technology, that makes use of linked-data, the means to notate an array of disparate data-sources within a singular document provides a method to inter-lay information sources; that may include but not be limited to, sources made available by commercial means. </span>\n\nLedger technology is also assistive, in decentralising the governance and discovery of electronic documents and the knowledge embodied by them.\n\n<span style=\"font-weight: 400;\">In so-doing, a requirement that is brought about, where any commercial actor to both assist in the free propagation of natural world knowledge accessibility, can be responded to; as to simply notate the natural world, rather than seeking necessarily to take sole and/or proprietary custodianship of its accessibility / discoverability in the broader environment.</span>\n\n<span style=\"font-weight: 400;\">Through the use of various forms of decentralised ledger technology, the manner in which the discovery and informatics storage methods be considered to be rendered support for these elements pertaining to the humanitarian and artificial realms; is by way of ledger technology, whilst preferentially considerate of linked-data based decentralised ledger technologies such as IPFS,</span><span style=\"font-weight: 400;\"> IPLD, Web Torrent alongside others</span><span style=\"font-weight: 400;\">. </span>\n\n<span style=\"font-weight: 400;\">Accessibility to these ledgers is made possible, in-effect, by way of credentials. The credentials interact with the knowledge-source as to permissively assert relations in connection to the supply of information that pertains to the specified form of knowledge that source is maintaining. Inforgs are also expected to support the means in which to supply high-quality statistical information alongside an array of other new forms of data that can be supplied in a ‘privacy preserving’ manner through the use of federated services.</span>"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","title":"DIDs and MultiSig"},"frontmatter":{"draft":false},"rawBody":"---\nid: 619\ntitle: 'DIDs and MultiSig'\ndate: '2018-10-04T11:36:31+10:00'\nauthor: ubiquitous\nlayout: page\nguid: 'https://www.webizen.net.au/?page_id=619'\ninline_featured_image:\n    - '0'\n---\n\nAs previously considered; there are an array of [integrally important ‘commons’](https://www.webizen.net.au/about/references/the-need-for-decentralised-open-linked-data/) and group activity works and resources that have an array of stakeholder tenancy environments that are in-turn bound to the [dignity vs. privacy](https://www.webizen.net.au/about/references/privacy-vs-dignity/) related issues.\n\nIt is technically sought, that the operation of a persons online systems may be carried out permissively and privately (save rule of law related needs). This is an important ICT construct requirement to support human dignity; alongside others, such as the importance of ensuring means for socio-economic participation, the ability to ensure systems are built upon a framework where humanity is the beneficiary of works made by us; alongside other considerations made by documents such as the [UDHR](http://www.un.org/en/documents/udhr/).\n\nThe means to provide an alternative framework to technically respond to these issues is entirely shaped by whether or not a ‘[knowledge banking industry](https://www.webizen.net.au/about/the-modernisation-of-socioeconomics/)‘ is made able to emerge. If information systems do not provide a means for persons to store and operate their own information systems environment in a manner rendered meaningful support by law; any solution built around alternative methods (based upon existing knowledge equity frameworks) become foundational in what solutions can be offered; and what is not able to be provided, due to these socioeconomically applied constraints.\n\nThe most meaningful constituent paradigm relating to the use of ledger technologies (or ‘blockchains’ as is most-often the commonly known, yet often misused term) and decentralised identifiers (that are not necessarily, [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) based.) becomes a circumstance of where it is the private keys are stored for access and meaningful use of these alternative technologies.\n\nLedger technologies all make use of cryptographic methods to make available, the means to ‘write’ to the ledger. This most often uses [Public Key cryptography](https://en.wikipedia.org/wiki/Public-key_cryptography), which in-turn requires the private key, to be stored somewhere.\n\nThe relationship between the beneficial owner of an account that is publicly presented as a public key; is often solely based upon their means to privately retain exclusive use over the private key. Algorithms can generate them, as is exampled on the [all private key](http://www.allprivatekeys.com/) website for bitcoin (in an unsophisticated manner) and beyond that, the means to keep them means the locations their stored in; need to retain the means for the owner of them, to use them.\n\nThis is more difficult than it sounds, particularly for non-technical users.\n\nSo the first part of the puzzle; is about storing the private keys somewhere that is supported to be owned by beneficial owner of those software tools. It is considered that an institutional ‘knowledge banking system’ can help with this. The next issue, is the amount of power consumed by some of the block-chain methods; which i’ll cover in a different post.\n\nInstrumentally; what is ‘multi-sig’ or [multi-signatures](https://en.bitcoin.it/wiki/Multisignature). Multi-Sig is the means to have multiple keys that provide the means to ‘unlock’ stuff. Through the use of Multi-Sig the means to ‘write’ to a ledger can be controlled by a group of keys and a subset of that group, who need to cooperatively use their key in-order to support write access. An example of how this done can be found [here.](https://coinb.in/multisig/)\n\nThe implications of this include; the means to assign a private key publicly to an object in the world, such as a coffee cup or parcel; via something like a [QR](https://en.wikipedia.org/wiki/QR_code) code or [NFC](https://en.wikipedia.org/wiki/Near-field_communication) chip, and make use of it in relation to others (via semantics) whose private-keys are otherwise kept private. So, what and why are ledgers important and what does any of this have to do with DIDs?\n\nwell. The importance of [DIDs](https://w3c-ccg.github.io/did-spec/) is the ability to build a bridge between traditional HTTP based URI services, and URI services (supporting linked-data) natively operated on protocols other than HTTP. This means in-turn, there is an ability to produce, preserve and support semantics using linked-data in relation to ‘groups’ and work-flows involving multiple participants. Additionally, and perhaps more importantly.\n\nAs reflected elsewhere; it is critical that the discovery of ‘commons assets’ is decentralised. A relatively easy way to think about this, is to consider the meaningful benefit [wikipedia](https://en.wikipedia.org/wiki/Wikipedia:About) provides. Similarly, [archive.org](https://archive.org/) is another critical infrastructure constituent to the web; yet both of these organisations operate, in-turn, a form of centralised custodianship model; that does (in the case of wikimedia products in particular) support identification of agents in relation to edits; but is otherwise operated globally via a single group.\n\nThe means to decentralise how this works and how the underlying content is made discoverable (even in a semi-private environment) is brought about through the use of cryptography, ledger technologies, DIDs and related apparatus. Therein; whilst the rules, for how these technologies are applied to various considerations; the mainstay of how this can be brought about is in-turn made possible by making use of DIDs and Multi-Sig structures.\n\nHumans are different, they require inforgs; which in-turn, needs some sort of institutional framework designed to support them, which i call, a knowledge banking industry.\n\nThe reason why this relates to ‘credentials’, is that the contributions made to all forms of different fields, in a decentralised data-storage structure (that in-turn, supports provenance related structural support frameworks); is in-turn still permissively applied in relation to a persons ‘inforg’; and in-turn, the infrastructure requirements brought about (and able to be supported) by ‘knowledge banking providers’ that in-turn improve the reliability of ICT."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","title":"Notes on Suffix Array and Manacher Algorithm"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: algorithm_and_data_structure\ntitle: Notes on Suffix Array and Manacher Algorithm\ndate: 2015-08-27\n---\n\nRecently I am working on some data structure questions, and I kind of\ndive into the \"Longest Palindromic Substring\" problem, master(do I?) the Suffix\nArray and Manacher algorithm. I found that they are impressively amazing and have something in common.\n\n**What is Suffix Array?**\n\nA suffix array is a sorted array of all suffixes of a string. For example, \"abracadabra\":\n\n$$\n\\\\newcommand\\\\T{\\\\Rule{0pt}{1em}{.3em}}\n\\\\begin{array}{|c|c|c|}\n\\\\hline\n  \\\\text{i}  & \\\\text{sa[i]} & \\\\text{S[sa[i]...]}    \\\\T \\\\\\\\\\\\hline\n  \\\\text{0}  & \\\\text{11}    & \\\\text{(empty string)} \\\\\\\\\\\\hline\n  \\\\text{1}  & \\\\text{10}    & \\\\text{a}              \\\\\\\\\\\\hline\n  \\\\text{2}  & \\\\text{7}     & \\\\text{abra}           \\\\\\\\\\\\hline\n  \\\\text{3}  & \\\\text{0}     & \\\\text{abracadabra}    \\\\\\\\\\\\hline\n  \\\\text{4}  & \\\\text{3}     & \\\\text{acadabra}       \\\\\\\\\\\\hline\n  \\\\text{5}  & \\\\text{5}     & \\\\text{adabra}         \\\\\\\\\\\\hline\n  \\\\text{6}  & \\\\text{8}     & \\\\text{bra}            \\\\\\\\\\\\hline\n  \\\\text{7}  & \\\\text{1}     & \\\\text{bracadabra}     \\\\\\\\\\\\hline\n  \\\\text{8}  & \\\\text{4}     & \\\\text{cadabra}        \\\\\\\\\\\\hline\n  \\\\text{9}  & \\\\text{6}     & \\\\text{dabra}          \\\\\\\\\\\\hline\n  \\\\text{10} & \\\\text{9}     & \\\\text{ra}             \\\\\\\\\\\\hline\n  \\\\text{11} & \\\\text{2}     & \\\\text{racadabra}      \\\\\\\\\\\\hline\n\\\\end{array}\n$$\n\n**How to compute suffix array efficiently?**\n\nManber and Myers invented a algorithm with runtime complexity \\\\(O(n log^{2} n)\\\\). The basic idea is *doubling*. Start with the character at each location, we first compute the orders of substrings with length 2 at each location, then use the results to compute the orders of substrings with length 4.  The assessed prefix length doubles in each iteration of the algorithm until a prefix is unique (or length >= n) and provides the rank of the associated suffix.\n\n**What is Manacher Algorithm?**\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","title":"Notes On Perceptrons"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Notes On Perceptrons\ndate: 2015-10-07\n---\n\nLeon Bottou recently gave a interesing talk about perceptrons: *\"Perceptrons Revisited\"* (you can check out this: [http://vdisk.weibo.com/s/tQWXdywc_B7L](http://vdisk.weibo.com/s/tQWXdywc_B7L)). Here are some my study notes.\n\n# What is *bayes error*?\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","title":"Notes On Object Detection"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Notes On Object Detection\ndate: 2015-11-04\n---\n\nKey points on object detection: (1) Good region proposal generation method; (2) Early rejection.\n\n\n\n\n**Make py-faster-rcnn support cudnn-v5**\n\n```\ngit clone https://github.com/rbgirshick/py-faster-rcnn\n```\n\nMerge to latest Caffe:\n\n```\ncd caffe-fast-rcnn\ngit remote add caffe https://github.com/BVLC/caffe.git\ngit fetch caffe\ngit merge -X theirs caffe/master\n```\n\nThen uncomment code line in include/caffe/layers/python_layer.hpp:\n\n```\nself_.attr(\"phase\") = static_cast<int>(this->phase_);\n```\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","title":"Notes On Caffe Development"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Notes On Caffe Development\ndate: 2015-11-10\n---\n\n# Installation\n\n**Build Caffe in Windows with Visual Studio 2013 + CUDA 6.5 + OpenCV 2.4.9**\n\n[https://initialneil.wordpress.com/2015/01/11/build-caffe-in-windows-with-visual-studio-2013-cuda-6-5-opencv-2-4-9/](https://initialneil.wordpress.com/2015/01/11/build-caffe-in-windows-with-visual-studio-2013-cuda-6-5-opencv-2-4-9/)\n\n**[2015-04-01] Build Caffe with Visual Studio 2013 on Windows 7 x64 using Cuda 7.0**\n\n[https://mthust.github.io/2015/04/Build-Caffe-with-Visual-Studio-2013-on-Windows-7-x64-using-Cuda-7.0/](https://mthust.github.io/2015/04/Build-Caffe-with-Visual-Studio-2013-on-Windows-7-x64-using-Cuda-7.0/)\n\n**OpenCV Installation in Windows**\n\n[http://docs.opencv.org/doc/tutorials/introduction/windows_install/windows_install.html](http://docs.opencv.org/doc/tutorials/introduction/windows_install/windows_install.html)\n\n# HDF5 vs. LMDB\n\nHDF5:\n\nSimple format to read/write.\n\nThe HDF5 files are always read entirely into memory, so can't have any HDF5 files exceed memory capacity. But can easily split data into several HDF5 files though (just put several paths to h5 files in text file). Caffe dev group suggests that divide original large h5 data into small ones so that each h5 data fits < 2 GB.\n\nI/O performance won't be nearly as good as LMDB.\n\nOther DataLayers do prefetching in a separate thread, HDF5Layer does not.\n\nCan only store float32 and float64 data - no uint8 means image data will be huge.\n\nLMDB:\n\nUses memory-mapped files, giving much better I/O performance. Works well with large dataset.\n\nExample:\n\n{% highlight bash %}\nlayer {\n  type: \"HDF5Data\"\n  top: \"X\" # same name as given in create_dataset!\n  top: \"y\"\n  hdf5_data_param {\n    source: \"train_h5_list.txt\" # do not give the h5 files directly, but the list.\n    batch_size: 32\n  }\n  include { phase:TRAIN }\n}\n{% endhighlight %}\n\n# Create LMDB from float data\n\n[http://stackoverflow.com/questions/31774953/test-labels-for-regression-caffe-float-not-allowed](http://stackoverflow.com/questions/31774953/test-labels-for-regression-caffe-float-not-allowed)\n\n[https://github.com/BVLC/caffe/issues/1698#issuecomment-70211045](https://github.com/BVLC/caffe/issues/1698#issuecomment-70211045)\n\n**Reference**\n\n[http://vision.stanford.edu/teaching/cs231n/slides/caffe_tutorial.pdf](http://vision.stanford.edu/teaching/cs231n/slides/caffe_tutorial.pdf)\n\n[http://deepdish.io/](http://deepdish.io/)\n\n[https://github.com/BVLC/caffe/issues/1470](https://github.com/BVLC/caffe/issues/1470)\n\n# \"lr_policy\" in Caffe\n\nfixed: the learning rate is keped fixed throughout the learning process.\n\ninv: the learning rate is decaying as ~1/T\n\n<img src=\"/assets/dl-materials/notes-on-caffe-dev/inv.png\" />\n\nstep: the learning rate is piece-wise constant, dropping every X iterations\n\n<img src=\"/assets/dl-materials/notes-on-caffe-dev/step.png\" />\n\nmultistep: piece-wise constant at arbitrary intervals\n\n<img src=\"/assets/dl-materials/notes-on-caffe-dev/multistep.png\" />\n\n**Reference**\n\n[http://stackoverflow.com/questions/30033096/what-is-lr-policy-in-caffe](http://stackoverflow.com/questions/30033096/what-is-lr-policy-in-caffe)\n\n# Iteration loss vs. Train Net loss\n\nThe `net output #k` result is the output of the net for that particular iteration / batch \nwhile the `Iteration T, loss = X` output is smoothed across iterations according to the `average_loss` field.\n\n# Enable `import caffe` of python file:\n\nAdd following line to .bashrc file:\n\n```\nexport PYTHONPATH=/path/to/caffe/python:$PYTHONPATH\n```\n\n# Extract CNN Features\n\n```\ndef __tranform_img(self, net, img, mean_data=None):\n    transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n    transformer.set_transpose('data', (2, 0, 1))\n    if (mean_data is not None):\n        transformer.set_mean('data', mean_data)\n\n    #transformer.set_raw_scale('data', 255)\n    transformer.set_channel_swap('data', (2, 1, 0))\n    return transformer.preprocess('data', img)\n\n# e.g,  GoogleNet pool5/7x7_s1\nimg = caffe.io.load_image(input_image_file)\ntrans_img = __tranform_img(net, img)\nnet.blobs['data'].data[...] = trans_img\nnet.forward()\nprob = np.squeeze(net.blobs['prob_main'].data)\nfeatures = np.squeeze(net.blobs['pool5/7x7_s1'].data)\n```\n\nref: [https://prateekvjoshi.com/2016/04/26/how-to-extract-feature-vectors-from-deep-neural-networks-in-python-caffe/](https://prateekvjoshi.com/2016/04/26/how-to-extract-feature-vectors-from-deep-neural-networks-in-python-caffe/)\n\n# Build/Make Errors\n\n1. fatal error: pyconfig.h: No such file or directory\n\nSolution:\n\n```\nexport CPLUS_INCLUDE_PATH=/usr/include/python2.7\n```\n\n# Runtime Warnings/Errors\n\n```\n/path/to/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for std::vector<boost::shared_ptr<caffe::Net<float> >, std::allocator<boost::shared_ptr<caffe::Net<float> > > > already registered; second conversion method ignored.\n```\n\nSolution:\n\n\n# Reading and Notes\n\n**DIY Deep Learning for Vision: A Tutorial With Caffe 报告笔记**\n\n[http://frank19900731.github.io/wx/2014-12-04-diy-deep-learning-for-vision-a-tutorial-with-caffe-bao-gao-bi-ji.html](http://frank19900731.github.io/wx/2014-12-04-diy-deep-learning-for-vision-a-tutorial-with-caffe-bao-gao-bi-ji.html)\n\n**Caffe_Manual(by Shicai Yang(@星空下的巫师))**\n\n[https://github.com/shicai/Caffe_Manual](https://github.com/shicai/Caffe_Manual)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","title":"Notes On L-BFGS"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Notes On L-BFGS\ndate: 2015-12-07\n---\n\n# Reference\n\n**Numerical Optimization (by O. Jorge Nocedal. Stephen J. Wright. 2006)**\n\n[http://home.agh.edu.pl/~pba/pdfdoc/Numerical_Optimization.pdf](http://home.agh.edu.pl/~pba/pdfdoc/Numerical_Optimization.pdf)\n\n**On Optimization Methods for Deep Learning**\n\n[http://www.icml-2011.org/papers/210_icmlpaper.pdf](http://www.icml-2011.org/papers/210_icmlpaper.pdf)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","title":"Softmax Vs Logistic Vs Sigmoid"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Softmax Vs Logistic Vs Sigmoid\ndate: 2015-12-10\n---\n\n# Softmax\n\n$$\n\\sigma(x) = \\frac{1} {1 + exp^{-x}}\n$$\n\n$$\n\\frac{d\\sigma(x)} {dx} = (1 - \\sigma(x)) * \\sigma(x)\n$$\n\n# Logistic\n\n# Sigmoid\n\n$$\ny_i = \\frac{e^{\\zeta_i}} {\\sum_{j \\in L} e^{\\zeta_j}}\n$$"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","title":"Notes On Deep Learning Training"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Notes On Deep Learning Training\ndate: 2015-12-12\n---\n\nThese notes come from a deep learning discussion group.\n\n# Convolution VS. Correlation\n\nIt comes from one basic fact that although we call CNN *Convolutional* Nerual Network, \nactually it is *Correlational* Nerual Network for most famous deep learning frameworks implement *Convolution* as *Correlation*.\n(except Theano, which does use Convolution to implement convolution layer). \n\nBengio in his book *Deep Learning* has pointed out this:\n\nmany neural network libraries implement arelated function called the cross-correlation, \nwhich is the same as convolution but without ﬂipping the kernel:\n\n(From [http://www.deeplearningbook.org/contents/convnets.html](http://www.deeplearningbook.org/contents/convnets.html),\nChapter 9. Convolutional Networks)\n\n# About activation function\n\nBatch Normalization and PReLU cannot work appropriately. (why?)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","title":"Notes On YOLO"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Notes On YOLO\ndate: 2015-12-14\n---\n\n# How does YOLO organise training data?\n\n**ground-truth:**\n\n49 x (1 + 20 + 4) =>  <br />\n  49 x (1 x obj_gt + 20 x classes_gt + 4 x box_gt)\n\n**predict-data:**\n\n49 x 20 + 49x(1x2) + 49x(4x2) =>  <br /> \n  49 x (20 x classes) + 49 x (2 x obj_confidence) + 49 x (2 x predict_boxes)\n  \n# Some questions..\n\n(1) The multi-part loss function differ from the code implementation:\n\n$$\n\\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{S} \\mathcal{1}_{ij}^{obj} (x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 \n$$\n\n$$\n+ \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{S} \\mathcal{1}_{ij}^{obj} (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2\n$$\n\nwhile in forward_detection_layer(), detection_layer.c, different loss calculation:\n\n{% highlight cpp %}\n*(l.cost) += pow(1-iou, 2);\n{% endhighlight %}\n\n- - -\n\n$$\n+ \\sum_{i=0}^{S^2} \\sum_{j=0}^{S} \\mathcal{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 \n$$\n\n{% highlight cpp %}\n*(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2);\n*(l.cost) += l.object_scale * pow(1-l.output[p_index], 2);\n{% endhighlight %}\n\n- - -\n\n$$\n+ \\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^{S} \\mathcal{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 \n$$\n\n{% highlight cpp %}\n*(l.cost) += l.noobject_scale*pow(l.output[p_index], 2);\n{% endhighlight %}\n\n- - -\n\n$$\n+ \\sum_{i=0}^{S^2} \\mathcal{1}_{i}^{obj} \\sum_{c \\in classes} (p_i(c) - \\hat{p}_i(c))^2 \n$$\n\n{% highlight cpp %}\n*(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2);\n{% endhighlight %}"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","title":"Notes On Inside-Outside Net"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Notes On Inside-Outside Net\ndate: 2015-12-28\n---\n\nThis paper explores expanding the approach of Fast R-CNN to include two additional sources of information:\n\n1) Multi-scale representaiton\n\n2) Contextual information "},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","title":"Notes On K-Means"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Notes On K-Means\ndate: 2016-01-06\n---\n\n**K-Means Clustering in OpenCV**\n\n[http://docs.opencv.org/3.0-beta/modules/core/doc/clustering.html](http://docs.opencv.org/3.0-beta/modules/core/doc/clustering.html)\n\n**K-Means 算法**\n\n- blog: [http://coolshell.cn/articles/7779.html](http://coolshell.cn/articles/7779.html)\n\n**Unsupervised learning or Clustering – K-means Gaussian mixture models**\n\n- slides: [http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/clustering.pdf](http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/clustering.pdf)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","title":"Notes On Quantized Convolutional Neural Networks"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Notes On Quantized Convolutional Neural Networks\ndate: 2016-01-07\n---\n\nExisting works:\n\n(1) Speed-up convolutional layers: **Low-rank approximation** and **Tensor decomposition**\n\n(2) Reduce storage consumption in fully-connected layers: **Parameter compression**\n\nDrawback: hard to achieve significant acceleration and compression simultaneously for the whole network."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","title":"Notes On Discrete Optimization"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_science\ntitle: Notes On Discrete Optimization\ndate: 2015-09-06\n---\n\nThis page is to record my study notes on a Cousera course *Discrete Optimization*. \nYou can check out on this: [https://www.coursera.org/course/optimization](https://www.coursera.org/course/optimization).\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","title":"Notes On Gecode"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_science\ntitle: Notes On Gecode\ndate: 2015-09-06\n---\n\n# Gecode\n\n- intro: \"Gecode is a toolkit for developing constraint-based systems and applications. \nGecode provides a constraint solver with state-of-the-art performance while being modular and extensible.\"\n- homepage: [http://www.gecode.org/](http://www.gecode.org/)\n- doc(\"Modeling and Programming with Gecode\"): [http://www.gecode.org/doc-latest/MPG.pdf](http://www.gecode.org/doc-latest/MPG.pdf)\n- github: [https://github.com/ampl/gecode](https://github.com/ampl/gecode)\n- slides(\"Tutorial on Gecode Constraint Programming\"): [http://www.cs.upc.edu/~erodri/webpage/cps/lab/cp/tutorial-gecode-slides/slides.pdf](http://www.cs.upc.edu/~erodri/webpage/cps/lab/cp/tutorial-gecode-slides/slides.pdf)\n- slides(\"Lecture 4: CP Solvers Gecode\"): [http://www.imada.sdu.dk/~marco/DM826/Slides/dm826-lec4.pdf](http://www.imada.sdu.dk/~marco/DM826/Slides/dm826-lec4.pdf)\n- blog(\"My Gecode page\"): [http://www.hakank.org/gecode/](http://www.hakank.org/gecode/)\n\n# Send More Money / Send Most Money\n\n**DM841 Discrete Optimization, Part II: Lecture 2 Example Gecode**\n\n- slides: [http://www.imada.sdu.dk/~marco/Teaching/AY2014-2015/DM841/Slides/dm841-p2-lec2.pdf](http://www.imada.sdu.dk/~marco/Teaching/AY2014-2015/DM841/Slides/dm841-p2-lec2.pdf)\n\n**hakank's code**\n\n[http://www.hakank.org/gecode/send_more_money_any_base.cpp](http://www.hakank.org/gecode/send_more_money_any_base.cpp)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","title":"Notes On Stanford CS2321n"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_vision\ntitle: Notes On Stanford CS2321n\ndate: 2016-02-21\n---\n\n# Lecture 3: Loss Functions and Optimization\n\nSVM Loss function\n\nSoftmax Classifier = Multinomial Logistic Regression\n\nAndrej says that \"usually they are end up working about the same\""},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon"},"frontmatter":{"draft":false},"rawBody":"\n# Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon\n\n4.4 Model Combination for detection\n\nThey pre-train two networks in ImageNet, using the same structure but different random initializations.\nThese two mAPs are similar: 59.1% vs. 59.2%.\nGiven the two models, first use either model to score all candidate windows on the test image.\nThen  perform NMS on the union of the two sets of candidate windows (with their scores). \nA more confident window given by one method can suppress those less confident given by the other method. \nAfter combination, the mAP is boosted to 60.9%. In 17 out of all 20\ncategories the combination performs better than either individual model. \nThis indicates that the two models are complementary.\n\nThey further find that the complementarity is mainly because of the convolutional layers. \nThey have tried to combine two randomly initialized fine-tuned results of the same convolutional model, and found no gain.\n\n# Deep Residual Learning for Image Recognition\n\nB. Object Detection Improvements\n\nSingle-model achieve mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9%.\n\nIn Faster R-CNN, the system is designed to learn region proposals and also object classifiers, \nso an ensemble can be used to boost both tasks. \nThey use an ensemble for proposing regions, and the union set of proposals are processed \nby an ensemble of per-region classifiers.\nThe mAP is 59.0% and 37.4% on the test-dev set.\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories"},"frontmatter":{"draft":false},"rawBody":"# PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories\n\n|    | PASCAL VOC       | COCO             | ImageNet              |\n|----|------------------|------------------|-----------------------|\n|1   | Aeroplane        | person           | accordion (手风琴)    |\n|2   | Bicycle          | bicycle          | airplane              |\n|3   | Bird             | car              | ant                   |\n|4   | Boat             | motorcycle       | antelope (羚羊)       |\n|5   | Bottle           | airplane         | apple                 |\n|6   | Bus              | bus              | armadillo (犰狳)      |\n|7   | Car              | train            | artichoke (洋蓟)      |\n|8   | Cat              | truck            | axe                   |\n|9   | Chair            | boat             | baby bed              |\n|10  | Cow              | traffic light    | backpack              |\n|11  | Dining table     | fire hydrant     | bagel (百吉饼)        |\n|12  | Dog              | stop sign        | balance beam          |\n|13  | Horse            | parking meter    | banana                |\n|14  | Motorbike        | bench            | band aid              |\n|15  | Person           | bird             | banjo (班卓琴)        |\n|16  | Potted plant     | cat              | baseball              |\n|17  | Sheep            | dog              | basketball            |\n|18  | Sofa             | horse            | bathing cap           |\n|19  | Train            | sheep            | beaker                |\n|20  | TV/Monitor       | cow              | bear                  |\n|21  |                  | elephant         | bee                   |\n|22  |                  | bear             | bell pepper           |\n|23  |                  | zebra            | bench                 |\n|24  |                  | giraffe          | bicycle               |\n|25  |                  | backpack         | binder                |\n|26  |                  | umbrella         | bird                  |\n|27  |                  | handbag          | bookshelf             |\n|28  |                  | tie              | bow                   |\n|29  |                  | suitcase         | bow tie               |\n|30  |                  | frisbee (飞盘)   | bowl                  |\n|31  |                  | skis             | brassiere             |\n|32  |                  | snowboard        | burrito (玉米煎饼)    |\n|33  |                  | sports ball      | bus                   |\n|34  |                  | kite             | butterfly             |\n|35  |                  | baseball bat     | camel                 |\n|36  |                  | baseball glove   | can opener            |\n|37  |                  | skateboard       | car                   |\n|38  |                  | surfboard        | cart                  |\n|39  |                  | tennis racket    | cattle                |\n|40  |                  | bottle           | cello (大提琴)        |\n|41  |                  | wine glass       | centipede (蜈蚣)      |\n|42  |                  | cup              | chain saw             |\n|43  |                  | fork             | chair                 |\n|44  |                  | knife            | chime                 |\n|45  |                  | spoon            | cocktail shaker       |\n|46  |                  | bowl             | coffee maker          |\n|47  |                  | banana           | computer keyboard     |\n|48  |                  | apple            | computer mouse        |\n|49  |                  | sandwich         | corkscrew (螺丝锥)    |\n|50  |                  | orange           | cream                 |\n|51  |                  | broccoli (西兰花)| croquet ball (槌球)   |\n|52  |                  | carrot           | crutch (拐杖)         |\n|53  |                  | hot dog          | cucumber (黄瓜)       |\n|54  |                  | pizza            | cup or mug            |\n|55  |                  | donut            | diaper                |\n|56  |                  | cake             | digital clock         |\n|57  |                  | chair            | dishwasher            |\n|58  |                  | couch            | dog                   |\n|59  |                  | potted plant     | domestic cat          |\n|60  |                  | bed              | dragonfly             |\n|61  |                  | dining table     | drum                  |\n|62  |                  | toilet           | dumbbell              |\n|63  |                  | tv               | electric fan          |\n|64  |                  | laptop           | elephant              |\n|65  |                  | mouse            | face powder           |\n|66  |                  | remote           | fig                   |\n|67  |                  | keyboard         | filing cabinet        |\n|68  |                  | cell phone       | flower pot            |\n|69  |                  | microwave        | flute (长笛)          |\n|70  |                  | oven             | fox                   |\n|71  |                  | toaster          | french horn           |\n|72  |                  | sink             | frog                  |\n|73  |                  | refrigerator     | frying pan            |\n|74  |                  | book             | giant panda           |\n|75  |                  | clock            | goldfish              |\n|76  |                  | vase             | golf ball             |\n|77  |                  | scissors         | golfcart              |\n|78  |                  | teddy bear       | guacamole (鳄梨色拉酱) |\n|79  |                  | hair drier       | guitar                |\n|80  |                  | toothbrush       | hair dryer            |\n|81  |                  |                  | hair spray (发胶)     |\n|82  |                  |                  | hamburger             |\n|83  |                  |                  | hammer                |\n|84  |                  |                  | hamster               |\n|85  |                  |                  | harmonica (口琴)      |\n|86  |                  |                  | harp (竖琴)           |\n|87  |                  |                  | hat with a wide brim  |\n|88  |                  |                  | head cabbage          |\n|89  |                  |                  | helmet                |\n|90  |                  |                  | hippopotamus (河马)   |\n|91  |                  |                  | horizontal bar (单杠) |\n|92  |                  |                  | horse                 |\n|93  |                  |                  | hotdog                |\n|94  |                  |                  | iPod                  |\n|95  |                  |                  | isopod (等足类动物)    |\n|96  |                  |                  | jellyfish (水母)      |\n|97  |                  |                  | koala bear (树袋熊)   |\n|98  |                  |                  | ladle (长柄勺)        |\n|99  |                  |                  | ladybug               |\n|100 |                  |                  | lamp                  |\n|101 |                  |                  | laptop                |\n|102 |                  |                  | lemon                 |\n|103 |                  |                  | lion                  |\n|104 |                  |                  | lipstick              |\n|105 |                  |                  | lizard                |\n|106 |                  |                  | lobster               |\n|107 |                  |                  | maillot (紧身衣/足球衫)|\n|108 |                  |                  | maraca (沙球)         |\n|109 |                  |                  | microphone            |\n|110 |                  |                  | microwave             |\n|111 |                  |                  | milk can              |\n|112 |                  |                  | miniskirt             |\n|113 |                  |                  | monkey                |\n|114 |                  |                  | motorcycle            |\n|115 |                  |                  | mushroom              |\n|116 |                  |                  | nail                  |\n|117 |                  |                  | neck brace            |\n|118 |                  |                  | oboe (双簧管)         |\n|119 |                  |                  | orange                |\n|120 |                  |                  | otter (水獭)          |\n|121 |                  |                  | pencil box            |\n|122 |                  |                  | pencil sharpener      |\n|123 |                  |                  | perfume               |\n|124 |                  |                  | person                |\n|125 |                  |                  | piano                 |\n|126 |                  |                  | pineapple (菠萝)      |\n|127 |                  |                  | ping-pong ball        |\n|128 |                  |                  | pitcher (大水罐)      |\n|129 |                  |                  | pizza                 |\n|130 |                  |                  | plastic bag           |\n|131 |                  |                  | plate rack (餐具架)   |\n|132 |                  |                  | pomegranate (石榴)    |\n|133 |                  |                  | popsicle (棒冰)       |\n|134 |                  |                  | porcupine (箭猪)      |\n|135 |                  |                  | power drill (机械钻)  |\n|136 |                  |                  | pretzel (椒盐卷饼)    |\n|137 |                  |                  | printer               |\n|138 |                  |                  | puck (冰球)           |\n|139 |                  |                  | punching bag          |\n|140 |                  |                  | purse                 |\n|141 |                  |                  | rabbit                |\n|142 |                  |                  | racket (球拍)         |\n|143 |                  |                  | ray                   |\n|144 |                  |                  | red panda             |\n|145 |                  |                  | refrigerator          |\n|146 |                  |                  | remote control        |\n|147 |                  |                  | rubber eraser (橡皮)  |\n|148 |                  |                  | rugby ball (橄榄球)   |\n|149 |                  |                  | ruler                 |\n|150 |                  |                  | salt or pepper shaker |\n|151 |                  |                  | saxophone             |\n|152 |                  |                  | scorpion              |\n|153 |                  |                  | screwdriver (螺丝刀)  |\n|154 |                  |                  | seal (海豹/印章)      |\n|155 |                  |                  | sheep                 |\n|156 |                  |                  | ski                   |\n|157 |                  |                  | skunk (臭鼬)          |\n|158 |                  |                  | snail                 |\n|159 |                  |                  | snake                 |\n|160 |                  |                  | snowmobile (机动雪橇)  |\n|161 |                  |                  | snowplow (雪犁)       |\n|162 |                  |                  | soap dispenser        |\n|163 |                  |                  | soccer ball           |\n|164 |                  |                  | sofa                  |\n|165 |                  |                  | spatula (抹刀/小铲)   |\n|166 |                  |                  | squirrel              |\n|167 |                  |                  | starfish              |\n|168 |                  |                  | stethoscope (听诊器)  |\n|169 |                  |                  | stove                 |\n|170 |                  |                  | strainer (过滤器)     |\n|171 |                  |                  | strawberry            |\n|172 |                  |                  | stretcher (担架)      |\n|173 |                  |                  | sunglasses            |\n|174 |                  |                  | swimming trunks (泳裤)|\n|175 |                  |                  | swine (猪)            |\n|176 |                  |                  | syringe (注射器)      |\n|177 |                  |                  | table                 |\n|178 |                  |                  | tape player           |\n|179 |                  |                  | tennis ball           |\n|180 |                  |                  | tick (扁虱)           |\n|181 |                  |                  | tie                   |\n|182 |                  |                  | tiger                 |\n|183 |                  |                  | toaster               |\n|184 |                  |                  | traffic light         |\n|185 |                  |                  | train                 |\n|186 |                  |                  | trombone (长号)       |\n|187 |                  |                  | trumpet (喇叭)        |\n|188 |                  |                  | turtle                |\n|189 |                  |                  | tv or monitor         |\n|190 |                  |                  | unicycle (单轮车)     |\n|191 |                  |                  | vacuum (真空吸尘器)    |\n|192 |                  |                  | violin                |\n|193 |                  |                  | volleyball            |\n|194 |                  |                  | waffle iron (华夫饼干烤模)|\n|195 |                  |                  | washer (垫圈)         |\n|196 |                  |                  | water bottle          |\n|197 |                  |                  | watercraft            |\n|198 |                  |                  | whale                 |\n|199 |                  |                  | wine bottle           |\n|200 |                  |                  | zebra                 |\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","title":"2016-12-30-Setup-Opengrok"},"frontmatter":{"draft":false},"rawBody":"Official guide:\n\n**Windows installation**\n\n[https://github.com/OpenGrok/OpenGrok/wiki/How-to-install-OpenGrok#Windows_installation](https://github.com/OpenGrok/OpenGrok/wiki/How-to-install-OpenGrok#Windows_installation)\n\n[http://algopadawan.blogspot.co.uk/2012/07/installing-opengrok-on-windows.html](http://algopadawan.blogspot.co.uk/2012/07/installing-opengrok-on-windows.html)\n\nOr you can check out on this (actually I just follow this blog):\n\n**如何把 opengrok 安装在 windows上**\n\n[http://www.itdadao.com/articles/c15a664698p0.html](http://www.itdadao.com/articles/c15a664698p0.html)\n\nOn Windows 7 x64, I install following sotfware:\n\n1. OpenGrok 0.13-rc5\n\n[https://github.com/OpenGrok/OpenGrok/releases](https://github.com/OpenGrok/OpenGrok/releases)\n\n2. tomcat 9.0\n\n[http://tomcat.apache.org/tomcat-9.0-doc/index.html](http://tomcat.apache.org/tomcat-9.0-doc/index.html)\n\n3. ctags 5.8\n\n[http://ctags.sourceforge.net/](http://ctags.sourceforge.net/)\n\nit works.\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","title":"2017-01-20-Packing-C++-Project-to-Single-Executable"},"frontmatter":{"draft":false},"rawBody":"## Using objcopy to embed data file in an executable:\n\n**Embedding a File in an Executable, aka Hello World, Version 5967**\n\n[http://www.linuxjournal.com/content/embedding-file-executable-aka-hello-world-version-5967](http://www.linuxjournal.com/content/embedding-file-executable-aka-hello-world-version-5967)\n\n**Linking binary data**\n\n[https://dvdhrm.wordpress.com/tag/objcopy/](https://dvdhrm.wordpress.com/tag/objcopy/)\n\nOn Ubuntu 14.04 LTS, 86_64:\n\n```\nobjcopy --input binary \\\n    --output elf64-x86-64 \\\n    --binary-architecture i386:x86-64 \\\n    youdatafile youdatafile.o\n```\n\n## How to deal with shared library?\n\nUse statifier or Ermine. Both tools take as input dynamically linked executable \nand as output create self-contained executable with all shared libraries embedded.\n\n[http://statifier.sourceforge.net/](http://statifier.sourceforge.net/)\n\n[http://www.magicermine.com/](http://www.magicermine.com/)\n\n```\n/path/to/ErmineLightTrial.x86_64 you_executable_file \\\n    --ld_library_path=/path/to/ld_lib/ \\\n    --output=new_executable_file\n```\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","title":"Notes On Tensorflow Development"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Notes On Tensorflow Development\ndate: 2017-04-13\n---\n\n# Install Bazel on Ubuntu 14.04\n\n**Install with installer:**\n\n[https://bazel.build/versions/master/docs/install-ubuntu.html#install-with-installer](https://bazel.build/versions/master/docs/install-ubuntu.html#install-with-installer)\n\nOfficial instruction is simple:\n\n```\nsudo apt-get install openjdk-8-jdk\n```\n\nBut due to network/proxy problem I cannot make it work. \nAfter some goolings I found a work-around if someone else also fails on installing openjdk-8-jdk following above command: intalling Oracle's jdk.\n\nStep 1: Download jdk8\n\nI choose to use jdk-8u121-linux-x64.tar.gz:\n\n[http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)\n\nStep 2: Uncompression and Copy to one target directory:\n\n```\nsudo mkdir /usr/lib/jvm\nsudo tar -zxvf jdk-8u121-linux-x64.tar.gz -C /usr/lib/jvm\n```\n\nStep 3: Modify some system PATHs:\n\nAdd following lines to your ~/.bashrc :\n\n```\n#set oracle jdk environment @20170413\nexport JAVA_HOME=/usr/lib/jvm/jdk1.8.0_121\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\nexport PATH=${JAVA_HOME}/bin:$PATH\n```\n\nDon't forget to enable these settings:\n\n```\nsource ~/.bashrc\n```\n\nStep 4: Set system default jdk version:\n\n```\njdk_ver=jdk1.8.0_121\n\nsudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/${jdk_ver}/bin/java 300\nsudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/${jdk_ver}/bin/javac 300\nsudo update-alternatives --install /usr/bin/jar jar /usr/lib/jvm/${jdk_ver}/bin/jar 300\nsudo update-alternatives --install /usr/bin/javah javah /usr/lib/jvm/${jdk_ver}/bin/javah 300\nsudo update-alternatives --install /usr/bin/javap javap /usr/lib/jvm/${jdk_ver}/bin/javap 300\n\nsudo update-alternatives --config java\n```\n\nYou might get similar outputs like below:\n\n```\nmy_account@node7:~/my_account/sw$ sudo ./set_system_default_jdk_version.sh\nupdate-alternatives: using /usr/lib/jvm/jdk1.8.0_121/bin/javac to provide /usr/bin/javac (javac) in auto mode\nupdate-alternatives: using /usr/lib/jvm/jdk1.8.0_121/bin/jar to provide /usr/bin/jar (jar) in auto mode\nupdate-alternatives: using /usr/lib/jvm/jdk1.8.0_121/bin/javah to provide /usr/bin/javah (javah) in auto mode\nupdate-alternatives: using /usr/lib/jvm/jdk1.8.0_121/bin/javap to provide /usr/bin/javap (javap) in auto mode\nThere are 2 choices for the alternative java (providing /usr/bin/java).\n\n  Selection    Path                                Priority   Status\n------------------------------------------------------------\n* 0            /usr/bin/gij-4.8                     1048      auto mode\n  1            /usr/bin/gij-4.8                     1048      manual mode\n  2            /usr/lib/jvm/jdk1.8.0_121/bin/java   300       manual mode\n\nPress enter to keep the current choice[*], or type selection number: 2\nupdate-alternatives: using /usr/lib/jvm/jdk1.8.0_121/bin/java to provide /usr/bin/java (java) in manual mode\n```\n\nRun `java -version`, you will get following output:\n\n```\nmy_account@node7:~/my_account/sw$ java -version\njava version \"1.8.0_121\"\nJava(TM) SE Runtime Environment (build 1.8.0_121-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)\n```\n\nStep 5:\n\nAnd finally you can successfully execute below command:\n\n```\nbazel-0.4.5-installer-linux-x86_64.sh --user\n```\n\nIt would be useful to add following line to your ~/.bashrc :\n```\n# for Bazel\nexport PATH=\"$PATH:$HOME/bin\"\n```\n\nDone.\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","title":"Big Data Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: big_data\ntitle: Big Data Resources\ndate: 2015-09-22\n---\n\n# Courses\n\n**MIT 6.S897: Large-Scale Systems(Matei Zaharia)**\n\n- instructor: Matei Zaharia\n- homepage: [http://people.csail.mit.edu/matei/courses/2015/6.S897/](http://people.csail.mit.edu/matei/courses/2015/6.S897/)\n\n# Papers\n\n**Learning to Hash for Indexing Big Data - A Survey**\n\n- paper: [http://arxiv.org/abs/1509.05472v1](http://arxiv.org/abs/1509.05472v1)\n\n**Random Forests for Big Data**\n\n- arxiv: [http://arxiv.org/abs/1511.08327](http://arxiv.org/abs/1511.08327)\n\n**Big data analytics: a survey**\n\n- paper: [http://www.journalofbigdata.com/content/pdf/s40537-015-0030-3.pdf](http://www.journalofbigdata.com/content/pdf/s40537-015-0030-3.pdf)\n\n**A Comparison of Big Data Frameworks on a Layered Dataflow Model**\n\n- arxiv: [http://arxiv.org/abs/1606.05293](http://arxiv.org/abs/1606.05293)\n\n**A survey of machine learning for big data processing**\n\n- paper: [http://link.springer.com/article/10.1186/s13634-016-0355-x](http://link.springer.com/article/10.1186/s13634-016-0355-x)\n\n**A Big Data Analysis Framework Using Apache Spark and Deep Learning**\n\n- intro: IEEE ICDM 2017 (International Conference on Data Mining) Workshop on Data Science and Big Data Analytics (DSBDA)\n- intro: University of Delhi & Manav Rachna University & CMU]\n- arxiv: [https://arxiv.org/abs/1711.09279](https://arxiv.org/abs/1711.09279)\n\n# Projects\n\n## Open Big Data Group\n\n**Open Big Data Group**\n\n- intro: This website contains a collection of libraries to be used in processing massive data size \nin highly distributed and paralleled environment\n- homepage: [http://openbigdatagroup.github.io/](http://openbigdatagroup.github.io/)\n\n**PLDA: Parallel C++ implementation of Latent Dirichlet Allocation**\n\n- homepage: [http://openbigdatagroup.github.io/plda/](http://openbigdatagroup.github.io/plda/)\n- github: [https://github.com/openbigdatagroup/plda](https://github.com/openbigdatagroup/plda)\n\n**PSVM: Parallelizing Support Vector Machines on Distributed Computers**\n\n- homepage: [http://openbigdatagroup.github.io/psvm/](http://openbigdatagroup.github.io/psvm/)\n- paper: [http://papers.nips.cc/paper/3202-parallelizing-support-vector-machines-on-distributed-computers.pdf](http://papers.nips.cc/paper/3202-parallelizing-support-vector-machines-on-distributed-computers.pdf)\n- github: [https://github.com/openbigdatagroup/psvm](https://github.com/openbigdatagroup/psvm)\n\n**PFP: Parallel FP-Growth for Query Recommendation**\n\n- homepage: [https://mahout.apache.org/users/misc/parallel-frequent-pattern-mining.html](https://mahout.apache.org/users/misc/parallel-frequent-pattern-mining.html)\n\n**Pspectralclustering: A parallel C++ implementation of Parallel Spectral Clustering**\n\n- homepage: [http://openbigdatagroup.github.io/pspectralclustering/](http://openbigdatagroup.github.io/pspectralclustering/)\n- github: [https://github.com/openbigdatagroup/pspectralclustering](https://github.com/openbigdatagroup/pspectralclustering)\n\n**Speedo: Parallelizing Stochastic Gradient Descent for Deep Convolutional Neural Network**\n\n- homepage: [http://openbigdatagroup.github.io/speedo/](http://openbigdatagroup.github.io/speedo/)\n- github: [https://github.com/openbigdatagroup/speedo](https://github.com/openbigdatagroup/speedo)\n\n# Videos\n\n**Awesome Big Data Algorithms**\n\n- youtube: [https://www.youtube.com/watch?v=jKBwGlYb13w](https://www.youtube.com/watch?v=jKBwGlYb13w)\n\n# Blog\n\n**Uncovering Big Bias with Big Data**\n\n- blog: [https://lawyerist.com/110584/big-bias-big-data/](https://lawyerist.com/110584/big-bias-big-data/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","title":"Robotics"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: robotics\ntitle: Robotics\ndate: 2015-09-26\n---\n\n# Courses\n\n**CS223A : Introduction to Robotics, Winter 2015**\n\n- homepage: [http://cs.stanford.edu/groups/manips/teaching/cs223a/](http://cs.stanford.edu/groups/manips/teaching/cs223a/)\n- opencourse: [http://open.163.com/special/opencourse/robotics.html](http://open.163.com/special/opencourse/robotics.html)\n\n**(Udacity) Artificial Intelligence for Robotics: Programming a Robotic Car**\n\n- homepage: [https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373](https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373)\n\n**Ludobots - Evolutionary Robotics**\n\n- intro: Josh Bongard, University of Vermont\n- homepage: [https://www.reddit.com/r/ludobots/wiki/index](https://www.reddit.com/r/ludobots/wiki/index)\n- start page: [https://www.reddit.com/r/ludobots/comments/4lctij/new_start_here/](https://www.reddit.com/r/ludobots/comments/4lctij/new_start_here/)\n- shedule: [https://docs.google.com/spreadsheets/d/1s-7Vedmf_KeC7xxA90HJCw9QTYQzHrRZQDz48H87GQk/edit#gid=0](https://docs.google.com/spreadsheets/d/1s-7Vedmf_KeC7xxA90HJCw9QTYQzHrRZQDz48H87GQk/edit#gid=0)\n- video lectures: [https://www.youtube.com/watch?v=EQgfrv4gOQY](https://www.youtube.com/watch?v=EQgfrv4gOQY)\n\n# Papers\n\n**Robots that can adapt like animals**\n\n- intro: Nature 2014\n- arxiv: [http://arxiv.org/abs/1407.3501](http://arxiv.org/abs/1407.3501)\n- code: [http://pages.isir.upmc.fr/~mouret/code/ite_source_code.tar.gz](http://pages.isir.upmc.fr/~mouret/code/ite_source_code.tar.gz)\n- videos: [http://www.nature.com/nature/journal/v521/n7553/full/nature14422.html#videos](http://www.nature.com/nature/journal/v521/n7553/full/nature14422.html#videos)\n\n**Robot Learning Manipulation Action Plans by ``Watching'' Unconstrained Videos From the World Wide Web**\n\n- paper: [http://www.umiacs.umd.edu/~yzyang/paper/YouCookMani_CameraReady.pdf](http://www.umiacs.umd.edu/~yzyang/paper/YouCookMani_CameraReady.pdf)\n\n**RoboBrain: Large-Scale Knowledge Engine for Robots(Stanford)**\n\n- paper: [http://arxiv.org/abs/1412.0691](http://arxiv.org/abs/1412.0691)\n\n**A Survey of Robotic Musicianship**\n\n![](http://deliveryimages.acm.org/10.1145/2820000/2818994/ins02.gif)\n![](http://deliveryimages.acm.org/10.1145/2820000/2818994/ins03.gif)\n![](http://deliveryimages.acm.org/10.1145/2820000/2818994/ins04.gif)\n![](http://deliveryimages.acm.org/10.1145/2820000/2818994/ins05.gif)\n\n- paper: [http://cacm.acm.org/magazines/2016/5/201594-a-survey-of-robotic-musicianship/fulltext](http://cacm.acm.org/magazines/2016/5/201594-a-survey-of-robotic-musicianship/fulltext)\n- pdf: [http://delivery.acm.org/10.1145/2820000/2818994/p100-bretan.pdf?ip=106.2.163.178&id=2818994&acc=OPEN&key=FA9C0C5108D19366%2EFA9C0C5108D19366%2E4D4702B0C3E38B35%2E6D218144511F3437&CFID=782936619&CFTOKEN=45889912&__acm__=1462790600_3fbc87eef6f7d60e7e061c117d80b1ca](http://delivery.acm.org/10.1145/2820000/2818994/p100-bretan.pdf?ip=106.2.163.178&id=2818994&acc=OPEN&key=FA9C0C5108D19366%2EFA9C0C5108D19366%2E4D4702B0C3E38B35%2E6D218144511F3437&CFID=782936619&CFTOKEN=45889912&__acm__=1462790600_3fbc87eef6f7d60e7e061c117d80b1ca)\n\n# Projects\n\n**The MRPT project**\n\n- intro: The Mobile Robot Programming Toolkit (MRPT)\n- project page: [http://www.mrpt.org/](http://www.mrpt.org/)\n- github: [https://github.com/MRPT/mrpt](https://github.com/MRPT/mrpt)\n\n# Hardware\n\n**Deep Learning Robot**\n\n![](/assets/robots/deepbot-research-robot-18.jpg)\n\n[https://www.autonomous.ai/deep-learning-robot](https://www.autonomous.ai/deep-learning-robot)\n\n# Resources\n\n**Awesome Robotics: A list of awesome Robotics resources**\n\n- github: [https://github.com/Kiloreux/awesome-robotics](https://github.com/Kiloreux/awesome-robotics)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","title":"Computer Science Resources"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: computer_science\r\ntitle: Computer Science Resources\r\ndate: 2015-10-01\r\n---\r\n\r\n# Courses\r\n\r\n**(Udacity) Georgia Tech Masters in CS**\r\n\r\n[https://www.udacity.com/courses/georgia-tech-masters-in-cs](https://www.udacity.com/courses/georgia-tech-masters-in-cs)\r\n\r\n**University of Virginia: cs6501 Spring 2013: Great Works in Computer Science**\r\n\r\n- homepage: [https://www.cs.virginia.edu/~evans/greatworks/](https://www.cs.virginia.edu/~evans/greatworks/)\r\n- syllabus: [https://www.cs.virginia.edu/~evans/greatworks/syllabus.html](https://www.cs.virginia.edu/~evans/greatworks/syllabus.html)\r\n- projects: [https://www.cs.virginia.edu/~evans/greatworks/projects.html](https://www.cs.virginia.edu/~evans/greatworks/projects.html)\r\n\r\n**University of California at Berkeley : CS294, Spring 2014: Evolution and Computation**\r\n\r\n- homepage: [http://www.eecs.berkeley.edu/~christos/evol/evolcomS14.html](http://www.eecs.berkeley.edu/~christos/evol/evolcomS14.html)\r\n\r\n**Introduction to Computer Science and Programming Using Python**\r\n\r\n- edx page: [https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-8](https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-8)\r\n\r\n**Game Theory Online**\r\n\r\n- youtube: [https://www.youtube.com/user/gametheoryonline/playlists](https://www.youtube.com/user/gametheoryonline/playlists)\r\n\r\n**Computer Science video courses**\r\n\r\n- intro: List of Computer Science courses with video lectures. \r\n- github: [https://github.com/Developer-Y/cs-video-courses](https://github.com/Developer-Y/cs-video-courses)\r\n\r\n## Database Systems\r\n\r\n**CMU 15-721 (SPRING 2016): Database Systems**\r\n\r\n- homepage: [http://15721.courses.cs.cmu.edu/spring2016/](http://15721.courses.cs.cmu.edu/spring2016/)\r\n- github: [https://github.com/mli/15721-ta](https://github.com/mli/15721-ta)\r\n\r\n# Operation System\r\n\r\n**Hack The Kernel**\r\n\r\n- intro: ops-class.org includes everything you need to learn about operating systems online.\r\n- homepage: [https://www.ops-class.org/](https://www.ops-class.org/)\r\n\r\n# Computer Graphics\r\n\r\n**Taichi 太極: Physically based Computer Graphics Library**\r\n\r\n- homepage: [http://taichi.graphics/](http://taichi.graphics/)\r\n- gtihub: [https://github.com/IteratorAdvance/taichi](https://github.com/IteratorAdvance/taichi)\r\n\r\n# Projects\r\n\r\n**Sketch: a Common Lisp environment for the creation of electronic art, visual design, game prototyping, game making, computer graphics, exploration of human-computer interaction and more**\r\n\r\n![](https://camo.githubusercontent.com/0a706993ecb761640ec70e974d39bfc98c5d1dea/687474703a2f2f692e696d6775722e636f6d2f4d4e5a55777a382e706e67)\r\n\r\n- github: [https://github.com/vydd/sketch](https://github.com/vydd/sketch)\r\n\r\n**DEAP: Distributed Evolutionary Algorithms in Python**\r\n\r\n- intro: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas.\r\n- github: [https://github.com/DEAP/deap](https://github.com/DEAP/deap)\r\n- docs: [http://deap.readthedocs.io/en/master/](http://deap.readthedocs.io/en/master/)\r\n\r\n**Mezzano, an operating system written in Common Lisp**\r\n\r\n![](https://camo.githubusercontent.com/1653851081ed45686c80978b12d76c1dff5feae4/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f34363735333031382f53637265656e73686f7425323066726f6d253230323031362d30332d31322532303134253341333625334135352e706e67)\r\n\r\n- github: [https://github.com/froggey/Mezzano](https://github.com/froggey/Mezzano)\r\n\r\n**Half-precision floating point library**\r\n\r\n- homepage: [http://half.sourceforge.net/](http://half.sourceforge.net/)\r\n- github: [https://github.com/headupinclouds/half](https://github.com/headupinclouds/half)\r\n\r\n**TREPL**\r\n\r\n- intro: TRE is an object oriented, functional programming language, \r\nthat enables user to view all processes happening inside a memory during program execution.\r\n- working example: [http://trepl.xyz/ide](http://trepl.xyz/ide)\r\n\r\n# Books\r\n\r\n**Introduction to Computer Music: Volume One**\r\n\r\n[http://www.indiana.edu/~emusic/etext/toc.shtml](http://www.indiana.edu/~emusic/etext/toc.shtml)\r\n\r\n# Blogs\r\n\r\n**Design Of A Modern Cache**\r\n\r\n- blog: [http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html](http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html)\r\n\r\n**Numeric matrix manipulation: The cheat sheet for MATLAB, Python NumPy, R, and Julia**\r\n\r\n[http://sebastianraschka.com/Articles/Matrix%20Cheat%20Sheet.html](http://sebastianraschka.com/Articles/Matrix%20Cheat%20Sheet.html)\r\n\r\n**Evolutionary Computation**\r\n\r\n![](http://www.alanzucconi.com/wp-content/uploads/2016/04/evolution1.png)\r\n\r\n- blog: [http://www.alanzucconi.com/](http://www.alanzucconi.com/)\r\n\r\n**The Cost of Knowledge**\r\n\r\n[http://thecostofknowledge.com/](http://thecostofknowledge.com/)\r\n\r\n**Landmark Contributions by Students in Computer Science**\r\n\r\n[http://lazowska.cs.washington.edu/Student_Achievements.pdf](http://lazowska.cs.washington.edu/Student_Achievements.pdf)\r\n\r\n**The Ultimate PCB Design Tutorial for Startups**\r\n\r\n![](http://www.nepsu.com/blog/img/nepsu_pcbdesignguide/mainpcblgoptimus.jpeg)\r\n\r\n- blog: [http://www.nepsu.com/nepsu_ultimate_pcb_design_guide.html](http://www.nepsu.com/nepsu_ultimate_pcb_design_guide.html)\r\n\r\n**How Humans Solve Complex Problems: The Case of the Knapsack Problem**\r\n\r\n- intro: Nature\r\n- paper: [http://www.nature.com/articles/srep34851](http://www.nature.com/articles/srep34851)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","title":"Database Systems Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_science\ntitle: Database Systems Resources\ndate: 2015-10-01\n---\n\n# Courses\n\n**CMU 15-721 (SPRING 2016): Database Systems**\n\n- homepage: [http://15721.courses.cs.cmu.edu/spring2016/](http://15721.courses.cs.cmu.edu/spring2016/)\n- github: [https://github.com/mli/15721-ta](https://github.com/mli/15721-ta)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","title":"Discrete Optimization Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_science\ntitle: Discrete Optimization Resources\ndate: 2015-10-01\n---\n\n# Constraint Programming\n\n**ON-LINE GUIDE TO CONSTRAINT PROGRAMMING (BY ROMAN BARTÁK)**\n\n- homepage: [http://kti.mff.cuni.cz/~bartak/constraints/](http://kti.mff.cuni.cz/~bartak/constraints/)\n\n**Constraint Programming**\n\n- page: [http://www.math.unipd.it/~frossi/chapter4.pdf](http://www.math.unipd.it/~frossi/chapter4.pdf)\n\n# Monte Carlo\n\n**Introduction to Monte Carlo Tree Search**\n\n- blog: [http://jeffbradberry.com/posts/2015/09/intro-to-monte-carlo-tree-search/](http://jeffbradberry.com/posts/2015/09/intro-to-monte-carlo-tree-search/)\n\n**Monte Carlo Tree Search**\n\n- blog: [http://mcts.ai/about/index.html](http://mcts.ai/about/index.html)\n- code: [http://mcts.ai/code/index.html](http://mcts.ai/code/index.html)\n- code(python): [http://mcts.ai/code/python.html](http://mcts.ai/code/python.html)\n- code(Java): [http://mcts.ai/code/java.html](http://mcts.ai/code/java.html)\n- zh(\"蒙特卡洛树搜索 MCTS\"): [http://www.jianshu.com/p/d011baff6b64](http://www.jianshu.com/p/d011baff6b64)\n\n**Monte Carlo theory, methods and examples**\n\n- homepage: [http://statweb.stanford.edu/~owen/mc/](http://statweb.stanford.edu/~owen/mc/)\n\n**A Survey of Monte Carlo Tree Search Methods**\n\n- paper: [http://ccg.doc.gold.ac.uk/papers/browne_tciaig12_1.pdf](http://ccg.doc.gold.ac.uk/papers/browne_tciaig12_1.pdf)\n- paper: [http://www.cameronius.com/cv/mcts-survey-master.pdf](http://www.cameronius.com/cv/mcts-survey-master.pdf)\n\n**Handbook of Markov Chain Monte Carlo**\n\n- book: [https://www.crcpress.com/Handbook-of-Markov-Chain-Monte-Carlo/Brooks-Gelman-Jones-Meng/p/book/9781420079418](https://www.crcpress.com/Handbook-of-Markov-Chain-Monte-Carlo/Brooks-Gelman-Jones-Meng/p/book/9781420079418)\n- homepage: [http://www.mcmchandbook.net/HandbookSampleChapters.html](http://www.mcmchandbook.net/HandbookSampleChapters.html)\n\n**Markov Chain Monte Carlo**\n\n- blog: [https://nicercode.github.io/guides/mcmc/](https://nicercode.github.io/guides/mcmc/)\n\n# Rectangle Bin Packing\n\n**Even More Rectangle Bin Packing**\n\n- blog: [http://clb.demon.fi/projects/even-more-rectangle-bin-packing](http://clb.demon.fi/projects/even-more-rectangle-bin-packing)\n- github: [https://github.com/juj/RectangleBinPack](https://github.com/juj/RectangleBinPack)\n\n**Solving the 2D Packing Problem**\n\n- page 1: [http://www.devx.com/dotnet/Article/36005](http://www.devx.com/dotnet/Article/36005)\n- page 2: [http://www.devx.com/dotnet/Article/36005/0/page/2](http://www.devx.com/dotnet/Article/36005/0/page/2)\n- page 3: [http://www.devx.com/dotnet/Article/36005/0/page/3](http://www.devx.com/dotnet/Article/36005/0/page/3)\n- page 4: [http://www.devx.com/dotnet/Article/36005/0/page/4](http://www.devx.com/dotnet/Article/36005/0/page/4)\n- page 5: [http://www.devx.com/dotnet/Article/36005/0/page/5](http://www.devx.com/dotnet/Article/36005/0/page/5)\n\n# Courses\n\n**Coursera: Discrete Optimization**\n\n- homepage: [https://www.coursera.org/course/optimization](https://www.coursera.org/course/optimization)\n\n**Saarland University: Constraint Programming: Services**\n\n- homepage: [http://www.ps.uni-saarland.de/courses/cp-ss07/services.html](http://www.ps.uni-saarland.de/courses/cp-ss07/services.html)\n\n**Combinatorial Problem Solving (CPS)**\n\n[http://www.cs.upc.edu/~erodri/webpage/cps/cps.html](http://www.cs.upc.edu/~erodri/webpage/cps/cps.html)\n\n# Books\n\n**Constraint Programming in Music**\n\n[http://as.wiley.com/WileyCDA/WileyTitle/productCd-1848212887.html](http://as.wiley.com/WileyCDA/WileyTitle/productCd-1848212887.html)\n\n# Blogs\n\n**Constraint Programming**\n\n- blog: [http://www.hakank.org/constraint_programming/](http://www.hakank.org/constraint_programming/)\n\n**My Constraint Programming Blog**\n\n- blog: [http://www.hakank.org/constraint_programming_blog/](http://www.hakank.org/constraint_programming_blog/)\n\n**Common constraint programming problems**\n\n- blog: [http://www.hakank.org/common_cp_models/](http://www.hakank.org/common_cp_models/)\n\n# Tools\n\n**Gecode**\n\n- intro: \"Gecode is a toolkit for developing constraint-based systems and applications. \nGecode provides a constraint solver with state-of-the-art performance while being modular and extensible.\"\n- homepage: [http://www.gecode.org/](http://www.gecode.org/)\n- doc(\"Modeling and Programming with Gecode\"): [http://www.gecode.org/doc-latest/MPG.pdf](http://www.gecode.org/doc-latest/MPG.pdf)\n- github: [https://github.com/ampl/gecode](https://github.com/ampl/gecode)\n- slides(\"Tutorial on Gecode Constraint Programming\"): [http://www.cs.upc.edu/~erodri/webpage/cps/lab/cp/tutorial-gecode-slides/slides.pdf](http://www.cs.upc.edu/~erodri/webpage/cps/lab/cp/tutorial-gecode-slides/slides.pdf)\n- slides(\"Lecture 4: CP Solvers Gecode\"): [http://www.imada.sdu.dk/~marco/DM826/Slides/dm826-lec4.pdf](http://www.imada.sdu.dk/~marco/DM826/Slides/dm826-lec4.pdf)\n- blog(\"My Gecode page\"): [http://www.hakank.org/gecode/](http://www.hakank.org/gecode/)\n\n**Google Optimization Tools**\n\n- homepage: [https://developers.google.com/optimization/](https://developers.google.com/optimization/)\n- github: [https://github.com/google/or-tools](https://github.com/google/or-tools)\n\n**LocalSolver: Mathematical optimization solver**\n\n- homepage: [http://www.localsolver.com/home.html](http://www.localsolver.com/home.html)\n\n# Resources\n\n**Reproducible Research in Computational Science**\n\n- intro: This site is intended to share the source codes of the latest advances \nin various technical fields to the best of my knowledge\n(including signal processing, computer vision, machine learning and neural computation)\n- homepage: [http://www.csee.wvu.edu/~xinl/source.html](http://www.csee.wvu.edu/~xinl/source.html)\n\n# Videos\n\n**Recent Developments in Combinatorial Optimization (Microsoft Research)**\n\n- homepage: [http://research.microsoft.com/apps/video/default.aspx?id=260488](http://research.microsoft.com/apps/video/default.aspx?id=260488)\n- video: [http://pan.baidu.com/s/1o6Wbn4I](http://pan.baidu.com/s/1o6Wbn4I)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","title":"Artificial Intelligence Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: artificial_intelligence\ntitle: Artificial Intelligence Resources\ndate: 2015-10-01\n---\n\n# Courses\n\n## Courses with Lecture Videos\n\n**UC Berkeley CS188 Intro to AI**\n\n- course schedule: [https://people.eecs.berkeley.edu/~russell/classes/cs188/f14/course_schedule.html](https://people.eecs.berkeley.edu/~russell/classes/cs188/f14/course_schedule.html)\n- lecture_videos: [https://people.eecs.berkeley.edu/~russell/classes/cs188/f14/lecture_videos.html](https://people.eecs.berkeley.edu/~russell/classes/cs188/f14/lecture_videos.html)\n- lecture videos: [http://ai.berkeley.edu/lecture_videos.html](http://ai.berkeley.edu/lecture_videos.html)\n\n**UC Berkeley CS188 Intro to AI -- Course Materials**\n\n- homepage: [http://ai.berkeley.edu/home.html](http://ai.berkeley.edu/home.html)\n\n**(edX) Artificial Intelligence (AI)**\n\n- homepage: [https://www.edx.org/course/artificial-intelligence-ai-columbiax-csmm-101x](https://www.edx.org/course/artificial-intelligence-ai-columbiax-csmm-101x)\n\n**MIT 6.S099: Artificial General Intelligence**\n\n[https://agi.mit.edu/](https://agi.mit.edu/)\n\n# Books\n\n**Notes on Artificial Intelligence (open source notebook)**\n\n- homepage: [http://frnsys.com/ai_notes/](http://frnsys.com/ai_notes/)\n- github: [https://github.com/frnsys/ai_notes](https://github.com/frnsys/ai_notes)\n\n**Artificial Intelligence - A Modern Approach (Russell And Norvig)**\n\n- github: [https://github.com/aimacode/aima-python](https://github.com/aimacode/aima-python)\n- github: [https://github.com/aimacode/aima-java](https://github.com/aimacode/aima-java)\n\n**Artificial Intelligence and Games**\n\n- intro: A Springer Textbook | By Georgios N. Yannakakis and Julian Togelius\n- homepage: [http://gameaibook.org/](http://gameaibook.org/)\n- draft: [http://gameaibook.org/book.pdf](http://gameaibook.org/book.pdf)\n\n# Papers\n\n**Building Machines That Learn and Think Like People**\n\n- arxiv: [http://arxiv.org/abs/1604.00289](http://arxiv.org/abs/1604.00289)\n\n**Learning Multiagent Communication with Backpropagation**\n\n- arxiv: [http://arxiv.org/abs/1605.07736](http://arxiv.org/abs/1605.07736)\n\n**Genetic Fuzzy based Artificial Intelligence for Unmanned Combat Aerial Vehicle Control in Simulated Air Combat Missions**\n\n- paper: [http://www.omicsgroup.org/journals/genetic-fuzzy-based-artificial-intelligence-for-unmanned-combat-aerialvehicle-control-in-simulated-air-combat-missions-2167-0374-1000144.pdf](http://www.omicsgroup.org/journals/genetic-fuzzy-based-artificial-intelligence-for-unmanned-combat-aerialvehicle-control-in-simulated-air-combat-missions-2167-0374-1000144.pdf)\n- blog: [http://magazine.uc.edu/editors_picks/recent_features/alpha.html](http://magazine.uc.edu/editors_picks/recent_features/alpha.html)\n\n# Projects\n\n**Universe: a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications**\n\n- homepage: [https://universe.openai.com/](https://universe.openai.com/)\n- github: [https://github.com/openai/universe](https://github.com/openai/universe)\n- github: [https://github.com/openai/universe-starter-agent](https://github.com/openai/universe-starter-agent)\n\n**Malmö: a platform for Artificial Intelligence experimentation and research built on top of Minecraft**\n\n![](https://mscorpmedia.azureedge.net/mscorpmedia/2016/07/Malmo-2-low-res.jpg)\n\n- intro: Project Malmo is a platform for Artificial Intelligence experimentation and research built on top of Minecraft. \nWe aim to inspire a new generation of research into challenging new problems presented by this unique environment.\n- github: [https://github.com/Microsoft/malmo](https://github.com/Microsoft/malmo)\n- blog: [https://blogs.microsoft.com/next/2016/07/07/project-malmo-lets-researchers-use-minecraft-ai-research-makes-public-debut/#sm.00000cprgapu2iec3xp063js2t3x3](https://blogs.microsoft.com/next/2016/07/07/project-malmo-lets-researchers-use-minecraft-ai-research-makes-public-debut/#sm.00000cprgapu2iec3xp063js2t3x3)\n- review: [http://www.zdnet.com/article/microsofts-project-malmo-goes-open-source/](http://www.zdnet.com/article/microsofts-project-malmo-goes-open-source/)\n\n**Introducing A.I. Experiments.**\n\n- intro: Explore machine learning by playing with pictures, language, music, code, and more.\n- homepage: [https://aiexperiments.withgoogle.com/](https://aiexperiments.withgoogle.com/)\n\n# Challenge / Competition\n\n**Halite: an artificial intelligence programming challenge**\n\n- intro: Contestants write bots to play an original multi-player turn-based strategy game played on a rectangular grid\n- homepage: [https://halite.io/](https://halite.io/)\n- github: [https://github.com/HaliteChallenge/Halite](https://github.com/HaliteChallenge/Halite)\n\n# Blogs\n\n**Artificial Intelligence Today**\n\n[http://oberheim.github.io/ai/2016/02/14/artificial-intelligence-today.html](http://oberheim.github.io/ai/2016/02/14/artificial-intelligence-today.html)\n\n**AlphaGo and AI Progress**\n\n[http://www.milesbrundage.com/blog-posts/alphago-and-ai-progress](http://www.milesbrundage.com/blog-posts/alphago-and-ai-progress)\n\n**CreativeAI: On the Democratisation & Escalation of Creativity — Chapter 01**\n\n[https://medium.com/@ArtificialExperience/creativeai-9d4b2346faf3#.nst0bspm9](https://medium.com/@ArtificialExperience/creativeai-9d4b2346faf3#.nst0bspm9)\n\n# Videos\n\n**The Future of AI**\n\n- intro: by Rich Sutton [UBC]\n- youtube: [https://www.youtube.com/watch?v=pD-FWetbvN8](https://www.youtube.com/watch?v=pD-FWetbvN8)\n- mirror: [http://pan.baidu.com/s/1dEbYp3N](http://pan.baidu.com/s/1dEbYp3N)\n\n**Towards General Artificial Intelligence**\n\n- author: Demis Hassabis, Google DeepMind\n- youtube: [https://www.youtube.com/watch?v=vQXAsdMa_8A](https://www.youtube.com/watch?v=vQXAsdMa_8A)\n\n**Artificial Intelligence Lecture Videos**\n\n- intro: MIT\n- homepage: [https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/)\n\n# Resources\n\n**Measuring the Progress of AI Research**\n\n- project page: [https://www.eff.org/ai/metrics](https://www.eff.org/ai/metrics)\n- github: [https://github.com/AI-metrics/AI-metrics](https://github.com/AI-metrics/AI-metrics)\n\n**Artificial Intelligence resources**\n\n- blog: [http://blog.digitalmind.io/post/artificial-intelligence-resources](http://blog.digitalmind.io/post/artificial-intelligence-resources)\n- blog: [https://medium.com/digitalmind/artificial-intelligence-resources-f4efeac949b4#.w4tu89t79](https://medium.com/digitalmind/artificial-intelligence-resources-f4efeac949b4#.w4tu89t79)\n\n**WHAT-AI-CAN-DO-FOR-YOU**\n\n- intro: Breakthrough AI Papers and CODE for Any Industry.\n- github: [https://github.com/ceobillionaire/WHAT-AI-CAN-DO-FOR-YOU](https://github.com/ceobillionaire/WHAT-AI-CAN-DO-FOR-YOU)\n\n**Model AI Assignments**\n\n[http://modelai.gettysburg.edu/](http://modelai.gettysburg.edu/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","title":"Algorithm and Data Structure Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: algorithm_and_data_structure\ntitle: Algorithm and Data Structure Resources\ndate: 2015-07-01\n---\n\n# Courses\n\n**Courses with Video Lectures**\n\n[http://cmlakhan.github.io/courses/videos.html](http://cmlakhan.github.io/courses/videos.html)\n\n**Stanford CS243: Program Analysis and Optimization**\n\n[http://suif.stanford.edu/~courses/cs243/](http://suif.stanford.edu/~courses/cs243/)\n\n**CMU 15-814: Types and Programming Languages**\n\n[http://www.cs.cmu.edu/~rwh/courses/typesys/](http://www.cs.cmu.edu/~rwh/courses/typesys/)\n\n**MIT: Introduction to Algorithms**\n[http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/index.htm](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/index.htm)\n\n**Princeton Computer Science 521: Advanced Algorithm Design**\n\n[http://www.cs.princeton.edu/courses/archive/fall15/cos521/](http://www.cs.princeton.edu/courses/archive/fall15/cos521/)\n\n**MIT: 6.006  Introduction to Algorithms**\n\n[https://stellar.mit.edu/S/course/6/sp16/6.006/materials.html](https://stellar.mit.edu/S/course/6/sp16/6.006/materials.html)\n\n**Datastructures and Algorithms (at Amsterdam University College) 2015-2016**\n\n- homepage: [http://www.cs.vu.nl/~tcs/datalg/](http://www.cs.vu.nl/~tcs/datalg/)\n- slides: [http://www.cs.vu.nl/~tcs/datalg/datalg-slides.pdf](http://www.cs.vu.nl/~tcs/datalg/datalg-slides.pdf)\n\n**MIT 6.851: Advanced Data Structures**\n\n[http://courses.csail.mit.edu/6.851/](http://courses.csail.mit.edu/6.851/)\n\n**MIT 6.046J: Design and Analysis of Algorithms (Spring 2015)**\n\n- lecture videos: [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-videos/](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-videos/)\n- lecture notes: [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-notes/](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-notes/)\n\n**Stanford CS243: Program Analysis and Optimization**\n\n[http://suif.stanford.edu/~courses/cs243/](http://suif.stanford.edu/~courses/cs243/)\n\n**MIT: Introduction to Algorithms**\n[http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/index.htm](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/index.htm)\n\n**(edX) Algorithms: Learn how to structure and use algorithms to solve real life problems.**\n\n[https://www.edx.org/course/algorithms-iitbombayx-cs213-3x](https://www.edx.org/course/algorithms-iitbombayx-cs213-3x)\n\n**(Coursera) Master Algorithmic Programming Techniques**\n\n- intro: Learn algorithms through programming and advance your software engineering or data science career\n- intro: Algorithmic Toolbox, Data Structures, Algorithms on Graphs, Algorithms on Strings, Advanced Algorithms and Complexity, Genome Assembly Programming Challenge\n- course-page: [https://www.coursera.org/specializations/data-structures-algorithms](https://www.coursera.org/specializations/data-structures-algorithms)\n\n# Papers\n\n**The JPS Pathfinding System**\n\n**JPS+: Over 100x Faster than A**\n\n- paper: [http://www.aaai.org/ocs/index.php/SOCS/SOCS12/paper/viewFile/5396/5212](http://www.aaai.org/ocs/index.php/SOCS/SOCS12/paper/viewFile/5396/5212)\n- video: [http://www.gdcvault.com/play/1022094/JPS-Over-100x-Faster-than](http://www.gdcvault.com/play/1022094/JPS-Over-100x-Faster-than)\n- mirror: [http://pan.baidu.com/s/1sjNtIVz](http://pan.baidu.com/s/1sjNtIVz)\n\n# Programming Contest\n\n**Stanford: CS 97SI: Introduction to Programming Contests**\n\n- course page: [http://web.stanford.edu/class/cs97si/](http://web.stanford.edu/class/cs97si/)\n\n**Stanford ACM-ICPC related materials**\n\n- intro: This is a repository for the Stanford ACM-ICPC teams. \nIt currently hosts (a) the team notebook, and (b) complete lecture slides for [CS 97SI](http://stanford.edu/class/cs97si/).\n- github: [https://github.com/jaehyunp/stanfordacm](https://github.com/jaehyunp/stanfordacm)\n\n**The Art of Programming Contest**\n\n- book: [https://www.comp.nus.edu.sg/~stevenha/database/Art_of_Programming_Contest_SE_for_uva.pdf](https://www.comp.nus.edu.sg/~stevenha/database/Art_of_Programming_Contest_SE_for_uva.pdf)\n- mirror: [https://pan.baidu.com/s/1qYQyXqc](https://pan.baidu.com/s/1qYQyXqc)\n\n**The Hitchhiker's Guide to the Programming Contest**\n\n- book: [http://comscigate.com/Books/contests/icpc.pdf](http://comscigate.com/Books/contests/icpc.pdf)\n- mirror: [https://pan.baidu.com/s/1i5I5M4H](https://pan.baidu.com/s/1i5I5M4H)\n\n**Competitive Programmer's Handbook**\n\n- homepage: [https://cses.fi/book.html](https://cses.fi/book.html)\n- pdf: [https://cses.fi/book.pdf](https://cses.fi/book.pdf)\n\n# Blogs\n\n**Know Thy Complexities!**\n\n![](http://bigocheatsheet.com/img/big-o-complexity.png)\n\n[http://bigocheatsheet.com/](http://bigocheatsheet.com/)\n\n**Visualizing Algorithms**\n\n![](https://bost.ocks.org/mike/algorithms/buy-vs-rent.gif)\n\n- github: [https://bost.ocks.org/mike/algorithms/](https://bost.ocks.org/mike/algorithms/)\n- video: [http://vimeo.com/112319901](http://vimeo.com/112319901)\n\n**Implementations of Algorithms & Datastructures from a Geek's Viewpoint**\n\n[http://www.geekviewpoint.com/](http://www.geekviewpoint.com/)\n\n**Z algorithm**\n\n[http://ivanyu.me/blog/2013/10/15/z-algorithm/](http://ivanyu.me/blog/2013/10/15/z-algorithm/)\n\n**Problem Solving with Algorithms and Data Structures (interactive Python online book)**\n\n[http://interactivepython.org/courselib/static/pythonds/index.html](http://interactivepython.org/courselib/static/pythonds/index.html)\n\n**Visualizing Algorithms**\n\n[http://bost.ocks.org/mike/algorithms/](http://bost.ocks.org/mike/algorithms/)\n\n**The missing method: Deleting from Okasaki's red-black trees**\n\n- blog: [http://matt.might.net/articles/red-black-delete/](http://matt.might.net/articles/red-black-delete/)\n\n**Fast Forward Labs: Probabilistic Data Structure Showdown: Cuckoo Filters vs. Bloom Filters**\n\n[http://blog.fastforwardlabs.com/post/153566952648/probabilistic-data-structure-showdown-cuckoo](http://blog.fastforwardlabs.com/post/153566952648/probabilistic-data-structure-showdown-cuckoo)\n\n**Data Structures Related to Machine Learning Algorithms**\n\n[https://dzone.com/articles/data-structures-related-to-machine-learning-algori](https://dzone.com/articles/data-structures-related-to-machine-learning-algori)\n\n**Using Self-Organizing Maps to solve the Traveling Salesman Problem**\n\n- blog: [https://diego.codes/post/som-tsp/](https://diego.codes/post/som-tsp/)\n- github: [https://github.com/DiegoVicen/som-tsp](https://github.com/DiegoVicen/som-tsp)\n\n## Visualization\n\n**Visualisation of A* path-finding algorithm in a maze**\n\n- reddit: [https://www.reddit.com/r/dataisbeautiful/comments/5eheul/visualisation_of_a_pathfinding_algorithm_in_a/](https://www.reddit.com/r/dataisbeautiful/comments/5eheul/visualisation_of_a_pathfinding_algorithm_in_a/)\n\n**Red Blob Games**\n\n[http://www.redblobgames.com/](http://www.redblobgames.com/)\n\n# Codes\n\n**Bloofi: A java implementation of multidimensional Bloom filters**\n\n- github: [https://github.com/lemire/bloofi](https://github.com/lemire/bloofi)\n\n**PathFinding.js: A comprehensive path-finding library for grid based games**\n\n![](/assets/algorithm_data_structure/pathfinding_js.PNG)\n\n- github: [https://github.com/qiao/PathFinding.js](https://github.com/qiao/PathFinding.js)\n- demo: [http://qiao.github.io/PathFinding.js/visual/](http://qiao.github.io/PathFinding.js/visual/)\n\n# Tools\n\n**Algorithm Visualizer**\n\n![](https://camo.githubusercontent.com/1d2e3b7d06c18d8e4e49d34cf06622b5d405b01a/687474703a2f2f692e67697068792e636f6d2f336f3645684a46677379536858364d48654d2e676966)\n\n- homepage: [http://jasonpark.me/AlgorithmVisualizer/](http://jasonpark.me/AlgorithmVisualizer/)\n- github: [https://github.com/parkjs814/AlgorithmVisualizer](https://github.com/parkjs814/AlgorithmVisualizer)\n\n**Visualizing String Matching**\n\n- intro: Naive / KMP / Boyer-Moore\n- homepage: [http://whocouldthat.be/visualizing-string-matching/](http://whocouldthat.be/visualizing-string-matching/)\n\n**Visualization of Sort Algorithms**\n\n[http://www.cs.usfca.edu/~galles/visualization/flash.html](http://www.cs.usfca.edu/~galles/visualization/flash.html)\n\n**Red/Black Tree Demonstration**\n\n[http://gauss.ececs.uc.edu/RedBlack/redblack.html](http://gauss.ececs.uc.edu/RedBlack/redblack.html)\n\n# Resources\n\n**The Algorithm Design Manual, 2nd Edition**\n\n- author: Steven Skiena\n- homepage: [http://www.algorist.com/#](http://www.algorist.com/#)\n\n**Solutions to Introduction to Algorithms**\n\n- github: [https://github.com/gzc/CLRS](https://github.com/gzc/CLRS)\n\n**Big-O Poster: Big-O Complexities / Poster of common algorithms used in Computer Science**\n\n![](https://cloud.githubusercontent.com/assets/1477672/16572711/6fd95eb8-4220-11e6-9389-c384da8553e0.jpg)\n\n- github: [https://github.com/ro31337/bigoposter](https://github.com/ro31337/bigoposter)\n\n# Reading and Questions\n\n**What are the lesser known but useful data structures?**\n\n- stackoverflow: [http://stackoverflow.com/questions/500607/what-are-the-lesser-known-but-useful-data-structures](http://stackoverflow.com/questions/500607/what-are-the-lesser-known-but-useful-data-structures)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","title":"Data Mining Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: data_mining\ntitle: Data Mining Resources\ndate: 2015-10-09\n---\n\n# Courses\n\n**CMU: Statistics 36-350: Data Mining(Fall 2009)**\n\n[http://www.stat.cmu.edu/~cshalizi/350/](http://www.stat.cmu.edu/~cshalizi/350/)\n\n# Books\n\n**Data Mining: Practical Machine Learning Tools and Techniques**\n\n[http://www.cs.waikato.ac.nz/ml/weka/book.html](http://www.cs.waikato.ac.nz/ml/weka/book.html)\n\n# Porjects\n\n**PyClustering**\n\n- intro: pyclustring is a Python, C++ data mining (clustering, graph coloring algorithms, oscillatory networks, neural networks, etc.) library.\n- github: [https://github.com/annoviko/pyclustering](https://github.com/annoviko/pyclustering)\n\n# Blogs\n\n**Data Mining in Python: A Guide**\n\n[https://www.springboard.com/blog/data-mining-python-tutorial/](https://www.springboard.com/blog/data-mining-python-tutorial/)\n\n# Resources\n\n**50 selected papers in Data Mining and Machine Learning**\n\n[http://bigdata-madesimple.com/50-selected-papers-in-data-mining-and-machine-learning/](http://bigdata-madesimple.com/50-selected-papers-in-data-mining-and-machine-learning/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","title":"Data Science Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: data_science\ntitle: Data Science Resources\ndate: 2015-10-09\n---\n\n# Books\n\n**Data Science at the Command Line**\n\n[http://datascienceatthecommandline.com/](http://datascienceatthecommandline.com/)\n\n**Data Science Challenges**\n\n[http://inverseprobability.com/2016/07/01/data-science-challenges](http://inverseprobability.com/2016/07/01/data-science-challenges)\n\n**Foundations of Data Science**\n\n- intro: John Hopcroft and Ravindran Kannan\n- book: [https://www.cs.cornell.edu/jeh/book2016June9.pdf](https://www.cs.cornell.edu/jeh/book2016June9.pdf)\n\n# Tutorials\n\n**Intelligent Data Analysis**\n\n- slides: [http://www.borgelt.net/slides/ida.pdf](http://www.borgelt.net/slides/ida.pdf)\n\n**Scaling Data Science in Python Tutorial**\n\n- github: [https://github.com/chdoig/dss-scaling-tutorial](https://github.com/chdoig/dss-scaling-tutorial)\n- slides: [https://speakerdeck.com/chdoig/scaling-ds-in-python](https://speakerdeck.com/chdoig/scaling-ds-in-python)\n\n**HackerMath: Introduction to Statistics and Basics of Mathematics for Data Science - The Hacker's Way**\n\n- intro: This is the repository for the full day workshop conducted at Fifth Elephant 2016 ([https://fifthelephant.in/2016/](https://fifthelephant.in/2016/))\n- github: [https://github.com/amitkaps/hackermath](https://github.com/amitkaps/hackermath)\n\n**Python Data Science Tutorials: common data analysis and machine learning tasks using python**\n\n- github: [https://github.com/ujjwalkarn/DataSciencePython](https://github.com/ujjwalkarn/DataSciencePython)\n\n**Getting Started with Data Science @ MSU Data Science**\n\n- slides: [https://speakerdeck.com/rasbt/getting-started-with-data-science-at-msu-data-science](https://speakerdeck.com/rasbt/getting-started-with-data-science-at-msu-data-science)\n\n**Helping our new Data Scientists start in Python: A guide to learning by doing**\n\n![](https://themarketingtechnologist-ghost.s3.amazonaws.com/2016/Sep/1200_628_marketingtechnologist_lbd_fb-1475154337738.jpg)\n\n- blog: [https://www.themarketingtechnologist.co/helping-our-new-data-scientists-start-in-python-a-guide-to-learning-by-doing/](https://www.themarketingtechnologist.co/helping-our-new-data-scientists-start-in-python-a-guide-to-learning-by-doing/)\n\n# Courses\n\n**General Assembly's Data Science course in Washington, DC**\n\n- course page: [https://generalassemb.ly/education/data-science/washington-dc/](https://generalassemb.ly/education/data-science/washington-dc/)\n- github: [https://github.com/justmarkham/DAT8](https://github.com/justmarkham/DAT8)\n\n**140.711/2 Advanced Data Science I/II**\n\n- homepage: [http://jtleek.com/advdatasci/](http://jtleek.com/advdatasci/)\n\n**Data Science From data to knowledge (September 12 to 14, 2016)**\n\n[http://data-science-ce-course.tk/](http://data-science-ce-course.tk/)\n\n**Harvard CS109 Data Science**\n\n- homepage: [http://cs109.github.io/2015/](http://cs109.github.io/2015/)\n- class material: [http://cs109.github.io/2015/pages/videos.html](http://cs109.github.io/2015/pages/videos.html)\n- github: [https://github.com/cs109/2015](https://github.com/cs109/2015)\n\n**Coursera: How to Win a Data Science Competition: Learn from Top Kagglers**\n\n[https://www.coursera.org/learn/competitive-data-science](https://www.coursera.org/learn/competitive-data-science)\n\n# Tools\n\n**Provision the Linux Data Science Virtual Machine**\n\n- blog: [https://azure.microsoft.com/en-gb/documentation/articles/machine-learning-data-science-linux-dsvm-intro/](https://azure.microsoft.com/en-gb/documentation/articles/machine-learning-data-science-linux-dsvm-intro/)\n- github: [https://github.com/Azure/azure-content/blob/master/articles/machine-learning/machine-learning-data-science-linux-dsvm-intro.md](https://github.com/Azure/azure-content/blob/master/articles/machine-learning/machine-learning-data-science-linux-dsvm-intro.md)\n\n**Deep Learning tools for the Data Science Virtual Machine**\n\n- github: [https://github.com/Microsoft/deep_learning_tools_for_dsvm](https://github.com/Microsoft/deep_learning_tools_for_dsvm)\n\n## Scrape Tools\n\n# Portia\n\n**Portia: Visual scraping for Scrapy**\n\n- intro: Portia is a tool that allows you to visually scrape websites \nwithout any programming knowledge required\n- homepage: [http://scrapinghub.com/portia/](http://scrapinghub.com/portia/)\n- github: [https://github.com/scrapinghub/portia/](https://github.com/scrapinghub/portia/)\n\n**Scrapely: A pure-python HTML screen-scraping library**\n\n- intro: Scrapely is a library for extracting structured data from HTML pages. \nGiven some example web pages and the data to be extracted, scrapely constructs a parser for all similar pages.\n- github: [https://github.com/scrapy/scrapely](https://github.com/scrapy/scrapely)\n\n# Blogs\n\n**Comprehensive learning path – Data Science in Python**\n[http://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/](http://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/)\n\n**Top Data Scientists to Follow & Best Data Science Tutorials on GitHub**\n\n[http://www.analyticsvidhya.com/blog/2015/07/github-special-data-scientists-to-follow-best-tutorials/](http://www.analyticsvidhya.com/blog/2015/07/github-special-data-scientists-to-follow-best-tutorials/)\n\n**Non-Mathematical Feature Engineering techniques for Data Science**\n\n[https://codesachin.wordpress.com/2016/06/25/non-mathematical-feature-engineering-techniques-for-data-science/](https://codesachin.wordpress.com/2016/06/25/non-mathematical-feature-engineering-techniques-for-data-science/)\n\n**Learning Path To Become Data Scientist – Step by Step Guide**\n\n![](https://i.imgur.com/EJspIqD.jpg)\n\n[http://www.bigdataanalyticsguide.com/2016/07/09/learning-path-become-data-scientist-step-step-guide/](http://www.bigdataanalyticsguide.com/2016/07/09/learning-path-become-data-scientist-step-step-guide/)\n\n**Data Science Basics: An Introduction to Ensemble Learners**\n\n- intro: Beginners, Boosting, Data Science, Ensemble methods\n- blog: [http://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html](http://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html)\n\n# Resources\n\n**Awesome Data Engineering: A curated list of data engineering tools for software developers**\n\n- github: [https://github.com/igorbarinov/awesome-data-engineering](https://github.com/igorbarinov/awesome-data-engineering)\n\n**Data science blogs**\n\n- github: [https://github.com/rushter/data-science-blogs](https://github.com/rushter/data-science-blogs)\n\n**data-science-practice-handbook: Awesome and Complete Practice Handbook For Data Science**\n\n- github: [https://github.com/wxyyxc1992/datascience-practice-handbook](https://github.com/wxyyxc1992/datascience-practice-handbook)\n\n**If you want to learn Data Science, start with one of these programming classes**\n\n- blog: [https://medium.freecodecamp.com/if-you-want-to-learn-data-science-start-with-one-of-these-programming-classes-fb694ffe780c#.quj2gi3f9](https://medium.freecodecamp.com/if-you-want-to-learn-data-science-start-with-one-of-these-programming-classes-fb694ffe780c#.quj2gi3f9)\n\n**Data Science, Machine Learning, and Artificial Intelligence Resources**\n\n- github: [https://github.com/acastrounis/data-science-machine-learning-ai-resources](https://github.com/acastrounis/data-science-machine-learning-ai-resources)\n\n**Data Science and Robots**\n\n[http://brohrer.github.io/blog.html](http://brohrer.github.io/blog.html)\n\n## Data Sources for Cool Data Science Projects\n\n- part 1: [http://blog.thedataincubator.com/2014/10/data-sources-for-cool-data-science-projects-part-1/](http://blog.thedataincubator.com/2014/10/data-sources-for-cool-data-science-projects-part-1/)\n- part 2: [http://blog.thedataincubator.com/2014/10/data-sources-for-cool-data-science-projects-part-2/](http://blog.thedataincubator.com/2014/10/data-sources-for-cool-data-science-projects-part-2/)\n- part 3: [http://blog.thedataincubator.com/2016/10/data-sources-for-cool-data-science-projects-part-3/](http://blog.thedataincubator.com/2016/10/data-sources-for-cool-data-science-projects-part-3/)\n- part 4: [http://blog.thedataincubator.com/2016/10/data-sources-for-cool-data-science-projects-part-4/](http://blog.thedataincubator.com/2016/10/data-sources-for-cool-data-science-projects-part-4/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","title":"Funny Stuffs Of Computer Science"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_science\ntitle: Funny Stuffs Of Computer Science\ndate: 2015-11-18\n---\n\n# Overflow\n\n\"Gangnam Style overflows INT_MAX, forces YouTube to go 64-bit\"\n\n[http://arstechnica.com/business/2014/12/gangnam-style-overflows-int_max-forces-youtube-to-go-64-bit/](http://arstechnica.com/business/2014/12/gangnam-style-overflows-int_max-forces-youtube-to-go-64-bit/)\n\n\"What to expect when Facing Gandhi (Civ V)\"\n\n[https://www.reddit.com/r/gaming/comments/1nvj2t/what_to_expect_when_facing_gandhi_civ_v/](https://www.reddit.com/r/gaming/comments/1nvj2t/what_to_expect_when_facing_gandhi_civ_v/)\n\n# Go (Weiqi)\n\n**Number of legal Go positions**\n\n![](http://tromp.github.io/img/go/legal19.gif)\n\n- blog: [http://tromp.github.io/go/legal.html](http://tromp.github.io/go/legal.html)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","title":"Computer Vision Datasets"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_vision\ntitle: Computer Vision Datasets\ndate: 2015-09-24\n---\n\n**Datasets who is the best at X ?**\n\n- blog: [http://rodrigob.github.io/are_we_there_yet/build/#datasets](http://rodrigob.github.io/are_we_there_yet/build/#datasets)\n\n**Computer Vision Datasets**\n\n- website: [http://clickdamage.com/sourcecode/index.html](http://clickdamage.com/sourcecode/index.html)\n- code: [http://clickdamage.com/sourcecode/cv_datasets.php](http://clickdamage.com/sourcecode/cv_datasets.php)\n- mirror: [http://pan.baidu.com/s/1pJmqD4n](http://pan.baidu.com/s/1pJmqD4n)\n\n**Introducing the Open Images Dataset**\n\n- blog: [https://research.googleblog.com/2016/09/introducing-open-images-dataset.html](https://research.googleblog.com/2016/09/introducing-open-images-dataset.html)\n- github: [https://github.com/openimages/dataset](https://github.com/openimages/dataset)\n- Academic Torrents: [http://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b](http://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b)\n\n**A parallel download util for Google's open image dataset**\n\n- github: [https://github.com/ejlb/google-open-image-download](https://github.com/ejlb/google-open-image-download)\n\n**Image & Vision Group - Datasets**\n\n- intro: Image & Vision , Clothing & Fashion, Computer Graphics, Video Sequences\n- homepage: [http://caiivg.weebly.com/dataset.html](http://caiivg.weebly.com/dataset.html)\n\n**Huizhong Chen - Datasets**\n\n- intro: Google I/O Dataset, Names 100 Dataset, Clothing Attributes Dataset,\nStanford Mobile Visual Search Dataset, CNN 2-Hours Videos Dataset\n- homepage: [http://huizhongchen.github.io/datasets.html#clothingattributedataset](http://huizhongchen.github.io/datasets.html#clothingattributedataset)\n\n# Classification / Recognition\n\n**A Large-Scale Car Dataset for Fine-Grained Categorization and Verification**\n\n![](http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/illustration.png)\n\n- project page: [http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html](http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html)\n- arxiv: [http://arxiv.org/abs/1506.08959](http://arxiv.org/abs/1506.08959)\n\n**CIFAR-10 / CIFAR100**\n\n- intro: The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. \nThere are 50000 training images and 10000 test images.\n- homepage: [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)\n\n**Tencent ML-Images**\n\n- intro: Largest multi-label image database; ResNet-101 model; 80.73% top-1 acc on ImageNet\n- github: [https://github.com/Tencent/tencent-ml-images](https://github.com/Tencent/tencent-ml-images)\n\n## Face\n\n**The MegaFace Benchmark: 1 Million Faces for Recognition at Scale**\n\n- homepage: [http://megaface.cs.washington.edu/](http://megaface.cs.washington.edu/)\n- arxiv: [http://arxiv.org/abs/1512.00596](http://arxiv.org/abs/1512.00596)\n\n**MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.08221](http://arxiv.org/abs/1607.08221)\n\n**MSR Image Recognition Challenge (IRC)**\n\n- homepage: [https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/](https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/)\n\n**UMDFaces: An Annotated Face Dataset for Training Deep Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.01484](https://arxiv.org/abs/1611.01484)\n\n## Vehicle\n\n**The Comprehensive Cars (CompCars) dataset**\n\n![](http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/illustration.png)\n\n[http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/](http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/)\n\n**BoxCars: Improving Fine-Grained Recognition of Vehicles Using 3-D Bounding Boxes in Traffic Surveillance [IEEE T-ITS]**\n\n![](https://medusa.fit.vutbr.cz/traffic/wp-content/uploads/2017/03/boxcars_bb_estimation_pipeline.png)\n\n[https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/](https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/)\n\n**Vehicle Make and Model Recognition Dataset (VMMRdb)**\n\n- intro: containing 9,170 classes consisting of 291,752 images, covering models manufactured between 1950 to 2016\n- homepage: [http://vmmrdb.cecsresearch.org/](http://vmmrdb.cecsresearch.org/)\n\n**Cars Dataset**\n\n![](http://ai.stanford.edu/~jkrause/cars/class_montage_flop.jpg)\n\n- intro: contains 16,185 images of 196 classes of cars.\n- homepage: [http://ai.stanford.edu/~jkrause/cars/car_dataset.html](http://ai.stanford.edu/~jkrause/cars/car_dataset.html)\n\n## Scene Recognition\n\n**Places: An Image Database for Deep Scene Understanding**\n\n- project page: [http://places.csail.mit.edu/index.html](http://places.csail.mit.edu/index.html)\n- arxiv: [https://arxiv.org/abs/1610.02055](https://arxiv.org/abs/1610.02055)\n\n**Places2**\n\n- intro: Places2 contains more than 10 million images comprising 400+ unique scene categories\n- homepage: [http://places2.csail.mit.edu/](http://places2.csail.mit.edu/)\n\n**The Places365-CNNs for Scene Classification**\n\n- github: [https://github.com/CSAILVision/places365](https://github.com/CSAILVision/places365)\n\n## MNIST\n\n**EMNIST: an extension of MNIST to handwritten letters**\n\n- arxiv: [https://arxiv.org/abs/1702.05373](https://arxiv.org/abs/1702.05373)\n\n**Fashion-MNIST**\n\n- arxiv: [https://arxiv.org/abs/1708.07747](https://arxiv.org/abs/1708.07747)\n- github: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)\n- benchmark: [http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/)\n\n# Food\n\n**3 Million Instacart Orders, Open Sourced**\n\n[https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)\n\n# Detection\n\n**YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video**\n\n- intro: YouTube-BoundingBoxes (YT-BB)\n- homepage: [https://research.google.com/youtubebb/](https://research.google.com/youtubebb/)\n- arxiv: [https://arxiv.org/abs/1702.00824](https://arxiv.org/abs/1702.00824)\n\n**DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects**\n\n[https://arxiv.org/abs/1804.00525](https://arxiv.org/abs/1804.00525)\n\n**Exclusively Dark (ExDark) Image Dataset**\n\n- intro: Exclusively Dark (ExDARK) dataset which to the best of our knowledge, \nis the largest collection of low-light images taken in very low-light environments to twilight (i.e 10 different conditions) \nto-date with image class and object level annotations.\n- github: [https://github.com/cs-chan/Exclusively-Dark-Image-Dataset](https://github.com/cs-chan/Exclusively-Dark-Image-Dataset)\n\n## Face Detection\n\n**FDDB: Face Detection Data Set and Benchmark**\n\n- homepage: [http://vis-www.cs.umass.edu/fddb/index.html](http://vis-www.cs.umass.edu/fddb/index.html)\n- results: [http://vis-www.cs.umass.edu/fddb/results.html](http://vis-www.cs.umass.edu/fddb/results.html)\n\n**WIDER FACE: A Face Detection Benchmark**\n\n![](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/intro.jpg)\n\n- homepage: [http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)\n- arxiv: [http://arxiv.org/abs/1511.06523](http://arxiv.org/abs/1511.06523)\n\n## Pedestrian Detection\n\n![](https://sshao0516.github.io/CrowdHuman/images/fig1.png)\n\n**Caltech Pedestrian Detection Benchmark**\n\n- homepage: [http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)\n\n**Caltech Pedestrian Dataset Converter**\n\n[https://github.com/mitmul/caltech-pedestrian-dataset-converter](https://github.com/mitmul/caltech-pedestrian-dataset-converter)\n\n**CityPersons: A Diverse Dataset for Pedestrian Detection**\n\n- arxiv: [https://arxiv.org/abs/1702.05693](https://arxiv.org/abs/1702.05693)\n- bitbucket: [https://bitbucket.org/shanshanzhang/citypersons](https://bitbucket.org/shanshanzhang/citypersons)\n- supplemental: [http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf)\n\n**CrowdHuman: A Benchmark for Detecting Human in a Crowd**\n\n- intro: CrowdHuman contains 15000, 4370 and 5000 images for training, validation, and testing, respectively.\na total of 470K human instances from train and validation subsets and 23 persons per image, with various kinds of occlusions in the dataset\n- homepage: [https://sshao0516.github.io/CrowdHuman/](https://sshao0516.github.io/CrowdHuman/)\n\n**EuroCity Persons Dataset**\n\n- intro: collected on-board a moving vehicle in 31 cities of 12 European countries, \nover 238200 person instances manually labeled in over 47300 images, \ncontains a large number of person orientation annotations (over 211200)\n- homepage: [https://eurocity-dataset.tudelft.nl/](https://eurocity-dataset.tudelft.nl/)\n- arxiv: [https://arxiv.org/abs/1805.07193](https://arxiv.org/abs/1805.07193)\n\n**WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild**\n\n![](http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/files/intro.jpg)\n\n- project page: [http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/](http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/)\n\n# Full-Body Annotations\n\n**COCO-WholeBody**\n\n[https://github.com/jin-s13/COCO-WholeBody](https://github.com/jin-s13/COCO-WholeBody)\n\n**Halpe Full-Body Human Keypoints and HOI-Det dataset**\n\n- intro: Halpe: full body human pose estimation and human-object interaction detection dataset\n- github:[https://github.com/Fang-Haoshu/Halpe-FullBody](https://github.com/Fang-Haoshu/Halpe-FullBody)\n\n## Vehicle Detection\n\n**Toyota Motor Europe (TME) Motorway Dataset**\n\n- intro: composed by 28 clips for a total of approximately 27 minutes (30000+ frames) with vehicle annotation\n- homepage: [http://cmp.felk.cvut.cz/data/motorway/](http://cmp.felk.cvut.cz/data/motorway/)\n\n**Welcome to BIT-Vehicle Dataset**\n\n![](http://iitlab.bit.edu.cn/mcislab/vehicledb/dataset.jpg)\n\n- intro: 9,850 vehicle images, sizes of 1600*1200 and 1920*1080 captured from two cameras at different time and places in the dataset\n- homepage: [http://iitlab.bit.edu.cn/mcislab/vehicledb/](http://iitlab.bit.edu.cn/mcislab/vehicledb/)\n\n# Vehicle Re-ID\n\n**A Large-Scale Dataset for Vehicle Re-Identification in the Wild**\n\n- github: [https://github.com/PKU-IMRE/VERI-Wild](https://github.com/PKU-IMRE/VERI-Wild)\n\n## Logo Detection\n\n**QMUL-OpenLogo: Open Logo Detection Challenge**\n\n- intro: QMUL-OpenLogo contains 27,083 images from 352 logo classes, \nbuilt by aggregating and refining 7 existing datasets and establishing an open logo detection evaluation protocol\n- homepage: [https://qmul-openlogo.github.io/](https://qmul-openlogo.github.io/)\n\n# Head Detection\n\n**SCUT-HEAD**\n\n- intro: SCUT HEAD is a large-scale head detection dataset, including 4405 images labeld with 111251 heads.\n- github: [https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release](https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release)\n\n**HollywoodHeads dataset**\n\n[http://www.di.ens.fr/willow/research/headdetection/](http://www.di.ens.fr/willow/research/headdetection/)\n\n**Brainwash dataset.**\n\n[https://exhibits.stanford.edu/data/catalog/sx925dc9385](https://exhibits.stanford.edu/data/catalog/sx925dc9385)\n\n## Detection From Video\n\n**YouTube-Objects dataset v2.2**\n\n- homepage: [http://calvin.inf.ed.ac.uk/datasets/youtube-objects-dataset/](http://calvin.inf.ed.ac.uk/datasets/youtube-objects-dataset/)\n\n**ILSVRC2015: Object detection from video (VID)**\n\n- homepage: [http://vision.cs.unc.edu/ilsvrc2015/download-videos-3j16.php#vid](http://vision.cs.unc.edu/ilsvrc2015/download-videos-3j16.php#vid)\n\n# Segmentation\n\n## Mapillary Vistas Dataset\n\n**Mapillary Vistas Dataset**\n\n- intro: 25,000 high-resolution images, 100 object categories, 60 of those instance-specific\n[https://www.mapillary.com/dataset/](https://www.mapillary.com/dataset/)\n\n**Releasing the World’s Largest Street-level Imagery Dataset for Teaching Machines to See**\n\n[http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html](http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html)\n\n**Multi-Human Parsing**\n\n![](https://lv-mhp.github.io/static/images/3.png)\n\n[https://lv-mhp.github.io/](https://lv-mhp.github.io/)\n\n# PASCAL VOC\n\n## Augmented Pascal VOC\n\n[http://home.bharathh.info/pubs/codes/SBD/download.html](http://home.bharathh.info/pubs/codes/SBD/download.html)\n\n# Supervisely Person\n\n- homepage: [https://supervise.ly/](https://supervise.ly/)\n- blog: [https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469](https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469)\n\n# Microsoft COCO\n\n- homepage: [http://mscoco.org/](http://mscoco.org/)\n- github: [https://github.com/pdollar/coco](https://github.com/pdollar/coco)\n\n# The Oxford-IIIT Pet Dataset\n\n- intro: a 37 category pet dataset with roughly 200 images for each class.\nAll images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation\n- homepage: [http://www.robots.ox.ac.uk/~vgg/data/pets/](http://www.robots.ox.ac.uk/~vgg/data/pets/)\n\n## COCO-Stuff\n\n**COCO-Stuff: Thing and Stuff Classes in Context**\n\n**COCO-Stuff 10K dataset v1.1**\n\n[https://arxiv.org/abs/1612.03716](https://arxiv.org/abs/1612.03716)\n[https://github.com/nightrome/cocostuff](https://github.com/nightrome/cocostuff)\n\n# Scene Parsing\n\n**MIT Scene Parsing Benchmark**\n\n[http://sceneparsing.csail.mit.edu/](http://sceneparsing.csail.mit.edu/)\n\n**ADE20K**\n\n- intro: train: 20,120 images, val: 2000 images. contains 150 stuff/object category labels (e.g., wall, sky, and tree) and 1,038 imagelevel scene descriptors (e.g., airport terminal, bedroom, and street).\n- homepage: [http://groups.csail.mit.edu/vision/datasets/ADE20K/](http://groups.csail.mit.edu/vision/datasets/ADE20K/)\n\n**Semantic Understanding of Scenes through the ADE20K Dataset**\n\n[https://arxiv.org/abs/1608.05442](https://arxiv.org/abs/1608.05442)\n\n# ImageNet\n\n- synsets: [http://image-net.org/challenges/LSVRC/2014/browse-det-synsets](http://image-net.org/challenges/LSVRC/2014/browse-det-synsets)\n\n**ImageNet-Utils**\n\n- intro: Utils to help download images by id, crop bounding box, label images, etc.\n- github: [https://github.com/tzutalin/ImageNet_Utils](https://github.com/tzutalin/ImageNet_Utils)\n\n# Captioning / Description\n\n**TGIF: A New Dataset and Benchmark on Animated GIF Description**\n\n- arxiv: [http://arxiv.org/abs/1604.02748](http://arxiv.org/abs/1604.02748)\n- github: [https://github.com/raingo/TGIF-Release](https://github.com/raingo/TGIF-Release)\n\n**Collecting Multilingual Parallel Video Descriptions Using Mechanical Turk**\n\n- intro: 1970 YouTube video snippets: 1200 training, 100 validation, 670 test\n- homepage: [http://www.cs.utexas.edu/users/ml/clamp/videoDescription/](http://www.cs.utexas.edu/users/ml/clamp/videoDescription/)\n\n# Video\n\n| Dataset          | # Videos     | # Classes   | Year     | Manually Labeled ? |\n|:----------------:|:----------- :|:-----------:|:--------:|:------------------:|\n| Kodak            |   1,358      |    25       |  2007    | ✓                 |\n| HMDB51           |   7000       |    51       |          |                    |\n| Charades         |   9848       |    157      |          |                    |\n| MCG-WEBV         |   234,414    |    15       |  2009    | ✓                 |\n| CCV              |   9,317      |    20       |  2011    | ✓                 |\n| UCF-101          |   13,320     |    101      |  2012    | ✓                 |\n| THUMOS-2         |   18,394     |    101      |  2014    | ✓                 |\n| MED-2014         |   ≈28,000    |    20       |  2014    | ✓                 |\n| Sports-1M        |   1M         |    487      |  2014    | ✗                 |\n| ActivityNet      |   27,801     |    203      |  2015    | ✓                 |\n| FCVID            |   91,223     |    239      |  2015    | ✓                 |\n\n**UCF101 - Action Recognition Data Set**\n\n![](http://crcv.ucf.edu/data/UCF101/UCF101.jpg)\n\n- homepage: [http://crcv.ucf.edu/data/UCF101.php](http://crcv.ucf.edu/data/UCF101.php)\n\n**HMDB51: A Large Video Database for Human Motion Recognition**\n\n- homepage: [http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)\n\n**ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding**\n\n- homepage: [http://activity-net.org/](http://activity-net.org/)\n- download: [http://activity-net.org/download.html](http://activity-net.org/download.html)\n- github: [https://github.com/activitynet](https://github.com/activitynet)\n\n**Sports-1M**\n\n- homepage: [https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md](https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md)\n- github: [https://github.com/gtoderici/sports-1m-dataset/](https://github.com/gtoderici/sports-1m-dataset/)\n- thumbnails: [http://cs.stanford.edu/people/karpathy/deepvideo/classes.html](http://cs.stanford.edu/people/karpathy/deepvideo/classes.html)\n\n**Charades Dataset**\n\n- intro: This dataset guides our research into unstructured video activity recogntion and commonsense reasoning for daily human activities.\n- intro: The dataset contains 66,500 temporal annotations for 157 action classes, \n41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos.\n- homepage: [http://allenai.org/plato/charades/](http://allenai.org/plato/charades/)\n\n**FCVID: Fudan-Columbia Video Dataset**\n\n- homepage: [http://bigvid.fudan.edu.cn/FCVID/](http://bigvid.fudan.edu.cn/FCVID/)\n\n**YouTube-8M: A Large-Scale Video Classification Benchmark**\n\n- homepage: [http://research.google.com/youtube8m/](http://research.google.com/youtube8m/)\n- arxiv: [http://arxiv.org/abs/1609.08675](http://arxiv.org/abs/1609.08675)\n\n**stabilized video frames**\n\n- intro: 9 TB, 35,000,000 clips, 32 frames\n- intro: Generating Videos with Scene Dynamics\n- homepage: [http://web.mit.edu/vondrick/tinyvideo/#data](http://web.mit.edu/vondrick/tinyvideo/#data)\n\n**The Kinetics Human Action Video Dataset**\n\n- intro: Google\n- homepage: [https://deepmind.com/research/open-source/open-source-datasets/kinetics/](https://deepmind.com/research/open-source/open-source-datasets/kinetics/)\n- arxiv: [https://arxiv.org/abs/1705.06950](https://arxiv.org/abs/1705.06950)\n\n**e-Lab Video Data Set(s)**\n\n- intro: \"Currently, e-VDS35 has 35 classes and a total of 2050 videos of roughly 10 seconds each (see histogram below). We are aiming to collect overall 1750 (50 × 35) videos with your help.\"\n- homepage: [https://engineering.purdue.edu/elab/eVDS](https://engineering.purdue.edu/elab/eVDS)\n\n**Video Dataset Overview**\n\n- intro: Sortable and searchable compilation of video dataset\n- arxiv: [https://www.di.ens.fr/~miech/datasetviz/](https://www.di.ens.fr/~miech/datasetviz/)\n\n# Scene\n\n**SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth**\n\n- intro: Imperial College London\n- project page: [https://robotvault.bitbucket.org/scenenet-rgbd.html](https://robotvault.bitbucket.org/scenenet-rgbd.html)\n- github: [https://arxiv.org/abs/1612.05079](https://arxiv.org/abs/1612.05079)\n- github: [https://github.com/jmccormac/pySceneNetRGBD](https://github.com/jmccormac/pySceneNetRGBD)\n\n# Autonomous Driving\n\n**BDD: Berkely Deep Drive**\n\n- intro: 100,000 HD video sequences of over 1,100-hour driving experience across many different times in the day, \nweather conditions, and driving scenarios\n- homepage: [http://bdd-data.berkeley.edu/](http://bdd-data.berkeley.edu/)\n- github: [https://github.com/ucbdrive/bdd-data](https://github.com/ucbdrive/bdd-data)\n\n# OCR\n\n**COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images**\n\n- homepage: [http://vision.cornell.edu/se3/coco-text/](http://vision.cornell.edu/se3/coco-text/)\n- arxiv: [http://arxiv.org/abs/1601.07140](http://arxiv.org/abs/1601.07140)\n\n**Chinese Text in the Wild**\n\n- intro: 32,285 high resolution images, 1,018,402 character instances, 3,850 character categories, 6 kinds of attributes\n- homepage: [https://ctwdataset.github.io/](https://ctwdataset.github.io/)\n- arxiv: [https://arxiv.org/abs/1803.00085](https://arxiv.org/abs/1803.00085)\n\n**ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views**\n\n- arxiv: [https://arxiv.org/abs/1903.10412](https://arxiv.org/abs/1903.10412)\n- github: [https://github.com/chongshengzhang/shopsign](https://github.com/chongshengzhang/shopsign)\n\n# Retrieval\n\nOxford5k\n\nParis6k\n\nOxford105k\n\nUKB\n\n**NUS-WIDE**\n\n**ImageNet-YahooQA**\n\n**University-1652**: \n\n![](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/Data.jpg)\n[[Paper]](https://arxiv.org/abs/2002.12186) \n[[Explore Drone-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_drone.jpg?raw=true)\n[[Explore Satellite-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_satellite.jpg?raw=true)\n[[Explore Street-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_street.jpg?raw=true)\n[[Video Sample]](https://www.youtube.com/embed/dzxXPp8tVn4?vq=hd1080)\n[[中文介绍]](https://zhuanlan.zhihu.com/p/110987552)\n- Dataset and Baseline Code: https://github.com/layumi/University1652-Baseline\n\n**DeepFashion: In-shop Clothes Retrieval**\n\n- intro: 7,982 number of clothing items; 52,712 number of in-shop clothes images, and ~200,000 cross-pose/scale pairs; Each image is annotated by bounding box, clothing type and pose type.\n- homepage: [http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html)\n\n# Person Re-ID\n\n| Dataset            | Description                                                              |\n| :----------------: | :---------------------------------------------------------------------:  |\n| CUHK01             | 971 identities, 3884 images, manually cropped                            |\n| CUHK02             | 1816 identities, 7264 images, manually cropped                           |\n| CUHK03             | 1360 identities, 13164 images, manually cropped + automatically detected |\n\n**Person Re-identification Datasets**\n\n- homepage: [http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html](http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html)\n- github: [https://github.com/RSL-NEU/person-reid-benchmark](https://github.com/RSL-NEU/person-reid-benchmark)\n\n**CUHK Person Re-identification Datasets**\n\n[http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html](http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html)\n\n**PRW (Person Re-identification in the Wild) Dataset**\n\n![](http://www.liangzheng.com.cn/Project/pipeline_prw.png)\n\n- homepage: [http://www.liangzheng.com.cn/Project/project_prw.html](http://www.liangzheng.com.cn/Project/project_prw.html)\n- github: [https://github.com/liangzheng06/PRW-baseline](https://github.com/liangzheng06/PRW-baseline)\n\n**Person Re-identification in the Wild**\n\n- intro: CVPR 2017 spotlight\n- arxiv: [https://arxiv.org/abs/1604.02531](https://arxiv.org/abs/1604.02531)\n\n**DukeMTMC-reID**\n\n- intro: DukeMTMC-reID is a subset of the DukeMTMC for image-based re-identification, in the format of the Market-1501 dataset\n- intro: 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images\n- github: [https://github.com/layumi/DukeMTMC-reID_evaluation](https://github.com/layumi/DukeMTMC-reID_evaluation)\n\n**DukeMTMC4ReID**\n\n- intro: DukeMTMC4ReID dataset\n- github: [https://github.com/NEU-Gou/DukeReID](https://github.com/NEU-Gou/DukeReID)\n\n**Person Re-ID (PRID) Dataset 2011**\n\n[https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/)\n\n**MARS (Motion Analysis and Re-identification Set) Dataset**\n\n- intro: an extension of the Market-1501 dataset\n- homepage: [http://www.liangzheng.com.cn/Project/project_mars.html](http://www.liangzheng.com.cn/Project/project_mars.html)\n- github: [https://github.com/liangzheng06/MARS-evaluation](https://github.com/liangzheng06/MARS-evaluation)\n\n**X-MARS Reordering of the MARS Dataset for Image to Video Evaluation**\n\n- intro: This repository provides the X-MARS dataset splits for image to video/tracklet evaluation\n- github: [https://github.com/andreas-eberle/x-mars](https://github.com/andreas-eberle/x-mars)\n\n**MSMT17**\n\n- intro: 15-camera (12 outdoor cameras, 3 indoor cameras), 4,101 Identities, 126,441 BBoxes\n- homepage: [http://www.pkuvmc.com/publications/longhui.html](http://www.pkuvmc.com/publications/longhui.html)\n- soa: [http://www.pkuvmc.com/publications/state_of_the_art.html](http://www.pkuvmc.com/publications/state_of_the_art.html)\n\n**Labeled Pedestrian in the Wild**\n\n- intro: train/test identities: 1,975/756\n- homepage: [http://liuyu.us/dataset/lpw/](http://liuyu.us/dataset/lpw/)\n\n**SenseReID**\n\n[https://drive.google.com/file/d/0B56OfSrVI8hubVJLTzkwV2VaOWM/view](https://drive.google.com/file/d/0B56OfSrVI8hubVJLTzkwV2VaOWM/view)\n\n**3DPeS**\n\n[http://www.openvisor.org/3dpes.asp](http://www.openvisor.org/3dpes.asp)\n\n**iQIYI-VID: A Large Dataset for Multi-modal Person Identification**\n\n[https://arxiv.org/abs/1811.07548](https://arxiv.org/abs/1811.07548)\n\n# Fashion\n\n**Large-scale Fashion (DeepFashion) Database**\n\n- intro: Attribute Prediction, Consumer-to-shop Clothes Retrieval, In-shop Clothes Retrieval, and Landmark Detection\n- homepage: [http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html)\n\n**Apparel classification with Style**\n\n![](http://people.ee.ethz.ch/~lbossard/projects/accv12/img/motivation.jpg)\n\n- intro: 15 clothing classes, 88951 images\n- homepage: [http://people.ee.ethz.ch/~lbossard/projects/accv12/index.html](http://people.ee.ethz.ch/~lbossard/projects/accv12/index.html)\n\n# Attribute Datasets\n\n**Attribute Datasets**\n\n- intro: in total 41,585 pedestrian samples, each of which is annotated with 72 attributes \nas well as viewpoints, occlusions, body parts information\n- homepage: [https://www.ecse.rpi.edu/homepages/cvrl/database/AttributeDataset.htm](https://www.ecse.rpi.edu/homepages/cvrl/database/AttributeDataset.htm)\n\n## Pedestrian Attribute Recognition\n\n**A Richly Annotated Dataset for Pedestrian Attribute Recognition**\n\n- homepage: [http://rap.idealtest.org/](http://rap.idealtest.org/)\n- arxiv: [https://arxiv.org/abs/1603.07054](https://arxiv.org/abs/1603.07054)\n\n**Pedestrian Attribute Recognition At Far Distance**\n\n- intro: PEdesTrian Attribute (PETA)\n- homepage: [http://mmlab.ie.cuhk.edu.hk/projects/PETA.html](http://mmlab.ie.cuhk.edu.hk/projects/PETA.html)\n- paper: [http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf](http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf)\n\n**Market-1501_Attribute**\n\n- github: [https://github.com/vana77/Market-1501_Attribute](https://github.com/vana77/Market-1501_Attribute)\n- blog: [https://vana77.github.io](https://vana77.github.io)\n\n**DukeMTMC-attribute**\n\n- github: [https://github.com/vana77/DukeMTMC-attribute](https://github.com/vana77/DukeMTMC-attribute)\n- blog: [https://vana77.github.io](https://vana77.github.io)\n\n**Parse27k**\n\n- intro: Pedestrian Attribute Recognition in Sequences\n- intro: >27,000 annotated pedestrians, 10 attributes\n- homepage: [https://www.vision.rwth-aachen.de/page/parse27k](https://www.vision.rwth-aachen.de/page/parse27k)\n- tools: [https://github.com/psudowe/parse27k_tools](https://github.com/psudowe/parse27k_tools)\n\n# Tracking\n\n**UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking**\n\n- homepage: [http://detrac-db.rit.albany.edu/](http://detrac-db.rit.albany.edu/)\n- arxiv: [https://arxiv.org/abs/1511.04136](https://arxiv.org/abs/1511.04136)\n\n**DukeMTMC: Duke Multi-Target, Multi-Camera Tracking Project**\n\n- intro: DukeMTMC aims to accelerate advances in multi-target multi-camera tracking. It provides a tracking system that works within and across cameras, a new large scale HD video data set recorded by 8 synchronized cameras with more than 7,000 single camera trajectories and over 2,000 unique identities\n- homepage: [http://vision.cs.duke.edu/DukeMTMC/](http://vision.cs.duke.edu/DukeMTMC/)\n\n**The WILDTRACK Seven-Camera HD Dataset**\n\n[https://cvlab.epfl.ch/data/wildtrack](https://cvlab.epfl.ch/data/wildtrack)\n\n**GOT-10k: Generic Object Tracking Benchmark**\n\n- intro: A large, high-diversity, one-shot database for generic object tracking in the wild\n- project page: [http://got-10k.aitestunion.com/](http://got-10k.aitestunion.com/)\n- github: [https://github.com/got-10k/toolkit](https://github.com/got-10k/toolkit)\n\n# Color Classification\n\n**Vehicle Color Recognition on an Urban Road by Feature Context**\n\n[http://mclab.eic.hust.edu.cn/~pchen/project.html](http://mclab.eic.hust.edu.cn/~pchen/project.html)\n\n# License Plate Detection and Recognition\n\n**Application-Oriented License Plate (AVOP) Database**\n\n[http://aolpr.ntust.edu.tw/lab/download.html](http://aolpr.ntust.edu.tw/lab/download.html)\n\n**CCPD: Chinese City Parking Dataset**\n\n- paper: [http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf](http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf)\n- github: [https://github.com/detectRecog/CCPD](https://github.com/detectRecog/CCPD)\n- dataset: [https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view](https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view)\n\n# Face Anti-Spoofing\n\n**CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.12342](https://arxiv.org/abs/2007.12342)\n- github: [https://github.com/Davidzhangyuanhan/CelebA-Spoof](https://github.com/Davidzhangyuanhan/CelebA-Spoof)\n\n# Tools\n\n**VoTT: Visual Object Tagging Tool 1.5**\n\n- intro: Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos\n- github: [https://github.com/Microsoft/VoTT](https://github.com/Microsoft/VoTT)\n\n**LabelImg: a graphical image annotation tool and label object bounding boxes in images**\n\n![](https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo2.png)\n\n- github: [https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg)\n\n**Pychet Labeller**\n\n- intro: A python based annotation/labelling toolbox for images. \nThe program allows the user to annotate individual objects in images.\n- github: [https://github.com/sbargoti/pychetlabeller](https://github.com/sbargoti/pychetlabeller)\n\n**ml-pyxis: Tool for reading and writing datasets of tensors (numpy.ndarray) with MessagePack and Lightning Memory-Mapped Database (LMDB).**\n\n- intro: Tool for reading and writing datasets of tensors in a Lightning Memory-Mapped Database (LMDB). \nDesigned to manage machine learning datasets with fast reading speeds.\n- github: [https://github.com/vicolab/ml-pyxis](https://github.com/vicolab/ml-pyxis)\n\n**Open Image Dataset downloader**\n\n- github: [https://github.com/e-lab/crawl-dataset](https://github.com/e-lab/crawl-dataset)\n\n**BBox-Label-Tool**\n\n- intro: A simple tool for labeling object bounding boxes in images\n- github: [https://github.com/puzzledqs/BBox-Label-Tool](https://github.com/puzzledqs/BBox-Label-Tool)\n\n**Data Labeler for Video**\n\n- intro: A GUI tool for conveniently label the objects in video, using the powerful object tracking.\n- github: [https://github.com//hahnyuan/video_labeler](https://github.com//hahnyuan/video_labeler)\n\n**Computer Vision Annotation Tool (CVAT)**\n\n![](https://raw.githubusercontent.com/opencv/cvat/master/cvat/apps/documentation/static/documentation/images/gif003.gif)\n\n- intro: Computer Vision Annotation Tool (CVAT) is a web-based tool which helps to annotate video and images for Computer Vision algorithms\n- github: [https://github.com/opencv/cvat](https://github.com/opencv/cvat)\n\n# Artist\n\n**BAM! The Behance Artistic Media Dataset**\n\n- intro: 2.5M artwork urls, 393K attribute labels, 74K short image descriptions/captions\n- project page: [https://bam-dataset.org/](https://bam-dataset.org/)\n- arxiv: [https://arxiv.org/abs/1704.08614](https://arxiv.org/abs/1704.08614)\n\n# Resources\n\n**CV Datasets on the web**\n\n[http://www.cvpapers.com/datasets.html](http://www.cvpapers.com/datasets.html)\n\n**Awesome Public Datasets**\n\n- intro: An awesome list of high-quality open datasets in public domains (on-going). By everyone, for everyone!\n- github: [https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets)\n\n**Machine Learning Repository**\n\n[https://archive.ics.uci.edu/ml/datasets.html](https://archive.ics.uci.edu/ml/datasets.html)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","title":"Features"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_vision\ntitle: Features\ndate: 2015-10-09\n---\n\n# HOG: Histogram of Oriented Gradients\n\n**Optimizing and Parallelizing Histogram of Oriented Gradients Computation**\n\n- project page: [http://bwasti.com/418](http://bwasti.com/418)\n\n**Histogram of Oriented Gradients**\n\n- notes: [http://www.cs.duke.edu/courses/fall15/compsci527/notes/hog.pdf](http://www.cs.duke.edu/courses/fall15/compsci527/notes/hog.pdf)\n\n**HOG Person Detector Tutorial**\n\n- blog: [https://chrisjmccormick.wordpress.com/2013/05/09/hog-person-detector-tutorial/](https://chrisjmccormick.wordpress.com/2013/05/09/hog-person-detector-tutorial/)\n\n# ICF: Integral Channel Features and Variants\n\n**Integral Channel Features (BMVC 2009)**\n\n- paper: [http://vision.ucsd.edu/sites/default/files/dollarBMVC09ChnFtrs_0.pdf](http://vision.ucsd.edu/sites/default/files/dollarBMVC09ChnFtrs_0.pdf)\n- codes: [http://libccv.org/doc/doc-icf/](http://libccv.org/doc/doc-icf/)\n\n**Pedestrian Detection at 100 Frames per Second (CVPR 2012)**\n\n**Seeking the Strongest Rigid Detector (CVPR 2013)**\n\n# ACF: Aggregate Channel Features\n\n- github: [https://github.com/pdollar/toolbox](https://github.com/pdollar/toolbox)\n\n# LDCF: \n\n- github: [https://github.com/pdollar/toolbox](https://github.com/pdollar/toolbox)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","title":"Recognition, Detection, Segmentation and Tracking"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_vision\ntitle: Recognition, Detection, Segmentation and Tracking\ndate: 2015-10-09\n---\n\n# Classification / Recognition\n\n**Generalized Hierarchical Matching for Sub-category Aware Object Classification (VOC2012 classification task winner)**\n\nslides: [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/workshop/Towards_VOC2012_NUSPSL.pdf](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/workshop/Towards_VOC2012_NUSPSL.pdf)\n\n## License Plate Recognition\n\n- homepage: [http://www.openalpr.com/](http://www.openalpr.com/)\n- github: [https://github.com/openalpr/openalpr](https://github.com/openalpr/openalpr)\n- tech review: [http://arstechnica.com/business/2015/12/new-open-source-license-plate-reader-software-lets-you-make-your-own-hot-list/](http://arstechnica.com/business/2015/12/new-open-source-license-plate-reader-software-lets-you-make-your-own-hot-list/)\n\n# Detection\n\n**Contextualizing Object Detection and Classification**\n\n- intro: CVPR 2010\n- paper: [http://www.lv-nus.org/papers/2011/cvpr2010-context_final.pdf](http://www.lv-nus.org/papers/2011/cvpr2010-context_final.pdf)\n\n**Diagnosing Error in Object Detectors**\n\n- author: Derek Hoiem, Yodsawalai Chodpathumwan, and Qieyun Dai\n- project page: [http://web.engr.illinois.edu/~dhoiem/projects/detectionAnalysis/](http://web.engr.illinois.edu/~dhoiem/projects/detectionAnalysis/)\n- paper: [http://web.engr.illinois.edu/~dhoiem/publications/eccv2012_detanalysis_derek.pdf](http://web.engr.illinois.edu/~dhoiem/publications/eccv2012_detanalysis_derek.pdf)\n- code+data: [http://web.engr.illinois.edu/~dhoiem/projects/counter.php?Down=detectionAnalysis/detectionAnalysis_eccv12_v2.zip&Save=detectionAnalysis_eccv12](http://web.engr.illinois.edu/~dhoiem/projects/counter.php?Down=detectionAnalysis/detectionAnalysis_eccv12_v2.zip&Save=detectionAnalysis_eccv12)\n- slides: [http://web.engr.illinois.edu/~dhoiem/presentations/DetectionAnalysis_ECCV2012.pptx](http://web.engr.illinois.edu/~dhoiem/presentations/DetectionAnalysis_ECCV2012.pptx)\n\n**Integrating Context and Occlusion for Car Detection by Hierarchical And-or Model**\n\n![](http://www.stat.ucla.edu/~boli/projects/context_occlusion/img/demo.png)\n\n- intro: ECCV 2014\n- homepage: [http://www.stat.ucla.edu/~boli/projects/context_occlusion/context_occlusion.html](http://www.stat.ucla.edu/~boli/projects/context_occlusion/context_occlusion.html)\n- paper: [http://www.stat.ucla.edu/~boli/projects/context_occlusion/publications/CarAOG_ECCV2014.pdf](http://www.stat.ucla.edu/~boli/projects/context_occlusion/publications/CarAOG_ECCV2014.pdf)\n- code: [http://www.stat.ucla.edu/~boli/projects/context_occlusion/publications/RGM_Release1.zip](http://www.stat.ucla.edu/~boli/projects/context_occlusion/publications/RGM_Release1.zip)\n\n**Learning And-Or Models to Represent Context and Occlusion for Car Detection and Viewpoint Estimation**\n\n- homepage: [http://www.stat.ucla.edu/~boli/projects/context_occlusion/context_occlusion.html](http://www.stat.ucla.edu/~boli/projects/context_occlusion/context_occlusion.html)\n- arxiv: [http://arxiv.org/abs/1501.07359](http://arxiv.org/abs/1501.07359)\n- code: [http://www.stat.ucla.edu/~boli/projects/context_occlusion/publications/RGM_Release1.zip](http://www.stat.ucla.edu/~boli/projects/context_occlusion/publications/RGM_Release1.zip)\n\n**Quickest Moving Object Detection**\n\n- arxiv: [http://arxiv.org/abs/1605.07586](http://arxiv.org/abs/1605.07586)\n\n**Automatic detection of moving objects in video surveillance**\n\n- arxiv: [http://arxiv.org/abs/1608.03617](http://arxiv.org/abs/1608.03617)\n\n**Real-time Webcam Barcode Detection with OpenCV and C++**\n\n![](http://www.codepool.biz/wp-content/uploads/2016/05/dbr_opencv_cplusplus.png)\n\n- blog: [http://www.codepool.biz/webcam-barcode-detection-opencv-cplusplus.html](http://www.codepool.biz/webcam-barcode-detection-opencv-cplusplus.html)\n- github: [https://github.com/dynamsoftlabs/cplusplus-webcam-barcode-reader](https://github.com/dynamsoftlabs/cplusplus-webcam-barcode-reader)\n\n**The Role of Context Selection in Object Detection**\n\n- arxiv: [http://arxiv.org/abs/1609.02948](http://arxiv.org/abs/1609.02948)\n\n**PersonRank: Detecting Important People in Images**\n\n[https://arxiv.org/abs/1711.01984](https://arxiv.org/abs/1711.01984)\n\n## DPM and DPM variants\n\n**Object detection with discriminatively trained part based models (DPM)**\n\n- paper: [https://www.cs.berkeley.edu/~rbg/papers/Object-Detection-with-Discriminatively-Trained-Part-Based-Models--Felzenszwalb-Girshick-McAllester-Ramanan.pdf](https://www.cs.berkeley.edu/~rbg/papers/Object-Detection-with-Discriminatively-Trained-Part-Based-Models--Felzenszwalb-Girshick-McAllester-Ramanan.pdf)\n- project page: [http://www.cs.berkeley.edu/~rbg/latent/](http://www.cs.berkeley.edu/~rbg/latent/)\n- FAQ: [http://people.cs.uchicago.edu/~rbg/latent/voc-release5-faq.html](http://people.cs.uchicago.edu/~rbg/latent/voc-release5-faq.html)\n- github: [https://github.com/rbgirshick/voc-dpm](https://github.com/rbgirshick/voc-dpm)\n\n**30hz object detection with dpm v5**\n\n- paper: [http://web.engr.illinois.edu/~msadegh2/papers/DPM30Hz.pdf](http://web.engr.illinois.edu/~msadegh2/papers/DPM30Hz.pdf)\n- project page: [http://vision.cs.illinois.edu/DPM30Hz/](http://vision.cs.illinois.edu/DPM30Hz/)\n- slides: [http://web.engr.illinois.edu/~msadegh2/papers/DPM30Hz.pptx](http://web.engr.illinois.edu/~msadegh2/papers/DPM30Hz.pptx)\n\n**The fastest deformable part model for object detection**\n\n- paper: [http://www.cbsr.ia.ac.cn/users/jjyan/Fastest_DPM.pdf](http://www.cbsr.ia.ac.cn/users/jjyan/Fastest_DPM.pdf)\n- Result on FDDB: [http://vis-www.cs.umass.edu/fddb/results.html](http://vis-www.cs.umass.edu/fddb/results.html)\n\n**Fast, accurate detection of 100,000 object classes on a single machine**\n\n- paper: [research.google.com/pubs/archive/41104.pdf](research.google.com/pubs/archive/41104.pdf)\n- paper: [https://courses.cs.washington.edu/courses/cse590v/13au/40814.pdf](https://courses.cs.washington.edu/courses/cse590v/13au/40814.pdf)\n\n**Deformable Part Models are Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1409.5403](http://arxiv.org/abs/1409.5403)\n- github: [https://github.com/rbgirshick/DeepPyramid](https://github.com/rbgirshick/DeepPyramid)\n\n**Tensor-based approach to accelerate deformable part models**\n\n[https://arxiv.org/abs/1707.03268](https://arxiv.org/abs/1707.03268)\n\n## Detection in Video\n\n**Expanding Object Detector’s HORIZON: Incremental Learning Framework for Object Detection in Videos**\n\n![](https://www.disneyresearch.com/wp-content/uploads/Expanding-Object-Detector%E2%80%99s-HORIZON-Incremental-Learning-Framework-for-Object-Detection-in-Videos-Image.jpg)\n\n- intro: CVPR 2015\n- project page: [https://www.disneyresearch.com/publication/expanding-object-detectors-horizon/](https://www.disneyresearch.com/publication/expanding-object-detectors-horizon/)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Kuznetsova_Expanding_Object_Detectors_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Kuznetsova_Expanding_Object_Detectors_2015_CVPR_paper.pdf)\n\n## Face Detection\n\n**Build a Face Detection App Using Node.js and OpenCV**\n\n[http://www.sitepoint.com/face-detection-nodejs-opencv/](http://www.sitepoint.com/face-detection-nodejs-opencv/)\n\n**FaceTracker: Real time deformable face tracking in C++ with OpenCV 2**\n\n- github: [https://github.com/kylemcdonald/FaceTracker](https://github.com/kylemcdonald/FaceTracker)\n\n**A Fast and Accurate Unconstrained Face Detector**\n\n![](https://github.com/CitrusRokid/OpenNPD)\n\n- homepage: [http://www.cbsr.ia.ac.cn/users/scliao/projects/npdface/index.html](http://www.cbsr.ia.ac.cn/users/scliao/projects/npdface/index.html)\n- github: [https://github.com/CitrusRokid/OpenNPD](https://github.com/CitrusRokid/OpenNPD)\n\n**libfacedetection: A binary library for face detection in images. You can use it free of charge with any purpose**\n\n- github: [https://github.com/ShiqiYu/libfacedetection](https://github.com/ShiqiYu/libfacedetection)\n\n**jQuery Face Detection Plugin: A jQuery plugin to detect faces on images, videos and canvases**\n\n- website: [http://facedetection.jaysalvat.com/](http://facedetection.jaysalvat.com/)\n- github: [https://github.com/jaysalvat/jquery.facedetection](https://github.com/jaysalvat/jquery.facedetection)\n\n**Spoofing 2D Face Detection: Machines See People Who Aren't There**\n\n- arxiv: [http://arxiv.org/abs/1608.02128](http://arxiv.org/abs/1608.02128)\n\n**Fall-Detection: Human Fall Detection from CCTV camera feed**\n\n![](https://camo.githubusercontent.com/472295e9f092c3a7224b90fe09bfe91c25a102c6/68747470733a2f2f73342e706f7374696d672e6f72672f3439743472656c6f642f67697068792e676966)\n\n- github: [https://github.com/harishrithish7/Fall-Detection](https://github.com/harishrithish7/Fall-Detection)\n\n## Edge detection\n\n**Image-feature-detection-using-Phase-Stretch-Transform**\n\n![](https://upload.wikimedia.org/wikipedia/commons/4/45/PST_edge_detection_on_barbara_image.tif)\n\n- github: [https://github.com/JalaliLabUCLA/Image-feature-detection-using-Phase-Stretch-Transform](https://github.com/JalaliLabUCLA/Image-feature-detection-using-Phase-Stretch-Transform)\n- wikipedia: [https://en.wikipedia.org/wiki/Phase_stretch_transform](https://en.wikipedia.org/wiki/Phase_stretch_transform)\n\n## Object Proposals\n\n**What makes for effective detection proposals?(PAMI 2015)**\n\n- homepage: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/how-good-are-detection-proposals-really/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/how-good-are-detection-proposals-really/)\n- arxiv: [http://arxiv.org/abs/1502.05082](http://arxiv.org/abs/1502.05082)\n- github: [https://github.com/hosang/detection-proposals](https://github.com/hosang/detection-proposals)\n\n**BING++: A Fast High Quality Object Proposal Generator at 100fps**\n\n- arxiv: [http://arxiv.org/abs/1511.04511](http://arxiv.org/abs/1511.04511)\n\n**Object Proposals**\n\n- intro: This is a library/API which can be used to generate bounding box/region proposals \nusing a large number of the existing object proposal approaches.\n- github: [https://github.com/batra-mlp-lab/object-proposals](https://github.com/batra-mlp-lab/object-proposals)\n\n**Segmentation Free Object Discovery in Video**\n\n- arxiv: [http://arxiv.org/abs/1609.00221](http://arxiv.org/abs/1609.00221)\n\n# Segmentation\n\n**Graph Based Image Segmentation**\n\n- project page: [http://cs.brown.edu/~pff/segment/](http://cs.brown.edu/~pff/segment/)\n\n**Pixelwise Image Saliency by Aggregation Complementary Appearance Contrast Measures with Edge-Preserving Coherence**\n\n![](http://vision.sysu.edu.cn/project/PISA/framework.png)\n\n- project page: [http://vision.sysu.edu.cn/project/PISA/](http://vision.sysu.edu.cn/project/PISA/)\n- paper: [http://vision.sysu.edu.cn/project/PISA/PISA_Final.pdf](http://vision.sysu.edu.cn/project/PISA/PISA_Final.pdf)\n- github: [https://github.com/kezewang/pixelwiseImageSaliencyAggregation](https://github.com/kezewang/pixelwiseImageSaliencyAggregation)\n\n**Joint Tracking and Segmentation of Multiple Targets**\n\n- paper: [http://www.milanton.de/files/cvpr2015/cvpr2015-anton.pdf](http://www.milanton.de/files/cvpr2015/cvpr2015-anton.pdf)\n- bitbuckt(Matlab): [https://bitbucket.org/amilan/segtracking](https://bitbucket.org/amilan/segtracking)\n\n**Supervised Evaluation of Image Segmentation Methods**\n\n- project page: [http://www.vision.ee.ethz.ch/~biwiproposals/seism/index.html](http://www.vision.ee.ethz.ch/~biwiproposals/seism/index.html)\n- github: [https://github.com/jponttuset/seism](https://github.com/jponttuset/seism)\n\n## Normalized Cut (N-cut)\n\n## Graph Cut\n\n## Grab Cut\n\n**“GrabCut” — Interactive Foreground Extraction using Iterated Graph Cuts**\n\n- paper: [http://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf](http://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf)\n\n**OpenCV 3.1: Interactive Foreground Extraction using GrabCut Algorithm**\n\n[http://docs.opencv.org/master/d8/d83/tutorial_py_grabcut.html#gsc.tab=0](http://docs.opencv.org/master/d8/d83/tutorial_py_grabcut.html#gsc.tab=0)\n\n## Video Segmentation\n\n**Bilateral Space Video Segmentation**\n\n![](https://graphics.ethz.ch/~perazzif/bvs/files/bvs_teaser.jpg)\n\n- intro: CVPR 2016\n- project page: [https://graphics.ethz.ch/~perazzif/bvs/index.html](https://graphics.ethz.ch/~perazzif/bvs/index.html)\n- github: [https://github.com/owang/BilateralVideoSegmentation](https://github.com/owang/BilateralVideoSegmentation)\n\n# Tracking\n\n**Online Object Tracking: A Benchmark**\n\n- intro: CVPR 2013\n- paper: [http://faculty.ucmerced.edu/mhyang/papers/cvpr13_benchmark.pdf](http://faculty.ucmerced.edu/mhyang/papers/cvpr13_benchmark.pdf)\n\n**Object Tracking Benchmark**\n\n- intro: PAMI 2015\n- paper: [http://faculty.ucmerced.edu/mhyang/papers/pami15_tracking_benchmark.pdf](http://faculty.ucmerced.edu/mhyang/papers/pami15_tracking_benchmark.pdf)\n\n**Visual Tracker Benchmark**\n\n[http://cvlab.hanyang.ac.kr/tracker_benchmark/benchmark_v10.html](http://cvlab.hanyang.ac.kr/tracker_benchmark/benchmark_v10.html)\n\n**MEEM: Robust Tracking via Multiple Experts using Entropy Minimization**\n\n![](http://cs-people.bu.edu/jmzhang/MEEM/frontpage.png)\n\n- intro: ECCV 2014\n- project page(code+data): [http://cs-people.bu.edu/jmzhang/MEEM/MEEM.html](http://cs-people.bu.edu/jmzhang/MEEM/MEEM.html)\n- paper: [http://cs-people.bu.edu/jmzhang/MEEM/MEEM-eccv-preprint.pdf](http://cs-people.bu.edu/jmzhang/MEEM/MEEM-eccv-preprint.pdf)\n- code: [http://cs-people.bu.edu/jmzhang/MEEM/MEEM_v1_release.zip](http://cs-people.bu.edu/jmzhang/MEEM/MEEM_v1_release.zip)\n- code: [http://cs-people.bu.edu/jmzhang/MEEM/MEEM_v1.1_release.zip](http://cs-people.bu.edu/jmzhang/MEEM/MEEM_v1.1_release.zip)\n\n**Struck: Structured Output Tracking with Kernels**\n\n- intro: ICCV 2011\n- paper: [http://mftp.mmcheng.net/Papers/StruckPAMI.pdf](http://mftp.mmcheng.net/Papers/StruckPAMI.pdf)\n- paper: [http://www.robots.ox.ac.uk/~tvg/publications/2015/struck-author.pdf](http://www.robots.ox.ac.uk/~tvg/publications/2015/struck-author.pdf)\n- paper: [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7360205](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7360205)\n- slides: [http://vision.stanford.edu/teaching/cs231b_spring1415/slides/struck_meng.pdf](http://vision.stanford.edu/teaching/cs231b_spring1415/slides/struck_meng.pdf)\n- github: [https://github.com/samhare/struck](https://github.com/samhare/struck)\n- github: [https://github.com/gnebehay/STRUCK](https://github.com/gnebehay/STRUCK)\n\n**High-Speed Tracking with Kernelized Correlation Filters**\n\n- intro: TPAMI 2015\n- project page: [http://www.robots.ox.ac.uk/~joao/circulant/index.html](http://www.robots.ox.ac.uk/~joao/circulant/index.html)\n- paper: [http://www.robots.ox.ac.uk/~joao/publications/henriques_tpami2015.pdf](http://www.robots.ox.ac.uk/~joao/publications/henriques_tpami2015.pdf)\n- github: [https://github.com/foolwood/KCF](https://github.com/foolwood/KCF)\n- github: [https://github.com/vojirt/kcf](https://github.com/vojirt/kcf)\n\n**Learning to Track: Online Multi-Object Tracking by Decision Making**\n\n![](http://cvgl.stanford.edu/projects/MDP_tracking/MDP.png)\n\n- intro: ICCV 2015\n- homepage: [http://cvgl.stanford.edu/projects/MDP_tracking/](http://cvgl.stanford.edu/projects/MDP_tracking/)\n- paper: [http://cvgl.stanford.edu/papers/xiang_iccv15.pdf](http://cvgl.stanford.edu/papers/xiang_iccv15.pdf)\n- slides: [https://yuxng.github.io/Xiang_ICCV15_12162015.pdf](https://yuxng.github.io/Xiang_ICCV15_12162015.pdf)\n- github: [https://github.com/yuxng/MDP_Tracking](https://github.com/yuxng/MDP_Tracking)\n\n**Joint Tracking and Segmentation of Multiple Targets**\n\n- paper: [http://www.milanton.de/files/cvpr2015/cvpr2015-anton.pdf](http://www.milanton.de/files/cvpr2015/cvpr2015-anton.pdf)\n- bitbuckt(Matlab): [https://bitbucket.org/amilan/segtracking](https://bitbucket.org/amilan/segtracking)\n\n**Robust Visual Tracking Via Consistent Low-Rank Sparse Learning**\n\n![](http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/Project_Tianzhu/zhang_IJCV14/material/paper_framework.jpg)\n\n- project page: [http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/Project_Tianzhu/zhang_IJCV14/Robust%20Visual%20Tracking%20Via%20Consistent%20Low-Rank%20Sparse.html](http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/Project_Tianzhu/zhang_IJCV14/Robust%20Visual%20Tracking%20Via%20Consistent%20Low-Rank%20Sparse.html)\n- paper: [http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/tianzhu%20zhang_files/Conference%20Papers/ECCV12_zhang_Low-Rank%20Sparse%20Learning.pdf](http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/tianzhu%20zhang_files/Conference%20Papers/ECCV12_zhang_Low-Rank%20Sparse%20Learning.pdf)\n\n**Staple: Complementary Learners for Real-Time Tracking**\n\n![](http://www.robots.ox.ac.uk/~luca/stuff/pipeline_horizontal.png)\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1512.01355](http://arxiv.org/abs/1512.01355)\n- homepage: [http://www.robots.ox.ac.uk/~luca/staple.html](http://www.robots.ox.ac.uk/~luca/staple.html)\n- github: [https://github.com/bertinetto/staple](https://github.com/bertinetto/staple)\n\n**Simple Online and Realtime Tracking**\n\n- intro: ICIP 2016. SORT = Simple Online and Realtime Tracking\n- intro: Simple, online, and realtime tracking of multiple objects in a video sequence\n- keywords: Kalman Filter, Hungarian algorithm, 260 Hz\n- arxiv: [http://arxiv.org/abs/1602.00763](http://arxiv.org/abs/1602.00763)\n- github: [https://github.com/abewley/sort](https://github.com/abewley/sort)\n- demo: [https://motchallenge.net/movies/ETH-Linthescher-SORT.mp4](https://motchallenge.net/movies/ETH-Linthescher-SORT.mp4)\n\n**Visual Tracking via Reliable Memories**\n\n- arxiv: [http://arxiv.org/abs/1602.01887](http://arxiv.org/abs/1602.01887)\n\n**Tracking Completion**\n\n- arxiv: [http://arxiv.org/abs/1608.08171](http://arxiv.org/abs/1608.08171)\n\n**Real-Time Visual Tracking: Promoting the Robustness of Correlation Filter Learning**\n\n- arxiv: [http://arxiv.org/abs/1608.08173](http://arxiv.org/abs/1608.08173)\n\n## Multi-camera Multi-Object Tracking\n\n**Multi-camera Multi-Object Tracking**\n\n[https://arxiv.org/abs/1709.07065](https://arxiv.org/abs/1709.07065)\n\n## Projects\n\n**Benchmark Results of Correlation Filters**\n\n- intro: Collect and share results for correlation filter trackers.\n- github: [https://github.com/HakaseH/CF_benchmark_results](https://github.com/HakaseH/CF_benchmark_results)\n\n**JS-face-tracking-demo**\n\n![](https://camo.githubusercontent.com/3cde9346fa47e7985235b22a9c032d149dcf7c6d/68747470733a2f2f692e696d6775722e636f6d2f6a36424f4b39662e676966)\n\n- demo: [https://kdzwinel.github.io/JS-face-tracking-demo/](https://kdzwinel.github.io/JS-face-tracking-demo/)\n- github: [https://github.com/kdzwinel/JS-face-tracking-demo](https://github.com/kdzwinel/JS-face-tracking-demo)\n\n**VOTR: Visual Object Tracking Repository**\n\n- intro: This repository aims at collecting state-of-the-art tracking algorithms that are freely available.\n- github: [https://github.com/gnebehay/VOTR](https://github.com/gnebehay/VOTR)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","title":"Working on OpenCV"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_vision\ntitle: Working on OpenCV\ndate: 2015-12-25\n---\n\n**Installation in Windows**\n\n[http://docs.opencv.org/2.4/doc/tutorials/introduction/windows_install/windows_install.html](http://docs.opencv.org/2.4/doc/tutorials/introduction/windows_install/windows_install.html)\n\n**OpenCV 2.4.9 + CUDA 6.5 + Visual Studio 2013 [中文教程]**\n\n- blog: [http://tieba.baidu.com/p/3329042929](http://tieba.baidu.com/p/3329042929)\n- eng: [http://initialneil.wordpress.com/2014/09/25/opencv-2-4-9-cuda-6-5-visual-studio-2013/](http://initialneil.wordpress.com/2014/09/25/opencv-2-4-9-cuda-6-5-visual-studio-2013/)\n\n**OpenCV 2.4 Cheat Sheet (C++)**\n\n[http://docs.opencv.org/3.0-last-rst/opencv_cheatsheet.pdf](http://docs.opencv.org/3.0-last-rst/opencv_cheatsheet.pdf)\n\n**OpenCV Main Page**\n\n[http://docs.opencv.org/master/index.html](http://docs.opencv.org/master/index.html)\n\n[https://fossies.org/dox/opencv-3.1.0/index.html](https://fossies.org/dox/opencv-3.1.0/index.html)\n\n**OpenCV 3.0 Speeding Up**\n\n- intro: T--API, OpenCL, UMat, HAL\n- slides: [http://www.cvrobot.net/wp-content/uploads/2015/06/OpenCV3_CVPR_2015_Speed.pptx](http://www.cvrobot.net/wp-content/uploads/2015/06/OpenCV3_CVPR_2015_Speed.pptx)\n\n**Learning OpenCV: OpenCV examples and tutorials ( C++ / Python )**\n\n- homepage: [http://www.learnopencv.com/](http://www.learnopencv.com/)\n\n**Reading and Writing Images**\n\n[http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html](http://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html)\n\n**Reading and Writing Video**\n\n[http://docs.opencv.org/3.0-beta/modules/videoio/doc/reading_and_writing_video.html](http://docs.opencv.org/3.0-beta/modules/videoio/doc/reading_and_writing_video.html)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","title":"Use FFmpeg to Capture I Frames of Video"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_vision\ntitle: Use FFmpeg to Capture I Frames of Video\ndate: 2016-03-03\n---\n\nFirst we need to download ffmpeg.exe from: [http://ffmpeg.zeranoe.com/builds/](http://ffmpeg.zeranoe.com/builds/)\n\nUse the commands below to extract all key frames (I, P, B) and their corresponding timecodes:\n\nOn Linux, it can be an one-off accomplishment:\n\n```\nffmpeg -i yourvideo.mp4 -vf select=\"eq(pict_type\\,PICT_TYPE_I)\" -vsync 2 -s 160x90 -f image2 thumbnails-%02d.jpeg -loglevel debug 2>&1 | grep \"pict_type:I -> select:1\" | cut -d \" \" -f 6 - > ffmpeg_decode-info.txt\n```\n\nOn Windows:\n\n```\nffmpeg.exe -i yourvideo.mp4 -vf select='eq(pict_type\\,I)' -vsync 2 -s 160x90 -f image2 thumbnails-%02d.jpeg -loglevel debug 2>&1 | findstr \"pict_type:I\" > ffmpeg_decode_info.txt\n```\n\nNow we will get a text file filled with formated text lines like:\n\n```\n\\[Parsed_select_0 @ 05253d60\\] n:134.000000 pts:135000.000000 t:5.400000 key:1 interlace_type:P pict_type:I scene:nan -> select:1.000000 select_out:0\n```\n\nSince we need only I frames' index, we can split each text line and get the target columns:\n\n```\n(for /f \"tokens=5 delims= \" %i in (ffmpeg_decode_info.txt) DO echo %i) > ffmpeg_iframe_index.txt\n```\n\n(Note: the file name \"ffmpeg_decode_info.txt\" should not contain any space. \nOtherwise we should add another option `usebackq`:\n\n```\n(for /f \"usebackq tokens=5 delims= \" %i in (\"ffmpeg decode info.txt\") DO echo %i) > ffmpeg_iframe_index.txt\n```\n\n)\n\nSometimes we would find some strange lines insert to ffmpeg_decode_info.txt \n(I don't know what it means, its insert location even whether it is inserted or not is unpredictable)\n\n```\nframe=   57 fps=2.7 q=2.1 size=N/A time=00:00:54.01 bitrate=N/A speed=2.56x   \n```\n\nConflict will appear if we don't add some special handling. \nUsually this line is appended with the normal text line that we need, just like below:\n\n```\nframe=   57 fps=2.7 q=2.1 size=N/A time=00:00:54.01 bitrate=N/A speed=2.56x   **\\r**\\[Parsed_select_0 @ 05253d60\\] n:134.000000 pts:135000.000000 t:5.400000 key:1 interlace_type:P pict_type:I scene:nan -> select:1.000000 select_out:0\n```\n\nSplit it by **\\r** will be Okay.\n\n# Reference\n\n[http://forum.doom9.org/archive/index.php/t-163553.html](http://forum.doom9.org/archive/index.php/t-163553.html)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","title":"Adversarial Attacks and Defences"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Adversarial Attacks and Defences\ndate: 2015-10-09\n---\n\n# Papers\n\n**Intriguing properties of neural networks**\n\n- arxiv: [http://arxiv.org/abs/1312.6199](http://arxiv.org/abs/1312.6199)\n\n**Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images**\n\n- intro: CVPR 2015\n- arxiv: [http://arxiv.org/abs/1412.1897](http://arxiv.org/abs/1412.1897)\n- github: [https://github.com/Evolving-AI-Lab/fooling/](https://github.com/Evolving-AI-Lab/fooling/)\n\n**Explaining and Harnessing Adversarial Examples**\n\n- intro: primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature\n- arxiv: [http://arxiv.org/abs/1412.6572](http://arxiv.org/abs/1412.6572)\n\n**Distributional Smoothing with Virtual Adversarial Training**\n\n- arxiv: [http://arxiv.org/abs/1507.00677](http://arxiv.org/abs/1507.00677)\n- github: [https://github.com/takerum/vat](https://github.com/takerum/vat)\n\n**Confusing Deep Convolution Networks by Relabelling**\n\n- arxiv: [http://arxiv.org/abs/1510.06925v1](http://arxiv.org/abs/1510.06925v1)\n\n**Exploring the Space of Adversarial Images**\n\n- arxiv: [http://arxiv.org/abs/1510.05328](http://arxiv.org/abs/1510.05328)\n- github: [https://github.com/tabacof/adversarial](https://github.com/tabacof/adversarial)\n\n**Learning with a Strong Adversary**\n\n- arxiv: [http://arxiv.org/abs/1511.03034](http://arxiv.org/abs/1511.03034)\n\n**Adversarial examples in the physical world**\n\n- author: Alexey Kurakin, Ian Goodfellow, Samy Bengio. Google Brain & OpenAI\n- arxiv: [http://arxiv.org/abs/1607.02533](http://arxiv.org/abs/1607.02533)\n\n**DeepFool: a simple and accurate method to fool deep neural networks**\n\n- arxiv: [http://arxiv.org/abs/1511.04599](http://arxiv.org/abs/1511.04599)\n- github: [https://github.com/LTS4/DeepFool](https://github.com/LTS4/DeepFool)\n\n**Adversarial Autoencoders**\n\n- arxiv: [http://arxiv.org/abs/1511.05644](http://arxiv.org/abs/1511.05644)\n- slides: [https://docs.google.com/presentation/d/1Lyp91JOSzXo0Kk8gPdgyQUDuqLV_PnSzJh7i5c8ZKjs/edit?pref=2&pli=1](https://docs.google.com/presentation/d/1Lyp91JOSzXo0Kk8gPdgyQUDuqLV_PnSzJh7i5c8ZKjs/edit?pref=2&pli=1)\n- notes(by Dustin Tran): [http://dustintran.com/blog/adversarial-autoencoders/](http://dustintran.com/blog/adversarial-autoencoders/)\n- TFD manifold: [http://www.comm.utoronto.ca/~makhzani/adv_ae/tfd.gif](http://www.comm.utoronto.ca/~makhzani/adv_ae/tfd.gif)\n- SVHN style manifold: [http://www.comm.utoronto.ca/~makhzani/adv_ae/svhn.gif](http://www.comm.utoronto.ca/~makhzani/adv_ae/svhn.gif)\n\n**Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization**\n\n- arxiv: [http://arxiv.org/abs/1511.05432](http://arxiv.org/abs/1511.05432)\n- github: [https://github.com/yutaroyamada/RobustTraining](https://github.com/yutaroyamada/RobustTraining)\n\n**(Deep Learning’s Deep Flaws)’s Deep Flaws (By Zachary Chase Lipton)**\n\n- blog: [http://www.kdnuggets.com/2015/01/deep-learning-flaws-universal-machine-learning.html](http://www.kdnuggets.com/2015/01/deep-learning-flaws-universal-machine-learning.html)\n\n**Deep Learning Adversarial Examples – Clarifying Misconceptions**\n\n- intro: By Ian Goodfellow, Google\n- blog: [http://www.kdnuggets.com/2015/07/deep-learning-adversarial-examples-misconceptions.html](http://www.kdnuggets.com/2015/07/deep-learning-adversarial-examples-misconceptions.html)\n\n**Adversarial Machines: Fooling A.Is (and turn everyone into a Manga)**\n\n- blog: [https://medium.com/@samim/adversarial-machines-998d8362e996#.iv3muefgt](https://medium.com/@samim/adversarial-machines-998d8362e996#.iv3muefgt)\n\n**How to trick a neural network into thinking a panda is a vulture**\n\n- blog: [https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture](https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture)\n\n**Assessing Threat of Adversarial Examples on Deep Neural Networks**\n\n- intro: pre-print version to appear in IEEE ICMLA 2016\n- arxiv: [https://arxiv.org/abs/1610.04256](https://arxiv.org/abs/1610.04256)\n\n**Safety Verification of Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1610.06940](https://arxiv.org/abs/1610.06940)\n\n**Adversarial Machine Learning at Scale**\n\n- intro: Google Brain & OpenAI\n- arxiv: [https://arxiv.org/abs/1611.01236](https://arxiv.org/abs/1611.01236)\n\n**Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks**\n\n[https://arxiv.org/abs/1704.01155](https://arxiv.org/abs/1704.01155)\n\n**Parseval Networks: Improving Robustness to Adversarial Examples**\n\n- intro: Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1704.08847](https://arxiv.org/abs/1704.08847)\n\n**Towards Deep Learning Models Resistant to Adversarial Attacks**\n\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1706.06083](https://arxiv.org/abs/1706.06083)\n\n**NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles**\n\n- intro: CVPR 2017 Spotlight Oral Workshop\n- arxiv: [https://arxiv.org/abs/1707.03501](https://arxiv.org/abs/1707.03501)\n\n**One pixel attack for fooling deep neural networks**\n\n- intro: Kyushu University\n- arxiv: [https://arxiv.org/abs/1710.08864](https://arxiv.org/abs/1710.08864)\n- github: [https://github.com/Hyperparticle/one-pixel-attack-keras](https://github.com/Hyperparticle/one-pixel-attack-keras)\n\n**Enhanced Attacks on Defensively Distilled Deep Neural Networks**\n\n[https://arxiv.org/abs/1711.05934](https://arxiv.org/abs/1711.05934)\n\n**Adversarial Attacks Beyond the Image Space**\n\n[https://arxiv.org/abs/1711.07183](https://arxiv.org/abs/1711.07183)\n\n**On the Robustness of Semantic Segmentation Models to Adversarial Attacks**\n\n[https://arxiv.org/abs/1711.09856](https://arxiv.org/abs/1711.09856)\n\n**Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser**\n\n[https://arxiv.org/abs/1712.02976](https://arxiv.org/abs/1712.02976)\n\n**A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations**\n\n[https://arxiv.org/abs/1712.02779](https://arxiv.org/abs/1712.02779)\n\n**Training Ensembles to Detect Adversarial Examples**\n\n[https://arxiv.org/abs/1712.04006](https://arxiv.org/abs/1712.04006)\n\n**Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models**\n\n- arxiv: [https://arxiv.org/abs/1712.04248](https://arxiv.org/abs/1712.04248)\n- openreview: [https://openreview.net/forum?id=SyZI0GWCZ](https://openreview.net/forum?id=SyZI0GWCZ)\n\n**Where Classification Fails, Interpretation Rises**\n\n- intro: Lehigh University\n- arxiv: [https://arxiv.org/abs/1712.00558](https://arxiv.org/abs/1712.00558)\n\n**Query-Efficient Black-box Adversarial Examples**\n\n[https://arxiv.org/abs/1712.07113](https://arxiv.org/abs/1712.07113)\n\n**Adversarial Examples: Attacks and Defenses for Deep Learning**\n\n- intro: University of Florida\n[https://arxiv.org/abs/1712.07107](https://arxiv.org/abs/1712.07107)\n\n**Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning Applications**\n\n[https://arxiv.org/abs/1712.07805](https://arxiv.org/abs/1712.07805)\n\n**Note on Attacking Object Detectors with Adversarial Stickers**\n\n- arxiv: [https://arxiv.org/abs/1712.08062](https://arxiv.org/abs/1712.08062)\n- demo: [https://iotsecurity.eecs.umich.edu/#yolo](https://iotsecurity.eecs.umich.edu/#yolo)\n\n**Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning**\n\n- intro: UC Berkeley\n- arxiv: [https://arxiv.org/abs/1712.05526](https://arxiv.org/abs/1712.05526)\n\n**Awesome Adversarial Examples for Deep Learning**\n\n[https://github.com/chbrian/awesome-adversarial-examples-dl](https://github.com/chbrian/awesome-adversarial-examples-dl)\n\n**Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning**\n\n[https://arxiv.org/abs/1712.05526](https://arxiv.org/abs/1712.05526)\n\n**Exploring the Space of Black-box Attacks on Deep Neural Networks**\n\n[https://arxiv.org/abs/1712.09491](https://arxiv.org/abs/1712.09491)\n\n**Adversarial Patch**\n\n[https://arxiv.org/abs/1712.09665](https://arxiv.org/abs/1712.09665)\n\n**Adversarial Generative Nets: Neural Network Attacks on State-of-the-Art Face Recognition**\n\n- intro: CMU & University of North Carolina at Chapel Hill\n- arxiv: [https://arxiv.org/abs/1801.00349](https://arxiv.org/abs/1801.00349)\n\n**Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey**\n\n[https://arxiv.org/abs/1801.00553](https://arxiv.org/abs/1801.00553)\n\n**Spatially transformed adversarial examples**\n\n[https://arxiv.org/abs/1801.02612](https://arxiv.org/abs/1801.02612)\n\n**Generating adversarial examples with adversarial networks**\n\n- intro: University of Michigan & UC Berkeley & MIT CSAIL\n- arxiv: [https://arxiv.org/abs/1801.02610](https://arxiv.org/abs/1801.02610)\n\n**Adversarial Spheres**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1801.02774](https://arxiv.org/abs/1801.02774)\n\n**LaVAN: Localized and Visible Adversarial Noise**\n\n- intro: Bar-Ilan University & DeepMind\n- arxiv: [https://arxiv.org/abs/1801.02608](https://arxiv.org/abs/1801.02608)\n\n**Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples**\n\n- intro: ICML 2018 Best Paper Award. MIT & UC Berkeley\n- arxiv: [https://arxiv.org/abs/1802.00420](https://arxiv.org/abs/1802.00420)\n- github: [https://github.com/anishathalye/obfuscated-gradients](https://github.com/anishathalye/obfuscated-gradients)\n\n**Adversarial Examples that Fool both Human and Computer Vision**\n\n- intro: Google Brain & Stanford University\n- arxiv: [https://arxiv.org/abs/1802.08195](https://arxiv.org/abs/1802.08195)\n\n**On the Suitability of Lp-norms for Creating and Preventing Adversarial Examples**\n\n[https://arxiv.org/abs/1802.09653](https://arxiv.org/abs/1802.09653)\n\n**Protecting JPEG Images Against Adversarial Attacks**\n\n- intro: IEEE Data Compression Conference\n- arxiv: [https://arxiv.org/abs/1803.00940](https://arxiv.org/abs/1803.00940)\n\n**Sparse Adversarial Perturbations for Videos**\n\n[https://arxiv.org/abs/1803.02536](https://arxiv.org/abs/1803.02536)\n\n**DeepDefense: Training Deep Neural Networks with Improved Robustness**\n\n- intro: Tsinghua National Laboratory for Information Science and Technology (TNList) & Intel Labs\n- arxiv: [https://arxiv.org/abs/1803.00404](https://arxiv.org/abs/1803.00404)\n\n**Improving Transferability of Adversarial Examples with Input Diversity**\n\n- arxiv: [https://arxiv.org/abs/1803.06978](https://arxiv.org/abs/1803.06978)\n- github: [https://github.com/cihangxie/DI-2-FGSM](https://github.com/cihangxie/DI-2-FGSM)\n\n**Adversarial Attacks and Defences Competition**\n\n- intro: Google Brain & Tsinghua University & The Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/1804.00097](https://arxiv.org/abs/1804.00097)\n\n**Semantic Adversarial Examples**\n\n[https://arxiv.org/abs/1804.00499](https://arxiv.org/abs/1804.00499)\n\n**Generating Natural Adversarial Examples**\n\n- intro: ICLR 2018\n- arxiv: [https://arxiv.org/abs/1710.11342](https://arxiv.org/abs/1710.11342)\n- github: [https://github.com/zhengliz/natural-adversary](https://github.com/zhengliz/natural-adversary)\n\n**An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks**\n\n- intro: Northeastern University & MIT-IBM Watson AI Lab & IBM Research AI\n- keywords: Deep Neural Networks; Adversarial Attacks; ADMM (Alternating Direction Method of Multipliers)\n- arxiv: [https://arxiv.org/abs/1804.03193](https://arxiv.org/abs/1804.03193)\n\n**On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses**\n\n[https://arxiv.org/abs/1804.03286](https://arxiv.org/abs/1804.03286)\n\n**VectorDefense: Vectorization as a Defense to Adversarial Examples**\n\n- keywrods: MNIST\n- arxiv: [https://arxiv.org/abs/1804.08529](https://arxiv.org/abs/1804.08529)\n\n**On the Limitation of MagNet Defense against L1-based Adversarial Examples**\n\n[https://arxiv.org/abs/1805.00310](https://arxiv.org/abs/1805.00310)\n\n**Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models**\n\n- intro: ICLR 2018\n- arxiv: [https://arxiv.org/abs/1805.06605](https://arxiv.org/abs/1805.06605)\n- github: [https://github.com/kabkabm/defensegan](https://github.com/kabkabm/defensegan)\n\n**Siamese networks for generating adversarial examples**\n\n[https://arxiv.org/abs/1805.01431](https://arxiv.org/abs/1805.01431)\n\n**Generative Adversarial Examples**\n\n- intro: Stanford University & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1805.07894](https://arxiv.org/abs/1805.07894)\n\n**Detecting Adversarial Examples via Key-based Network**\n\n[https://arxiv.org/abs/1806.00580](https://arxiv.org/abs/1806.00580)\n\n**Adversarial Attacks on Variational Autoencoders**\n\n[https://arxiv.org/abs/1806.04646](https://arxiv.org/abs/1806.04646)\n\n**Non-Negative Networks Against Adversarial Attacks**\n\n- intro: Laboratory for Physical Sciences & Nvidia\n- arxiv: [https://arxiv.org/abs/1806.06108](https://arxiv.org/abs/1806.06108)\n\n**Gradient Similarity: An Explainable Approach to Detect Adversarial Attacks against Deep Learning**\n\n[https://arxiv.org/abs/1806.10707](https://arxiv.org/abs/1806.10707)\n\n**Adversarial Reprogramming of Neural Networks**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1806.11146](https://arxiv.org/abs/1806.11146)\n\n**Defend Deep Neural Networks Against Adversarial Examples via Fixed andDynamic Quantized Activation Functions**\n\n- intro: University of Central Florida & JD AI Research & Tencent AI Lab\n- arxiv: [https://arxiv.org/abs/1807.06714](https://arxiv.org/abs/1807.06714)\n\n**Motivating the Rules of the Game for Adversarial Example Research**\n\n- intro: Google Brain & Princeton\n- arxiv: [https://arxiv.org/abs/1807.06732](https://arxiv.org/abs/1807.06732)\n\n**Defense Against Adversarial Attacks with Saak Transform**\n\n[https://arxiv.org/abs/1808.01785](https://arxiv.org/abs/1808.01785)\n\n**Are adversarial examples inevitable?**\n\n[https://arxiv.org/abs/1809.02104](https://arxiv.org/abs/1809.02104)\n\n**Open Set Adversarial Examples**\n\n[https://arxiv.org/abs/1809.02681](https://arxiv.org/abs/1809.02681)\n\n**Towards Query Efficient Black-box Attacks: An Input-free Perspective**\n\n- intro: 11th ACM Workshop on Artificial Intelligence and Security (AISec) with the 25th ACM Conference on Computer and Communications Security (CCS)\n- arxiv: [https://arxiv.org/abs/1809.02918](https://arxiv.org/abs/1809.02918)\n\n**SparseFool: a few pixels make a big difference**\n\n[https://arxiv.org/abs/1811.02248](https://arxiv.org/abs/1811.02248)\n[https://github.com/LTS4/SparseFool](https://github.com/LTS4/SparseFool)\n\n**Lightweight Lipschitz Margin Training for Certified Defense against Adversarial Examples**\n\n[https://arxiv.org/abs/1811.08080](https://arxiv.org/abs/1811.08080)\n\n**Adversarial Defense by Stratified Convolutional Sparse Coding**\n\n[https://arxiv.org/abs/1812.00037](https://arxiv.org/abs/1812.00037)\n\n**Learning Transferable Adversarial Examples via Ghost Networks**\n\n- intro: Johns Hopkins University & University of Oxford\n- arxiv: [https://arxiv.org/abs/1812.03413](https://arxiv.org/abs/1812.03413)\n\n**Feature Denoising for Improving Adversarial Robustness**\n\n- intro: Johns Hopkins University & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1812.03411](https://arxiv.org/abs/1812.03411)\n\n**Defense-VAE: A Fast and Accurate Defense against Adversarial Attacks**\n\n- intro: Georgia State University\n- arxiv: [https://arxiv.org/abs/1812.06570](https://arxiv.org/abs/1812.06570)\n\n**Curls & Whey: Boosting Black-Box Adversarial Attacks**\n\n- intro: CVPR 2019 Oral\n- arxiv: [https://arxiv.org/abs/1904.01160](https://arxiv.org/abs/1904.01160)\n\n**Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks**\n\n[https://arxiv.org/abs/1904.00887](https://arxiv.org/abs/1904.00887)\n\n**Black-box Adversarial Attacks on Video Recognition Models**\n\n[https://arxiv.org/abs/1904.05181](https://arxiv.org/abs/1904.05181)\n\n**Interpreting Adversarial Examples with Attributes**\n\n[https://arxiv.org/abs/1904.08279](https://arxiv.org/abs/1904.08279)\n\n**On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1907.11684](https://arxiv.org/abs/1907.11684)\n\n**Deep Neural Rejection against Adversarial Examples**\n\n[https://arxiv.org/abs/1910.00470](https://arxiv.org/abs/1910.00470)\n\n**Unrestricted Adversarial Attacks for Semantic Segmentation**\n\n[https://arxiv.org/abs/1910.02354](https://arxiv.org/abs/1910.02354)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","title":"Audio / Image / Video Generation"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Audio / Image / Video Generation\r\ndate: 2015-10-09\r\n---\r\n\r\n# Papers\r\n\r\n**Optimizing Neural Networks That Generate Images**\r\n\r\n- intro: 2014 PhD thesis\r\n- paper : [http://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf](http://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf)\r\n- github: [https://github.com/mrkulk/Unsupervised-Capsule-Network](https://github.com/mrkulk/Unsupervised-Capsule-Network)\r\n\r\n**Learning to Generate Chairs, Tables and Cars with Convolutional Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1411.5928](http://arxiv.org/abs/1411.5928)\r\n\r\n**DRAW: A Recurrent Neural Network For Image Generation**\r\n\r\n- intro: Google DeepMind\r\n- arxiv: [http://arxiv.org/abs/1502.04623](http://arxiv.org/abs/1502.04623)\r\n- github: [https://github.com/vivanov879/draw](https://github.com/vivanov879/draw)\r\n- github(Theano): [https://github.com/jbornschein/draw](https://github.com/jbornschein/draw)\r\n- github(Lasagne): [https://github.com/skaae/lasagne-draw](https://github.com/skaae/lasagne-draw)\r\n- youtube: [https://www.youtube.com/watch?v=Zt-7MI9eKEo&hd=1](https://www.youtube.com/watch?v=Zt-7MI9eKEo&hd=1)\r\n- video: [http://pan.baidu.com/s/1gd3W6Fh](http://pan.baidu.com/s/1gd3W6Fh)\r\n\r\n**What is DRAW (Deep Recurrent Attentive Writer)?**\r\n\r\n![](http://kvfrans.com/content/images/2016/06/gencnn-afe135ff8d2725325a22455a488562b0e1cb7ac6a3f60b3cecb373fd043eb202.svg)\r\n\r\n- blog: [http://kvfrans.com/what-is-draw-deep-recurrent-attentive-writer/](http://kvfrans.com/what-is-draw-deep-recurrent-attentive-writer/)\r\n- github(tensorflow): [https://github.com/kvfrans/draw](https://github.com/kvfrans/draw)\r\n\r\n**Colorizing the DRAW Model**\r\n\r\n- blog: [http://kvfrans.com/colorizing-the-draw-model/](http://kvfrans.com/colorizing-the-draw-model/)\r\n- github: [https://github.com/kvfrans/draw-color](https://github.com/kvfrans/draw-color)\r\n\r\n**Understanding and Implementing Deepmind's DRAW Model**\r\n\r\n- blog: [http://evjang.com/articles/draw](http://evjang.com/articles/draw)\r\n- github: [https://github.com/ericjang/draw](https://github.com/ericjang/draw)\r\n\r\n**Generative Image Modeling Using Spatial LSTMs**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.03478](http://arxiv.org/abs/1506.03478)\r\n- github: [https://github.com/lucastheis/ride/](https://github.com/lucastheis/ride/)\r\n\r\n**Conditional generative adversarial nets for convolutional face generation**\r\n\r\n- paper: [http://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf](http://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf)\r\n- blog: [http://www.foldl.me/2015/conditional-gans-face-generation/](http://www.foldl.me/2015/conditional-gans-face-generation/)\r\n- github: [https://github.com/hans/adversarial](https://github.com/hans/adversarial)\r\n\r\n**Generating Images from Captions with Attention**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.02793](http://arxiv.org/abs/1511.02793)\r\n- github: [https://github.com/emansim/text2image](https://github.com/emansim/text2image)\r\n- demo: [http://www.cs.toronto.edu/~emansim/cap2im.html](http://www.cs.toronto.edu/~emansim/cap2im.html)\r\n\r\n**Attribute2Image: Conditional Image Generation from Visual Attributes**\r\n\r\n- intro: University of Michigan & Adobe Research & NEC Labs\r\n- project page: [https://sites.google.com/site/attribute2image/](https://sites.google.com/site/attribute2image/)\r\n- arxiv: [http://arxiv.org/abs/1512.00570](http://arxiv.org/abs/1512.00570)\r\n- github(Torch): [https://github.com/xcyan/eccv16_attr2img](https://github.com/xcyan/eccv16_attr2img)\r\n\r\n**Autoencoding beyond pixels using a learned similarity metric**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.09300](http://arxiv.org/abs/1512.09300)\r\n- demo: [http://algoalgebra.csa.iisc.ernet.in/deepimagine/](http://algoalgebra.csa.iisc.ernet.in/deepimagine/)\r\n- github: [https://github.com/andersbll/autoencoding_beyond_pixels](https://github.com/andersbll/autoencoding_beyond_pixels)\r\n- github(Tensorflow): [https://github.com/timsainb/Tensorflow-MultiGPU-VAE-GAN](https://github.com/timsainb/Tensorflow-MultiGPU-VAE-GAN)\r\n- video: [http://video.weibo.com/show?fid=1034:f00b4e5a34e8c1ebe78ccd00da95f9e0](http://video.weibo.com/show?fid=1034:f00b4e5a34e8c1ebe78ccd00da95f9e0)\r\n- github: [https://github.com/stitchfix/fauxtograph](https://github.com/stitchfix/fauxtograph)\r\n\r\n**Deep Visual Analogy-Making**\r\n\r\n![](https://raw.githubusercontent.com/carpedm20/visual-analogy-tensorflow/83893d866557239a890053b55cb7105ebf54045e/assets/model.png)\r\n\r\n- paper: [https://papers.nips.cc/paper/5845-deep-visual-analogy-making](https://papers.nips.cc/paper/5845-deep-visual-analogy-making)\r\n- github(Tensorflow): [https://github.com/carpedm20/visual-analogy-tensorflow](https://github.com/carpedm20/visual-analogy-tensorflow)\r\n- slides: [http://slideplayer.com/slide/9147672/](http://slideplayer.com/slide/9147672/)\r\n- mirror: [http://pan.baidu.com/s/1pKgrdnt](http://pan.baidu.com/s/1pKgrdnt)\r\n\r\n**Pixel Recurrent Neural Networks**\r\n\r\n- intro: Google DeepMind. ICML 2016 best paper. PixelRNN\r\n- arxiv: [http://arxiv.org/abs/1601.06759](http://arxiv.org/abs/1601.06759)\r\n- github: [https://github.com/igul222/pixel_rnn](https://github.com/igul222/pixel_rnn)\r\n- github(Tensorflow): [https://github.com/carpedm20/pixel-rnn-tensorflow](https://github.com/carpedm20/pixel-rnn-tensorflow)\r\n- notes(by Hugo Larochelle): [https://www.evernote.com/shard/s189/sh/fdf61a28-f4b6-491b-bef1-f3e148185b18/aba21367d1b3730d9334ed91d3250848](https://www.evernote.com/shard/s189/sh/fdf61a28-f4b6-491b-bef1-f3e148185b18/aba21367d1b3730d9334ed91d3250848)\r\n- video(by Hugo Larochelle): [https://www.periscope.tv/hugo_larochelle/1ypKdnMkjBnJW](https://www.periscope.tv/hugo_larochelle/1ypKdnMkjBnJW)\r\n\r\n**Generating images with recurrent adversarial networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.05110](http://arxiv.org/abs/1602.05110)\r\n- github: [https://github.com/jiwoongim/GRAN](https://github.com/jiwoongim/GRAN)\r\n\r\n**Pixel-Level Domain Transfer**\r\n\r\n- intro: ECCV 2016\r\n- github(Torch): [https://github.com/fxia22/PixelDTGAN](https://github.com/fxia22/PixelDTGAN)\r\n- author page(Code and dataset): [https://dgyoo.github.io/](https://dgyoo.github.io/)\r\n\r\n**Generative Adversarial Text to Image Synthesis**\r\n\r\n![](https://raw.githubusercontent.com/reedscot/icml2016/master/images/dcgan_network.jpg)\r\n\r\n- intro: ICML 2016\r\n- arxiv: [http://arxiv.org/abs/1605.05396](http://arxiv.org/abs/1605.05396)\r\n- project page: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/embeddings-for-image-classification/generative-adversarial-text-to-image-synthesis/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/embeddings-for-image-classification/generative-adversarial-text-to-image-synthesis/)\r\n- github: [https://github.com/reedscot/icml2016](https://github.com/reedscot/icml2016)\r\n- code+dataset: [http://datasets.d2.mpi-inf.mpg.de/akata/cub_txt.tar.gz](http://datasets.d2.mpi-inf.mpg.de/akata/cub_txt.tar.gz)\r\n\r\n**Conditional Image Generation with PixelCNN Decoders**\r\n\r\n- intro: Google DeepMind. PixelCNN 2.0\r\n- arxiv: [http://arxiv.org/abs/1606.05328](http://arxiv.org/abs/1606.05328)\r\n- github(Theano): [https://github.com/kundan2510/pixelCNN](https://github.com/kundan2510/pixelCNN)\r\n- gtihub(Torch): [https://github.com/dritchie/pixelCNN](https://github.com/dritchie/pixelCNN)\r\n- github(Tensorflow): [https://github.com/anantzoid/Conditional-PixelCNN-decoder](https://github.com/anantzoid/Conditional-PixelCNN-decoder)\r\n\r\n**Inverting face embeddings with convolutional neural networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.04189](http://arxiv.org/abs/1606.04189)\r\n- github: [https://github.com/pavelgonchar/face-transfer-tensorflow](https://github.com/pavelgonchar/face-transfer-tensorflow)\r\n\r\n**Unsupervised Cross-Domain Image Generation**\r\n\r\n![](https://raw.githubusercontent.com/yunjey/dtn-tensorflow/master/jpg/dtn.jpg)\r\n\r\n- intro: Facebook AI Research. Domain Transfer Network (DTN)\r\n- arxiv: [https://arxiv.org/abs/1611.02200](https://arxiv.org/abs/1611.02200)\r\n- github(TensorFlow): [https://github.com/yunjey/dtn-tensorflow](https://github.com/yunjey/dtn-tensorflow)\r\n\r\n**PixelCNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications**\r\n\r\n- intro: OpenAI\r\n- arxiv: [https://arxiv.org/abs/1701.05517](https://arxiv.org/abs/1701.05517)\r\n- paper: [http://openreview.net/pdf?id=BJrFC6ceg](http://openreview.net/pdf?id=BJrFC6ceg)\r\n- github: [https://github.com/openai/pixel-cnn](https://github.com/openai/pixel-cnn)\r\n\r\n**Generating Interpretable Images with Controllable Structure**\r\n\r\n- intro: Google DeepMind\r\n- paper: [http://www.scottreed.info/files/iclr2017.pdf](http://www.scottreed.info/files/iclr2017.pdf)\r\n\r\n**Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.00215](https://arxiv.org/abs/1612.00215)\r\n\r\n**Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space**\r\n\r\n- intro: University of Wyoming & Geometric Intelligence & Montreal Institute for Learning Algorithms & University of Freiburg\r\n- project page: [http://www.evolvingai.org/ppgn](http://www.evolvingai.org/ppgn)\r\n- paper: [http://www.evolvingai.org/files/nguyen2016ppgn_v1.pdf](http://www.evolvingai.org/files/nguyen2016ppgn_v1.pdf)\r\n- github: [https://github.com/Evolving-AI-Lab/ppgn](https://github.com/Evolving-AI-Lab/ppgn)\r\n\r\n**Image Generation and Editing with Variational Info Generative AdversarialNetworks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.04568](https://arxiv.org/abs/1701.04568)\r\n\r\n**DeepFace: Face Generation using Deep Learning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.01876](https://arxiv.org/abs/1701.01876)\r\n\r\n**Multi-View Image Generation from a Single-View**\r\n\r\n- intro: Southwest Jiaotong University & National University of Singapore\r\n- arxiv: [https://arxiv.org/abs/1704.04886](https://arxiv.org/abs/1704.04886)\r\n\r\n**Generative Cooperative Net for Image Generation and Data Augmentation**\r\n\r\n[https://arxiv.org/abs/1705.02887](https://arxiv.org/abs/1705.02887)\r\n\r\n**Statistics of Deep Generated Images**\r\n\r\n[https://arxiv.org/abs/1708.02688](https://arxiv.org/abs/1708.02688)\r\n\r\n**Sketch-to-Image Generation Using Deep Contextual Completion**\r\n\r\n[https://arxiv.org/abs/1711.08972](https://arxiv.org/abs/1711.08972)\r\n\r\n**Energy-relaxed Wassertein GANs(EnergyWGAN): Towards More Stable and High Resolution Image Generation**\r\n\r\n[https://arxiv.org/abs/1712.01026](https://arxiv.org/abs/1712.01026)\r\n\r\n**Spatial PixelCNN: Generating Images from Patches**\r\n\r\n[https://arxiv.org/abs/1712.00714](https://arxiv.org/abs/1712.00714)\r\n\r\n**Visual to Sound: Generating Natural Sound for Videos in the Wild**\r\n\r\n- intro: University of North Carolina at Chapel Hill & Adobe Research\r\n- project page: [http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html](http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html)\r\n- arxiv: [https://arxiv.org/abs/1712.01393](https://arxiv.org/abs/1712.01393)\r\n\r\n**Semi-supervised FusedGAN for Conditional Image Generation**\r\n\r\n[https://arxiv.org/abs/1801.05551](https://arxiv.org/abs/1801.05551)\r\n\r\n**Image Transformer**\r\n\r\n- intro: Google Brain & UC Berkeley\r\n- arxiv: [https://arxiv.org/abs/1802.05751](https://arxiv.org/abs/1802.05751)\r\n\r\n**Unpaired Multi-Domain Image Generation via Regularized Conditional GANs**\r\n\r\n[https://arxiv.org/abs/1805.02456](https://arxiv.org/abs/1805.02456)\r\n\r\n**Transferring GANs: generating images from limited data**\r\n\r\n- intro: Universitat Aut`onoma de Barcelona\r\n- arxiv: [https://arxiv.org/abs/1805.01677](https://arxiv.org/abs/1805.01677)\r\n- github: [https://github.com/yaxingwang/Transferring-GANs](https://github.com/yaxingwang/Transferring-GANs)\r\n\r\n**Cross Domain Image Generation through Latent Space Exploration with Adversarial Loss**\r\n\r\n[https://arxiv.org/abs/1805.10130](https://arxiv.org/abs/1805.10130)\r\n\r\n# Face Image Generation\r\n\r\n**Fader Networks: Manipulating Images by Sliding Attributes**\r\n\r\n- intro: NIPS 2017. Facebook AI Research & Sorbonne Université\r\n- arxiv: [https://arxiv.org/abs/1706.00409](https://arxiv.org/abs/1706.00409)\r\n- github: [https://github.com//facebookresearch/FaderNetworks](https://github.com//facebookresearch/FaderNetworks)\r\n\r\n# Person Image Generation\r\n\r\n**Disentangled Person Image Generation**\r\n\r\n- intro: CVPR 2018 spotlight\r\n- intro: KU-Leuven/PSI & Max Planck Institute for Informatics & ETH Zurich\r\n- arxiv: [https://arxiv.org/abs/1712.02621](https://arxiv.org/abs/1712.02621)\r\n\r\n**Pose Guided Person Image Generation**\r\n\r\n- intro: NIPS 2017\r\n- arxiv: [https://arxiv.org/abs/1705.09368](https://arxiv.org/abs/1705.09368)\r\n- poster: [https://homes.esat.kuleuven.be/~liqianma/NIPS17_PG2/NIPS17_PG2_poster.pdf](https://homes.esat.kuleuven.be/~liqianma/NIPS17_PG2/NIPS17_PG2_poster.pdf)\r\n\r\n**Deformable GANs for Pose-based Human Image Generation**\r\n\r\n- intro: University of Trento & Inria Grenoble Rhone-Alpes\r\n- arxiv: [https://arxiv.org/abs/1801.00055](https://arxiv.org/abs/1801.00055)\r\n- github: [https://github.com/AliaksandrSiarohin/pose-gan](https://github.com/AliaksandrSiarohin/pose-gan)\r\n\r\n**Unpaired Pose Guided Human Image Generation**\r\n\r\n[https://arxiv.org/abs/1901.02284](https://arxiv.org/abs/1901.02284)\r\n\r\n# Video Generation\r\n\r\n**MoCoGAN: Decomposing Motion and Content for Video Generation**\r\n\r\n- arxiv: [https://arxiv.org/abs/1707.04993](https://arxiv.org/abs/1707.04993)\r\n- github: [https://github.com/sergeytulyakov/mocogan](https://github.com/sergeytulyakov/mocogan)\r\n- github(PyTorch): [https://github.com/DLHacks/mocogan](https://github.com/DLHacks/mocogan)\r\n\r\n**Attentive Semantic Video Generation using Captions**\r\n\r\n[https://arxiv.org/abs/1708.05980](https://arxiv.org/abs/1708.05980)\r\n\r\n**Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture**\r\n\r\n- intro: AAAI2018. The University of Tokyo\r\n- project page: [http://www.mi.t.u-tokyo.ac.jp/assets/publication/hierarchical_video_generation_sup/](http://www.mi.t.u-tokyo.ac.jp/assets/publication/hierarchical_video_generation_sup/)\r\n- arxiv: [https://arxiv.org/abs/1711.09618](https://arxiv.org/abs/1711.09618)\r\n\r\n**Towards an Understanding of Our World by GANing Videos in the Wild**\r\n\r\n- intro: ETH Zurich\r\n- arxiv: [https://arxiv.org/abs/1711.11453](https://arxiv.org/abs/1711.11453)\r\n- github: [https://github.com//bernhard2202/improved-video-gan](https://github.com//bernhard2202/improved-video-gan)\r\n\r\n**Video Generation from Single Semantic Label Map**\r\n\r\n- intro: CVPR 2019\r\n- arxiv: [https://arxiv.org/abs/1903.04480](https://arxiv.org/abs/1903.04480)\r\n- github: [https://github.com/junting/seg2vid](https://github.com/junting/seg2vid)\r\n\r\n# Deep Generative Model\r\n\r\n**Digit Fantasies by a Deep Generative Model**\r\n\r\n- demo: [http://www.dpkingma.com/sgvb_mnist_demo/demo.html](http://www.dpkingma.com/sgvb_mnist_demo/demo.html)\r\n\r\n**Conditional generative adversarial nets for convolutional face generation**\r\n\r\n- paper: [http://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf](http://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf)\r\n- blog: [http://www.foldl.me/2015/conditional-gans-face-generation/](http://www.foldl.me/2015/conditional-gans-face-generation/)\r\n- github: [https://github.com/hans/adversarial](https://github.com/hans/adversarial)\r\n\r\n**Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks**\r\n\r\n- intro: NIPS 2015\r\n- project page: [http://soumith.ch/eyescream/](http://soumith.ch/eyescream/)\r\n- homepage: [http://www.cs.nyu.edu/~denton/](http://www.cs.nyu.edu/~denton/)\r\n- arxiv: [http://arxiv.org/abs/1506.05751](http://arxiv.org/abs/1506.05751)\r\n- code: [http://soumith.ch/eyescream/](http://soumith.ch/eyescream/)\r\n- notes: [http://colinraffel.com/wiki/deep_generative_image_models_using_a_laplacian_pyramid_of_adversarial_networks](http://colinraffel.com/wiki/deep_generative_image_models_using_a_laplacian_pyramid_of_adversarial_networks)\r\n\r\n**Torch convolutional GAN: Generating Faces with Torch**\r\n\r\n- blog: [http://torch.ch/blog/2015/11/13/gan.html](http://torch.ch/blog/2015/11/13/gan.html)\r\n- github: [https://github.com/skaae/torch-gan](https://github.com/skaae/torch-gan)\r\n\r\n**One-Shot Generalization in Deep Generative Models**\r\n\r\n- intro: Google DeepMind. ICML 2016\r\n- arxiv: [http://arxiv.org/abs/1603.05106](http://arxiv.org/abs/1603.05106)\r\n\r\n**Generative Image Modeling using Style and Structure Adversarial Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.05631](http://arxiv.org/abs/1603.05631)\r\n- github: [https://github.com/xiaolonw/ss-gan](https://github.com/xiaolonw/ss-gan)\r\n\r\n**Synthesizing Dynamic Textures and Sounds by Spatial-Temporal Generative ConvNet**\r\n\r\n![](http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet_file/video/water_stone1.gif)\r\n\r\n- project page: [http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet.html](http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet.html)\r\n- paper: [http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet_file/doc/STGConvNet.pdf](http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet_file/doc/STGConvNet.pdf)\r\n\r\n**Synthesizing the preferred inputs for neurons in neural networks via deep generator networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.09304](http://arxiv.org/abs/1605.09304)\r\n\r\n**ArtGAN: Artwork Synthesis with Conditional Categorial GANs**\r\n\r\n- arxiv: [https://arxiv.org/abs/1702.03410](https://arxiv.org/abs/1702.03410)\r\n\r\n**Learning to Generate Chairs with Generative Adversarial Nets**\r\n\r\n[https://arxiv.org/abs/1705.10413](https://arxiv.org/abs/1705.10413)\r\n\r\n# Blogs\r\n\r\n**Torch convolutional GAN: Generating Faces with Torch**\r\n\r\n- blog: [http://torch.ch/blog/2015/11/13/gan.html](http://torch.ch/blog/2015/11/13/gan.html)\r\n- github: [https://github.com/skaae/torch-gan](https://github.com/skaae/torch-gan)\r\n\r\n**Generating Large Images from Latent Vectors**\r\n\r\n[http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)\r\n\r\n**Generating Faces with Deconvolution Networks**\r\n\r\n![](https://raw.githubusercontent.com/zo7/facegen/master/img/example.gif)\r\n\r\n- blog: [https://zo7.github.io/blog/2016/09/25/generating-faces.html](https://zo7.github.io/blog/2016/09/25/generating-faces.html)\r\n- github: [https://github.com/zo7/facegen](https://github.com/zo7/facegen)\r\n\r\n**Attention Models in Image and Caption Generation**\r\n\r\n- blog: [https://casmls.github.io/general/2016/10/16/attention_model.html](https://casmls.github.io/general/2016/10/16/attention_model.html)\r\n\r\n**Deconvolution and Checkerboard Artifacts**\r\n\r\n- :star::star::star::star::star:\r\n- intro: Google Brain & Université de Montréal\r\n- blog: [http://distill.pub/2016/deconv-checkerboard/](http://distill.pub/2016/deconv-checkerboard/)\r\n\r\n# Projects\r\n\r\n**Generate cat images with neural networks**\r\n\r\n- github: [https://github.com/aleju/cat-generator](https://github.com/aleju/cat-generator)\r\n\r\n**TF-VAE-GAN-DRAW**\r\n\r\n- intro: A collection of generative methods implemented with TensorFlow \r\n(Deep Convolutional Generative Adversarial Networks (DCGAN), \r\nVariational Autoencoder (VAE) \r\nand DRAW: A Recurrent Neural Network For Image Generation).\r\n- github: [https://github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW](https://github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW)\r\n\r\n**Generating Large Images from Latent Vectors**\r\n\r\n![](http://blog.otoro.net/assets/20160401/png/generator_example.png)\r\n\r\n- project page: [http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)\r\n- github: [https://github.com/hardmaru/cppn-gan-vae-tensorflow](https://github.com/hardmaru/cppn-gan-vae-tensorflow)\r\n\r\n**Generating Large Images from Latent Vectors - Part Two**\r\n\r\n- project page: [http://blog.otoro.net/2016/06/02/generating-large-images-from-latent-vectors-part-two/](http://blog.otoro.net/2016/06/02/generating-large-images-from-latent-vectors-part-two/)\r\n- github: [https://github.com/hardmaru/resnet-cppn-gan-tensorflow](https://github.com/hardmaru/resnet-cppn-gan-tensorflow)\r\n\r\n**Analyzing 50k fonts using deep neural networks**\r\n\r\n- blog: [https://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks/](https://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks/)\r\n- github: [https://github.com/erikbern/deep-fonts](https://github.com/erikbern/deep-fonts)\r\n\r\n**Generate cat images with neural networks**\r\n\r\n- intro: GAN, spatial transformers, weight initialization and LeakyReLUs.\r\n- github: [https://github.com/aleju/cat-generator](https://github.com/aleju/cat-generator)\r\n\r\n**Generate human faces with neural networks**\r\n\r\n- github: [https://github.com/aleju/face-generator](https://github.com/aleju/face-generator)\r\n\r\n**A TensorFlow implementation of DeepMind's WaveNet paper**\r\n\r\n- intro: This is a TensorFlow implementation of the WaveNet generative neural network architecture for image generation.\r\n- github: [https://github.com/Zeta36/tensorflow-image-wavenet](https://github.com/Zeta36/tensorflow-image-wavenet)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","title":"Deep Learning and Autonomous Driving"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Deep Learning and Autonomous Driving\r\ndate: 2015-10-09\r\n---\r\n\r\n# Courses\r\n\r\n**(Toronto) CSC2541: Visual Perception for Autonomous Driving, Winter 2016**\r\n\r\n![](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/driving.jpg)\r\n\r\n- homepage: [http://www.cs.toronto.edu/~urtasun/courses/CSC2541/CSC2541_Winter16.html](http://www.cs.toronto.edu/~urtasun/courses/CSC2541/CSC2541_Winter16.html)\r\n\r\n**(MIT) 6.S094: Deep Learning for Self-Driving Cars**\r\n\r\n- homepage: [http://selfdrivingcars.mit.edu/](http://selfdrivingcars.mit.edu/)\r\n- github: [https://github.com/lexfridman/deepcars](https://github.com/lexfridman/deepcars)\r\n- youtube: [https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)\r\n- mirror: [https://pan.baidu.com/s/1boLRFaB](https://pan.baidu.com/s/1boLRFaB)\r\n\r\n**How to Land An Autonomous Vehicle Job: Coursework**\r\n\r\n- blog: [https://medium.com/self-driving-cars/how-to-land-an-autonomous-vehicle-job-coursework-e7acc2bfe740#.7vfjx3i1j](https://medium.com/self-driving-cars/how-to-land-an-autonomous-vehicle-job-coursework-e7acc2bfe740#.7vfjx3i1j)\r\n\r\n# Papers\r\n\r\n**An Empirical Evaluation of Deep Learning on Highway Driving**\r\n\r\n- arxiv: [http://arxiv.org/abs/1504.01716](http://arxiv.org/abs/1504.01716)\r\n- github: [https://github.com/brodyh/caffe](https://github.com/brodyh/caffe)\r\n\r\n**Real-time Joint Object Detection and Semantic Segmentation Network for Automated Driving**\r\n\r\n- intro: NeurIPS 2018 Workshop on Machine Learning on the Phone and other Consumer Devices (MLPCD 2)\r\n- arxiv: [https://arxiv.org/abs/1901.03912](https://arxiv.org/abs/1901.03912)\r\n\r\n**Optical Flow augmented Semantic Segmentation networks for Automated Driving**\r\n\r\n- intro: VISAPP 2019 Oral\r\n- arxiv: [https://arxiv.org/abs/1901.07355](https://arxiv.org/abs/1901.07355)\r\n\r\n**AuxNet: Auxiliary tasks enhanced Semantic Segmentation for Automated Driving**\r\n\r\n- intro: Short Paper for a poster presentation at VISAPP 2019\r\n- arxiv: [https://arxiv.org/abs/1901.05808](https://arxiv.org/abs/1901.05808)\r\n\r\n**Design of Real-time Semantic Segmentation Decoder for Automated Driving**\r\n\r\n- intro: VISAPP 2019\r\n- arxiv: [https://arxiv.org/abs/1901.06580](https://arxiv.org/abs/1901.06580)\r\n\r\n**Hierarchical Multi-task Deep Neural Network Architecture for End-to-End Driving**\r\n\r\n[https://arxiv.org/abs/1902.03466](https://arxiv.org/abs/1902.03466)\r\n\r\n## DeepDriving\r\n\r\n**DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving**\r\n\r\n![](http://deepdriving.cs.princeton.edu/teaser.jpg)\r\n\r\n- project page: [http://deepdriving.cs.princeton.edu/](http://deepdriving.cs.princeton.edu/)\r\n- paper: [http://deepdriving.cs.princeton.edu/paper.pdf](http://deepdriving.cs.princeton.edu/paper.pdf)\r\n- code: [http://deepdriving.cs.princeton.edu/DeepDriving.zip](http://deepdriving.cs.princeton.edu/DeepDriving.zip)\r\n\r\n**End to End Learning for Self-Driving Cars**\r\n\r\n- intro: NVIDIA DevBox and Torch 7, 30 FPS\r\n- arxiv: [http://arxiv.org/abs/1604.07316](http://arxiv.org/abs/1604.07316)\r\n- blog: [https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/)\r\n- demo: [https://www.youtube.com/watch?v=NJU9ULQUwng&feature=youtu.be](https://www.youtube.com/watch?v=NJU9ULQUwng&feature=youtu.be)\r\n- github: [https://github.com/SullyChen/Nvidia-Autopilot-TensorFlow](https://github.com/SullyChen/Nvidia-Autopilot-TensorFlow)\r\n\r\n**End-to-End Deep Learning for Self-Driving Cars**\r\n\r\n![](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/08/training-624x291.png)\r\n\r\n- blog: [https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/)\r\n\r\n- - -\r\n\r\n**Can we unify monocular detectors for autonomous driving by using the pixel-wise semantic segmentation of CNNs?**\r\n\r\n- arxiv: [http://arxiv.org/abs/1607.00971](http://arxiv.org/abs/1607.00971)\r\n\r\n## BRAIN4CARS: Cabin Sensing for Safe and Personalized Driving\r\n\r\n**Brain4Cars: Sensory-Fusion Recurrent Neural Models for Driver Activity Anticipation**\r\n\r\n**Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep Learning Architecture**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.00740](http://arxiv.org/abs/1601.00740)\r\n\r\n**Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models**\r\n\r\n![](http://brain4cars.com/img/demo.jpg)\r\n\r\n- arxiv: [http://arxiv.org/abs/1504.02789](http://arxiv.org/abs/1504.02789)\r\n- github: [https://github.com/asheshjain399/ICCV2015_Brain4Cars](https://github.com/asheshjain399/ICCV2015_Brain4Cars)\r\n\r\n**Recurrent Neural Networks for Driver Activity Anticipation via Sensory-Fusion Architecture**\r\n\r\n![](http://brain4cars.com/img/fusionRNN.png)\r\n\r\n- project page: [http://www.brain4cars.com/](http://www.brain4cars.com/)\r\n- arxiv: [http://arxiv.org/abs/1509.05016](http://arxiv.org/abs/1509.05016)\r\n- github: [https://github.com/asheshjain399/RNNexp](https://github.com/asheshjain399/RNNexp)\r\n\r\n**Long-term Planning by Short-term Prediction**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.01580](http://arxiv.org/abs/1602.01580)\r\n\r\n**Learning a Driving Simulator**\r\n\r\n![](http://research.comma.ai/images/selfsteer.gif)\r\n\r\n- introo: by hacker Geohot\r\n- project page: [http://research.comma.ai/](http://research.comma.ai/)\r\n- arxiv: [http://arxiv.org/abs/1608.01230](http://arxiv.org/abs/1608.01230)\r\n- paper: [https://github.com/commaai/research/blob/master/paper/commalds.pdf](https://github.com/commaai/research/blob/master/paper/commalds.pdf)\r\n- github: [https://github.com/commaai/research](https://github.com/commaai/research)\r\n\r\n**Comma.ai open-sources the data it used for its first successful driverless trips**\r\n\r\n- blog: [https://techcrunch.com/2016/08/03/comma-ai-open-sources-the-data-it-used-for-its-first-successful-driverless-trips/](https://techcrunch.com/2016/08/03/comma-ai-open-sources-the-data-it-used-for-its-first-successful-driverless-trips/)\r\n\r\n**Autonomous driving challenge: To Infer the property of a dynamic object based on its motion pattern using recurrent neural network**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.00361](http://arxiv.org/abs/1609.00361)\r\n\r\n**Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.03295](https://arxiv.org/abs/1610.03295)\r\n\r\n**Learning from Maps: Visual Common Sense for Autonomous Driving**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.08583](https://arxiv.org/abs/1611.08583)\r\n\r\n**SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial Networks**\r\n\r\n- intro: Accepted at the Deep Learning for Action and Interaction Workshop, 30th Conference on Neural Information Processing Systems (NIPS 2016)\r\n- arxiv: [https://arxiv.org/abs/1611.08788](https://arxiv.org/abs/1611.08788)\r\n\r\n**MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving**\r\n\r\n- intro: first place on Kitti Road Segmentation. \r\njoint classification, detection and semantic segmentation via a unified architecture, less than 100 ms to perform all tasks\r\n- arxiv: [https://arxiv.org/abs/1612.07695](https://arxiv.org/abs/1612.07695)\r\n- github: [https://github.com/MarvinTeichmann/MultiNet](https://github.com/MarvinTeichmann/MultiNet)\r\n\r\n**Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention**\r\n\r\n- intro: UC Berkeley\r\n- arxiv: [https://arxiv.org/abs/1703.10631](https://arxiv.org/abs/1703.10631)\r\n\r\n**Virtual to Real Reinforcement Learning for Autonomous Driving**\r\n\r\n- intro: Shanghai Jiao Tong University & UC Berkeley & Tsinghua University\r\n- arxiv: [https://arxiv.org/abs/1704.03952](https://arxiv.org/abs/1704.03952)\r\n\r\n**Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art**\r\n\r\n- homepage: [http://www.cvlibs.net/projects/autonomous_vision_survey/](http://www.cvlibs.net/projects/autonomous_vision_survey/)\r\n- arxiv: [https://arxiv.org/abs/1704.05519](https://arxiv.org/abs/1704.05519)\r\n\r\n**Deep Reinforcement Learning framework for Autonomous Driving**\r\n\r\n[https://arxiv.org/abs/1704.02532](https://arxiv.org/abs/1704.02532)\r\n\r\n**Systematic Testing of Convolutional Neural Networks for Autonomous Driving**\r\n\r\n[https://arxiv.org/abs/1708.03309](https://arxiv.org/abs/1708.03309)\r\n\r\n**MODNet: Moving Object Detection Network with Motion and Appearance for Autonomous Driving**\r\n\r\n[https://arxiv.org/abs/1709.04821](https://arxiv.org/abs/1709.04821)\r\n\r\n**CFENet: An Accurate and Efficient Single-Shot Object Detector for Autonomous Driving**\r\n\r\n- intro: CVPR 2018 Workshop of Autonomous Driving (WAD)\r\n- arxiv: [https://arxiv.org/abs/1806.09790](https://arxiv.org/abs/1806.09790)\r\n\r\n**LaneNet: Real-Time Lane Detection Networks for Autonomous Driving**\r\n\r\n- intro: Duke University & Horizon Robotics, Inc.\r\n- arxiv: [https://arxiv.org/abs/1807.01726](https://arxiv.org/abs/1807.01726)\r\n\r\n**Learning End-to-end Autonomous Driving using Guided Auxiliary Supervision**\r\n\r\n[https://arxiv.org/abs/1808.10393](https://arxiv.org/abs/1808.10393)\r\n\r\n**Rethinking Self-driving: Multi-task Knowledge for Better Generalization and Accident Explanation Ability**\r\n\r\n- intro: Waseda University\r\n- arxiv: [https://arxiv.org/abs/1809.11100](https://arxiv.org/abs/1809.11100)\r\n- demo: [https://www.youtube.com/watch?v=N7ePnnZZwdE](https://www.youtube.com/watch?v=N7ePnnZZwdE)\r\n\r\n**Pixel and Feature Level Based Domain Adaption for Object Detection in Autonomous Driving**\r\n\r\n[https://arxiv.org/abs/1810.00345](https://arxiv.org/abs/1810.00345)\r\n\r\n**Multi-task Learning with Attention for End-to-end Autonomous Driving**\r\n\r\n- intro: CVPR 2021 Workshop on Autonomous Driving\r\n- arxiv: [https://arxiv.org/abs/2104.10753](https://arxiv.org/abs/2104.10753)\r\n\r\n**MP3: A Unified Model to Map, Perceive, Predict and Plan**\r\n\r\n- intro: Uber ATG & University of Toronto\r\n- arxiv: [https://arxiv.org/abs/2101.06806](https://arxiv.org/abs/2101.06806)\r\n\r\n**Level 2 Autonomous Driving on a Single Device: Diving into the Devils of Openpilot**\r\n\r\n- intro: Shanghai AI Laboratory & Shanghai Jiao Tong University & UCSD & SenseTime\r\n- arxiv: [https://arxiv.org/abs/2206.08176](https://arxiv.org/abs/2206.08176)\r\n- github: [https://github.com/OpenPerceptionX/Openpilot-Deepdive](https://github.com/OpenPerceptionX/Openpilot-Deepdive)\r\n\r\n**Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras**\r\n\r\n- intro: ICRA 2022\r\n- intro: University of Michigan & Ford Motor Company\r\n- arxiv: [https://arxiv.org/abs/2206.09770](https://arxiv.org/abs/2206.09770)\r\n\r\n**ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning**\r\n\r\n- intro: ECCV 2022\r\n- intro: Shanghai Jiao Tong University & Shanghai AI Laboratory & The University of California & JD Explore Academy\r\n- arxiv: [https://arxiv.org/abs/2207.07601](https://arxiv.org/abs/2207.07601)\r\n- github: [https://github.com/OpenPerceptionX/ST-P3](https://github.com/OpenPerceptionX/ST-P3)\r\n\r\n# Projects\r\n\r\n**Caffe-Autopilot: Car autopilot software that uses C++, BVLC Caffe, OpenCV, and SFML**\r\n\r\n- github: [https://github.com/SullyChen/Caffe-Autopilot](https://github.com/SullyChen/Caffe-Autopilot)\r\n\r\n**Self Driving Car Demo**\r\n\r\n- intro; A project that trains a virtual car to how to move an object around a screen (drive itself) \r\nwithout running into obstacles using a type of reinforcement learning called Q-Learning\r\n- github: [https://github.com/llSourcell/Self-Driving-Car-Demo/](https://github.com/llSourcell/Self-Driving-Car-Demo/)\r\n\r\n**Autoware: Open-source software for urban autonomous driving**\r\n\r\n- github: [https://github.com/CPFL/Autoware](https://github.com/CPFL/Autoware)\r\n\r\n**Open Sourcing 223GB of Driving Data**\r\n\r\n- homepage: [https://udacity.com/self-driving-car](https://udacity.com/self-driving-car)\r\n- blog: [https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5#.q8nk5bfpp](https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5#.q8nk5bfpp)\r\n- github: [https://github.com/udacity/self-driving-car](https://github.com/udacity/self-driving-car)\r\n\r\n**Machine Learning for RC Cars**\r\n\r\n![](https://camo.githubusercontent.com/5c0b9d7703d0ac318ce336a73c7440952b941459/68747470733a2f2f7468756d62732e6766796361742e636f6d2f4461726c696e67466f726b65644163616369617261742d73697a655f726573747269637465642e676966)\r\n\r\n- github: [https://github.com/kendricktan/suiron](https://github.com/kendricktan/suiron)\r\n\r\n**Self Driving (Toy) Ferrari**\r\n\r\n- github: [https://github.com/RyanZotti/Self-Driving-Car](https://github.com/RyanZotti/Self-Driving-Car)\r\n\r\n**Lane Finding Project for Self-Driving Car ND**\r\n\r\n- github: [https://github.com/udacity/CarND-LaneLines-P1](https://github.com/udacity/CarND-LaneLines-P1)\r\n\r\n**Instructions on how to get your development environment ready for Udacity Self Driving Car (SDC) Challenges**\r\n\r\n- github: [https://github.com/gtarobotics/self-driving-car](https://github.com/gtarobotics/self-driving-car)\r\n\r\n**DeepDrive: self-driving car AI**\r\n\r\n- intro: Caffe Model / Dataset / Tips and Tricks\r\n- homepage: [http://deepdrive.io/](http://deepdrive.io/)\r\n\r\n**DeepDrive setup: Run a self-driving car simulator from the comfort of your own PC**\r\n\r\n- github: [https://github.com/crizCraig/deepdrive](https://github.com/crizCraig/deepdrive)\r\n\r\n**DeepTesla: End-to-End Learning from Human and Autopilot Driving**\r\n\r\n[http://selfdrivingcars.mit.edu/deeptesla/](http://selfdrivingcars.mit.edu/deeptesla/)\r\n\r\n**DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car**\r\n\r\n- arxiv: [https://arxiv.org/abs/1712.08644](https://arxiv.org/abs/1712.08644)\r\n- github: [https://github.com//heechul/picar](https://github.com//heechul/picar)\r\n\r\n**Autonomous Driving in Reality with Reinforcement Learning and Image Translation**\r\n\r\n- intro: Shanghai Jiao Tong University\r\n- arxiv: [https://arxiv.org/abs/1801.05299](https://arxiv.org/abs/1801.05299)\r\n\r\n**End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars with Visual Perception**\r\n\r\n[https://arxiv.org/abs/1801.06734](https://arxiv.org/abs/1801.06734)\r\n\r\n# Blogs\r\n\r\n**Self-driving cars: How far away are we REALLY from autonomous cars?(7 Aug 2015)**\r\n\r\n[http://www.alphr.com/cars/1001329/self-driving-cars-how-far-away-are-we-really-from-autonomous-cars](http://www.alphr.com/cars/1001329/self-driving-cars-how-far-away-are-we-really-from-autonomous-cars)\r\n\r\n**Practice makes perfect: Driverless cars will learn from their mistakes(9 Oct 2015)**\r\n\r\n[http://www.alphr.com/cars/1001713/practice-makes-perfect-driverless-cars-will-learn-from-their-mistakes](http://www.alphr.com/cars/1001713/practice-makes-perfect-driverless-cars-will-learn-from-their-mistakes)\r\n\r\n**Eyes on the Road: How Autonomous Cars Understand What They’re Seeing**\r\n\r\n- blog: [http://blogs.nvidia.com/blog/2016/01/05/eyes-on-the-road-how-autonomous-cars-understand-what-theyre-seeing/](http://blogs.nvidia.com/blog/2016/01/05/eyes-on-the-road-how-autonomous-cars-understand-what-theyre-seeing/)\r\n\r\n**Human-in-the-loop deep learning will help drive autonomous cars**\r\n\r\n![](https://venturebeat.com/wp-content/uploads/2016/05/Ubercar.jpg)\r\n\r\n[http://venturebeat.com/2016/06/25/human-in-the-loop-deep-learning-will-help-drive-autonomous-cars/](http://venturebeat.com/2016/06/25/human-in-the-loop-deep-learning-will-help-drive-autonomous-cars/)\r\n\r\n**Using reinforcement learning in Python to teach a virtual car to avoid obstacles**\r\n\r\n- part 1: [https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.rneyuerga](https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.rneyuerga)\r\n- part 2: [https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.1pt1lli4c](https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.1pt1lli4c)\r\n- part 3: [https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-an-rc-car-to-avoid-obstacles-part-3-a1d063ac962f#.jwzm2v1r4](https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-an-rc-car-to-avoid-obstacles-part-3-a1d063ac962f#.jwzm2v1r4)\r\n- github: [https://github.com/harvitronix/reinforcement-learning-car](https://github.com/harvitronix/reinforcement-learning-car)\r\n\r\n**Autonomous RC car using Raspberry Pi and Neural Networks**\r\n\r\n![](https://camo.githubusercontent.com/42bd1644f47af0eec0a484011444d9a2ffe2bda9/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f64437942764c6a573658302f6d617872657364656661756c742e6a7067)\r\n\r\n- blog: [http://www.multunus.com/blog/2016/07/autonomous-rc-car-using-raspberry-pi-and-neural-networks/](http://www.multunus.com/blog/2016/07/autonomous-rc-car-using-raspberry-pi-and-neural-networks/)\r\n- github: [https://github.com/multunus/autonomous-rc-car](https://github.com/multunus/autonomous-rc-car)\r\n\r\n**The Road Ahead: Autonomous Vehicles Startup Ecosystem**\r\n\r\n![](https://cdn-images-1.medium.com/max/800/1*58utPvF5Lfyw9PpRjfaGpA.jpeg)\r\n\r\n[https://medium.com/the-mission/the-road-ahead-autonomous-vehicles-startup-ecosystem-3c91d546673d#.gft1xyh9l](https://medium.com/the-mission/the-road-ahead-autonomous-vehicles-startup-ecosystem-3c91d546673d#.gft1xyh9l)\r\n\r\n**Deep Driving - A revolutionary AI technique is about to transform the self-driving car**\r\n\r\n[https://www.technologyreview.com/s/602600/deep-driving/](https://www.technologyreview.com/s/602600/deep-driving/)\r\n\r\n**Visualizations for regressing wheel steering angles in self driving cars with Keras**\r\n\r\n- blog: [http://jacobcv.blogspot.jp/2016/10/visualizations-for-regressing-wheel.html](http://jacobcv.blogspot.jp/2016/10/visualizations-for-regressing-wheel.html)\r\n- github: [https://github.com/jacobgil/keras-steering-angle-visualizations](https://github.com/jacobgil/keras-steering-angle-visualizations)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","title":"Image / Video Captioning"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Image / Video Captioning\r\ndate: 2015-10-09\r\n---\r\n\r\n# Papers\r\n\r\n**Im2Text: Describing Images Using 1 Million Captioned Photographs**\r\n\r\n![](http://vision.cs.stonybrook.edu/~vicente/sbucaptions/im2text_files/im2text.png)\r\n\r\n- paper: [http://tamaraberg.com/papers/generation_nips2011.pdf](http://tamaraberg.com/papers/generation_nips2011.pdf)\r\n- project: [http://vision.cs.stonybrook.edu/~vicente/sbucaptions/](http://vision.cs.stonybrook.edu/~vicente/sbucaptions/)\r\n\r\n**Long-term Recurrent Convolutional Networks for Visual Recognition and Description**\r\n\r\n![](http://jeffdonahue.com/lrcn/images/lrcn_tasks.png)\r\n\r\n- intro: Oral presentation at CVPR 2015. LRCN\r\n- project page: [http://jeffdonahue.com/lrcn/](http://jeffdonahue.com/lrcn/)\r\n- arxiv: [http://arxiv.org/abs/1411.4389](http://arxiv.org/abs/1411.4389)\r\n- github: [https://github.com/BVLC/caffe/pull/2033](https://github.com/BVLC/caffe/pull/2033)\r\n\r\n## Show and Tell\r\n\r\n**Show and Tell: A Neural Image Caption Generator**\r\n\r\n- intro: Google\r\n- arxiv: [http://arxiv.org/abs/1411.4555](http://arxiv.org/abs/1411.4555)\r\n- github: [https://github.com/karpathy/neuraltalk](https://github.com/karpathy/neuraltalk)\r\n- gitxiv: [http://gitxiv.com/posts/7nofxjoYBXga5XjtL/show-and-tell-a-neural-image-caption-nic-generator](http://gitxiv.com/posts/7nofxjoYBXga5XjtL/show-and-tell-a-neural-image-caption-nic-generator)\r\n- github: [https://github.com/apple2373/chainer_caption_generation](https://github.com/apple2373/chainer_caption_generation)\r\n- github(TensorFlow): [https://github.com/tensorflow/models/tree/master/im2txt](https://github.com/tensorflow/models/tree/master/im2txt)\r\n- github(TensorFlow): [https://github.com/zsdonghao/Image-Captioning](https://github.com/zsdonghao/Image-Captioning)\r\n\r\n**Image caption generation by CNN and LSTM** \r\n\r\n![](http://1.bp.blogspot.com/-RgNlhmSJvgU/Vna0V_GNGXI/AAAAAAAAACw/tjYqwuGHekM/s400/Screen%2BShot%2B2015-12-20%2Bat%2B08.59.26.png)\r\n\r\n- blog: [http://t-satoshi.blogspot.com/2015/12/image-caption-generation-by-cnn-and-lstm.html](http://t-satoshi.blogspot.com/2015/12/image-caption-generation-by-cnn-and-lstm.html)\r\n- github: [https://github.com/jazzsaxmafia/show_and_tell.tensorflow](https://github.com/jazzsaxmafia/show_and_tell.tensorflow)\r\n\r\n**Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.06647](http://arxiv.org/abs/1609.06647)\r\n- github: [https://github.com/tensorflow/models/tree/master/im2txt](https://github.com/tensorflow/models/tree/master/im2txt)\r\n\r\n**Learning a Recurrent Visual Representation for Image Caption Generation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1411.5654](http://arxiv.org/abs/1411.5654)\r\n\r\n**Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation**\r\n\r\n- intro: CVPR 2015\r\n- paper: [http://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf](http://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf)\r\n\r\n**Deep Visual-Semantic Alignments for Generating Image Descriptions**\r\n\r\n- intro: \"propose a multimodal deep network that aligns various interesting \r\nregions of the image, represented using a CNN feature, with associated words. \r\nThe learned correspondences are then used to train a bi-directional RNN. \r\nThis model is able, not only to generate descriptions for images, but also \r\nto localize different segments of the sentence to their corresponding image regions.\"\r\n- project page: [http://cs.stanford.edu/people/karpathy/deepimagesent/](http://cs.stanford.edu/people/karpathy/deepimagesent/)\r\n- arxiv: [http://arxiv.org/abs/1412.2306](http://arxiv.org/abs/1412.2306)\r\n- slides: [http://www.cs.toronto.edu/~vendrov/DeepVisualSemanticAlignments_Class_Presentation.pdf](http://www.cs.toronto.edu/~vendrov/DeepVisualSemanticAlignments_Class_Presentation.pdf)\r\n- github: [https://github.com/karpathy/neuraltalk](https://github.com/karpathy/neuraltalk)\r\n- demo: [http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/](http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/)\r\n\r\n**Deep Captioning with Multimodal Recurrent Neural Networks**\r\n\r\n- intro: m-RNN. ICLR 2015\r\n- intro: \"combines the functionalities of the CNN and RNN by introducing a new multimodal layer, \r\nafter the embedding and recurrent layers of the RNN.\"\r\n- homepage: [http://www.stat.ucla.edu/~junhua.mao/m-RNN.html](http://www.stat.ucla.edu/~junhua.mao/m-RNN.html)\r\n- arxiv: [http://arxiv.org/abs/1412.6632](http://arxiv.org/abs/1412.6632)\r\n- github: [https://github.com/mjhucla/mRNN-CR](https://github.com/mjhucla/mRNN-CR)\r\n- github: [https://github.com/mjhucla/TF-mRNN](https://github.com/mjhucla/TF-mRNN)\r\n\r\n## Show, Attend and Tell\r\n\r\n**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (ICML 2015)**\r\n\r\n![](http://kelvinxu.github.io/projects/diags/model_diag.png)\r\n\r\n- project page: [http://kelvinxu.github.io/projects/capgen.html](http://kelvinxu.github.io/projects/capgen.html)\r\n- arxiv: [http://arxiv.org/abs/1502.03044](http://arxiv.org/abs/1502.03044)\r\n- github: [https://github.com/kelvinxu/arctic-captions](https://github.com/kelvinxu/arctic-captions)\r\n- github: [https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow](https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow)\r\n- github(TensorFlow): [https://github.com/yunjey/show-attend-and-tell-tensorflow](https://github.com/yunjey/show-attend-and-tell-tensorflow)\r\n- demo: [http://www.cs.toronto.edu/~rkiros/abstract_captions.html](http://www.cs.toronto.edu/~rkiros/abstract_captions.html)\r\n\r\n**Automatically describing historic photographs**\r\n\r\n- website: [https://staff.fnwi.uva.nl/d.elliott/loc/](https://staff.fnwi.uva.nl/d.elliott/loc/)\r\n\r\n- - -\r\n\r\n**Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images**\r\n\r\n- arxiv: [http://arxiv.org/abs/1504.06692](http://arxiv.org/abs/1504.06692)\r\n- homepage: [http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html](http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html)\r\n- github: [https://github.com/mjhucla/NVC-Dataset](https://github.com/mjhucla/NVC-Dataset)\r\n\r\n**What value do explicit high level concepts have in vision to language problems?**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.01144](http://arxiv.org/abs/1506.01144)\r\n\r\n**Aligning where to see and what to tell: image caption with region-based attention and scene factorization**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.06272](http://arxiv.org/abs/1506.06272)\r\n\r\n**Learning FRAME Models Using CNN Filters for Knowledge Visualization (CVPR 2015)**\r\n\r\n- project page: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html)\r\n- arxiv: [http://arxiv.org/abs/1509.08379](http://arxiv.org/abs/1509.08379)\r\n- code+data: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/deepFRAME_1.1.zip](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/deepFRAME_1.1.zip)\r\n\r\n**Generating Images from Captions with Attention**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.02793](http://arxiv.org/abs/1511.02793)\r\n- github: [https://github.com/emansim/text2image](https://github.com/emansim/text2image)\r\n- demo: [http://www.cs.toronto.edu/~emansim/cap2im.html](http://www.cs.toronto.edu/~emansim/cap2im.html)\r\n\r\n**Order-Embeddings of Images and Language**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.06361](http://arxiv.org/abs/1511.06361)\r\n- github: [https://github.com/ivendrov/order-embedding](https://github.com/ivendrov/order-embedding)\r\n\r\n**DenseCap: Fully Convolutional Localization Networks for Dense Captioning**\r\n\r\n- project page: [http://cs.stanford.edu/people/karpathy/densecap/](http://cs.stanford.edu/people/karpathy/densecap/)\r\n- arxiv: [http://arxiv.org/abs/1511.07571](http://arxiv.org/abs/1511.07571)\r\n- github(Torch): [https://github.com/jcjohnson/densecap](https://github.com/jcjohnson/densecap)\r\n\r\n**Expressing an Image Stream with a Sequence of Natural Sentences**\r\n\r\n- intro: NIPS 2015. CRCN\r\n- nips-page: [http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences](http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences)\r\n- paper: [http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf](http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf)\r\n- paper: [http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf](http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf)\r\n- author-page: [http://www.cs.cmu.edu/~gunhee/](http://www.cs.cmu.edu/~gunhee/)\r\n- github: [https://github.com/cesc-park/CRCN](https://github.com/cesc-park/CRCN)\r\n\r\n**Multimodal Pivots for Image Caption Translation**\r\n\r\n- intro: ACL 2016\r\n- arxiv: [http://arxiv.org/abs/1601.03916](http://arxiv.org/abs/1601.03916)\r\n\r\n**Image Captioning with Deep Bidirectional LSTMs**\r\n\r\n- intro: ACMMM 2016\r\n- arxiv: [http://arxiv.org/abs/1604.00790](http://arxiv.org/abs/1604.00790)\r\n- github(Caffe): [https://github.com/deepsemantic/image_captioning](https://github.com/deepsemantic/image_captioning)\r\n- demo: [https://youtu.be/a0bh9_2LE24](https://youtu.be/a0bh9_2LE24)\r\n\r\n**Encode, Review, and Decode: Reviewer Module for Caption Generation**\r\n\r\n**Review Network for Caption Generation**\r\n\r\n- intro: NIPS 2016\r\n- arxiv: [https://arxiv.org/abs/1605.07912](https://arxiv.org/abs/1605.07912)\r\n- github: [https://github.com/kimiyoung/review_net](https://github.com/kimiyoung/review_net)\r\n\r\n**Attention Correctness in Neural Image Captioning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.09553](http://arxiv.org/abs/1605.09553)\r\n\r\n**Image Caption Generation with Text-Conditional Semantic Attention**\r\n\r\n- arxiv: [https://arxiv.org/abs/1606.04621](https://arxiv.org/abs/1606.04621)\r\n- github: [https://github.com/LuoweiZhou/e2e-gLSTM-sc](https://github.com/LuoweiZhou/e2e-gLSTM-sc)\r\n\r\n**DeepDiary: Automatic Caption Generation for Lifelogging Image Streams**\r\n\r\n- intro: ECCV International Workshop on Egocentric Perception, Interaction, and Computing\r\n- arxiv: [http://arxiv.org/abs/1608.03819](http://arxiv.org/abs/1608.03819)\r\n\r\n**phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning**\r\n\r\n- intro: ACCV 2016\r\n- arxiv: [http://arxiv.org/abs/1608.05813](http://arxiv.org/abs/1608.05813)\r\n\r\n**Captioning Images with Diverse Objects**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.07770](http://arxiv.org/abs/1606.07770)\r\n\r\n**Learning to generalize to new compositions in image understanding**\r\n\r\n- arxiv: [http://arxiv.org/abs/1608.07639](http://arxiv.org/abs/1608.07639)\r\n\r\n**Generating captions without looking beyond objects**\r\n\r\n- intro: ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)\r\n- arxiv: [https://arxiv.org/abs/1610.03708](https://arxiv.org/abs/1610.03708)\r\n\r\n**SPICE: Semantic Propositional Image Caption Evaluation**\r\n\r\n![](http://www.panderson.me/images/spice-concept.png)\r\n\r\n- intro: ECCV 2016\r\n- project page: [http://www.panderson.me/spice/](http://www.panderson.me/spice/)\r\n- paper: [http://www.panderson.me/images/SPICE.pdf](http://www.panderson.me/images/SPICE.pdf)\r\n- github: [https://github.com/peteanderson80/SPICE](https://github.com/peteanderson80/SPICE)\r\n\r\n**Boosting Image Captioning with Attributes**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.01646](https://arxiv.org/abs/1611.01646)\r\n\r\n**Bootstrap, Review, Decode: Using Out-of-Domain Textual Data to Improve Image Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.05321](https://arxiv.org/abs/1611.05321)\r\n\r\n**A Hierarchical Approach for Generating Descriptive Image Paragraphs**\r\n\r\n- intro: Stanford University\r\n- arxiv: [https://arxiv.org/abs/1611.06607](https://arxiv.org/abs/1611.06607)\r\n\r\n**Dense Captioning with Joint Inference and Visual Context**\r\n\r\n- intro: Snap Inc.\r\n- arxiv: [https://arxiv.org/abs/1611.06949](https://arxiv.org/abs/1611.06949)\r\n\r\n**Optimization of image description metrics using policy gradient methods**\r\n\r\n- intro: University of Oxford & Google\r\n- arxiv: [https://arxiv.org/abs/1612.00370](https://arxiv.org/abs/1612.00370)\r\n\r\n**Areas of Attention for Image Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.01033](https://arxiv.org/abs/1612.01033)\r\n\r\n**Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning**\r\n\r\n- intro: CVPR 2017\r\n- arxiv: [https://arxiv.org/abs/1612.01887](https://arxiv.org/abs/1612.01887)\r\n- github: [https://github.com/jiasenlu/AdaptiveAttention](https://github.com/jiasenlu/AdaptiveAttention)\r\n\r\n**Recurrent Image Captioner: Describing Images with Spatial-Invariant Transformation and Attention Filtering**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.04949](https://arxiv.org/abs/1612.04949)\r\n\r\n**Recurrent Highway Networks with Language CNN for Image Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.07086](https://arxiv.org/abs/1612.07086)\r\n\r\n**Top-down Visual Saliency Guided by Captions**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.07360](https://arxiv.org/abs/1612.07360)\r\n- github: [https://github.com/VisionLearningGroup/caption-guided-saliency](https://github.com/VisionLearningGroup/caption-guided-saliency)\r\n\r\n**MAT: A Multimodal Attentive Translator for Image Captioning**\r\n\r\n[https://arxiv.org/abs/1702.05658](https://arxiv.org/abs/1702.05658)\r\n\r\n**Deep Reinforcement Learning-based Image Captioning with Embedding Reward**\r\n\r\n- intro: Snap Inc & Google Inc\r\n- arxiv: [https://arxiv.org/abs/1704.03899](https://arxiv.org/abs/1704.03899)\r\n\r\n**Attend to You: Personalized Image Captioning with Context Sequence Memory Networks**\r\n\r\n- intro: CVPR 2017\r\n- arxiv: [https://arxiv.org/abs/1704.06485](https://arxiv.org/abs/1704.06485)\r\n- github: [https://github.com/cesc-park/attend2u](https://github.com/cesc-park/attend2u)\r\n\r\n**Punny Captions: Witty Wordplay in Image Descriptions**\r\n\r\n[https://arxiv.org/abs/1704.08224](https://arxiv.org/abs/1704.08224)\r\n\r\n**Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner**\r\n\r\n[https://arxiv.org/abs/1705.00930](https://arxiv.org/abs/1705.00930)\r\n\r\n**Actor-Critic Sequence Training for Image Captioning**\r\n\r\n- intro: Queen Mary University of London & Yang’s Accounting Consultancy Ltd\r\n- keywords: actor-critic reinforcement learning\r\n- arxiv: [https://arxiv.org/abs/1706.09601](https://arxiv.org/abs/1706.09601)\r\n\r\n**What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?**\r\n\r\n- intro: Proceedings of the 10th International Conference on Natural Language Generation (INLG'17)\r\n- arxiv: [https://arxiv.org/abs/1708.02043](https://arxiv.org/abs/1708.02043)\r\n\r\n**Stack-Captioning: Coarse-to-Fine Learning for Image Captioning**\r\n\r\n[https://arxiv.org/abs/1709.03376](https://arxiv.org/abs/1709.03376)\r\n\r\n**Self-Guiding Multimodal LSTM - when we do not have a perfect training dataset for image captioning**\r\n\r\n[https://arxiv.org/abs/1709.05038](https://arxiv.org/abs/1709.05038)\r\n\r\n**Contrastive Learning for Image Captioning**\r\n\r\n- intro: NIPS 2017\r\n- arxiv: [https://arxiv.org/abs/1710.02534](https://arxiv.org/abs/1710.02534)\r\n\r\n**Phrase-based Image Captioning with Hierarchical LSTM Model**\r\n\r\n- intro: ACCV2016 extension, phrase-based image captioning\r\n- arxiv: [https://arxiv.org/abs/1711.05557](https://arxiv.org/abs/1711.05557)\r\n\r\n**Convolutional Image Captioning**\r\n\r\n[https://arxiv.org/abs/1711.09151](https://arxiv.org/abs/1711.09151)\r\n\r\n**Show-and-Fool: Crafting Adversarial Examples for Neural Image Captioning**\r\n\r\n[https://arxiv.org/abs/1712.02051](https://arxiv.org/abs/1712.02051)\r\n\r\n**Improved Image Captioning with Adversarial Semantic Alignment**\r\n\r\n- intro: IBM Research\r\n- arxiv: [https://arxiv.org/abs/1805.00063](https://arxiv.org/abs/1805.00063)\r\n\r\n**Object Counts! Bringing Explicit Detections Back into Image Captioning**\r\n\r\n- intro: NAACL 2018\r\n- arxiv: [https://arxiv.org/abs/1805.00314](https://arxiv.org/abs/1805.00314)\r\n\r\n**Defoiling Foiled Image Captions**\r\n\r\n- intro: NAACL 2018\r\n- arxiv: [https://arxiv.org/abs/1805.06549](https://arxiv.org/abs/1805.06549)\r\n\r\n**SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1805.07030](https://arxiv.org/abs/1805.07030)\r\n\r\n**Improving Image Captioning with Conditional Generative Adversarial Nets**\r\n\r\n[https://arxiv.org/abs/1805.07112](https://arxiv.org/abs/1805.07112)\r\n\r\n**CNN+CNN: Convolutional Decoders for Image Captioning**\r\n\r\n[https://arxiv.org/abs/1805.09019](https://arxiv.org/abs/1805.09019)\r\n\r\n**Diverse and Controllable Image Captioning with Part-of-Speech Guidance**\r\n\r\n[https://arxiv.org/abs/1805.12589](https://arxiv.org/abs/1805.12589)\r\n\r\n**Learning to Evaluate Image Captioning**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1806.06422](https://arxiv.org/abs/1806.06422)\r\n\r\n**Topic-Guided Attention for Image Captioning**\r\n\r\n- intro: ICIP 2018\r\n- arxiv: [https://arxiv.org/abs/1807.03514](https://arxiv.org/abs/1807.03514)\r\n\r\n**Context-Aware Visual Policy Network for Sequence-Level Image Captioning**\r\n\r\n- intro: ACM MM 2018 oral\r\n- arxiv: [https://arxiv.org/abs/1808.05864](https://arxiv.org/abs/1808.05864)\r\n- github: [https://github.com/daqingliu/CAVP](https://github.com/daqingliu/CAVP)\r\n\r\n**Exploring Visual Relationship for Image Captioning**\r\n\r\n- intro: ECCV 2018\r\n- arxiv: [https://arxiv.org/abs/1809.07041](https://arxiv.org/abs/1809.07041)\r\n\r\n**Boosted Attention: Leveraging Human Attention for Image Captioning**\r\n\r\n- intro: ECCV 2018\r\n- arxiv: [https://arxiv.org/abs/1904.00767](https://arxiv.org/abs/1904.00767)\r\n\r\n**Image Captioning as Neural Machine Translation Task in SOCKEYE**\r\n\r\n[https://arxiv.org/abs/1810.04101](https://arxiv.org/abs/1810.04101)\r\n\r\n**Unsupervised Image Captioning**\r\n\r\n[https://arxiv.org/abs/1811.10787](https://arxiv.org/abs/1811.10787)\r\n\r\n**Attend More Times for Image Captioning**\r\n\r\n[https://arxiv.org/abs/1812.03283](https://arxiv.org/abs/1812.03283)\r\n\r\n# Object Descriptions\r\n\r\n**Generation and Comprehension of Unambiguous Object Descriptions**\r\n\r\n- arxiv: [https://arxiv.org/abs/1511.02283](https://arxiv.org/abs/1511.02283)\r\n- github: [https://github.com/mjhucla/Google_Refexp_toolbox](https://github.com/mjhucla/Google_Refexp_toolbox)\r\n\r\n# Video Captioning / Description\r\n\r\n**Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework**\r\n\r\n- intro: AAAI 2015\r\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf)\r\n- paper: [http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf](http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf)\r\n\r\n**Translating Videos to Natural Language Using Deep Recurrent Neural Networks**\r\n\r\n![](https://www.cs.utexas.edu/~vsub/imgs/naacl-15-overview.png)\r\n\r\n- intro: NAACL-HLT 2015 camera ready\r\n- project page: [https://www.cs.utexas.edu/~vsub/naacl15_project.html](https://www.cs.utexas.edu/~vsub/naacl15_project.html)\r\n- arxiv: [http://arxiv.org/abs/1412.4729](http://arxiv.org/abs/1412.4729)\r\n- slides: [https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_slides.pdf](https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_slides.pdf)\r\n- code+data: [https://www.cs.utexas.edu/~vsub/naacl15_project.html#code](https://www.cs.utexas.edu/~vsub/naacl15_project.html#code)\r\n\r\n**Describing Videos by Exploiting Temporal Structure**\r\n\r\n- arxiv: [http://arxiv.org/abs/1502.08029](http://arxiv.org/abs/1502.08029)\r\n- github: [https://github.com/yaoli/arctic-capgen-vid](https://github.com/yaoli/arctic-capgen-vid)\r\n\r\n**SA-tensorflow: Soft attention mechanism for video caption generation**\r\n\r\n![](https://raw.githubusercontent.com/tsenghungchen/SA-tensorflow/master/README_files/head.png)\r\n\r\n- github: [https://github.com/tsenghungchen/SA-tensorflow](https://github.com/tsenghungchen/SA-tensorflow)\r\n\r\n**Sequence to Sequence -- Video to Text**\r\n\r\n![](http://www.cs.utexas.edu/~vsub/imgs/S2VTarchitecture.png)\r\n\r\n- intro: ICCV 2015. S2VT\r\n- project page: [http://vsubhashini.github.io/s2vt.html](http://vsubhashini.github.io/s2vt.html)\r\n- arxiv: [http://arxiv.org/abs/1505.00487](http://arxiv.org/abs/1505.00487)\r\n- slides: [https://www.cs.utexas.edu/~vsub/pdf/S2VT_slides.pdf](https://www.cs.utexas.edu/~vsub/pdf/S2VT_slides.pdf)\r\n- github(Caffe): [https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt)\r\n- github(TensorFlow): [https://github.com/jazzsaxmafia/video_to_sequence](https://github.com/jazzsaxmafia/video_to_sequence)\r\n\r\n**Jointly Modeling Embedding and Translation to Bridge Video and Language**\r\n\r\n- arxiv: [http://arxiv.org/abs/1505.01861](http://arxiv.org/abs/1505.01861)\r\n\r\n**Video Description using Bidirectional Recurrent Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1604.03390](http://arxiv.org/abs/1604.03390)\r\n\r\n**Bidirectional Long-Short Term Memory for Video Description**\r\n\r\n- arxiv: [https://arxiv.org/abs/1606.04631](https://arxiv.org/abs/1606.04631)\r\n\r\n**3 Ways to Subtitle and Caption Your Videos Automatically Using Artificial Intelligence**\r\n\r\n- blog: [http://photography.tutsplus.com/tutorials/3-ways-to-subtitle-and-caption-your-videos-automatically-using-artificial-intelligence--cms-26834](http://photography.tutsplus.com/tutorials/3-ways-to-subtitle-and-caption-your-videos-automatically-using-artificial-intelligence--cms-26834)\r\n\r\n**Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1608.04959](http://arxiv.org/abs/1608.04959)\r\n\r\n**Grounding and Generation of Natural Language Descriptions for Images and Videos**\r\n\r\n- intro: Anna Rohrbach. Allen Institute for Artificial Intelligence (AI2)\r\n- youtube: [https://www.youtube.com/watch?v=fE3FX8FowiU](https://www.youtube.com/watch?v=fE3FX8FowiU)\r\n\r\n**Video Captioning and Retrieval Models with Semantic Attention**\r\n\r\n- intro: Winner of three (fill-in-the-blank, multiple-choice test, and movie retrieval) out of four tasks of the LSMDC 2016 Challenge (Workshop in ECCV 2016)\r\n- arxiv: [https://arxiv.org/abs/1610.02947](https://arxiv.org/abs/1610.02947)\r\n\r\n**Spatio-Temporal Attention Models for Grounded Video Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.04997](https://arxiv.org/abs/1610.04997)\r\n\r\n**Video and Language: Bridging Video and Language with Deep Learning**\r\n\r\n- intro: ECCV-MM 2016. captioning, commenting, alignment\r\n- slides: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/Video-and-Language-ECCV-MM-2016-Tao-Mei-Pub.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/Video-and-Language-ECCV-MM-2016-Tao-Mei-Pub.pdf)\r\n\r\n**Recurrent Memory Addressing for describing videos**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.06492](https://arxiv.org/abs/1611.06492)\r\n\r\n**Video Captioning with Transferred Semantic Attributes**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.07675](https://arxiv.org/abs/1611.07675)\r\n\r\n**Adaptive Feature Abstraction for Translating Video to Language**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.07837](https://arxiv.org/abs/1611.07837)\r\n\r\n**Semantic Compositional Networks for Visual Captioning**\r\n\r\n- intro: CVPR 2017. Duke University & Tsinghua University & MSR\r\n- arxiv: [https://arxiv.org/abs/1611.08002](https://arxiv.org/abs/1611.08002)\r\n- github: [https://github.com/zhegan27/SCN_for_video_captioning](https://github.com/zhegan27/SCN_for_video_captioning)\r\n\r\n**Hierarchical Boundary-Aware Neural Encoder for Video Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.09312](https://arxiv.org/abs/1611.09312)\r\n\r\n**Attention-Based Multimodal Fusion for Video Description**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.03126](https://arxiv.org/abs/1701.03126)\r\n\r\n**Weakly Supervised Dense Video Captioning**\r\n\r\n- intro: CVPR 2017\r\n- arxiv: [https://arxiv.org/abs/1704.01502](https://arxiv.org/abs/1704.01502)\r\n\r\n**Generating Descriptions with Grounded and Co-Referenced People**\r\n\r\n- intro: CVPR 2017. movie description\r\n- arxiv: [https://arxiv.org/abs/1704.01518](https://arxiv.org/abs/1704.01518)\r\n\r\n**Multi-Task Video Captioning with Video and Entailment Generation**\r\n\r\n- intro: ACL 2017. UNC Chapel Hill\r\n- arxiv: [https://arxiv.org/abs/1704.07489](https://arxiv.org/abs/1704.07489)\r\n\r\n**Dense-Captioning Events in Videos**\r\n\r\n- project page: [http://cs.stanford.edu/people/ranjaykrishna/densevid/](http://cs.stanford.edu/people/ranjaykrishna/densevid/)\r\n- arxiv: [https://arxiv.org/abs/1705.00754](https://arxiv.org/abs/1705.00754)\r\n\r\n**Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1706.01231](https://arxiv.org/abs/1706.01231)\r\n\r\n**Reinforced Video Captioning with Entailment Rewards**\r\n\r\n- intro: EMNLP 2017. UNC Chapel Hill\r\n- arxiv: [https://arxiv.org/abs/1708.02300](https://arxiv.org/abs/1708.02300)\r\n\r\n**End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering**\r\n\r\n- intro: CVPR 2017. Winner of three (fill-in-the-blank, multiple-choice test, and movie retrieval) out of four tasks of the LSMDC 2016 Challenge\r\n- arxiv: [https://arxiv.org/abs/1610.02947](https://arxiv.org/abs/1610.02947)\r\n- slides: [https://drive.google.com/file/d/0B9nOObAFqKC9aHl2VWJVNFp1bFk/view](https://drive.google.com/file/d/0B9nOObAFqKC9aHl2VWJVNFp1bFk/view)\r\n\r\n**From Deterministic to Generative: Multi-Modal Stochastic RNNs for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1708.02478](https://arxiv.org/abs/1708.02478)\r\n\r\n**Grounded Objects and Interactions for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1711.06354](https://arxiv.org/abs/1711.06354)\r\n\r\n**Integrating both Visual and Audio Cues for Enhanced Video Caption**\r\n\r\n[https://arxiv.org/abs/1711.08097](https://arxiv.org/abs/1711.08097)\r\n\r\n**Video Captioning via Hierarchical Reinforcement Learning**\r\n\r\n[https://arxiv.org/abs/1711.11135](https://arxiv.org/abs/1711.11135)\r\n\r\n**Consensus-based Sequence Training for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1712.09532](https://arxiv.org/abs/1712.09532)\r\n\r\n**Less Is More: Picking Informative Frames for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1803.01457](https://arxiv.org/abs/1803.01457)\r\n\r\n**End-to-End Video Captioning with Multitask Reinforcement Learning**\r\n\r\n[https://arxiv.org/abs/1803.07950](https://arxiv.org/abs/1803.07950)\r\n\r\n**End-to-End Dense Video Captioning with Masked Transformer**\r\n\r\n- intro: CVPR 2018. University of Michigan & Salesforce Research\r\n- arxiv: [https://arxiv.org/abs/1804.00819](https://arxiv.org/abs/1804.00819)\r\n\r\n**Reconstruction Network for Video Captioning**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1803.11438](https://arxiv.org/abs/1803.11438)\r\n\r\n**Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning**\r\n\r\n- intro: CVPR 2018 spotlight paper\r\n- arxiv: [https://arxiv.org/abs/1804.00100](https://arxiv.org/abs/1804.00100)\r\n\r\n**Jointly Localizing and Describing Events for Dense Video Captioning**\r\n\r\n- intro: CVPR 2018 Spotlight, Rank 1 in ActivityNet Captions Challenge 2017\r\n- arxiv: [https://arxiv.org/abs/1804.08274](https://arxiv.org/abs/1804.08274)\r\n\r\n**Contextualize, Show and Tell: A Neural Visual Storyteller**\r\n\r\n[https://arxiv.org/abs/1806.00738](https://arxiv.org/abs/1806.00738)\r\n\r\n**RUC+CMU: System Report for Dense Captioning Events in Videos**\r\n\r\n- intro: Winner in ActivityNet 2018 Dense Video Captioning challenge\r\n- arxiv: [https://arxiv.org/abs/1806.08854](https://arxiv.org/abs/1806.08854)\r\n\r\n**Streamlined Dense Video Captioning**\r\n\r\n- intro: CVPR 2019\r\n- arxiv: [https://arxiv.org/abs/1904.03870](https://arxiv.org/abs/1904.03870)\r\n\r\n# Projects\r\n\r\n**Learning CNN-LSTM Architectures for Image Caption Generation: An implementation of CNN-LSTM image caption generator architecture that achieves close to state-of-the-art results on the MSCOCO dataset.**\r\n\r\n- github: [https://github.com/mosessoh/CNN-LSTM-Caption-Generator](https://github.com/mosessoh/CNN-LSTM-Caption-Generator)\r\n\r\n**screengrab-caption: an openframeworks app that live-captions your desktop screen with a neural net**\r\n\r\n- intro: openframeworks app which grabs your desktop screen, then sends it to darknet for captioning. \r\nworks great with video calls.\r\n- github: [https://github.com/genekogan/screengrab-caption](https://github.com/genekogan/screengrab-caption)\r\n\r\n# Tools\r\n\r\n**CaptionBot (Microsoft)**\r\n\r\n- website: [https://www.captionbot.ai/](https://www.captionbot.ai/)\r\n\r\n# Blogs\r\n\r\n**Captioning Novel Objects in Images**\r\n\r\n[http://bair.berkeley.edu/jacky/2017/08/08/novel-object-captioning/](http://bair.berkeley.edu/jacky/2017/08/08/novel-object-captioning/)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","title":"Acceleration and Model Compression"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Acceleration and Model Compression\ndate: 2015-10-09\n---\n\n# Papers\n\n**High-Performance Neural Networks for Visual Object Classification**\n\n- intro: \"reduced network parameters by randomly removing connections before training\"\n- arxiv: [http://arxiv.org/abs/1102.0183](http://arxiv.org/abs/1102.0183)\n\n**Predicting Parameters in Deep Learning**\n\n- intro: \"decomposed the weighting matrix into two low-rank matrices\"\n- arxiv: [http://arxiv.org/abs/1306.0543](http://arxiv.org/abs/1306.0543)\n\n**Neurons vs Weights Pruning in Artificial Neural Networks**\n\n- paper: [http://journals.ru.lv/index.php/ETR/article/view/166](http://journals.ru.lv/index.php/ETR/article/view/166)\n\n**Exploiting Linear Structure Within Convolutional Networks for Efﬁcient Evaluation**\n\n- intro: \"presented a series of low-rank decomposition designs for convolutional kernels. \nsingular value decomposition was adopted for the matrix factorization\"\n- paper: [http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf](http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf)\n\n**cuDNN: Efficient Primitives for Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1410.0759](https://arxiv.org/abs/1410.0759)\n- download: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)\n\n**Efficient and accurate approximations of nonlinear convolutional networks**\n\n- intro: \"considered the subsequent nonlinear units while learning the low-rank decomposition\"\n- arxiv: [http://arxiv.org/abs/1411.4229](http://arxiv.org/abs/1411.4229)\n\n**Convolutional Neural Networks at Constrained Time Cost**\n\n- arxiv: [https://arxiv.org/abs/1412.1710](https://arxiv.org/abs/1412.1710)\n\n**Flattened Convolutional Neural Networks for Feedforward Acceleration**\n\n- intro: ICLR 2015\n- arxiv: [http://arxiv.org/abs/1412.5474](http://arxiv.org/abs/1412.5474)\n- github: [https://github.com/jhjin/flattened-cnn](https://github.com/jhjin/flattened-cnn)\n\n**Compressing Deep Convolutional Networks using Vector Quantization**\n\n- intro: \"this paper showed that vector quantization had a clear advantage \nover matrix factorization methods in compressing fully-connected layers.\"\n- arxiv: [http://arxiv.org/abs/1412.6115](http://arxiv.org/abs/1412.6115)\n\n**Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition**\n\n- intro: \"a low-rank CPdecomposition was adopted to \ntransform a convolutional layer into multiple layers of lower complexity\"\n- arxiv: [http://arxiv.org/abs/1412.6553](http://arxiv.org/abs/1412.6553)\n\n**Deep Fried Convnets**\n\n- intro: \"fully-connected layers were replaced by a single “Fastfood” layer for end-to-end training with convolutional layers\"\n- arxiv: [http://arxiv.org/abs/1412.7149](http://arxiv.org/abs/1412.7149)\n\n**Fast Convolutional Nets With fbfft: A GPU Performance Evaluation**\n\n- intro: Facebook. ICLR 2015\n- arxiv: [http://arxiv.org/abs/1412.7580](http://arxiv.org/abs/1412.7580)\n- github: [http://facebook.github.io/fbcunn/fbcunn/](http://facebook.github.io/fbcunn/fbcunn/)\n\n**Caffe con Troll: Shallow Ideas to Speed Up Deep Learning**\n\n- intro: a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals\n- arxiv: [http://arxiv.org/abs/1504.04343](http://arxiv.org/abs/1504.04343)\n\n**Compressing Neural Networks with the Hashing Trick**\n\n![](http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/hashednets.png)\n\n- intro: HashedNets. ICML 2015\n- intro: \"randomly grouped connection weights into hash buckets, and then fine-tuned network parameters with back-propagation\"\n- project page: [http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/index.html](http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/index.html)\n- arxiv: [http://arxiv.org/abs/1504.04788](http://arxiv.org/abs/1504.04788)\n- code: [http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/HashedNets.zip](http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/HashedNets.zip)\n\n**PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1504.08362](https://arxiv.org/abs/1504.08362)\n- github: [https://github.com/mfigurnov/perforated-cnn-matconvnet](https://github.com/mfigurnov/perforated-cnn-matconvnet)\n- github: [https://github.com/mfigurnov/perforated-cnn-caffe](https://github.com/mfigurnov/perforated-cnn-caffe)\n\n**Accelerating Very Deep Convolutional Networks for Classification and Detection**\n\n- intro: \"considered the subsequent nonlinear units while learning the low-rank decomposition\"\n- arxiv: [http://arxiv.org/abs/1505.06798](http://arxiv.org/abs/1505.06798)\n\n**Fast ConvNets Using Group-wise Brain Damage**\n\n- intro: \"applied group-wise pruning to the convolutional tensor \nto decompose it into the multiplications of thinned dense matrices\"\n- arxiv: [http://arxiv.org/abs/1506.02515](http://arxiv.org/abs/1506.02515)\n\n**Learning both Weights and Connections for Efficient Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1506.02626](http://arxiv.org/abs/1506.02626)\n\n**Data-free parameter pruning for Deep Neural Networks**\n\n- intro: \"proposed to remove redundant neurons instead of network connections\"\n- arxiv: [http://arxiv.org/abs/1507.06149](http://arxiv.org/abs/1507.06149)\n\n**Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding**\n\n- intro: ICLR 2016 Best Paper\n- intro: \"reduced the size of AlexNet by 35x from 240MB to 6.9MB, the size of VGG16 by 49x from 552MB to 11.3MB, with no loss of accuracy\"\n- arxiv: [http://arxiv.org/abs/1510.00149](http://arxiv.org/abs/1510.00149)\n- video: [http://videolectures.net/iclr2016_han_deep_compression/](http://videolectures.net/iclr2016_han_deep_compression/)\n\n**Structured Transforms for Small-Footprint Deep Learning**\n\n- intro: NIPS 2015\n- arxiv: [https://arxiv.org/abs/1510.01722](https://arxiv.org/abs/1510.01722)\n- paper: [https://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning](https://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning)\n\n**ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines**\n\n- arxiv: [http://arxiv.org/abs/1510.06706](http://arxiv.org/abs/1510.06706)\n- github: [https://github.com/seung-lab/znn-release](https://github.com/seung-lab/znn-release)\n\n**Reducing the Training Time of Neural Networks by Partitioning**\n\n- arxiv: [http://arxiv.org/abs/1511.02954](http://arxiv.org/abs/1511.02954)\n\n**Convolutional neural networks with low-rank regularization**\n\n- arxiv: [http://arxiv.org/abs/1511.06067](http://arxiv.org/abs/1511.06067)\n- github: [https://github.com/chengtaipu/lowrankcnn](https://github.com/chengtaipu/lowrankcnn)\n\n**CNNdroid: Open Source Library for GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android**\n\n- arxiv: [https://arxiv.org/abs/1511.07376](https://arxiv.org/abs/1511.07376)\n- paper: [http://dl.acm.org/authorize.cfm?key=N14731](http://dl.acm.org/authorize.cfm?key=N14731)\n- slides: [http://sharif.edu/~matin/pub/2016_mm_slides.pdf](http://sharif.edu/~matin/pub/2016_mm_slides.pdf)\n- github: [https://github.com/ENCP/CNNdroid](https://github.com/ENCP/CNNdroid)\n\n**EIE: Efficient Inference Engine on Compressed Deep Neural Network**\n\n- intro: ISCA 2016\n- arxiv: [http://arxiv.org/abs/1602.01528](http://arxiv.org/abs/1602.01528)\n- slides: [http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf](http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf)\n- slides: [http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf](http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf)\n\n**Convolutional Tables Ensemble: classification in microseconds**\n\n- arxiv: [http://arxiv.org/abs/1602.04489](http://arxiv.org/abs/1602.04489)\n\n**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size**\n\n- intro: DeepScale & UC Berkeley\n- arxiv: [http://arxiv.org/abs/1602.07360](http://arxiv.org/abs/1602.07360)\n- github: [https://github.com/DeepScale/SqueezeNet](https://github.com/DeepScale/SqueezeNet)\n- homepage: [http://songhan.github.io/SqueezeNet-Deep-Compression/](http://songhan.github.io/SqueezeNet-Deep-Compression/)\n- github: [https://github.com/songhan/SqueezeNet-Deep-Compression](https://github.com/songhan/SqueezeNet-Deep-Compression)\n- note: [https://www.evernote.com/shard/s146/sh/108eea91-349b-48ba-b7eb-7ac8f548bee9/5171dc6b1088fba05a4e317f7f5d32a3](https://www.evernote.com/shard/s146/sh/108eea91-349b-48ba-b7eb-7ac8f548bee9/5171dc6b1088fba05a4e317f7f5d32a3)\n- github(Keras): [https://github.com/DT42/squeezenet_demo](https://github.com/DT42/squeezenet_demo)\n- github(Keras): [https://github.com/rcmalli/keras-squeezenet](https://github.com/rcmalli/keras-squeezenet)\n- github(PyTorch): [https://github.com/gsp-27/pytorch_Squeezenet](https://github.com/gsp-27/pytorch_Squeezenet)\n\n**SqueezeNet-Residual**\n\n- intro: Residual-SqueezeNet improves the top-1 accuracy of SqueezeNet by 2.9% on ImageNet without changing the model size(only 4.8MB).\n- github: [https://github.com/songhan/SqueezeNet-Residual](https://github.com/songhan/SqueezeNet-Residual)\n\n**Lab41 Reading Group: SqueezeNet**\n\n[https://medium.com/m/global-identity?redirectUrl=https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75](https://medium.com/m/global-identity?redirectUrl=https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75)\n\n**Simplified_SqueezeNet**\n\n- intro: An improved version of SqueezeNet networks\n- github(Caffe): [https://github.com/NidabaSystems/Simplified_SqueezeNet](https://github.com/NidabaSystems/Simplified_SqueezeNet)\n\n**SqueezeNet Keras Dogs vs. Cats demo**\n\n- github: [https://github.com/chasingbob/squeezenet-keras](https://github.com/chasingbob/squeezenet-keras)\n\n**Convolutional Neural Networks using Logarithmic Data Representation**\n\n- arxiv: [http://arxiv.org/abs/1603.01025](http://arxiv.org/abs/1603.01025)\n\n**DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices**\n\n- paper: [http://niclane.org/pubs/deepx_ipsn.pdf](http://niclane.org/pubs/deepx_ipsn.pdf)\n\n**Hardware-oriented Approximation of Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.03168](http://arxiv.org/abs/1604.03168)\n- homepage: [http://ristretto.lepsucd.com/](http://ristretto.lepsucd.com/)\n- github(\"Ristretto: Caffe-based approximation of convolutional neural networks\"): [https://github.com/pmgysel/caffe](https://github.com/pmgysel/caffe)\n\n**Deep Neural Networks Under Stress**\n\n- intro: ICIP 2016\n- arxiv: [http://arxiv.org/abs/1605.03498](http://arxiv.org/abs/1605.03498)\n- github: [https://github.com/MicaelCarvalho/DNNsUnderStress](https://github.com/MicaelCarvalho/DNNsUnderStress)\n\n**ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks using Angle Sensitive Pixels**\n\n- arxiv: [http://arxiv.org/abs/1605.03621](http://arxiv.org/abs/1605.03621)\n\n**Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups**\n\n- intro: \"for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU).\nFor the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, \nwhile maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).\"\n- arxiv: [https://arxiv.org/abs/1605.06489](https://arxiv.org/abs/1605.06489)\n\n**Functional Hashing for Compressing Neural Networks**\n\n- intro: FunHashNN\n- arxiv: [http://arxiv.org/abs/1605.06560](http://arxiv.org/abs/1605.06560)\n\n**Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1605.06402](http://arxiv.org/abs/1605.06402)\n\n**YodaNN: An Ultra-Low Power Convolutional Neural Network Accelerator Based on Binary Weights**\n\n- arxiv: [https://arxiv.org/abs/1606.05487](https://arxiv.org/abs/1606.05487)\n\n**Learning Structured Sparsity in Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1608.03665](http://arxiv.org/abs/1608.03665)\n\n**Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial \"Bottleneck\" Structure**\n\n[https://arxiv.org/abs/1608.04337](https://arxiv.org/abs/1608.04337)\n\n**Dynamic Network Surgery for Efficient DNNs**\n\n- intro: NIPS 2016\n- intro: compress the number of parameters in LeNet-5 and AlexNet by a factor of 108× and 17.7× respectively\n- arxiv: [http://arxiv.org/abs/1608.04493](http://arxiv.org/abs/1608.04493)\n- github(official. Caffe): [https://github.com/yiwenguo/Dynamic-Network-Surgery](https://github.com/yiwenguo/Dynamic-Network-Surgery)\n\n**Scalable Compression of Deep Neural Networks**\n\n- intro: ACM Multimedia 2016\n- arxiv: [http://arxiv.org/abs/1608.07365](http://arxiv.org/abs/1608.07365)\n\n**Pruning Filters for Efficient ConvNets**\n\n- intro: NIPS Workshop on Efficient Methods for Deep Neural Networks (EMDNN), 2016\n- arxiv: [http://arxiv.org/abs/1608.08710](http://arxiv.org/abs/1608.08710)\n\n**Fixed-point Factorized Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.01972](https://arxiv.org/abs/1611.01972)\n\n**Ultimate tensorization: compressing convolutional and FC layers alike**\n\n- intro: NIPS 2016 workshop: Learning with Tensors: Why Now and How?\n- arxiv: [https://arxiv.org/abs/1611.03214](https://arxiv.org/abs/1611.03214)\n- github: [https://github.com/timgaripov/TensorNet-TF](https://github.com/timgaripov/TensorNet-TF)\n\n**Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning**\n\n- intro: \"the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss\"\n- arxiv: [https://arxiv.org/abs/1611.05128](https://arxiv.org/abs/1611.05128)\n\n**Net-Trim: A Layer-wise Convex Pruning of Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.05162](https://arxiv.org/abs/1611.05162)\n\n**LCNN: Lookup-based Convolutional Neural Network**\n\n- intro: \"Our fastest LCNN offers 37.6x speed up over AlexNet while maintaining 44.3% top-1 accuracy.\"\n- arxiv: [https://arxiv.org/abs/1611.06473](https://arxiv.org/abs/1611.06473)\n\n**Deep Tensor Convolution on Multicores**\n\n- intro: present the first practical CPU implementation of tensor convolution optimized for deep networks of small kernels\n- arxiv: [https://arxiv.org/abs/1611.06565](https://arxiv.org/abs/1611.06565)\n\n**Training Sparse Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.06694](https://arxiv.org/abs/1611.06694)\n\n**FINN: A Framework for Fast, Scalable Binarized Neural Network Inference**\n\n- intro: Xilinx Research Labs & Norwegian University of Science and Technology & University of Sydney\n- intro: 25th International Symposium on Field-Programmable Gate Arrays\n- keywords: FPGA\n- paper: [http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf](http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf)\n- arxiv: [https://arxiv.org/abs/1612.07119](https://arxiv.org/abs/1612.07119)\n- github: [https://github.com/Xilinx/BNN-PYNQ](https://github.com/Xilinx/BNN-PYNQ)\n\n**Deep Learning with INT8 Optimization on Xilinx Devices**\n\n- intro: \"Xilinx's integrated DSP architecture can achieve 1.75X solution-level performance \nat INT8 deep learning operations than other FPGA DSP architectures\"\n- paper: [https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf](https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf)\n\n**Parameter Compression of Recurrent Neural Networks and Degredation of Short-term Memory**\n\n- arxiv: [https://arxiv.org/abs/1612.00891](https://arxiv.org/abs/1612.00891)\n\n**An OpenCL(TM) Deep Learning Accelerator on Arria 10**\n\n- intro: FPGA 2017\n- arxiv: [https://arxiv.org/abs/1701.03534](https://arxiv.org/abs/1701.03534)\n\n**The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning**\n\n- intro: CMU & Universitat Paderborn]\n- arxiv: [https://arxiv.org/abs/1701.04465](https://arxiv.org/abs/1701.04465)\n\n**DL-gleaning: An Approach For Improving Inference Speed And Accuracy**\n\n- intro: Electronics Telecommunications Research Institute (ETRI)\n- paper: [https://openreview.net/pdf?id=Hynn8SHOx](https://openreview.net/pdf?id=Hynn8SHOx)\n\n**Energy Saving Additive Neural Network**\n\n- intro: Middle East Technical University & Bilkent University\n- arxiv: [https://arxiv.org/abs/1702.02676](https://arxiv.org/abs/1702.02676)\n\n**Soft Weight-Sharing for Neural Network Compression**\n\n- intro: ICLR 2017. University of Amsterdam\n- arxiv: [https://arxiv.org/abs/1702.04008](https://arxiv.org/abs/1702.04008)\n- github: [https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression](https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression)\n\n**A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1703.04071](https://arxiv.org/abs/1703.04071)\n\n**Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations**\n\n- intro: ARM Research\n- arxiv: [https://arxiv.org/abs/1703.03073](https://arxiv.org/abs/1703.03073)\n\n**DyVEDeep: Dynamic Variable Effort Deep Neural Networks**\n\n[https://arxiv.org/abs/1704.01137](https://arxiv.org/abs/1704.01137)\n\n**Bayesian Compression for Deep Learning**\n\n[https://arxiv.org/abs/1705.08665](https://arxiv.org/abs/1705.08665)\n\n**A Kernel Redundancy Removing Policy for Convolutional Neural Network**\n\n[https://arxiv.org/abs/1705.10748](https://arxiv.org/abs/1705.10748)\n\n**Gated XNOR Networks: Deep Neural Networks with Ternary Weights and Activations under a Unified Discretization Framework**\n\n- keywords: discrete state transition (DST)\n- arxiv: [https://arxiv.org/abs/1705.09283](https://arxiv.org/abs/1705.09283)\n\n**SEP-Nets: Small and Effective Pattern Networks**\n\n- intro: The University of Iowa & Snap Research\n- arxiv: [https://arxiv.org/abs/1706.03912](https://arxiv.org/abs/1706.03912)\n\n**MEC: Memory-efficient Convolution for Deep Neural Network**\n\n- intro: ICML 2017\n- arxiv: [https://arxiv.org/abs/1706.06873](https://arxiv.org/abs/1706.06873)\n\n**Data-Driven Sparse Structure Selection for Deep Neural Networks**\n\n[https://arxiv.org/abs/1707.01213](https://arxiv.org/abs/1707.01213)\n\n**An End-to-End Compression Framework Based on Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1708.00838](https://arxiv.org/abs/1708.00838)\n\n**Domain-adaptive deep network compression**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.01041](https://arxiv.org/abs/1709.01041)\n- github: [https://github.com/mmasana/DALR](https://github.com/mmasana/DALR)\n\n**Binary-decomposed DCNN for accelerating computation and compressing model without retraining**\n\n[https://arxiv.org/abs/1709.04731](https://arxiv.org/abs/1709.04731)\n\n**Improving Efficiency in Convolutional Neural Network with Multilinear Filters**\n\n[https://arxiv.org/abs/1709.09902](https://arxiv.org/abs/1709.09902)\n\n**A Survey of Model Compression and Acceleration for Deep Neural Networks**\n\n- intro: IEEE Signal Processing Magazine. IBM Thoms J. Watson Research Center & Tsinghua University & Huazhong University of Science and Technology\n- arxiv: [https://arxiv.org/abs/1710.09282](https://arxiv.org/abs/1710.09282)\n\n**Compression-aware Training of Deep Networks**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1711.02638](https://arxiv.org/abs/1711.02638)\n\n**Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method**\n\n[https://arxiv.org/abs/1711.06528](https://arxiv.org/abs/1711.06528)\n\n**Reducing Deep Network Complexity with Fourier Transform Methods**\n\n- intro: Harvard University\n- arxiv: [https://arxiv.org/abs/1801.01451](https://arxiv.org/abs/1801.01451)\n- github: [https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods](https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods)\n\n**EffNet: An Efficient Structure for Convolutional Neural Networks**\n\n- intro: Aptiv & University of Wupperta\n- arxiv: [https://arxiv.org/abs/1801.06434](https://arxiv.org/abs/1801.06434)\n\n**Universal Deep Neural Network Compression**\n\n[https://arxiv.org/abs/1802.02271](https://arxiv.org/abs/1802.02271)\n\n**Paraphrasing Complex Network: Network Compression via Factor Transfer**\n\n[https://arxiv.org/abs/1802.04977](https://arxiv.org/abs/1802.04977)\n\n**Compressing Neural Networks using the Variational Information Bottleneck**\n\n- intro: Tsinghua University & ShanghaiTech University & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1802.10399](https://arxiv.org/abs/1802.10399)\n\n**Adversarial Network Compression**\n\n[https://arxiv.org/abs/1803.10750](https://arxiv.org/abs/1803.10750)\n\n**Expanding a robot's life: Low power object recognition via FPGA-based DCNN deployment**\n\n- intro: MOCAST 2018\n- arxiv: [https://arxiv.org/abs/1804.00512](https://arxiv.org/abs/1804.00512)\n\n**Accelerating CNN inference on FPGAs: A Survey**\n\n- intro: [Institut Pascal]\n- arxiv: [https://arxiv.org/abs/1806.01683](https://arxiv.org/abs/1806.01683)\n\n**Doubly Nested Network for Resource-Efficient Inference**\n\n[https://arxiv.org/abs/1806.07568](https://arxiv.org/abs/1806.07568)\n\n**Smallify: Learning Network Size while Training**\n\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1806.03723](https://arxiv.org/abs/1806.03723)\n\n**Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on Embedded FPGAs**\n\n- intro: 27th International Symposium on Field-Programmable Gate Arrays, February 2019\n- arxiv: [https://arxiv.org/abs/1811.08634](https://arxiv.org/abs/1811.08634)\n\n**Cascaded Projection: End-to-End Network Compression and Acceleration**\n\n[https://arxiv.org/abs/1903.04988](https://arxiv.org/abs/1903.04988)\n\n**FALCON: Fast and Lightweight Convolution for Compressing and Accelerating CNN**\n\n[https://arxiv.org/abs/1909.11321](https://arxiv.org/abs/1909.11321)\n\n# Compressing Deep Neural Network\n\n**Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions**\n\n- intro: ICML 2018\n- arxiv: [https://arxiv.org/abs/1806.09228](https://arxiv.org/abs/1806.09228)\n- github: [https://github.com/Sandbox3aster/Deep-K-Means-pytorch](https://github.com/Sandbox3aster/Deep-K-Means-pytorch)\n\n**Optimize Deep Convolutional Neural Network with Ternarized Weights and High Accuracy**\n\n- intro: University of Central Florida & Tencent AI lab, Seattle\n- arxiv: [https://arxiv.org/abs/1807.07948](https://arxiv.org/abs/1807.07948)\n\n**Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks**\n\n[https://arxiv.org/abs/1808.05240](https://arxiv.org/abs/1808.05240)\n\n**ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1809.01330](https://arxiv.org/abs/1809.01330)\n\n**A Framework for Fast and Efficient Neural Network Compression**\n\n[https://arxiv.org/abs/1811.12781](https://arxiv.org/abs/1811.12781)\n\n**ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples**\n\n[https://arxiv.org/abs/1811.12673](https://arxiv.org/abs/1811.12673)\n\n# Pruning\n\n**ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression**\n\n- intro: ICCV 2017. Nanjing University & Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/1707.06342](https://arxiv.org/abs/1707.06342)\n- github(Caffe): [https://github.com/Roll920/ThiNet](https://github.com/Roll920/ThiNet)\n\n**Neuron Pruning for Compressing Deep Networks using Maxout Architectures**\n\n- intro: GCPR 2017\n- arxiv: [https://arxiv.org/abs/1707.06838](https://arxiv.org/abs/1707.06838)\n\n**Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional Network with Bayesian Optimization**\n\n- intro: BMVC 2017 oral. Simon Fraser University\n- arxiv: [https://arxiv.org/abs/1707.09102](https://arxiv.org/abs/1707.09102)\n\n**Prune the Convolutional Neural Networks with Sparse Shrink**\n\n[https://arxiv.org/abs/1708.02439](https://arxiv.org/abs/1708.02439)\n\n**NISP: Pruning Networks using Neuron Importance Score Propagation**\n\n- intro: University of Maryland & IBM T. J. Watson Research\n- arxiv: [https://arxiv.org/abs/1711.05908](https://arxiv.org/abs/1711.05908)\n\n**Automated Pruning for Deep Neural Network Compression**\n\n[https://arxiv.org/abs/1712.01721](https://arxiv.org/abs/1712.01721)\n\n**Learning to Prune Filters in Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1801.07365](https://arxiv.org/abs/1801.07365)\n\n**Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.10447](https://arxiv.org/abs/1801.10447)\n\n**A novel channel pruning method for deep neural network compression**\n\n[https://arxiv.org/abs/1805.11394](https://arxiv.org/abs/1805.11394)\n\n**PCAS: Pruning Channels with Attention Statistics**\n\n- intro: Oki Electric Industry Co., Ltd\n- arxiv: [https://arxiv.org/abs/1806.05382](https://arxiv.org/abs/1806.05382)\n\n**Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1808.06866](https://arxiv.org/abs/1808.06866)\n- github: [https://github.com/he-y/soft-filter-pruning](https://github.com/he-y/soft-filter-pruning)\n\n**Progressive Deep Neural Networks Acceleration via Soft Filter Pruning**\n\n[https://arxiv.org/abs/1808.07471](https://arxiv.org/abs/1808.07471)\n\n**Pruning neural networks: is it time to nip it in the bud?**\n\n[https://arxiv.org/abs/1810.04622](https://arxiv.org/abs/1810.04622)\n\n**Rethinking the Value of Network Pruning**\n\n[https://arxiv.org/abs/1810.05270](https://arxiv.org/abs/1810.05270)\n\n**Dynamic Channel Pruning: Feature Boosting and Suppression**\n\n[https://arxiv.org/abs/1810.05331](https://arxiv.org/abs/1810.05331)\n\n**Interpretable Convolutional Filter Pruning**\n\n[https://arxiv.org/abs/1810.07322](https://arxiv.org/abs/1810.07322)\n\n**Progressive Weight Pruning of Deep Neural Networks using ADMM**\n\n[https://arxiv.org/abs/1810.07378](https://arxiv.org/abs/1810.07378)\n\n**Pruning Deep Neural Networks using Partial Least Squares**\n\n- arxiv: [https://arxiv.org/abs/1810.07610](https://arxiv.org/abs/1810.07610)\n- github: [https://github.com/arturjordao/PruningNeuralNetworks](https://github.com/arturjordao/PruningNeuralNetworks)\n\n**Hybrid Pruning: Thinner Sparse Networks for Fast Inference on Edge Devices**\n\n[https://arxiv.org/abs/1811.00482](https://arxiv.org/abs/1811.00482)\n\n**Discrimination-aware Channel Pruning for Deep Neural Networks**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.11809](https://arxiv.org/abs/1810.11809)\n\n**Stability Based Filter Pruning for Accelerating Deep CNNs**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1811.08321](https://arxiv.org/abs/1811.08321)\n\n**Structured Pruning for Efficient ConvNets via Incremental Regularization**\n\n- intro: NIPS 2018 workshop on \"Compact Deep Neural Network Representation with Industrial Applications\"\n- arxiv: [https://arxiv.org/abs/1811.08390](https://arxiv.org/abs/1811.08390)\n\n**Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1811.08589](https://arxiv.org/abs/1811.08589)\n\n**A Layer Decomposition-Recomposition Framework for Neuron Pruning towards Accurate Lightweight Networks**\n\n- intro: AAAI 2019 as oral\n- intro: Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1812.06611](https://arxiv.org/abs/1812.06611)\n\n**Quantized Guided Pruning for Efficient Hardware Implementations of Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1812.11337](https://arxiv.org/abs/1812.11337)\n\n**Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning**\n\n[https://arxiv.org/abs/1901.07827](https://arxiv.org/abs/1901.07827)\n\n**Partition Pruning: Parallelization-Aware Pruning for Deep Neural Networks**\n\n[https://arxiv.org/abs/1901.11391](https://arxiv.org/abs/1901.11391)\n\n**Pruning from Scratch**\n\n- intro: Tsinghua University & Ant Financial & Huawei Noah’s Ark Lab\n- arxiv: [https://arxiv.org/abs/1909.12579](https://arxiv.org/abs/1909.12579)\n\n**Global Sparse Momentum SGD for Pruning Very Deep Neural Networks**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1909.12778](https://arxiv.org/abs/1909.12778)\n\n**FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization**\n\n- openreview: [https://openreview.et/forum?id=rJeUPlrYvr](https://openreview.net/forum?id=rJeUPlrYvr)\n- paper: [https://openreview.net/pdf?id=rJeUPlrYvr](https://openreview.net/pdf?id=rJeUPlrYvr)\n- github: [https://github.com/anonymous47823493/FNNP](https://github.com/anonymous47823493/FNNP)\n\n**Pruning Filter in Filter**\n\n- intro: NeurIPS 2020\n- intro: Harbin Institute of Technology & Tencent Youtu Lab\n- arxiv: [https://arxiv.org/abs/2009.14410](https://arxiv.org/abs/2009.14410)\n\n# Low-Precision Networks\n\n**Accelerating Deep Convolutional Networks using low-precision and sparsity**\n\n- intro: Intel Labs\n- arxiv: [https://arxiv.org/abs/1610.00324](https://arxiv.org/abs/1610.00324)\n\n**Deep Learning with Low Precision by Half-wave Gaussian Quantization**\n\n- intro: HWGQ-Net\n- arxiv: [https://arxiv.org/abs/1702.00953](https://arxiv.org/abs/1702.00953)\n\n**Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights**\n\n- intro: ICLR 2017\n- arxiv: [https://arxiv.org/abs/1702.03044](https://arxiv.org/abs/1702.03044)\n- openreview: [https://openreview.net/forum?id=HyQJ-mclg&noteId=HyQJ-mclg](https://openreview.net/forum?id=HyQJ-mclg&noteId=HyQJ-mclg)\n\n**ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1706.02393](https://arxiv.org/abs/1706.02393)\n- github: [https://github.com/gudovskiy/ShiftCNN](https://github.com/gudovskiy/ShiftCNN)\n\n**Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM**\n\n- intro: Alibaba Group\n- keywords: alternating direction method of multipliers (ADMM)\n- arxiv: [https://arxiv.org/abs/1707.09870](https://arxiv.org/abs/1707.09870)\n\n**Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization**\n\n- intro: BMVC 2017 Oral\n- arxiv: [https://arxiv.org/abs/1708.01001](https://arxiv.org/abs/1708.01001)\n\n**Compressing Low Precision Deep Neural Networks Using Sparsity-Induced Regularization in Ternary Networks**\n\n- intro: ICONIP 2017\n- arxiv: [https://arxiv.org/abs/1709.06262](https://arxiv.org/abs/1709.06262)\n\n**Learning Low Precision Deep Neural Networks through Regularization**\n\n[https://arxiv.org/abs/1809.00095](https://arxiv.org/abs/1809.00095)\n\n**Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference**\n\n[https://arxiv.org/abs/1809.04191](https://arxiv.org/abs/1809.04191)\n\n**SQuantizer: Simultaneous Learning for Both Sparse and Low-precision Neural Networks**\n\n- intro: Movidius, AIPG, Intel\n- arxiv: [https://arxiv.org/abs/1812.08301](https://arxiv.org/abs/1812.08301)\n\n# Quantized Neural Networks\n\n**Quantized Convolutional Neural Networks for Mobile Devices**\n\n- intro: Q-CNN\n- intro: \"Extensive experiments on the ILSVRC-12 benchmark demonstrate \n4 ∼ 6× speed-up and 15 ∼ 20× compression with merely one percentage loss of classification accuracy\"\n- arxiv: [http://arxiv.org/abs/1512.06473](http://arxiv.org/abs/1512.06473)\n- github: [https://github.com/jiaxiang-wu/quantized-cnn](https://github.com/jiaxiang-wu/quantized-cnn)\n\n**Training Quantized Nets: A Deeper Understanding**\n\n- intro: University of Maryland & Cornell University\n- arxiv: [https://arxiv.org/abs/1706.02379](https://arxiv.org/abs/1706.02379)\n\n**Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks**\n\n- intro: the top-5 error rate of 4-bit quantized GoogLeNet model is 12.7%\n- arxiv: [https://arxiv.org/abs/1706.07145](https://arxiv.org/abs/1706.07145)\n\n**Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference**\n\n- intro: CVPR 2018. Google\n- arxiv: [https://arxiv.org/abs/1712.05877](https://arxiv.org/abs/1712.05877)\n\n**Deep Neural Network Compression with Single and Multiple Level Quantization**\n\n- intro: AAAI 2018. Shanghai Jiao Tong University & University of Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1803.03289](https://arxiv.org/abs/1803.03289)\n\n**Quantizing deep convolutional networks for efficient inference: A whitepaper**\n\n- intro: Google\n- arxiv: [https://arxiv.org/abs/1806.08342](https://arxiv.org/abs/1806.08342)\n\n**CascadeCNN: Pushing the Performance Limits of Quantisation in Convolutional Neural Networks**\n\n- intro: 28th International Conference on Field Programmable Logic & Applications (FPL), 2018\n- arxiv: [https://arxiv.org/abs/1807.05053](https://arxiv.org/abs/1807.05053)\n\n**Bridging the Accuracy Gap for 2-bit Quantized Neural Networks (QNN)**\n\n- intro: IBM Research AI\n- arxiv: [https://arxiv.org/abs/1807.06964](https://arxiv.org/abs/1807.06964)\n\n**Joint Training of Low-Precision Neural Network with Quantization Interval Parameters**\n\n[https://arxiv.org/abs/1808.05779](https://arxiv.org/abs/1808.05779)\n\n**Differentiable Fine-grained Quantization for Deep Neural Network Compression**\n\n[https://arxiv.org/abs/1810.10351](https://arxiv.org/abs/1810.10351)\n\n**HAQ: Hardware-Aware Automated Quantization**\n\n[https://arxiv.org/abs/1811.08886](https://arxiv.org/abs/1811.08886)\n\n**DNQ: Dynamic Network Quantization**\n\n- intro: Shanghai Jiao Tong University & Qualcomm AI Research\n- arxiv: [https://arxiv.org/abs/1812.02375](https://arxiv.org/abs/1812.02375)\n\n**Trained Rank Pruning for Efficient Deep Neural Networks**\n\n- intro: Shanghai Jiao Tong University & Qualcomm AI Research & Duke University\n- arxiv: [https://arxiv.org/abs/1812.02402](https://arxiv.org/abs/1812.02402)\n\n**Training Quantized Network with Auxiliary Gradient Module**\n\n- intro: The University of Adelaide & Australian Centre for Robotic Vision & South China University of Technology\n- arxiv: [https://arxiv.org/abs/1903.11236](https://arxiv.org/abs/1903.11236)\n\n**FLightNNs: Lightweight Quantized Deep Neural Networks for Fast and Accurate Inference**\n\n- intro: Carnegie Mellon University\n- intro: [https://arxiv.org/abs/1904.02835](https://arxiv.org/abs/1904.02835)\n\n**And the Bit Goes Down: Revisiting the Quantization of Neural Networks**\n\n- intro: Facebook AI Research & Univ Rennes\n- arxiv: [https://arxiv.org/abs/1907.05686](https://arxiv.org/abs/1907.05686)\n\n**Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.05033](https://arxiv.org/abs/1908.05033)\n\n**Bit Efficient Quantization for Deep Neural Networks**\n\n- intro: NeurIPS workshop 2019\n- arxiv: [https://arxiv.org/abs/1910.04877](https://arxiv.org/abs/1910.04877)\n\n**Quantization Networks**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1911.09464](https://arxiv.org/abs/1911.09464)\n\n**Adaptive Loss-aware Quantization for Multi-bit Networks**\n\n[https://arxiv.org/abs/1912.08883](https://arxiv.org/abs/1912.08883)\n\n**Distribution Adaptive INT8 Quantization for Training CNNs**\n\n- intro: AAAI 2021\n- intro: Machine Intelligence Technology Lab, Alibaba Group\n- arxiv: [https://arxiv.org/abs/2102.04782](https://arxiv.org/abs/2102.04782)\n\n**Distance-aware Quantization**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.06983](https://arxiv.org/abs/2108.06983)\n\n# Binary Convolutional Neural Networks / Binarized Neural Networks\n\n**BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**\n\n**Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**\n\n[https://arxiv.org/abs/1602.02830](https://arxiv.org/abs/1602.02830)\n\n**Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations**\n\n[https://arxiv.org/abs/1609.07061](https://arxiv.org/abs/1609.07061)\n\n**XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1603.05279](http://arxiv.org/abs/1603.05279)\n- github(Torch): [https://github.com/mrastegari/XNOR-Net](https://github.com/mrastegari/XNOR-Net)\n\n**XNOR-Net++: Improved Binary Neural Networks**\n\n- intro: BMVC 2019\n- arxiv: [https://arxiv.org/abs/1909.13863](https://arxiv.org/abs/1909.13863)\n\n**DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients**\n\n[https://arxiv.org/abs/1606.06160](https://arxiv.org/abs/1606.06160)\n\n**A 7.663-TOPS 8.2-W Energy-efficient FPGA Accelerator for Binary Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.06392](https://arxiv.org/abs/1702.06392)\n\n**Espresso: Efficient Forward Propagation for BCNNs**\n\n- arxiv: [https://arxiv.org/abs/1705.07175](https://arxiv.org/abs/1705.07175)\n- github: [https://github.com/fpeder/espresso](https://github.com/fpeder/espresso)\n\n**BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet**\n\n- keywords: Binary Neural Networks (BNNs)\n- arxiv: [https://arxiv.org/abs/1705.09864](https://arxiv.org/abs/1705.09864)\n- github: [https://github.com/hpi-xnor](https://github.com/hpi-xnor)\n\n**ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1706.02393](https://arxiv.org/abs/1706.02393)\n\n**Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration**\n\n- intro: Embedded Vision Workshop (CVPRW). UC San Diego & UC Los Angeles & Cornell University\n- arxiv: [https://arxiv.org/abs/1707.04693](https://arxiv.org/abs/1707.04693)\n\n**Embedded Binarized Neural Networks**\n\n[https://arxiv.org/abs/1709.02260](https://arxiv.org/abs/1709.02260)\n\n**Compact Hash Code Learning with Binary Deep Neural Network**\n\n- intro: Singapore University of Technology and Design\n- arxiv: [https://arxiv.org/abs/1712.02956](https://arxiv.org/abs/1712.02956)\n\n**Build a Compact Binary Neural Network through Bit-level Sensitivity and Data Pruning**\n\n[https://arxiv.org/abs/1802.00904](https://arxiv.org/abs/1802.00904)\n\n**From Hashing to CNNs: Training BinaryWeight Networks via Hashing**\n\n[https://arxiv.org/abs/1802.02733](https://arxiv.org/abs/1802.02733)\n\n**Energy Efficient Hadamard Neural Networks**\n\n- keywords: Binary Weight and Hadamard-transformed Image Network (BWHIN), Binary Weight Network (BWN), Hadamard-transformed Image Network (HIN)\n- arxiv: [https://arxiv.org/abs/1805.05421](https://arxiv.org/abs/1805.05421)\n\n**Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?**\n\n[https://arxiv.org/abs/1806.07550](https://arxiv.org/abs/1806.07550)\n\n**Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.00278](https://arxiv.org/abs/1808.00278)\n\n**Training Compact Neural Networks with Binary Weights and Low Precision Activations**\n\n[https://arxiv.org/abs/1808.02631](https://arxiv.org/abs/1808.02631)\n\n**Training wide residual networks for deployment using a single bit for each weight**\n\n- intro: ICLR 2018\n- arxiv: [https://arxiv.org/abs/1802.08530](https://arxiv.org/abs/1802.08530)\n- github(official, PyTorch): [https://github.com/szagoruyko/binary-wide-resnet](https://github.com/szagoruyko/binary-wide-resnet)\n\n**Composite Binary Decomposition Networks**\n\n[https://arxiv.org/abs/1811.06668](https://arxiv.org/abs/1811.06668)\n\n**Training Competitive Binary Neural Networks from Scratch**\n\n- intro: University of Potsdam\n- intro: BMXNet v2: An Open-Source Binary Neural Network Implementation Based on MXNet\n- arxiv: [https://arxiv.org/abs/1812.01965](https://arxiv.org/abs/1812.01965)\n- github: [https://github.com/hpi-xnor/BMXNet-v2](https://github.com/hpi-xnor/BMXNet-v2)\n\n**Regularizing Activation Distribution for Training Binarized Deep Networks**\n\n- intro: Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/1904.02823](https://arxiv.org/abs/1904.02823)\n\n**GBCNs: Genetic Binary Convolutional Networks for Enhancing the Performance of 1-bit DCNNs**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.11634](https://arxiv.org/abs/1911.11634)\n\n**Training Binary Neural Networks with Real-to-Binary Convolutions**\n\n- intro: ICLR 2020\n- intro: Samsung AI Research Center, Cambridge, UK & The University of Nottingham\n- arxiv: [https://arxiv.org/abs/2003.11535](https://arxiv.org/abs/2003.11535)\n- github: [https://github.com/brais-martinez/real2binary](https://github.com/brais-martinez/real2binary)\n\n# Accelerating / Fast Algorithms\n\n**Fast Algorithms for Convolutional Neural Networks**\n\n- intro: \"2.6x as fast as Caffe when comparing CPU implementations\"\n- keywords: Winograd's minimal filtering algorithms\n- arxiv: [http://arxiv.org/abs/1509.09308](http://arxiv.org/abs/1509.09308)\n- github: [https://github.com/andravin/wincnn](https://github.com/andravin/wincnn)\n- slides: [http://homes.cs.washington.edu/~cdel/presentations/Fast_Algorithms_for_Convolutional_Neural_Networks_Slides_reading_group_uw_delmundo_slides.pdf](http://homes.cs.washington.edu/~cdel/presentations/Fast_Algorithms_for_Convolutional_Neural_Networks_Slides_reading_group_uw_delmundo_slides.pdf)\n- discussion: [https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895](https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/?](https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/?)\n\n**Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units**\n\n- intro: Hong Kong Baptist University\n- arxiv: [https://arxiv.org/abs/1704.07724](https://arxiv.org/abs/1704.07724)\n\n**NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps**\n\n[https://arxiv.org/abs/1706.01406](https://arxiv.org/abs/1706.01406)\n\n**Channel Pruning for Accelerating Very Deep Neural Networks**\n\n- intro: ICCV 2017. Megvii Inc\n- arxiv: [https://arxiv.org/abs/1707.06168](https://arxiv.org/abs/1707.06168)\n- github: [https://github.com/yihui-he/channel-pruning](https://github.com/yihui-he/channel-pruning)\n\n**DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices**\n\n[https://arxiv.org/abs/1708.04728](https://arxiv.org/abs/1708.04728)\n\n**Learning Efficient Convolutional Networks through Network Slimming**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.06519](https://arxiv.org/abs/1708.06519)\n\n**SparCE: Sparsity aware General Purpose Core Extensions to Accelerate Deep Neural Networks**\n\n[https://arxiv.org/abs/1711.06315](https://arxiv.org/abs/1711.06315)\n\n**Accelerating Convolutional Neural Networks for Continuous Mobile Vision via Cache Reuse**\n\n- keywords: CNNCache\n- arxiv: [https://arxiv.org/abs/1712.01670](https://arxiv.org/abs/1712.01670)\n\n**Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1712.07493](https://arxiv.org/abs/1712.07493)\n\n**SBNet: Sparse Blocks Network for Fast Inference**\n\n- intro: Uber\n- project page: [https://eng.uber.com/sbnet/](https://eng.uber.com/sbnet/)\n- arxiv: [https://arxiv.org/abs/1801.02108](https://arxiv.org/abs/1801.02108)\n- github: [https://github.com/uber/sbnet](https://github.com/uber/sbnet)\n\n**Accelerating deep neural networks with tensor decompositions**\n\n- blog: [https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning](https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning)\n- github: [https://github.com/jacobgil/pytorch-tensor-decompositions](https://github.com/jacobgil/pytorch-tensor-decompositions)\n\n**A Survey on Acceleration of Deep Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1802.00939](https://arxiv.org/abs/1802.00939)\n\n**Recurrent Residual Module for Fast Inference in Videos**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1802.09723](https://arxiv.org/abs/1802.09723)\n\n**Co-Design of Deep Neural Nets and Neural Net Accelerators for Embedded Vision Applications**\n\n- intro: UC Berkeley & Samsung Research\n- arxiv: [https://arxiv.org/abs/1804.10642](https://arxiv.org/abs/1804.10642)\n\n**Towards Efficient Convolutional Neural Network for Domain-Specific Applications on FPGA**\n\n[https://arxiv.org/abs/1809.03318](https://arxiv.org/abs/1809.03318)\n\n**Accelerating Deep Neural Networks with Spatial Bottleneck Modules**\n\n[https://arxiv.org/abs/1809.02601](https://arxiv.org/abs/1809.02601)\n\n**FPGA Implementation of Convolutional Neural Networks with Fixed-Point Calculations**\n\n[https://arxiv.org/abs/1808.09945](https://arxiv.org/abs/1808.09945)\n\n**Extended Bit-Plane Compression for Convolutional Neural Network Accelerators**\n\n[https://arxiv.org/abs/1810.03979](https://arxiv.org/abs/1810.03979)\n\n**DAC: Data-free Automatic Acceleration of Convolutional Networks**\n\n- intro: WACV 2019\n- intro: Qualcomm AI Research & Lehigh University\n- arxiv: [https://arxiv.org/abs/1812.08374](https://arxiv.org/abs/1812.08374)\n\n**Learning Instance-wise Sparsity for Accelerating Deep Models**\n\n- intro: IJCAI 2019\n- arxiv: [https://arxiv.org/abs/1907.11840](https://arxiv.org/abs/1907.11840)\n\n# Code Optimization\n\n**Production Deep Learning with NVIDIA GPU Inference Engine**\n\n![](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/06/GIE_GoogLeNet_top10kernels-1.png)\n\n- intro: convolution, bias, and ReLU layers are fused to form a single layer: CBR\n- blog: [https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/](https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/)\n\n**speed improvement by merging batch normalization and scale #5**\n\n- github issue: [https://github.com/sanghoon/pva-faster-rcnn/issues/5](https://github.com/sanghoon/pva-faster-rcnn/issues/5)\n\n**Add a tool to merge 'Conv-BN-Scale' into a single 'Conv' layer.**\n\n[https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c/](https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c/)\n\n**Low-memory GEMM-based convolution algorithms for deep neural networks**\n\n[https://arxiv.org/abs/1709.03395](https://arxiv.org/abs/1709.03395)\n\n# Projects\n\n**Accelerate Convolutional Neural Networks**\n\n- intro: \"This tool aims to accelerate the test-time computation and decrease number of parameters of deep CNNs.\"\n- github: [https://github.com/dmlc/mxnet/tree/master/tools/accnn](https://github.com/dmlc/mxnet/tree/master/tools/accnn)\n\n## OptNet\n\n**OptNet - reducing memory usage in torch neural networks**\n\n- github: [https://github.com/fmassa/optimize-net](https://github.com/fmassa/optimize-net)\n\n**NNPACK: Acceleration package for neural networks on multi-core CPUs**\n\n![](https://camo.githubusercontent.com/376828536285f7a1a4f054aaae998e805023f489/68747470733a2f2f6d6172617479737a637a612e6769746875622e696f2f4e4e5041434b2f4e4e5041434b2e706e67)\n\n- github: [https://github.com/Maratyszcza/NNPACK](https://github.com/Maratyszcza/NNPACK)\n- comments(Yann LeCun): [https://www.facebook.com/yann.lecun/posts/10153459577707143](https://www.facebook.com/yann.lecun/posts/10153459577707143)\n\n**Deep Compression on AlexNet**\n\n- github: [https://github.com/songhan/Deep-Compression-AlexNet](https://github.com/songhan/Deep-Compression-AlexNet)\n\n**Tiny Darknet**\n\n- github: [http://pjreddie.com/darknet/tiny-darknet/](http://pjreddie.com/darknet/tiny-darknet/)\n\n**CACU: Calculate deep convolution neurAl network on Cell Unit**\n\n- github: [https://github.com/luhaofang/CACU](https://github.com/luhaofang/CACU)\n\n**keras_compressor: Model Compression CLI Tool for Keras**\n\n- blog: [https://nico-opendata.jp/ja/casestudy/model_compression/index.html](https://nico-opendata.jp/ja/casestudy/model_compression/index.html)\n- github: [https://github.com/nico-opendata/keras_compressor](https://github.com/nico-opendata/keras_compressor)\n\n# Blogs\n\n**Neural Networks Are Impressively Good At Compression**\n\n[https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/)\n\n**“Mobile friendly” deep convolutional neural networks**\n\n- part 1: [https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-1-331120ad40f9#.uy64ladvz](https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-1-331120ad40f9#.uy64ladvz)\n- part 2: [https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9#.u58fkuak3](https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9#.u58fkuak3)\n\n**Lab41 Reading Group: Deep Compression**\n\n- blog: [https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.hbqzn8wfu](https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.hbqzn8wfu)\n\n**Accelerating Machine Learning**\n\n![](http://www.linleygroup.com/mpr/h/2016/11561/U26_F4v2.png)\n\n- blog: [http://www.linleygroup.com/mpr/article.php?id=11561](http://www.linleygroup.com/mpr/article.php?id=11561)\n\n**Compressing and regularizing deep neural networks**\n\n[https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks](https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks)\n\n**How fast is my model?**\n\n[http://machinethink.net/blog/how-fast-is-my-model/](http://machinethink.net/blog/how-fast-is-my-model/)\n\n# Talks / Videos\n\n**Deep compression and EIE: Deep learning model compression, design space exploration and hardware acceleration**\n\n- youtube: [https://www.youtube.com/watch?v=baZOmGSSUAg](https://www.youtube.com/watch?v=baZOmGSSUAg)\n\n**Deep Compression, DSD Training and EIE: Deep Neural Network Model Compression, Regularization and Hardware Acceleration**\n\n[http://research.microsoft.com/apps/video/default.aspx?id=266664](http://research.microsoft.com/apps/video/default.aspx?id=266664)\n\n**Tailoring Convolutional Neural Networks for Low-Cost, Low-Power Implementation**\n\n- intro: tutorial at the May 2015 Embedded Vision Summit\n- youtube: [https://www.youtube.com/watch?v=xACJBACStaU](https://www.youtube.com/watch?v=xACJBACStaU)\n\n# Resources\n\n**awesome-model-compression-and-acceleration**\n\n[https://github.com/sun254/awesome-model-compression-and-acceleration](https://github.com/sun254/awesome-model-compression-and-acceleration)\n\n**Embedded-Neural-Network**\n\n- intro: collection of works aiming at reducing model sizes or the ASIC/FPGA accelerator for machine learning\n- github: [https://github.com/ZhishengWang/Embedded-Neural-Network](https://github.com/ZhishengWang/Embedded-Neural-Network)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","title":"Deep learning Courses"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Deep learning Courses\r\ndate: 2015-10-09\r\n---\r\n\r\n# Deep Learning\r\n\r\n**EECS 598: Unsupervised Feature Learning**\r\n\r\n- instructor: Honglak Lee\r\n- homepage: [http://web.eecs.umich.edu/~honglak/teaching/eecs598/schedule.html](http://web.eecs.umich.edu/~honglak/teaching/eecs598/schedule.html)\r\n\r\n**NVIDIA’s Deep Learning Courses**\r\n\r\n[https://developer.nvidia.com/deep-learning-courses](https://developer.nvidia.com/deep-learning-courses)\r\n\r\n**ECE 6504 Deep Learning for Perception**\r\n\r\n- instructor: Dhruv Batra (Virginia Tech)\r\n- homepage: [https://computing.ece.vt.edu/~f15ece6504/](https://computing.ece.vt.edu/~f15ece6504/)\r\n\r\n**University of Oxford: Machine Learning: 2014-2015**\r\n\r\n- homepage: [https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\r\n- lectures: [http://pan.baidu.com/s/1bndbxJh#path=%252FDeep%2520Learning%2520Lectures](http://pan.baidu.com/s/1bndbxJh#path=%252FDeep%2520Learning%2520Lectures)\r\n- github: [https://github.com/oxford-cs-ml-2015/](https://github.com/oxford-cs-ml-2015/)\r\n\r\n**University of Birmingham 2014: Introduction to Neural Computation (Level 4/M); Neural Computation (Level 3/H)(by John A. Bullinaria)**\r\n\r\n[http://www.cs.bham.ac.uk/~jxb/inc.html](http://www.cs.bham.ac.uk/~jxb/inc.html)\r\n\r\n**CMU: Deep Learning**\r\n\r\n- instructor: Bhiksha Raj\r\n- homepage: [http://deeplearning.cs.cmu.edu/](http://deeplearning.cs.cmu.edu/)\r\n\r\n**stat212b: Topics Course on Deep Learning for Spring 2016**\r\n\r\n- homepage: [http://joanbruna.github.io/stat212b/](http://joanbruna.github.io/stat212b/)\r\n- github: [https://github.com/joanbruna/stat212b](https://github.com/joanbruna/stat212b)\r\n- pan: [http://pan.baidu.com/s/1sk7TKtf#path=%252Fstat212b%2520-%2520Topics%2520Course%2520on%2520Deep%2520Learning%2520for%2520Spring%25202016](http://pan.baidu.com/s/1sk7TKtf#path=%252Fstat212b%2520-%2520Topics%2520Course%2520on%2520Deep%2520Learning%2520for%2520Spring%25202016)\r\n\r\n**Good materials on deep learning**\r\n\r\n[http://eclass.cc/courselists/117_deep_learning](http://eclass.cc/courselists/117_deep_learning)\r\n\r\n**Deep Learning: Course by Yann LeCun at Collège de France 2016(Slides in English)**\r\n\r\n- homepage: [https://www.facebook.com/yann.lecun/posts/10153505343037143](https://www.facebook.com/yann.lecun/posts/10153505343037143)\r\n- downloads: [https://drive.google.com/open?id=0BxKBnD5y2M8NclFWSXNxa0JlZTg](https://drive.google.com/open?id=0BxKBnD5y2M8NclFWSXNxa0JlZTg)\r\n\r\n**CSC321 Winter 2015: Introduction to Neural Networks**\r\n\r\n- homepage: [http://www.cs.toronto.edu/~rgrosse/csc321/calendar.html](http://www.cs.toronto.edu/~rgrosse/csc321/calendar.html)\r\n\r\n**ELEG 5040: Advanced Topics in Signal Processing (Introduction to Deep Learning)**\r\n\r\n- instructors: Xiaogang Wang. The Chinese University of Hong Kong - Spring 2015\r\n- intro: Homework, Homework Solutions, Lecture Notes, General Resources, Tutorial Notes, CUDA/GPU programming tutorial\r\n- homepage: [https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/resources](https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/resources)\r\n\r\n**Self-Study Courses for Deep Learning (NVIDIA Deep Learning Institute)**\r\n\r\n- homepage: [https://developer.nvidia.com/deep-learning-courses](https://developer.nvidia.com/deep-learning-courses)\r\n\r\n**Introduction to Deep Learning**\r\n\r\n- homepage: [https://beta.bigdatauniversity.com/courses/introduction-deep-learning/](https://beta.bigdatauniversity.com/courses/introduction-deep-learning/)\r\n\r\n**Deep Learning Courses**\r\n\r\n- blog: [http://machinelearningmastery.com/deep-learning-courses/](http://machinelearningmastery.com/deep-learning-courses/)\r\n\r\n**Creative Applications of Deep Learning w/ Tensorflow**\r\n\r\n![](https://raw.githubusercontent.com/pkmital/CADL/master/imgs/cadl-coursecard.png)\r\n\r\n- homepage: [https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-i/info](https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow-i/info)\r\n- github(ourse materials/Homework materials): [https://github.com/pkmital/CADL](https://github.com/pkmital/CADL)\r\n\r\n**Deep Learning School: September 24-25, 2016 Stanford, CA**\r\n\r\n- homepage: [http://www.bayareadlschool.org/](http://www.bayareadlschool.org/)\r\n- day 1: [https://www.youtube.com/watch?v=9dXiAecyJrY](https://www.youtube.com/watch?v=9dXiAecyJrY)\r\n- day 2: [https://www.youtube.com/watch?v=eyovmAtoUx0](https://www.youtube.com/watch?v=eyovmAtoUx0)\r\n- github: [https://github.com/lamblin/bayareadlschool](https://github.com/lamblin/bayareadlschool)\r\n- reddit: [https://amp.reddit.com/r/MachineLearning/comments/54shmi/great_new_introductory_talks_on_various_subfields/](https://amp.reddit.com/r/MachineLearning/comments/54shmi/great_new_introductory_talks_on_various_subfields/)\r\n- mirror: [https://pan.baidu.com/s/1gfBe2fL](https://pan.baidu.com/s/1gfBe2fL)\r\n\r\n**CSC 2541 Fall 2016: Differentiable Inference and Generative Models**\r\n\r\n![](http://www.cs.toronto.edu/~duvenaud/courses/csc2541/gan-crop.jpg)\r\n\r\n- homepage: [http://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html](http://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html)\r\n\r\n**CS 294-131: Special Topics in Deep Learning (Fall, 2016)**\r\n\r\n[https://berkeley-deep-learning.github.io/cs294-dl-f16/](https://berkeley-deep-learning.github.io/cs294-dl-f16/)\r\n\r\n**Fork of Lempitsky DL for HSE master students.**\r\n\r\n- github: [https://github.com/yandexdataschool/HSE_deeplearning](https://github.com/yandexdataschool/HSE_deeplearning)\r\n\r\n**ELEG 5040: Advanced Topics in Signal Processing (Introduction to Deep Learning)**\r\n\r\n- resources: [https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/resources](https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/resources)\r\n\r\n**CS 20SI: Tensorflow for Deep Learning Research**\r\n\r\n- homepage: [http://web.stanford.edu/class/cs20si/](http://web.stanford.edu/class/cs20si/)\r\n- github: [https://github.com/chiphuyen/stanford-tensorflow-tutorials](https://github.com/chiphuyen/stanford-tensorflow-tutorials)\r\n\r\n**Deep Learning with TensorFlow**\r\n\r\n[https://bigdatauniversity.com/courses/deep-learning-tensorflow/](https://bigdatauniversity.com/courses/deep-learning-tensorflow/)\r\n\r\n**Deep Learning course**\r\n\r\n- github: [https://github.com/ddtm/dl-course](https://github.com/ddtm/dl-course)\r\n\r\n**CSE 599G1: Deep Learning System**\r\n\r\n- homepage: [http://dlsys.cs.washington.edu/](http://dlsys.cs.washington.edu/)\r\n- assignments: [http://dlsys.cs.washington.edu/assignments](http://dlsys.cs.washington.edu/assignments)\r\n\r\n**CSC 321 Winter 2017: Intro to Neural Networks and Machine Learning**\r\n\r\n[http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/)\r\n\r\n**Theories of Deep Learning (STATS 385)**\r\n\r\n- homepage: [https://stats385.github.io/](https://stats385.github.io/)\r\n- video: [https://www.researchgate.net/project/Theories-of-Deep-Learning](https://www.researchgate.net/project/Theories-of-Deep-Learning)\r\n- mirror: [https://www.bilibili.com/video/av16136625/](https://www.bilibili.com/video/av16136625/)\r\n\r\n**CS230: Deep Learning Spring 2018**\r\n\r\n[https://web.stanford.edu/class/cs230/](https://web.stanford.edu/class/cs230/)\r\n\r\n## With Video Lectures\r\n\r\n**Deep Learning: Taking machine learning to the next level (Udacity)**\r\n\r\n- instructor: Vincent Vanhoucke (Google), Arpan Chakraborty\r\n- homepage: [https://www.udacity.com/course/deep-learning--ud730](https://www.udacity.com/course/deep-learning--ud730)\r\n- homepage: [https://cn.udacity.com/course/deep-learning--ud730/](https://cn.udacity.com/course/deep-learning--ud730/)\r\n- homepage: [https://classroom.udacity.com/courses/ud730/lessons/6370362152/concepts/63798118150923](https://classroom.udacity.com/courses/ud730/lessons/6370362152/concepts/63798118150923)\r\n- assignments: [https://github.com/tdhopper/udacity-deep-learning](https://github.com/tdhopper/udacity-deep-learning)\r\n- ipn: [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb)\r\n- ipn: [http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb](http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb)\r\n- assignments: [https://github.com/Arn-O/udacity-deep-learning](https://github.com/Arn-O/udacity-deep-learning)\r\n\r\n**Neural networks class - Université de Sherbrooke**\r\n\r\n- instructor: Hugo Larochelle\r\n- youtube: [https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)\r\n- video: [http://pan.baidu.com/s/1bnwEe8R](http://pan.baidu.com/s/1bnwEe8R)\r\n- course content: [http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)\r\n- google group: [https://groups.google.com/forum/#!forum/neural-networks-online-course](https://groups.google.com/forum/#!forum/neural-networks-online-course)\r\n\r\n**Deep Learning: Theoretical Motivations**\r\n\r\n- author: Yoshua Bengio\r\n- published: Sept. 13, 2015. (Deep Learning Summer School, Montreal 2015)\r\n- video: [http://videolectures.net/deeplearning2015_bengio_theoretical_motivations/](http://videolectures.net/deeplearning2015_bengio_theoretical_motivations/)\r\n- blog: [http://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html](http://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html)\r\n\r\n**University of Waterloo: STAT 946 - Deep Learning**\r\n\r\n- homepage: [https://uwaterloo.ca/data-science/deep-learning](https://uwaterloo.ca/data-science/deep-learning)\r\n- video+slides: [http://pan.baidu.com/s/1sjTRgjN](http://pan.baidu.com/s/1sjTRgjN)\r\n\r\n**Deep Learning (2016) - BME 595A, Eugenio Culurciello, Purdue University**\r\n\r\n- course shedule: [http://t.cn/RVYQa69?u=1402400261&m=4034720314226808&cu=2261580215&ru=1402400261&rm=4034708389597157](http://t.cn/RVYQa69?u=1402400261&m=4034720314226808&cu=2261580215&ru=1402400261&rm=4034708389597157)\r\n- mirror: [https://pan.baidu.com/s/1hsBJOpQ](https://pan.baidu.com/s/1hsBJOpQ)\r\n- video: [https://www.youtube.com/playlist?list=PLNgy4gid0G9cbw5OjwG2jxvFqYDqkGnpJ](https://www.youtube.com/playlist?list=PLNgy4gid0G9cbw5OjwG2jxvFqYDqkGnpJ)\r\n- mirror: [https://pan.baidu.com/s/1bpKb5Cj](https://pan.baidu.com/s/1bpKb5Cj)\r\n\r\n**UVA DEEP LEARNING COURSE**\r\n\r\n- intro: MSc in Artificial Intelligence for the University of Amsterdam.\r\n- homepage: [http://uvadlc.github.io/](http://uvadlc.github.io/)\r\n- assignments: [https://github.com/uvadlc/uvadlc_practicals_2016](https://github.com/uvadlc/uvadlc_practicals_2016)\r\n\r\n**Practical Deep Learning For Coders, Part 1**\r\n\r\n- intro: 10 hours a week for 7 weeks\r\n- homepage: [http://course.fast.ai/](http://course.fast.ai/)\r\n- youtube: [https://www.youtube.com/playlist?list=PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM](https://www.youtube.com/playlist?list=PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM)\r\n- mirror: [https://pan.baidu.com/s/1eRLK742#list/path=%2F](https://pan.baidu.com/s/1eRLK742#list/path=%2F)\r\n- github: [https://github.com/fastai/courses](https://github.com/fastai/courses)\r\n- blog: [http://www.kdnuggets.com/2016/12/deep-learning-coders-mooc-jeremy-howard.html](http://www.kdnuggets.com/2016/12/deep-learning-coders-mooc-jeremy-howard.html)\r\n\r\n**T81-558:Applications of Deep Neural Networks**\r\n\r\n- intro: Washington University\r\n- course page: [https://sites.wustl.edu/jeffheaton/t81-558/](https://sites.wustl.edu/jeffheaton/t81-558/)\r\n- youtube: [https://www.youtube.com/playlist?list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN](https://www.youtube.com/playlist?list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN)\r\n- github: [https://github.com/jeffheaton/t81_558_deep_learning](https://github.com/jeffheaton/t81_558_deep_learning)\r\n\r\n**CS294-129 Designing, Visualizing and Understanding Deep Neural Networks**\r\n\r\n- homepage: [https://bcourses.berkeley.edu/courses/1453965/pages/cs294-129-designing-visualizing-and-understanding-deep-neural-networks](https://bcourses.berkeley.edu/courses/1453965/pages/cs294-129-designing-visualizing-and-understanding-deep-neural-networks)\r\n- lecture video: [https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm](https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm)\r\n\r\n**MIT 6.S191: Introduction to Deep Learning**\r\n\r\n- homepage: [http://introtodeeplearning.com/index.html](http://introtodeeplearning.com/index.html)\r\n- schedule(Slides+Videos): [http://introtodeeplearning.com/schedule.html](http://introtodeeplearning.com/schedule.html)\r\n- github: [https://github.com/yala/introdeeplearning](https://github.com/yala/introdeeplearning)\r\n- youtube: [https://www.youtube.com/playlist?list=PLkkuNyzb8LmxFutYuPA7B4oiMn6cjD6Rs](https://www.youtube.com/playlist?list=PLkkuNyzb8LmxFutYuPA7B4oiMn6cjD6Rs)\r\n- mirror: [https://pan.baidu.com/s/1qXXDCoG#list/path=%2F](https://pan.baidu.com/s/1qXXDCoG#list/path=%2F)\r\n\r\n**Edx: Deep Learning Explained**\r\n\r\n- intro: Microsoft\r\n- course page: [https://www.edx.org/course/deep-learning-explained-microsoft-dat236x](https://www.edx.org/course/deep-learning-explained-microsoft-dat236x)\r\n\r\n# Computer Vision\r\n\r\n**Stanford CS231n: Convolutional Neural Networks for Visual Recognition (Spring 2017)**\r\n\r\n- youtube: [https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\r\n- mirror: [http://www.bilibili.com/video/av13260183/](http://www.bilibili.com/video/av13260183/)\r\n\r\n**Stanford CS231n: Convolutional Neural Networks for Visual Recognition (Winter 2016)**\r\n\r\n- homepage: [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)\r\n- homepage: [http://vision.stanford.edu/teaching/cs231n/index.html](http://vision.stanford.edu/teaching/cs231n/index.html)\r\n- syllabus: [http://vision.stanford.edu/teaching/cs231n/syllabus.html](http://vision.stanford.edu/teaching/cs231n/syllabus.html)\r\n- course notes: [http://cs231n.github.io/](http://cs231n.github.io/)\r\n- youtube: [https://www.youtube.com/watch?v=NfnWJUyUJYU&feature=youtu.be](https://www.youtube.com/watch?v=NfnWJUyUJYU&feature=youtu.be)\r\n- mirror: [http://pan.baidu.com/s/1pKsTivp](http://pan.baidu.com/s/1pKsTivp)\r\n- mirror: [http://pan.baidu.com/s/1c2wR8dy](http://pan.baidu.com/s/1c2wR8dy)\r\n- assignment 1: [http://cs231n.github.io/assignments2016/assignment1/](http://cs231n.github.io/assignments2016/assignment1/)\r\n- assignment 2: [http://cs231n.github.io/assignments2016/assignment2/](http://cs231n.github.io/assignments2016/assignment2/)\r\n- assignment 3: [http://cs231n.github.io/assignments2016/assignment3/](http://cs231n.github.io/assignments2016/assignment3/)\r\n\r\n**ITP-NYU - Spring 2016**\r\n\r\n- Video lectures and course notes: [http://ml4a.github.io/classes/itp-S16/](http://ml4a.github.io/classes/itp-S16/)\r\n\r\n**Deep Learning for Computer Vision Barcelona: Summer seminar UPC TelecomBCN (July 4-8, 2016)**\r\n\r\n- intro: This course will cover the basic principles and applications of deep learning to computer vision problems, \r\nsuch as image classification, object detection or text captioning.\r\n- homepage(slides+videos): [http://imatge-upc.github.io/telecombcn-2016-dlcv/](http://imatge-upc.github.io/telecombcn-2016-dlcv/)\r\n- homepage: [https://imatge.upc.edu/web/teaching/deep-learning-computer-vision](https://imatge.upc.edu/web/teaching/deep-learning-computer-vision)\r\n- youtube: [https://www.youtube.com/user/imatgeupc/videos?shelf_id=0&sort=dd&view=0](https://www.youtube.com/user/imatgeupc/videos?shelf_id=0&sort=dd&view=0)\r\n\r\n**DLCV - Deep Learning for Computer Vision**\r\n\r\n- homepage: [https://imatge.upc.edu/web/teaching/deep-learning-computer-vision](https://imatge.upc.edu/web/teaching/deep-learning-computer-vision)\r\n\r\n**Advanced Computer Vision Cap6412**\r\n\r\n- homepage: [http://crcv.ucf.edu/courses/CAP6412/Spring2018/](http://crcv.ucf.edu/courses/CAP6412/Spring2018/)\r\n- video: [https://www.youtube.com/playlist?list=PLd3hlSJsX_ImoNaeX5vFrxogGXTSmS993](https://www.youtube.com/playlist?list=PLd3hlSJsX_ImoNaeX5vFrxogGXTSmS993)\r\n\r\n# Natural Language Processing\r\n\r\n**CS224n: Natural Language Processing with Deep Learning**\r\n\r\n- intro: This course is a merger of Stanford's previous cs224n course and cs224d\r\n- homepage: [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)\r\n\r\n**Course notes for CS224N Winter17**\r\n\r\n[https://github.com/stanfordnlp/cs224n-winter17-notes](https://github.com/stanfordnlp/cs224n-winter17-notes)\r\n\r\n**Stanford CS224d: Deep Learning for Natural Language Processing**\r\n\r\n- homepage: [http://cs224d.stanford.edu/](http://cs224d.stanford.edu/)\r\n- syllabus: [http://cs224d.stanford.edu/syllabus.html](http://cs224d.stanford.edu/syllabus.html)\r\n- lecture notes: [https://cs224d.stanford.edu/lecture_notes/](https://cs224d.stanford.edu/lecture_notes/)\r\n\r\n**Code for Stanford CS224D: deep learning for natural language understanding**\r\n\r\n- github: [https://github.com/bogatyy/cs224d](https://github.com/bogatyy/cs224d)\r\n\r\n**CMU CS 11-747, Fall 2017: Neural Networks for NLP**\r\n\r\n- intro: by Graham Neubig\r\n- course page: [http://phontron.com/class/nn4nlp2017/](http://phontron.com/class/nn4nlp2017/)\r\n- github: [https://github.com/neubig/nn4nlp2017-code](https://github.com/neubig/nn4nlp2017-code)\r\n- video: [https://www.bilibili.com/video/av14153689/](https://www.bilibili.com/video/av14153689/)\r\n\r\n**Deep Learning for NLP - Lecture October 2015**\r\n\r\n- github: [https://github.com/UKPLab/deeplearning4nlp-tutorial/tree/master/2015-10_Lecture](https://github.com/UKPLab/deeplearning4nlp-tutorial/tree/master/2015-10_Lecture)\r\n\r\n**Harvard University: CS287: Natural Language Processing**\r\n\r\n[http://cs287.fas.harvard.edu/](http://cs287.fas.harvard.edu/)\r\n\r\n**Deep Learning for Natural Language Processing:  2016-2017**\r\n\r\n- intro: Oxford Deep NLP 2017 course\r\n- homepage: [http://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/](http://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/)\r\n- github: [https://github.com/oxford-cs-deepnlp-2017/lectures](https://github.com/oxford-cs-deepnlp-2017/lectures)\r\n- youtube: [https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm](https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm)\r\n- mirror: [https://pan.baidu.com/s/1dFvGHUh#list/path=%2F](https://pan.baidu.com/s/1dFvGHUh#list/path=%2F)\r\n- mirror: [https://pan.baidu.com/s/1c2tcC96](https://pan.baidu.com/s/1c2tcC96)\r\n\r\n# GPU Programming\r\n\r\n**Course on CUDA Programming on NVIDIA GPUs, July 27--31, 2015**\r\n\r\n- homepage: [http://people.maths.ox.ac.uk/gilesm/cuda/](http://people.maths.ox.ac.uk/gilesm/cuda/)\r\n\r\n**An Introduction to GPU Programming using Theano**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=eVd2TqEkVp0](https://www.youtube.com/watch?v=eVd2TqEkVp0)\r\n- video: [http://pan.baidu.com/s/1c1i6LtI#path=%252F](http://pan.baidu.com/s/1c1i6LtI#path=%252F)\r\n\r\n**GPU Programming**\r\n\r\n- homepage: [http://courses.cms.caltech.edu/cs179/](http://courses.cms.caltech.edu/cs179/)\r\n\r\n# Parallel Programming\r\n\r\n**Intro to Parallel Programming Using CUDA to Harness the Power of GPUs (Udacity)**\r\n\r\n[https://www.udacity.com/course/intro-to-parallel-programming--cs344](https://www.udacity.com/course/intro-to-parallel-programming--cs344)\r\n\r\n**Fundamentals of Accelerated Computing with CUDA C/C++**\r\n\r\n- intro: Learn to use CUDA C/C++ tools and techniques to accelerate CPU-only applications to run on massively parallel GPUs.\r\n- homepage: [https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about)\r\n\r\n# Workshops\r\n\r\n**Deep Learning: Theory, Algorithms, and Applications**\r\n\r\n- homepage: [http://doc.ml.tu-berlin.de/dlworkshop2017/](http://doc.ml.tu-berlin.de/dlworkshop2017/)\r\n- video: [https://www.youtube.com/playlist?list=PLJOzdkh8T5kqCNV_v1w2tapvtJDZYiohW](https://www.youtube.com/playlist?list=PLJOzdkh8T5kqCNV_v1w2tapvtJDZYiohW)\r\n- mirror: [https://www.bilibili.com/video/av15565354/](https://www.bilibili.com/video/av15565354/)\r\n\r\n# Resources\r\n\r\n**Open Source Deep Learning Curriculum**\r\n\r\n[http://www.deeplearningweekly.com/pages/open_source_deep_learning_curriculum](http://www.deeplearningweekly.com/pages/open_source_deep_learning_curriculum)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","title":"Deep Learning Frameworks"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Deep Learning Frameworks\r\ndate: 2015-10-09\r\n---\r\n\r\n# Amazon DSSTNE\r\n\r\n**Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine**\r\n\r\n- intro: Deep Scalable Sparse Tensor Network Engine (DSSTNE) is an Amazon developed library \r\nfor building Deep Learning (DL) machine learning (ML) models\r\n- github: [https://github.com/amznlabs/amazon-dsstne](https://github.com/amznlabs/amazon-dsstne)\r\n\r\n# Apache SINGA\r\n\r\n- project-website: [http://singa.incubator.apache.org/](http://singa.incubator.apache.org/)\r\n- github: [https://github.com/apache/incubator-singa](https://github.com/apache/incubator-singa)\r\n- paper: [http://www.comp.nus.edu.sg/~ooibc/singaopen-mm15.pdf](http://www.comp.nus.edu.sg/~ooibc/singaopen-mm15.pdf)\r\n- paper: [http://www.comp.nus.edu.sg/~ooibc/singa-tomm.pdf](http://www.comp.nus.edu.sg/~ooibc/singa-tomm.pdf)\r\n\r\n# Blocks\r\n\r\n**Blocks: A Theano framework for building and training neural networks**\r\n\r\n- github: [https://github.com/mila-udem/blocks](https://github.com/mila-udem/blocks)\r\n\r\n**Blocks and Fuel: Frameworks for deep learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.00619](http://arxiv.org/abs/1506.00619)\r\n\r\n# BrainCore\r\n\r\n**BrainCore: The iOS and OS X neural network framework**\r\n\r\n[https://github.com/aleph7/BrainCore](https://github.com/aleph7/BrainCore)\r\n\r\n# Brainstorm\r\n\r\n**Brainstorm: Fast, flexible and fun neural networks**\r\n\r\n- github: [https://github.com/IDSIA/brainstorm](https://github.com/IDSIA/brainstorm)\r\n\r\n# Caffe\r\n\r\n**Caffe: Convolutional Architecture for Fast Feature Embedding**\r\n\r\n- github: [https://github.com/BVLC/caffe](https://github.com/BVLC/caffe)\r\n- paper: [http://arxiv.org/abs/1408.5093](http://arxiv.org/abs/1408.5093)\r\n- tutorial: [http://tutorial.caffe.berkeleyvision.org/](http://tutorial.caffe.berkeleyvision.org/)\r\n- slides: [http://vision.stanford.edu/teaching/cs231n/slides/caffe_tutorial.pdf](http://vision.stanford.edu/teaching/cs231n/slides/caffe_tutorial.pdf)\r\n- slides: [http://vision.princeton.edu/courses/COS598/2015sp/slides/Caffe/caffe_tutorial.pdf](http://vision.princeton.edu/courses/COS598/2015sp/slides/Caffe/caffe_tutorial.pdf)\r\n- caffe-doc: [http://caffe.berkeleyvision.org/doxygen/index.html](http://caffe.berkeleyvision.org/doxygen/index.html)\r\n- tutorials(\"CAFFE with CUDA\"): [http://pan.baidu.com/s/1i4kmpyH](http://pan.baidu.com/s/1i4kmpyH)\r\n\r\n**OpenCL Caffe**\r\n\r\n- intro: an experimental, community-maintained branch\r\n- github: [https://github.com/BVLC/caffe/tree/opencl](https://github.com/BVLC/caffe/tree/opencl)\r\n\r\n**Caffe on both Linux and Windows**\r\n\r\n- github: [https://github.com/Microsoft/caffe](https://github.com/Microsoft/caffe)\r\n\r\n**ApolloCaffe: a fork of Caffe that supports dynamic networks**\r\n\r\n- homepage: [http://apollocaffe.com/](http://apollocaffe.com/)\r\n- github: [http://github.com/Russell91/apollocaffe](http://github.com/Russell91/apollocaffe)\r\n\r\n**fb-caffe-exts: Some handy utility libraries and tools for the Caffe deep learning framework**\r\n\r\n- intro: fb-caffe-exts is a collection of extensions developed at FB while using Caffe in (mainly) production scenarios.\r\n- github: [https://github.com/facebook/fb-caffe-exts](https://github.com/facebook/fb-caffe-exts)\r\n\r\n**Caffe-Android-Lib: Porting caffe to android platform**\r\n\r\n- github: [https://github.com/sh1r0/caffe-android-lib](https://github.com/sh1r0/caffe-android-lib)\r\n\r\n**caffe-android-demo: An android caffe demo app exploiting caffe pre-trained ImageNet model for image classification**\r\n\r\n- github: [https://github.com/sh1r0/caffe-android-demo](https://github.com/sh1r0/caffe-android-demo)\r\n\r\n**Caffe.js: Run Caffe models in the browser using ConvNetJS**\r\n\r\n- github: [https://github.com/chaosmail/caffejs/](https://github.com/chaosmail/caffejs/)\r\n- demo: [http://chaosmail.github.io/caffejs/models.html](http://chaosmail.github.io/caffejs/models.html)\r\n\r\n**Intel Caffe**\r\n\r\n- intro: This fork of BVLC/Caffe is dedicated to improving performance of this deep learning framework when running on CPU, \r\nin particular Intel® Xeon processors (HSW+) and Intel® Xeon Phi processors\r\n- github [https://github.com/intel/caffe](https://github.com/intel/caffe)\r\n\r\n**NVIDIA Caffe**\r\n\r\n[https://github.com/NVIDIA/caffe](https://github.com/NVIDIA/caffe)\r\n\r\n**Mini-Caffe**\r\n\r\n- intro: Minimal runtime core of Caffe, Forward only, GPU support and Memory efficiency.\r\n- github: [https://github.com/luoyetx/mini-caffe](https://github.com/luoyetx/mini-caffe)\r\n\r\n**Caffe on Mobile Devices**\r\n\r\n- intro: Optimized (for size and speed) Caffe lib for iOS and Android with demo APP.\r\n- github: [https://github.com/solrex/caffe-mobile](https://github.com/solrex/caffe-mobile)\r\n\r\n**CaffeOnACL**\r\n\r\n- intro: Using ARM Compute Library (NEON+GPU) to speed up caffe; Providing utilities to debug, profile and tune application performance\r\n- github: [https://github.com/OAID/caffeOnACL](https://github.com/OAID/caffeOnACL)\r\n\r\n## Multi-GPU / MPI Caffe\r\n\r\n**Caffe with OpenMPI-based Multi-GPU support**\r\n\r\n- intro: A fork of Caffe with OpenMPI-based Multi-GPU (mainly data parallel) support for action recognition and more.\r\n- github: [https://github.com/yjxiong/caffe/tree/mem](https://github.com/yjxiong/caffe/tree/mem)\r\n\r\n**mpi-caffe: Model-distributed Deep Learning with Caffe and MPI**\r\n\r\n- project page: [https://computing.ece.vt.edu/~steflee/mpi-caffe.html](https://computing.ece.vt.edu/~steflee/mpi-caffe.html)\r\n- github: [https://github.com/steflee/mpi-caffe](https://github.com/steflee/mpi-caffe)\r\n\r\n**Caffe-MPI for Deep Learning**\r\n\r\n- github: [https://github.com/Caffe-MPI/Caffe-MPI.github.io](https://github.com/Caffe-MPI/Caffe-MPI.github.io)\r\n- slides: [http://mug.mvapich.cse.ohio-state.edu/static/media/mug/presentations/2016/Caffe-MPI_A_Parallel_Framework_on_the_GPU_Clusters.pdf](http://mug.mvapich.cse.ohio-state.edu/static/media/mug/presentations/2016/Caffe-MPI_A_Parallel_Framework_on_the_GPU_Clusters.pdf)\r\n\r\n## Caffe Utils\r\n\r\n**Caffe-model**\r\n\r\n- intro: Python script to generate prototxt on Caffe, specially the inception_v3\\inception_v4\\inception_resnet\\fractalnet\r\n- github: [https://github.com/soeaver/caffe-model](https://github.com/soeaver/caffe-model)\r\n\r\n# Caffe2\r\n\r\n**Caffe2: A New Lightweight, Modular, and Scalable Deep Learning Framework**\r\n\r\n- intro: Caffe2 is a deep learning framework made with expression, speed, and modularity in mind. \r\nIt is an experimental refactoring of Caffe, and allows a more flexible way to organize computation.\r\n- homepage: [https://caffe2.ai/](https://caffe2.ai/)\r\n- github [https://github.com/caffe2/caffe2](https://github.com/caffe2/caffe2)\r\n- github [https://github.com/Yangqing/caffe2](https://github.com/Yangqing/caffe2)\r\n- model zoo: [https://caffe2.ai/docs/zoo.html](https://caffe2.ai/docs/zoo.html)\r\n- models: [https://github.com/caffe2/models](https://github.com/caffe2/models)\r\n\r\n# CDNN2\r\n\r\n**CDNN2 - CEVA Deep Neural Network Software Framework**\r\n\r\n- intro: Accelerating the development of Artificial Intelligence and its deployment in Low-Power Embedded Systems\r\n- homepage: [http://launch.ceva-dsp.com/cdnn2/](http://launch.ceva-dsp.com/cdnn2/)\r\n\r\n- blog: [http://www.tomshardware.com/news/ceva-cdnn2-tensorflow-embedded-systems,32158.html](http://www.tomshardware.com/news/ceva-cdnn2-tensorflow-embedded-systems,32158.html)\r\n\r\n# Chainer\r\n\r\n**Chainer: a neural network framework**\r\n\r\n- website: [http://chainer.org/](http://chainer.org/)\r\n- github: [https://github.com/pfnet/chainer](https://github.com/pfnet/chainer)\r\n- benchmark: [http://chainer.readthedocs.org/en/latest/comparison.html](http://chainer.readthedocs.org/en/latest/comparison.html)\r\n\r\n**Introduction to Chainer: Neural Networks in Python**\r\n\r\n- blog: [http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/](http://multithreaded.stitchfix.com/blog/2015/12/09/intro-to-chainer/)\r\n- github: [https://github.com/stitchfix/Algorithms-Notebooks](https://github.com/stitchfix/Algorithms-Notebooks)\r\n\r\n# CNTK\r\n\r\n**CNTK: Computational Network Toolkit**\r\n\r\n- github: [https://github.com/Microsoft/CNTK](https://github.com/Microsoft/CNTK)\r\n- book: [http://research.microsoft.com/pubs/226641/CNTKBook-20160121.pdf](http://research.microsoft.com/pubs/226641/CNTKBook-20160121.pdf)\r\n- tutorial: [http://research.microsoft.com/en-us/um/people/dongyu/CNTK-Tutorial-NIPS2015.pdf](http://research.microsoft.com/en-us/um/people/dongyu/CNTK-Tutorial-NIPS2015.pdf)\r\n\r\n**An Introduction to Computational Networks and the Computational Network Toolkit**\r\n\r\n[http://research.microsoft.com/apps/pubs/?id=226641](http://research.microsoft.com/apps/pubs/?id=226641)\r\n\r\n# ConvNetJS\r\n\r\n**ConvNetJS: Deep Learning in Javascript. Train Convolutional Neural Networks (or ordinary ones) in your browser**\r\n\r\n- github: [https://github.com/karpathy/convnetjs](https://github.com/karpathy/convnetjs)\r\n\r\n# DeepBeliefSDK\r\n\r\n**DeepBeliefSDK: The SDK for Jetpac's iOS, Android, Linux, and OS X Deep Belief image recognition framework**\r\n\r\n- github: [https://github.com/jetpacapp/DeepBeliefSDK](https://github.com/jetpacapp/DeepBeliefSDK)\r\n- demo: [https://github.com/jetpacapp/Jetpac-Deep-Belief-Demo-App](https://github.com/jetpacapp/Jetpac-Deep-Belief-Demo-App)\r\n- demo: [https://github.com/jetpacapp/Jetpac-Deep-Belief-Learner-Demo-App](https://github.com/jetpacapp/Jetpac-Deep-Belief-Learner-Demo-App)\r\n\r\n# DeepDetect\r\n\r\n**DeepDetect: Open Source API & Deep Learning Server**\r\n\r\n- webiste: [http://www.deepdetect.com/](http://www.deepdetect.com/)\r\n- github: [https://github.com/beniz/deepdetect](https://github.com/beniz/deepdetect)\r\n\r\n# Deeplearning4j (DL4J)\r\n\r\n**Deeplearning4j: Deep Learning for Java**\r\n\r\n- homepage: [http://deeplearning4j.org/](http://deeplearning4j.org/)\r\n- github: [https://github.com/deeplearning4j/deeplearning4j](https://github.com/deeplearning4j/deeplearning4j)\r\n\r\n**Deeplearning4j images for cuda and hadoop.**\r\n\r\n- github: [https://github.com/deeplearning4j/docker](https://github.com/deeplearning4j/docker)\r\n\r\n**Deeplearning4J Examples**\r\n\r\n- intro: Deeplearning4j Examples (DL4J, DL4J Spark, DataVec)\r\n- github: [https://github.com/deeplearning4j/dl4j-examples](https://github.com/deeplearning4j/dl4j-examples)\r\n\r\n# DeepLearningKit\r\n\r\n**DeepLearningKit: Open Source Deep Learning Framework for Apple's tvOS, iOS and OS X**\r\n\r\n- homepage: [http://deeplearningkit.org/](http://deeplearningkit.org/)\r\n- github: [https://github.com/DeepLearningKit/DeepLearningKit](https://github.com/DeepLearningKit/DeepLearningKit)\r\n\r\n**Tutorial — Using DeepLearningKit with iOS for iPhone and iPad**\r\n\r\n[https://medium.com/@atveit/tutorial-using-deeplearningkit-with-ios-for-iphone-and-ipad-de727679bae4#.1bvnhxhjo](https://medium.com/@atveit/tutorial-using-deeplearningkit-with-ios-for-iphone-and-ipad-de727679bae4#.1bvnhxhjo)\r\n\r\n# DeepSpark\r\n\r\n**DeepSpark: Deeplearning framework running on Spark**\r\n\r\n- github: [https://github.com/deepspark/deepspark](https://github.com/deepspark/deepspark)\r\n- homepage: [http://deepspark.snu.ac.kr/](http://deepspark.snu.ac.kr/)\r\n- arxiv: [http://arxiv.org/abs/1602.08191](http://arxiv.org/abs/1602.08191)\r\n\r\n# DIGITS\r\n\r\n**DIGITS: the Deep Learning GPU Training System**\r\n\r\n- homepage: [https://developer.nvidia.com/digits](https://developer.nvidia.com/digits)\r\n- github: [https://github.com/NVIDIA/DIGITS](https://github.com/NVIDIA/DIGITS)\r\n\r\n# dp\r\n\r\n**dp: A deep learning library for streamlining research and development using the Torch7 distribution**\r\n\r\n- github: [https://github.com/nicholas-leonard/dp](https://github.com/nicholas-leonard/dp)\r\n- manual: [https://dp.readthedocs.org/en/latest/](https://dp.readthedocs.org/en/latest/)\r\n- manual: [https://github.com/nicholas-leonard/dp/blob/master/doc/index.md](https://github.com/nicholas-leonard/dp/blob/master/doc/index.md)\r\n\r\n# Dragon\r\n\r\n**Dragon: A Computation Graph Virtual Machine Based Deep Learning Framework**\r\n\r\n- arxiv: [https://arxiv.org/abs/1707.08265](https://arxiv.org/abs/1707.08265)\r\n- github: [https://github.com/neopenx/Dragon](https://github.com/neopenx/Dragon)\r\n\r\n# DyNet\r\n\r\n**DyNet: The Dynamic Neural Network Toolkit **\r\n\r\n- paper: [https://arxiv.org/abs/1701.03980](https://arxiv.org/abs/1701.03980)\r\n- github: [https://github.com/clab/dynet](https://github.com/clab/dynet)\r\n\r\n**DyNet Benchmarks**\r\n\r\n- github: [https://github.com/neulab/dynet-benchmark](https://github.com/neulab/dynet-benchmark)\r\n\r\n# IDLF\r\n\r\n**IDLF: The Intel® Deep Learning Framework**\r\n\r\n- website: [https://01.org/zh/intel-deep-learning-framework?langredirect=1](https://01.org/zh/intel-deep-learning-framework?langredirect=1)\r\n- github: [https://github.com/01org/idlf](https://github.com/01org/idlf)\r\n\r\n# Keras\r\n\r\n**Keras: Deep Learning library for Theano and TensorFlow**\r\n\r\n- github: [https://github.com/fchollet/keras](https://github.com/fchollet/keras)\r\n- blog: [http://blog.keras.io/introducing-keras-10.html](http://blog.keras.io/introducing-keras-10.html)\r\n- docs: [http://keras.io/getting-started/functional-api-guide/](http://keras.io/getting-started/functional-api-guide/)\r\n\r\n**MarcBS/keras fork**\r\n\r\n- github: [https://github.com/MarcBS/keras](https://github.com/MarcBS/keras)\r\n\r\n**Hera: Train/evaluate a Keras model, get metrics streamed to a dashboard in your browser.**\r\n\r\n![](https://cloud.githubusercontent.com/assets/5866348/16719660/13460bee-46e2-11e6-8ab1-56873807390d.gif)\r\n\r\n- github: [https://github.com/jakebian/hera](https://github.com/jakebian/hera)\r\n\r\n**Installing Keras for deep learning**\r\n\r\n- blog: [http://www.pyimagesearch.com/2016/07/18/installing-keras-for-deep-learning/](http://www.pyimagesearch.com/2016/07/18/installing-keras-for-deep-learning/)\r\n\r\n**Keras Applications - deep learning models that are made available alongside pre-trained weights**\r\n\r\n[https://keras.io/applications/](https://keras.io/applications/)\r\n\r\n**Keras resources: Directory of tutorials and open-source code repositories for working with Keras, the Python deep learning library**\r\n\r\n- github: [https://github.com/fchollet/keras-resources](https://github.com/fchollet/keras-resources)\r\n\r\n**Keras.js: Run trained Keras models in the browser, with GPU support**\r\n\r\n- homepage: [https://transcranial.github.io/keras-js/](https://transcranial.github.io/keras-js/)\r\n- github: [https://github.com/transcranial/keras-js](https://github.com/transcranial/keras-js)\r\n\r\n**keras2cpp**\r\n\r\n- intro: This is a bunch of code to port Keras neural network model into pure C++.\r\n- github: [https://github.com/pplonski/keras2cpp](https://github.com/pplonski/keras2cpp)\r\n\r\n**keras-cn: Chinese keras documents with more examples, explanations and tips.**\r\n\r\n- github: [https://github.com/MoyanZitto/keras-cn](https://github.com/MoyanZitto/keras-cn)\r\n\r\n**Kerasify: Small library for running Keras models from a C++ application**\r\n\r\n[https://github.com/moof2k/kerasify](https://github.com/moof2k/kerasify)\r\n\r\n# Knet\r\n\r\n**Knet: Koç University deep learning framework**\r\n\r\n- intro: Knet (pronounced \"kay-net\") is the Koç University deep learning framework implemented in Julia by Deniz Yuret and collaborators.\r\n- github: [https://github.com/denizyuret/Knet.jl](https://github.com/denizyuret/Knet.jl)\r\n- doc: [https://knet.readthedocs.org/en/latest/](https://knet.readthedocs.org/en/latest/)\r\n\r\n# Lasagne\r\n\r\n**Lasagne: Lightweight library to build and train neural networks in Theano**\r\n\r\n- github: [https://github.com/Lasagne/Lasagne](https://github.com/Lasagne/Lasagne)\r\n- docs: [http://lasagne.readthedocs.org/en/latest/](http://lasagne.readthedocs.org/en/latest/)\r\n\r\n# Leaf\r\n\r\n**Leaf: The Hacker's Machine Learning Engine**\r\n\r\n- homepage: [http://autumnai.github.io/leaf/leaf/index.html](http://autumnai.github.io/leaf/leaf/index.html)\r\n- github: [https://github.com/autumnai/leaf](https://github.com/autumnai/leaf)\r\n- homepage: [http://autumnai.com/leaf/book/leaf.html](http://autumnai.com/leaf/book/leaf.html)\r\n- homepage(\"The Hacker's Machine Intelligence Platform\"): [http://autumnai.com/](http://autumnai.com/)\r\n\r\n# LightNet\r\n\r\n![](https://raw.githubusercontent.com/yechengxi/LightNet/master/LightNet.png)\r\n\r\n**LightNet: A Versatile, Standalone and Matlab-based Environment for Deep Learning**\r\n\r\n- homepage: [http://www.umiacs.umd.edu/~yzyang/LightNet/](http://www.umiacs.umd.edu/~yzyang/LightNet/)\r\n- github: [https://github.com/yechengxi/lightnet](https://github.com/yechengxi/lightnet)\r\n\r\n# MatConvNet\r\n\r\n**MatConvNet: CNNs for MATLAB**\r\n\r\n- homepage: [http://www.vlfeat.org/matconvnet/](http://www.vlfeat.org/matconvnet/)\r\n- github: [https://github.com/vlfeat/matconvnet](https://github.com/vlfeat/matconvnet)\r\n\r\n# Marvin\r\n\r\n**Marvin: A minimalist GPU-only N-dimensional ConvNet framework**\r\n\r\n- homepage: [http://marvin.is/](http://marvin.is/)\r\n- github: [https://github.com/PrincetonVision/marvin](https://github.com/PrincetonVision/marvin)\r\n\r\n**MatConvNet: CNNs for MATLAB**\r\n\r\n- homepage: [http://www.vlfeat.org/matconvnet/](http://www.vlfeat.org/matconvnet/)\r\n- pretianed models: [http://www.vlfeat.org/matconvnet/pretrained/](http://www.vlfeat.org/matconvnet/pretrained/)\r\n\r\n# Mocha.jl\r\n\r\n**Mocha.jl: Deep Learning for Julia**\r\n\r\n- homepage: [http://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/](http://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/)\r\n- github: [https://github.com/pluskid/Mocha.jl](https://github.com/pluskid/Mocha.jl)\r\n\r\n# MXNet\r\n\r\n**MXNet**\r\n\r\n![](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/banner.png)\r\n\r\n- github: [https://github.com/dmlc/mxnet](https://github.com/dmlc/mxnet)\r\n\r\n**MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems**\r\n\r\n- paper: [https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/paper/mxnet-learningsys.pdf](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/paper/mxnet-learningsys.pdf)\r\n\r\n**MXNet Model Gallery: Pre-trained Models of DMLC Project**\r\n\r\n- github: [https://github.com/dmlc/mxnet-model-gallery](https://github.com/dmlc/mxnet-model-gallery)\r\n\r\n**a short introduction to mxnet design and implementation (chinese)**\r\n\r\n- github: [https://github.com/dmlc/mxnet/blob/master/doc/overview_chn.md](https://github.com/dmlc/mxnet/blob/master/doc/overview_chn.md)\r\n- github-issues: [https://github.com/dmlc/mxnet/issues/797](https://github.com/dmlc/mxnet/issues/797)\r\n\r\n**Deep learning for hackers with MXnet (1) GPU installation and MNIST**\r\n\r\n[https://no2147483647.wordpress.com/2015/12/07/deep-learning-for-hackers-with-mxnet-1/](https://no2147483647.wordpress.com/2015/12/07/deep-learning-for-hackers-with-mxnet-1/)\r\n\r\n**mxnet_Efficient, Flexible Deep Learning Framework**\r\n\r\n- slides: [http://vdisk.weibo.com/s/z5dg0jVVHv2pn/1450157571](http://vdisk.weibo.com/s/z5dg0jVVHv2pn/1450157571)\r\n\r\n**Use Caffe operator in MXNet**\r\n\r\n- blog: [http://dmlc.ml/mxnet/2016/07/29/use-caffe-operator-in-mxnet.html](http://dmlc.ml/mxnet/2016/07/29/use-caffe-operator-in-mxnet.html)**\r\n\r\n**Deep Learning in a Single File for Smart Devices**\r\n\r\n[https://mxnet.readthedocs.org/en/latest/tutorial/smart_device.html](https://mxnet.readthedocs.org/en/latest/tutorial/smart_device.html)\r\n\r\n**MXNet Pascal Titan X benchmark**\r\n\r\n- blog: [http://dmlc.ml/mxnet/2016/08/03/mxnet-titanx-benchmark.html](http://dmlc.ml/mxnet/2016/08/03/mxnet-titanx-benchmark.html)\r\n\r\n**用MXnet实战深度学习之一:安装GPU版mxnet并跑一个MNIST手写数字识别**\r\n\r\n[http://phunter.farbox.com/post/mxnet-tutorial1](http://phunter.farbox.com/post/mxnet-tutorial1)\r\n\r\n**用MXnet实战深度学习之二:Neural art**\r\n\r\n[http://phunter.farbox.com/post/mxnet-tutorial2](http://phunter.farbox.com/post/mxnet-tutorial2)\r\n\r\n**Programming Models and Systems Design for Deep Learning**\r\n\r\n- video: [http://research.microsoft.com/apps/video/default.aspx?id=262396](http://research.microsoft.com/apps/video/default.aspx?id=262396)\r\n- video: [http://pan.baidu.com/s/1mgSnj64](http://pan.baidu.com/s/1mgSnj64)\r\n\r\n**Awesome MXNet**\r\n\r\n- intro: This page contains a curated list of awesome MXnet examples, tutorials and blogs.\r\n- github: [https://github.com/dmlc/mxnet/blob/master/example/README.md](https://github.com/dmlc/mxnet/blob/master/example/README.md)\r\n\r\n**Getting Started with MXNet**\r\n\r\n[https://indico.io/blog/getting-started-with-mxnet/](https://indico.io/blog/getting-started-with-mxnet/)\r\n\r\n**gtc_tutorial: MXNet Tutorial for NVidia GTC 2016**\r\n\r\n- report: [http://on-demand.gputechconf.com/gtc/2016/video/S6853.html](http://on-demand.gputechconf.com/gtc/2016/video/S6853.html)\r\n- tutorial: [http://on-demand.gputechconf.com/gtc/2016/video/L6143.html](http://on-demand.gputechconf.com/gtc/2016/video/L6143.html)\r\n- video: [http://pan.baidu.com/s/1eS58Gue](http://pan.baidu.com/s/1eS58Gue)\r\n- github: [https://github.com/dmlc/mxnet-gtc-tutorial](https://github.com/dmlc/mxnet-gtc-tutorial)\r\n\r\n**MXNET Dependency Engine**\r\n\r\n- blog: [http://yuyang0.github.io/articles/mxnet-engine.html](http://yuyang0.github.io/articles/mxnet-engine.html)\r\n\r\n**MXNET是这样压榨深度学习的内存消耗的**\r\n\r\n- doc: [https://github.com/dmlc/mxnet/blob/master/docs/zh/note_memory.md](https://github.com/dmlc/mxnet/blob/master/docs/zh/note_memory.md)\r\n\r\n**WhatsThis-iOS: MXNet WhatThis Example for iOS**\r\n\r\n- github: [https://github.com/pppoe/WhatsThis-iOS](https://github.com/pppoe/WhatsThis-iOS)\r\n\r\n**MXNET-MPI: Embedding MPI parallelism in Parameter Server Task Model for scaling Deep Learning**\r\n\r\n- intro: IBM T J Watson Research Center\r\n- arxiv: [https://arxiv.org/abs/1801.03855](https://arxiv.org/abs/1801.03855)\r\n\r\n# ncnn\r\n\r\n- intro: ncnn is a high-performance neural network inference framework optimized for the mobile platform\r\n- github: [https://github.com/Tencent/ncnn](https://github.com/Tencent/ncnn)\r\n\r\n# neocortex.js\r\n\r\n**Run trained deep neural networks in the browser or node.js**\r\n\r\n- homepage: [http://scienceai.github.io/neocortex/](http://scienceai.github.io/neocortex/)\r\n- github: [https://github.com/scienceai/neocortex](https://github.com/scienceai/neocortex)\r\n\r\n# Neon\r\n\r\n**Neon: Nervana’s Python-based deep learning library**\r\n\r\n- website: [http://neon.nervanasys.com/docs/latest/index.html](http://neon.nervanasys.com/docs/latest/index.html)\r\n- github: [https://github.com/NervanaSystems/neon](https://github.com/NervanaSystems/neon)\r\n- website: [https://www.nervanasys.com/learn/](https://www.nervanasys.com/learn/)\r\n\r\n**Tools to convert Caffe models to neon's serialization format**\r\n\r\n- github: [https://github.com/NervanaSystems/caffe2neon](https://github.com/NervanaSystems/caffe2neon)\r\n\r\n**Nervana’s Deep Learning Course**\r\n\r\n- homepage: [https://www.nervanasys.com/deep-learning-tutorials/](https://www.nervanasys.com/deep-learning-tutorials/)\r\n- github: [https://github.com/NervanaSystems/neon_course](https://github.com/NervanaSystems/neon_course)\r\n\r\n# NNabla\r\n\r\n**NNabla - Neural Network Libraries by Sony**\r\n\r\n- intro: NNabla - Neural Network Libraries NNabla is a deep learning framework that is intended to be used for research, development and production. We aim it running everywhere like desktop PCs, HPC clusters, embedded devices and production servers.\r\n- homepage: [https://nnabla.org/](https://nnabla.org/)\r\n- github: [https://github.com/sony/nnabla](https://github.com/sony/nnabla)\r\n\r\n# OpenDeep\r\n\r\n**OpenDeep: a fully modular & extensible deep learning framework in Python**\r\n\r\n- intro: Modular & extensible deep learning framework built on Theano\r\n- homepage: [http://www.opendeep.org/](http://www.opendeep.org/)\r\n- github: [https://github.com/vitruvianscience/opendeep](https://github.com/vitruvianscience/opendeep)\r\n\r\n# OpenNN\r\n\r\n**OpenNN - Open Neural Networks Library**\r\n\r\n![](http://opennn.net/images/OpenNN%20screenshot.png)\r\n\r\n- homepage: [http://opennn.net/](http://opennn.net/)\r\n- github: [https://github.com/artelnics/opennn](https://github.com/artelnics/opennn)\r\n\r\n# Paddle\r\n\r\n**PaddlePaddle: PArallel Distributed Deep LEarning**\r\n\r\n- homepage: [http://www.paddlepaddle.org/](http://www.paddlepaddle.org/)\r\n- github: [https://github.com/baidu/Paddle](https://github.com/baidu/Paddle)\r\n- installation: [http://www.paddlepaddle.org/doc/build/](http://www.paddlepaddle.org/doc/build/)\r\n\r\n**基于Spark的异构分布式深度学习平台**\r\n\r\n[http://geek.csdn.net/news/detail/58867](http://geek.csdn.net/news/detail/58867)\r\n\r\n# Petuum\r\n\r\n**Petuum: a distributed machine learning framework**\r\n\r\n- website: [http://petuum.github.io/](http://petuum.github.io/)\r\n- github: [https://github.com/petuum/bosen](https://github.com/petuum/bosen)\r\n\r\n# PlaidML\r\n\r\n**PlaidML: A framework for making deep learning work everywhere**\r\n\r\n- homepage: [http://vertex.ai/](http://vertex.ai/)\r\n- github: [https://github.com/plaidml/plaidml](https://github.com/plaidml/plaidml)\r\n\r\n# Platoon\r\n\r\n**Platoon: Multi-GPU mini-framework for Theano**\r\n\r\n- github: [https://github.com/mila-udem/platoon](https://github.com/mila-udem/platoon)\r\n\r\n# Poseidon\r\n\r\n**Poseidon: Distributed Deep Learning Framework on Petuum**\r\n\r\n- github: [https://github.com/petuum/poseidon](https://github.com/petuum/poseidon)\r\n- wiki: [https://github.com/petuum/poseidon/wiki](https://github.com/petuum/poseidon/wiki)\r\n\r\n# Purine\r\n\r\n**Purine: A bi-graph based deep learning framework**\r\n\r\n- github: [https://github.com/purine/purine2](https://github.com/purine/purine2)\r\n- arxiv: [http://arxiv.org/abs/1412.6249](http://arxiv.org/abs/1412.6249)\r\n\r\n# PyTorch\r\n\r\n**PyTorch**\r\n\r\n- github: [https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)\r\n\r\n**Datasets, Transforms and Models specific to Computer Vision**\r\n\r\n[https://github.com/pytorch/vision/](https://github.com/pytorch/vision/)\r\n\r\n**Convert torch to pytorch**\r\n\r\n[https://github.com/clcarwin/convert_torch_to_pytorch](https://github.com/clcarwin/convert_torch_to_pytorch)\r\n\r\n# TensorFlow\r\n\r\n**TensorFlow**\r\n\r\n- website: [http://tensorflow.org/](http://tensorflow.org/)\r\n- github: [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)\r\n- github: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime)\r\n- tutorial: [http://tensorflow.org/tutorials](http://tensorflow.org/tutorials)\r\n- tutorial: [https://github.com/nlintz/TensorFlow-Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\r\n- stackoverflow: [https://stackoverflow.com/questions/tagged/tensorflow](https://stackoverflow.com/questions/tagged/tensorflow)\r\n- benchmark: [https://github.com/soumith/convnet-benchmarks/issues/66](https://github.com/soumith/convnet-benchmarks/issues/66)\r\n\r\n**Benchmarks**\r\n\r\n- intro: A selection of image classification models were tested across multiple platforms to create a point of reference for the TensorFlow community\r\n- homepage: [https://www.tensorflow.org/performance/benchmarks](https://www.tensorflow.org/performance/benchmarks)\r\n\r\n## TensorDebugger (TDB)\r\n\r\n**TensorDebugger(TDB): Interactive, node-by-node debugging and visualization for TensorFlow**\r\n\r\n![](https://camo.githubusercontent.com/4c671d2b359c9984472f37a73136971fd60e76e4/687474703a2f2f692e696d6775722e636f6d2f6e30506d58516e2e676966)\r\n\r\n- github: [https://github.com/ericjang/tdb](https://github.com/ericjang/tdb)\r\n\r\n**ofxMSATensorFlow: OpenFrameworks addon for Google's data-flow graph based numerical computation / machine intelligence library TensorFlow.**\r\n\r\n- github: [https://github.com/memo/ofxMSATensorFlow](https://github.com/memo/ofxMSATensorFlow)\r\n\r\n**TFLearn: Deep learning library featuring a higher-level API for TensorFlow**\r\n\r\n- homepage: [http://tflearn.org/](http://tflearn.org/)\r\n- github: [https://github.com/tflearn/tflearn](https://github.com/tflearn/tflearn)\r\n- examples: [https://github.com/tflearn/tflearn/blob/0.1.0/examples/README.md](https://github.com/tflearn/tflearn/blob/0.1.0/examples/README.md)\r\n\r\n**TensorFlow on Spark**\r\n\r\n- github: [https://github.com/adatao/tensorspark](https://github.com/adatao/tensorspark)\r\n\r\n**TensorBoard**\r\n\r\n**TensorFlow.jl: A Julia wrapper for the TensorFlow Python library**\r\n\r\n- github: [https://github.com/benmoran/TensorFlow.jl](https://github.com/benmoran/TensorFlow.jl)\r\n\r\n**TensorLayer: Deep learning and Reinforcement learning library for TensorFlow**\r\n\r\n- github: [https://github.com/zsdonghao/tensorlayer](https://github.com/zsdonghao/tensorlayer)\r\n- docs: [http://tensorlayer.readthedocs.io/en/latest/](http://tensorlayer.readthedocs.io/en/latest/)\r\n\r\n**OpenCL support for TensorFlow**\r\n\r\n- github: [https://github.com/benoitsteiner/tensorflow-opencl](https://github.com/benoitsteiner/tensorflow-opencl)\r\n\r\n**Pretty Tensor: Fluent Networks in TensorFlow**\r\n\r\n- github: [https://github.com/google/prettytensor](https://github.com/google/prettytensor)\r\n- docs: [https://github.com/google/prettytensor/blob/master/docs/pretty_tensor_top_level.md](https://github.com/google/prettytensor/blob/master/docs/pretty_tensor_top_level.md)\r\n- tutorials: [https://github.com/google/prettytensor/tree/master/prettytensor/tutorial](https://github.com/google/prettytensor/tree/master/prettytensor/tutorial)\r\n\r\n**Rust language bindings for TensorFlow**\r\n\r\n- github: [https://github.com/tensorflow/rust](https://github.com/tensorflow/rust)\r\n\r\n**TensorFlow Ecosystem: Integration of TensorFlow with other open-source frameworks**\r\n\r\n- github: [https://github.com/tensorflow/ecosystem](https://github.com/tensorflow/ecosystem)\r\n\r\n**Caffe to TensorFlow**\r\n\r\n- intro: Convert Caffe models to TensorFlow.\r\n- github: [https://github.com/ethereon/caffe-tensorflow](https://github.com/ethereon/caffe-tensorflow)\r\n\r\n**TensorFlow Mobile**\r\n\r\n[https://www.tensorflow.org/mobile/](https://www.tensorflow.org/mobile/)\r\n\r\n## Papers\r\n\r\n**TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.04467](http://arxiv.org/abs/1603.04467)\r\n- whitepaper: [http://download.tensorflow.org/paper/whitepaper2015.pdf](http://download.tensorflow.org/paper/whitepaper2015.pdf)\r\n\r\n**TensorFlow: A system for large-scale machine learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.08695](http://arxiv.org/abs/1605.08695)\r\n\r\n**TensorFlow Distributions**\r\n\r\n[https://arxiv.org/abs/1711.10604](https://arxiv.org/abs/1711.10604)\r\n\r\n## Tutorials\r\n\r\n**TensorFlow 官方文档中文版**\r\n\r\n- tutorial-zh: [https://github.com/jikexueyuanwiki/tensorflow-zh](https://github.com/jikexueyuanwiki/tensorflow-zh)\r\n- homepage: [http://wiki.jikexueyuan.com/project/tensorflow-zh/](http://wiki.jikexueyuan.com/project/tensorflow-zh/)\r\n\r\n# Theano\r\n\r\n**Theano**\r\n\r\n- website: [http://deeplearning.net/software/theano/index.html](http://deeplearning.net/software/theano/index.html)\r\n- github: [https://github.com/Theano/Theano](https://github.com/Theano/Theano)\r\n\r\n**Theano-Tutorials: Bare bones introduction to machine learning from linear regression to convolutional neural networks using Theano**\r\n\r\n- github: [https://github.com/Newmu/Theano-Tutorials](https://github.com/Newmu/Theano-Tutorials)\r\n\r\n**Theano: A Python framework for fast computation of mathematical expressions**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.02688](http://arxiv.org/abs/1605.02688)\r\n\r\n**Configuring Theano For High Performance Deep Learning**\r\n\r\n[http://www.johnwittenauer.net/configuring-theano-for-high-performance-deep-learning/](http://www.johnwittenauer.net/configuring-theano-for-high-performance-deep-learning/)\r\n\r\n**Theano: a short practical guide**\r\n\r\n- slides: [http://folinoid.com/show/theano/](http://folinoid.com/show/theano/)\r\n\r\n**Ian Goodfellow's Tutorials on Theano**\r\n\r\n- slides: [http://pan.baidu.com/s/1slbzhF3#path=%252F%25E6%2588%2591%25E7%259A%2584%25E5%2588%2586%25E4%25BA%25AB%252F201604%252FIan%2520Goodfellow's%2520Tutorials%2520on%2520Theano](http://pan.baidu.com/s/1slbzhF3#path=%252F%25E6%2588%2591%25E7%259A%2584%25E5%2588%2586%25E4%25BA%25AB%252F201604%252FIan%2520Goodfellow's%2520Tutorials%2520on%2520Theano)\r\n- github(\"theano_exercises\"): [https://github.com/goodfeli/theano_exercises](https://github.com/goodfeli/theano_exercises)\r\n\r\n**Plato: A library built on top of Theano**\r\n\r\n- github: [https://github.com/petered/plato](https://github.com/petered/plato)\r\n- tutorial: [https://rawgit.com/petered/plato/master/plato_tutorial.html](https://rawgit.com/petered/plato/master/plato_tutorial.html)\r\n\r\n**Theano Windows Install Guide**\r\n\r\n- github: [https://github.com/mrakgr/Tutorials/blob/master/theano_install.md](https://github.com/mrakgr/Tutorials/blob/master/theano_install.md)\r\n\r\n**Theano-MPI: a Theano-based Distributed Training Framework**\r\n\r\n- arxiv: [https://arxiv.org/abs/1605.08325](https://arxiv.org/abs/1605.08325)\r\n- github: [https://github.com/uoguelph-mlrg/Theano-MPI](https://github.com/uoguelph-mlrg/Theano-MPI)\r\n\r\n# tiny-dnn (tiny-cnn)\r\n\r\n**tiny-dnn: A header only, dependency-free deep learning framework in C++11**\r\n\r\n- inrtro: tiny-dnn is a C++11 implementation of deep learning. \r\nIt is suitable for deep learning on limited computational resource, embedded systems and IoT devices.\r\n- github: [https://github.com/tiny-dnn/tiny-dnn](https://github.com/tiny-dnn/tiny-dnn)\r\n- github: [https://github.com/nyanp/tiny-cnn](https://github.com/nyanp/tiny-cnn)\r\n\r\n**Deep learning with C++ - an introduction to tiny-dnn**\r\n\r\n- slides: [http://www.slideshare.net/ssuser756ec5/deep-learning-with-c-an-introduction-to-tinydnn](http://www.slideshare.net/ssuser756ec5/deep-learning-with-c-an-introduction-to-tinydnn)\r\n\r\n# Torch\r\n\r\n**Torch**\r\n\r\n- website: [http://torch.ch/](http://torch.ch/)\r\n- github: [https://github.com/torch/torch7](https://github.com/torch/torch7)\r\n- cheatsheet: [https://github.com/torch/torch7/wiki/Cheatsheet](https://github.com/torch/torch7/wiki/Cheatsheet) \r\n- tutorials(\"Getting started with Torch\"): [http://torch.ch/docs/getting-started.html#_](http://torch.ch/docs/getting-started.html#_)\r\n\r\n**loadcaffe: Load Caffe networks in Torch7**\r\n\r\n- github: [https://github.com/szagoruyko/loadcaffe](https://github.com/szagoruyko/loadcaffe)\r\n\r\n**Applied Deep Learning for Computer Vision with Torch**\r\n\r\n- homepage: [https://github.com/soumith/cvpr2015](https://github.com/soumith/cvpr2015)\r\n\r\n**pytorch: Python wrappers for torch and lua**\r\n\r\n- github: [https://github.com/hughperkins/pytorch](https://github.com/hughperkins/pytorch)\r\n\r\n**Torch Toolbox: A collection of snippets and libraries for Torch**\r\n\r\n- github: [https://github.com/e-lab/torch-toolbox](https://github.com/e-lab/torch-toolbox)\r\n\r\n**cltorch: a Hardware-Agnostic Backend for the Torch Deep Neural Network Library, Based on OpenCL**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.04884](http://arxiv.org/abs/1606.04884)\r\n- github: [https://github.com/hughperkins/cltorch](https://github.com/hughperkins/cltorch)\r\n\r\n**Torchnet: An Open-Source Platform for (Deep) Learning Research**\r\n\r\n- paper: [https://lvdmaaten.github.io/publications/papers/Torchnet_2016.pdf](https://lvdmaaten.github.io/publications/papers/Torchnet_2016.pdf)\r\n- github: [https://github.com/torchnet/torchnet](https://github.com/torchnet/torchnet)\r\n\r\n**THFFmpeg: Torch bindings for FFmpeg (reading videos only)**\r\n\r\n- github: [https://github.com/MichaelMathieu/THFFmpeg](https://github.com/MichaelMathieu/THFFmpeg)\r\n\r\n**caffegraph: Load Caffe networks in Torch7 using nngraph**\r\n\r\n- github: [https://github.com/nhynes/caffegraph](https://github.com/nhynes/caffegraph)\r\n\r\n**Optimized-Torch: Intel Torch is dedicated to improving Torch performance when running on CPU**\r\n\r\n- intro: Intel Torch gets 4.66x speedup using the convnet-benchmarks which includes AlexNet,VGG-E,GoogLenet,ResidualNet\r\n- github: [https://github.com/xhzhao/optimized-torch](https://github.com/xhzhao/optimized-torch)\r\n- benchmark: [https://github.com/xhzhao/Optimized-Torch-benchmark](https://github.com/xhzhao/Optimized-Torch-benchmark)\r\n\r\n**Torch Video Tutorials**\r\n\r\n- github: [https://github.com/Atcold/torch-Video-Tutorials](https://github.com/Atcold/torch-Video-Tutorials)\r\n\r\n**Torch in Action**\r\n\r\n- github: [https://github.com/nicholas-leonard/torch-in-action](https://github.com/nicholas-leonard/torch-in-action)\r\n\r\n# VELES\r\n\r\n**VELES: Distributed platform for rapid Deep learning application development**\r\n\r\n- website: [https://velesnet.ml/](https://velesnet.ml/)\r\n- github: [https://github.com/Samsung/veles](https://github.com/Samsung/veles)\r\n- workflow: [https://velesnet.ml/forge/forge.html](https://velesnet.ml/forge/forge.html)\r\n\r\n# WebDNN\r\n\r\n**WebDNN: Fastest DNN Execution Framework on Web Browser**\r\n\r\n- homepage: [https://mil-tokyo.github.io/webdnn/](https://mil-tokyo.github.io/webdnn/)\r\n- github: [https://github.com/mil-tokyo/webdnn](https://github.com/mil-tokyo/webdnn)\r\n\r\n# Yann\r\n\r\n**Yann: Yet Another Neural Network Toolbox**\r\n\r\n- intro: It is a toolbox for building and learning convolutional neural networks, built on top of theano\r\n- github: [https://github.com/ragavvenkatesan/yann](https://github.com/ragavvenkatesan/yann)\r\n- docs: [http://yann.readthedocs.io/en/master/](http://yann.readthedocs.io/en/master/)\r\n\r\n# Benchmarks\r\n\r\n**Easy benchmarking of all publicly accessible implementations of convnets**\r\n\r\n[https://github.com/soumith/convnet-benchmarks](https://github.com/soumith/convnet-benchmarks)\r\n\r\n**Stanford DAWN Deep Learning Benchmark (DAWNBench) - An End-to-End Deep Learning Benchmark and Competition**\r\n\r\n[http://dawn.cs.stanford.edu/benchmark/index.html](http://dawn.cs.stanford.edu/benchmark/index.html)\r\n\r\n# Tutorials\r\n\r\n**Deep Learning Implementations and Frameworks (DLIF)**\r\n\r\n- tutorial: [https://sites.google.com/site/dliftutorial/](https://sites.google.com/site/dliftutorial/)\r\n- github: [https://github.com/delta2323/DLIF-tutorial](https://github.com/delta2323/DLIF-tutorial)\r\n\r\n# Papers\r\n\r\n**Comparative Study of Deep Learning Software Frameworks**\r\n\r\n- intro: Caffe / Neon / TensorFlow / Theano / Torch\r\n- arxiv: [http://arxiv.org/abs/1511.06435](http://arxiv.org/abs/1511.06435)\r\n- github: [https://github.com/DL-Benchmarks/DL-Benchmarks](https://github.com/DL-Benchmarks/DL-Benchmarks)\r\n\r\n**Benchmarking State-of-the-Art Deep Learning Software Tools**\r\n\r\n- intro: Caffe, CNTK, MXNet, TensorFlow, and Torch\r\n- project page: [http://dlbench.comp.hkbu.edu.hk/](http://dlbench.comp.hkbu.edu.hk/)\r\n- arxiv: [http://arxiv.org/abs/1608.07249](http://arxiv.org/abs/1608.07249)\r\n\r\n# Projects\r\n\r\n**TensorFuse: Common interface for Theano, CGT, and TensorFlow**\r\n\r\n- github: [https://github.com/dementrock/tensorfuse](https://github.com/dementrock/tensorfuse)\r\n\r\n**DeepRosetta: An universal deep learning models conversor**\r\n\r\n- github: [https://github.com/edgarriba/DeepRosetta](https://github.com/edgarriba/DeepRosetta)\r\n\r\n**Deep Learning Model Convertors**\r\n\r\n[https://github.com/ysh329/deep-learning-model-convertor](https://github.com/ysh329/deep-learning-model-convertor)\r\n\r\n# References\r\n\r\n**Frameworks and Libraries for Deep Learning**\r\n\r\n[http://creative-punch.net/2015/07/frameworks-and-libraries-for-deep-learning/](http://creative-punch.net/2015/07/frameworks-and-libraries-for-deep-learning/)\r\n\r\n**TensorFlow vs. Theano vs. Torch**\r\n\r\n[https://github.com/zer0n/deepframeworks/blob/master/README.md](https://github.com/zer0n/deepframeworks/blob/master/README.md)\r\n\r\n**Evaluation of Deep Learning Toolkits**\r\n\r\n[https://github.com/zer0n/deepframeworks/blob/master/README.md](https://github.com/zer0n/deepframeworks/blob/master/README.md)\r\n\r\n**Deep Machine Learning libraries and frameworks**\r\n\r\n[https://medium.com/@abduljaleel/deep-machine-learning-libraries-and-frameworks-5fdf2bb6bfbe#.q1mhj7c36](https://medium.com/@abduljaleel/deep-machine-learning-libraries-and-frameworks-5fdf2bb6bfbe#.q1mhj7c36)\r\n\r\n**Torch vs Theano**\r\n\r\n- blog: [http://fastml.com/torch-vs-theano/](http://fastml.com/torch-vs-theano/)\r\n\r\n**Deep Learning Software: NVIDIA Deep Learning SDK**\r\n\r\n![](https://developer.nvidia.com/sites/default/files/akamai/cuda/images/deeplearning/digits-2-gpu-utilization_faded.png)\r\n\r\n[https://developer.nvidia.com/deep-learning-software](https://developer.nvidia.com/deep-learning-software)\r\n\r\n**A comparison of deep learning frameworks**\r\n\r\n- intro: Theano/CGT/Torch/MXNet\r\n- gist: [https://gist.github.com/bartvm/69adf7aad100d58831b0](https://gist.github.com/bartvm/69adf7aad100d58831b0)\r\n- webo: [http://weibo.com/p/1001603946281180481229](http://weibo.com/p/1001603946281180481229)\r\n\r\n**TensorFlow Meets Microsoft’s CNTK**\r\n\r\n- blog: [http://esciencegroup.com/2016/02/08/tensorflow-meets-microsofts-cntk/](http://esciencegroup.com/2016/02/08/tensorflow-meets-microsofts-cntk/)\r\n\r\n**Is there a case for still using Torch, Theano, Brainstorm, MXNET and not switching to TensorFlow?**\r\n\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/47qh90/is_there_a_case_for_still_using_torch_theano/][https://www.reddit.com/r/MachineLearning/comments/47qh90/is_there_a_case_for_still_using_torch_theano/]\r\n\r\n**DL4J vs. Torch vs. Theano vs. Caffe vs. TensorFlow**\r\n\r\n[http://deeplearning4j.org/compare-dl4j-torch7-pylearn.html](http://deeplearning4j.org/compare-dl4j-torch7-pylearn.html)\r\n\r\n**Popular Deep Learning Libraries**\r\n\r\n- blog: [http://machinelearningmastery.com/popular-deep-learning-libraries/](http://machinelearningmastery.com/popular-deep-learning-libraries/)\r\n\r\n**The simple example of Theano and Lasagne super power**\r\n\r\n![](https://grzegorzgwardys.files.wordpress.com/2016/05/modified_cnn.png?w=640)\r\n\r\n[https://grzegorzgwardys.wordpress.com/2016/05/15/the-simple-example-of-theano-and-lasagne-super-power/](https://grzegorzgwardys.wordpress.com/2016/05/15/the-simple-example-of-theano-and-lasagne-super-power/)\r\n\r\n**Comparison of deep learning software**\r\n\r\n- wiki: [https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software)\r\n\r\n**A Look at Popular Machine Learning Frameworks**\r\n\r\n- blog: [http://redmonk.com/fryan/2016/06/06/a-look-at-popular-machine-learning-frameworks/](http://redmonk.com/fryan/2016/06/06/a-look-at-popular-machine-learning-frameworks/)\r\n\r\n**5 Deep Learning Projects You Can No Longer Overlook**\r\n\r\n- keywords: Leaf / tiny-cnn / Layered / Brain / neon\r\n- blog: [http://www.kdnuggets.com/2016/07/five-deep-learning-projects-cant-overlook.html](http://www.kdnuggets.com/2016/07/five-deep-learning-projects-cant-overlook.html)\r\n\r\n**Comparison of Deep Learning Libraries After Years of Use**\r\n\r\n- intro: Torch / MxNet / Theano / Caffe\r\n- blog:[http://www.erogol.com/comparison-deep-learning-libraries-years-use/](http://www.erogol.com/comparison-deep-learning-libraries-years-use/)\r\n\r\n**Deep Learning Part 1: Comparison of Symbolic Deep Learning Frameworks**\r\n\r\n- intro: Theano / TensorFlow / MXNET\r\n- blog: [http://blog.revolutionanalytics.com/2016/08/deep-learning-part-1.html](http://blog.revolutionanalytics.com/2016/08/deep-learning-part-1.html)\r\n\r\n**Deep Learning Frameworks Compared**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=MDP9FfsNx60](https://www.youtube.com/watch?v=MDP9FfsNx60)\r\n- github: [https://github.com/llSourcell/tensorflow_vs_theano](https://github.com/llSourcell/tensorflow_vs_theano)\r\n\r\n**DL4J vs. Torch vs. Theano vs. Caffe vs. TensorFlow**\r\n\r\n[https://deeplearning4j.org/compare-dl4j-torch7-pylearn.html](https://deeplearning4j.org/compare-dl4j-torch7-pylearn.html)\r\n\r\n**Deep Learning frameworks: a review before finishing 2016**\r\n\r\n[https://medium.com/@ricardo.guerrero/deep-learning-frameworks-a-review-before-finishing-2016-5b3ab4010b06#.a6fdrqssl](https://medium.com/@ricardo.guerrero/deep-learning-frameworks-a-review-before-finishing-2016-5b3ab4010b06#.a6fdrqssl)\r\n\r\n**The Anatomy of Deep Learning Frameworks**\r\n\r\n[https://medium.com/@gokul_uf/the-anatomy-of-deep-learning-frameworks-46e2a7af5e47](https://medium.com/@gokul_uf/the-anatomy-of-deep-learning-frameworks-46e2a7af5e47)\r\n\r\n**Python Deep Learning Frameworks Reviewed**\r\n\r\n[https://indico.io/blog/python-deep-learning-frameworks-reviewed/](https://indico.io/blog/python-deep-learning-frameworks-reviewed/)\r\n\r\n**Apple’s deep learning frameworks: BNNS vs. Metal CNN**\r\n\r\n[http://machinethink.net/blog/apple-deep-learning-bnns-versus-metal-cnn/](http://machinethink.net/blog/apple-deep-learning-bnns-versus-metal-cnn/)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","title":"Deep Learning Software and Hardware"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Deep Learning Software and Hardware\r\ndate: 2015-10-09\r\n---\r\n\r\n# Papers\r\n\r\n**Accelerating Deep Convolutional Neural Networks Using Specialized Hardware**\r\n\r\n- paper: [http://research.microsoft.com/pubs/240715/CNN%20Whitepaper.pdf](http://research.microsoft.com/pubs/240715/CNN%20Whitepaper.pdf)\r\n\r\n# Installation / Deploying\r\n\r\n**Setting up a Deep Learning Machine from Scratch (Software): Instructions for setting up the software on your deep learning machine**\r\n\r\n- intro: A detailed guide to setting up your machine for deep learning research. \r\nIncludes instructions to install drivers, tools and various deep learning frameworks. \r\nThis was tested on a 64 bit machine with Nvidia Titan X, running Ubuntu 14.04\r\n- github: [https://github.com/saiprashanths/dl-setup](https://github.com/saiprashanths/dl-setup)\r\n\r\n**How to install CUDA Toolkit and cuDNN for deep learning**\r\n\r\n- blog: [http://www.pyimagesearch.com/2016/07/04/how-to-install-cuda-toolkit-and-cudnn-for-deep-learning/](http://www.pyimagesearch.com/2016/07/04/how-to-install-cuda-toolkit-and-cudnn-for-deep-learning/)\r\n\r\n**Deploying Deep Learning: Guide to deploying deep-learning inference networks and realtime object detection with TensorRT and Jetson TX1.**\r\n\r\n- github: [https://github.com/dusty-nv/jetson-inference](https://github.com/dusty-nv/jetson-inference)\r\n\r\n**Install Log**\r\n\r\n- intro: setting up Caffe on a cluster running Redhat 6.3 (Santiago) without having root\r\n- github: [https://github.com/yosinski/caffe/blob/jason_public/doc/linux-no-root-install-log.md](https://github.com/yosinski/caffe/blob/jason_public/doc/linux-no-root-install-log.md)\r\n\r\n**Lessons Learned from Deploying Deep Learning at Scale**\r\n\r\n- blog: [http://blog.algorithmia.com/deploying-deep-learning-cloud-services/](http://blog.algorithmia.com/deploying-deep-learning-cloud-services/)\r\n\r\n## Docker\r\n\r\n**All-in-one Docker image for Deep Learning**\r\n\r\n- intro: An all-in-one Docker image for deep learning. \r\nContains all the popular DL frameworks (TensorFlow, Theano, Torch, Caffe, etc.)\r\n- github: [https://github.com/saiprashanths/dl-docker](https://github.com/saiprashanths/dl-docker)\r\n\r\n**NVIDIA Docker: GPU Server Application Deployment Made Easy**\r\n\r\n![](https://cloud.githubusercontent.com/assets/3028125/12213714/5b208976-b632-11e5-8406-38d379ec46aa.png)\r\n\r\n- blog: [https://devblogs.nvidia.com/parallelforall/nvidia-docker-gpu-server-application-deployment-made-easy/](https://devblogs.nvidia.com/parallelforall/nvidia-docker-gpu-server-application-deployment-made-easy/)\r\n- github: [https://github.com/NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker)\r\n\r\n**Deep learning base image for Docker (Tensorflow, Caffe, MXNet, Torch, Openface, etc.)**\r\n\r\n[https://github.com/dominiek/deep-base](https://github.com/dominiek/deep-base)\r\n\r\n**Deepo: a Docker image with a full reproducible deep learning research environment**\r\n\r\n- intro: A Docker image containing almost all popular deep learning frameworks: theano, tensorflow, sonnet, pytorch, keras, lasagne, mxnet, cntk, chainer, caffe, torch.\r\n- project page: [https://hub.docker.com/r/ufoym/deepo/](https://hub.docker.com/r/ufoym/deepo/)\r\n- github: [https://github.com//ufoym/deepo](https://github.com//ufoym/deepo)\r\n\r\n## Cloud\r\n\r\n**SuperVessel Cloud for POWER/OpenPOWER LoginRegisterTutorials**\r\n\r\n[http://www.ptopenlab.com/](http://www.ptopenlab.com/)\r\n\r\n**Building Deep Neural Networks in the Cloud with Azure GPU VMs, MXNet and Microsoft R Server**\r\n\r\n[https://blogs.technet.microsoft.com/machinelearning/2016/09/15/building-deep-neural-networks-in-the-cloud-with-azure-gpu-vms-mxnet-and-microsoft-r-server/](https://blogs.technet.microsoft.com/machinelearning/2016/09/15/building-deep-neural-networks-in-the-cloud-with-azure-gpu-vms-mxnet-and-microsoft-r-server/)\r\n\r\n**Microsoft open sources its next-gen cloud hardware design**\r\n\r\n- blog: [https://techcrunch.com/2016/10/31/microsoft-open-sources-its-next-gen-cloud-hardware-design/](https://techcrunch.com/2016/10/31/microsoft-open-sources-its-next-gen-cloud-hardware-design/)\r\n\r\n**Google Taps AMD For Accelerating Machine Learning In The Cloud**\r\n\r\n[http://www.forbes.com/sites/aarontilley/2016/11/15/google-taps-amd-for-accelerating-machine-learning-in-the-cloud/#3549d8554181](http://www.forbes.com/sites/aarontilley/2016/11/15/google-taps-amd-for-accelerating-machine-learning-in-the-cloud/#3549d8554181)\r\n\r\n## Amazon EC2\r\n\r\n**Deep Learning AMI on AWS Marketplace**\r\n\r\n[https://aws.amazon.com/marketplace/pp/B01M0AXXQB](https://aws.amazon.com/marketplace/pp/B01M0AXXQB)\r\n\r\n**We Have To Go Deeper: AWS p2.xlarge GPU optimized deep learning cluster-grenade**\r\n\r\n- github: [https://github.com/Miej/GoDeeper](https://github.com/Miej/GoDeeper)\r\n\r\n**A GPU enabled AMI for Deep Learning**\r\n\r\n- blog: [https://blog.empiricalci.com/a-gpu-enabled-ami-for-deep-learning-5aa3d694b630#.9339zxm4e](https://blog.empiricalci.com/a-gpu-enabled-ami-for-deep-learning-5aa3d694b630#.9339zxm4e)\r\n\r\n**Keras with GPU on Amazon EC2 – a step-by-step instruction**\r\n\r\n[https://medium.com/@mateuszsieniawski/keras-with-gpu-on-amazon-ec2-a-step-by-step-instruction-4f90364e49ac#.k27d0mqir](https://medium.com/@mateuszsieniawski/keras-with-gpu-on-amazon-ec2-a-step-by-step-instruction-4f90364e49ac#.k27d0mqir)\r\n\r\n## Microsoft R Server\r\n\r\n**Training Deep Neural Networks on ImageNet Using Microsoft R Server and Azure GPU VMs**\r\n\r\n- blog: [https://blogs.technet.microsoft.com/machinelearning/2016/11/15/imagenet-deep-neural-network-training-using-microsoft-r-server-and-azure-gpu-vms/](https://blogs.technet.microsoft.com/machinelearning/2016/11/15/imagenet-deep-neural-network-training-using-microsoft-r-server-and-azure-gpu-vms/)\r\n\r\n# Hardware System\r\n\r\n**I: Building a Deep Learning (Dream) Machine**\r\n\r\n- blog: [http://graphific.github.io/posts/building-a-deep-learning-dream-machine/](http://graphific.github.io/posts/building-a-deep-learning-dream-machine/)\r\n- slides: [http://www.slideshare.net/roelofp/building-a-deep-learning-dream-machine](http://www.slideshare.net/roelofp/building-a-deep-learning-dream-machine)\r\n\r\n**II: Running a Deep Learning (Dream) Machine**\r\n\r\n- blog: [http://graphific.github.io/posts/running-a-deep-learning-dream-machine/](http://graphific.github.io/posts/running-a-deep-learning-dream-machine/)\r\n\r\n**A Full Hardware Guide to Deep Learning**\r\n\r\n- blog: [http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/](http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/)\r\n\r\n**Build your own Deep Learning Box**\r\n\r\n![](https://annalyzin.files.wordpress.com/2016/05/deep-stack.png?w=276&h=264)\r\n\r\n- blog: [https://annalyzin.wordpress.com/2016/05/19/build-a-deep-learning-box/](https://annalyzin.wordpress.com/2016/05/19/build-a-deep-learning-box/)\r\n\r\n**32-TFLOP Deep Learning GPU Box: A super-fast linux-based machine with multiple GPUs for training deep neural nets**\r\n\r\n![](https://cdn.hackaday.io/images/9225171465012519301.JPG)\r\n\r\n[https://hackaday.io/project/12070-32-tflop-deep-learning-gpu-box](https://hackaday.io/project/12070-32-tflop-deep-learning-gpu-box)\r\n\r\n**Hands-on with the NVIDIA DIGITS DevBox for Deep Learning**\r\n\r\n![](http://www.pyimagesearch.com/wp-content/uploads/2016/06/handson_devbox_unboxed-768x576.jpg)\r\n\r\n- blog: [http://www.pyimagesearch.com/2016/06/06/hands-on-with-the-nvidia-digits-devbox-for-deep-learning/](http://www.pyimagesearch.com/2016/06/06/hands-on-with-the-nvidia-digits-devbox-for-deep-learning/)\r\n\r\n**Considerations when setting up deep learning hardware**\r\n\r\n![](http://www.pyimagesearch.com/wp-content/uploads/2016/06/dl_considerations_rack_05.jpg)\r\n\r\n- blog: [http://www.pyimagesearch.com/2016/06/13/considerations-when-setting-up-deep-learning-hardware/](http://www.pyimagesearch.com/2016/06/13/considerations-when-setting-up-deep-learning-hardware/)\r\n\r\n**Building a Workstation for Deep Learning**\r\n\r\n- slides: [http://www.slideshare.net/PetteriTeikariPhD/deep-learning-workstation](http://www.slideshare.net/PetteriTeikariPhD/deep-learning-workstation)\r\n\r\n**Deep Learning Machine: First build experience**\r\n\r\n- blog: [https://medium.com/@vivek.yadav/deep-learning-machine-first-build-experience-d04abf198831#.1d6q5mw9m](https://medium.com/@vivek.yadav/deep-learning-machine-first-build-experience-d04abf198831#.1d6q5mw9m)\r\n\r\n**Building a machine learning/deep learning workstation for under $5000**\r\n\r\n- blog: [https://www.analyticsvidhya.com/blog/2016/11/building-a-machine-learning-deep-learning-workstation-for-under-5000/](https://www.analyticsvidhya.com/blog/2016/11/building-a-machine-learning-deep-learning-workstation-for-under-5000/)\r\n\r\n**Hardware Guide: Neural Networks on GPUs (Updated 2016-1-30)**\r\n\r\n- intro: by Joseph Redmon\r\n- blog: [http://pjreddie.com/darknet/hardware-guide/](http://pjreddie.com/darknet/hardware-guide/)\r\n\r\n**Building Your Own Deep Learning Box**\r\n\r\n[https://medium.com/@bfortuner/building-your-own-deep-learning-box-47b918aea1eb#.4r5zchk4f](https://medium.com/@bfortuner/building-your-own-deep-learning-box-47b918aea1eb#.4r5zchk4f)\r\n\r\n**Setting up a Deep learning machine in a lazy yet quick way**\r\n[https://medium.com/@sravsatuluri/setting-up-a-deep-learning-machine-in-a-lazy-yet-quick-way-be2642318850#.jrxrkfxa2](https://medium.com/@sravsatuluri/setting-up-a-deep-learning-machine-in-a-lazy-yet-quick-way-be2642318850#.jrxrkfxa2)\r\n\r\n**Deep Confusion: Misadventures In Building A Deep Learning Machine**\r\n\r\n[http://www.topbots.com/deep-confusion-misadventures-in-building-a-machine-learning-server/](http://www.topbots.com/deep-confusion-misadventures-in-building-a-machine-learning-server/)\r\n\r\n**DIY-Deep-Learning-Workstation**\r\n\r\n- intro: Build a deep learning workstation from scratch (HW & SW). \r\n- github: [https://github.com/charlesq34/DIY-Deep-Learning-Workstation](https://github.com/charlesq34/DIY-Deep-Learning-Workstation)\r\n\r\n# GPU\r\n\r\n**Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning**\r\n\r\n- blog: [http://timdettmers.com/2017/04/09/which-gpu-for-deep-learning/](http://timdettmers.com/2017/04/09/which-gpu-for-deep-learning/)\r\n\r\n**从深度学习选择什么样的gpu来谈谈gpu的硬件架构**\r\n\r\n- blog: [http://chenrudan.github.io/blog/2015/12/20/introductionofgpuhardware.html](http://chenrudan.github.io/blog/2015/12/20/introductionofgpuhardware.html)\r\n\r\n**GPU折腾手记——2015 (by 李沐)**\r\n\r\n- blog: [http://mli.github.io/gpu/2016/01/17/build-gpu-clusters/](http://mli.github.io/gpu/2016/01/17/build-gpu-clusters/)\r\n\r\n**HPC, Deep Learning and GPUs(2016 Stanford HPC Conference)**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=JwgoC-1V_38](https://www.youtube.com/watch?v=JwgoC-1V_38)\r\n- video: [http://pan.baidu.com/s/1pKrSvOZ](http://pan.baidu.com/s/1pKrSvOZ)\r\n\r\n**Modern GPU 2.0: Design patterns for GPU computing**\r\n\r\n- intro: Modern GPU is code and commentary intended to promote new and productive ways of thinking about GPU computing.\r\n- homepage: [http://nvlabs.github.io/moderngpu/](http://nvlabs.github.io/moderngpu/)\r\n- github: [https://github.com/nvlabs/moderngpu](https://github.com/nvlabs/moderngpu)\r\n\r\n**CuMF: CUDA-Acclerated ALS on mulitple GPUs.**\r\n\r\n![](https://raw.githubusercontent.com/wei-tan/CuMF/master/images/mf.png)\r\n\r\n- github: [https://github.com/wei-tan/CuMF](https://github.com/wei-tan/CuMF)\r\n\r\n**Basic Performance Analysis of NVIDIA GPU Accelerator Cards for Deep Learning Applications**\r\n\r\n- wihte paper: [https://www.amax.com/enterprise/pdfs/Deep%20Learning%20Performance%20Analysis.pdf](https://www.amax.com/enterprise/pdfs/Deep%20Learning%20Performance%20Analysis.pdf)\r\n\r\n**CuPy : NumPy-like API accelerated with CUDA**\r\n\r\n- github: [https://github.com/pfnet/cupy](https://github.com/pfnet/cupy)\r\n\r\n**NumPy GPU acceleration**\r\n\r\n- blog: [http://scottsievert.com/blog/2016/07/01/numpy-gpu/](http://scottsievert.com/blog/2016/07/01/numpy-gpu/)\r\n\r\n**Efficient Convolutional Neural Network Inference on Mobile GPUs (Embedded Vision Summit)**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=ximyhmm17UM](https://www.youtube.com/watch?v=ximyhmm17UM)\r\n\r\n**Deep Learning with Multiple GPUs on Rescale: Torch**\r\n\r\n- blog: [https://blog.rescale.com/deep-learning-with-multiple-gpus-on-rescale-torch/](https://blog.rescale.com/deep-learning-with-multiple-gpus-on-rescale-torch/)\r\n\r\n**GPU-accelerated Theano & Keras on Windows 10 native**\r\n\r\n- arxiv: [https://github.com/philferriere/dlwin](https://github.com/philferriere/dlwin)\r\n\r\n**NVIDIA Announces Quadro GP100 - Big Pascal Comes to Workstations**\r\n\r\n[http://www.anandtech.com/show/11102/nvidia-announces-quadro-gp100](http://www.anandtech.com/show/11102/nvidia-announces-quadro-gp100)\r\n\r\n# FPGA\r\n\r\n**Recurrent Neural Networks Hardware Implementation on FPGA**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.05552](http://arxiv.org/abs/1511.05552)\r\n\r\n**Is implementing deep learning on FPGAs a natural next step after the success with GPUs?**\r\n\r\n- quora: [https://www.quora.com/Is-implementing-deep-learning-on-FPGAs-a-natural-next-step-after-the-success-with-GPUs](https://www.quora.com/Is-implementing-deep-learning-on-FPGAs-a-natural-next-step-after-the-success-with-GPUs)\r\n\r\n**Efficient Implementation of Neural Network Systems Built on FPGAs, Programmed with OpenCL**\r\n\r\n- paper: [https://www.altera.com/content/dam/altera-www/global/en_US/pdfs/literature/solution-sheets/efficient_neural_networks.pdf?utm_source=Altera&utm_medium=link&utm_campaign=OpenCL_15_1&utm_content=NA_efficient-neural-networks-solution-sheet-download-link](https://www.altera.com/content/dam/altera-www/global/en_US/pdfs/literature/solution-sheets/efficient_neural_networks.pdf?utm_source=Altera&utm_medium=link&utm_campaign=OpenCL_15_1&utm_content=NA_efficient-neural-networks-solution-sheet-download-link)\r\n\r\n**Deep Learning on FPGAs: Past, Present, and Future**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.04283](http://arxiv.org/abs/1602.04283)\r\n\r\n**FPGAs Challenge GPUs as a Platform for Deep Learning**\r\n\r\n- blog: [https://www.tractica.com/automation-robotics/fpgas-challenge-gpus-as-a-platform-for-deep-learning/](https://www.tractica.com/automation-robotics/fpgas-challenge-gpus-as-a-platform-for-deep-learning/)\r\n\r\n**Convolution Neural Network CNN Implementation on Altera FPGA using OpenCL**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=78Qd5t-Mn0s](https://www.youtube.com/watch?v=78Qd5t-Mn0s)\r\n\r\n**Accelerating Deep Learning Using Altera FPGAs (Embedded Vision Summit)**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=HlBC9qBqZRs](https://www.youtube.com/watch?v=HlBC9qBqZRs)\r\n- slides: [http://www.slideshare.net/embeddedvision/accelerating-deep-learning-using-altera-fpgas-a-presentation-from-intel](http://www.slideshare.net/embeddedvision/accelerating-deep-learning-using-altera-fpgas-a-presentation-from-intel)\r\n\r\n**Machine Learning on FPGAs: Neural Networks**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=3iCifD8gZ0Q](https://www.youtube.com/watch?v=3iCifD8gZ0Q)\r\n\r\n**Comprehensive Evaluation of OpenCL-based Convolutional Neural Network Accelerators in Xilinx and Altera FPGAs**\r\n\r\n- arxiv: [https://arxiv.org/abs/1609.09296](https://arxiv.org/abs/1609.09296)\r\n\r\n**Microsoft Goes All in for FPGAs to Build Out AI Cloud**\r\n\r\n- blog: [https://www.top500.org/news/microsoft-goes-all-in-for-fpgas-to-build-out-cloud-based-ai/](https://www.top500.org/news/microsoft-goes-all-in-for-fpgas-to-build-out-cloud-based-ai/)\r\n\r\n**Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1609.09671](https://arxiv.org/abs/1609.09671)\r\n- github: [https://github.com/dicecco1/fpga_caffe](https://github.com/dicecco1/fpga_caffe)\r\n\r\n**Intel Unveils FPGA to Accelerate Neural Networks**\r\n\r\n[http://datacenterfrontier.com/intel-unveils-fpga-to-accelerate-ai-neural-networks/](http://datacenterfrontier.com/intel-unveils-fpga-to-accelerate-ai-neural-networks/)\r\n\r\n**Deep Learning with FPGA**\r\n\r\n- blog: [https://amundtveit.com/2016/11/23/deep-learning-with-fpga/](https://amundtveit.com/2016/11/23/deep-learning-with-fpga/)\r\n\r\n**A General Neural Network Hardware Architecture on FPGA**\r\n\r\n- intro: University of Birmingham\r\n- arxiv: [https://arxiv.org/abs/1711.05860](https://arxiv.org/abs/1711.05860)\r\n\r\n**Approximate FPGA-based LSTMs under Computation Time Constraints**\r\n\r\n- intro: ARC 2018\r\n- arxiv: [https://arxiv.org/abs/1801.02190](https://arxiv.org/abs/1801.02190)\r\n\r\n# ARM / Processor\r\n\r\n**'Neural network' spotted deep inside Samsung's Galaxy S7 silicon brain: Secrets of Exynos M1 cores spilled**\r\n\r\n![](https://regmedia.co.uk/2016/08/22/samsung_m1_icache.jpg?x=648&y=364&infer_y=1)\r\n\r\n- blog: [http://www.theregister.co.uk/2016/08/22/samsung_m1_core/?mt=1471918256061](http://www.theregister.co.uk/2016/08/22/samsung_m1_core/?mt=1471918256061)\r\n\r\n**Intel will add deep-learning instructions to its processors**\r\n\r\n- blog: [http://lemire.me/blog/2016/10/14/intel-will-add-deep-learning-instructions-to-its-processors/](http://lemire.me/blog/2016/10/14/intel-will-add-deep-learning-instructions-to-its-processors/)\r\n\r\n# SRAM\r\n\r\n**ShiDianNao: Shifting Vision Processing Closer to the Sensor**\r\n[http://lap.epfl.ch/files/content/sites/lap/files/shared/publications/DuJun15_ShiDianNaoShiftingVisionProcessingCloserToTheSensor_ISCA15.pdf](http://lap.epfl.ch/files/content/sites/lap/files/shared/publications/DuJun15_ShiDianNaoShiftingVisionProcessingCloserToTheSensor_ISCA15.pdf)\r\n\r\n# Blogs\r\n\r\n**Emerging \"Universal\" FPGA, GPU Platform for Deep Learning**\r\n\r\n![](http://www.nextplatform.com/wp-content/uploads/2016/06/CNNLabProg.png)\r\n\r\n- blog: [http://www.nextplatform.com/2016/06/29/universal-fpga-gpu-platform-deep-learning/](http://www.nextplatform.com/2016/06/29/universal-fpga-gpu-platform-deep-learning/)\r\n\r\n**An Early Look at Startup Graphcore’s Deep Learning Chip**\r\n\r\n[https://www.nextplatform.com/2017/03/09/early-look-startup-graphcores-deep-learning-chip/](https://www.nextplatform.com/2017/03/09/early-look-startup-graphcores-deep-learning-chip/)\r\n\r\n**Hardware for Deep Learning**\r\n\r\n[https://medium.com/towards-data-science/hardware-for-deep-learning-8d9b03df41a](https://medium.com/towards-data-science/hardware-for-deep-learning-8d9b03df41a)\r\n\r\n# Videos\r\n\r\n**Energy-efficient Hardware for Embedded Vision and Deep Convolutional Neural Networks**\r\n\r\n- intro: September 2016 Embedded Vision Alliance Member Meeting Presentation: MIT\r\n- youtube: [https://www.youtube.com/watch?v=dO_lHz87DVM](https://www.youtube.com/watch?v=dO_lHz87DVM)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","title":"Deep Learning Tricks"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Deep Learning Tricks\r\ndate: 2015-10-09\r\n---\r\n\r\n# Papers\r\n\r\n**Practical recommendations for gradient-based training of deep architectures**\r\n\r\n- author: Yoshua Bengio\r\n- arxiv: [http://arxiv.org/abs/1206.5533](http://arxiv.org/abs/1206.5533)\r\n\r\n**Bag of Tricks for Image Classification with Convolutional Neural Networks**\r\n\r\n- intro: Amazon Web Services\r\n- arxiv: [https://arxiv.org/abs/1812.01187](https://arxiv.org/abs/1812.01187)\r\n\r\n# Blogs\r\n\r\n**Efficient BackProp**\r\n\r\n- intro: Neural Networks: Tricks of the Trade, 2nd\r\n- blog: [http://blog.csdn.net/zouxy09/article/details/45288129](http://blog.csdn.net/zouxy09/article/details/45288129)\r\n\r\n**Deep Learning for Vision: Tricks of the Trade**\r\n\r\n- intro: CVPR. Marc’Aurelio Ranzato\r\n- slides: [http://bavm2013.splashthat.com/img/events/46439/assets/34a7.ranzato.pdf](http://bavm2013.splashthat.com/img/events/46439/assets/34a7.ranzato.pdf)\r\n\r\n**Optimizing RNN performance**\r\n\r\n- intro: Silicon Valley AI Lab\r\n- keywords: Optimize GEMM, parallel GPU, GRU and LSTM...\r\n- blog: [http://svail.github.io/](http://svail.github.io/)\r\n\r\n**Must Know Tips/Tricks in Deep Neural Networks**\r\n\r\n- intro: by Xiu-Shen Wei, NJU LAMDA\r\n- blog: [http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)\r\n- slides: [http://lamda.nju.edu.cn/weixs/slide/CNNTricks_slide.pdf](http://lamda.nju.edu.cn/weixs/slide/CNNTricks_slide.pdf)\r\n\r\n**Training Tricks from Deeplearning4j**\r\n\r\n[http://deeplearning4j.org/trainingtricks.html](http://deeplearning4j.org/trainingtricks.html)\r\n\r\n**Suggestions for DL from Llya Sutskeve**\r\n\r\n- intro: data, preprocessing, mini-batch, gradient normalization, learning rate, weight initialization, data augmentation, dropout and ensemble\r\n- blog: [http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html](http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html)\r\n\r\n**Efficient Training Strategies for Deep Neural Network Language Models**\r\n\r\n- intro: batch-size, initial learning rate, network initialization\r\n- blog: [https://fb56552f-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearningworkshopnips2014/71.pdf?attachauth=ANoY7cp_eDwTXPm6iWHdBRhlIsgPASEAwkW-exLSOsz467mge7zLCkBMWznOu_G90vGVtqNvXOusc4z6cC6hEnHk6YzHtuEr_kyU0fyme7asaECN0zvoNwDk5258CueoB6fY3WtLvbJzYok1xiIeWSFYtk5mKXCXFDMI6djwhjCX1xi0GEEv_x7uMQwTdQlDItZ3kgLnZ2RjctQmIXDCu58fS3Wby4vWX3CkhMIf_EpCXx7jDn_M2SM%3D&attredirects=0](https://fb56552f-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearningworkshopnips2014/71.pdf?attachauth=ANoY7cp_eDwTXPm6iWHdBRhlIsgPASEAwkW-exLSOsz467mge7zLCkBMWznOu_G90vGVtqNvXOusc4z6cC6hEnHk6YzHtuEr_kyU0fyme7asaECN0zvoNwDk5258CueoB6fY3WtLvbJzYok1xiIeWSFYtk5mKXCXFDMI6djwhjCX1xi0GEEv_x7uMQwTdQlDItZ3kgLnZ2RjctQmIXDCu58fS3Wby4vWX3CkhMIf_EpCXx7jDn_M2SM%3D&attredirects=0)\r\n\r\n**Neural Networks Best Practice**\r\n\r\n- intro: Uber\r\n- paper: [http://www.kentran.net/2013/04/neural-network-best-practices.html](http://www.kentran.net/2013/04/neural-network-best-practices.html)\r\n\r\n**Dark Knowledge from Hinton**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=EK61htlw8hY](https://www.youtube.com/watch?v=EK61htlw8hY)\r\n- slides: [http://www.ttic.edu/dl/dark14.pdf](http://www.ttic.edu/dl/dark14.pdf)\r\n- notes: [http://deepdish.io/2014/10/28/hintons-dark-knowledge/](http://deepdish.io/2014/10/28/hintons-dark-knowledge/)\r\n- notes: [http://fastml.com/geoff-hintons-dark-knowledge/](http://fastml.com/geoff-hintons-dark-knowledge/)\r\n\r\n**Stochastic Gradient Descent Tricks(Leon Bottou)**\r\n\r\n[http://leon.bottou.org/publications/pdf/tricks-2012.pdf](http://leon.bottou.org/publications/pdf/tricks-2012.pdf)\r\n\r\n**Advice for applying Machine Learning**\r\n\r\n[https://jmetzen.github.io/2015-01-29/ml_advice.html](https://jmetzen.github.io/2015-01-29/ml_advice.html)\r\n\r\n**How to Debug Learning Algorithm for Regression Model**\r\n\r\n[http://vitalflux.com/machine-learning-debug-learning-algorithm-regression-model/](http://vitalflux.com/machine-learning-debug-learning-algorithm-regression-model/)\r\n\r\n**Large-scale L-BFGS using MapReduce**\r\n\r\n- intro: NIPS 2014\r\n- paper: [http://papers.nips.cc/paper/5333-large-scale-l-bfgs-using-mapreduce.pdf](http://papers.nips.cc/paper/5333-large-scale-l-bfgs-using-mapreduce.pdf)\r\n\r\n**Selecting good features**\r\n\r\n– Part I: univariate selection: [http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/](http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/)\r\n– Part II: linear models and regularization: [http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/](http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/)\r\n– Part III: random forests: [http://blog.datadive.net/selecting-good-features-part-iii-random-forests/](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\r\n– Part IV: stability selection, RFE and everything side by side: [http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/](http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/)\r\n\r\n**机器学习代码心得之有监督学习的模块**\r\n\r\n[http://www.weibo.com/p/1001603795687165852957](http://www.weibo.com/p/1001603795687165852957)\r\n\r\n**Stochastic Gradient Boosting: Choosing the Best Number of Iterations**\r\n\r\n- intro: Kaggle winner YANIR SEROUSSI\r\n- blog: [http://yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/](http://yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/)\r\n\r\n**Large-Scale High-Precision Topic Modeling on Twitter**\r\n\r\n- intro: Twitter senior researcher. KDD 2014\r\n- paper: [http://www.eeshyang.com/papers/KDD14Jubjub.pdf](http://www.eeshyang.com/papers/KDD14Jubjub.pdf)\r\n\r\n**H2O World - Top 10 Deep Learning Tips & Tricks - Arno Candel**\r\n\r\n[http://www.slideshare.net/0xdata/h2o-world-top-10-deep-learning-tips-tricks-arno-candel](http://www.slideshare.net/0xdata/h2o-world-top-10-deep-learning-tips-tricks-arno-candel)\r\n\r\n**How To Improve Deep Learning Performance: 20 Tips, Tricks and Techniques That You Can Use To Fight Overfitting and Get Better Generalization**\r\n\r\n[http://machinelearningmastery.com/improve-deep-learning-performance/](http://machinelearningmastery.com/improve-deep-learning-performance/)\r\n\r\n**Neural Network Training Speed Trick**\r\n\r\n[https://medium.com/machine-learning-at-petiteprogrammer/neural-network-training-speed-trick-92d6b22a7754#.4v6qukpn7](https://medium.com/machine-learning-at-petiteprogrammer/neural-network-training-speed-trick-92d6b22a7754#.4v6qukpn7)\r\n\r\n**The Black Magic of Deep Learning - Tips and Tricks for the practitioner**\r\n\r\n[http://nmarkou.blogspot.ru/2017/02/the-black-magic-of-deep-learning-tips.html](http://nmarkou.blogspot.ru/2017/02/the-black-magic-of-deep-learning-tips.html)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","title":"Deep Learning Tutorials"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Deep Learning Tutorials\r\ndate: 2015-10-09\r\n---\r\n\r\n# Tutorials\r\n\r\n![](/assets/cnn-materials/LeNet5.png)\r\n\r\n**Deep learning**\r\n\r\n- intro: From Wikipedia, the free encyclopedia\r\n- blog: [https://www.wikiwand.com/en/Deep_learning](https://www.wikiwand.com/en/Deep_learning)\r\n\r\n**Toward Theoretical Understanding of Deep Learning**\r\n\r\n- intro: ICML 2018 Tutorial. by Sanjeev Arora, Princeton University\r\n- slides: [https://www.dropbox.com/s/qonozmne0x4x2r3/deepsurveyICML18final.pptx?dl=0](https://www.dropbox.com/s/qonozmne0x4x2r3/deepsurveyICML18final.pptx?dl=0)\r\n- mirror: [https://pan.baidu.com/s/1r_lz6rMoSIinvfovMFFbug](https://pan.baidu.com/s/1r_lz6rMoSIinvfovMFFbug)\r\n\r\n**VGG Convolutional Neural Networks Practical**\r\n\r\n![](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/images/cover.png)\r\n\r\n- homepage: [http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html)\r\n- github: [https://github.com/vedaldi/practical-cnn](https://github.com/vedaldi/practical-cnn)\r\n\r\n**Hacker's guide to Neural Networks**\r\n\r\n[http://karpathy.github.io/neuralnets/](http://karpathy.github.io/neuralnets/)\r\n\r\n**Deep Learning Tutorials**\r\n\r\n- website: [http://deeplearning.net/tutorial/](http://deeplearning.net/tutorial/)\r\n- code: [https://github.com/lisa-lab/DeepLearningTutorials](https://github.com/lisa-lab/DeepLearningTutorials)\r\n\r\n**Deep Learning in a Nutshell: Core Concepts**\r\n\r\n![](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/11/fig1.png)\r\n\r\n[http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/](http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)\r\n\r\n**Deep Learning in a Nutshell: History and Training**\r\n\r\n[http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/](http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/)\r\n\r\n**A Deep Learning Tutorial: From Perceptrons to Deep Networks**\r\n\r\n[http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)\r\n\r\n**Deep Neural Networks (with Python code)**\r\n\r\n- paper: [http://scholarbank.nus.edu.sg/bitstream/handle/10635/120564/DeepNeuralNetworks.pdf?sequence=1](http://scholarbank.nus.edu.sg/bitstream/handle/10635/120564/DeepNeuralNetworks.pdf?sequence=1)\r\n\r\n**Three Classes of Deep Learning Architectures and Their Applications: A Tutorial Survey**\r\n\r\n- paper: [http://research.microsoft.com/pubs/192937/Transactions-APSIPA.pdf](http://research.microsoft.com/pubs/192937/Transactions-APSIPA.pdf)\r\n\r\n**Stanford Unsupervised Feature Learning and Deep Learning Tutorial: UFLDL Tutorial**\r\n\r\n- homepage: [http://ufldl.stanford.edu/tutorial/](http://ufldl.stanford.edu/tutorial/)\r\n- programming exercises: [https://github.com/amaas/stanford_dl_ex](https://github.com/amaas/stanford_dl_ex)\r\n\r\n**The Unreasonable Effectiveness of Deep Learning (LeCun)**\r\n\r\n- slides: [http://www.ee.ucl.ac.uk/sahd2014/resources/LeCun.pdf](http://www.ee.ucl.ac.uk/sahd2014/resources/LeCun.pdf)\r\n\r\n**Deep learning from the bottom up**\r\n\r\n![](https://i.imgur.com/SOjew3N.png)\r\n\r\n- blog: [https://www.metacademy.org/roadmaps/rgrosse/deep_learning](https://www.metacademy.org/roadmaps/rgrosse/deep_learning)\r\n\r\n**Introduction to Deep Learning with Python (By Alec Radford. Theano)**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=S75EdAcXHKk&hd=1](https://www.youtube.com/watch?v=S75EdAcXHKk&hd=1)\r\n\r\n**New to deep learning? Here are 4 easy lessons from Google**\r\n\r\n- blog: [https://gigaom.com/2015/01/29/new-to-deep-learning-here-are-4-easy-lessons-from-google/](https://gigaom.com/2015/01/29/new-to-deep-learning-here-are-4-easy-lessons-from-google/)\r\n\r\n**Deep Learning 101**\r\n\r\n- blog: [http://markus.com/deep-learning-101/](http://markus.com/deep-learning-101/)\r\n\r\n**Neural Networks Demystified**\r\n\r\n- Part 1: Data and Architecture: [https://www.youtube.com/watch?v=bxe2T-V8XRs](https://www.youtube.com/watch?v=bxe2T-V8XRs)\r\n- Part 2: Forward Propagation: [https://www.youtube.com/watch?v=UJwK6jAStmg](https://www.youtube.com/watch?v=UJwK6jAStmg)\r\n- Part 3: Gradient Descent: [https://www.youtube.com/watch?v=5u0jaA3qAGk](https://www.youtube.com/watch?v=5u0jaA3qAGk)\r\n- Part 4: Backpropagation: [https://www.youtube.com/watch?v=GlcnxUlrtek](https://www.youtube.com/watch?v=GlcnxUlrtek)\r\n- Part 5: Numerical Gradient Checking: [https://www.youtube.com/watch?v=pHMzNW8Agq4](https://www.youtube.com/watch?v=pHMzNW8Agq4)\r\n- Part 6: Training: [https://www.youtube.com/watch?v=9KM9Td6RVgQ](https://www.youtube.com/watch?v=9KM9Td6RVgQ)\r\n- Part 7: Overfitting, Testing, and Regularization: [https://www.youtube.com/watch?v=S4ZUwgesjS8](https://www.youtube.com/watch?v=S4ZUwgesjS8)\r\n\r\n- all-pack: [http://pan.baidu.com/s/1dDq5oNB](http://pan.baidu.com/s/1dDq5oNB)\r\n\r\n**Deep Learning SIMPLIFIED**\r\n\r\n- playlist: [https://www.youtube.com/playlist?list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu](https://www.youtube.com/playlist?list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu)\r\n\r\n**A 'Brief' History of Neural Nets and Deep Learning**\r\n\r\n- part 1: [http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/)\r\n- part 2: [http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-2/](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-2/)\r\n- part 3: [http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-3/](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-3/)\r\n- part 4: [http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/](http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/)\r\n\r\n**Deep Neural Networks — An Overview**\r\n\r\n[https://medium.com/@asjad/deep-neural-networks-an-overview-480112b12a13#.i7apzmnso](https://medium.com/@asjad/deep-neural-networks-an-overview-480112b12a13#.i7apzmnso)\r\n\r\n**A Tutorial on Deep Neural Networks for Intelligent Systems**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.07249](http://arxiv.org/abs/1603.07249)\r\n\r\n**Deep Learning for Computer Vision – Introduction to Convolution Neural Networks**\r\n\r\n- blog: [http://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/](http://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/)\r\n\r\n**BI Lab Deep Learning Tutorial**\r\n\r\n- github: [https://github.com/bi-lab/deeplearning_tutorial](https://github.com/bi-lab/deeplearning_tutorial)\r\n\r\n**Deep Learning Tutorials**\r\n\r\n- github: [https://github.com/sjchoi86/Deep-Learning-101](https://github.com/sjchoi86/Deep-Learning-101)\r\n\r\n**Neural Network Architectures**\r\n\r\n- blog: [http://culurciello.github.io/tech/2016/06/04/nets.html](http://culurciello.github.io/tech/2016/06/04/nets.html)\r\n\r\n**A Practical Introduction to Deep Learning with Caffe and Python**\r\n\r\n- blog: [http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/](http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/)\r\n\r\n**Notes on Convolutional Neural Networks**\r\n\r\n- homepage: [http://cogprints.org/5869/](http://cogprints.org/5869/)\r\n- paper: [http://cogprints.org/5869/1/cnn_tutorial.pdf](http://cogprints.org/5869/1/cnn_tutorial.pdf)\r\n\r\n**Feed Forward and Backward Run in Deep Convolution Neural Network**\r\n\r\n- intro: 20th International Conference on Computer Vision and Image Processing\r\n- arxiv: [https://arxiv.org/abs/1711.03278](https://arxiv.org/abs/1711.03278)\r\n\r\n**Convolutional Networks**\r\n\r\n[http://deeplearning4j.org/convolutionalnets.html](http://deeplearning4j.org/convolutionalnets.html)\r\n\r\n**Exploring convolutional neural networks with DL4J**\r\n\r\n- blog: [http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/](http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/)\r\n\r\n**Understanding Convolutional Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.09081](http://arxiv.org/abs/1605.09081)\r\n\r\n**Laws, Sausages and ConvNets**\r\n\r\n![](http://www.trivialorwrong.com/assets/laws-sausages-and-convnets_files/convnet-example.png)\r\n\r\n- blog: [http://www.trivialorwrong.com/2016/06/01/laws-sausages-and-convnets.html](http://www.trivialorwrong.com/2016/06/01/laws-sausages-and-convnets.html)\r\n\r\n**Convolutional Neural Networks (CNNs): An Illustrated Explanation**\r\n\r\n![](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_5.png)\r\n\r\n- blog: [http://xrds.acm.org/blog/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/](http://xrds.acm.org/blog/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/)\r\n\r\n**intro_deep: Introduction tutorials to deep learning with Theano and OpenDeep**\r\n\r\n- slides: [https://docs.google.com/presentation/d/1cg9Tn2wWwqJmaSSDnlBDBEETD5SyV6TJSD8qiDJFgEM](https://docs.google.com/presentation/d/1cg9Tn2wWwqJmaSSDnlBDBEETD5SyV6TJSD8qiDJFgEM)\r\n- mirror: [http://pan.baidu.com/s/1hqIR0yC](http://pan.baidu.com/s/1hqIR0yC)\r\n- youtube: [https://www.youtube.com/watch?v=afUvcD3tEoQ](https://www.youtube.com/watch?v=afUvcD3tEoQ)\r\n- mirror: [http://pan.baidu.com/s/1qWHp7xa](http://pan.baidu.com/s/1qWHp7xa)\r\n- github: [https://github.com/mbeissinger/intro_deep](https://github.com/mbeissinger/intro_deep)\r\n\r\n**Deep Learning on Java by Breandan Considine**\r\n\r\n- video: [https://speakerdeck.com/breandan/deep-learning-on-java](https://speakerdeck.com/breandan/deep-learning-on-java)\r\n\r\n**Using Convolutional Neural Networks and TensorFlow for Image Classification (NYC TensorFlow meetup)**\r\n\r\n- blog: [http://blog.altoros.com/using-convolutional-neural-networks-and-tensorflow-for-image-classification-and-search.html](http://blog.altoros.com/using-convolutional-neural-networks-and-tensorflow-for-image-classification-and-search.html)\r\n\r\n**Neural networks with Theano and Lasagne**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=dtGhSE1PFh0](https://www.youtube.com/watch?v=dtGhSE1PFh0)\r\n- mirror: [http://pan.baidu.com/s/1kUl3PvL](http://pan.baidu.com/s/1kUl3PvL)\r\n- github: [https://github.com/ebenolson/pydata2015](https://github.com/ebenolson/pydata2015)\r\n\r\n**Introduction to Deep Learning**\r\n\r\n- github: [https://github.com/rouseguy/intro2deeplearning](https://github.com/rouseguy/intro2deeplearning)\r\n- slides: [https://speakerdeck.com/bargava/introduction-to-deep-learning](https://speakerdeck.com/bargava/introduction-to-deep-learning)\r\n\r\n**Introduction to Deep Learning for Image Recognition - SciPy US 2016**\r\n\r\n- github: [https://github.com/rouseguy/scipyUS2016_dl-image](https://github.com/rouseguy/scipyUS2016_dl-image)\r\n- slides: [https://speakerdeck.com/bargava/introduction-to-deep-learning-for-image-processing](https://speakerdeck.com/bargava/introduction-to-deep-learning-for-image-processing)\r\n\r\n**Deep learning tutorials (2nd ed.)**\r\n\r\n- github: [https://github.com/sjchoi86/dl_tutorials](https://github.com/sjchoi86/dl_tutorials)\r\n\r\n- - -\r\n\r\n**A Beginner's Guide To Understanding Convolutional Neural Networks**\r\n\r\n![](https://adeshpande3.github.io/assets/Cover.png)\r\n\r\n- part 1: [https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)\r\n\r\n**A Beginner's Guide To Understanding Convolutional Neural Networks Part 2**\r\n\r\n![](https://adeshpande3.github.io/assets/Cover2nd.png)\r\n\r\n- part 2: [https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)\r\n\r\n**The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)**\r\n\r\n![](https://adeshpande3.github.io/assets/Cover3rd.png)\r\n\r\n- part 3: [https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)\r\n\r\n- - -\r\n\r\n**Deep Learning Part 1: Comparison of Symbolic Deep Learning Frameworks**\r\n\r\n- blog: [http://blog.revolutionanalytics.com/2016/08/deep-learning-part-1.html](http://blog.revolutionanalytics.com/2016/08/deep-learning-part-1.html)\r\n\r\n**Deep Learning Part 2: Transfer Learning and Fine-tuning Deep Convolutional Neural Networks**\r\n\r\n- blog: [http://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html](http://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html)\r\n\r\n**Deep Learning Part 3: Combining Deep Convolutional Neural Network with Recurrent Neural Network**\r\n\r\n- blog: [http://blog.revolutionanalytics.com/2016/09/deep-learning-part-3.html](http://blog.revolutionanalytics.com/2016/09/deep-learning-part-3.html)\r\n\r\n- - -\r\n\r\n**Introduction to Deep Learning for Image Processing**\r\n\r\n- slides: [https://speakerdeck.com/bargava/introduction-to-deep-learning-for-image-processing](https://speakerdeck.com/bargava/introduction-to-deep-learning-for-image-processing)\r\n\r\n**The best explanation of Convolutional Neural Networks on the Internet!**\r\n\r\n- blog: [https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8](https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8)\r\n\r\n**The Evolution and Core Concepts of Deep Learning & Neural Networks**\r\n\r\n- blog: [https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/](https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/)\r\n\r\n**An Intuitive Explanation of Convolutional Neural Networks**\r\n\r\n![](https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif?w=748)\r\n\r\n- blog: [https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\r\n\r\n**How Convolutional Neural Networks Work**\r\n\r\n![](http://brohrer.github.io/images/cnn5.png)\r\n\r\n- blog: [http://brohrer.github.io/how_convolutional_neural_networks_work.html](http://brohrer.github.io/how_convolutional_neural_networks_work.html)\r\n\r\n**Preliminary Note on the Complexity of a Neural Network**\r\n\r\n- blog: [http://r2rt.com/preliminary-note-on-the-complexity-of-a-neural-network.html](http://r2rt.com/preliminary-note-on-the-complexity-of-a-neural-network.html)\r\n\r\n**Deep Learning Tutorial**\r\n\r\n- intro: Hung-yi Lee. 李宏毅\r\n- slides: [http://www.slideshare.net/tw_dsconf/ss-62245351?qid=c0f0f97a-6ca8-4df0-97e2-984452215ee7&v=&b=&from_search=1](http://www.slideshare.net/tw_dsconf/ss-62245351?qid=c0f0f97a-6ca8-4df0-97e2-984452215ee7&v=&b=&from_search=1)\r\n- mirror: [https://pan.baidu.com/s/1mhMhuFQ](https://pan.baidu.com/s/1mhMhuFQ)\r\n\r\n**Jupyter notebooks and code for Intro to DL talk at Genesys**\r\n\r\n- blog: [http://sujitpal.blogspot.com/2016/08/kerasjupyter-notebooks-for-my.html](http://sujitpal.blogspot.com/2016/08/kerasjupyter-notebooks-for-my.html)\r\n- github: [https://github.com/sujitpal/intro-dl-talk-code](https://github.com/sujitpal/intro-dl-talk-code)\r\n\r\n**Learn Deep Learning the Hard Way**\r\n\r\n![](https://cdn-images-1.medium.com/max/800/1*ZrPDlOUX8B0htGfnY_bzIw.png)\r\n\r\n- blog: [https://medium.com/artifacia/learn-deep-learning-the-hard-way-e5d844f9fbc1#.yitf25xg5](https://medium.com/artifacia/learn-deep-learning-the-hard-way-e5d844f9fbc1#.yitf25xg5)\r\n\r\n**A Complete Guide on Getting Started with Deep Learning in Python**\r\n\r\n- blog: [https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/](https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/)\r\n\r\n**Deep learning for complete beginners: Recognising handwritten digits**\r\n\r\n- blog: [http://online.cambridgecoding.com/notebooks/cca_admin/deep-learning-for-complete-beginners-recognising-handwritten-digits](http://online.cambridgecoding.com/notebooks/cca_admin/deep-learning-for-complete-beginners-recognising-handwritten-digits)\r\n\r\n**Deep learning for complete beginners: Using convolutional nets to recognise images**\r\n\r\n- blog: [http://online.cambridgecoding.com/notebooks/cca_admin/convolutional-neural-networks-with-keras](http://online.cambridgecoding.com/notebooks/cca_admin/convolutional-neural-networks-with-keras)\r\n\r\n**Deep learning for complete beginners: neural network fine-tuning techniques**\r\n\r\n- blog: [http://online.cambridgecoding.com/notebooks/cca_admin/neural-networks-tuning-techniques](http://online.cambridgecoding.com/notebooks/cca_admin/neural-networks-tuning-techniques)\r\n\r\n**How do Convolutional Neural Networks work?**\r\n\r\n![](http://brohrer.github.io/images/cnn14.png)\r\n\r\n[http://brohrer.github.io/how_convolutional_neural_networks_work.html](http://brohrer.github.io/how_convolutional_neural_networks_work.html)\r\n\r\n**Creating a Neural Network That Can Tell if a Name Is Male or Female, in JavaScript**\r\n\r\n- blog: [https://medium.com/@hathibel/creating-a-neural-network-that-can-tell-if-a-name-is-male-or-female-in-javascript-3061029be396#.89l3znc2l](https://medium.com/@hathibel/creating-a-neural-network-that-can-tell-if-a-name-is-male-or-female-in-javascript-3061029be396#.89l3znc2l)\r\n\r\n**Softmax Classifiers Explained**\r\n\r\n- blog: [http://www.pyimagesearch.com/2016/09/12/softmax-classifiers-explained/](http://www.pyimagesearch.com/2016/09/12/softmax-classifiers-explained/)\r\n\r\n**The Softmax function and its derivative**\r\n\r\n- blog: [http://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/](http://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\r\n\r\n**How an algorithm behind Deep Learning works**\r\n\r\n- video: [http://blog.revolutionanalytics.com/2016/09/how-the-algorithm-behind-deep-learning-works.html](http://blog.revolutionanalytics.com/2016/09/how-the-algorithm-behind-deep-learning-works.html)\r\n- slides: [https://github.com/brohrer/public-hosting/raw/master/How_CNNs_work.pdf](https://github.com/brohrer/public-hosting/raw/master/How_CNNs_work.pdf)\r\n- blog: [http://www.kdnuggets.com/2016/08/brohrer-convolutional-neural-networks-explanation.html](http://www.kdnuggets.com/2016/08/brohrer-convolutional-neural-networks-explanation.html)\r\n- mirror: [http://v.youku.com/v_show/id_XMTcyNTgwNDQyOA==.html](http://v.youku.com/v_show/id_XMTcyNTgwNDQyOA==.html)\r\n\r\n**The Neural Network Zoo**\r\n\r\n![](http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png)\r\n\r\n[http://www.asimovinstitute.org/neural-network-zoo/](http://www.asimovinstitute.org/neural-network-zoo/)\r\n\r\n**Recognising Beer with TensorFlow**\r\n\r\n- blog: [https://medium.com/@chrishawkins/recognising-beer-with-tensorflow-9dedfee3c3c0#.pn5gm3fgc](https://medium.com/@chrishawkins/recognising-beer-with-tensorflow-9dedfee3c3c0#.pn5gm3fgc)\r\n- gist: [https://gist.github.com/chrishawkins/177e37756c833768a21d446cc4921c6e](https://gist.github.com/chrishawkins/177e37756c833768a21d446cc4921c6e)\r\n\r\n**Deep learning architecture diagrams**\r\n\r\n- intro: LSTM diagrams\r\n- blog: [http://fastml.com/deep-learning-architecture-diagrams/](http://fastml.com/deep-learning-architecture-diagrams/)\r\n\r\n**Getting Started with Deep Learning and Python**\r\n\r\n- blog: [http://www.pyimagesearch.com/2014/09/22/getting-started-deep-learning-python](http://www.pyimagesearch.com/2014/09/22/getting-started-deep-learning-python)\r\n\r\n**Deep Learning Practicals**\r\n\r\n- intro: Video playlist of Torch Video Tutorials\r\n- youtube: [https://www.youtube.com/playlist?list=PLLHTzKZzVU9ebuL6DCclzI54MrPNFGqbW](https://www.youtube.com/playlist?list=PLLHTzKZzVU9ebuL6DCclzI54MrPNFGqbW)\r\n- mirror: [https://pan.baidu.com/s/1skMFGkt](https://pan.baidu.com/s/1skMFGkt)\r\n\r\n**A simple workflow for deep learning**\r\n\r\n- blog: [https://cartesianfaith.com/2016/09/29/a-simple-workflow-for-deep-learning/](https://cartesianfaith.com/2016/09/29/a-simple-workflow-for-deep-learning/)\r\n- github: [https://github.com/zatonovo/deep_learning_ex](https://github.com/zatonovo/deep_learning_ex)\r\n\r\n**A primer on universal function approximation with deep learning (in Torch and R)**\r\n\r\n- blog: [https://cartesianfaith.com/2016/09/23/a-primer-on-universal-function-approximation-with-deep-learning-in-torch-and-r/](https://cartesianfaith.com/2016/09/23/a-primer-on-universal-function-approximation-with-deep-learning-in-torch-and-r/)\r\n\r\n**An Introduction to Implementing Neural Networks using TensorFlow**\r\n\r\n[https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/](https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/)\r\n\r\n**A Gentle Introduction to Convolutional Neural Networks**\r\n\r\n- blog: [https://miguelgfierro.com/blog/2016/a-gentle-introduction-to-convolutional-neural-networks/](https://miguelgfierro.com/blog/2016/a-gentle-introduction-to-convolutional-neural-networks/)\r\n\r\n**Beginning Machine Learning with Keras and TensorFlow**\r\n\r\n- blog: [http://blog.thoughtram.io/machine-learning/2016/09/23/beginning-ml-with-keras-and-tensorflow.html](http://blog.thoughtram.io/machine-learning/2016/09/23/beginning-ml-with-keras-and-tensorflow.html)\r\n\r\n**Shortest Way to Deep Learning**\r\n\r\n- blog: [https://blog.asadmemon.com/shortest-way-to-deep-learning-41e704d65ef#.19iowho8t](https://blog.asadmemon.com/shortest-way-to-deep-learning-41e704d65ef#.19iowho8t)\r\n\r\n**Deep learning with Matlab**\r\n\r\n- intro: Covered topics of the presentation:\r\nMachine learning workflow, Extracting feaures from images (colours, edges, corners, etc.)\r\n- youtube: [https://www.youtube.com/watch?v=r4D3NxQ0Xhg](https://www.youtube.com/watch?v=r4D3NxQ0Xhg)\r\n\r\n**Convolutional neural networks for computer vision with Matlab**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=MwUic9FCBJI](https://www.youtube.com/watch?v=MwUic9FCBJI)\r\n\r\n**Neural Net Computing Explodes**\r\n\r\n- blog: [http://semiengineering.com/neural-net-computing-explodes/](http://semiengineering.com/neural-net-computing-explodes/)\r\n\r\n**Tutorial: Optimizing Neural Networks using Keras (with Image recognition case study)**\r\n\r\n[https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/](https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/)\r\n\r\n**15 Deep Learning Tutorials**\r\n\r\n- blog: [http://www.datasciencecentral.com/profiles/blogs/15-deep-learning-tutorials](http://www.datasciencecentral.com/profiles/blogs/15-deep-learning-tutorials)\r\n\r\n**Deep Learning Episode 1: Optimizing DeepMind's A3C on Torch**\r\n\r\n[http://www.allinea.com/blog/201607/deep-learning-episode-1-optimizing-deepminds-a3c-torch](http://www.allinea.com/blog/201607/deep-learning-episode-1-optimizing-deepminds-a3c-torch)\r\n\r\n**Deep Learning Episode 2: Scaling TensorFlow over multiple EC2 GPU nodes**\r\n\r\n[http://www.allinea.com/blog/201608/deep-learning-episode-2-scaling-tensorflow-over-multiple-ec2-gpu-nodes](http://www.allinea.com/blog/201608/deep-learning-episode-2-scaling-tensorflow-over-multiple-ec2-gpu-nodes)\r\n\r\n**Deep Learning Episode 3: Supercomputer vs Pong**\r\n\r\n[http://www.allinea.com/blog/201610/deep-learning-episode-3-supercomputer-vs-pong](http://www.allinea.com/blog/201610/deep-learning-episode-3-supercomputer-vs-pong)\r\n\r\n**Deep Learning Episode 4: Supercomputer vs Pong II**\r\n\r\n[http://www.allinea.com/blog/201610/deep-learning-episode-4-supercomputer-vs-pong-ii](http://www.allinea.com/blog/201610/deep-learning-episode-4-supercomputer-vs-pong-ii)\r\n\r\n**Nuts and Bolts of Applying Deep Learning — Summary**\r\n\r\n- blog: [https://medium.com/@aniketvartak/nuts-and-bolts-of-applying-deep-learning-summary-84b8a8e873d5#.y7xidlhb4](https://medium.com/@aniketvartak/nuts-and-bolts-of-applying-deep-learning-summary-84b8a8e873d5#.y7xidlhb4)\r\n\r\n**Intro to Deep Learning for Computer Vision**\r\n\r\n[http://chaosmail.github.io/deeplearning/2016/10/22/intro-to-deep-learning-for-computer-vision/](http://chaosmail.github.io/deeplearning/2016/10/22/intro-to-deep-learning-for-computer-vision/)\r\n\r\n**If I Can Learn to Play Atari, I Can Learn TensorFlow**\r\n\r\n- intro: Here is a summary of new deep learning libraries, tools, and updates to existing frameworks.\r\n- blog: [https://dzone.com/articles/deep-learning-resources](https://dzone.com/articles/deep-learning-resources)\r\n\r\n**TensorFlow workshop materials**\r\n\r\n- github: [https://github.com/amygdala/tensorflow-workshop](https://github.com/amygdala/tensorflow-workshop)\r\n\r\n**Some theorems on deep learning**\r\n\r\n- intro: Tomaso Poggio [MIT]\r\n- youtube: [https://www.youtube.com/watch?v=YVjvRvvVn4w](https://www.youtube.com/watch?v=YVjvRvvVn4w)\r\n- mirror: [https://pan.baidu.com/s/1o8o7LjW](https://pan.baidu.com/s/1o8o7LjW)\r\n\r\n**Pokemon, Colors, and Deep Learning**\r\n\r\n- blog: [https://juandes.com/pokemon-colors-and-deep-learning-95fb715be46](https://juandes.com/pokemon-colors-and-deep-learning-95fb715be46)\r\n- github: [https://github.com/juandes/PokemonTypesDeepLearning](https://github.com/juandes/PokemonTypesDeepLearning)\r\n\r\n**Why Deep Learning is Radically Different from Machine Learning**\r\n\r\n- blog: [https://medium.com/intuitionmachine/why-deep-learning-is-radically-different-from-machine-learning-945a4a65da4d#.yuqxqxpsz](https://medium.com/intuitionmachine/why-deep-learning-is-radically-different-from-machine-learning-945a4a65da4d#.yuqxqxpsz)\r\n\r\n**Deep Learning: The Unreasonable Effectiveness of Randomness**\r\n\r\n- blog: [https://medium.com/intuitionmachine/deep-learning-the-unreasonable-effectiveness-of-randomness-14d5aef13f87#.1n775yl4i](https://medium.com/intuitionmachine/deep-learning-the-unreasonable-effectiveness-of-randomness-14d5aef13f87#.1n775yl4i)\r\n\r\n**Deep Meta-Learning : Machines now Bootstrap Themselves**\r\n\r\n- blog: [https://medium.com/intuitionmachine/deep-learning-can-now-create-itself-92e7ff0d59a7#.ml0dy8m9a](https://medium.com/intuitionmachine/deep-learning-can-now-create-itself-92e7ff0d59a7#.ml0dy8m9a)\r\n\r\n**Are Deep Neural Networks Creative?**\r\n\r\n- blog: [http://www.kdnuggets.com/2016/05/deep-neural-networks-creative-deep-learning-art.html](http://www.kdnuggets.com/2016/05/deep-neural-networks-creative-deep-learning-art.html)\r\n\r\n**Are Deep Neural Networks Creative? v2**\r\n\r\n- blog: [http://approximatelycorrect.com/2016/11/11/are-deep-neural-networks-creative/](http://approximatelycorrect.com/2016/11/11/are-deep-neural-networks-creative/)\r\n\r\n**Develop/Train A Convolutional Neural Netwok For MNIST Dataset**\r\n\r\n- github: [https://github.com/mirjalil/DataScience/blob/master/notebooks/deeplearning/tensorflow_03_CNN.ipynb](https://github.com/mirjalil/DataScience/blob/master/notebooks/deeplearning/tensorflow_03_CNN.ipynb)\r\n\r\n**Rethinking Generalization in Deep Learning**\r\n\r\n[https://medium.com/intuitionmachine/rethinking-generalization-in-deep-learning-ec66ed684ace#.tcnsqik5w](https://medium.com/intuitionmachine/rethinking-generalization-in-deep-learning-ec66ed684ace#.tcnsqik5w)\r\n\r\n**The hard thing about deep learning**\r\n\r\n- blog: [https://www.oreilly.com/ideas/the-hard-thing-about-deep-learning](https://www.oreilly.com/ideas/the-hard-thing-about-deep-learning)\r\n\r\n**The hard thing about deep learning**\r\n\r\n- blog: [https://www.oreilly.com/ideas/the-hard-thing-about-deep-learning](https://www.oreilly.com/ideas/the-hard-thing-about-deep-learning)\r\n\r\n**Introduction to Autoencoders**\r\n\r\n- blog: [https://pgaleone.eu/neural-networks/2016/11/18/introduction-to-autoencoders/](https://pgaleone.eu/neural-networks/2016/11/18/introduction-to-autoencoders/)\r\n\r\n**Two Days to a Demo**\r\n\r\n- blog: [https://developer.nvidia.com/embedded/twodaystoademo](https://developer.nvidia.com/embedded/twodaystoademo)\r\n\r\n**Deep Learning Tutorials for 10 Weeks**\r\n\r\n- github: [https://github.com/sjchoi86/dl_tutorials_4th](https://github.com/sjchoi86/dl_tutorials_4th)\r\n\r\n**Deep Learning in Clojure With Cortex**\r\n\r\n- blog: [http://gigasquidsoftware.com/blog/2016/12/27/deep-learning-in-clojure-with-cortex/](http://gigasquidsoftware.com/blog/2016/12/27/deep-learning-in-clojure-with-cortex/)\r\n\r\n**A Guide to Deep Learning by YerevaNN**\r\n\r\n- blog: [http://yerevann.com/a-guide-to-deep-learning/](http://yerevann.com/a-guide-to-deep-learning/)\r\n\r\n**Learning to Learn, to Program, to Explore and to Seek Knowledge**\r\n\r\n- intro: Nando de Freitas, NIPS 2016\r\n- youtube: [https://www.youtube.com/watch?v=tPWGGwmgwG0](https://www.youtube.com/watch?v=tPWGGwmgwG0)\r\n- mirror: [https://pan.baidu.com/s/1b2VZsE](https://pan.baidu.com/s/1b2VZsE)\r\n\r\n**Have Fun with Machine Learning: A Guide for Beginners**\r\n\r\n- intro: An absolute beginner's guide to Machine Learning and Image Classification with Neural Networks\r\n- github: [https://github.com/humphd/have-fun-with-machine-learning](https://github.com/humphd/have-fun-with-machine-learning)\r\n\r\n**Deep Learning Cheat Sheet**\r\n\r\n- blog: [https://hackernoon.com/deep-learning-cheat-sheet-25421411e460#.e0jwzxpkm](https://hackernoon.com/deep-learning-cheat-sheet-25421411e460#.e0jwzxpkm)\r\n\r\n**How to train your Deep Neural Network**\r\n\r\n[http://rishy.github.io//ml/2017/01/05/how-to-train-your-dnn/](http://rishy.github.io//ml/2017/01/05/how-to-train-your-dnn/)\r\n\r\n**A deep learning traffic light detector using dlib and a few images from Google street view**\r\n\r\n- blog: [https://sagivtech.com/2016/11/10/post-2/](https://sagivtech.com/2016/11/10/post-2/)\r\n\r\n**Recognizing Traffic Lights With Deep Learning**\r\n\r\n- blog: [https://medium.com/@davidbrai/recognizing-traffic-lights-with-deep-learning-23dae23287cc#.k22tnf37a](https://medium.com/@davidbrai/recognizing-traffic-lights-with-deep-learning-23dae23287cc#.k22tnf37a)\r\n- github: [https://github.com/davidbrai/deep-learning-traffic-lights](https://github.com/davidbrai/deep-learning-traffic-lights)\r\n\r\n**Tutorials for deep learning**\r\n\r\n- github: [https://github.com/oduerr/dl_tutorial](https://github.com/oduerr/dl_tutorial)\r\n\r\n**The Holographic Principle: Why Deep Learning Works**\r\n\r\n[https://medium.com/intuitionmachine/the-holographic-principle-and-deep-learning-52c2d6da8d9](https://medium.com/intuitionmachine/the-holographic-principle-and-deep-learning-52c2d6da8d9)\r\n\r\n**Deep Neural Networks - A Brief History**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.05549](https://arxiv.org/abs/1701.05549)\r\n\r\n**Fundamental Deep Learning code in TFLearn, Keras, Theano and TensorFlow**\r\n\r\n- blog: [https://insights.untapt.com/fundamental-deep-learning-code-in-tflearn-keras-theano-and-tensorflow-66be10a03227#.hoaw8fp9p](https://insights.untapt.com/fundamental-deep-learning-code-in-tflearn-keras-theano-and-tensorflow-66be10a03227#.hoaw8fp9p)\r\n- slides: [https://static1.squarespace.com/static/5362fa11e4b035b5651b7f7e/t/588fb378cd0f687201a2e317/1485812622873/Jon_Krohn_NYHackR_Deep_Learning_2017_01_30.pdf](https://static1.squarespace.com/static/5362fa11e4b035b5651b7f7e/t/588fb378cd0f687201a2e317/1485812622873/Jon_Krohn_NYHackR_Deep_Learning_2017_01_30.pdf)\r\n\r\n**Deep Neural Network from scratch**\r\n\r\n[https://matrices.io/deep-neural-network-from-scratch/](https://matrices.io/deep-neural-network-from-scratch/)\r\n\r\n**Convolutional Neural Networks**\r\n\r\n[https://github.com/Alfredvc/cnn_workshop](https://github.com/Alfredvc/cnn_workshop)\r\n\r\n**Exploring Optimizers**\r\n\r\n[https://github.com//KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb](https://github.com//KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb)\r\n\r\n**A Gentle Introduction to Exploding Gradients in Neural Networks**\r\n\r\n[https://machinelearningmastery.com/exploding-gradients-in-neural-networks/](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)\r\n\r\n**Only Numpy: (Why I do Manual Back Propagation) Implementing Multi Channel/Layer Convolution Neural Network on Numpy with Interactive Code**\r\n\r\n[https://medium.com/swlh/only-numpy-why-i-do-manual-back-propagation-implementing-multi-channel-layer-convolution-neural-7d83242fcc24](https://medium.com/swlh/only-numpy-why-i-do-manual-back-propagation-implementing-multi-channel-layer-convolution-neural-7d83242fcc24)\r\n\r\n**92.45% on CIFAR-10 in Torch**\r\n\r\n- intro: Dropout after Convolution\r\n- blog: [http://torch.ch/blog/2015/07/30/cifar.html](http://torch.ch/blog/2015/07/30/cifar.html)\r\n\r\n# Convolution\r\n\r\n**Understanding Convolutions**\r\n\r\n- blog: [http://colah.github.io/posts/2014-07-Understanding-Convolutions/](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)\r\n\r\n**Note on the implementation of a convolutional neural networks**\r\n\r\n- intro: CS231n, Convolutional layer, Pooling layer, Forward pass, Backward pass\r\n- blog: [http://cthorey.github.io./backprop_conv/](http://cthorey.github.io./backprop_conv/)\r\n- github: [https://github.com/cthorey/CS231](https://github.com/cthorey/CS231)\r\n\r\n**Convolution in Caffe: a memo**\r\n\r\n- blog: [https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo](https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo)\r\n\r\n**我对卷积的理解**\r\n\r\n![](https://sfault-image.b0.upaiyun.com/151/531/1515312697-56f91edbf29d6_articlex)\r\n\r\n- blog: [http://mengqi92.github.io/2015/10/06/convolution/](http://mengqi92.github.io/2015/10/06/convolution/)\r\n- blog: [https://segmentfault.com/a/1190000004706582](https://segmentfault.com/a/1190000004706582)\r\n\r\n**An Analysis of Convolution for Inference**\r\n\r\n[http://www.slideshare.net/nervanasys/an-analysis-of-convolution-for-inference](http://www.slideshare.net/nervanasys/an-analysis-of-convolution-for-inference)\r\n\r\n**Understanding Convolution in Deep Learning**\r\n\r\n- blog: [http://timdettmers.com/2015/03/26/convolution-deep-learning/](http://timdettmers.com/2015/03/26/convolution-deep-learning/)\r\n\r\n**A guide to convolution arithmetic for deep learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.07285](http://arxiv.org/abs/1603.07285)\r\n- github: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)\r\n\r\n**Going beyond full utilization: The inside scoop on Nervana’s Winograd kernels**\r\n\r\n![](https://www.nervanasys.com/wp-content/uploads/2016/06/wino_2b_filter_transform.png)\r\n\r\n- blog: [https://www.nervanasys.com/winograd-2/](https://www.nervanasys.com/winograd-2/)\r\n\r\n**Playing with convolutions in TensorFlow: From a short introduction to convolution to a complete model**\r\n\r\n- blog: [http://mourafiq.com/2016/08/10/playing-with-convolutions-in-tensorflow.html](http://mourafiq.com/2016/08/10/playing-with-convolutions-in-tensorflow.html)\r\n- github: [https://github.com/mouradmourafiq/tensorflow-convolution-models](https://github.com/mouradmourafiq/tensorflow-convolution-models)\r\n\r\n**How convolutional neural networks see the world: An exploration of convnet filters with Keras**\r\n\r\n- blog: [https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\r\n\r\n**One by One [ 1 x 1 ] Convolution - counter-intuitively useful**\r\n\r\n[http://iamaaditya.github.io/2016/03/one-by-one-convolution/](http://iamaaditya.github.io/2016/03/one-by-one-convolution/)\r\n\r\n**Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize**\r\n\r\n- intro: Twitter\r\n- arxiv: [https://arxiv.org/abs/1707.02937](https://arxiv.org/abs/1707.02937)\r\n\r\n# Receptive Field\r\n\r\n**A guide to receptive field arithmetic for Convolutional Neural Networks**\r\n\r\n[https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807](https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)\r\n\r\n# Momentum\r\n\r\n**Why Momentum Really Works**\r\n\r\n- blog: [http://distill.pub/2017/momentum/](http://distill.pub/2017/momentum/)\r\n- github: [https://github.com/distillpub/post--momentum](https://github.com/distillpub/post--momentum)\r\n\r\n## maxDNN\r\n\r\n**maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs**\r\n\r\n- arxiv: [http://arxiv.org/abs/1501.06633](http://arxiv.org/abs/1501.06633)\r\n- github: [https://github.com/eBay/maxDNN](https://github.com/eBay/maxDNN)\r\n\r\n# GEMM (General Matrix Matrix Multiply)\r\n\r\n**Why GEMM is at the heart of deep learning**\r\n\r\n![](https://petewarden.files.wordpress.com/2015/04/im2col_corrected.png)\r\n\r\n- blog: [http://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/](http://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)\r\n\r\n**A full walk through of the SGEMM implementation**\r\n\r\n- github-wiki: [https://github.com/NervanaSystems/maxas/wiki/SGEMM](https://github.com/NervanaSystems/maxas/wiki/SGEMM)\r\n\r\n# Backpropagation\r\n\r\n**Learning representations by back-propagating errors**\r\n\r\n**Learning Internal Representations by Error Propagating**\r\n\r\n- author: David E. Rumelhart, Geoffrey E. Hinton & Ronald J. Williams. 1986\r\n- paper: [http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf](http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf)\r\n- mirror: [http://pan.baidu.com/s/1bo30gHp](http://pan.baidu.com/s/1bo30gHp)\r\n- mirror: [http://pan.baidu.com/s/1kVfJ4of](http://pan.baidu.com/s/1kVfJ4of)\r\n\r\n**Calculus on Computational Graphs: Backpropagation**\r\n\r\n- blog: [http://colah.github.io/posts/2015-08-Backprop/](http://colah.github.io/posts/2015-08-Backprop/)\r\n\r\n**Styles of Truncated Backpropagation**\r\n\r\n- blog: [http://r2rt.com/styles-of-truncated-backpropagation.html](http://r2rt.com/styles-of-truncated-backpropagation.html)\r\n\r\n**Is BackPropagation Necessary?**\r\n\r\n![](http://i2.wp.com/deliprao.com/wp-content/uploads/2016/08/Selection_010-1.png?w=617)\r\n\r\n- blog: [http://deliprao.com/archives/191](http://deliprao.com/archives/191)\r\n\r\n**Backpropagation In Convolutional LSTMs**\r\n\r\n[https://www.doc.ic.ac.uk/~ahanda/ConvLSTMs.pdf](https://www.doc.ic.ac.uk/~ahanda/ConvLSTMs.pdf)\r\n\r\n## Backward Pass on Conv Layer\r\n\r\n**Convolutional Neural Networks backpropagation: from intuition to derivation**\r\n\r\n![](https://grzegorzgwardys.files.wordpress.com/2016/01/convolution-mlp-mapping.png?w=640)\r\n\r\n- blog: [https://grzegorzgwardys.wordpress.com/2016/04/22/8/](https://grzegorzgwardys.wordpress.com/2016/04/22/8/)\r\n\r\n**Backpropagation In Convolutional Neural Networks**\r\n\r\n![](http://www.jefkine.com/assets/images/cnn.png)\r\n\r\n- blog: [http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/](http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/)\r\n\r\n**Why do we rotate weights when computing the gradients in a convolution layer of a convolution network?**\r\n\r\n[http://soumith.ch/ex/pages/2014/08/07/why-rotate-weights-convolution-gradient/](http://soumith.ch/ex/pages/2014/08/07/why-rotate-weights-convolution-gradient/)\r\n\r\n**Note on the implementation of a convolutional neural networks**\r\n\r\n[http://cthorey.github.io./backprop_conv/](http://cthorey.github.io./backprop_conv/)\r\n\r\n# Attention\r\n\r\n**Attention in a Convolutional Neural Net**\r\n\r\n![](http://www.danvatterott.com/images/BMM_CNN/visual_system_models.png)\r\n\r\n- blog: [http://www.danvatterott.com/blog/2016/09/20/attention-in-a-convolutional-neural-net/](http://www.danvatterott.com/blog/2016/09/20/attention-in-a-convolutional-neural-net/)\r\n\r\n**Attention-based Networks**\r\n\r\n- intro: M. Malinowski. Max Planck Institut Informatik\r\n- slides: [http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf](http://download.mpi-inf.mpg.de/d2/mmalinow-slides/attention_networks.pdf)\r\n\r\n**Attention in Neural Networks and How to Use It**\r\n\r\n[http://akosiorek.github.io/ml/2017/10/14/visual-attention.html](http://akosiorek.github.io/ml/2017/10/14/visual-attention.html)\r\n\r\n# Softmax\r\n\r\n**Hierarchical softmax and negative sampling: short notes worth telling**\r\n\r\n[https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08](https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08)\r\n\r\n# Caffe\r\n\r\n**DIY Deep Learning for Vision:  a Hands-On Tutorial with Caffe**\r\n\r\n- homepage: [http://tutorial.caffe.berkeleyvision.org/](http://tutorial.caffe.berkeleyvision.org/)\r\n- slides: [https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.gc2fcdcce7_216_0](https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.gc2fcdcce7_216_0)\r\n\r\n**Deep learning tutorial on Caffe technology : basic commands, Python and C++ code**\r\n\r\n[http://christopher5106.github.io/deep/learning/2015/09/04/Deep-learning-tutorial-on-Caffe-Technology.html](http://christopher5106.github.io/deep/learning/2015/09/04/Deep-learning-tutorial-on-Caffe-Technology.html)\r\n\r\n**Using Caffe with your own dataset**\r\n\r\n[https://medium.com/@alexrachnog/using-caffe-with-your-own-dataset-b0ade5d71233](https://medium.com/@alexrachnog/using-caffe-with-your-own-dataset-b0ade5d71233)\r\n\r\n**OpenCV 3.0.0-dev: Load Caffe framework models**\r\n\r\n[http://docs.opencv.org/master/d5/de7/tutorial_dnn_googlenet.html#gsc.tab=0](http://docs.opencv.org/master/d5/de7/tutorial_dnn_googlenet.html#gsc.tab=0)\r\n\r\n# Chainer\r\n\r\n**Chainer Info**\r\n\r\n[https://github.com/hidetomasuoka/chainer-info](https://github.com/hidetomasuoka/chainer-info)\r\n\r\n# Keras\r\n\r\n**Keras tutorial**\r\n\r\n- intro: Tutorial teaching the basics of Keras and some deep learning concepts\r\n- github: [https://github.com/jfsantos/keras-tutorial](https://github.com/jfsantos/keras-tutorial)\r\n\r\n**Keras Tutorial: The Ultimate Beginner's Guide to Deep Learning in Python**\r\n\r\n[https://elitedatascience.com/keras-tutorial-deep-learning-in-python](https://elitedatascience.com/keras-tutorial-deep-learning-in-python)\r\n\r\n**Deep Learning with Keras: Tutorial @ EuroScipy 2016**\r\n\r\n- github: [https://github.com/leriomaggio/deep-learning-keras-tensorflow](https://github.com/leriomaggio/deep-learning-keras-tensorflow)\r\n\r\n**Transfer Learning and Fine Tuning for Cross Domain Image Classification with Keras**\r\n\r\n- slides: [http://www.slideshare.net/sujitpal/transfer-learning-and-fine-tuning-for-cross-domain-image-classification-with-keras](http://www.slideshare.net/sujitpal/transfer-learning-and-fine-tuning-for-cross-domain-image-classification-with-keras)\r\n- mirror: [https://pan.baidu.com/s/1gfn1xuj](https://pan.baidu.com/s/1gfn1xuj)\r\n- github: [https://github.com/sujitpal/fttl-with-keras](https://github.com/sujitpal/fttl-with-keras)\r\n\r\n# MXNet\r\n\r\n**10 Deep Learning projects based on Apache MXNet**\r\n\r\n[https://medium.com/@julsimon/10-deep-learning-projects-based-on-apache-mxnet-8231109f3f64](https://medium.com/@julsimon/10-deep-learning-projects-based-on-apache-mxnet-8231109f3f64)\r\n\r\n**Awesome MXNet(Beta)**\r\n\r\n[https://github.com/chinakook/Awesome-MXNet](https://github.com/chinakook/Awesome-MXNet)\r\n\r\n## TVM\r\n\r\n**Optimize Deep Learning GPU Operators with TVM: A Depthwise Convolution Example**\r\n\r\n[http://tvmlang.org/2017/08/22/Optimize-Deep-Learning-GPU-Operators-with-TVM-A-Depthwise-Convolution-Example.html](http://tvmlang.org/2017/08/22/Optimize-Deep-Learning-GPU-Operators-with-TVM-A-Depthwise-Convolution-Example.html)\r\n\r\n# Theano\r\n\r\n**Theano Tutorial @ LTI, Carnegie Mellon University**\r\n\r\n- github: [https://github.com/k-kawakami/Theano_Tutorial/blob/master/Theano_Tutorial.ipynb](https://github.com/k-kawakami/Theano_Tutorial/blob/master/Theano_Tutorial.ipynb)\r\n\r\n**An Introduction to MXNet/Gluon**\r\n\r\n- intro: @李沐\r\n- github: [https://github.com/mli/cvpr17](https://github.com/mli/cvpr17)\r\n\r\n# TensorFlow\r\n\r\n**LearningTensorFlow.com: A beginners guide to a powerful framework.**\r\n\r\n- homepge: [http://learningtensorflow.com/index.html](http://learningtensorflow.com/index.html)\r\n\r\n**TensorFlow Examples: TensorFlow tutorials and code examples for beginners**\r\n\r\n- github: [https://github.com/aymericdamien/TensorFlow-Examples](https://github.com/aymericdamien/TensorFlow-Examples)\r\n\r\n**Awesome TensorFlow: A curated list of awesome TensorFlow experiments, libraries, and projects**\r\n\r\n- github: [https://github.com/jtoy/awesome-tensorflow/](https://github.com/jtoy/awesome-tensorflow/)\r\n\r\n**The Good, Bad, & Ugly of TensorFlow: A survey of six months rapid evolution (+ tips/hacks and code to fix the ugly stuff)**\r\n\r\n- blog: [https://indico.io/blog/the-good-bad-ugly-of-tensorflow/](https://indico.io/blog/the-good-bad-ugly-of-tensorflow/)\r\n\r\n**Tensorflow Tutorials using Jupyter Notebook**\r\n\r\n- github: [https://github.com/sjchoi86/Tensorflow-101](https://github.com/sjchoi86/Tensorflow-101)\r\n\r\n**TensorFlow Tutorial**\r\n\r\n- homepage: [http://terryum.io/ml_practice/2016/05/28/TFIntroSlides/](http://terryum.io/ml_practice/2016/05/28/TFIntroSlides/)\r\n- slides: [https://s3.amazonaws.com/www.terryum.io/images/TensorFlow_Intro_160529.pptx](https://s3.amazonaws.com/www.terryum.io/images/TensorFlow_Intro_160529.pptx)\r\n- mirror: [http://pan.baidu.com/s/1c5cICY](http://pan.baidu.com/s/1c5cICY)\r\n- github: [https://github.com/terryum/TensorFlow_Exercises](https://github.com/terryum/TensorFlow_Exercises)\r\n\r\n**FIRST CONTACT WITH TENSORFLOW**\r\n\r\n- ebook: [http://www.jorditorres.org/first-contact-with-tensorflow/](http://www.jorditorres.org/first-contact-with-tensorflow/)\r\n\r\n**Introduction to TensorFlow**\r\n\r\n- github: [https://github.com/nihit/TensorFlow101](https://github.com/nihit/TensorFlow101)\r\n\r\n**TensorFlow-Tutorials: Simple tutorials using Google's TensorFlow Framework**\r\n\r\n- github: [https://github.com/nlintz/TensorFlow-Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\r\n\r\n**Neural Network Toolbox on TensorFlow**\r\n\r\n- github: [https://github.com/ppwwyyxx/tensorpack](https://github.com/ppwwyyxx/tensorpack)\r\n\r\n**Awesome Tensorflow Implementations**\r\n\r\n- github: [https://github.com/TensorFlowKR/awesome_tensorflow_implementations](https://github.com/TensorFlowKR/awesome_tensorflow_implementations)\r\n\r\n**The Ultimate List of TensorFlow Resources: Books, Tutorials & More**\r\n\r\n- blog: [https://hackerlists.com/tensorflow-resources/](https://hackerlists.com/tensorflow-resources/)\r\n\r\n**Install TensorFlow: Slides and code from our TensorFlow Workshop**\r\n\r\n- github: [https://github.com/random-forests/tensorflow-workshop](https://github.com/random-forests/tensorflow-workshop)\r\n\r\n**A Tour of TensorFlow**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.01178](https://arxiv.org/abs/1610.01178)\r\n\r\n**TensorFlow Tutorials**\r\n\r\n- youtube: [https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)\r\n- github: [https://github.com/Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials)\r\n\r\n**Shapes and dynamic dimensions in TensorFlow**\r\n\r\n- blog: [https://blog.metaflow.fr/shapes-and-dynamic-dimensions-in-tensorflow-7b1fe79be363#.1293uf94t](https://blog.metaflow.fr/shapes-and-dynamic-dimensions-in-tensorflow-7b1fe79be363#.1293uf94t)\r\n\r\n**TensorFlow saving/restoring and mixing multiple models**\r\n\r\n[https://blog.metaflow.fr/tensorflow-saving-restoring-and-mixing-multiple-models-c4c94d5d7125#.242xy4d46](https://blog.metaflow.fr/tensorflow-saving-restoring-and-mixing-multiple-models-c4c94d5d7125#.242xy4d46)\r\n\r\n**Getting to Know TensorFlow**\r\n\r\n- blog: [https://hackernoon.com/machine-learning-with-tensorflow-8873fdee2b68#.90j2jz5av](https://hackernoon.com/machine-learning-with-tensorflow-8873fdee2b68#.90j2jz5av)\r\n\r\n**Image Classification and Segmentation with Tensorflow and TF-Slim**\r\n[http://warmspringwinds.github.io/tensorflow/tf-slim/2016/10/30/image-classification-and-segmentation-using-tensorflow-and-tf-slim/](http://warmspringwinds.github.io/tensorflow/tf-slim/2016/10/30/image-classification-and-segmentation-using-tensorflow-and-tf-slim/)\r\n\r\n**Not another MNIST tutorial with TensorFlow**\r\n\r\n![](https://d3tdunqjn7n0wj.cloudfront.net/720x480/9728631593_21fb6f5f41_k_crop-526131ac6f07dc1b2f316a30c992b758.jpg)\r\n\r\n- blog: [https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow](https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow)\r\n\r\n**Dive Into TensorFlow**\r\n\r\n- Part I: Getting Started with TensorFlow: [http://textminingonline.com/dive-into-tensorflow-part-i-getting-started-with-tensorflow](http://textminingonline.com/dive-into-tensorflow-part-i-getting-started-with-tensorflow)\r\n- Part II: Basic Concepts: [http://textminingonline.com/dive-into-tensorflow-part-ii-basic-concepts](http://textminingonline.com/dive-into-tensorflow-part-ii-basic-concepts)\r\n- Part III: GTX 1080+Ubuntu16.04+CUDA8.0+cuDNN5.0+TensorFlow: [http://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow](http://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow)\r\n- Part IV: Hello MNIST: [http://textminingonline.com/dive-into-tensorflow-part-iv-hello-mnist](http://textminingonline.com/dive-into-tensorflow-part-iv-hello-mnist)\r\n- Part V: Deep MNIST: [http://textminingonline.com/dive-into-tensorflow-part-v-deep-mnist](http://textminingonline.com/dive-into-tensorflow-part-v-deep-mnist)\r\n- Part VI: Beyond Deep Learning: [http://textminingonline.com/dive-into-tensorflow-part-vi-beyond-deep-learning](http://textminingonline.com/dive-into-tensorflow-part-vi-beyond-deep-learning)\r\n\r\n**TensorFlow Exercises - focusing on the comparison with NumPy.**\r\n\r\n- github: [https://github.com/Kyubyong/tensorflow-exercises](https://github.com/Kyubyong/tensorflow-exercises)\r\n\r\n**A Gentle Guide to Using Batch Normalization in Tensorflow**\r\n\r\n- blog: [http://ruishu.io/2016/12/27/batchnorm/](http://ruishu.io/2016/12/27/batchnorm/)\r\n\r\n**Using TensorFlow in Windows with a GPU**\r\n\r\n- blog: [http://www.heatonresearch.com/2017/01/01/tensorflow-windows-gpu.html](http://www.heatonresearch.com/2017/01/01/tensorflow-windows-gpu.html)\r\n\r\n**Tensorflow and deep learning - without a PhD**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=vq2nnJ4g6N0](https://www.youtube.com/watch?v=vq2nnJ4g6N0)\r\n- mirror: [https://pan.baidu.com/s/1o8HF9R8](https://pan.baidu.com/s/1o8HF9R8)\r\n- blog: [https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0)\r\n\r\n**4 Steps To Learn TensorFlow When You Already Know scikit-learn**\r\n[https://medium.com/@Zelros/4-steps-to-learn-tensorflow-when-you-already-know-scikit-learn-3cd0340456b5#.q206au7u9](https://medium.com/@Zelros/4-steps-to-learn-tensorflow-when-you-already-know-scikit-learn-3cd0340456b5#.q206au7u9)\r\n\r\n**Gentlest Introduction to Tensorflow**\r\n\r\n- part 1: [https://medium.com/all-of-us-are-belong-to-machines/the-gentlest-introduction-to-tensorflow-248dc871a224#.fxyclr1ui](https://medium.com/all-of-us-are-belong-to-machines/the-gentlest-introduction-to-tensorflow-248dc871a224#.fxyclr1ui)\r\n- part 2: [https://medium.com/all-of-us-are-belong-to-machines/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f#.vf7p9upg2](https://medium.com/all-of-us-are-belong-to-machines/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f#.vf7p9upg2)\r\n- part 3: [https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c#.bvjru1f88](https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c#.bvjru1f88)\r\n- part 4: [https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54#.seh1fbr24](https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54#.seh1fbr24)\r\n\r\n**learn code with tensorflow**\r\n\r\n- github: [https://github.com/burness/tensorflow-101](https://github.com/burness/tensorflow-101)\r\n\r\n**TensorFlow Machine Learning Cookbook**\r\n\r\n- book: [https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-cookbook](https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-cookbook)\r\n- github: [https://github.com/nfmcclure/tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook)\r\n\r\n**TensorFlow Image Recognition on a Raspberry Pi**\r\n\r\n[http://svds.com/tensorflow-image-recognition-raspberry-pi/](http://svds.com/tensorflow-image-recognition-raspberry-pi/)\r\n\r\n**TensorFlow For Machine Intelligence**\r\n\r\n- book: [https://bleedingedgepress.com/tensor-flow-for-machine-intelligence/](https://bleedingedgepress.com/tensor-flow-for-machine-intelligence/)\r\n- github: [https://github.com/backstopmedia/tensorflowbook](https://github.com/backstopmedia/tensorflowbook)\r\n\r\n**Installing TensorFlow on Raspberry Pi 3 (and probably 2 as well)**\r\n\r\n- intro: TensorFlow for Raspberry Pi\r\n- github: [https://github.com/samjabrahams/tensorflow-on-raspberry-pi](https://github.com/samjabrahams/tensorflow-on-raspberry-pi)\r\n\r\n**CodinGame: Deep Learning - TensorFlow**\r\n\r\n- homepage: [https://www.codingame.com/games/machine-learning?utm_source=CodinGame&utm_medium=Email&utm_campaign=tensorflow](https://www.codingame.com/games/machine-learning?utm_source=CodinGame&utm_medium=Email&utm_campaign=tensorflow)\r\n\r\n**A Practical Guide for Debugging Tensorflow Codes**\r\n\r\n**Debugging Tips on TensorFlow**\r\n\r\n- slides: [https://wookayin.github.io/TensorflowKR-2016-talk-debugging](https://wookayin.github.io/TensorflowKR-2016-talk-debugging)\r\n- github: [https://github.com/wookayin/TensorflowKR-2016-talk-debugging](https://github.com/wookayin/TensorflowKR-2016-talk-debugging)\r\n\r\n**Tensorflow Projects: Deep learning using tensorflow**\r\n\r\n- intro: A repo of everything deep and neurally related. \r\nImplementations and ideas are largely based on papers from arxiv and implementations, tutorials from the internet.\r\n- github: [https://github.com/shekkizh/TensorflowProjects](https://github.com/shekkizh/TensorflowProjects)\r\n\r\n**Machine Learning with TensorFlow**\r\n\r\n- homepage: [http://www.tensorflowbook.com/](http://www.tensorflowbook.com/)\r\n- github: [https://github.com/BinRoot/TensorFlow-Book](https://github.com/BinRoot/TensorFlow-Book)\r\n- blog: [https://www.manning.com/books/machine-learning-with-tensorflow](https://www.manning.com/books/machine-learning-with-tensorflow)\r\n\r\n**Convolutional Networks: from TensorFlow to iOS BNNS**\r\n\r\n- blog: [https://paiv.github.io/blog/2016/09/25/tensorflow-to-bnns.html](https://paiv.github.io/blog/2016/09/25/tensorflow-to-bnns.html)\r\n- github: [https://github.com/paiv/mnist-bnns](https://github.com/paiv/mnist-bnns)\r\n\r\n**Android TensorFlow Machine Learning Example**\r\n\r\n- blog: [https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc#.ysg0ss9r2](https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc#.ysg0ss9r2)\r\n- github: [https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample](https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample)\r\n\r\n**TensorFlow and Deep Learning Tutorials**\r\n\r\n[https://github.com/wagamamaz/tensorflow-tutorial](https://github.com/wagamamaz/tensorflow-tutorial)\r\n\r\n**Finetuning AlexNet with TensorFlow**\r\n\r\n- blog: [https://kratzert.github.io/kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html](https://kratzert.github.io/kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html)\r\n- github: [https://github.com/kratzert/finetune_alexnet_with_tensorflow](https://github.com/kratzert/finetune_alexnet_with_tensorflow)\r\n\r\n**Deep Learning examples using Tensorflow**\r\n\r\n[https://github.com/aditya101993/Deep-Learning](https://github.com/aditya101993/Deep-Learning)\r\n\r\n**How To Write Your Own Tensorflow in C++**\r\n\r\n[https://oneraynyday.github.io/ml/2017/10/20/Tensorflow-C++/](https://oneraynyday.github.io/ml/2017/10/20/Tensorflow-C++/)\r\n\r\n## Tensorflow on Android\r\n\r\n**A Guide to Running Tensorflow Models on Android**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=kFWKdLOxykE](https://www.youtube.com/watch?v=kFWKdLOxykE)\r\n- mirror: [http://www.bilibili.com/video/av9806881/index_10.html](http://www.bilibili.com/video/av9806881/index_10.html)\r\n- github: [https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android](https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android)\r\n\r\n**TensorFlow Android stand-alone demo**\r\n\r\n- intro: Android demo source files extracted from original TensorFlow source. (TensorFlow r0.10)\r\n- github: [https://github.com/miyosuda/TensorFlowAndroidDemo](https://github.com/miyosuda/TensorFlowAndroidDemo)\r\n\r\n# Torch\r\n\r\n**Torch Developer Guide**\r\n\r\n- github: [https://github.com/Atcold/Torch-Developer-Guide](https://github.com/Atcold/Torch-Developer-Guide)\r\n\r\n# PyTorch\r\n\r\n**Practical PyTorch tutorials**\r\n\r\n- github: [https://github.com/spro/practical-pytorch](https://github.com/spro/practical-pytorch)\r\n\r\n**The Incredible PyTorch**\r\n\r\n- github: [https://github.com/ritchieng/the-incredible-pytorch](https://github.com/ritchieng/the-incredible-pytorch)\r\n\r\n**PyTorch quick start: Classifying an image**\r\n\r\n- blog: [http://blog.outcome.io/pytorch-quick-start-classifying-an-image/](http://blog.outcome.io/pytorch-quick-start-classifying-an-image/)\r\n- ipn: [https://gist.github.com/jbencook/9918217f866c1aa9967391ba62d123b5](https://gist.github.com/jbencook/9918217f866c1aa9967391ba62d123b5)\r\n\r\n**tutorial for researchers to learn deep learning with pytorch.**\r\n\r\n[https://github.com/yunjey/pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial)\r\n\r\n# Building a System for Fun!\r\n\r\n**Facial Recognition On A Jetson TX1 In Tensorflow**\r\n\r\n- blog: [http://www.mattkrzus.com/face.html](http://www.mattkrzus.com/face.html)\r\n\r\n**Build an AI Cat Chaser with Jetson TX1 and Caffe**\r\n\r\n![](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/07/cat1-2.jpeg)\r\n\r\n- blog: [https://devblogs.nvidia.com/parallelforall/ai-cat-chaser-jetson-tx1-caffe/](https://devblogs.nvidia.com/parallelforall/ai-cat-chaser-jetson-tx1-caffe/)\r\n\r\n**Deep Learning in Aerial Systems Using Jetson**\r\n\r\n- blog: [https://devblogs.nvidia.com/parallelforall/deep-learning-in-aerial-systems-jetson/](https://devblogs.nvidia.com/parallelforall/deep-learning-in-aerial-systems-jetson/)\r\n- github: [https://github.com/amitibo/auvsi-targets](https://github.com/amitibo/auvsi-targets)\r\n\r\n**Cherry Autonomous Racecar (CAR): NCAT ECE Senior Design Project**\r\n\r\n- intro: Implementation of the CNN from End to End Learning for Self-Driving Cars on a Nvidia Jetson TX1 using Tensorflow and ROS\r\n- github: [https://github.com/DJTobias/Cherry-Autonomous-Racecar](https://github.com/DJTobias/Cherry-Autonomous-Racecar)\r\n\r\n## Traffic Signs Classification\r\n\r\n**Traffic signs classification with Deep Learning.**\r\n\r\n- blog: [https://hackernoon.com/traffic-signs-classification-with-deep-learning-b0cb03e23efb#.n0fjehwo6](https://hackernoon.com/traffic-signs-classification-with-deep-learning-b0cb03e23efb#.n0fjehwo6)\r\n- github: [https://github.com/MehdiSv/TrafficSignsRecognition/](https://github.com/MehdiSv/TrafficSignsRecognition/)\r\n\r\n**Traffic Sign Recognition with TensorFlow**\r\n\r\n- blog: [https://medium.com/@waleedka/traffic-sign-recognition-with-tensorflow-629dffc391a6#.2khbv6a9a](https://medium.com/@waleedka/traffic-sign-recognition-with-tensorflow-629dffc391a6#.2khbv6a9a)\r\n\r\n**Traffic signs classification with a convolutional network**\r\n\r\n[http://navoshta.com/traffic-signs-classification/](http://navoshta.com/traffic-signs-classification/)\r\n\r\n**Convolutional Neural Network for Traffic Sign Classification — CarND**\r\n\r\n- blog: [https://medium.com/@gruby/convolutional-neural-network-for-traffic-sign-classification-carnd-e46e95453899#.lfutfs21v](https://medium.com/@gruby/convolutional-neural-network-for-traffic-sign-classification-carnd-e46e95453899#.lfutfs21v)\r\n\r\n# Talks\r\n\r\n**A Tour of Deep Learning With C++**\r\n\r\n- intro: CppCon 2017, Peter Goldsborough\r\n- youtube: [https://www.youtube.com/watch?v=9-1lcss0NMg](https://www.youtube.com/watch?v=9-1lcss0NMg)\r\n- bilibili: [https://www.bilibili.com/video/av20675156/](https://www.bilibili.com/video/av20675156/)\r\n\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","title":"Deep Learning with Machine Learning"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Deep Learning with Machine Learning\ndate: 2015-10-09\n---\n\n# Bayesian\n\n**Scalable Bayesian Optimization Using Deep Neural Networks**\n\n- intro: ICML 2015\n- paper: [http://jmlr.org/proceedings/papers/v37/snoek15.html](http://jmlr.org/proceedings/papers/v37/snoek15.html)\n- arxiv: [http://arxiv.org/abs/1502.05700](http://arxiv.org/abs/1502.05700)\n- github: [https://github.com/bshahr/torch-dngo](https://github.com/bshahr/torch-dngo)\n\n**Bayesian Dark Knowledge**\n\n- paper: [http://arxiv.org/abs/1506.04416](http://arxiv.org/abs/1506.04416)\n- notes: [Notes on Bayesian Dark Knowledge](https://www.evernote.com/shard/s189/sh/92cc4cbf-285e-4038-af08-c6d9e4aee6ea/d505237e82dc81be9859bc82f3902f9f)\n\n**Memory-based Bayesian Reasoning with Deep Learning**\n\n- intro: Google DeepMind\n- slides: [http://blog.shakirm.com/wp-content/uploads/2015/11/CSML_BayesDeep.pdf](http://blog.shakirm.com/wp-content/uploads/2015/11/CSML_BayesDeep.pdf)\n\n**Towards Bayesian Deep Learning: A Survey**\n\n- arxiv: [http://arxiv.org/abs/1604.01662](http://arxiv.org/abs/1604.01662)\n\n**Towards Bayesian Deep Learning: A Framework and Some Existing Methods**\n\n- intro: IEEE Transactions on Knowledge and Data Engineering (TKDE), 2016\n- arxiv: [http://arxiv.org/abs/1608.06884](http://arxiv.org/abs/1608.06884)\n\n**Bayesian Deep Learning: Neural Networks in PyMC3 estimated with Variational Inference**\n\n- blog: [http://blog.quantopian.com/bayesian-deep-learning/](http://blog.quantopian.com/bayesian-deep-learning/)\n\n**Bayesian Deep Learning Part II: Bridging PyMC3 and Lasagne to build a Hierarchical Neural Network**\n\n- blog: [http://twiecki.github.io/blog/2016/07/05/bayesian-deep-learning/](http://twiecki.github.io/blog/2016/07/05/bayesian-deep-learning/)\n\n**Deep Bayesian Active Learning with Image Data**\n\n- project page: [http://mlg.eng.cam.ac.uk/yarin/publications.html#Gal2016Active](http://mlg.eng.cam.ac.uk/yarin/publications.html#Gal2016Active)\n- arxiv: [https://arxiv.org/abs/1703.02910](https://arxiv.org/abs/1703.02910)\n\n**Deep Learning: A Bayesian Perspective**\n\n- intro: University of Chicago & George Mason University\n- arxiv: [https://arxiv.org/abs/1706.00473](https://arxiv.org/abs/1706.00473)\n- paper: [https://projecteuclid.org/euclid.ba/1510801992](https://projecteuclid.org/euclid.ba/1510801992)\n\n**On Bayesian Deep Learning and Deep Bayesian Learning**\n\n- intro: University of Oxford & DeepMind\n- lecture: [http://csml.stats.ox.ac.uk/news/2017-12-08-ywteh-breiman-lecture/](http://csml.stats.ox.ac.uk/news/2017-12-08-ywteh-breiman-lecture/)\n- video: [https://www.facebook.com/nipsfoundation/videos/1555493854541848/](https://www.facebook.com/nipsfoundation/videos/1555493854541848/)\n- mirror: [https://www.bilibili.com/video/av17121345/](https://www.bilibili.com/video/av17121345/)\n\n**Bayesian Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1801.07710](https://arxiv.org/abs/1801.07710)\n- github: [https://github.com/mullachv/MLExp](https://github.com/mullachv/MLExp)\n\n**Bayesian Convolutional Neural Networks**\n\n- intro: NeuralSpace\n- arxiv: [https://arxiv.org/abs/1806.05978](https://arxiv.org/abs/1806.05978)\n\n**Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam**\n\n- intro: ICML 2018\n- intro: RIKEN Center for Advanced Intelligence project & University of British Columbia & University of Oxford\n- arxiv: [https://arxiv.org/abs/1806.04854](https://arxiv.org/abs/1806.04854)\n- github: [https://github.com/emtiyaz/vadam](https://github.com/emtiyaz/vadam)\n\n# Bag of Words (BoW)\n\n**Deep Learning Transcends the Bag of Words**\n\n- blog: [http://www.kdnuggets.com/2015/12/deep-learning-outgrows-bag-words-recurrent-neural-networks.html](http://www.kdnuggets.com/2015/12/deep-learning-outgrows-bag-words-recurrent-neural-networks.html)\n\n**E2BoWs: An End-to-End Bag-of-Words Model via Deep Convolutional Neural Network**\n\n- intro: ChinaMM 2017, image retrieval\n- arxiv: [https://arxiv.org/abs/1709.05903](https://arxiv.org/abs/1709.05903)\n\n# Boosting\n\n**Deep Boosting**\n\n- intro: ICML 2014\n- paper: [http://www.cs.princeton.edu/~usyed/CortesMohriSyedICML2014.pdf](http://www.cs.princeton.edu/~usyed/CortesMohriSyedICML2014.pdf)\n- github: [https://github.com/google/deepboost](https://github.com/google/deepboost)\n\n**Deep Incremental Boosting**\n\n[https://arxiv.org/abs/1708.03704](https://arxiv.org/abs/1708.03704)\n\n# Bootstrap\n\n**Training Deep Neural Networks on Noisy Labels with Bootstrapping**\n\n- arxiv: [http://arxiv.org/abs/1412.6596](http://arxiv.org/abs/1412.6596)\n\n**ConvCSNet: A Convolutional Compressive Sensing Framework Based on Deep Learning**\n\n# Compressive Sensing\n\n**ConvCSNet: A Convolutional Compressive Sensing Framework Based on Deep Learning**\n\n[https://arxiv.org/abs/1801.10342](https://arxiv.org/abs/1801.10342)\n\n**Perceptual Compressive Sensing**\n\n[https://arxiv.org/abs/1802.00176](https://arxiv.org/abs/1802.00176)\n\n**Full Image Recover for Block-Based Compressive Sensing**\n\n[https://arxiv.org/abs/1802.00179](https://arxiv.org/abs/1802.00179)\n\n# Conditional Random Fields\n\n**Deep Markov Random Field for Image Modeling**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1609.02036](http://arxiv.org/abs/1609.02036)\n- github: [https://github.com/zhirongw/deep-mrf](https://github.com/zhirongw/deep-mrf)\n\n**Deep, Dense, and Low-Rank Gaussian Conditional Random Fields**\n\n- arxiv: [https://arxiv.org/abs/1611.09051](https://arxiv.org/abs/1611.09051)\n\n**DeepCRF: Neural Networks and CRFs for Sequence Labeling**\n\n- intro: A implementation of Conditional Random Fields (CRFs) with Deep Learning Method\n- github: [https://github.com/aonotas/deep-crf](https://github.com/aonotas/deep-crf)\n\n# Decision Tree\n\n**Deep Neural Decision Forests**\n\n- intro: ICCV 2015. Microsoft Research. ICCV'15 Marr Prize\n- paper: [http://research.microsoft.com/pubs/255952/ICCV15_DeepNDF_main.pdf](http://research.microsoft.com/pubs/255952/ICCV15_DeepNDF_main.pdf)\n- slides: [https://docs.google.com/presentation/d/1Ze7BAiWbMPyF0ax36D-aK00VfaGMGvvgD_XuANQW1gU/edit#slide=id.p](https://docs.google.com/presentation/d/1Ze7BAiWbMPyF0ax36D-aK00VfaGMGvvgD_XuANQW1gU/edit#slide=id.p)\n- github: [https://github.com/chrischoy/fully-differentiable-deep-ndf-tf](https://github.com/chrischoy/fully-differentiable-deep-ndf-tf)\n- supplement: [http://research.microsoft.com/pubs/255952/ICCV15_DeepNDF_suppl.pdf](http://research.microsoft.com/pubs/255952/ICCV15_DeepNDF_suppl.pdf)\n- notes: [http://pan.baidu.com/s/1jGRWem6](http://pan.baidu.com/s/1jGRWem6)\n\n**Neural Network and Decision Tree**\n\n- blog: [http://rotationsymmetry.github.io/2015/07/18/neural-network-decision-tree/](http://rotationsymmetry.github.io/2015/07/18/neural-network-decision-tree/)\n\n**Decision Forests, Convolutional Networks and the Models in-Between**\n\n- arxiv: [http://arxiv.org/abs/1603.01250](http://arxiv.org/abs/1603.01250)\n- notes: [http://blog.csdn.net/stdcoutzyx/article/details/50993124](http://blog.csdn.net/stdcoutzyx/article/details/50993124)\n\n**Distilling a Neural Network Into a Soft Decision Tree**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1711.09784](https://arxiv.org/abs/1711.09784)\n- github(PyTorch): [https://github.com//kimhc6028/soft-decision-tree](https://github.com//kimhc6028/soft-decision-tree)\n\n**End-to-end Learning of Deterministic Decision Trees**\n\n[https://arxiv.org/abs/1712.02743](https://arxiv.org/abs/1712.02743)\n\n**Deep Neural Decision Trees**\n\n- intro: presented at 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden\n- arxiv: [https://arxiv.org/abs/1806.06988](https://arxiv.org/abs/1806.06988)\n- github: [https://github.com/wOOL/DNDT](https://github.com/wOOL/DNDT)\n\n**Adaptive Neural Trees**\n\n- intro: ICML 2019\n- intro: University College London & Imperial College London & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1807.06699](https://arxiv.org/abs/1807.06699)\n- paper: [http://proceedings.mlr.press/v97/tanno19a.html](http://proceedings.mlr.press/v97/tanno19a.html)\n- github(official): [https://github.com/rtanno21609/AdaptiveNeuralTrees](https://github.com/rtanno21609/AdaptiveNeuralTrees)\n\n**Visualizing the decision-making process in deep neural decision forest**\n\n- intro:  CVPR 2019 workshops on explainable AI\n- arxiv: [https://arxiv.org/abs/1904.09201](https://arxiv.org/abs/1904.09201)\n- github: [https://github.com/Nicholasli1995/VisualizingNDF](https://github.com/Nicholasli1995/VisualizingNDF)\n\n**NBDT: Neural-Backed Decision Trees**\n\n- project page: [http://nbdt.alvinwan.com/](http://nbdt.alvinwan.com/)\n- arxiv: [https://arxiv.org/abs/2004.00221](https://arxiv.org/abs/2004.00221)\n- github: [https://github.com/alvinwan/neural-backed-decision-trees](https://github.com/alvinwan/neural-backed-decision-trees)\n\n# Dictionary Learning\n\n**Greedy Deep Dictionary Learning**\n\n- arxiv: [http://arxiv.org/abs/1602.00203](http://arxiv.org/abs/1602.00203)\n\n**Sparse Factorization Layers for Neural Networks with Limited Supervision**\n\n- arxiv: [https://arxiv.org/abs/1612.04468](https://arxiv.org/abs/1612.04468)\n\n**Online Convolutional Dictionary Learning**\n\n[https://arxiv.org/abs/1709.00106](https://arxiv.org/abs/1709.00106)\n\n**Deep Dictionary Learning: A PARametric NETwork Approach**\n\n[https://arxiv.org/abs/1803.04022](https://arxiv.org/abs/1803.04022)\n\n**Deep Micro-Dictionary Learning and Coding Network**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1809.04185](https://arxiv.org/abs/1809.04185)\n\n# Fisher Vectors\n\n**Backpropagation Training for Fisher Vectors within Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.02549](https://arxiv.org/abs/1702.02549)\n\n**Semi-supervised Fisher vector network**\n\n[https://arxiv.org/abs/1801.04438](https://arxiv.org/abs/1801.04438)\n\n# Gaussian Processes\n\n**Questions on Deep Gaussian Processes**\n\n- blog: [http://inverseprobability.com/2015/02/28/questions-on-deep-gaussian-processes/](http://inverseprobability.com/2015/02/28/questions-on-deep-gaussian-processes/)\n\n**Qs – Deep Gaussian Processes**\n\n![](https://www.opendatascience.com/wp-content/uploads/2016/05/Gaussian.jpg)\n\n- blog: [https://www.opendatascience.com/blog/qs-deep-gaussian-processes/](https://www.opendatascience.com/blog/qs-deep-gaussian-processes/)\n\n**Practical Learning of Deep Gaussian Processes via Random Fourier Features**\n\n- arxiv: [https://arxiv.org/abs/1610.04386](https://arxiv.org/abs/1610.04386)\n\n**Deep Learning with Gaussian Process**\n\n- blog: [https://amundtveit.com/2016/12/02/deep-learning-with-gaussian-process/](https://amundtveit.com/2016/12/02/deep-learning-with-gaussian-process/)\n\n**Doubly Stochastic Variational Inference for Deep Gaussian Processes**\n\n- arxiv: [https://arxiv.org/abs/1705.08933](https://arxiv.org/abs/1705.08933)\n- github: [https://github.com/thangbui/deepGP_approxEP](https://github.com/thangbui/deepGP_approxEP)\n- github: [https://github.com/ICL-SML/Doubly-Stochastic-DGP](https://github.com/ICL-SML/Doubly-Stochastic-DGP)\n\n**Deep Gaussian Mixture Models**\n\n[https://arxiv.org/abs/1711.06929](https://arxiv.org/abs/1711.06929)\n\n**How Deep Are Deep Gaussian Processes?**\n\n[https://arxiv.org/abs/1711.11280](https://arxiv.org/abs/1711.11280)\n\n**Variational inference for deep Gaussian processes**\n\n- intro: NIPS workshop on Advances in Approximate Bayesian Inference 2017\n- slide: [http://adamian.github.io/talks/Damianou_NIPS17.pdf](http://adamian.github.io/talks/Damianou_NIPS17.pdf)\n\n**Deep Gaussian Processes with Decoupled Inducing Inputs**\n\n- intro: University of Cambridge & University of Seville\n- arxiv: [https://arxiv.org/abs/1801.02939](https://arxiv.org/abs/1801.02939)\n\n**Gaussian Process Behaviour in Wide Deep Neural Networks**\n\n- intro: University of Cambridge\n- arxiv: [https://arxiv.org/abs/1804.11271](https://arxiv.org/abs/1804.11271)\n- github: [https://github.com/widedeepnetworks/widedeepnetworks](https://github.com/widedeepnetworks/widedeepnetworks)\n\n**Differentiable Compositional Kernel Learning for Gaussian Processes**\n\n- intro: ICML 2018. University of Toronto\n- arxiv: [https://arxiv.org/abs/1806.04326](https://arxiv.org/abs/1806.04326)\n\n**Deep Convolutional Networks as shallow Gaussian Processes**\n\n- intro: University of Cambridge\n- arxiv: [https://arxiv.org/abs/1808.05587](https://arxiv.org/abs/1808.05587)\n- github: [https://github.com/rhaps0dy/convnets-as-gps](https://github.com/rhaps0dy/convnets-as-gps)\n\n**Deep convolutional Gaussian processes**\n\n- intro: Aalto university\n- arxiv: [https://arxiv.org/abs/1810.03052](https://arxiv.org/abs/1810.03052)\n- github: [https://github.com/kekeblom/DeepCGP](https://github.com/kekeblom/DeepCGP)\n\n**Graph Convolutional Gaussian Processes**\n\n- intro: ICML 2019\n- arxiv: [https://arxiv.org/abs/1905.05739](https://arxiv.org/abs/1905.05739)\n\n**Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1910.12478](https://arxiv.org/abs/1910.12478)\n- github: [https://github.com/thegregyang/GP4A](https://github.com/thegregyang/GP4A)\n\n# Graphical Models\n\n**GibbsNet: Iterative Adversarial Inference for Deep Graphical Models**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1712.04120](https://arxiv.org/abs/1712.04120)\n\n# GMM\n\n**DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.09211](https://arxiv.org/abs/1808.09211)\n\n# HMM\n\n**Unsupervised Neural Hidden Markov Models**\n\n- intro: EMNLP 2016\n- paper: [http://www.isi.edu/natural-language/mt/neural-hmm16.pdf](http://www.isi.edu/natural-language/mt/neural-hmm16.pdf)\n- github: [https://github.com/ketranm/neuralHMM](https://github.com/ketranm/neuralHMM)\n\n# Histogram\n\n**Learnable Histogram: Statistical Context Features for Deep Neural Networks**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1804.09398](https://arxiv.org/abs/1804.09398)\n\n# Kalman Filter\n\n**Deep Robust Kalman Filter**\n\n[https://arxiv.org/abs/1703.02310](https://arxiv.org/abs/1703.02310)\n\n# Kernel Methods\n\n**Kernel Methods for Deep Learning**\n\n- intro: NIPS 2009\n- paper: [https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning](https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning)\n- paper: [http://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf](http://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf)\n\n**Deep Kernel Learning**\n\n- arxiv: [http://arxiv.org/abs/1511.02222](http://arxiv.org/abs/1511.02222)\n\n**Stochastic Variational Deep Kernel Learning**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1611.00336](https://arxiv.org/abs/1611.00336)\n- code: [https://people.orie.cornell.edu/andrew/code/#SVDKL](https://people.orie.cornell.edu/andrew/code/#SVDKL)\n\n**A Deep Learning Approach To Multiple Kernel Fusion**\n\n- arxiv: [https://arxiv.org/abs/1612.09007](https://arxiv.org/abs/1612.09007)\n\n**Optimizing Kernel Machines using Deep Learning**\n\n- keywords: DKMO (Deep Kernel Machine Optimization)\n- arxiv: [https://arxiv.org/abs/1711.05374](https://arxiv.org/abs/1711.05374)\n\n**Stacked Kernel Network**\n\n[https://arxiv.org/abs/1711.09219](https://arxiv.org/abs/1711.09219)\n\n**Deep Embedding Kernel**\n\n- intro: Kennesaw State University\n- arxiv: [https://arxiv.org/abs/1804.05806](https://arxiv.org/abs/1804.05806)\n\n**Learning Explicit Deep Representations from Deep Kernel Networks**\n\n[https://arxiv.org/abs/1804.11159](https://arxiv.org/abs/1804.11159)\n\n# k-Nearest Neighbors\n\n**Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning**\n\n- intro: Pennsylvania State University\n- arxiv: [https://arxiv.org/abs/1803.04765](https://arxiv.org/abs/1803.04765)\n- arxiv-vanity: [https://www.arxiv-vanity.com/papers/1803.04765/](https://www.arxiv-vanity.com/papers/1803.04765/)\n\n# LBP\n\n**Deep Local Binary Patterns**\n\n[https://arxiv.org/abs/1711.06597](https://arxiv.org/abs/1711.06597)\n\n**Local Binary Pattern Networks**\n\n- intro: local binary pattern networks or LBPNet\n- arxiv: [https://arxiv.org/abs/1803.07125](https://arxiv.org/abs/1803.07125)\n\n**Neural Nearest Neighbors Networks**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.12575](https://arxiv.org/abs/1810.12575)\n- github: [https://github.com/visinf/n3net/](https://github.com/visinf/n3net/)\n\n# Deep Logistic Regression\n\n**Single-Label Multi-Class Image Classification by Deep Logistic Regression**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.08400](https://arxiv.org/abs/1811.08400)\n\n# Probabilistic Programming\n\n**Deep Probabilistic Programming with Edward**\n\n- intro: Columbia University & Adobe Research & Google\n- poster: [http://dustintran.com/papers/TranHoffmanMurphyBrevdoSaurousBlei2016_poster.pdf](http://dustintran.com/papers/TranHoffmanMurphyBrevdoSaurousBlei2016_poster.pdf)\n\n# SVM\n\n**Large-scale Learning with SVM and Convolutional for Generic Object Categorization**\n\n- paper: [http://yann.lecun.com/exdb/publis/pdf/huang-lecun-06.pdf](http://yann.lecun.com/exdb/publis/pdf/huang-lecun-06.pdf)\n\n**Convolutional Neural Support Vector Machines:Hybrid Visual Pattern Classifiers for Multi-robot Systems**\n\n- paper: [http://people.idsia.ch/~nagi/conferences/idsia/icmla2012.pdf](http://people.idsia.ch/~nagi/conferences/idsia/icmla2012.pdf)\n\n**Deep Learning using Linear Support Vector Machines**\n\n- intro: Workshop on Representational Learning, ICML 2013\n- arxiv: [https://arxiv.org/abs/1306.0239](https://arxiv.org/abs/1306.0239)\n- paper: [http://deeplearning.net/wp-content/uploads/2013/03/dlsvm.pdf](http://deeplearning.net/wp-content/uploads/2013/03/dlsvm.pdf)\n- github: [https://github.com/momer/deep-learning-faces](https://github.com/momer/deep-learning-faces)\n- code: [https://code.google.com/p/deeplearning-faces/](https://code.google.com/p/deeplearning-faces/)\n\n**Deep Support Vector Machines**\n\n- video: [http://videolectures.net/roks2013_wiering_vector/](http://videolectures.net/roks2013_wiering_vector/)\n- slides: [http://www.esat.kuleuven.be/sista/ROKS2013/files/presentations/DSVM_ROKS_2013_WIERING.pdf](http://www.esat.kuleuven.be/sista/ROKS2013/files/presentations/DSVM_ROKS_2013_WIERING.pdf)\n**Trusting SVM for Piecewise Linear CNNs**\n\n- intro: PL-CNNs\n- arxiv: [https://arxiv.org/abs/1611.02185](https://arxiv.org/abs/1611.02185)\n\n# Random Forest\n\n**Towards the effectiveness of Deep Convolutional Neural Network based Fast Random Forest Classifier**\n\n- arxiv: [http://arxiv.org/abs/1609.08864](http://arxiv.org/abs/1609.08864)\n\n**Deep Forest: Towards An Alternative to Deep Neural Networks**\n\n- projetc: [http://lamda.nju.edu.cn/code_gcForest.ashx](http://lamda.nju.edu.cn/code_gcForest.ashx)\n- arxiv: [https://arxiv.org/abs/1702.08835](https://arxiv.org/abs/1702.08835)\n- github(official): [https://github.com/kingfengji/gcForest](https://github.com/kingfengji/gcForest)\n\n**Forward Thinking: Building Deep Random Forests**\n\n- arxiv: [https://arxiv.org/abs/1705.07366](https://arxiv.org/abs/1705.07366)\n- github: [https://github.com/tkchris93/ForwardThinking](https://github.com/tkchris93/ForwardThinking)\n\n**Deep Regression Forests for Age Estimation**\n\n[https://arxiv.org/abs/1712.07195](https://arxiv.org/abs/1712.07195)\n\n**Deep Differentiable Random Forests for Age Estimation**\n\n[https://arxiv.org/abs/1907.10665](https://arxiv.org/abs/1907.10665)\n\n# Template Matching\n\n**QATM: Quality-Aware Template Matching For Deep Learning**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.07254](https://arxiv.org/abs/1903.07254)\n- github(official, Tensorflow): [https://github.com/cplusx/QATM](https://github.com/cplusx/QATM)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","title":"Face Recognition"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Face Recognition\ndate: 2015-10-09\n---\n\n# Papers\n\n**Deep Learning Face Representation from Predicting 10,000 Classes**\n\n- intro: CVPR 2014. DeepID\n- paper: [http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf](http://mmlab.ie.cuhk.edu.hk/pdf/YiSun_CVPR14.pdf)\n- github: [https://github.com/stdcoutzyx/DeepID_FaceClassify](https://github.com/stdcoutzyx/DeepID_FaceClassify)\n\n**基于Caffe的DeepID2实现**\n\n- 1. [http://www.miaoerduo.com/deep-learning/%E5%9F%BA%E4%BA%8Ecaffe%E7%9A%84deepid2%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%8A%EF%BC%89.html](http://www.miaoerduo.com/deep-learning/%E5%9F%BA%E4%BA%8Ecaffe%E7%9A%84deepid2%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%8A%EF%BC%89.html)\n- 2. [http://www.miaoerduo.com/deep-learning/%E5%9F%BA%E4%BA%8Ecaffe%E7%9A%84deepid2%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%AD%EF%BC%89.html](http://www.miaoerduo.com/deep-learning/%E5%9F%BA%E4%BA%8Ecaffe%E7%9A%84deepid2%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%AD%EF%BC%89.html)\n- 3. [http://www.miaoerduo.com/deep-learning/%E5%9F%BA%E4%BA%8Ecaffe%E7%9A%84deepid2%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%8B%EF%BC%89.html](http://www.miaoerduo.com/deep-learning/%E5%9F%BA%E4%BA%8Ecaffe%E7%9A%84deepid2%E5%AE%9E%E7%8E%B0%EF%BC%88%E4%B8%8B%EF%BC%89.html)\n\n**Deeply learned face representations are sparse, selective, and robust**\n\n- intro: DeepID2+\n- arxiv: [http://arxiv.org/abs/1412.1265](http://arxiv.org/abs/1412.1265)\n- video: [http://research.microsoft.com/apps/video/?id=260023](http://research.microsoft.com/apps/video/?id=260023)\n- mirror: [http://pan.baidu.com/s/1boufl3x](http://pan.baidu.com/s/1boufl3x)\n\n**MobileID: Face Model Compression by Distilling Knowledge from Neurons**\n\n- intro: AAAI 2016 Oral. CUHK\n- intro: MobileID is an extremely fast face recognition system by distilling knowledge from DeepID2\n- project page: [http://personal.ie.cuhk.edu.hk/~lz013/projects/MobileID.html](http://personal.ie.cuhk.edu.hk/~lz013/projects/MobileID.html)\n- paper: [http://personal.ie.cuhk.edu.hk/~pluo/pdf/aaai16-face-model-compression.pdf](http://personal.ie.cuhk.edu.hk/~pluo/pdf/aaai16-face-model-compression.pdf)\n- github: [https://github.com/liuziwei7/mobile-id](https://github.com/liuziwei7/mobile-id)\n\n**Deep Face Recognition**\n\n- intro: BMVC 2015\n- paper: [http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf](http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf)\n- homepage: [http://www.robots.ox.ac.uk/~vgg/software/vgg_face/](http://www.robots.ox.ac.uk/~vgg/software/vgg_face/)\n- github(Keras): [https://github.com/rcmalli/keras-vggface](https://github.com/rcmalli/keras-vggface)\n\n## FaceNet\n\n**FaceNet: A Unified Embedding for Face Recognition and Clustering**\n\n- intro: Google Inc. CVPR 2015\n- arxiv: [http://arxiv.org/abs/1503.03832](http://arxiv.org/abs/1503.03832)\n- github(Tensorflow): [https://github.com/davidsandberg/facenet](https://github.com/davidsandberg/facenet)\n- github(Caffe): [https://github.com/hizhangp/triplet](https://github.com/hizhangp/triplet)\n\n**Real time face detection and recognition**\n\n- intro: Real time face detection and recognition base on opencv/tensorflow/mtcnn/facenet\n- github: [https://github.com/shanren7/real_time_face_recognition](https://github.com/shanren7/real_time_face_recognition)\n\n- - -\n\n**Targeting Ultimate Accuracy: Face Recognition via Deep Embedding**\n\n- intro: CVPR 2015\n- arxiv: [http://arxiv.org/abs/1506.07310](http://arxiv.org/abs/1506.07310)\n\n**Learning Robust Deep Face Representation**\n\n- arxiv: [https://arxiv.org/abs/1507.04844](https://arxiv.org/abs/1507.04844)\n\n**A Light CNN for Deep Face Representation with Noisy Labels**\n\n- arxiv: [https://arxiv.org/abs/1511.02683](https://arxiv.org/abs/1511.02683)\n- github: [https://github.com/AlfredXiangWu/face_verification_experiment](https://github.com/AlfredXiangWu/face_verification_experiment)\n\n**Pose-Aware Face Recognition in the Wild**\n\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Masi_Pose-Aware_Face_Recognition_CVPR_2016_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Masi_Pose-Aware_Face_Recognition_CVPR_2016_paper.pdf)\n\n**Recurrent Regression for Face Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.06999](http://arxiv.org/abs/1607.06999)\n\n**A Discriminative Feature Learning Approach for Deep Face Recognition**\n\n- intro: ECCV 2016\n- intro: center loss\n- paper: [http://ydwen.github.io/papers/WenECCV16.pdf](http://ydwen.github.io/papers/WenECCV16.pdf)\n- github: [https://github.com/ydwen/caffe-face](https://github.com/ydwen/caffe-face)\n- github: [https://github.com/pangyupo/mxnet_center_loss](https://github.com/pangyupo/mxnet_center_loss)\n\n**Deep Face Recognition with Center Invariant Loss**\n\n- intro: ACM MM Workshop\n- paper: [http://www1.ece.neu.edu/~yuewu/files/2017/twu024.pdf](http://www1.ece.neu.edu/~yuewu/files/2017/twu024.pdf)\n\n**How Image Degradations Affect Deep CNN-based Face Recognition?**\n\n- arxiv: [http://arxiv.org/abs/1608.05246](http://arxiv.org/abs/1608.05246)\n\n**VIPLFaceNet: An Open Source Deep Face Recognition SDK**\n\n- keywords: VIPLFaceNet / SeetaFace Engine\n- arxiv: [http://arxiv.org/abs/1609.03892](http://arxiv.org/abs/1609.03892)\n\n**SeetaFace Engine**\n\n- intro: SeetaFace Engine is an open source C++ face recognition engine, which can run on CPU with no third-party dependence.\n- github: [https://github.com/seetaface/SeetaFaceEngine](https://github.com/seetaface/SeetaFaceEngine)\n\n**A Discriminative Feature Learning Approach for Deep Face Recognition**\n\n- intro: ECCV 2016\n- paper: [http://ydwen.github.io/papers/WenECCV16.pdf](http://ydwen.github.io/papers/WenECCV16.pdf)\n\n**Sparsifying Neural Network Connections for Face Recognition**\n\n- paper: [http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr16.pdf](http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr16.pdf)\n\n**Range Loss for Deep Face Recognition with Long-tail**\n\n- arxiv: [https://arxiv.org/abs/1611.08976](https://arxiv.org/abs/1611.08976)\n\n**Towards End-to-End Face Recognition through Alignment Learning**\n\n- intro: Tsinghua University\n- arxiv: [https://arxiv.org/abs/1701.07174](https://arxiv.org/abs/1701.07174)\n\n**Multi-Task Convolutional Neural Network for Face Recognition**\n\n- arxiv: [https://arxiv.org/abs/1702.04710](https://arxiv.org/abs/1702.04710)\n\n**SphereFace: Deep Hypersphere Embedding for Face Recognition**\n\n- intro: CVPR 2017\n- arxiv: [http://wyliu.com/papers/LiuCVPR17.pdf](http://wyliu.com/papers/LiuCVPR17.pdf)\n- github: [https://github.com/wy1iu/sphereface](https://github.com/wy1iu/sphereface)\n- demo: [http://v-wb.youku.com/v_show/id_XMjk3NTc1NjMxMg==.html](http://v-wb.youku.com/v_show/id_XMjk3NTc1NjMxMg==.html)\n\n**Learning towards Minimum Hyperspherical Energy**\n\n- intro: NeurIPS 2018\n- keywords: SphereFace+\n- arxiv: [https://arxiv.org/abs/1805.09298](https://arxiv.org/abs/1805.09298)\n- github: [https://github.com/wy1iu/sphereface-plus](https://github.com/wy1iu/sphereface-plus)\n\n**Low Resolution Face Recognition Using a Two-Branch Deep Convolutional Neural Network Architecture**\n\n- intro: Amirkabir University of Technology & MIT\n- arxiv: [https://arxiv.org/abs/1706.06247](https://arxiv.org/abs/1706.06247)\n\n**Enhancing Convolutional Neural Networks for Face Recognition with Occlusion Maps and Batch Triplet Loss**\n\n[https://arxiv.org/abs/1707.07923](https://arxiv.org/abs/1707.07923)\n\n**Improving Heterogeneous Face Recognition with Conditional Adversarial Networks**\n\n[https://arxiv.org/abs/1709.02848](https://arxiv.org/abs/1709.02848)\n\n**Face Sketch Matching via Coupled Deep Transform Learning**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1710.02914](https://arxiv.org/abs/1710.02914)\n\n**Face Recognition via Centralized Coordinate Learning**\n\n- intro: centralized coordinate learning (CCL)\n- arxiv: [https://arxiv.org/abs/1801.05678](https://arxiv.org/abs/1801.05678)\n\n**ArcFace: Additive Angular Margin Loss for Deep Face Recognition**\n\n- arxiv: [https://arxiv.org/abs/1801.07698](https://arxiv.org/abs/1801.07698)\n- github: [https://github.com/deepinsight/insightface](https://github.com/deepinsight/insightface)\n\n**CosFace: Large Margin Cosine Loss for Deep Face Recognition**\n\n[https://arxiv.org/abs/1801.09414](https://arxiv.org/abs/1801.09414)\n\n**Ring loss: Convex Feature Normalization for Face Recognition**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.00130](https://arxiv.org/abs/1803.00130)\n\n**Pose-Robust Face Recognition via Deep Residual Equivariant Mapping**\n\n- intro: CVPR 2018. CUHK & SenseTime Research\n- arxiv: [https://arxiv.org/abs/1803.00839](https://arxiv.org/abs/1803.00839)\n- github: [https://github.com/penincillin/DREAM](https://github.com/penincillin/DREAM)\n\n**Deep Face Recognition: A Survey**\n\n- intro: BUPT\n- arxiv: [https://arxiv.org/abs/1804.06655](https://arxiv.org/abs/1804.06655)\n\n**Surveillance Face Recognition Challenge**\n\n[https://arxiv.org/abs/1804.09691](https://arxiv.org/abs/1804.09691)\n\n**Towards Interpretable Face Recognition**\n\n[https://arxiv.org/abs/1805.00611](https://arxiv.org/abs/1805.00611)\n\n**Scalable Angular Discriminative Deep Metric Learning for Face Recognition**\n\n[https://arxiv.org/abs/1804.10899](https://arxiv.org/abs/1804.10899)\n\n**Minimum Margin Loss for Deep Face Recognition**\n\n[https://arxiv.org/abs/1805.06741](https://arxiv.org/abs/1805.06741)\n\n**Wildest Faces: Face Detection and Recognition in Violent Settings**\n\n[https://arxiv.org/abs/1805.07566](https://arxiv.org/abs/1805.07566)\n\n**Deep Imbalanced Learning for Face Recognition and Attribute Prediction**\n\n[https://arxiv.org/abs/1806.00194](https://arxiv.org/abs/1806.00194)\n\n**Accurate and Efficient Similarity Search for Large Scale Face Recognition**\n\n- intro: BUPT\n- arxiv: [https://arxiv.org/abs/1806.00365](https://arxiv.org/abs/1806.00365)\n\n**Face Recognition in Low Quality Images: A Survey**\n\n[https://arxiv.org/abs/1805.11519](https://arxiv.org/abs/1805.11519)\n\n**Low Resolution Face Recognition in the Wild**\n\n[https://arxiv.org/abs/1805.11529](https://arxiv.org/abs/1805.11529)\n\n**From Face Recognition to Models of Identity: A Bayesian Approach to Learning about Unknown Identities from Unsupervised Data**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.07872](https://arxiv.org/abs/1807.07872)\n\n**Git Loss for Deep Face Recognition**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.08512](https://arxiv.org/abs/1807.08512)\n\n**Multicolumn Networks for Face Recognition**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.09192](https://arxiv.org/abs/1807.09192)\n\n**The Devil of Face Recognition is in the Noise**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.11649](https://arxiv.org/abs/1807.11649)\n- paper: [http://openaccess.thecvf.com/content_ECCV_2018/papers/Liren_Chen_The_Devil_of_ECCV_2018_paper.pdf](http://openaccess.thecvf.com/content_ECCV_2018/papers/Liren_Chen_The_Devil_of_ECCV_2018_paper.pdf)\n- dataset: [https://github.com/fwang91/IMDb-Face](https://github.com/fwang91/IMDb-Face)\n\n**Deep Sketch-Photo Face Recognition Assisted by Facial Attributes**\n\n[https://arxiv.org/abs/1808.00059](https://arxiv.org/abs/1808.00059两)\n\n**Global Norm-Aware Pooling for Pose-Robust Face Recognition at Low False Positive Rate**\n\n[https://arxiv.org/abs/1808.00435](https://arxiv.org/abs/1808.00435)\n\n**Pairwise Relational Networks for Face Recognition**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.04976](https://arxiv.org/abs/1808.04976)\n\n**Orthogonal Deep Features Decomposition for Age-Invariant Face Recognition**\n\n[https://arxiv.org/abs/1810.07599](https://arxiv.org/abs/1810.07599)\n\n**Pairwise Relational Networks using Local Appearance Features for Face Recognition**\n\n- intro: NIPS 2018 R2L workshop\n- arxiv: [https://arxiv.org/abs/1811.06405](https://arxiv.org/abs/1811.06405)\n\n**Low-resolution Face Recognition in the Wild via Selective Knowledge Distillation**\n\n[https://arxiv.org/abs/1811.09998](https://arxiv.org/abs/1811.09998)\n\nL**ow-Resolution Face Recognition**\n\n- arxiv: [https://arxiv.org/abs/1811.08965](https://arxiv.org/abs/1811.08965)\n- dataset: [https://qmul-tinyface.github.io/](https://qmul-tinyface.github.io/)\n\n**MobiFace: A Lightweight Deep Learning Face Recognition on Mobile Devices**\n\n[https://arxiv.org/abs/1811.11080](https://arxiv.org/abs/1811.11080)\n\n**Attacks on State-of-the-Art Face Recognition using Attentional Adversarial Attack Generative Network**\n\n[https://arxiv.org/abs/1811.12026](https://arxiv.org/abs/1811.12026)\n\n**Support Vector Guided Softmax Loss for Face Recognition**\n\n- intro: JD AI research & Chinese Academy of Science\n- arxiv: [https://arxiv.org/abs/1812.11317](https://arxiv.org/abs/1812.11317)\n\n**Linkage Based Face Clustering via Graph Convolution Network**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.11306](https://arxiv.org/abs/1903.11306)\n\n**Deep Learning for Face Recognition: Pride or Prejudiced?**\n\n[https://arxiv.org/abs/1904.01219](https://arxiv.org/abs/1904.01219)\n\n**Efficient Decision-based Black-box Adversarial Attacks on Face Recognition**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.04433](https://arxiv.org/abs/1904.04433)\n\n**Decorrelated Adversarial Learning for Age-Invariant Face Recognition**\n\n- intro: Tencent AI Lab\n- arxiv: [https://arxiv.org/abs/1904.04972](https://arxiv.org/abs/1904.04972)\n\n**ShrinkTeaNet: Million-scale Lightweight Face Recognition via Shrinking Teacher-Student Networks**\n\n[https://arxiv.org/abs/1905.10620](https://arxiv.org/abs/1905.10620)\n\n**Attentional Feature-Pair Relation Networks for Accurate Face Recognition**\n\n[https://arxiv.org/abs/1908.06255](https://arxiv.org/abs/1908.06255)\n\n**Occlusion Robust Face Recognition Based on Mask Learning with PairwiseDifferential Siamese Network**\n\n[https://arxiv.org/abs/1908.06290](https://arxiv.org/abs/1908.06290)\n\n**Towards Flops-constrained Face Recognition**\n\n- intro: ICCV 2019 LFR workshop\n- arxiv: [https://arxiv.org/abs/1909.00632](https://arxiv.org/abs/1909.00632)\n\n**VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition**\n\n- intro: ICCV workshop 2019, champion of deepglintlight track of LFR (2019) challenge\n- intro: Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1910.04985](https://arxiv.org/abs/1910.04985)\n- paper: [http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Yan_VarGFaceNet_An_Efficient_Variable_Group_Convolutional_Neural_Network_for_Lightweight_ICCVW_2019_paper.pdf](http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Yan_VarGFaceNet_An_Efficient_Variable_Group_Convolutional_Neural_Network_for_Lightweight_ICCVW_2019_paper.pdf)\n- github: [https://github.com/zma-c-137/VarGFaceNet](https://github.com/zma-c-137/VarGFaceNet)\n\n**Towards Flops-constrained Face Recognition**\n\n- intro: ICCV 2019 LFR workshop\n- intro: 1st place in the ICCV19 Lightweight Face Recognition Challenge, large video track\n- keywords: Trojans\n- arxiv: [https://arxiv.org/abs/1909.00632](https://arxiv.org/abs/1909.00632)\n- paper: [https://github.com/sciencefans/trojans-face-recognizer](https://github.com/sciencefans/trojans-face-recognizer)\n\n**Unknown Identity Rejection Loss: Utilizing Unlabeled Data for Face Recognition**\n\n- intro: ICCV 2019 Lightweight Face Recognition Challenge & Workshop\n- arxiv: [https://arxiv.org/abs/1910.10896](https://arxiv.org/abs/1910.10896)\n\n**FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization**\n\n- intro: Michigan State University & Youtu Lab, Tencent & Microsoft Cloud and AI & Zhejiang University\n- arxiv: [https://arxiv.org/abs/1911.11680](https://arxiv.org/abs/1911.11680)\n\n**Mis-classified Vector Guided Softmax Loss for Face Recognition**\n\n- intro: AAAI 2019 oral\n- arxiv: [https://arxiv.org/abs/1912.00833](https://arxiv.org/abs/1912.00833)\n\n**Domain Balancing: Face Recognition on Long-Tailed Domains**\n\n- intro: CVPR 2020\n- intro: Chinese Academy of Sciences & Tianjin University\n- arxiv: [https://arxiv.org/abs/2003.13791](https://arxiv.org/abs/2003.13791)\n\n**CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition**\n\n- intro: CVPR 2020\n- intro: Youtu Lab, Tencent & Zhejiang University & Michigan State University\n- arixv: [https://arxiv.org/abs/2004.00288](https://arxiv.org/abs/2004.00288)\n- github: [https://github.com/HuangYG123/CurricularFace](https://github.com/HuangYG123/CurricularFace)\n\n**Loss Function Search for Face Recognition**\n\n- intro: ICML 2020\n- arxiv: [https://arxiv.org/abs/2007.06542](https://arxiv.org/abs/2007.06542)\n\n**Explainable Face Recognition**\n\n- intro: ECCV 2020\n- project page: [https://stresearch.github.io/xfr/](https://stresearch.github.io/xfr/)\n- arxiv: [https://arxiv.org/abs/2008.00916](https://arxiv.org/abs/2008.00916)\n- github: [https://github.com/stresearch/xfr](https://github.com/stresearch/xfr)\n\n**BroadFace: Looking at Tens of Thousands of People at Once for Face Recognition**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.06674](https://arxiv.org/abs/2008.06674)\n\n**Searching for Alignment in Face Recognition**\n\n- intro: AAAI 2021\n- arxiv: [https://arxiv.org/abs/2102.05447](https://arxiv.org/abs/2102.05447)\n\n**When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework**\n\n- intro: CVPR 2021\n- intro: Fudan University\n- arxiv: [https://arxiv.org/abs/2103.01520](https://arxiv.org/abs/2103.01520)\n- github: [https://github.com/Hzzone/MTLFace](https://github.com/Hzzone/MTLFace)\n\n**Unmasking Face Embeddings by Self-restrained Triplet Loss for Accurate Masked Face Recognition**\n\n[https://arxiv.org/abs/2103.01716](https://arxiv.org/abs/2103.01716)\n\n**WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition**\n\n- intro: CVPR 2021\n- intro: Tsinghua University & XForwardAI & Imperial College London\n- project page: [https://www.face-benchmark.org/](https://www.face-benchmark.org/)\n- arxiv: [https://arxiv.org/abs/2103.04098](https://arxiv.org/abs/2103.04098)\n\n**Face Transformer for Recognition**\n\n- intro: Beijing University of Posts and Telecommunications\n- arxiv: [https://arxiv.org/abs/2103.14803](https://arxiv.org/abs/2103.14803)\n\n**MagFace: A Universal Representation for Face Recognition and Quality Assessment**\n\n- intro: CVPR 2021 oral\n- intro: Algorithm Research, Aibee Inc.\n- arxiv: [https://arxiv.org/abs/2103.06627](https://arxiv.org/abs/2103.06627)\n- github: [https://github.com/IrvingMeng/MagFace](https://github.com/IrvingMeng/MagFace)\n\n**Dynamic Class Queue for Large Scale Face Recognition In the Wild**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2105.11113](https://arxiv.org/abs/2105.11113)\n- github: [https://github.com/bilylee/DCQ](https://github.com/bilylee/DCQ)\n\n**End2End Occluded Face Recognition by Masking Corrupted Features**\n\n- intro: TPAMI 2021\n- arxiv: [https://arxiv.org/abs/2108.09468](https://arxiv.org/abs/2108.09468)\n\n**AdaFace: Quality Adaptive Margin for Face Recognition**\n\n- intro: CVPR 2022 oral\n- arxiv: [https://arxiv.org/abs/2204.00964](https://arxiv.org/abs/2204.00964)\n- github: [https://github.com/mk-minchul/AdaFace](https://github.com/mk-minchul/AdaFace)\n\n**CoupleFace: Relation Matters for Face Recognition Distillation**\n\n- intro: Beihang University & SenseTime Group Limited & The University of Sydney\n- arxiv: [https://arxiv.org/abs/2204.05502](https://arxiv.org/abs/2204.05502)\n\n# Face Verification\n\n**Deep Learning Face Representation by Joint Identification-Verification**\n\n- intro: DeepID2\n- paper: [http://papers.nips.cc/paper/5416-analog-memories-in-a-balanced-rate-based-network-of-e-i-neurons](http://papers.nips.cc/paper/5416-analog-memories-in-a-balanced-rate-based-network-of-e-i-neurons)\n\n**DeepFace: Closing the Gap to Human-Level Performance in Face Verification**\n\n- intro: CVPR 2014. Facebook AI Research\n- paper: [https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf)\n- slides: [http://valse.mmcheng.net/ftp/20141126/MingYang.pdf](http://valse.mmcheng.net/ftp/20141126/MingYang.pdf)\n- github: [https://github.com/RiweiChen/DeepFace](https://github.com/RiweiChen/DeepFace)\n\n**Triplet Probabilistic Embedding for Face Verification and Clustering**\n\n- intro: Oral Paper in BTAS 2016; NVIDIA Best paper Award\n- arxiv: [https://arxiv.org/abs/1604.05417](https://arxiv.org/abs/1604.05417)\n- github(Keras): [https://github.com/meownoid/face-identification-tpe](https://github.com/meownoid/face-identification-tpe)\n\n**DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification**\n\n[https://arxiv.org/abs/1611.05271](https://arxiv.org/abs/1611.05271)\n\n**Hybrid Deep Learning for Face Verification**\n\n- intro: TPAMI 2016. CNN+RBM\n- paper: [http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTpami16.pdf](http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTpami16.pdf)\n\n**NormFace: L2 Hypersphere Embedding for Face Verification**\n\n- arxiv: [https://arxiv.org/abs/1704.06369](https://arxiv.org/abs/1704.06369)\n- github: [https://github.com/happynear/NormFace](https://github.com/happynear/NormFace)\n\n**L2-constrained Softmax Loss for Discriminative Face Verification**\n\n[https://arxiv.org/abs/1703.09507](https://arxiv.org/abs/1703.09507)\n\n**von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification**\n\n[https://arxiv.org/abs/1706.04264](https://arxiv.org/abs/1706.04264)\n\n**Model Distillation with Knowledge Transfer in Face Classification, Alignment and Verification**\n\n[https://arxiv.org/abs/1709.02929](https://arxiv.org/abs/1709.02929)\n\n**Additive Margin Softmax for Face Verification**\n\n- keywords: additive margin Softmax (AM-Softmax),\n- arxiv: [https://arxiv.org/abs/1801.05599](https://arxiv.org/abs/1801.05599)\n- github: [https://github.com/happynear/AMSoftmax](https://github.com/happynear/AMSoftmax)\n\n**MobileFaceNets: Efficient CNNs for Accurate Real-time Face Verification on Mobile Devices**\n\n- intro: Beijing Jiaotong University & Watchdata Inc\n- arxiv: [https://arxiv.org/abs/1804.07573](https://arxiv.org/abs/1804.07573)\n\n**DisguiseNet : A Contrastive Approach for Disguised Face Verification in the Wild**\n\n[https://arxiv.org/abs/1804.09669](https://arxiv.org/abs/1804.09669)\n\n**An Experimental Evaluation of Covariates Effects on Unconstrained Face Verification**\n\n[https://arxiv.org/abs/1808.05508](https://arxiv.org/abs/1808.05508)\n\n**Verification of Very Low-Resolution Faces Using An Identity-Preserving Deep Face Super-Resolution Network**\n\n[https://arxiv.org/abs/1903.10974](https://arxiv.org/abs/1903.10974)\n\n**xCos: An Explainable Cosine Metric for Face Verification Task**\n\n[https://arxiv.org/abs/2003.05383](https://arxiv.org/abs/2003.05383)\n\n# Facial Attributes Classification\n\n**A Jointly Learned Deep Architecture for Facial Attribute Analysis and Face Detection in the Wild**\n\n[https://arxiv.org/abs/1707.08705](https://arxiv.org/abs/1707.08705)\n\n**A Deep Cascade Network for Unaligned Face Attribute Classification**\n\n[https://arxiv.org/abs/1709.03851](https://arxiv.org/abs/1709.03851)\n\n**A Deep Face Identification Network Enhanced by Facial Attributes Prediction**\n\n[https://arxiv.org/abs/1805.00324](https://arxiv.org/abs/1805.00324)\n\n**Multi-task Learning of Cascaded CNN for Facial Attribute Classification**\n\n- intro: Xiamen University & Xiamen University of Technology\n- arxiv: [https://arxiv.org/abs/1805.01290](https://arxiv.org/abs/1805.01290)\n\n**Multi-label Learning Based Deep Transfer Neural Network for Facial Attribute Classification**\n\n- intro: Xiamen University & Xiamen University of Technology\n- arxiv: [https://arxiv.org/abs/1805.01282](https://arxiv.org/abs/1805.01282)\n\n**A Survey to Deep Facial Attribute Analysis**\n\n[https://arxiv.org/abs/1812.10265](https://arxiv.org/abs/1812.10265)\n\n**Registration-free Face-SSD: Single shot analysis of smiles, facial attributes, and affect in the wild**\n\n- intro: Elsevier CVIU 2019\n- arxiv: [https://arxiv.org/abs/1902.04042](https://arxiv.org/abs/1902.04042)\n\n# Video Face Recognition\n\n**Neural Aggregation Network for Video Face Recognition**\n\n- intro: CVPR 2017\n- keywords: Neural Aggregation Network (NAN)\n- arxiv: [https://arxiv.org/abs/1603.05474](https://arxiv.org/abs/1603.05474)\n\n**Attention-Set based Metric Learning for Video Face Recognition**\n\n[https://arxiv.org/abs/1704.03805](https://arxiv.org/abs/1704.03805)\n\n**SeqFace: Make full use of sequence information for face recognitio**\n\n- arxiv: [https://arxiv.org/abs/1803.06524](https://arxiv.org/abs/1803.06524)\n- github: [https://github.com/huangyangyu/SeqFace](https://github.com/huangyangyu/SeqFace)\n\n**Video Face Recognition: Component-wise Feature Aggregation Network (C-FAN)**\n\n- intro: Michigan State University\n- arxiv: [https://arxiv.org/abs/1902.07327](https://arxiv.org/abs/1902.07327)\n\n# Facial Point / Landmark Detection\n\n**Deep Convolutional Network Cascade for Facial Point Detection**\n\n![](http://mmlab.ie.cuhk.edu.hk/archive/CNN/data/Picture1.png)\n\n- homepage: [http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm](http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm)\n- paper: [http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf](http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf)\n- github: [https://github.com/luoyetx/deep-landmark](https://github.com/luoyetx/deep-landmark)\n\n**Facial Landmark Detection by Deep Multi-task Learning**\n\n- intro: ECCV 2014\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html](http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html)\n- paper: [http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf](http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf)\n- github(Matlab): [https://github.com/zhzhanp/TCDCN-face-alignment](https://github.com/zhzhanp/TCDCN-face-alignment)\n\n**A Recurrent Encoder-Decoder Network for Sequential Face Alignment**\n\n- intro: ECCV 2016 oral\n- project page: [https://sites.google.com/site/xipengcshomepage/eccv2016](https://sites.google.com/site/xipengcshomepage/eccv2016)\n- arxiv: [https://arxiv.org/abs/1608.05477](https://arxiv.org/abs/1608.05477)\n- slides: [https://drive.google.com/file/d/0B-FLp_bljv_1OTVrMF9OM21IbW8/view](https://drive.google.com/file/d/0B-FLp_bljv_1OTVrMF9OM21IbW8/view)\n- github: [https://github.com/xipeng13/recurrent-face-alignment](https://github.com/xipeng13/recurrent-face-alignment)\n\n**RED-Net: A Recurrent Encoder-Decoder Network for Video-based Face Alignment**\n\n- intro: IJCV\n- arxiv: [https://arxiv.org/abs/1801.06066](https://arxiv.org/abs/1801.06066)\n\n**Detecting facial landmarks in the video based on a hybrid framework**\n\n- arxiv: [http://arxiv.org/abs/1609.06441](http://arxiv.org/abs/1609.06441)\n\n**Deep Constrained Local Models for Facial Landmark Detection**\n\n- arxiv: [https://arxiv.org/abs/1611.08657](https://arxiv.org/abs/1611.08657)\n\n**Effective face landmark localization via single deep network**\n\n- arxiv: [https://arxiv.org/abs/1702.02719](https://arxiv.org/abs/1702.02719)\n\n**A Convolution Tree with Deconvolution Branches: Exploiting Geometric Relationships for Single Shot Keypoint Detection**\n\n[https://arxiv.org/abs/1704.01880](https://arxiv.org/abs/1704.01880)\n\n**Deep Alignment Network: A convolutional neural network for robust face alignment**\n\n- intro: CVPRW 2017\n- arxiv: [https://arxiv.org/abs/1706.01789](https://arxiv.org/abs/1706.01789)\n- gihtub: [https://github.com/MarekKowalski/DeepAlignmentNetwork](https://github.com/MarekKowalski/DeepAlignmentNetwork)\n\n**Joint Multi-view Face Alignment in the Wild**\n\n[https://arxiv.org/abs/1708.06023](https://arxiv.org/abs/1708.06023)\n\n**FacePoseNet: Making a Case for Landmark-Free Face Alignment**\n\n[https://arxiv.org/abs/1708.07517](https://arxiv.org/abs/1708.07517)\n\n**Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1711.06753](https://arxiv.org/abs/1711.06753)\n\n**Brute-Force Facial Landmark Analysis With A 140,000-Way Classifier**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1802.01777](https://arxiv.org/abs/1802.01777)\n- github: [https://github.com/mtli/BFFL](https://github.com/mtli/BFFL)\n\n**Style Aggregated Network for Facial Landmark Detection**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.04108](https://arxiv.org/abs/1803.04108)\n- github: [https://github.com/D-X-Y/SAN](https://github.com/D-X-Y/SAN)\n\n**Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment**\n\n[https://arxiv.org/abs/1803.05588](https://arxiv.org/abs/1803.05588)\n\n**Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors**\n\n- intro: Minor modifications to the CVPR 2018 version (add missing references)\n- arxiv: [https://arxiv.org/abs/1807.00966](https://arxiv.org/abs/1807.00966)\n\n**Deep Multi-Center Learning for Face Alignment**\n\n- arxiv: [https://arxiv.org/abs/1808.01558](https://arxiv.org/abs/1808.01558)\n- github: [https://github.com/ZhiwenShao/MCNet-Extension](https://github.com/ZhiwenShao/MCNet-Extension)\n\n**Hierarchical binary CNNs for landmark localization with limited resources**\n\n- intro: TPAMI 2018\n- arxiv: [https://arxiv.org/abs/1808.04803](https://arxiv.org/abs/1808.04803)\n\n**Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.00342](https://arxiv.org/abs/1811.00342)\n- github: [https://github.com/tyshiwo/FHR_alignment](https://github.com/tyshiwo/FHR_alignment)\n\n**Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment**\n\n[https://arxiv.org/abs/1812.01936](https://arxiv.org/abs/1812.01936)\n\n**PFLD: A Practical Facial Landmark Detector**\n\n- arxiv: [https://arxiv.org/abs/1902.10859](https://arxiv.org/abs/1902.10859)\n- github: [https://github.com/guoqiangqi/PFLD](https://github.com/guoqiangqi/PFLD)\n\n**Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.10661](https://arxiv.org/abs/1903.10661)\n\n**An Efficient Multitask Neural Network for Face Alignment, Head Pose Estimation and Face Tracking**\n\n- intro: University of Technology Sydney\n- arxiv: [https://arxiv.org/abs/2103.07615](https://arxiv.org/abs/2103.07615)\n\n**HIH: Towards More Accurate Face Alignment via Heatmap in Heatmap**\n\n[https://arxiv.org/abs/2104.03100](https://arxiv.org/abs/2104.03100)\n\n**Subpixel Heatmap Regression for Facial Landmark Localization**\n\n- intro: BMVC 2021\n- intro: Samsung AI Center, Cambridge, UK & Queen Mary University London\n- project page: [https://www.adrianbulat.com/face-alignment](https://www.adrianbulat.com/face-alignment)\n- arxiv: [https://arxiv.org/abs/2111.02360](https://arxiv.org/abs/2111.02360)\n- github: [https://github.com/1adrianb/face-alignment](https://github.com/1adrianb/face-alignment)\n\n**RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection**\n\n- intro: The Chinese University of Hong Kong & The Hong Kong University of Science and Technology & IIAI & Terminus Group & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/2207.03917](https://arxiv.org/abs/2207.03917)\n\n# Face Synthesis\n\n**Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis**\n\n- intro: ICCV 2017\n- keywords: TP-GAN\n- arxiv: [https://arxiv.org/abs/1704.04086](https://arxiv.org/abs/1704.04086)\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Beyond_Face_Rotation_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Beyond_Face_Rotation_ICCV_2017_paper.pdf)\n- github(official, Tensorflow): [https://github.com/HRLTY/TP-GAN](https://github.com/HRLTY/TP-GAN)\n\n# Projects\n\n**Using MXNet for Face-related Algorithm**\n\n- github: [https://github.com/tornadomeet/mxnet-face](https://github.com/tornadomeet/mxnet-face)\n\n**clmtrackr: Javascript library for precise tracking of facial features via Constrained Local Models**\n\n- github: [https://github.com/auduno/clmtrackr](https://github.com/auduno/clmtrackr)\n- blog: [http://auduno.com/post/61888277175/fitting-faces](http://auduno.com/post/61888277175/fitting-faces)\n- demo: [http://auduno.github.io/clmtrackr/examples/facesubstitution.html](http://auduno.github.io/clmtrackr/examples/facesubstitution.html)\n- demo: [http://auduno.github.io/clmtrackr/face_deformation_video.html](http://auduno.github.io/clmtrackr/face_deformation_video.html)\n- demo: [http://auduno.github.io/clmtrackr/examples/clm_emotiondetection.html](http://auduno.github.io/clmtrackr/examples/clm_emotiondetection.html)\n- demo: [http://auduno.com/post/84214587523/twisting-faces](http://auduno.com/post/84214587523/twisting-faces)\n\n**DeepLogo**\n\n- intro: A brand logo recognition system using deep convolutional neural networks.\n- github: [https://github.com/satojkovic/DeepLogo](https://github.com/satojkovic/DeepLogo)\n\n**Deep-Leafsnap**\n\n- intro: LeafSnap replicated using deep neural networks to test accuracy compared to traditional computer vision methods.\n- github: [https://github.com/sujithv28/Deep-Leafsnap](https://github.com/sujithv28/Deep-Leafsnap)\n\n**FaceVerification: An Experimental Implementation of Face Verification, 96.8% on LFW**\n\n- github: [https://github.com/happynear/FaceVerification](https://github.com/happynear/FaceVerification)\n\n**InsightFace**\n\n- intro: Face Recognition Project on MXnet\n- arxiv: [https://github.com//deepinsight/insightface](https://github.com//deepinsight/insightface)\n\n## OpenFace\n\n**OpenFace: Face Recognition with Deep Neural Networks**\n\n- homepage: [http://cmusatyalab.github.io/openface/](http://cmusatyalab.github.io/openface/)\n- github: [https://github.com/cmusatyalab/openface](https://github.com/cmusatyalab/openface)\n- github: [https://github.com/aybassiouny/OpenFaceCpp](https://github.com/aybassiouny/OpenFaceCpp)\n\n**OpenFace 0.2.0: Higher accuracy and halved execution time**\n\n- homepage: [http://bamos.github.io/2016/01/19/openface-0.2.0/](http://bamos.github.io/2016/01/19/openface-0.2.0/)\n\n**OpenFace: A general-purpose face recognition library with mobile applications**\n\n- paper: [http://reports-archive.adm.cs.cmu.edu/anon/anon/usr0/ftp/2016/CMU-CS-16-118.pdf](http://reports-archive.adm.cs.cmu.edu/anon/anon/usr0/ftp/2016/CMU-CS-16-118.pdf)\n\n**OpenFace: an open source facial behavior analysis toolkit**\n\n![](https://raw.githubusercontent.com/TadasBaltrusaitis/OpenFace/master/imgs/multi_face_img.png)\n\n- intro: a state-of-the art open source tool intended for facial landmark detection, head pose estimation, \nfacial action unit recognition, and eye-gaze estimation.\n- github: [https://github.com/TadasBaltrusaitis/OpenFace](https://github.com/TadasBaltrusaitis/OpenFace)\n\n# Resources\n\n**Face-Resources**\n\n- github: [https://github.com/betars/Face-Resources](https://github.com/betars/Face-Resources)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","title":"Fun With Deep Learning"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Fun With Deep Learning\r\ndate: 2015-10-09\r\n---\r\n\r\n# Painting\r\n\r\n**Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting**\r\n\r\n- intro: ICML 2012\r\n- arxiv: [https://arxiv.org/abs/1206.4634](https://arxiv.org/abs/1206.4634)\r\n\r\n## Emoji\r\n\r\n**Brewing EmojiNet**\r\n\r\n![](http://engineering.curalate.com/assets/2016-01-13-Emojinet/demo02.png)\r\n\r\n- blog: [http://engineering.curalate.com/2016/01/20/emojinet.html](http://engineering.curalate.com/2016/01/20/emojinet.html)\r\n- website: [https://emojini.curalate.com/](https://emojini.curalate.com/)\r\n\r\n**Image2Emoji: Zero-shot Emoji Prediction for Visual Media**\r\n\r\n- paper: [http://isis-data.science.uva.nl/cgmsnoek/pub/cappallo-image2emoji-mm2015.pdf](http://isis-data.science.uva.nl/cgmsnoek/pub/cappallo-image2emoji-mm2015.pdf)\r\n\r\n**Teaching Robots to Feel: Emoji & Deep Learning 👾 💭 💕**\r\n\r\n![](http://getdango.com/postimg/RNN-figure.png)\r\n\r\n- blog: [http://getdango.com/emoji-and-deep-learning.html](http://getdango.com/emoji-and-deep-learning.html)\r\n- app: [https://play.google.com/store/apps/details?id=co.dango.emoji.gif](https://play.google.com/store/apps/details?id=co.dango.emoji.gif)\r\n\r\n**Text input with relevant emoji sorted with deeplearning**\r\n\r\n- homepage: [http://codepen.io/Idlework/pen/xOgGqM](http://codepen.io/Idlework/pen/xOgGqM)\r\n\r\n## Sketch\r\n\r\n**Sketch-a-Net that Beats Humans**\r\n\r\n![](/assets/fun_with_dl/Sketch-a-Net_that_Beats_Humans.png)\r\n\r\n- project page: [http://www.eecs.qmul.ac.uk/~tmh/downloads.html](http://www.eecs.qmul.ac.uk/~tmh/downloads.html)\r\n- arxiv: [http://arxiv.org/abs/1501.07873](http://arxiv.org/abs/1501.07873)\r\n- paper: [http://www.eecs.qmul.ac.uk/~tmh/papers/yu2015sketchanet.pdf](http://www.eecs.qmul.ac.uk/~tmh/papers/yu2015sketchanet.pdf)\r\n- code: [http://www.eecs.qmul.ac.uk/~tmh/downloads/SketchANet_Code.zip](http://www.eecs.qmul.ac.uk/~tmh/downloads/SketchANet_Code.zip)\r\n\r\n**How Do Humans Sketch Objects?**\r\n\r\n![](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/teaser_siggraph.jpg)\r\n\r\n- project page: [http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/)\r\n- paper: [http://cybertron.cg.tu-berlin.de/eitz/pdf/2012_siggraph_classifysketch.pdf](http://cybertron.cg.tu-berlin.de/eitz/pdf/2012_siggraph_classifysketch.pdf)\r\n- github: [https://github.com/Zebreu/SketchingAI](https://github.com/Zebreu/SketchingAI)\r\n- gitxiv: [http://gitxiv.com/posts/ZBCxEc9g3Fg5xCQ6n/sketchingai](http://gitxiv.com/posts/ZBCxEc9g3Fg5xCQ6n/sketchingai)\r\n\r\n**Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup (SIGGRAPH 2016)**\r\n\r\n![](http://hi.cs.waseda.ac.jp/~esimo/images/sketch/overview.png)\r\n\r\n- homepage: [http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch/](http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch/)\r\n- paper: [http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraSIGGRAPH2016.pdf](http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraSIGGRAPH2016.pdf)\r\n\r\n**Convolutional Sketch Inversion**\r\n\r\n![](/assets/fun_with_dl/Convolutional_Sketch_Inversion.png)\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.03073](http://arxiv.org/abs/1606.03073)\r\n- review: [https://www.technologyreview.com/s/601684/machine-vision-algorithm-learns-to-transform-hand-drawn-sketches-into-photorealistic-images/](https://www.technologyreview.com/s/601684/machine-vision-algorithm-learns-to-transform-hand-drawn-sketches-into-photorealistic-images/)\r\n- review: [https://techcrunch.com/2016/07/24/researchers-use-neural-networks-to-turn-face-sketches-into-photos/](https://techcrunch.com/2016/07/24/researchers-use-neural-networks-to-turn-face-sketches-into-photos/)\r\n\r\n**Sketch Me That Shoe (CVPR 2016)**\r\n\r\n![](http://www.eecs.qmul.ac.uk/~qian/Qian's%20Materials/0001.jpg)\r\n\r\n- project page: [http://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html](http://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html)\r\n- paper: [http://www.eecs.qmul.ac.uk/~qian/SketchMeThatShoe.pdf](http://www.eecs.qmul.ac.uk/~qian/SketchMeThatShoe.pdf)\r\n- github: [https://github.com/seuliufeng/DeepSBIR](https://github.com/seuliufeng/DeepSBIR)\r\n\r\n**Mastering Sketching: Adversarial Augmentation for Structured Prediction**\r\n\r\n- keywords: Sketch Simplification\r\n- project page: [http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch_master/](http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch_master/)\r\n- arxiv: [https://arxiv.org/abs/1703.08966](https://arxiv.org/abs/1703.08966)\r\n- github: [https://github.com/bobbens/sketch_simplification](https://github.com/bobbens/sketch_simplification)\r\n\r\n**SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis**\r\n\r\n- intro: Georgia Institute of Technology\r\n- arxiv: [https://arxiv.org/abs/1801.02753](https://arxiv.org/abs/1801.02753)\r\n\r\n## Image Colorization\r\n\r\n**Deep Colorization**\r\n\r\n- paper: [http://www.cs.cityu.edu.hk/~qiyang/publications/iccv-15.pdf](http://www.cs.cityu.edu.hk/~qiyang/publications/iccv-15.pdf)\r\n\r\n**Learning Large-Scale Automatic Image Colorization**\r\n\r\n- paper: [http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Deshpande_Learning_Large-Scale_Automatic_ICCV_2015_paper.pdf](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Deshpande_Learning_Large-Scale_Automatic_ICCV_2015_paper.pdf)\r\n\r\n**Learning Representations for Automatic Colorization**\r\n\r\n![](http://people.cs.uchicago.edu/~larsson/colorization/overview.png)\r\n\r\n- homepage: [http://people.cs.uchicago.edu/~larsson/colorization/](http://people.cs.uchicago.edu/~larsson/colorization/)\r\n- arxiv: [http://arxiv.org/abs/1603.06668](http://arxiv.org/abs/1603.06668)\r\n- github: [https://github.com/gustavla/autocolorize](https://github.com/gustavla/autocolorize)\r\n\r\n**Colorful Image Colorization**\r\n\r\n![](http://richzhang.github.io/colorization/resources/images/net_diagram.jpg)\r\n\r\n- intro: ECCV 2016\r\n- project page: [http://richzhang.github.io/colorization/](http://richzhang.github.io/colorization/)\r\n- arxiv: [http://arxiv.org/abs/1603.08511](http://arxiv.org/abs/1603.08511)\r\n- github: [https://github.com/richzhang/colorization](https://github.com/richzhang/colorization)\r\n- demo: [http://demos.algorithmia.com/colorize-photos/](http://demos.algorithmia.com/colorize-photos/)\r\n- github: [https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Colorful](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Colorful)\r\n github(Tensorflow): [https://github.com/nilboy/colorization-tf](https://github.com/nilboy/colorization-tf)\r\n\r\n **Colorising Black & White Photos using Deep Learning**\r\n\r\n [https://hackernoon.com/colorising-black-white-photos-using-deep-learning-4da22a05f531](https://hackernoon.com/colorising-black-white-photos-using-deep-learning-4da22a05f531)\r\n\r\n - - -\r\n\r\n**Automatic Colorization (Tensorflow + VGG)**\r\n\r\n![](http://tinyclouds.org/colorize/best/6.jpg)\r\n\r\n- blog: [http://tinyclouds.org/colorize/](http://tinyclouds.org/colorize/)\r\n\r\n**colornet: Neural Network to colorize grayscale images**\r\n\r\n[https://github.com/pavelgonchar/colornet](https://github.com/pavelgonchar/colornet)\r\n\r\n**Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification (SIGGRAPH 2016)**\r\n\r\n![](http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/images/model.png)\r\n\r\n- homepage: [http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/](http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/)\r\n- paper: [http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf](http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf)\r\n- github(Torch7): [https://github.com/satoshiiizuka/siggraph2016_colorization](https://github.com/satoshiiizuka/siggraph2016_colorization)\r\n\r\n**Convolutional autoencoder to colorize greyscale images**\r\n\r\n- github: [https://github.com/YerevaNN/neural-colorizer](https://github.com/YerevaNN/neural-colorizer)\r\n\r\n**Image-Color: A deep learning approach to colorizing images**\r\n\r\n- github: [https://github.com/cameronfabbri/Colorful-Image-Colorization](https://github.com/cameronfabbri/Colorful-Image-Colorization)\r\n\r\n**Creating an artificial artist: Color your photos using Neural Networks**\r\n\r\n- blog: [https://www.analyticsvidhya.com/blog/2016/11/creating-an-artificial-artist-color-your-photos-using-neural-networks/](https://www.analyticsvidhya.com/blog/2016/11/creating-an-artificial-artist-color-your-photos-using-neural-networks/)\r\n\r\n**Paints Chainer: line drawing colorization using chainer**\r\n\r\n- github: [https://github.com/pfnet/PaintsChainer](https://github.com/pfnet/PaintsChainer)\r\n- demo: [http://paintschainer.preferred.tech/](http://paintschainer.preferred.tech/)\r\n\r\n**Unsupervised Diverse Colorization via Generative Adversarial Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1702.06674](https://arxiv.org/abs/1702.06674)\r\n\r\n**(DE)^2 CO: Deep Depth Colorization**\r\n\r\n[https://arxiv.org/abs/1703.10881](https://arxiv.org/abs/1703.10881)\r\n\r\n**A Neural Representation of Sketch Drawings**\r\n\r\n- intro: Google Brain\r\n- arxiv: [https://arxiv.org/abs/1704.03477](https://arxiv.org/abs/1704.03477)\r\n\r\n**Real-Time User-Guided Image Colorization with Learned Deep Priors**\r\n\r\n- intro: SIGGRAPH 2017\r\n- project page: [https://richzhang.github.io/ideepcolor/](https://richzhang.github.io/ideepcolor/)\r\n- arxiv: [https://arxiv.org/abs/1705.02999](https://arxiv.org/abs/1705.02999)\r\n- github(official, Caffe): [https://github.com/junyanz/interactive-deep-colorization](https://github.com/junyanz/interactive-deep-colorization)\r\n\r\n**PixColor: Pixel Recursive Colorization**\r\n\r\n- intro: Google Research\r\n- arxiv: [https://arxiv.org/abs/1705.07208](https://arxiv.org/abs/1705.07208)\r\n\r\n**cGAN-based Manga Colorization Using a Single Training Image**\r\n\r\n- intro: University of Tokyo\r\n- arxiv: [https://arxiv.org/abs/1706.06918](https://arxiv.org/abs/1706.06918)\r\n\r\n**Interactive Deep Colorization With Simultaneous Global and Local Inputs**\r\n\r\n[https://arxiv.org/abs/1801.09083](https://arxiv.org/abs/1801.09083)\r\n\r\n**Image Colorization with Generative Adversarial Networks**\r\n\r\n[https://arxiv.org/abs/1803.05400](https://arxiv.org/abs/1803.05400)\r\n\r\n**Learning to Color from Language**\r\n\r\n- intro: Allen Institute of Artificial Intelligence & University of Massachusetts\r\n- arxiv: [https://arxiv.org/abs/1804.06026](https://arxiv.org/abs/1804.06026)\r\n\r\n**Deep Exemplar-based Colorization**\r\n\r\n- intro: Siggraph 2018\r\n- arxiv: [https://arxiv.org/abs/1807.06587](https://arxiv.org/abs/1807.06587)\r\n\r\n**Pixel-level Semantics Guided Image Colorization**\r\n\r\n[https://arxiv.org/abs/1808.01597](https://arxiv.org/abs/1808.01597)\r\n\r\n**User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks**\r\n\r\n- intro: 2018 ACM Multimedia Conference (MM '18)\r\n- arxiv: [https://arxiv.org/abs/1808.03240](https://arxiv.org/abs/1808.03240)\r\n\r\n**Pixelated Semantic Colorization**\r\n\r\n[https://arxiv.org/abs/1901.10889](https://arxiv.org/abs/1901.10889)\r\n\r\n**Colorization Transformer**\r\n\r\n- intro: ICLR 2021\r\n- intro: Google Research, Brain Team\r\n- arxiv: [https://arxiv.org/abs/2102.04432](https://arxiv.org/abs/2102.04432)\r\n- openreview: [https://openreview.net/forum?id=5NA1PinlGFu](https://openreview.net/forum?id=5NA1PinlGFu)\r\n- github: [https://github.com/google-research/google-research/tree/master/coltran](https://github.com/google-research/google-research/tree/master/coltran)\r\n\r\n# Sounds\r\n\r\n**Visually Indicated Sounds**\r\n\r\n![](http://vis.csail.mit.edu/pipeline.jpg)\r\n\r\n- project page: [http://vis.csail.mit.edu/](http://vis.csail.mit.edu/)\r\n- arxiv: [http://arxiv.org/abs/1512.08512](http://arxiv.org/abs/1512.08512)\r\n\r\n# Music\r\n\r\n**GRUV: Algorithmic Music Generation using Recurrent Neural Networks**\r\n\r\n- github: [https://github.com/MattVitelli/GRUV](https://github.com/MattVitelli/GRUV)\r\n\r\n**DeepHear - Composing and harmonizing music with neural networks**\r\n\r\n- website: [http://web.mit.edu/felixsun/www/neural-music.html](http://web.mit.edu/felixsun/www/neural-music.html)\r\n- github: [https://github.com/fephsun/neuralnetmusic](https://github.com/fephsun/neuralnetmusic)\r\n\r\n**Using AutoHarp and a Character-Based RNN to Create MIDI Drum Loops**\r\n\r\n- website: [http://www.inquisitivists.com/2015/09/16/using-autoharp-and-a-character-based-rnn-to-create-midi-drum-loops](http://www.inquisitivists.com/2015/09/16/using-autoharp-and-a-character-based-rnn-to-create-midi-drum-loops)\r\n\r\n**Musical Audio Synthesis Using Autoencoding Neural Nets**\r\n\r\n- paper: [http://www.cs.dartmouth.edu/~sarroff/papers/sarroff2014a.pdf](http://www.cs.dartmouth.edu/~sarroff/papers/sarroff2014a.pdf)\r\n- github: [https://github.com/woodshop/deepAutoController/tree/icmc_smc_2014](https://github.com/woodshop/deepAutoController/tree/icmc_smc_2014)\r\n- video: [https://vimeo.com/121827215](https://vimeo.com/121827215)\r\n\r\n**sound-rnn: Generating sound using recurrent neural networks**\r\n\r\n- github: [https://github.com/johnglover/sound-rnn](https://github.com/johnglover/sound-rnn)\r\n- blog: [http://www.johnglover.net/blog/generating-sound-with-rnns.html](http://www.johnglover.net/blog/generating-sound-with-rnns.html)\r\n\r\n**Using LSTM Recurrent Neural Networks for Music Generation (Project for AI Prac Fall 2015 at Cornell)**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=aSr8_QQYpYM](https://www.youtube.com/watch?v=aSr8_QQYpYM)\r\n- video: [http://video.weibo.com/show?fid=1034:4be01d679bb1a68a634fe0f589caa779](http://video.weibo.com/show?fid=1034:4be01d679bb1a68a634fe0f589caa779)\r\n\r\n**Visually Indicated Sounds (MIT. 2015)**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.08512](http://arxiv.org/abs/1512.08512)\r\n\r\n**Training a Recurrent Neural Network to Compose Music**\r\n\r\n- blog: [https://maraoz.com/2016/02/02/abc-rnn/](https://maraoz.com/2016/02/02/abc-rnn/)\r\n\r\n**LSTM Realbook**\r\n\r\n![](https://keunwoochoi.files.wordpress.com/2016/02/band-in-a-box-195117-2.jpeg?w=788&h=474)\r\n\r\n- blog: [https://keunwoochoi.wordpress.com/2016/02/19/lstm-realbook/](https://keunwoochoi.wordpress.com/2016/02/19/lstm-realbook/)\r\n- github: [https://github.com/keunwoochoi/lstm_real_book](https://github.com/keunwoochoi/lstm_real_book)\r\n\r\n**LSTMetallica: Generation drum tracks by learning the drum tracks of 60 Metallica songs**\r\n\r\n- blog: [https://keunwoochoi.wordpress.com/2016/02/23/lstmetallica/](https://keunwoochoi.wordpress.com/2016/02/23/lstmetallica/)\r\n\r\n**deepjazz: Deep learning driven jazz generation using Keras & Theano!**\r\n\r\n- homepage: [https://jisungk.github.io/deepjazz/](https://jisungk.github.io/deepjazz/)\r\n- github：[https://github.com/jisungk/deepjazz](https://github.com/jisungk/deepjazz)\r\n\r\n**Magenta: Music and Art Generation with Machine Intelligence**\r\n\r\n- homepage: [http://magenta.tensorflow.org/](http://magenta.tensorflow.org/)\r\n- github: [https://github.com/tensorflow/magenta](https://github.com/tensorflow/magenta)\r\n\r\n**Music Transcription with Convolutional Neural Networks**\r\n\r\n- blog: [https://www.lunaverus.com/cnn](https://www.lunaverus.com/cnn)\r\n- download: [https://www.lunaverus.com/download](https://www.lunaverus.com/download)\r\n\r\n**Long Short-Term Memory Recurrent Neural Network Architectures for Generating Music and Japanese Lyrics**\r\n\r\n- paper: [http://cslab1.bc.edu/~csacademics/pdf/16Mikami.pdf](http://cslab1.bc.edu/~csacademics/pdf/16Mikami.pdf)\r\n- github: [https://github.com/mikamia](https://github.com/mikamia)\r\n\r\n**BachBot: Use deep learning to generate and harmonize music in the style of Bach**\r\n\r\n- intro: BachBot is a research project utilizing long short term memory (LSTMs) to generate Bach compositions\r\n- homepage: [http://bachbot.com/](http://bachbot.com/)\r\n- github: [https://github.com/feynmanliang/bachbot](https://github.com/feynmanliang/bachbot)\r\n\r\n**Generate Music in TensorFlow**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=ZE7qWXX05T0](https://www.youtube.com/watch?v=ZE7qWXX05T0)\r\n- github: [https://github.com/llSourcell/Music_Generator_Demo](https://github.com/llSourcell/Music_Generator_Demo)\r\n\r\n**Generate new lyrics in the style of any artist using LSTMs and TensorFlow**\r\n\r\n- github: [https://github.com/dyelax/encore.ai](https://github.com/dyelax/encore.ai)\r\n\r\n**sound-GAN: Generative Adversial Network for music composition**\r\n\r\n- github: [https://github.com/jacotar/sound-GAN](https://github.com/jacotar/sound-GAN)\r\n\r\n**Analyzing Six Deep Learning Tools for Music Generation**\r\n\r\n- intro: Magenta / DeepJazz / BachBot / FlowMachines / WaveNet / GRUV\r\n- blog: [http://www.asimovinstitute.org/notes-vs-waves/](http://www.asimovinstitute.org/notes-vs-waves/)\r\n\r\n**WIMP2: Creating Music with AI: Highlights of Current Research**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=5K4hn6cBUPU](https://www.youtube.com/watch?v=5K4hn6cBUPU)\r\n\r\n**Song From PI: A Musically Plausible Network for Pop Music Generation**\r\n\r\n- paper: [http://openreview.net/pdf?id=ByBwSPcex](http://openreview.net/pdf?id=ByBwSPcex)\r\n- project page: [http://www.cs.toronto.edu/songfrompi/](http://www.cs.toronto.edu/songfrompi/)\r\n\r\n**Grammar Argumented LSTM Neural Networks with Note-Level Encoding for Music Composition**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.05416](https://arxiv.org/abs/1611.05416)\r\n\r\n**用TensorFlow生成周杰伦歌词**\r\n\r\n- blog: [http://leix.me/2016/11/28/tensorflow-lyrics-generation/](http://leix.me/2016/11/28/tensorflow-lyrics-generation/)\r\n- github: [https://github.com/leido/char-rnn-cn](https://github.com/leido/char-rnn-cn)\r\n\r\n**Hip-Hop - Generating lyrics with RNNs**\r\n\r\n- blog: [http://affinelayer.com/hiphop/index.html](http://affinelayer.com/hiphop/index.html)\r\n\r\n**Metis Final Project: Music Composition with LSTMs**\r\n\r\n[http://blog.naoya.io/metis-final-project-music-composition-with-lstms/](http://blog.naoya.io/metis-final-project-music-composition-with-lstms/)\r\n\r\n**Neural Translation of Musical Style**\r\n\r\n- blog: [http://imanmalik.com/cs/2017/06/05/neural-style.html](http://imanmalik.com/cs/2017/06/05/neural-style.html)\r\n- github: [https://github.com/imalikshake/StyleNet](https://github.com/imalikshake/StyleNet)\r\n\r\n# Poetry\r\n\r\n**NeuralSnap: Generates poetry from images using convolutional and recurrent neural networks**\r\n\r\n- github: [https://github.com/rossgoodwin/neuralsnap](https://github.com/rossgoodwin/neuralsnap)\r\n\r\n**Generating Chinese Classical Poems with RNN Encoder-Decoder**\r\n\r\n- arxiv: [http://arxiv.org/abs/1604.01537](http://arxiv.org/abs/1604.01537)\r\n\r\n**Chinese Poetry Generation with Planning based Neural Network**\r\n\r\n- intro: COLING 2016. University of Science and Technology of China & Baidu\r\n- arxiv: [https://arxiv.org/abs/1610.09889](https://arxiv.org/abs/1610.09889)\r\n- blog: [http://freecoder.me/archives/213.html](http://freecoder.me/archives/213.html)\r\n\r\n# Weiqi (Go)\r\n\r\n**Teaching Deep Convolutional Neural Networks to Play Go**\r\n\r\n- arxiv: [http://arxiv.org/abs/1412.3409](http://arxiv.org/abs/1412.3409)\r\n- demo: [https://chrisc36.github.io/deep-go/](https://chrisc36.github.io/deep-go/)\r\n\r\n**Move Evaluation in Go Using Deep Convolutional Neural Networks(Google DeepMind, Google Brain)**\r\n\r\n- arxiv: [http://arxiv.org/abs/1412.6564](http://arxiv.org/abs/1412.6564)\r\n\r\n**Training Deep Convolutional Neural Networks to Play Go**\r\n\r\n- paper: [http://jmlr.org/proceedings/papers/v37/clark15.pdf](http://jmlr.org/proceedings/papers/v37/clark15.pdf)\r\n\r\n**Computer Go Research - The Challenges Ahead (Martin Müller. IEEE CIG 2015)**\r\n\r\n- homepage: [https://webdocs.cs.ualberta.ca/~mmueller/talks.html](https://webdocs.cs.ualberta.ca/~mmueller/talks.html)\r\n\r\n**GoCNN: Using CNN for Go (Weiqi/Baduk) board evaluation with tensorflow**\r\n\r\n- github: [https://github.com/jmgilmer/GoCNN](https://github.com/jmgilmer/GoCNN)\r\n\r\n**DarkGo: Go in Darknet**\r\n\r\n- homepage: [http://pjreddie.com/darknet/darkgo-go-in-darknet/](http://pjreddie.com/darknet/darkgo-go-in-darknet/)\r\n\r\n**BetaGo: Go bots for the people**\r\n\r\n- homepage: [http://maxpumperla.github.io/betago/](http://maxpumperla.github.io/betago/)\r\n- github: [https://github.com/maxpumperla/betago](https://github.com/maxpumperla/betago)\r\n\r\n**Deep Learning and the Game of Go**\r\n\r\n- book: [https://www.manning.com/books/deep-learning-and-the-game-of-go](https://www.manning.com/books/deep-learning-and-the-game-of-go)\r\n- github: [https://github.com//maxpumperla/deep_learning_and_the_game_of_go](https://github.com//maxpumperla/deep_learning_and_the_game_of_go)\r\n\r\n## DarkForest\r\n\r\n**Better Computer Go Player with Neural Network and Long-term Prediction (Facebook AI Research)**\r\n\r\n![](https://raw.githubusercontent.com/facebookresearch/darkforestGo/master/figure.png)\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.06410](http://arxiv.org/abs/1511.06410)\r\n- github: [https://github.com/facebookresearch/darkforestGo](https://github.com/facebookresearch/darkforestGo)\r\n- MIT tech review: [http://www.technologyreview.com/view/544181/how-facebooks-ai-researchers-built-a-game-changing-go-engine/](http://www.technologyreview.com/view/544181/how-facebooks-ai-researchers-built-a-game-changing-go-engine/)\r\n\r\n## AlphaGo\r\n\r\n**Mastering the game of Go with deep neural networks and tree search**\r\n\r\n![](http://k.sinaimg.cn/n/sports/transform/20160128/RGVK-fxnzanh0214327.jpg/w570778.jpg)\r\n\r\n- intro: AlphaGo. Google DeepMind\r\n- homepage: [http://www.deepmind.com/alpha-go.html](http://www.deepmind.com/alpha-go.html)\r\n- paper: [https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf](https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf)\r\n- naturep page: [http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)\r\n- paper: [https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf](https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf)\r\n- slides: [http://www.bioinfo.org.cn/~casp/temp/alphago_slides.pdf](http://www.bioinfo.org.cn/~casp/temp/alphago_slides.pdf)\r\n- blog: [http://www.furidamu.org/blog/2016/01/26/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search/](http://www.furidamu.org/blog/2016/01/26/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search/)\r\n- blog(\"AlphaGo: From Intuitive Learning to Holistic Knowledge\"): [https://caminao.wordpress.com/2016/02/01/alphago/](https://caminao.wordpress.com/2016/02/01/alphago/)\r\n- github: [https://github.com/Rochester-NRT/AlphaGo](https://github.com/Rochester-NRT/AlphaGo)\r\n\r\n**AlphaGo Teach**\r\n\r\n- intro: Let the AlphaGo Teaching Tool help you find new and creative ways of playing Go\r\n- homepage: [https://alphagoteach.deepmind.com/](https://alphagoteach.deepmind.com/)\r\n\r\n**AlphaGo的分析**\r\n\r\n- intro: by 田渊栋\r\n- blog: [http://zhuanlan.zhihu.com/yuandong/20607684](http://zhuanlan.zhihu.com/yuandong/20607684)\r\n\r\n**How Alphago Works**\r\n\r\n- slides: [http://www.slideshare.net/ShaneSeungwhanMoon/how-alphago-works](http://www.slideshare.net/ShaneSeungwhanMoon/how-alphago-works)\r\n- slides: [http://pan.baidu.com/s/1qXwagGW](http://pan.baidu.com/s/1qXwagGW)\r\n\r\n**AlphaGo in Depth**\r\n\r\n- intro: by Mark Chang\r\n- slides: [http://www.slideshare.net/ckmarkohchang/alphago-in-depth?qid=283ab3bc-7d04-4e14-a205-b0b671ca4099](http://www.slideshare.net/ckmarkohchang/alphago-in-depth?qid=283ab3bc-7d04-4e14-a205-b0b671ca4099)\r\n- mirror: [https://pan.baidu.com/s/1i5JNeRj](https://pan.baidu.com/s/1i5JNeRj)\r\n\r\n**Leela**\r\n\r\n- intro: Leela is a strong Go playing program combining advances in Go programming and \r\nfurther original research into a small, easy to use graphical interface.\r\n- homepage: [https://sjeng.org/leela.html](https://sjeng.org/leela.html)\r\n\r\n**Mastering the game of Go without human knowledge**\r\n\r\n- nature page: [http://www.nature.com/nature/journal/v550/n7676/full/nature24270.html](http://www.nature.com/nature/journal/v550/n7676/full/nature24270.html)\r\n- paper: [https://deepmind.com/documents/119/agz_unformatted_nature.pdf](https://deepmind.com/documents/119/agz_unformatted_nature.pdf)\r\n- notes: [https://blog.acolyer.org/2017/11/17/mastering-the-game-of-go-without-human-knowledge/](https://blog.acolyer.org/2017/11/17/mastering-the-game-of-go-without-human-knowledge/)\r\n\r\n**Computer Go & AlphaGo Zero**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=6fKG4wJ7uBk](https://www.youtube.com/watch?v=6fKG4wJ7uBk)\r\n- mirror: [https://www.bilibili.com/video/av16428694/](https://www.bilibili.com/video/av16428694/)\r\n- slides: [https://drive.google.com/file/d/1rmUyIitEmAtMUKdKEnlHRfmXtpyoxxey/view](https://drive.google.com/file/d/1rmUyIitEmAtMUKdKEnlHRfmXtpyoxxey/view)\r\n\r\n**AlphaZero: Mastering Games without Human Knowledge - NIPS 2017**\r\n\r\n- intro: Keynote by David Silver on AlphaGo, AlphaGo Zero and AlphaZero, at the 2017 NIPS Deep Reinforcement Learning Symposium, 6 Dec, Long Beach, CA\r\n- youtube: [https://www.youtube.com/watch?v=A3ekFcZ3KNw](https://www.youtube.com/watch?v=A3ekFcZ3KNw)\r\n- mirror: [https://www.bilibili.com/video/av17210816/](https://www.bilibili.com/video/av17210816/)\r\n\r\n**PhoenixGo**\r\n\r\n- intro: Go AI program which implement the AlphaGo Zero paper\r\n- github: [https://github.com/Tencent/PhoenixGo](https://github.com/Tencent/PhoenixGo)\r\n\r\n**The future is here – AlphaZero learns chess**\r\n\r\n[https://en.chessbase.com/post/the-future-is-here-alphazero-learns-chess](https://en.chessbase.com/post/the-future-is-here-alphazero-learns-chess)\r\n\r\n**AlphaGo Zero Cheat Sheet**\r\n\r\n[https://applied-data.science/static/main/res/alpha_go_zero_cheat_sheet.png](https://applied-data.science/static/main/res/alpha_go_zero_cheat_sheet.png)\r\n\r\n# Chess\r\n\r\n**Giraffe: Using Deep Reinforcement Learning to Play Chess**\r\n\r\n- intro: MSc thesis\r\n- arxiv: [http://arxiv.org/abs/1509.01549](http://arxiv.org/abs/1509.01549)\r\n\r\n**Spawkfish: neural network based chess engine**\r\n\r\n- homepage: [http://spawk.fish/](http://spawk.fish/)\r\n\r\n**Chess position evaluation with convolutional neural network in Julia**\r\n\r\n- blog: [http://int8.io/chess-position-evaluation-with-convolutional-neural-networks-in-julia/](http://int8.io/chess-position-evaluation-with-convolutional-neural-networks-in-julia/)\r\n\r\n**Deep Learning for ... Chess**\r\n\r\n- blog: [http://blog.yhat.com/posts/deep-learning-chess.html](http://blog.yhat.com/posts/deep-learning-chess.html)\r\n- github: [https://github.com/erikbern/deep-pink](https://github.com/erikbern/deep-pink)\r\n\r\n**DeepChess: End-to-End Deep Neural Network for Automatic Learning in Chess**\r\n\r\n- intro: Winner of Best Paper Award in ICANN 2016\r\n- arxiv: [https://arxiv.org/abs/1711.09667](https://arxiv.org/abs/1711.09667)\r\n- paper: [http://www.cs.tau.ac.il/~wolf/papers/deepchess.pdf](http://www.cs.tau.ac.il/~wolf/papers/deepchess.pdf)\r\n- github: [https://github.com/mr-press/DeepChess](https://github.com/mr-press/DeepChess)\r\n\r\n**Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm**\r\n\r\n- intro: DeepMind\r\n- arxiv: [https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815)\r\n\r\n# Game\r\n\r\n**Learning Game of Life with a Convolutional Neural Network**\r\n\r\n- blog: [http://danielrapp.github.io/cnn-gol/](http://danielrapp.github.io/cnn-gol/)\r\n- github: [https://github.com/DanielRapp/cnn-gol](https://github.com/DanielRapp/cnn-gol)\r\n\r\n**Reinforcement Learning using Tensor Flow: A deep Q learning demonstration using Google Tensorflow**\r\n\r\n![](https://raw.githubusercontent.com/nivwusquorum/tensorflow-deepq/master/data/example.gif)\r\n\r\n- github: [https://github.com/nivwusquorum/tensorflow-deepq](https://github.com/nivwusquorum/tensorflow-deepq)\r\n\r\n**Poker-CNN: A Pattern Learning Strategy for Making Draws and Bets in Poker Games Using Convolutional Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1509.06731](http://arxiv.org/abs/1509.06731)\r\n- paper: [http://colinraffel.com/publications/aaai2016poker.pdf](http://colinraffel.com/publications/aaai2016poker.pdf)\r\n- github: [https://github.com/moscow25/deep_draw](https://github.com/moscow25/deep_draw)\r\n- slides: [https://drive.google.com/file/d/0B5eOIUHA0khiMjN1YnEtZHMwams/view](https://drive.google.com/file/d/0B5eOIUHA0khiMjN1YnEtZHMwams/view)\r\n- slides: [http://pan.baidu.com/s/1nu5zpZ7](http://pan.baidu.com/s/1nu5zpZ7)\r\n\r\n**TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games**\r\n\r\n- intro: Connecting Torch to StarCraft\r\n- arxiv: [https://arxiv.org/abs/1611.00625](https://arxiv.org/abs/1611.00625)\r\n- github: [https://github.com/TorchCraft/TorchCraft](https://github.com/TorchCraft/TorchCraft)\r\n\r\n**BlizzCon 2016 DeepMind and StarCraft II Deep Learning Panel Transcript**\r\n\r\n- part 1: [http://starcraft.blizzplanet.com/blog/comments/blizzcon-2016-deepmind-and-starcraft-ii-deep-learning-panel-transcript](http://starcraft.blizzplanet.com/blog/comments/blizzcon-2016-deepmind-and-starcraft-ii-deep-learning-panel-transcript)\r\n- part 2: [http://starcraft.blizzplanet.com/blog/comments/blizzcon-2016-deepmind-and-starcraft-ii-deep-learning-panel-transcript/2](http://starcraft.blizzplanet.com/blog/comments/blizzcon-2016-deepmind-and-starcraft-ii-deep-learning-panel-transcript/2)\r\n\r\n**DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.01724](https://arxiv.org/abs/1701.01724)\r\n- github: [https://github.com/lifrordi/DeepStack-Leduc](https://github.com/lifrordi/DeepStack-Leduc)\r\n\r\n**Gym StarCraft: StarCraft environment for OpenAI Gym, based on Facebook's TorchCraft**\r\n\r\n- intro: Gym StarCraft is an environment bundle for OpenAI Gym. \r\nIt is based on Facebook's TorchCraft, which is a bridge between Torch and StarCraft for AI research.\r\n- github: [https://github.com/deepcraft/gym-starcraft](https://github.com/deepcraft/gym-starcraft)\r\n\r\n**Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games**\r\n\r\n[https://arxiv.org/abs/1703.10069](https://arxiv.org/abs/1703.10069)\r\n\r\n**Learning Macromanagement in StarCraft from Replays using Deep Learning**\r\n\r\n- intro: CIG 2017. IT University of Copenhagen\r\n- arxiv: [https://arxiv.org/abs/1707.03743](https://arxiv.org/abs/1707.03743)\r\n\r\n**Multi-platform Version of StarCraft: Brood War in a Docker Container: Technical Report**\r\n\r\n- intro: Czech Technical University in Prague\r\n- arxiv: [https://arxiv.org/abs/1801.02193](https://arxiv.org/abs/1801.02193)\r\n- gihtub: [https://github.com/Games-and-Simulations/sc-docker](https://github.com/Games-and-Simulations/sc-docker)\r\n\r\n**Macro action selection with deep reinforcement learning in StarCraft**\r\n\r\n- intro: Bilibili & Nanjing University\r\n- arxiv: [https://arxiv.org/abs/1812.00336](https://arxiv.org/abs/1812.00336)\r\n- github: [https://github.com/Bilibili/LastOrder](https://github.com/Bilibili/LastOrder)\r\n\r\n## DeepLeague\r\n\r\n**DeepLeague: leveraging computer vision and deep learning on the League of Legends mini map + giving away a dataset of over 100,000 labeled images to further esports analytics research**\r\n\r\n- blog: [https://medium.com/@farzatv/deepleague-leveraging-computer-vision-and-deep-learning-on-the-league-of-legends-mini-map-giving-d275fd17c4e0](https://medium.com/@farzatv/deepleague-leveraging-computer-vision-and-deep-learning-on-the-league-of-legends-mini-map-giving-d275fd17c4e0)\r\n\r\n**DeepLeague (Part 2): The Technical Details**\r\n\r\n- blog: [https://medium.com/@farzatv/deepleague-part-2-the-technical-details-374439e7e09a](https://medium.com/@farzatv/deepleague-part-2-the-technical-details-374439e7e09a)\r\n- github: [https://github.com/farzaa/DeepLeague](https://github.com/farzaa/DeepLeague)\r\n\r\n# Courses\r\n\r\n**Learning Machines**\r\n\r\n[http://www.patrickhebron.com/learning-machines/](http://www.patrickhebron.com/learning-machines/)\r\n\r\n**Learning Bit by Bit**\r\n\r\n[https://itp.nyu.edu/varwiki/Syllabus/LearningBitbyBitS10](https://itp.nyu.edu/varwiki/Syllabus/LearningBitbyBitS10)\r\n\r\n**MACHINE LEARNING FOR MUSICIANS AND ARTISTS (Course opens January 2016)**\r\n\r\n[https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info](https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info)\r\n\r\n**Machine learning for artists @ ITP-NYU, Spring 2016**\r\n\r\n![](http://www.kdnuggets.com/wp-content/uploads/mona-lisa.jpg)\r\n\r\n- videos/lectures/course notes: [http://ml4a.github.io/classes/itp-S16/]([http://ml4a.github.io/classes/itp-S16/])\r\n- index: [http://ml4a.github.io/index/](http://ml4a.github.io/index/)\r\n- github: [https://github.com/ml4a/ml4a.github.io](https://github.com/ml4a/ml4a.github.io)\r\n- notes: [http://www.kdnuggets.com/2016/04/machine-learning-artists-video-lectures-notes.html](http://www.kdnuggets.com/2016/04/machine-learning-artists-video-lectures-notes.html)\r\n- blog: [https://medium.com/@genekogan/machine-learning-for-artists-e93d20fdb097#.25w95beqb](https://medium.com/@genekogan/machine-learning-for-artists-e93d20fdb097#.25w95beqb)\r\n\r\n**Machine Learning for Artists @ OpenDot, November 2016**\r\n\r\n- homepage: [http://ml4a.github.io/classes/opendot/](http://ml4a.github.io/classes/opendot/)\r\n\r\n**The Neural Aesthetic @ SchoolOfMa, Summer 2016**\r\n\r\n[http://ml4a.github.io/classes/neural-aesthetic/](http://ml4a.github.io/classes/neural-aesthetic/)\r\n\r\n# Blogs\r\n\r\n**Review of machine / deep learning in an artistic context**\r\n\r\n[https://medium.com/@memoakten/machine-deep-learning-in-an-artistic-context-441f28774bcc#.gegpq99ag](https://medium.com/@memoakten/machine-deep-learning-in-an-artistic-context-441f28774bcc#.gegpq99ag)\r\n\r\n**Apprentice Work**\r\n\r\n![](https://d267cvn3rvuq91.cloudfront.net/i/images/gayfordx2079.jpg?sw=590&cx=93&cy=28&cw=1892&ch=2522)\r\n\r\n[https://www.technologyreview.com/s/600762/apprentice-work/](https://www.technologyreview.com/s/600762/apprentice-work/)\r\n\r\n**Exploring the Intersection of Art and Machine Intelligence**\r\n\r\n![](https://2.bp.blogspot.com/-RMPIwkAonnI/VstG8b2VZrI/AAAAAAAAA4c/8yYzUt2HF4g/s1600/image02.png)\r\n\r\n[http://googleresearch.blogspot.jp/2016/02/exploring-intersection-of-art-and.html](http://googleresearch.blogspot.jp/2016/02/exploring-intersection-of-art-and.html)\r\n\r\n**Using machine learning to generate music**\r\n\r\n![](http://api.ning.com/files/AOzcN6l-SPr7b3kjg*PpWCTfYlK36W6nG2KcQswQ4YcxWwLfUnqzTUJReEyVoBJtX*4vbP-d19qoLm2TBspkdZ9ZQ40Z1Pb7/maxresdefault.jpg?width=450)\r\n\r\n[http://www.datasciencecentral.com/profiles/blogs/using-machine-learning-to-generate-music](http://www.datasciencecentral.com/profiles/blogs/using-machine-learning-to-generate-music)\r\n\r\n**art in the age of machine intelligence**\r\n\r\n[https://medium.com/artists-and-machine-intelligence/what-is-ami-ccd936394a83#.hyt4ei9a9](https://medium.com/artists-and-machine-intelligence/what-is-ami-ccd936394a83#.hyt4ei9a9)\r\n\r\n**Understanding Aesthetics with Deep Learning**\r\n\r\n![](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/02/faces_CNNs.jpg)\r\n\r\n[https://devblogs.nvidia.com/parallelforall/understanding-aesthetics-deep-learning/](https://devblogs.nvidia.com/parallelforall/understanding-aesthetics-deep-learning/)\r\n\r\n**Go, Marvin Minsky, and the Chasm that AI Hasn’t Yet Crossed**\r\n\r\nblog: [https://medium.com/backchannel/has-deepmind-really-passed-go-adc85e256bec#.inx8nfid0](https://medium.com/backchannel/has-deepmind-really-passed-go-adc85e256bec#.inx8nfid0)\r\n\r\n**A Return to Machine Learning**\r\n\r\n- intro: This post is aimed at artists and other creative people who are interested in a survey of recent developments in machine learning research that intersect with art and culture.\r\n- blog: [https://medium.com/@kcimc/a-return-to-machine-learning-2de3728558eb#.bp2b1ax2x](https://medium.com/@kcimc/a-return-to-machine-learning-2de3728558eb#.bp2b1ax2x)\r\n\r\n# Resources\r\n\r\n**Music, Art and Machine Intelligence Workshop 2016**\r\n\r\n- youtube: [https://www.youtube.com/playlist?list=PL8h7Hk91ScJ2NKFS_i8y4DYk4Zg1LHn0a](https://www.youtube.com/playlist?list=PL8h7Hk91ScJ2NKFS_i8y4DYk4Zg1LHn0a)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","title":"Generative Adversarial Networks"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Generative Adversarial Networks\r\ndate: 2015-10-09\r\n---\r\n\r\n**Generative Adversarial Networks**\r\n\r\n**Generative Adversarial Nets**\r\n\r\n- arxiv: [http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661)\r\n- paper: [https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\r\n- github: [https://github.com/goodfeli/adversarial](https://github.com/goodfeli/adversarial)\r\n- github: [https://github.com/aleju/cat-generator](https://github.com/aleju/cat-generator)\r\n\r\n**Adversarial Feature Learning**\r\n\r\n- intro: ICLR 2017\r\n- arxiv: [https://arxiv.org/abs/1605.09782](https://arxiv.org/abs/1605.09782)\r\n- github: [https://github.com/jeffdonahue/bigan](https://github.com/jeffdonahue/bigan)\r\n\r\n**Generative Adversarial Networks**\r\n\r\n- intro: by Ian Goodfellow, NIPS 2016 tutorial\r\n- arxiv: [https://arxiv.org/abs/1701.00160](https://arxiv.org/abs/1701.00160)\r\n- slides: [http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf](http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf)\r\n- mirror: [https://pan.baidu.com/s/1gfBNYW7](https://pan.baidu.com/s/1gfBNYW7)\r\n\r\n**Adversarial Examples and Adversarial Training**\r\n\r\n- intro: NIPS 2016, Ian Goodfellow OpenAI\r\n- slides: [http://www.iangoodfellow.com/slides/2016-12-9-AT.pdf](http://www.iangoodfellow.com/slides/2016-12-9-AT.pdf)\r\n\r\n**How to Train a GAN? Tips and tricks to make GANs work**\r\n\r\n- github: [https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks)\r\n\r\n**Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks**\r\n\r\n- intro: CatGAN\r\n- arxiv: [http://arxiv.org/abs/1511.06390](http://arxiv.org/abs/1511.06390)\r\n\r\n**Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks**\r\n\r\n- intro: DCGAN\r\n- arxiv: [http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434)\r\n- github: [https://github.com/jazzsaxmafia/dcgan_tensorflow](https://github.com/jazzsaxmafia/dcgan_tensorflow)\r\n- github: [https://github.com/Newmu/dcgan_code](https://github.com/Newmu/dcgan_code)\r\n- github: [https://github.com/mattya/chainer-DCGAN](https://github.com/mattya/chainer-DCGAN)\r\n- github: [https://github.com/soumith/dcgan.torch](https://github.com/soumith/dcgan.torch)\r\n- github: [https://github.com/carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)\r\n\r\n**InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets**\r\n\r\n- arxiv: [https://arxiv.org/abs/1606.03657](https://arxiv.org/abs/1606.03657)\r\n- github: [https://github.com/openai/InfoGAN](https://github.com/openai/InfoGAN)\r\n- github(Tensorflow): [https://github.com/buriburisuri/supervised_infogan](https://github.com/buriburisuri/supervised_infogan)\r\n\r\n**Learning Interpretable Latent Representations with InfoGAN: A tutorial on implementing InfoGAN in Tensorflow**\r\n\r\n- blog: [https://medium.com/@awjuliani/learning-interpretable-latent-representations-with-infogan-dd710852db46#.r0kur3aum](https://medium.com/@awjuliani/learning-interpretable-latent-representations-with-infogan-dd710852db46#.r0kur3aum)\r\n- github: [https://gist.github.com/awjuliani/c9ecd8b37d33d6855cd4ed9aa16ce89f#file-infogan-tutorial-ipynb](https://gist.github.com/awjuliani/c9ecd8b37d33d6855cd4ed9aa16ce89f#file-infogan-tutorial-ipynb)\r\n\r\n**Coupled Generative Adversarial Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1606.07536](https://arxiv.org/abs/1606.07536)\r\n\r\n**Energy-based Generative Adversarial Network**\r\n\r\n- intro: EBGAN\r\n- author: Junbo Zhao, Michael Mathieu, Yann LeCun\r\n- arxiv: [http://arxiv.org/abs/1609.03126](http://arxiv.org/abs/1609.03126)\r\n- github(Tensorflow): [https://github.com/buriburisuri/ebgan](https://github.com/buriburisuri/ebgan)\r\n\r\n**SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient**\r\n\r\n- github: [https://github.com/LantaoYu/SeqGAN](https://github.com/LantaoYu/SeqGAN)\r\n\r\n**Connecting Generative Adversarial Networks and Actor-Critic Methods**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.01945](https://arxiv.org/abs/1610.01945)\r\n\r\n**Generative Adversarial Nets from a Density Ratio Estimation Perspective**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.02920](https://arxiv.org/abs/1610.02920)\r\n\r\n**Unrolled Generative Adversarial Networks**\r\n\r\n- paper: [http://openreview.net/pdf?id=BydrOIcle](http://openreview.net/pdf?id=BydrOIcle)\r\n- github: [https://github.com/bstriner/keras-adversarial](https://github.com/bstriner/keras-adversarial)\r\n\r\n**Generative Adversarial Networks as Variational Training of Energy Based Models**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.01799](https://arxiv.org/abs/1611.01799)\r\n- github: [https://github.com/Shuangfei/vgan](https://github.com/Shuangfei/vgan)\r\n\r\n**Multi-class Generative Adversarial Networks with the L2 Loss Function**\r\n\r\n**Least Squares Generative Adversarial Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.04076](https://arxiv.org/abs/1611.04076)\r\n\r\n**Inverting The Generator Of A Generative Adversarial Networ**\r\n\r\n- intro: NIPS 2016 Workshop on Adversarial Training\r\n- arxiv: [https://arxiv.org/abs/1611.05644](https://arxiv.org/abs/1611.05644)\r\n\r\n**ml4a-invisible-cities**\r\n\r\n- project page: [https://opendot.github.io/ml4a-invisible-cities/](https://opendot.github.io/ml4a-invisible-cities/)\r\n- arxiv: [https://github.com/opendot/ml4a-invisible-cities](https://github.com/opendot/ml4a-invisible-cities)\r\n\r\n**Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.06430](https://arxiv.org/abs/1611.06430)\r\n\r\n**Associative Adversarial Networks**\r\n\r\n- intro: NIPS 2016 Workshop on Adversarial Training\r\n- arxiv: [https://arxiv.org/abs/1611.06953](https://arxiv.org/abs/1611.06953)\r\n\r\n**Temporal Generative Adversarial Nets**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.06624](https://arxiv.org/abs/1611.06624)\r\n\r\n**Handwriting Profiling using Generative Adversarial Networks**\r\n\r\n- intro: Accepted at The Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17 Student Abstract and Poster Program)\r\n- arxiv: [https://arxiv.org/abs/1611.08789](https://arxiv.org/abs/1611.08789)\r\n\r\n**C-RNN-GAN: Continuous recurrent neural networks with adversarial training**\r\n\r\n- intro: Constructive Machine Learning Workshop (CML) at NIPS 2016\r\n- project page: [http://mogren.one/publications/2016/c-rnn-gan/](http://mogren.one/publications/2016/c-rnn-gan/)\r\n- arxiv: [https://arxiv.org/abs/1611.09904](https://arxiv.org/abs/1611.09904)\r\n- github: [https://github.com/olofmogren/c-rnn-gan](https://github.com/olofmogren/c-rnn-gan)\r\n\r\n**Ensembles of Generative Adversarial Networks**\r\n\r\n- intro: NIPS 2016 Workshop on Adversarial Training\r\n- arxiv: [https://arxiv.org/abs/1612.00991](https://arxiv.org/abs/1612.00991)\r\n\r\n**Improved generator objectives for GANs**\r\n\r\n- intro: NIPS 2016 Workshop on Adversarial Training\r\n- arxiv: [https://arxiv.org/abs/1612.02780](https://arxiv.org/abs/1612.02780)\r\n\r\n**Stacked Generative Adversarial Networks**\r\n\r\n- intro: SGAN\r\n- arxiv: [https://arxiv.org/abs/1612.04357](https://arxiv.org/abs/1612.04357)\r\n- github: [https://github.com/xunhuang1995/SGAN](https://github.com/xunhuang1995/SGAN)\r\n\r\n**Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks**\r\n\r\n- intro: Google Brain & Google Research\r\n- arxiv: [https://arxiv.org/abs/1612.05424](https://arxiv.org/abs/1612.05424)\r\n\r\n**AdaGAN: Boosting Generative Models**\r\n\r\n- intro: Max Planck Institute for Intelligent Systems & Google Brain\r\n- arxiv: [https://arxiv.org/abs/1701.02386](https://arxiv.org/abs/1701.02386)\r\n\r\n**Towards Principled Methods for Training Generative Adversarial Networks**\r\n\r\n- intro: Courant Institute of Mathematical Sciences & Facebook AI Research\r\n- arxiv: [https://arxiv.org/abs/1701.04862](https://arxiv.org/abs/1701.04862)\r\n\r\n**Wasserstein GAN**\r\n\r\n- intro: Courant Institute of Mathematical Sciences & Facebook AI Research\r\n- arxiv: [https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)\r\n- github: [https://github.com/martinarjovsky/WassersteinGAN](https://github.com/martinarjovsky/WassersteinGAN)\r\n- github: [https://github.com/Zardinality/WGAN-tensorflow](https://github.com/Zardinality/WGAN-tensorflow)\r\n- github(Tensorflow/Keras): [https://github.com/kuleshov/tf-wgan](https://github.com/kuleshov/tf-wgan)\r\n- github: [https://github.com/shekkizh/WassersteinGAN.tensorflow](https://github.com/shekkizh/WassersteinGAN.tensorflow)\r\n- gist: [https://gist.github.com/soumith/71995cecc5b99cda38106ad64503cee3](https://gist.github.com/soumith/71995cecc5b99cda38106ad64503cee3)\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/5qxoaz/r_170107875_wasserstein_gan/](https://www.reddit.com/r/MachineLearning/comments/5qxoaz/r_170107875_wasserstein_gan/)\r\n\r\n**Improved Training of Wasserstein GANs**\r\n\r\n- intro: NIPS 2017\r\n- arxiv: [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)\r\n- github(TensorFlow): [https://github.com/igul222/improved_wgan_training](https://github.com/igul222/improved_wgan_training)\r\n- github: [https://github.com/jalola/improved-wgan-pytorch](https://github.com/jalola/improved-wgan-pytorch)\r\n\r\n**On the effect of Batch Normalization and Weight Normalization in Generative Adversarial Networks**\r\n\r\n**On the Effects of Batch and Weight Normalization in Generative Adversarial Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1704.03971](https://arxiv.org/abs/1704.03971)\r\n- github: [https://github.com/stormraiser/GAN-weight-norm](https://github.com/stormraiser/GAN-weight-norm)\r\n\r\n**Controllable Generative Adversarial Network**\r\n\r\n- intro: Korea University\r\n- arxiv: [https://arxiv.org/abs/1708.00598](https://arxiv.org/abs/1708.00598)\r\n\r\n**Generative Adversarial Networks: An Overview**\r\n\r\n- intro: Imperial College London & Victoria University of Wellington & University of Montreal & Cortexica Vision Systems Ltd\r\n- intro: IEEE Signal Processing Magazine Special Issue on Deep Learning for Visual Understanding\r\n- arxiv: [https://arxiv.org/abs/1710.07035](https://arxiv.org/abs/1710.07035)\r\n\r\n**CyCADA: Cycle-Consistent Adversarial Domain Adaptation**\r\n\r\n[https://arxiv.org/abs/1711.03213](https://arxiv.org/abs/1711.03213)\r\n\r\n**Spectral Normalization for Generative Adversarial Networks**\r\n\r\n[https://openreview.net/forum?id=B1QRgziT-](https://openreview.net/forum?id=B1QRgziT-)\r\n\r\n**Are GANs Created Equal? A Large-Scale Study**\r\n\r\n- intro: Google Brain\r\n- arxiv: [https://arxiv.org/abs/1711.10337](https://arxiv.org/abs/1711.10337)\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/7gwip3/d_googles_large_scale_gantuning_paper_unfairly/](https://www.reddit.com/r/MachineLearning/comments/7gwip3/d_googles_large_scale_gantuning_paper_unfairly/)\r\n\r\n**GAGAN: Geometry-Aware Generative Adverserial Networks**\r\n\r\n[https://arxiv.org/abs/1712.00684](https://arxiv.org/abs/1712.00684)\r\n\r\n**CycleGAN: a Master of Steganography**\r\n\r\n- intro: NIPS 2017, workshop on Machine Deception\r\n- arxiv: [https://arxiv.org/abs/1712.02950](https://arxiv.org/abs/1712.02950)\r\n\r\n**PacGAN: The power of two samples in generative adversarial networks**\r\n\r\n- intro: CMU & University of Illinois at Urbana-Champaign\r\n- arxiv: [https://arxiv.org/abs/1712.04086](https://arxiv.org/abs/1712.04086)\r\n\r\n**ComboGAN: Unrestrained Scalability for Image Domain Translation**\r\n\r\n- arxiv: [https://arxiv.org/abs/1712.06909](https://arxiv.org/abs/1712.06909)\r\n- github: [https://github.com/AAnoosheh/ComboGAN](https://github.com/AAnoosheh/ComboGAN)\r\n\r\n**Decoupled Learning for Conditional Adversarial Networks**\r\n\r\n[https://arxiv.org/abs/1801.06790](https://arxiv.org/abs/1801.06790)\r\n\r\n**No Modes left behind: Capturing the data distribution effectively using GANs**\r\n\r\n- intro: AAAI 2018\r\n- arxiv: [https://arxiv.org/abs/1802.00771](https://arxiv.org/abs/1802.00771)\r\n\r\n**Improving GAN Training via Binarized Representation Entropy (BRE) Regularization**\r\n\r\n- intro: ICLR 2018\r\n- arxiv: [https://arxiv.org/abs/1805.03644](https://arxiv.org/abs/1805.03644)\r\n- github: [https://github.com/BorealisAI/bre-gan](https://github.com/BorealisAI/bre-gan)\r\n\r\n**On GANs and GMMs**\r\n\r\n[https://arxiv.org/abs/1805.12462](https://arxiv.org/abs/1805.12462)\r\n\r\n**The Unusual Effectiveness of Averaging in GAN Training**\r\n\r\n[https://arxiv.org/abs/1806.04498](https://arxiv.org/abs/1806.04498)\r\n\r\n**Understanding the Effectiveness of Lipschitz Constraint in Training of GANs via Gradient Analysis**\r\n\r\n[https://arxiv.org/abs/1807.00751](https://arxiv.org/abs/1807.00751)\r\n\r\n**The GAN Landscape: Losses, Architectures, Regularization, and Normalization**\r\n\r\n- intro: Google Brain\r\n- arxiv: [https://arxiv.org/abs/1807.04720](https://arxiv.org/abs/1807.04720)\r\n- github: [https://github.com/google/compare_gan](https://github.com/google/compare_gan)\r\n\r\n**Which Training Methods for GANs do actually Converge?**\r\n\r\n- intro: ICML 2018. MPI Tübingen & Microsoft Research\r\n- project page: [https://avg.is.tuebingen.mpg.de/publications/meschedericml2018](https://avg.is.tuebingen.mpg.de/publications/meschedericml2018)\r\n- paper: [https://avg.is.tuebingen.mpg.de/uploads_file/attachment/attachment/424/Mescheder2018ICML.pdf](https://avg.is.tuebingen.mpg.de/uploads_file/attachment/attachment/424/Mescheder2018ICML.pdf)\r\n- github: [https://github.com/LMescheder/GAN_stability](https://github.com/LMescheder/GAN_stability)\r\n\r\n**Convergence Problems with Generative Adversarial Networks (GANs)**\r\n\r\n- intro: University of Oxford\r\n- arxiv: [https://arxiv.org/abs/1806.11382](https://arxiv.org/abs/1806.11382)\r\n\r\n**Bayesian CycleGAN via Marginalizing Latent Sampling**\r\n\r\n[https://arxiv.org/abs/1811.07465](https://arxiv.org/abs/1811.07465)\r\n\r\n**GAN Dissection: Visualizing and Understanding Generative Adversarial Networks**\r\n\r\n[https://arxiv.org/abs/1811.10597](https://arxiv.org/abs/1811.10597)\r\n\r\n**Do GAN Loss Functions Really Matter?**\r\n\r\n[https://arxiv.org/abs/1811.09567](https://arxiv.org/abs/1811.09567)\r\n\r\n# Image-to-Image Translation\r\n\r\n## Pix2Pix\r\n\r\n**Image-to-Image Translation with Conditional Adversarial Networks**\r\n\r\n![](https://phillipi.github.io/pix2pix/images/teaser_v3.png)\r\n\r\n- intro: CVPR 2017\r\n- project page: [https://phillipi.github.io/pix2pix/](https://phillipi.github.io/pix2pix/)\r\n- arxiv: [https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004)\r\n- github: [https://github.com/phillipi/pix2pix](https://github.com/phillipi/pix2pix)\r\n- github(TensorFlow): [https://github.com/yenchenlin/pix2pix-tensorflow](https://github.com/yenchenlin/pix2pix-tensorflow)\r\n- github(Chainer): [https://github.com/mattya/chainer-pix2pix](https://github.com/mattya/chainer-pix2pix)\r\n- github(PyTorch): [https://github.com/mrzhu-cool/pix2pix-pytorch](https://github.com/mrzhu-cool/pix2pix-pytorch)\r\n- github(Chainer): [https://github.com/wuhuikai/chainer-pix2pix](https://github.com/wuhuikai/chainer-pix2pix)\r\n\r\n**Remastering Classic Films in Tensorflow with Pix2Pix**\r\n\r\n- blog: [https://hackernoon.com/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503#.6dmahnt8n](https://hackernoon.com/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503#.6dmahnt8n)\r\n- github: [https://github.com/awjuliani/Pix2Pix-Film](https://github.com/awjuliani/Pix2Pix-Film)\r\n- model: [https://drive.google.com/file/d/0B8x0IeJAaBccNFVQMkQ0QW15TjQ/view](https://drive.google.com/file/d/0B8x0IeJAaBccNFVQMkQ0QW15TjQ/view)\r\n\r\n**Image-to-Image Translation in Tensorflow**\r\n\r\n- blog: [http://affinelayer.com/pix2pix/index.html](http://affinelayer.com/pix2pix/index.html)\r\n- github: [https://github.com/affinelayer/pix2pix-tensorflow](https://github.com/affinelayer/pix2pix-tensorflow)\r\n\r\n**webcam pix2pix**\r\n\r\n[https://github.com/memo/webcam-pix2pix-tensorflow](https://github.com/memo/webcam-pix2pix-tensorflow)\r\n\r\n---\r\n\r\n**Unsupervised Image-to-Image Translation with Generative Adversarial Networks**\r\n\r\n- intro: Imperial College London & Indian Institute of Technology\r\n- arxiv: [https://arxiv.org/abs/1701.02676](https://arxiv.org/abs/1701.02676)\r\n\r\n**Unsupervised Image-to-Image Translation Networks**\r\n\r\n- intro: NIPS 2017 Spotlight\r\n- intro: unsupervised/unpaired image-to-image translation using coupled GANs\r\n- project page: [http://research.nvidia.com/publication/2017-12_Unsupervised-Image-to-Image-Translation](http://research.nvidia.com/publication/2017-12_Unsupervised-Image-to-Image-Translation)\r\n- arxiv: [https://arxiv.org/abs/1703.00848](https://arxiv.org/abs/1703.00848)\r\n- github: [https://github.com/mingyuliutw/UNIT](https://github.com/mingyuliutw/UNIT)\r\n\r\n**Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks**\r\n\r\n- intro: UC Berkeley\r\n- project page: [https://junyanz.github.io/CycleGAN/](https://junyanz.github.io/CycleGAN/)\r\n- arxiv: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)\r\n- github(official, Torch): [https://github.com/junyanz/CycleGAN](https://github.com/junyanz/CycleGAN)\r\n- github(official, PyTorch): [https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\r\n- github(PyTorch): [https://github.com/eveningglow/semi-supervised-CycleGAN](https://github.com/eveningglow/semi-supervised-CycleGAN)\r\n- github(Chainer): [https://github.com/Aixile/chainer-cyclegan](https://github.com/Aixile/chainer-cyclegan)\r\n\r\n**CycleGAN and pix2pix in PyTorch**\r\n\r\n- intro: Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more)\r\n- github: [https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\r\n\r\n**Perceptual Adversarial Networks for Image-to-Image Transformation**\r\n\r\n[https://arxiv.org/abs/1706.09138](https://arxiv.org/abs/1706.09138)\r\n\r\n**XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings**\r\n\r\n- intro: IST Austria & Google Brain & Google Research\r\n- arxiv: [https://arxiv.org/abs/1711.05139](https://arxiv.org/abs/1711.05139)\r\n\r\n**In2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks**\r\n\r\n[https://arxiv.org/abs/1711.09334](https://arxiv.org/abs/1711.09334)\r\n\r\n**StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation**\r\n\r\n- intro: Korea University & Clova AI Research\r\n- arxiv: [https://arxiv.org/abs/1711.09020](https://arxiv.org/abs/1711.09020)\r\n- github: [https://github.com//yunjey/StarGAN](https://github.com//yunjey/StarGAN)\r\n\r\n**Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation**\r\n\r\n[https://arxiv.org/abs/1711.09554](https://arxiv.org/abs/1711.09554)\r\n\r\n**Toward Multimodal Image-to-Image Translation**\r\n\r\n- intro: NIPS 2017. BicycleGAN\r\n- project page: [https://junyanz.github.io/BicycleGAN/](https://junyanz.github.io/BicycleGAN/)\r\n- arxiv: [https://arxiv.org/abs/1711.11586](https://arxiv.org/abs/1711.11586)\r\n- github(official, PyTorch): [https://github.com//junyanz/BicycleGAN](https://github.com//junyanz/BicycleGAN)\r\n- github: [https://github.com/gitlimlab/BicycleGAN-Tensorflow](https://github.com/gitlimlab/BicycleGAN-Tensorflow)\r\n- github: [https://github.com/kvmanohar22/img2imgGAN](https://github.com/kvmanohar22/img2imgGAN)\r\n- github: [https://github.com/eveningglow/BicycleGAN-pytorch](https://github.com/eveningglow/BicycleGAN-pytorch)\r\n\r\n**Face Translation between Images and Videos using Identity-aware CycleGAN**\r\n\r\n[https://arxiv.org/abs/1712.00971](https://arxiv.org/abs/1712.00971)\r\n\r\n**Unsupervised Multi-Domain Image Translation with Domain-Specific Encoders/Decoders**\r\n\r\n[https://arxiv.org/abs/1712.02050](https://arxiv.org/abs/1712.02050)\r\n\r\n**High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs**\r\n\r\n- intro: NVIDIA Corporation, UC Berkeley\r\n- project page: [https://tcwang0509.github.io/pix2pixHD/](https://tcwang0509.github.io/pix2pixHD/)\r\n- arxiv: [https://arxiv.org/abs/1711.11585](https://arxiv.org/abs/1711.11585)\r\n- github: [https://github.com/NVIDIA/pix2pixHD](https://github.com/NVIDIA/pix2pixHD)\r\n- youtube: [https://www.youtube.com/watch?v=3AIpPlzM_qs&feature=youtu.be](https://www.youtube.com/watch?v=3AIpPlzM_qs&feature=youtu.be)\r\n\r\n**On the Effectiveness of Least Squares Generative Adversarial Networks**\r\n\r\n[https://arxiv.org/abs/1712.06391](https://arxiv.org/abs/1712.06391)\r\n\r\n**GANs for Limited Labeled Data**\r\n\r\n- intro: Ian Goodfellow\r\n- slides: [http://www.iangoodfellow.com/slides/2017-12-09-label.pdf](http://www.iangoodfellow.com/slides/2017-12-09-label.pdf)\r\n\r\n**Defending Against Adversarial Examples**\r\n\r\n- intro: Ian Goodfellow\r\n- slides: [http://www.iangoodfellow.com/slides/2017-12-08-defending.pdf](http://www.iangoodfellow.com/slides/2017-12-08-defending.pdf)\r\n\r\n**Conditional Image-to-Image Translation**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1805.00251](https://arxiv.org/abs/1805.00251)\r\n\r\n**XOGAN: One-to-Many Unsupervised Image-to-Image Translation**\r\n\r\n[https://arxiv.org/abs/1805.07277](https://arxiv.org/abs/1805.07277)\r\n\r\n**Unsupervised Attention-guided Image to Image Translation**\r\n\r\n[https://arxiv.org/abs/1806.02311](https://arxiv.org/abs/1806.02311)\r\n\r\n**Exemplar Guided Unsupervised Image-to-Image Translation**\r\n\r\n[https://arxiv.org/abs/1805.11145](https://arxiv.org/abs/1805.11145)\r\n\r\n**Improving Shape Deformation in Unsupervised Image-to-Image Translation**\r\n\r\n[https://arxiv.org/abs/1808.04325](https://arxiv.org/abs/1808.04325)\r\n\r\n**Video-to-Video Synthesis**\r\n\r\n- arxiv: [https://arxiv.org/abs/1808.06601](https://arxiv.org/abs/1808.06601)\r\n- github: [https://github.com/NVIDIA/vid2vid](https://github.com/NVIDIA/vid2vid)\r\n\r\n**Segmentation Guided Image-to-Image Translation with Adversarial Networks**\r\n\r\n[https://arxiv.org/abs/1901.01569](https://arxiv.org/abs/1901.01569)\r\n\r\n**ForkGAN: Seeing into the rainy night**\r\n\r\n- intro: ECCV 2020 oral\r\n- intro: UISEE Technology & Kyoto University & University of Pennsylvania\r\n- paper: [https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480154.pdf](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480154.pdf)\r\n- github: [https://github.com/zhengziqiang/ForkGAN](https://github.com/zhengziqiang/ForkGAN)\r\n- presentation: [https://www.youtube.com/watch?v=O2nxRsSwkzs&t=1s](https://www.youtube.com/watch?v=O2nxRsSwkzs&t=1s)\r\n\r\n# Projects\r\n\r\n**Generative Adversarial Networks with Keras**\r\n\r\n- github: [https://github.com/phreeza/keras-GAN](https://github.com/phreeza/keras-GAN)\r\n\r\n**Generative Adversarial Network Demo for Fresh Machine Learning #2**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=deyOX6Mt_As&feature=em-uploademail](https://www.youtube.com/watch?v=deyOX6Mt_As&feature=em-uploademail)\r\n- github: [https://github.com/llSourcell/Generative-Adversarial-Network-Demo](https://github.com/llSourcell/Generative-Adversarial-Network-Demo)\r\n- demo: [http://cs.stanford.edu/people/karpathy/gan/](http://cs.stanford.edu/people/karpathy/gan/)\r\n\r\n**TextGAN: A generative adversarial network for text generation, written in TensorFlow.**\r\n\r\n- github: [https://github.com/AustinStoneProjects/TextGAN](https://github.com/AustinStoneProjects/TextGAN)\r\n\r\n**cleverhans v0.1: an adversarial machine learning library**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.00768](https://arxiv.org/abs/1610.00768)\r\n- github: [https://github.com/openai/cleverhans](https://github.com/openai/cleverhans)\r\n\r\n**Deep Convolutional Variational Autoencoder w/ Adversarial Network**\r\n\r\n- intro: An implementation of the deep convolutional generative adversarial network, combined with a varational autoencoder\r\n- github: [https://github.com/staturecrane/dcgan_vae_torch](https://github.com/staturecrane/dcgan_vae_torch)\r\n\r\n**A versatile GAN(generative adversarial network) implementation. Focused on scalability and ease-of-use.**\r\n\r\n- github: [https://github.com/255BITS/HyperGAN](https://github.com/255BITS/HyperGAN)\r\n\r\n**AdaGAN: Boosting Generative Models**\r\n\r\n- intro: AdaGAN: greedy iterative procedure to train mixtures of GANs\r\n- intro: Max Planck Institute for Intelligent Systems & Google Brain\r\n- arxiv: [https://arxiv.org/abs/1701.02386](https://arxiv.org/abs/1701.02386)\r\n- github: [https://github.com/tolstikhin/adagan](https://github.com/tolstikhin/adagan)\r\n\r\n**TensorFlow-GAN (TFGAN)**\r\n\r\n- intro: TFGAN: A Lightweight Library for Generative Adversarial Networks\r\n- github: [https://github.com//tensorflow/tensorflow/tree/master/tensorflow/contrib/gan](https://github.com//tensorflow/tensorflow/tree/master/tensorflow/contrib/gan)\r\n- blog: [https://research.googleblog.com/2017/12/tfgan-lightweight-library-for.html](https://research.googleblog.com/2017/12/tfgan-lightweight-library-for.html)\r\n\r\n# Blogs\r\n\r\n**Generative Adversial Networks Explained**\r\n\r\n- blog: [http://kvfrans.com/generative-adversial-networks-explained/](http://kvfrans.com/generative-adversial-networks-explained/)\r\n\r\n**Generative Adversarial Autoencoders in Theano**\r\n\r\n- blog: [https://swarbrickjones.wordpress.com/2016/01/24/generative-adversarial-autoencoders-in-theano/](https://swarbrickjones.wordpress.com/2016/01/24/generative-adversarial-autoencoders-in-theano/)\r\n- github: [https://github.com/mikesj-public/dcgan-autoencoder](https://github.com/mikesj-public/dcgan-autoencoder)\r\n\r\n**An introduction to Generative Adversarial Networks (with code in TensorFlow)**\r\n\r\n- blog: [http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/](http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/)\r\n- github: [https://github.com/AYLIEN/gan-intro](https://github.com/AYLIEN/gan-intro)\r\n\r\n**Difficulties training a Generative Adversarial Network**\r\n\r\n- github: [https://github.com/shekkizh/neuralnetworks.thought-experiments/blob/master/Generative%20Models/GAN/Readme.md](https://github.com/shekkizh/neuralnetworks.thought-experiments/blob/master/Generative%20Models/GAN/Readme.md)\r\n\r\n**Are Energy-Based GANs any more energy-based than normal GANs?**\r\n\r\n[http://www.inference.vc/are-energy-based-gans-actually-energy-based/](http://www.inference.vc/are-energy-based-gans-actually-energy-based/)\r\n\r\n**Generative Adversarial Networks Explained with a Classic Spongebob Squarepants Episode: Plus a Tensorflow tutorial for implementing your own GAN**\r\n\r\n- blog: [https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.rpiunhdjh](https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39#.rpiunhdjh)\r\n- gist: [https://gist.github.com/awjuliani/8ebf356d03ffee139659807be7fa2611](https://gist.github.com/awjuliani/8ebf356d03ffee139659807be7fa2611)\r\n\r\n**Deep Learning Research Review Week 1: Generative Adversarial Nets**\r\n\r\n![](https://adeshpande3.github.io/assets/Cover5th.png)\r\n\r\n- blog: [https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets)\r\n\r\n**Stability of Generative Adversarial Networks**\r\n\r\n- blog: [http://www.araya.org/archives/1183](http://www.araya.org/archives/1183)\r\n\r\n**Instance Noise: A trick for stabilising GAN training**\r\n\r\n- blog: [http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/](http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/)\r\n\r\n**Generating Fine Art in 300 Lines of Code**\r\n\r\n- intro: DCGAN\r\n- blog: [https://medium.com/@richardherbert/generating-fine-art-in-300-lines-of-code-4d37218216a6#.63qm8ef9g](https://medium.com/@richardherbert/generating-fine-art-in-300-lines-of-code-4d37218216a6#.63qm8ef9g)\r\n\r\n# Talks / Videos\r\n\r\n**Generative Adversarial Network visualization**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=mObnwR-u8pc](https://www.youtube.com/watch?v=mObnwR-u8pc)\r\n\r\n# Resources\r\n\r\n**The GAN Zoo**\r\n\r\n- intro: A list of all named GANs!\r\n- github: [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)\r\n\r\n**AdversarialNetsPapers: The classical Papers about adversial nets**\r\n\r\n- github: [https://github.com/zhangqianhui/AdversarialNetsPapers](https://github.com/zhangqianhui/AdversarialNetsPapers)\r\n\r\n**GAN Timeline**\r\n\r\n- intro: A timeline showing the development of Generative Adversarial Networks (GAN)\r\n- github: [https://github.com//dongb5/GAN-Timeline](https://github.com//dongb5/GAN-Timeline)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","title":"Graph Convolutional Networks"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Graph Convolutional Networks\r\ndate: 2015-10-09\r\n---\r\n\r\n**Learning Convolutional Neural Networks for Graphs**\r\n\r\n- intro: ICML 2016\r\n- arxiv: [http://arxiv.org/abs/1605.05273](http://arxiv.org/abs/1605.05273)\r\n\r\n**Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering**\r\n\r\n- arxiv: [https://arxiv.org/abs/1606.09375](https://arxiv.org/abs/1606.09375)\r\n- github: [https://github.com/mdeff/cnn_graph](https://github.com/mdeff/cnn_graph)\r\n- github: [https://github.com/pfnet-research/chainer-graph-cnn](https://github.com/pfnet-research/chainer-graph-cnn)\r\n\r\n**Semi-Supervised Classification with Graph Convolutional Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.02907](http://arxiv.org/abs/1609.02907)\r\n- github: [https://github.com/tkipf/gcn](https://github.com/tkipf/gcn)\r\n- blog: [http://tkipf.github.io/graph-convolutional-networks/](http://tkipf.github.io/graph-convolutional-networks/)\r\n\r\n**Graph Based Convolutional Neural Network**\r\n\r\n- intro: BMVC 2016\r\n- arxiv: [http://arxiv.org/abs/1609.08965](http://arxiv.org/abs/1609.08965)\r\n\r\n**How powerful are Graph Convolutions? (review of Kipf & Welling, 2016)**\r\n\r\n[http://www.inference.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/](http://www.inference.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/)\r\n\r\n**Graph Convolutional Networks**\r\n\r\n![](http://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png)\r\n\r\n- blog: [http://tkipf.github.io/graph-convolutional-networks/](http://tkipf.github.io/graph-convolutional-networks/)\r\n\r\n**DeepGraph: Graph Structure Predicts Network Growth**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.06251](https://arxiv.org/abs/1610.06251)\r\n\r\n**Deep Learning with Sets and Point Clouds**\r\n\r\n- intro: CMU\r\n- arxiv: [https://arxiv.org/abs/1611.04500](https://arxiv.org/abs/1611.04500)\r\n\r\n**Deep Learning on Graphs**\r\n\r\n- lecture: [https://figshare.com/articles/Deep_Learning_on_Graphs/4491686](https://figshare.com/articles/Deep_Learning_on_Graphs/4491686)\r\n\r\n**Robust Spatial Filtering with Graph Convolutional Neural Networks**\r\n\r\n[https://arxiv.org/abs/1703.00792](https://arxiv.org/abs/1703.00792)\r\n\r\n**Modeling Relational Data with Graph Convolutional Networks**\r\n\r\n[https://arxiv.org/abs/1703.06103](https://arxiv.org/abs/1703.06103)\r\n\r\n**Distance Metric Learning using Graph Convolutional Networks: Application to Functional Brain Networks**\r\n\r\n- intro: Imperial College London\r\n- arxiv: [https://arxiv.org/abs/1703.02161](https://arxiv.org/abs/1703.02161)\r\n\r\n**Deep Learning on Graphs with Graph Convolutional Networks**\r\n\r\n- slides: [http://tkipf.github.io/misc/GCNSlides.pdf](http://tkipf.github.io/misc/GCNSlides.pdf)\r\n\r\n**Deep Learning on Graphs with Keras**\r\n\r\n- intro:; Keras implementation of Graph Convolutional Networks\r\n- github: [https://github.com/tkipf/keras-gcn](https://github.com/tkipf/keras-gcn)\r\n\r\n**Learning Graph While Training: An Evolving Graph Convolutional Neural Network**\r\n\r\n[https://arxiv.org/abs/1708.04675](https://arxiv.org/abs/1708.04675)\r\n\r\n**Graph Attention Networks**\r\n\r\n- intro: ICLR 2018\r\n- intro: University of Cambridge & Centre de Visio per Computador, UAB & Montreal Institute for Learning Algorithms\r\n- project page: [http://petar-v.com/GAT/](http://petar-v.com/GAT/)\r\n- arxiv: [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)\r\n- github: [https://github.com/PetarV-/GAT](https://github.com/PetarV-/GAT)\r\n\r\n**Residual Gated Graph ConvNets**\r\n\r\n[https://arxiv.org/abs/1711.07553](https://arxiv.org/abs/1711.07553)\r\n\r\n**Probabilistic and Regularized Graph Convolutional Networks**\r\n\r\n- intro: CMU\r\n- arxiv: [https://arxiv.org/abs/1803.04489](https://arxiv.org/abs/1803.04489)\r\n\r\n**Videos as Space-Time Region Graphs**\r\n\r\n[https://arxiv.org/abs/1806.01810](https://arxiv.org/abs/1806.01810)\r\n\r\n**Relational inductive biases, deep learning, and graph networks**\r\n\r\n- intro: DeepMind & Google Brain & MIT & University of Edinburgh\r\n- arxiv: [https://arxiv.org/abs/1806.01261](https://arxiv.org/abs/1806.01261)\r\n\r\n**Can GCNs Go as Deep as CNNs?**\r\n\r\n- project: [https://sites.google.com/view/deep-gcns](https://sites.google.com/view/deep-gcns)\r\n- arxiv: [https://arxiv.org/abs/1904.03751](https://arxiv.org/abs/1904.03751)\r\n- slides: [https://docs.google.com/presentation/d/1L82wWymMnHyYJk3xUKvteEWD5fX0jVRbCbI65Cxxku0/edit#slide=id.p](https://docs.google.com/presentation/d/1L82wWymMnHyYJk3xUKvteEWD5fX0jVRbCbI65Cxxku0/edit#slide=id.p)\r\n- github(official, TensorFlow): [https://github.com/lightaime/deep_gcns](https://github.com/lightaime/deep_gcns)\r\n\r\n**GMNN: Graph Markov Neural Networks**\r\n\r\n- intro: ICML 2019\r\n- ariv: [https://arxiv.org/abs/1905.06214](https://arxiv.org/abs/1905.06214)\r\n- github: [https://github.com/DeepGraphLearning/GMNN](https://github.com/DeepGraphLearning/GMNN)\r\n\r\n**DeepGCNs: Making GCNs Go as Deep as CNNs**\r\n\r\n- intro:  ICCV 2019 Oral\r\n- arxiv: [https://arxiv.org/abs/1910.06849](https://arxiv.org/abs/1910.06849)\r\n- github: [https://github.com/lightaime/deep_gcns_torch](https://github.com/lightaime/deep_gcns_torch)\r\n- github: [https://github.com/lightaime/deep_gcns](https://github.com/lightaime/deep_gcns)\r\n\r\n**Rethinking pooling in graph neural networks**\r\n\r\n- intro: NeurIPS 2020\r\n- arxiv: [https://arxiv.org/abs/2010.11418](https://arxiv.org/abs/2010.11418)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","title":"Acceleration and Model Compression"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Acceleration and Model Compression\ndate: 2015-10-09\n---\n\n# Papers\n\n**Distilling the Knowledge in a Neural Network**\n\n- intro: NIPS 2014 Deep Learning Workshop\n- author: Geoffrey Hinton, Oriol Vinyals, Jeff Dean\n- arxiv: [http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531)\n- blog: [http://fastml.com/geoff-hintons-dark-knowledge/](http://fastml.com/geoff-hintons-dark-knowledge/)\n- notes: [https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/distilling-the-knowledge-in-a-nn.md](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/distilling-the-knowledge-in-a-nn.md)\n\n**Deep Model Compression: Distilling Knowledge from Noisy Teachers**\n\n- arxiv: [https://arxiv.org/abs/1610.09650](https://arxiv.org/abs/1610.09650)\n- github: [https://github.com/chengshengchan/model_compression](https://github.com/chengshengchan/model_compression)]\n\n**A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning**\n\n- intro: CVPR 2017\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)\n\n**Like What You Like: Knowledge Distill via Neuron Selectivity Transfer**\n\n- intro: TuSimple\n- arxiv: [https://arxiv.org/abs/1707.01219](https://arxiv.org/abs/1707.01219)\n- github: [https://github.com/TuSimple/neuron-selectivity-transfer](https://github.com/TuSimple/neuron-selectivity-transfer)\n\n**Learning Loss for Knowledge Distillation with Conditional Adversarial Networks**\n\n[https://arxiv.org/abs/1709.00513](https://arxiv.org/abs/1709.00513)\n\n**Data-Free Knowledge Distillation for Deep Neural Networks**\n\n[https://arxiv.org/abs/1710.07535](https://arxiv.org/abs/1710.07535)\n\n**Knowledge Projection for Deep Neural Networks**\n\n[https://arxiv.org/abs/1710.09505](https://arxiv.org/abs/1710.09505)\n\n**Moonshine: Distilling with Cheap Convolutions**\n\n[https://arxiv.org/abs/1711.02613](https://arxiv.org/abs/1711.02613)\n\n**model_compression: Implementation of model compression with knowledge distilling method**\n\n- github: [https://github.com/chengshengchan/model_compression](https://github.com/chengshengchan/model_compression)\n\n**Neural Network Distiller**\n\n- intro: Neural Network Distiller: a Python package for neural network compression research\n- project page: [https://nervanasystems.github.io/distiller/](https://nervanasystems.github.io/distiller/)\n- github: [https://github.com/NervanaSystems/distiller](https://github.com/NervanaSystems/distiller)\n\n**Knowledge Distillation in Generations: More Tolerant Teachers Educate Better Students**\n\n- intro: The Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/1805.05551](https://arxiv.org/abs/1805.05551)\n\n**Improving Knowledge Distillation with Supporting Adversarial Samples**\n\n[https://arxiv.org/abs/1805.05532](https://arxiv.org/abs/1805.05532)\n\n**Recurrent knowledge distillation**\n\n- intro: ICIP 2018\n- arxiv: [https://arxiv.org/abs/1805.07170](https://arxiv.org/abs/1805.07170)\n\n**Knowledge Distillation by On-the-Fly Native Ensemble**\n\n[https://arxiv.org/abs/1806.04606](https://arxiv.org/abs/1806.04606)\n\n**Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher**\n\n- intro: Washington State University & DeepMind\n- arxiv: [https://arxiv.org/abs/1902.03393](https://arxiv.org/abs/1902.03393)\n\n**Correlation Congruence for Knowledge Distillation**\n\n- intro: NUDT & SenseTime & BUAA & CUHK\n- keywords: Correlation Congruence Knowledge Distillation (CCKD)\n- arxiv: [https://arxiv.org/abs/1904.01802](https://arxiv.org/abs/1904.01802)\n\n**Similarity-Preserving Knowledge Distillation**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1907.09682](https://arxiv.org/abs/1907.09682)\n\n**Highlight Every Step: Knowledge Distillation via Collaborative Teaching**\n\n[https://arxiv.org/pdf/1907.09643.pdf](https://arxiv.org/pdf/1907.09643.pdf)\n\n**Ensemble Knowledge Distillation for Learning Improved and Efficient Networks**\n\n[https://arxiv.org/abs/1909.08097](https://arxiv.org/abs/1909.08097)\n\n**Revisit Knowledge Distillation: a Teacher-free Framework**\n\n- arxiv: [https://arxiv.org/abs/1909.11723](https://arxiv.org/abs/1909.11723)\n- github: [https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation](https://github.com/yuanli2333/Teacher-free-Knowledge-Distillation)\n\n**On the Efficacy of Knowledge Distillation**\n\n- intro: Cornell University\n- arxiv: [https://arxiv.org/abs/1910.01348](https://arxiv.org/abs/1910.01348)\n\n**Training convolutional neural networks with cheap convolutions and online distillation**\n\n- arxiv: [https://arxiv.org/abs/1909.13063](https://arxiv.org/abs/1909.13063)\n- github: [https://github.com/EthanZhangYC/OD-cheap-convolution](https://github.com/EthanZhangYC/OD-cheap-convolution)\n\n**Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation**\n\n[https://arxiv.org/abs/1911.05329](https://arxiv.org/abs/1911.05329)\n\n**Preparing Lessons: Improve Knowledge Distillation with Better Supervision**\n\n- intro: Xi’an Jiaotong University & Meituan\n- keywords: Knowledge Adjustment (KA), Dynamic Temperature Distillation (DTD)\n- arxiv: [https://arxiv.org/abs/1911.07471](https://arxiv.org/abs/1911.07471)\n\n**QKD: Quantization-aware Knowledge Distillation**\n\n[https://arxiv.org/abs/1911.12491](https://arxiv.org/abs/1911.12491)\n\n**Explaining Knowledge Distillation by Quantifying the Knowledge**\n\n[https://arxiv.org/abs/2003.03622](https://arxiv.org/abs/2003.03622)\n\n**Knowledge distillation via adaptive instance normalization**\n\n[https://arxiv.org/abs/2003.04289](https://arxiv.org/abs/2003.04289)\n\n**Distillating Knowledge from Graph Convolutional Networks**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.10477](https://arxiv.org/abs/2003.10477)\n\n**Regularizing Class-wise Predictions via Self-knowledge Distillation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.13964](https://arxiv.org/abs/2003.13964)\n\n**Online Knowledge Distillation with Diverse Peers**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1912.00350](https://arxiv.org/abs/1912.00350)\n- github: [https://github.com/DefangChen/OKDDip](https://github.com/DefangChen/OKDDip)\n\n**Channel Distillation: Channel-Wise Attention for Knowledge Distillation**\n\n- arxiv: [https://arxiv.org/abs/2006.01683](https://arxiv.org/abs/2006.01683)\n- github: [https://github.com/zhouzaida/channel-distillation](https://github.com/zhouzaida/channel-distillation)\n\n**Peer Collaborative Learning for Online Knowledge Distillation**\n\n- intro: Queen Mary University of London\n- arxiv: [https://arxiv.org/abs/2006.04147](https://arxiv.org/abs/2006.04147)\n\n**Knowledge Distillation for Multi-task Learning**\n\n- intro: University of Edinburgh\n- arxiv: [https://arxiv.org/abs/2007.06889](https://arxiv.org/abs/2007.06889)\n\n**Differentiable Feature Aggregation Search for Knowledge Distillation**\n\n[https://arxiv.org/abs/2008.00506](https://arxiv.org/abs/2008.00506)\n\n**Prime-Aware Adaptive Distillation**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.01458](https://arxiv.org/abs/2008.01458)\n\n**Knowledge Transfer via Dense Cross-Layer Mutual-Distillation**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.07816](https://arxiv.org/abs/2008.07816)\n- github: [https://github.com/sundw2014/DCM](https://github.com/sundw2014/DCM)\n\n**Matching Guided Distillation**\n\n- intro: ECCV 2020\n- intro: Aibee Inc.\n- project page: [http://kaiyuyue.com/mgd/](http://kaiyuyue.com/mgd/)\n- arxiv: [https://arxiv.org/abs/2008.09958](https://arxiv.org/abs/2008.09958)\n\n**Domain Adaptation Through Task Distillation**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.11911](https://arxiv.org/abs/2008.11911)\n- github: [https://github.com/bradyz/task-distillation](https://github.com/bradyz/task-distillation)\n\n**Spherical Knowledge Distillation**\n\n[https://arxiv.org/abs/2010.07485](https://arxiv.org/abs/2010.07485)\n\n**In Defense of Feature Mimicking for Knowledge Distillation**\n\n- intro: Nanjing University\n- arxiv: [https://arxiv.org/abs/2011.01424](https://arxiv.org/abs/2011.01424)\n\n**Online Ensemble Model Compression using Knowledge Distillation**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/2011.07449](https://arxiv.org/abs/2011.07449)\n\n**Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.08273](https://arxiv.org/abs/2103.08273)\n- github(Pytorch): [https://github.com/MingiJi/FRSKD](https://github.com/MingiJi/FRSKD)\n\n**Decoupled Knowledge Distillation**\n\n- intro: CVPR 2022\n- intro: MEGVII Technology & Waseda University & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2203.08679](https://arxiv.org/abs/2203.08679)\n\n**Knowledge Distillation via the Target-aware Transformer**\n\n- intro: CVPR 2022 Oral\n- intro: RMIT University & Alibaba Group & ReLER & Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/2205.10793](https://arxiv.org/abs/2205.10793)\n\n**Knowledge Distillation from A Stronger Teacher**\n\n- intro: SenseTime Research & The University of Sydney & University of Science and Technology of China\n- arxiv: [https://arxiv.org/abs/2205.10536](https://arxiv.org/abs/2205.10536)\n- github: [https://github.com/hunto/DIST_KD](https://github.com/hunto/DIST_KD)\n\n# Resources\n\n**Awesome Knowledge-Distillation**\n\n[https://github.com/FLHonker/Awesome-Knowledge-Distillation](https://github.com/FLHonker/Awesome-Knowledge-Distillation)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","title":"LiDAR 3D Object Detection"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: LiDAR 3D Object Detection\ndate: 2015-10-09\n---\n\n# Papers\n\n**Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1609.06666](https://arxiv.org/abs/1609.06666)\n\n**VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection**\n\n- intro: Apple Inc\n- arxiv: [https://arxiv.org/abs/1711.06396](https://arxiv.org/abs/1711.06396)\n\n**Complex-YOLO: Real-time 3D Object Detection on Point Clouds**\n\n- intro: Valeo Schalter und Sensoren GmbH & Ilmenau University of Technology\n- arxiv: [https://arxiv.org/abs/1803.06199](https://arxiv.org/abs/1803.06199)\n\n**Focal Loss in 3D Object Detection**\n\n- intro: IEEE RA-L 2019\n- project page: [https://sites.google.com/view/fl3d](https://sites.google.com/view/fl3d)\n- arxiv: [https://arxiv.org/abs/1809.06065](https://arxiv.org/abs/1809.06065)\n- github: [https://github.com/pyun-ram/FL3D](https://github.com/pyun-ram/FL3D)\n\n**PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1812.04244](https://arxiv.org/abs/1812.04244)\n- github(official): [https://github.com/sshaoshuai/PV-RCNN](https://github.com/sshaoshuai/PV-RCNN)\n- github(official): [https://github.com/sshaoshuai/PointRCNN](https://github.com/sshaoshuai/PointRCNN)\n\n**3D Object Detection Using Scale Invariant and Feature Reweighting Networks**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1901.02237](https://arxiv.org/abs/1901.02237)\n\n**3D Backbone Network for 3D Object Detection**\n\n[https://arxiv.org/abs/1901.08373](https://arxiv.org/abs/1901.08373)\n\n**Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds**\n\n[https://arxiv.org/abs/1904.07537](https://arxiv.org/abs/1904.07537)\n\n**Point-Voxel CNN for Efficient 3D Deep Learning**\n\n- intro: NeurIPS 2019 Spotlight\n- project page: [https://hanlab.mit.edu/projects/pvcnn/](https://hanlab.mit.edu/projects/pvcnn/)\n- arxiv: [https://arxiv.org/abs/1907.03739](https://arxiv.org/abs/1907.03739)\n- github: [https://github.com/mit-han-lab/pvcnn](https://github.com/mit-han-lab/pvcnn)\n\n**IoU Loss for 2D/3D Object Detection**\n\n- intro: 3d vision 2019\n- arxiv: [https://arxiv.org/abs/1908.03851](https://arxiv.org/abs/1908.03851)\n\n**Deep Hough Voting for 3D Object Detection in Point Clouds**\n\n- intro: ICCV 2019\n- intro: Facebook AI Research & Stanford University\n- keywords: VoteNet\n- arxiv: [https://arxiv.org/abs/1904.09664](https://arxiv.org/abs/1904.09664)\n- github: [https://github.com/facebookresearch/votenet](https://github.com/facebookresearch/votenet)\n\n**Fast Point R-CNN**\n\n- intro: ICCV 2019\n- intro: CUHK & Tencent YouTu Lab\n- arxiv: [https://arxiv.org/abs/1908.02990](https://arxiv.org/abs/1908.02990)\n\n**Interpolated Convolutional Networks for 3D Point Cloud Understanding**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.04512](https://arxiv.org/abs/1908.04512)\n\n**PointPillars: Fast Encoders for Object Detection from Point Clouds**\n\n- intro: nuTonomy: an APTIV company\n- keywords: a single stage\n- arxiv: [http://http://arxiv.org/abs/1812.05784](http://http://arxiv.org/abs/1812.05784)\n- github(official): [https://github.com/nutonomy/second.pytorch](https://github.com/nutonomy/second.pytorch)\n\n**LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving**\n\n- intro: CVPR 2019\n- intro: Uber Advanced Technologies Group\n- arxiv: [https://arxiv.org/abs/1903.08701](https://arxiv.org/abs/1903.08701)\n\n**Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation**\n\n- intro: CVPR Workshop on Autonomous Driving 2019\n- keywords: LaserNet++\n- arxiv: [https://arxiv.org/abs/1904.11466](https://arxiv.org/abs/1904.11466)\n\n**Part-A^2 Net: 3D Part-Aware and Aggregation Neural Network for Object Detection from Point Cloud**\n\n**From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network**\n\n- intro: TPAMI 2020\n- arxiv: [https://arxiv.org/abs/1907.03670](https://arxiv.org/abs/1907.03670)\n- github(official): [https://github.com/sshaoshuai/PartA2-Net](https://github.com/sshaoshuai/PartA2-Net)\n- github(official): [https://github.com/open-mmlab/OpenPCDet](https://github.com/open-mmlab/OpenPCDet)\n\n**Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection**\n\n- intro: CVPR 2019\n- intro: winner of nuScenes 3D Object Detection challenge in WAD\n- arxiv: [https://arxiv.org/abs/1908.09492](https://arxiv.org/abs/1908.09492)\n- github: https://github.com/ZhengWG/Class-balanced-Grouping-and-Sampling-for-Point-Cloud-3D-Object-Detection\n\n**End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds**\n\n- intro: CoRL 2019\n- intro: Waymo LLC & Google Brain\n- keywords: dynamic voxelization\n- arxiv: [https://arxiv.org/abs/1910.06528](https://arxiv.org/abs/1910.06528)\n\n**SampleNet: Differentiable Point Cloud Sampling**\n\n- intro: CVPR 2020 oral\n- intro: Tel Aviv University\n- arxiv: [https://arxiv.org/abs/1912.03663](https://arxiv.org/abs/1912.03663)\n- github: [https://github.com/itailang/SampleNet](https://github.com/itailang/SampleNet)\n\n**TANet: Robust 3D Object Detection from Point Clouds with Triple Attention**\n\n- intro: AAAI 2020 oral\n- intro: Huazhong University of Science and Technology & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1912.05163](https://arxiv.org/abs/1912.05163)\n- github: [https://github.com/happinesslz/TANet](https://github.com/happinesslz/TANet)\n\n**PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/1912.13192](https://arxiv.org/abs/1912.13192)\n- github(official): [https://github.com/open-mmlab/OpenPCDet](https://github.com/open-mmlab/OpenPCDet)\n\n**Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud**\n\n- intro: CVPR 2020\n- intro: Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/2003.01251](https://arxiv.org/abs/2003.01251)\n- github: [https://github.com/WeijingShi/Point-GNN](https://github.com/WeijingShi/Point-GNN)\n\n**PV-RCNN: The Top-Performing LiDAR-only Solutions for 3D Detection / 3D Tracking / Domain Adaptation of Waymo Open Dataset Challenges**\n\n- arxiv: [https://arxiv.org/abs/2008.12599](https://arxiv.org/abs/2008.12599)\n- github: [https://github.com/open-mmlab/OpenPCDet](https://github.com/open-mmlab/OpenPCDet)\n\n**Deformable PV-RCNN: Improving 3D Object Detection with Learned Deformations**\n\n- intro: ECCV 2020 Workshop on Perception for Autonomous Driving\n- intro: University of Waterloo\n- arxiv: [https://arxiv.org/abs/2008.08766](https://arxiv.org/abs/2008.08766)\n- github: [https://github.com/AutoVision-cloud/Deformable-PV-RCNN](https://github.com/AutoVision-cloud/Deformable-PV-RCNN)\n\n**3DSSD: Point-based 3D Single Stage Object Detector**\n\n- intro: CVPR 2020 Oral\n- arxiv: [https://arxiv.org/abs/2002.10187](https://arxiv.org/abs/2002.10187)\n- github: [https://github.com/Jia-Research-Lab/3DSSD](https://github.com/Jia-Research-Lab/3DSSD)\n\n**HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection**\n\n- intro: CVPR 2020\n- intro: DEEPROUTE.AI\n- arxiv: [https://arxiv.org/abs/2003.00186](https://arxiv.org/abs/2003.00186)\n- paper: [http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_HVNet_Hybrid_Voxel_Network_for_LiDAR_Based_3D_Object_Detection_CVPR_2020_paper.pdf](http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_HVNet_Hybrid_Voxel_Network_for_LiDAR_Based_3D_Object_Detection_CVPR_2020_paper.pdf)\n\n**SSN: Shape Signature Networks for Multi-class Object Detection from Point Clouds**\n\n- intro: ECCV 2020\n- intro: CUHK & SenseTime Research & Hong Kong Baptist University\n- arxiv: [https://arxiv.org/abs/2004.02774](https://arxiv.org/abs/2004.02774)\n- github(mmdetection3d): [https://github.com/xinge008/SSN](https://github.com/xinge008/SSN)\n\n**Range Conditioned Dilated Convolutions for Scale Invariant 3D Object Detection**\n\n- intro: Google Research & Waymo LLC\n- keywords: Range Conditioned Dilation (RCD)\n- arxiv: [https://arxiv.org/abs/2005.09927](https://arxiv.org/abs/2005.09927)\n\n**Deep Learning for LiDAR Point Clouds in Autonomous Driving: A Review**\n\n- intro: University of Waterloo & Sun Yat-Sen University & Xilinx Technology & Ryerson University\n- arxiv: [https://arxiv.org/abs/2005.09830](https://arxiv.org/abs/2005.09830)\n\n**Structure Aware Single-stage 3D Object Detection from Point Cloud**\n\n- intro: CVPR 2020\n- intro: The Hong Kong Polytechnic University & DAMO Academy, Alibaba Group\n- intro: SA-SSD\n- paper: [http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.pdf](http://openaccess.thecvf.com/content_CVPR_2020/papers/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.pdf)\n- paper: [https://www4.comp.polyu.edu.hk/~cslzhang/paper/SA-SSD.pdf](https://www4.comp.polyu.edu.hk/~cslzhang/paper/SA-SSD.pdf)\n- github: [https://github.com/skyhehe123/SA-SSD](https://github.com/skyhehe123/SA-SSD)\n\n**Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection**\n\n- intro: CVPR 2020\n- intro: Fudan University & Baidu Inc. & University of Science and Technology of China\n- arxiv: [https://arxiv.org/abs/2006.04356](https://arxiv.org/abs/2006.04356)\n\n**SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds**\n\n[https://arxiv.org/abs/2006.04043](https://arxiv.org/abs/2006.04043)\n\n**Stereo RGB and Deeper LIDAR Based Network for 3D Object Detection**\n\n[https://arxiv.org/abs/2006.05187](https://arxiv.org/abs/2006.05187)\n\n**Generative Sparse Detection Networks for 3D Single-shot Object Detection**\n\n- intro: Stanford University & NVIDIA\n- arxiv: [https://arxiv.org/abs/2006.12356](https://arxiv.org/abs/2006.12356)\n\n**Local Grid Rendering Networks for 3D Object Detection in Point Clouds**\n\n[https://arxiv.org/abs/2007.02099](https://arxiv.org/abs/2007.02099)\n\n**InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling**\n\n- intro: University of Maryland & Salesforce Research\n- arxiv: [https://arxiv.org/abs/2007.08556](https://arxiv.org/abs/2007.08556)\n\n**EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection**\n\n- intro: ECCV 2020\n- intro: Huazhong University of Science and Technology\n- arxiv: [https://arxiv.org/abs/2007.08856](https://arxiv.org/abs/2007.08856)\n- github: [https://github.com/happinesslz/EPNet](https://github.com/happinesslz/EPNet)\n\n**Pillar-based Object Detection for Autonomous Driving**\n\n- intro: ECCV 2020\n- intro: MIT & Google\n- arxiv: [https://arxiv.org/abs/2007.10323](https://arxiv.org/abs/2007.10323)\n- github(TensorFlow): [https://github.com/WangYueFt/pillar-od](https://github.com/WangYueFt/pillar-od)\n\n**Weakly Supervised 3D Object Detection from Lidar Point Cloud**\n\n- intro: ECCV 2020\n- intro: Beijing Institute of Technology & ETH Zurich & Inception Institute of Artificial Intelligence\n- arxiv: [https://arxiv.org/abs/2007.11901](https://arxiv.org/abs/2007.11901)\n- github: [https://github.com/hlesmqh/WS3D](https://github.com/hlesmqh/WS3D)\n\n**An LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds**\n\n- intro: ECCV 2020\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/2007.12392](https://arxiv.org/abs/2007.12392)\n\n**Part-Aware Data Augmentation for 3D Object Detection in Point Cloud**\n\n[https://arxiv.org/abs/2007.13373](https://arxiv.org/abs/2007.13373)\n\n**Weakly Supervised 3D Object Detection from Point Clouds**\n\n- intro: ACM MM 2020\n- intro: MIT & Microsoft Research\n- arxiv: [https://arxiv.org/abs/2007.13970](https://arxiv.org/abs/2007.13970)\n- github: [https://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection](https://github.com/Zengyi-Qin/Weakly-Supervised-3D-Object-Detection)\n\n**Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.16100](https://arxiv.org/abs/2007.16100)\n\n**Global Context Aware Convolutions for 3D Point Cloud Understanding**\n\n[https://arxiv.org/abs/2008.02986](https://arxiv.org/abs/2008.02986)\n\n**DeepLiDARFlow: A Deep Learning Architecture For Scene Flow Estimation Using Monocular Camera and Sparse LiDAR**\n\n- intro: IROS 2020\n- arxiv: [https://arxiv.org/abs/2008.08136](https://arxiv.org/abs/2008.08136)\n- github: [https://github.com/dfki-av/DeepLiDARFlow](https://github.com/dfki-av/DeepLiDARFlow)\n\n**PointMixup: Augmentation for Point Clouds**\n\n- intro: ECCV 2020 spotlight\n- arxiv: [https://arxiv.org/abs/2008.06374](https://arxiv.org/abs/2008.06374)\n\n**Cross-Modality 3D Object Detection**\n\n- intro: WACV 2021\n- arxiv: [https://arxiv.org/abs/2008.10436](https://arxiv.org/abs/2008.10436)\n\n**LC-NAS: Latency Constrained Neural Architecture Search for Point Cloud Networks**\n\n[https://arxiv.org/abs/2008.10309](https://arxiv.org/abs/2008.10309)\n\n**Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving**\n\n- intro: Uber Advanced Technologies Group\n- arxiv: [https://arxiv.org/abs/2008.11901](https://arxiv.org/abs/2008.11901)\n\n**DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with Dynamic Voxelization and 3D Group Convolution**\n\n- intro: DESR Lab, Hong Kong University of Science and Technology\n- arxiv: [https://arxiv.org/abs/2009.02918](https://arxiv.org/abs/2009.02918)\n\n**Joint Pose and Shape Estimation of Vehicles from LiDAR Data**\n\n- intro: Argo AI & Microsoft & Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/2009.03964](https://arxiv.org/abs/2009.03964)\n\n**Deep Learning for 3D Point Cloud Understanding: A Survey**\n\n- arxiv: [https://arxiv.org/abs/2009.08920](https://arxiv.org/abs/2009.08920)\n- github: [https://github.com/SHI-Labs/3D-Point-Cloud-Learning](https://github.com/SHI-Labs/3D-Point-Cloud-Learning)\n\n**Multi-Frame to Single-Frame: Knowledge Distillation for 3D Object Detection**\n\n- intro: ECCV 2020 Workshop on Perception for Autonomous Driving\n- intro: MIT & Google & Stanford\n- arxiv: [https://arxiv.org/abs/2009.11859](https://arxiv.org/abs/2009.11859)\n\n**Torch-Points3D: A Modular Multi-Task Frameworkfor Reproducible Deep Learning on 3D Point Clouds**\n\n- arxiv: [https://arxiv.org/abs/2010.04642](https://arxiv.org/abs/2010.04642)\n- github: [https://github.com/nicolas-chaulet/torch-points3d](https://github.com/nicolas-chaulet/torch-points3d)\n\n**MLOD: Awareness of Extrinsic Perturbation in Multi-LiDAR 3D Object Detection for Autonomous Driving**\n\n- intro: The Hong Kong University of Science and Technology\n- project page: [https://ram-lab.com/file/site/mlod/](https://ram-lab.com/file/site/mlod/)\n- arxiv: [https://arxiv.org/abs/2010.11702](https://arxiv.org/abs/2010.11702)\n\n**StrObe: Streaming Object Detection from LiDAR Packets**\n\n- intro: CoRL 2020\n- intro: Uber Advanced Technologies Group & University of Toronto\n- arxiv: [https://arxiv.org/abs/2011.06425](https://arxiv.org/abs/2011.06425)\n\n**MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models**\n\n- intro: NeurIPS 2020\n- intro: Uber Advanced Technologies Group & University of Waterloo & University of Toronto\n- arxiv: [https://arxiv.org/abs/2011.07590](https://arxiv.org/abs/2011.07590)\n\n**LiDAR-based Panoptic Segmentation via Dynamic Shifting Network**\n\n- intro: Nanyang Technological University & Chinese University of Hong Kong\n- intro: Rank 1st place in the leaderboard of SemanticKITTI Panoptic Segmentation (accessed at 2020-11-16)\n- arxiv: [https://arxiv.org/abs/2011.11964](https://arxiv.org/abs/2011.11964)\n- github: [https://github.com/hongfz16/DS-Net](https://github.com/hongfz16/DS-Net)\n\n**CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud**\n\n- intro: AAAI 2021\n- intro: The Chinese University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2012.03015](https://arxiv.org/abs/2012.03015)\n- github: [https://github.com/Vegeta2020/CIA-SSD](https://github.com/Vegeta2020/CIA-SSD)\n\n**PC-RGNN: Point Cloud Completion and Graph Neural Network for 3D Object Detection**\n\n- intro: Beihang University\n- arxiv: [https://arxiv.org/abs/2012.10412](https://arxiv.org/abs/2012.10412)\n\n**Achieving Real-Time LiDAR 3D Object Detection on a Mobile Device**\n\n[https://arxiv.org/abs/2012.13801](https://arxiv.org/abs/2012.13801)\n\n**Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection**\n\n- intro: AAAI 2021\n- arxiv: [https://arxiv.org/abs/2012.15712](https://arxiv.org/abs/2012.15712)\n- github: [https://github.com/djiajunustc/Voxel-R-CNN](https://github.com/djiajunustc/Voxel-R-CNN)\n\n**RTS3D: Real-time Stereo 3D Detection from 4D Feature-Consistency Embedding Space for Autonomous Driving**\n\n- arxiv: [https://arxiv.org/abs/2012.15072](https://arxiv.org/abs/2012.15072)\n- github: [https://github.com/Banconxuan/RTS3D](https://github.com/Banconxuan/RTS3D)\n\n**Self-Attention Based Context-Aware 3D Object Detection**\n\n- intro: University of Waterloo\n- arxiv: [https://arxiv.org/abs/2101.02672](https://arxiv.org/abs/2101.02672)\n- github: [https://github.com/AutoVision-cloud/SA-Det3D](https://github.com/AutoVision-cloud/SA-Det3D)\n\n**A Simple and Efficient Multi-task Network for 3D Object Detection and Road Understanding**\n\n- arxiv: [https://arxiv.org/abs/2103.04056](https://arxiv.org/abs/2103.04056)\n- github: [https://github.com/frankfengdi/LidarMTL](https://github.com/frankfengdi/LidarMTL)\n\n**ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.05346](https://arxiv.org/abs/2103.05346)\n- github: [https://github.com/CVMI-Lab/ST3D](https://github.com/CVMI-Lab/ST3D)\n\n**RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2103.10039](https://arxiv.org/abs/2103.10039)\n- github: [https://github.com/TuSimple/RangeDet](https://github.com/TuSimple/RangeDet)\n\n**Stereo CenterNet based 3D Object Detection for Autonomous Driving**\n\n[https://arxiv.org/abs/2103.11071](https://arxiv.org/abs/2103.11071)\n\n**LiDAR R-CNN: An Efficient and Universal 3D Object Detector**\n\n- intro: CVPR 2021\n- intro: TuSimple\n- arxiv: [https://arxiv.org/abs/2103.15297](https://arxiv.org/abs/2103.15297)\n- github: [https://github.com/tusimple/LiDAR_RCNN](https://github.com/tusimple/LiDAR_RCNN)\n\n**HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection**\n\n- intro: CVPR 2021\n[https://arxiv.org/abs/2104.00902](https://arxiv.org/abs/2104.00902)\n\n**Group-Free 3D Object Detection via Transformers**\n\n- intro: University of Science and Technology of China & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/2104.00678](https://arxiv.org/abs/2104.00678)\n- github: [https://github.com/zeliu98/Group-Free-3D](https://github.com/zeliu98/Group-Free-3D)\n\n**SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.09804](https://arxiv.org/abs/2104.09804)\n- github: [https://github.com/Vegeta2020/SE-SSD](https://github.com/Vegeta2020/SE-SSD)\n\n**BEVDetNet: Bird's Eye View LiDAR Point Cloud based Real-time 3D Object Detection for Autonomous Driving**\n\n[https://arxiv.org/abs/2104.10780](https://arxiv.org/abs/2104.10780)\n\n**Investigating Attention Mechanism in 3D Point Cloud Object Detection**\n\n- intro: Australian National University & Data61-CSIRO, Australia & University of Technology Sydney & Nanyang Technological University\n- arxiv: [https://arxiv.org/abs/2108.00620](https://arxiv.org/abs/2108.00620)\n- github: [https://github.com/ShiQiu0419/attentions_in_3D_detection](https://github.com/ShiQiu0419/attentions_in_3D_detection)\n\n**Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection**\n\n- intro: ICCV 2021\n- intro: The Chinese University of Hong Kong & Huawei Noah’s Ark Lab & HKUST & Sun Yat-Sen University\n- arxiv: [https://arxiv.org/abs/2109.02499](https://arxiv.org/abs/2109.02499)\n\n**Voxel Transformer for 3D Object Detection**\n\n- intro: ICCV 2021\n- intro: The Chinese University of Hong Kong & National University of Singapore & Huawei Noah’s Ark Lab & HKUST & Sun Yat-Sen University\n- arxiv: [https://arxiv.org/abs/2109.02497](https://arxiv.org/abs/2109.02497)\n\n**A Versatile Multi-View Framework for LiDAR-based 3D Object Detection with Guidance from Panoptic Segmentation**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.02133](https://arxiv.org/abs/2203.02133)\n\n**Point Density-Aware Voxels for LiDAR 3D Object Detection**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.05662](https://arxiv.org/abs/2203.05662)\n- github: [https://github.com/TRAILab/PDV](https://github.com/TRAILab/PDV)\n\n**VISTA: Boosting 3D Object Detection via Dual Cross-VIew SpaTial Attention**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.09704](https://arxiv.org/abs/2203.09704)\n- github: [https://github.com/Gorilla-Lab-SCUT/VISTA](https://github.com/Gorilla-Lab-SCUT/VISTA)\n\n**Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds**\n\n- intro: CVPR 2022\n- intro: National University of Defense Technology & University of Oxford\n- arxiv: [https://arxiv.org/abs/2203.11139](https://arxiv.org/abs/2203.11139)\n- github: [https://github.com/yifanzhang713/IA-SSD](https://github.com/yifanzhang713/IA-SSD)\n\nL**iDAR Distillation: Bridging the Beam-Induced Domain Gap for 3D Object Detection**\n\n- intro: Tsinghua University & State Key Lab of Intelligent Technologies and Systems & Gaussian Robotics\n- arxiv: [https://arxiv.org/abs/2203.14956](https://arxiv.org/abs/2203.14956)\n- github: [https://github.com/weiyithu/LiDAR-Distillation](https://github.com/weiyithu/LiDAR-Distillation)\n\n**Point2Seq: Detecting 3D Objects as Sequences**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.13394](https://arxiv.org/abs/2203.13394)\n- github: [https://github.com/ocNflag/point2seq](https://github.com/ocNflag/point2seq)\n\n**OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.06577](https://arxiv.org/abs/2204.06577)\n- github: [https://github.com/dschinagl/occam](https://github.com/dschinagl/occam)\n\n**PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D Detection**\n\n- intro: Tsinghua University & Xi’an Jiaotong University & DIDI\n- arxiv: [https://arxiv.org/abs/2205.11098](https://arxiv.org/abs/2205.11098)\n- github: [https://github.com/RunpeiDong/PointDistiller](https://github.com/RunpeiDong/PointDistiller)\n\n**PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection**\n\n- intro: Harbin Institute of Technology & Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/2205.07403](https://arxiv.org/abs/2205.07403)\n\n**Fully Convolutional One-Stage 3D Object Detection on LiDAR Range Images**\n\n- intro: Meituan Inc. & Zhejiang University & Northwestern Polytechnical University\n- arxiv: [https://arxiv.org/abs/2205.13764](https://arxiv.org/abs/2205.13764)\n\n**Voxel Field Fusion for 3D Object Detection**\n\n- intro: CVPR 2022\n- intro: The Chinese University of Hong Kong & The University of Hong Kong & MEGVII Technology & SmartMore\n- arxiv: [https://arxiv.org/abs/2205.15938](https://arxiv.org/abs/2205.15938)\n- github: [https://github.com/dvlab-research/VFF](https://github.com/dvlab-research/VFF)\n\n**Unifying Voxel-based Representation with Transformer for 3D Object Detection**\n\n- intro: The Chinese University of Hong Kong & The University of Hong Kong & MEGVII Technology & SmartMore4\n- arxiv: [https://arxiv.org/abs/2206.00630](https://arxiv.org/abs/2206.00630)\n- github: [https://github.com/dvlab-research/UVTR](https://github.com/dvlab-research/UVTR)\n\n**LidarMultiNet: Unifying LiDAR Semantic Segmentation, 3D Object Detection, and Panoptic Segmentation in a Single Multi-task Network**\n\n- intro: TuSimple & University of Central Florida\n- intro: Official 1st Place Solution for the Waymo Open Dataset Challenges 2022 - 3D Semantic Segmentation\n- arxiv: [https://arxiv.org/abs/2206.11428](https://arxiv.org/abs/2206.11428)\n\n**Rethinking IoU-based Optimization for Single-stage 3D Object Detection**\n\n- intro: ECCV 2022\n- intro: Zhejiang University & Alibaba Cloud Computing Ltd. & National University of Singapore\n- arxiv: [https://arxiv.org/abs/2207.09332](https://arxiv.org/abs/2207.09332)\n- github: [https://github.com/hlsheng1/RDIoU](https://github.com/hlsheng1/RDIoU)\n\n**Embracing Single Stride 3D Object Detector with Sparse Transformer**\n\n- intro: CVPR 2022\n- intro: CASIA & UIUC & CMU & THU & TuSimple\n- arxiv: [https://arxiv.org/abs/2112.06375](https://arxiv.org/abs/2112.06375)\n- github: [https://github.com/TuSimple/SST](https://github.com/TuSimple/SST)\n- zhihu: [https://zhuanlan.zhihu.com/p/476056546](https://zhuanlan.zhihu.com/p/476056546)\n\n**Fully Sparse 3D Object Detection**\n\n- intro: ECCV 2022\n- intro: CASIA & TuSimple\n- arxiv: [https://arxiv.org/abs/2207.10035](https://arxiv.org/abs/2207.10035)\n- github: [https://github.com/TuSimple/SST](https://github.com/TuSimple/SST)\n\n# Anchor-free 3D Detection\n\n**Object as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots**\n\n- intro: Samsung Inc & Johns Hopkins University & South China University of Technology\n- keywords: Object as Hotspots (OHS)\n- arxiv: [https://arxiv.org/abs/1912.12791](https://arxiv.org/abs/1912.12791)\n\n**CenterNet3D: An Anchor free Object Detector for Autonomous Driving**\n\n- keywords: Non-Maximum Suppression free\n- arxiv: [https://arxiv.org/abs/2007.07214](https://arxiv.org/abs/2007.07214)\n- github: [https://github.com/wangguojun2018/CenterNet3d](https://github.com/wangguojun2018/CenterNet3d)\n\n**AFDet: Anchor Free One Stage 3D Object Detection**\n\n- intro: Horizon Robotics\n- intro: CVPR Workshop 2020\n- intro: Baseline detector for the 1st place solutions of Waymo Open Dataset Challenges 2020\n- arxiv: [https://arxiv.org/abs/2006.12671](https://arxiv.org/abs/2006.12671)\n\n**Real-Time Anchor-Free Single-Stage 3D Detection with IoU-Awareness**\n\n- intro: Horizon Robotics\n- keywords: AFDetV2\n- arxiv: [https://arxiv.org/abs/2107.14342](https://arxiv.org/abs/2107.14342)\n\n**1st Place Solution for Waymo Open Dataset Challenge -- 3D Detection and Domain Adaptation**\n\n- intro: Horizon Robotics\n- arxiv: [https://arxiv.org/abs/2006.15505](https://arxiv.org/abs/2006.15505)\n\n**FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection**\n\n- intro: Samsung AI Center Moscow\n- arxiv: [https://arxiv.org/abs/2112.00322](https://arxiv.org/abs/2112.00322)\n- github: [https://github.com/samsunglabs/fcaf3d](https://github.com/samsunglabs/fcaf3d)\n\n# 3D Semantic Segmentation\n\n**PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.14032](https://arxiv.org/abs/2003.14032)\n- github: [https://github.com/edwardzhou130/PolarSeg](https://github.com/edwardzhou130/PolarSeg)\n\n**Cloud Transformers**\n\n- intro: Samsung AI Center Moscow & Skolkovo Institute of Science and Technology\n- arxiv: [https://arxiv.org/abs/2007.11679](https://arxiv.org/abs/2007.11679)\n\n**Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation**\n\n- intro: CUHK & ShanghaiTech University & SenseTime Research\n- arxiv: [https://arxiv.org/abs/2008.01550](https://arxiv.org/abs/2008.01550)\n- github: [https://github.com/xinge008/Cylinder3D](https://github.com/xinge008/Cylinder3D)\n\n**Projected-point-based Segmentation: A New Paradigm for LiDAR Point Cloud Segmentation**\n\n[https://arxiv.org/abs/2008.03928](https://arxiv.org/abs/2008.03928)\n\n# pseudo-LiDAR\n\n**Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving**\n\n- intro: CVPR 2019\n- project page: [https://mileyan.github.io/pseudo_lidar/](https://mileyan.github.io/pseudo_lidar/)\n- arxiv: [https://arxiv.org/abs/1812.07179](https://arxiv.org/abs/1812.07179)\n- gtihub(official): [https://github.com/mileyan/pseudo_lidar](https://github.com/mileyan/pseudo_lidar)\n\n**Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving**\n\n- intro: ICLR 2020 Poster\n- openreview: [https://openreview.net/forum?id=BJedHRVtPB](https://openreview.net/forum?id=BJedHRVtPB)\n- github(official): [https://github.com/mileyan/Pseudo_Lidar_V2](https://github.com/mileyan/Pseudo_Lidar_V2)\n\n**End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2004.03080](https://arxiv.org/abs/2004.03080)\n- github: [https://github.com/mileyan/pseudo-LiDAR_e2e](https://github.com/mileyan/pseudo-LiDAR_e2e)\n\n**Rethinking Pseudo-LiDAR Representation**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.04582](https://arxiv.org/abs/2008.04582)\n- github: [https://github.com/xinzhuma/patchnet](https://github.com/xinzhuma/patchnet)\n\n**Demystifying Pseudo-LiDAR for Monocular 3D Object Detection**\n\n- intro: University of Trento & Fondazione Bruno Kessler & Facebook\n- arxiv: [https://arxiv.org/abs/2012.05796](https://arxiv.org/abs/2012.05796)\n\n**Is Pseudo-Lidar needed for Monocular 3D Object detection?**\n\n- intro: ICCV 2021\n- intro: Toyota Research Institute\n- arxiv: [https://arxiv.org/abs/2108.06417](https://arxiv.org/abs/2108.06417)\n\n**ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection**\n\n- intro: Beijing Institute of Technology & Baidu Research & National Engineering Laboratory of Deep Learning Technology and Application & University of Macau & University of Technology Sydney\n- arxiv: [https://arxiv.org/abs/2207.12654](https://arxiv.org/abs/2207.12654)\n- github: [https://github.com/yinjunbo/ProposalContrast](https://github.com/yinjunbo/ProposalContrast)\n\n# Multi-Modal 3D Object Detection\n\n- intro:  University of Science and Technology & Harbin Institute of Technology & SenseTime Research & The Chinese University of Hong Kong & IIIS, Tsinghua University\n**AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection**\n- arxiv: [https://arxiv.org/abs/2201.06493](https://arxiv.org/abs/2201.06493)\n\n# 3D Detection and Tracking\n\n**Joint Monocular 3D Vehicle Detection and Tracking**\n\n- intro: ICCV 2019\n- project page: [https://eborboihuc.github.io/Mono-3DT/](https://eborboihuc.github.io/Mono-3DT/)\n- arxiv: [https://arxiv.org/abs/1811.10742](https://arxiv.org/abs/1811.10742)\n- github(official): [https://github.com/ucbdrive/3d-vehicle-tracking](https://github.com/ucbdrive/3d-vehicle-tracking)\n\n**Center-based 3D Object Detection and Tracking**\n\n- intro: UT Austin\n- intro: 3D Object Detection and Tracking using center points in the bird-eye view.\n- arxiv: [https://arxiv.org/abs/2006.11275](https://arxiv.org/abs/2006.11275)\n- github: [https://github.com/tianweiy/CenterPoint](https://github.com/tianweiy/CenterPoint)\n\n**3D Object Detection and Tracking Based on Streaming Data**\n\n- intro: ICRA 2020\n- arxiv: [https://arxiv.org/abs/2009.06169](https://arxiv.org/abs/2009.06169)\n\n**Uncertainty-Aware Voxel based 3D Object Detection and Tracking with von-Mises Loss**\n\n- intro: University of Michigan\n- arxiv: [https://arxiv.org/abs/2011.02553](https://arxiv.org/abs/2011.02553)\n\n**Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for Autonomous Driving**\n\n- intro: IROS 2021\n- arxiv: [https://arxiv.org/abs/2108.04602](https://arxiv.org/abs/2108.04602)\n\n# 3D MOT\n\n**AutoSelect: Automatic and Dynamic Detection Selection for 3D Multi-Object Tracking**\n\n- intro: Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/2012.05894](https://arxiv.org/abs/2012.05894)\n\n**Probabilistic 3D Multi-Modal, Multi-Object Tracking for Autonomous Driving**\n\n- intro: Stanford University & Toyota Research Institute\n- arxiv: [https://arxiv.org/abs/2012.13755](https://arxiv.org/abs/2012.13755)\n\n**Monocular Quasi-Dense 3D Object Tracking**\n\n[https://arxiv.org/abs/2103.07351](https://arxiv.org/abs/2103.07351)\n\n**Lite-FPN for Keypoint-based Monocular 3D Object Detection**\n\n- arxiv: [https://arxiv.org/abs/2105.00268](https://arxiv.org/abs/2105.00268)\n- github: [https://github.com/yanglei18/Lite-FPN](https://github.com/yanglei18/Lite-FPN)\n\n**RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection**\n\n- intro: CVPR 2021\n- intro: Waymo LLC & Google\n- arxiv: [https://arxiv.org/abs/2106.13365](https://arxiv.org/abs/2106.13365)\n\n**VIN: Voxel-based Implicit Network for Joint 3D Object Detection and Segmentation for Lidars**\n\n[https://arxiv.org/abs/2107.02980](https://arxiv.org/abs/2107.02980)\n\n**Geometry Uncertainty Projection Network for Monocular 3D Object Detection**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2107.13774](https://arxiv.org/abs/2107.13774)\n\n**Exploring Simple 3D Multi-Object Tracking for Autonomous Driving**\n\n- intro: ICCV 2021\n- intro: QCraft & Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/2108.10312](https://arxiv.org/abs/2108.10312)\n\n# Transformer\n\n**Point Transformer**\n\n- intro: Ulm University\n- keywords: SortNet\n- arxiv: [https://arxiv.org/abs/2011.00931](https://arxiv.org/abs/2011.00931)\n\n**Temporal-Channel Transformer for 3D Lidar-Based Video Object Detection in Autonomous Driving**\n\n[https://arxiv.org/abs/2011.13628](https://arxiv.org/abs/2011.13628)\n\n**Point Transformer**\n\n- intro: University of Oxford & The Chinese University of Hong Kong & Intel Labs\n- arxiv: [https://arxiv.org/abs/2012.09164](https://arxiv.org/abs/2012.09164)\n\n**3D Object Detection with Pointformer**\n\n- intro: Tsinghua University & BNRist & Alexa AI, Amazon / Columbia University\n- arxiv: [https://arxiv.org/abs/2012.11409](https://arxiv.org/abs/2012.11409)\n\n**M3DeTR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers**\n\n- intro: University of Maryland & Fudan University\n- arxiv: [https://arxiv.org/abs/2104.11896](https://arxiv.org/abs/2104.11896)\n\n**Improving 3D Object Detection with Channel-wise Transformer**\n\n- intro: ICCV 2021\n- intro: Zhejiang University & DAMO Academy, Alibaba Group\n- arxiv: [https://arxiv.org/abs/2108.10723](https://arxiv.org/abs/2108.10723)\n- github: [https://github.com/hlsheng1/CT3D](https://github.com/hlsheng1/CT3D)\n\n**TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers**\n\n- intro CVPR 2022\n- intro: Hong Kong University of Science and Technology & Huawei & City University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2203.11496](https://arxiv.org/abs/2203.11496)\n- github: [https://github.com/XuyangBai/TransFusion/](https://github.com/XuyangBai/TransFusion/)\n\n# Projects\n\n**OpenLidarPerceptron**\n\n- intro: OpenLidarPerceptron is an open source project for LiDAR-based 3D scene perception.\n- github: [https://github.com/open-mmlab/OpenLidarPerceptron](https://github.com/open-mmlab/OpenLidarPerceptron)\n\n**Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds**\n\n- github(PyTorch): [https://github.com/maudzung/SFA3D](https://github.com/maudzung/SFA3D)\n\n# Resources\n\n**Awesome-Automanous-3D-Detection-Methods**\n\n[https://github.com/tyjiang1997/awesome-Automanous-3D-detection-methods](https://github.com/tyjiang1997/awesome-Automanous-3D-detection-methods)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","title":"Neural Architecture Search"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Neural Architecture Search\ndate: 2015-10-09\n---\n\n# Papers\n\n**Neural Architecture Search with Reinforcement Learning**\n\n- intro: Google Brain\n- paper: [https://openreview.net/pdf?id=r1Ue8Hcxg](https://openreview.net/pdf?id=r1Ue8Hcxg)\n\n**Neural Optimizer Search with Reinforcement Learning**\n\n- intro: ICML 2017\n- arxiv: [https://arxiv.org/abs/1709.07417](https://arxiv.org/abs/1709.07417)\n\n**Learning Transferable Architectures for Scalable Image Recognition**\n\n- intro: Google Brain\n- keywords: Neural Architecture Search Network (NASNet), AutoML\n- arxiv: [https://arxiv.org/abs/1707.07012](https://arxiv.org/abs/1707.07012)\n- gtihub: [https://github.com//titu1994/Keras-NASNet](https://github.com//titu1994/Keras-NASNet)\n- blog: [https://research.googleblog.com/2017/11/automl-for-large-scale-image.html](https://research.googleblog.com/2017/11/automl-for-large-scale-image.html)\n- github: [https://github.com/titu1994/neural-architecture-search](https://github.com/titu1994/neural-architecture-search)\n\n**The First Step-by-Step Guide for Implementing Neural Architecture Search with Reinforcement Learning Using TensorFlow**\n\n- blog: [https://lab.wallarm.com/the-first-step-by-step-guide-for-implementing-neural-architecture-search-with-reinforcement-99ade71b3d28](https://lab.wallarm.com/the-first-step-by-step-guide-for-implementing-neural-architecture-search-with-reinforcement-99ade71b3d28)\n- github: [https://github.com/wallarm/nascell-automl](https://github.com/wallarm/nascell-automl)\n\n**Practical Network Blocks Design with Q-Learning**\n\n[https://arxiv.org/abs/1708.05552](https://arxiv.org/abs/1708.05552)\n\n**Transfer Learning to Learn with Multitask Neural Model Search**\n\n- intro: Stanford University & Google Research\n- keywords: Multitask Neural Model Search (MNMS)\n- arxiv: [https://arxiv.org/abs/1710.10776](https://arxiv.org/abs/1710.10776)\n\n**Simple And Efficient Architecture Search for Convolutional Neural Networks**\n\n- intro: Bosch Center for Artificial Intelligence & University of Freiburg\n- arxiv: [https://arxiv.org/abs/1711.04528](https://arxiv.org/abs/1711.04528)\n\n**Progressive Neural Architecture Search**\n\n- intri: Johns Hopkins University & Google Brain & Google Cloud & Stanford University & Google AI\n- arxiv: [https://arxiv.org/abs/1712.00559](https://arxiv.org/abs/1712.00559)\n\n**Finding Competitive Network Architectures Within a Day Using UCT**\n\n- intro: IBM Research AI – Ireland\n- arxiv: [https://arxiv.org/abs/1712.07420](https://arxiv.org/abs/1712.07420)\n\n**Regularized Evolution for Image Classifier Architecture Search**\n\n[https://arxiv.org/abs/1802.01548](https://arxiv.org/abs/1802.01548)\n\n**Efficient Neural Architecture Search via Parameters Sharing**\n\n- intro: Google Brain & CMU & Stanford University\n- arxiv: [https://arxiv.org/abs/1802.03268](https://arxiv.org/abs/1802.03268)\n- github: [https://github.com/carpedm20/ENAS-pytorch](https://github.com/carpedm20/ENAS-pytorch)\n- github: [https://github.com/melodyguan/enas](https://github.com/melodyguan/enas)\n\n**Neural Architecture Search with Bayesian Optimisation and Optimal Transport**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1802.07191](https://arxiv.org/abs/1802.07191)\n\n**AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search**\n\n- intro: Brown University & Northeastern University\n- arxiv: [https://arxiv.org/abs/1805.07440](https://arxiv.org/abs/1805.07440)\n\n**DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures**\n\n- intro: National Tsing-Hua University & Google\n[https://arxiv.org/abs/1806.08198](https://arxiv.org/abs/1806.08198)\n\n**DARTS: Differentiable Architecture Search**\n\n- intro: ICLR 2019\n- intro: Google & CMU\n- arxiv: [https://arxiv.org/abs/1806.09055](https://arxiv.org/abs/1806.09055)\n- github(official): [https://github.com/dragen1860/DARTS-PyTorch](https://github.com/dragen1860/DARTS-PyTorch)\n- gtihub: [https://github.com/quark0/darts](https://github.com/quark0/darts)\n\n**DARTS+: Improved Differentiable Architecture Search with Early Stopping**\n\n[https://arxiv.org/abs/1909.06035](https://arxiv.org/abs/1909.06035)\n\n**Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search**\n\n- intro: ICML 2018 AutoML Workshop\n- intro: University of Freiburg\n- arxiv: [https://arxiv.org/abs/1807.06906](https://arxiv.org/abs/1807.06906)\n\n**MnasNet: Platform-Aware Neural Architecture Search for Mobile**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1807.11626](https://arxiv.org/abs/1807.11626)\n- github: [https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet)\n\n**Efficient Progressive Neural Architecture Search**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1808.00391](https://arxiv.org/abs/1808.00391)\n\n**Reinforced Evolutionary Neural Architecture Search**\n\n[https://arxiv.org/abs/1808.00193](https://arxiv.org/abs/1808.00193)\n\n**Teacher Guided Architecture Search**\n\n[https://arxiv.org/abs/1808.01405](https://arxiv.org/abs/1808.01405)\n\n**BlockQNN: Efficient Block-wise Neural Network Architecture Generation**\n\n[https://arxiv.org/abs/1808.05584](https://arxiv.org/abs/1808.05584)\n\n**Neural Architecture Search: A Survey**\n\n- intro: Bosch Center for Artificial Intelligence & University of Freiburg\n- arxiv: [https://arxiv.org/abs/1808.05377](https://arxiv.org/abs/1808.05377)\n\n**Searching for Efficient Multi-Scale Architectures for Dense Image Prediction**\n\n- intro: NIPS 2018\n- intro: Google Inc.\n- arxiv: [https://arxiv.org/abs/1809.04184](https://arxiv.org/abs/1809.04184)\n\n**NSGA-NET: A Multi-Objective Genetic Algorithm for Neural Architecture Search**\n\n- arxiv: [https://arxiv.org/abs/1810.03522](https://arxiv.org/abs/1810.03522)\n- gtihub: [https://github.com/ianwhale/nsga-net](https://github.com/ianwhale/nsga-net)\n\n**Graph HyperNetworks for Neural Architecture Search**\n\n[https://arxiv.org/abs/1810.05749](https://arxiv.org/abs/1810.05749)\n\n**Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells**\n\n[https://arxiv.org/abs/1810.10804](https://arxiv.org/abs/1810.10804)\n\n**InstaNAS: Instance-aware Neural Architecture Search**\n\n- intro: ICML 2019 AutoML Workshop\n- project page: [https://hubert0527.github.io/InstaNAS/](https://hubert0527.github.io/InstaNAS/)\n- arxiv: [https://arxiv.org/abs/1811.10201](https://arxiv.org/abs/1811.10201)\n- github: [https://github.com/AnjieZheng/InstaNAS](https://github.com/AnjieZheng/InstaNAS)\n\n**ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware**\n\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1812.00332](https://arxiv.org/abs/1812.00332)\n- github: [https://github.com/MIT-HAN-LAB/ProxylessNAS](https://github.com/MIT-HAN-LAB/ProxylessNAS)\n\n**FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search**\n\n- intro: UC Berkeley & Princeton University & Facebook Inc.\n- arxiv: [https://arxiv.org/abs/1812.03443](https://arxiv.org/abs/1812.03443)\n\n**IRLAS: Inverse Reinforcement Learning for Architecture Search**\n\n- intro: SenseTime & CUHK\n- arxiv: [https://arxiv.org/abs/1812.05285](https://arxiv.org/abs/1812.05285)\n\n**EAT-NAS: Elastic Architecture Transfer for Accelerating Large-scale Neural Architecture Search**\n\n- intro: Huazhong University of Science and Technology & Horizon Robotics & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1901.05884](https://arxiv.org/abs/1901.05884)\n\n**DVOLVER: Efficient Pareto-Optimal Neural Network Architecture Search**\n\n[https://arxiv.org/abs/1902.01654](https://arxiv.org/abs/1902.01654)\n\n**MFAS: Multimodal Fusion Architecture Search**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.06496](https://arxiv.org/abs/1903.06496)\n\n**Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural Architecture Search**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.03777](https://arxiv.org/abs/1903.03777)\n\n**sharpDARTS: Faster and More Accurate Differentiable Architecture Search**\n\n[https://arxiv.org/abs/1903.09900](https://arxiv.org/abs/1903.09900)\n\n**Network Slimming by Slimmable Networks: Towards One-Shot Architecture Search for Channel Numbers**\n\n[https://arxiv.org/abs/1903.11728](https://arxiv.org/abs/1903.11728)\n\n**AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search**\n\n- intro: Brown University & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1903.11059](https://arxiv.org/abs/1903.11059)\n- github: [https://github.com/linnanwang/AlphaX-NASBench101](https://github.com/linnanwang/AlphaX-NASBench101)\n\n**Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours**\n\n[https://arxiv.org/abs/1904.02877](https://arxiv.org/abs/1904.02877)\n[https://github.com/dstamoulis/single-path-nas](https://github.com/dstamoulis/single-path-nas)\n\n**Resource Constrained Neural Network Architecture Search**\n\n[https://arxiv.org/abs/1904.03786](https://arxiv.org/abs/1904.03786)\n\n**NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.07392](https://arxiv.org/abs/1904.07392)\n\n**Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation**\n\n[https://arxiv.org/abs/1904.12760](https://arxiv.org/abs/1904.12760)\n\n**Searching for MobileNetV3**\n\n- intro: Google AI & Google Brain\n- arxiv: [https://arxiv.org/abs/1905.02244](https://arxiv.org/abs/1905.02244)\n\n**Network Pruning via Transformable Architecture Search**\n\n- arxiv: [https://arxiv.org/abs/1905.09717](https://arxiv.org/abs/1905.09717)\n- github: [https://github.com/D-X-Y/TAS](https://github.com/D-X-Y/TAS)\n\n**EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks**\n\n- intro: ICML 2019\n- arxiv: [https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946)\n- github(TensorFlow): [https://github.com/mingxingtan/efficientnet](https://github.com/mingxingtan/efficientnet)\n- github: [https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)\n\n**EfficientNetV2: Smaller Models and Faster Training**\n\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/2104.00298](https://arxiv.org/abs/2104.00298)\n- github: [https://github.com/google/automl/tree/master/efficientnetv2](https://github.com/google/automl/tree/master/efficientnetv2)\n\n**Dynamic Distribution Pruning for Efficient Network Architecture Search**\n\n- arxiv: [https://arxiv.org/abs/1905.13543](https://arxiv.org/abs/1905.13543)\n- github: [https://github.com/tanglang96/DDPNAS](https://github.com/tanglang96/DDPNAS)\n\n**XNAS: Neural Architecture Search with Expert Advice**\n\n- intro: Machine Intelligence Technology, Alibaba Group\n- arxiv: [https://arxiv.org/abs/1906.08031](https://arxiv.org/abs/1906.08031)\n\n**Densely Connected Search Space for More Flexible Neural Architecture Search**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/1906.09607](https://arxiv.org/abs/1906.09607)\n- github(official): [https://github.com/JaminFong/DenseNAS](https://github.com/JaminFong/DenseNAS)\n\n**FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search**\n\n- intro: Xiaomi AI Lab\n- arxiv: [https://arxiv.org/abs/1907.01845](https://arxiv.org/abs/1907.01845)\n- github: [https://github.com/fairnas/FairNAS](https://github.com/fairnas/FairNAS)\n\n**PC-DARTS: Partial Channel Connections for Memory-Efficient Differentiable Architecture Search**\n\n- arxiv: [https://arxiv.org/abs/1907.05737](https://arxiv.org/abs/1907.05737)\n- github: [https://github.com/yuhuixu1993/PC-DARTS](https://github.com/yuhuixu1993/PC-DARTS)\n\n**XferNAS: Transfer Neural Architecture Search**\n\n[https://arxiv.org/abs/1907.08307](https://arxiv.org/abs/1907.08307)\n\n**ScarletNAS: Bridging the Gap Between Scalability and Fairness in Neural Architecture Search**\n\n- intro: Xiaomi AI Lab\n- arxiv: [https://arxiv.org/abs/1908.06022](https://arxiv.org/abs/1908.06022)\n- github: [https://github.com/xiaomi-automl/SCARLET-NAS](https://github.com/xiaomi-automl/SCARLET-NAS)\n\n**MANAS: Multi-Agent Neural Architecture Search**\n\n[https://arxiv.org/abs/1909.01051](https://arxiv.org/abs/1909.01051)\n\n**HM-NAS: Efficient Neural Architecture Search via Hierarchical Masking**\n\n- intro: ICCV 2019 Neural Architects Workshop\n- arxiv: [https://arxiv.org/abs/1909.00122](https://arxiv.org/abs/1909.00122)\n\n**CARS: Continuous Evolution for Efficient Neural Architecture Search**\n\n[https://arxiv.org/abs/1909.04977](https://arxiv.org/abs/1909.04977)\n\n**Understanding and Robustifying Differentiable Architecture Search**\n\n[https://arxiv.org/abs/1909.09656](https://arxiv.org/abs/1909.09656)\n\n**StacNAS: Towards stable and consistent optimization for differentiable Neural Architecture Search**\n\n[https://arxiv.org/abs/1909.11926](https://arxiv.org/abs/1909.11926)\n\n**Scheduled Differentiable Architecture Search for Visual Recognition**\n\n[https://arxiv.org/abs/1909.10236](https://arxiv.org/abs/1909.10236)\n\n**Searching for A Robust Neural Architecture in Four GPU Hours**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1910.04465](https://arxiv.org/abs/1910.04465)\n- github: [https://github.com/D-X-Y/NAS-Projects](https://github.com/D-X-Y/NAS-Projects)\n\n**One-Shot Neural Architecture Search via Self-Evaluated Template Network**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.05733](https://arxiv.org/abs/1910.05733)\n- github: [https://github.com/D-X-Y/NAS-Projects](https://github.com/D-X-Y/NAS-Projects)\n\n**Binarized Neural Architecture Search**\n\n[https://arxiv.org/abs/1911.10862](https://arxiv.org/abs/1911.10862)\n\n**Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search**\n\n- intro: Xiaomi AI Lab\n- arxiv: [https://arxiv.org/abs/1911.12126](https://arxiv.org/abs/1911.12126)\n- github: [https://github.com/xiaomi-automl/FairDARTS](https://github.com/xiaomi-automl/FairDARTS)\n\n**Blockwisely Supervised Neural Architecture Search with Knowledge Distillation**\n\n- intro: CVPR 2020\n- intro: 1DarkMatter AI Research & Monash University & Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/1911.13053](https://arxiv.org/abs/1911.13053)\n- github: [https://github.com/changlin31/DNA](https://github.com/changlin31/DNA)\n\n**AtomNAS: Fine-Grained End-to-End Neural Architecture Search**\n\n- intro: ICLR 2020\n- arxiv: [https://arxiv.org/abs/1912.09640](https://arxiv.org/abs/1912.09640)\n- github: [https://github.com/meijieru/AtomNAS](https://github.com/meijieru/AtomNAS)\n\n**BATS: Binary ArchitecTure Search**\n\n- arxiv: [https://arxiv.org/abs/2003.01711](https://arxiv.org/abs/2003.01711)\n- github: [https://github.com/1adrianb/binary-nas](https://github.com/1adrianb/binary-nas)\n\n**GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet**\n\n- intro: CVPR 2020\n- intro: SenseTime & Tsinghua University & HUST\n- arxiv: [https://arxiv.org/abs/2003.11236](https://arxiv.org/abs/2003.11236)\n\n**BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models**\n\n- intro:  Google Brain &  University of Illinois at Urbana-Champaign\n- arxiv: [https://arxiv.org/abs/2003.11142](https://arxiv.org/abs/2003.11142)\n\n**MTL-NAS: Task-Agnostic Neural Architecture Search towards General-Purpose Multi-Task Learning**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.14058](https://arxiv.org/abs/2003.14058)\n- github: [https://github.com/bhpfelix/MTLNAS](https://github.com/bhpfelix/MTLNAS)\n\n**FedNAS: Federated Deep Learning via Neural Architecture Search**\n\n- intro: CVPR 2020\n- intro: University of Southern California\n- arxiv: [https://arxiv.org/abs/2004.08546](https://arxiv.org/abs/2004.08546)\n- github: [https://github.com/chaoyanghe/FedNAS](https://github.com/chaoyanghe/FedNAS)\n\n**Rethinking Performance Estimation in Neural Architecture Search**\n\n[https://arxiv.org/abs/2005.09917](https://arxiv.org/abs/2005.09917)\n\n**AutoHAS: Differentiable Hyper-parameter and Architecture Search**\n\n[https://arxiv.org/abs/2006.03656](https://arxiv.org/abs/2006.03656)\n\n**Breaking the Curse of Space Explosion: Towards Efficient NAS with Curriculum Searc**\n\n- intro: ICML 2020\n- intro: Tencent AI Lab\n- arxiv: [https://arxiv.org/abs/2007.07197](https://arxiv.org/abs/2007.07197)\n\n**Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search**\n\n- intro: NeurIPS 2020\n- arxiv: [https://arxiv.org/abs/2010.15821](https://arxiv.org/abs/2010.15821)\n- github: [https://github.com/microsoft/cream](https://github.com/microsoft/cream)\n\n**UniNet: Unified Architecture Search with Convolution, Transformer, and MLP**\n\n- intro: CUHK-SenseTime Joint Laboratory & The Chinese University of Hong Kong & SenseTime Research\n- arxiv: [https://arxiv.org/abs/2110.04035](https://arxiv.org/abs/2110.04035)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","title":"Natural Language Processing"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Natural Language Processing\r\ndate: 2015-10-09\r\n---\r\n\r\n# Tutorials\r\n\r\n![](/assets/deep_learning/NLP/Deep Learning For NLP.jpg)\r\n\r\n**Practical Neural Networks for NLP**\r\n\r\n- intro: EMNLP 2016\r\n- github: [https://github.com/clab/dynet_tutorial_examples](https://github.com/clab/dynet_tutorial_examples)\r\n\r\n**Structured Neural Networks for NLP: From Idea to Code**\r\n\r\n- slides: [https://github.com/neubig/yrsnlp-2016/blob/master/neubig16yrsnlp.pdf](https://github.com/neubig/yrsnlp-2016/blob/master/neubig16yrsnlp.pdf)\r\n- github: [https://github.com/neubig/yrsnlp-2016](https://github.com/neubig/yrsnlp-2016)\r\n\r\n**Understanding Deep Learning Models in NLP**\r\n\r\n[http://nlp.yvespeirsman.be/blog/understanding-deeplearning-models-nlp/](http://nlp.yvespeirsman.be/blog/understanding-deeplearning-models-nlp/)\r\n\r\n**Deep learning for natural language processing, Part 1**\r\n\r\n[https://softwaremill.com/deep-learning-for-nlp/](https://softwaremill.com/deep-learning-for-nlp/)\r\n\r\n# Neural Models\r\n\r\n**Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models**\r\n\r\n- intro: NIPS 2014 deep learning workshop\r\n- arxiv: [http://arxiv.org/abs/1411.2539](http://arxiv.org/abs/1411.2539)\r\n- github: [https://github.com/ryankiros/visual-semantic-embedding](https://github.com/ryankiros/visual-semantic-embedding)\r\n- results: [http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html](http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html)\r\n- demo: [http://deeplearning.cs.toronto.edu/i2t](http://deeplearning.cs.toronto.edu/i2t)\r\n\r\n**Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1503.00075](http://arxiv.org/abs/1503.00075)\r\n- github: [https://github.com/stanfordnlp/treelstm](https://github.com/stanfordnlp/treelstm)\r\n- github(Theano): [https://github.com/ofirnachum/tree_rnn](https://github.com/ofirnachum/tree_rnn)\r\n\r\n**Visualizing and Understanding Neural Models in NLP**\r\n\r\n![](https://camo.githubusercontent.com/bd5126b7abf6c3dfa50ca1cd5c93fc559ec9eb35/687474703a2f2f7374616e666f72642e6564752f2537456a697765696c2f76697375616c312e706e67)\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.01066](http://arxiv.org/abs/1506.01066)\r\n- github: [https://github.com/jiweil/Visualizing-and-Understanding-Neural-Models-in-NLP](https://github.com/jiweil/Visualizing-and-Understanding-Neural-Models-in-NLP)\r\n\r\n**Character-Aware Neural Language Models**\r\n\r\n- paper: [http://arxiv.org/abs/1508.06615](http://arxiv.org/abs/1508.06615)\r\n- github: [https://github.com/yoonkim/lstm-char-cnn](https://github.com/yoonkim/lstm-char-cnn)\r\n\r\n**Skip-Thought Vectors**\r\n\r\n- paper: [http://arxiv.org/abs/1506.06726](http://arxiv.org/abs/1506.06726)\r\n- github: [https://github.com/ryankiros/skip-thoughts](https://github.com/ryankiros/skip-thoughts)\r\n\r\n**A Primer on Neural Network Models for Natural Language Processing**\r\n\r\n- arxiv: [http://arxiv.org/abs/1510.00726](http://arxiv.org/abs/1510.00726)\r\n\r\n**Character-aware Neural Language Models**\r\n\r\n- arxiv: [http://arxiv.org/abs/1508.06615](http://arxiv.org/abs/1508.06615)\r\n\r\n**Neural Variational Inference for Text Processing**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.06038](http://arxiv.org/abs/1511.06038)\r\n- notes: [http://dustintran.com/blog/neural-variational-inference-for-text-processing/](http://dustintran.com/blog/neural-variational-inference-for-text-processing/)\r\n- github: [https://github.com/carpedm20/variational-text-tensorflow](https://github.com/carpedm20/variational-text-tensorflow)\r\n- github: [https://github.com/cheng6076/NVDM](https://github.com/cheng6076/NVDM)\r\n\r\n# Sequence to Sequence Learning\r\n\r\n**Generating Text with Deep Reinforcement Learning**\r\n\r\n- intro: NIPS 2015\r\n- arxiv: [http://arxiv.org/abs/1510.09202](http://arxiv.org/abs/1510.09202)\r\n\r\n**MUSIO: A Deep Learning based Chatbot Getting Smarter**\r\n\r\n- homepage: [http://ec2-204-236-149-143.us-west-1.compute.amazonaws.com:9000/](http://ec2-204-236-149-143.us-west-1.compute.amazonaws.com:9000/)\r\n- github(Torch7): [https://github.com/deepcoord/seq2seq](https://github.com/deepcoord/seq2seq)\r\n\r\n# Translation\r\n\r\n**Learning phrase representations using rnn encoder-decoder for statistical machine translation**\r\n\r\n- intro: GRU. EMNLP 2014\r\n- arxiv: [http://arxiv.org/abs/1406.1078](http://arxiv.org/abs/1406.1078)\r\n\r\n**Neural Machine Translation by Jointly Learning to Align and Translate**\r\n\r\n- intro: ICLR 2015\r\n- arxiv: [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)\r\n- github: [https://github.com/lisa-groundhog/GroundHog](https://github.com/lisa-groundhog/GroundHog)\r\n\r\n**Multi-Source Neural Translation**\r\n\r\n- intro: \"report up to +4.8 Bleu increases on top of a very strong attention-based neural translation model.\"\r\n- arxiv: [Multi-Source Neural Translation](Multi-Source Neural Translation)\r\n- github(Zoph_RNN): [https://github.com/isi-nlp/Zoph_RNN](https://github.com/isi-nlp/Zoph_RNN)\r\n- video: [http://research.microsoft.com/apps/video/default.aspx?id=260336](http://research.microsoft.com/apps/video/default.aspx?id=260336)\r\n\r\n**Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.01073](http://arxiv.org/abs/1601.01073)\r\n- github: [https://github.com/nyu-dl/dl4mt-multi](https://github.com/nyu-dl/dl4mt-multi)\r\n- notes: [https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/multi-way-nmt-shared-attention.md](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/multi-way-nmt-shared-attention.md)\r\n\r\n**Modeling Coverage for Neural Machine Translation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.04811](http://arxiv.org/abs/1601.04811)\r\n\r\n**A Character-level Decoder without Explicit Segmentation for Neural Machine Translation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.06147](http://arxiv.org/abs/1603.06147)\r\n- github: [https://github.com/nyu-dl/dl4mt-cdec](https://github.com/nyu-dl/dl4mt-cdec)\r\n\r\n**NEMATUS: Attention-based encoder-decoder model for neural machine translation**\r\n\r\n- github: [https://github.com/rsennrich/nematus](https://github.com/rsennrich/nematus)\r\n\r\n**Variational Neural Machine Translation**\r\n\r\n- intro: EMNLP 2016\r\n- arxiv: [https://arxiv.org/abs/1605.07869](https://arxiv.org/abs/1605.07869)\r\n- github: [https://github.com/DeepLearnXMU/VNMT](https://github.com/DeepLearnXMU/VNMT)\r\n\r\n**Neural Network Translation Models for Grammatical Error Correction**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.00189](http://arxiv.org/abs/1606.00189)\r\n\r\n**Linguistic Input Features Improve Neural Machine Translation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.02892](http://arxiv.org/abs/1606.02892)\r\n- github: [https://github.com/rsennrich/nematus](https://github.com/rsennrich/nematus)\r\n\r\n**Sequence-Level Knowledge Distillation**\r\n\r\n- intro: EMNLP 2016\r\n- arxiv: [http://arxiv.org/abs/1606.07947](http://arxiv.org/abs/1606.07947)\r\n- github: [https://github.com/harvardnlp/nmt-android](https://github.com/harvardnlp/nmt-android)\r\n\r\n**Neural Machine Translation: Breaking the Performance Plateau**\r\n\r\n- slides: [http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf](http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf)\r\n\r\n**Tips on Building Neural Machine Translation Systems**\r\n\r\n- github: [https://github.com/neubig/nmt-tips](https://github.com/neubig/nmt-tips)\r\n\r\n**Semi-Supervised Learning for Neural Machine Translation**\r\n\r\n- intro: ACL 2016. Tsinghua University & Baidu Inc\r\n- arxiv: [http://arxiv.org/abs/1606.04596](http://arxiv.org/abs/1606.04596)\r\n\r\n**EUREKA-MangoNMT: A C++ toolkit for neural machine translation for CPU**\r\n\r\n- github: [https://github.com/jiajunzhangnlp/EUREKA-MangoNMT](https://github.com/jiajunzhangnlp/EUREKA-MangoNMT)\r\n\r\n**Deep Character-Level Neural Machine Translation**\r\n\r\n![](https://raw.githubusercontent.com/SwordYork/DCNMT/master/dcnmt.png)\r\n\r\n- github: [https://github.com/SwordYork/DCNMT](https://github.com/SwordYork/DCNMT)\r\n\r\n**Neural Machine Translation Implementations**\r\n\r\n- github: [https://github.com/jonsafari/nmt-list](https://github.com/jonsafari/nmt-list)\r\n\r\n**Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.08144v1](http://arxiv.org/abs/1609.08144v1)\r\n\r\n**Learning to Translate in Real-time with Neural Machine Translation**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.00388](https://arxiv.org/abs/1610.00388)\r\n\r\n**Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.01108](https://arxiv.org/abs/1610.01108)\r\n- github: [https://github.com/emjotde/amunmt](https://github.com/emjotde/amunmt)\r\n\r\n**Fully Character-Level Neural Machine Translation without Explicit Segmentation**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.03017](https://arxiv.org/abs/1610.03017)\r\n- github: [https://github.com/nyu-dl/dl4mt-c2c](https://github.com/nyu-dl/dl4mt-c2c)\r\n\r\n**Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.03164](https://arxiv.org/abs/1610.03164)\r\n\r\n**Neural Machine Translation in Linear Time**\r\n\r\n- intro: ByteNet\r\n- arxiv: [https://arxiv.org/abs/1610.10099](https://arxiv.org/abs/1610.10099)\r\n- github: [https://github.com/paarthneekhara/byteNet-tensorflow](https://github.com/paarthneekhara/byteNet-tensorflow)\r\n- github(Tensorflow): [https://github.com/buriburisuri/ByteNet](https://github.com/buriburisuri/ByteNet)\r\n\r\n**Neural Machine Translation with Reconstruction**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.01874](https://arxiv.org/abs/1611.01874)\r\n\r\n**A Convolutional Encoder Model for Neural Machine Translation**\r\n\r\n- intro: ACL 2017. Facebook AI Research\r\n- arxiv: [https://arxiv.org/abs/1611.02344](https://arxiv.org/abs/1611.02344)\r\n- github: [https://github.com//pravarmahajan/cnn-encoder-nmt](https://github.com//pravarmahajan/cnn-encoder-nmt)\r\n\r\n**Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.04798](https://arxiv.org/abs/1611.04798)\r\n\r\n**MXNMT: MXNet based Neural Machine Translation**\r\n\r\n- github: [https://github.com/magic282/MXNMT](https://github.com/magic282/MXNMT)\r\n\r\n**Doubly-Attentive Decoder for Multi-modal Neural Machine Translation**\r\n\r\n- intro: Dublin City University & Trinity College Dublin\r\n- arxiv: [https://arxiv.org/abs/1702.01287](https://arxiv.org/abs/1702.01287)\r\n\r\n**Massive Exploration of Neural Machine Translation Architectures**\r\n\r\n- intro: Google Brain\r\n- arxiv: [https://arxiv.org/abs/1703.03906](https://arxiv.org/abs/1703.03906)\r\n- github: [https://github.com/google/seq2seq/](https://github.com/google/seq2seq/)\r\n\r\n**Depthwise Separable Convolutions for Neural Machine Translation**\r\n\r\n- intro: Google Brain & University of Toronto\r\n- arxiv: [https://arxiv.org/abs/1706.03059](https://arxiv.org/abs/1706.03059)\r\n\r\n**Deep Architectures for Neural Machine Translation**\r\n\r\n- intro: WMT 2017 research track. University of Edinburgh & Charles University\r\n- arxiv: [https://arxiv.org/abs/1707.07631](https://arxiv.org/abs/1707.07631)\r\n- github: [https://github.com/Avmb/deep-nmt-architectures](https://github.com/Avmb/deep-nmt-architectures)\r\n\r\n**Marian: Fast Neural Machine Translation in C++**\r\n\r\n- intro: Microsoft & Adam Mickiewicz University in Poznan & University of Edinburgh\r\n- homepage: [https://marian-nmt.github.io/](https://marian-nmt.github.io/)\r\n- arxiv: [https://arxiv.org/abs/1804.00344](https://arxiv.org/abs/1804.00344)\r\n- github: [https://github.com/marian-nmt/marian](https://github.com/marian-nmt/marian)\r\n\r\n**Sockeye**\r\n\r\n- intro: Sequence-to-sequence framework with a focus on Neural Machine Translation based on Apache MXNet\r\n- arxiv: [https://github.com/awslabs/sockeye/](https://github.com/awslabs/sockeye/)\r\n\r\n# Summarization\r\n\r\n**Extraction of Salient Sentences from Labelled Documents**\r\n\r\n- arxiv: [http://arxiv.org/abs/1412.6815](http://arxiv.org/abs/1412.6815)\r\n- github: [https://github.com/mdenil/txtnets](https://github.com/mdenil/txtnets)\r\n- notes: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2014/06/model-visualizing-summarising-conv-net.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2014/06/model-visualizing-summarising-conv-net.md)\r\n\r\n**A Neural Attention Model for Abstractive Sentence Summarization**\r\n\r\n- intro: EMNLP 2015. Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1509.00685](http://arxiv.org/abs/1509.00685)\r\n- github: [https://github.com/facebook/NAMAS](https://github.com/facebook/NAMAS)\r\n- github(TensorFlow): [https://github.com/carpedm20/neural-summary-tensorflow](https://github.com/carpedm20/neural-summary-tensorflow)\r\n\r\n**A Convolutional Attention Network for Extreme Summarization of Source Code**\r\n\r\n![](https://camo.githubusercontent.com/95dfe4b12b966b664fd441b19430405520a859a9/687474703a2f2f7333322e706f7374696d672e6f72672f7263326664793079642f53637265656e5f53686f745f323031365f30355f30395f61745f31305f31385f33365f504d2e706e67)\r\n\r\n- homepage: [http://groups.inf.ed.ac.uk/cup/codeattention/](http://groups.inf.ed.ac.uk/cup/codeattention/)\r\n- arxiv: [http://arxiv.org/abs/1602.03001](http://arxiv.org/abs/1602.03001)\r\n- github: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2016/02/conv-attention-network-source-code-summarization.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2016/02/conv-attention-network-source-code-summarization.md)\r\n\r\n**Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond**\r\n\r\n- intro: BM Watson & Université de Montréal\r\n- arxiv: [http://arxiv.org/abs/1602.06023](http://arxiv.org/abs/1602.06023)\r\n\r\n**textsum: Text summarization with TensorFlow**\r\n\r\n- blog: [https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html](https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html)\r\n- github: [https://github.com/tensorflow/models/tree/master/textsum](https://github.com/tensorflow/models/tree/master/textsum)\r\n\r\n**How to Run Text Summarization with TensorFlow**\r\n\r\n- blog: [https://medium.com/@surmenok/how-to-run-text-summarization-with-tensorflow-d4472587602d#.mll1rqgjg](https://medium.com/@surmenok/how-to-run-text-summarization-with-tensorflow-d4472587602d#.mll1rqgjg)\r\n- github: [https://github.com/surmenok/TextSum](https://github.com/surmenok/TextSum)\r\n\r\n# Reading Comprehension\r\n\r\n**Text Comprehension with the Attention Sum Reader Network**\r\n\r\n**Text Understanding with the Attention Sum Reader Network**\r\n\r\n- intro: ACL 2016\r\n- arxiv: [https://arxiv.org/abs/1603.01547](https://arxiv.org/abs/1603.01547)\r\n- github: [https://github.com/rkadlec/asreader](https://github.com/rkadlec/asreader)\r\n\r\n**A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.02858](http://arxiv.org/abs/1606.02858)\r\n- github: [https://github.com/danqi/rc-cnn-dailymail](https://github.com/danqi/rc-cnn-dailymail)\r\n\r\n**Consensus Attention-based Neural Networks for Chinese Reading Comprehension**\r\n\r\n- arxiv: [http://arxiv.org/abs/1607.02250](http://arxiv.org/abs/1607.02250)\r\n- dataset(\"HFL-RC\"): [http://hfl.iflytek.com/chinese-rc/](http://hfl.iflytek.com/chinese-rc/)\r\n\r\n**Separating Answers from Queries for Neural Reading Comprehension**\r\n\r\n- arxiv: [http://arxiv.org/abs/1607.03316](http://arxiv.org/abs/1607.03316)\r\n- github: [https://github.com/dirkweissenborn/qa_network](https://github.com/dirkweissenborn/qa_network)\r\n\r\n**Attention-over-Attention Neural Networks for Reading Comprehension**\r\n\r\n- arxiv: [http://arxiv.org/abs/1607.04423](http://arxiv.org/abs/1607.04423)\r\n- github: [https://github.com/OlavHN/attention-over-attention](https://github.com/OlavHN/attention-over-attention)\r\n\r\n**Teaching Machines to Read and Comprehend CNN News and Children Books using Torch**\r\n\r\n- github: [https://github.com/ganeshjawahar/torch-teacher](https://github.com/ganeshjawahar/torch-teacher)\r\n\r\n**Reasoning with Memory Augmented Neural Networks for Language Comprehension**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.06454](https://arxiv.org/abs/1610.06454)\r\n\r\n**Bidirectional Attention Flow: Bidirectional Attention Flow for Machine Comprehension**\r\n\r\n- project page: [https://allenai.github.io/bi-att-flow/](https://allenai.github.io/bi-att-flow/)\r\n- github: [https://github.com/allenai/bi-att-flow](https://github.com/allenai/bi-att-flow)\r\n\r\n**NewsQA: A Machine Comprehension Dataset**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.09830](https://arxiv.org/abs/1611.09830)\r\n- dataset: [http://datasets.maluuba.com/NewsQA](http://datasets.maluuba.com/NewsQA)\r\n- github: [https://github.com/Maluuba/newsqa](https://github.com/Maluuba/newsqa)\r\n\r\n**Gated-Attention Readers for Text Comprehension**\r\n\r\n- intro: CMU\r\n- arxiv: [https://arxiv.org/abs/1606.01549](https://arxiv.org/abs/1606.01549)\r\n- github: [https://github.com/bdhingra/ga-reader](https://github.com/bdhingra/ga-reader)\r\n\r\n**Get To The Point: Summarization with Pointer-Generator Networks**\r\n\r\n- intro: ACL 2017. Stanford University & Google Brain\r\n- arxiv: [https://arxiv.org/abs/1704.04368](https://arxiv.org/abs/1704.04368)\r\n- github: [https://github.com/abisee/pointer-generator](https://github.com/abisee/pointer-generator)\r\n\r\n# Language Understanding\r\n\r\n**Recurrent Neural Networks with External Memory for Language Understanding**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.00195](http://arxiv.org/abs/1506.00195)\r\n- github: [https://github.com/npow/RNN-EM](https://github.com/npow/RNN-EM)\r\n\r\n**Neural Semantic Encoders**\r\n\r\n- intro: EACL 2017\r\n- arxiv: [https://arxiv.org/abs/1607.04315](https://arxiv.org/abs/1607.04315)\r\n- github(Keras): [https://github.com/pdasigi/neural-semantic-encoders](https://github.com/pdasigi/neural-semantic-encoders)\r\n\r\n**Neural Tree Indexers for Text Understanding**\r\n\r\n- arxiv: [https://arxiv.org/abs/1607.04492](https://arxiv.org/abs/1607.04492)\r\n- bitbucket: [https://bitbucket.org/tsendeemts/nti/src](https://bitbucket.org/tsendeemts/nti/src)\r\n\r\n**Better Text Understanding Through Image-To-Text Transfer**\r\n\r\n- intro: Google Brain & Technische Universität München\r\n- arxiv: [https://arxiv.org/abs/1705.08386](https://arxiv.org/abs/1705.08386)\r\n\r\n# Text Classification\r\n\r\n**Convolutional Neural Networks for Sentence Classification**\r\n\r\n- intro: EMNLP 2014\r\n- arxiv: [http://arxiv.org/abs/1408.5882](http://arxiv.org/abs/1408.5882)\r\n- github(Theano): [https://github.com/yoonkim/CNN_sentence](https://github.com/yoonkim/CNN_sentence)\r\n- github(Torch): [https://github.com/harvardnlp/sent-conv-torch](https://github.com/harvardnlp/sent-conv-torch)\r\n- github(Keras): [https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras](https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras)\r\n- github(Tensorflow): [https://github.com/abhaikollara/CNN-Sentence-Classification](https://github.com/abhaikollara/CNN-Sentence-Classification)\r\n\r\n**Recurrent Convolutional Neural Networks for Text Classification**\r\n\r\n- paper: [http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552](http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552)\r\n- github: [https://github.com/knok/rcnn-text-classification](https://github.com/knok/rcnn-text-classification)\r\n\r\n**Character-level Convolutional Networks for Text Classification**\r\n\r\n- intro: NIPS 2015. \"Text Understanding from Scratch\"\r\n- arxiv: [http://arxiv.org/abs/1509.01626](http://arxiv.org/abs/1509.01626)\r\n- github: [https://github.com/zhangxiangxiao/Crepe](https://github.com/zhangxiangxiao/Crepe)\r\n- datasets: [http://goo.gl/JyCnZq](http://goo.gl/JyCnZq)\r\n- github(TensorFlow): [https://github.com/mhjabreel/CharCNN](https://github.com/mhjabreel/CharCNN)\r\n\r\n**A C-LSTM Neural Network for Text Classification**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.08630](http://arxiv.org/abs/1511.08630)\r\n\r\n**Rationale-Augmented Convolutional Neural Networks for Text Classification**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.04469](http://arxiv.org/abs/1605.04469)\r\n\r\n**Text classification using DIGITS and Torch7**\r\n\r\n- github: [https://github.com/NVIDIA/DIGITS/tree/master/examples/text-classification](https://github.com/NVIDIA/DIGITS/tree/master/examples/text-classification)\r\n\r\n**Recurrent Neural Network for Text Classification with Multi-Task Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.05101](http://arxiv.org/abs/1605.05101)\r\n\r\n**Deep Multi-Task Learning with Shared Memory**\r\n\r\n- intro: EMNLP 2016\r\n- arxiv: [https://arxiv.org/abs/1609.07222](https://arxiv.org/abs/1609.07222)\r\n\r\n**Virtual Adversarial Training for Semi-Supervised Text Classification**\r\n\r\n**Adversarial Training Methods for Semi-Supervised Text Classification**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.07725](http://arxiv.org/abs/1605.07725)\r\n- notes: [https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/adversarial-text-classification.md](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/adversarial-text-classification.md)\r\n\r\n**Sentence Convolution Code in Torch: Text classification using a convolutional neural network**\r\n\r\n- github: [https://github.com/harvardnlp/sent-conv-torch](https://github.com/harvardnlp/sent-conv-torch)\r\n\r\n**Bag of Tricks for Efficient Text Classification**\r\n\r\n- intro: Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1607.01759](http://arxiv.org/abs/1607.01759)\r\n- github: [https://github.com/kemaswill/fasttext_torch](https://github.com/kemaswill/fasttext_torch)\r\n- github: [https://github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)\r\n\r\n**Actionable and Political Text Classification using Word Embeddings and LSTM**\r\n\r\n- arxiv: [http://arxiv.org/abs/1607.02501](http://arxiv.org/abs/1607.02501)\r\n\r\n**Implementing a CNN for Text Classification in TensorFlow**\r\n\r\n- blog: [http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)\r\n\r\n**fancy-cnn: Multiparadigm Sequential Convolutional Neural Networks for text classification**\r\n\r\n- github: [https://github.com/textclf/fancy-cnn](https://github.com/textclf/fancy-cnn)\r\n\r\n**Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.00718](http://arxiv.org/abs/1609.00718)\r\n\r\n**Tweet Classification using RNN and CNN**\r\n\r\n- github: [https://github.com/ganeshjawahar/tweet-classify](https://github.com/ganeshjawahar/tweet-classify)\r\n\r\n**Hierarchical Attention Networks for Document Classification**\r\n\r\n- intro: CMU & MSR. NAACL 2016\r\n- paper: [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)\r\n- github(TensorFlow): [https://github.com/raviqqe/tensorflow-font2char2word2sent2doc](https://github.com/raviqqe/tensorflow-font2char2word2sent2doc)\r\n- github(TensorFlow): [https://github.com/ematvey/deep-text-classifier](https://github.com/ematvey/deep-text-classifier)\r\n\r\n**AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.01884](https://arxiv.org/abs/1611.01884)\r\n- github(MXNet): [https://github.com/Ldpe2G/AC-BLSTM](https://github.com/Ldpe2G/AC-BLSTM)\r\n\r\n**Generative and Discriminative Text Classification with Recurrent Neural Networks**\r\n\r\n- intro: DeepMind\r\n- arxiv: [https://arxiv.org/abs/1703.01898](https://arxiv.org/abs/1703.01898)\r\n\r\n**Adversarial Multi-task Learning for Text Classification**\r\n\r\n- intro: ACL 2017\r\n- arxiv: [https://arxiv.org/abs/1704.05742](https://arxiv.org/abs/1704.05742)\r\n- data: [http://nlp.fudan.edu.cn/data/](http://nlp.fudan.edu.cn/data/)\r\n\r\n**Deep Text Classification Can be Fooled**\r\n\r\n- intro: Renmin University of China\r\n- arxiv: [https://arxiv.org/abs/1704.08006](https://arxiv.org/abs/1704.08006)\r\n\r\n**Deep neural network framework for multi-label text classification**\r\n\r\n- github: [https://github.com/inspirehep/magpie](https://github.com/inspirehep/magpie)\r\n\r\n**Multi-Task Label Embedding for Text Classification**\r\n\r\n- intro: Shanghai Jiao Tong University\r\n- arxiv: [https://arxiv.org/abs/1710.07210](https://arxiv.org/abs/1710.07210)\r\n\r\n# Text Clustering\r\n\r\n**Self-Taught Convolutional Neural Networks for Short Text Clustering**\r\n\r\n- intro: Chinese Academy of Sciences. accepted for publication in Neural Networks\r\n- arxiv: [https://arxiv.org/abs/1701.00185](https://arxiv.org/abs/1701.00185)\r\n- github: [https://github.com/jacoxu/STC2](https://github.com/jacoxu/STC2)\r\n\r\n# Alignment\r\n\r\n**Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.06724](http://arxiv.org/abs/1506.06724)\r\n- github: [https://github.com/ryankiros/neural-storyteller](https://github.com/ryankiros/neural-storyteller)\r\n\r\n# Dialog\r\n\r\n**Visual Dialog**\r\n\r\n- webiste: [http://visualdialog.org/](http://visualdialog.org/)\r\n- arxiv: [https://arxiv.org/abs/1611.08669](https://arxiv.org/abs/1611.08669)\r\n- github: [https://github.com/batra-mlp-lab/visdial-amt-chat](https://github.com/batra-mlp-lab/visdial-amt-chat)\r\n- github(Torch): [https://github.com/batra-mlp-lab/visdial](https://github.com/batra-mlp-lab/visdial)\r\n- github(PyTorch): [https://github.com/Cloud-CV/visual-chatbot](https://github.com/Cloud-CV/visual-chatbot)\r\n- demo: [http://visualchatbot.cloudcv.org/](http://visualchatbot.cloudcv.org/)\r\n\r\n**Papers, code and data from FAIR for various memory-augmented nets with application to text understanding and dialogue.**\r\n\r\n- post: [https://www.facebook.com/yann.lecun/posts/10154070851697143](https://www.facebook.com/yann.lecun/posts/10154070851697143)\r\n\r\n**Neural Emoji Recommendation in Dialogue Systems**\r\n\r\n- intro: Tsinghua University & Baidu\r\n- arxiv: [https://arxiv.org/abs/1612.04609](https://arxiv.org/abs/1612.04609)\r\n\r\n# Memory Networks\r\n\r\n**Neural Turing Machines**\r\n\r\n- paper: [http://arxiv.org/abs/1410.5401](http://arxiv.org/abs/1410.5401)\r\n- Chs: [http://www.jianshu.com/p/94dabe29a43b](http://www.jianshu.com/p/94dabe29a43b)\r\n- github: [https://github.com/shawntan/neural-turing-machines](https://github.com/shawntan/neural-turing-machines)\r\n- github: [https://github.com/DoctorTeeth/diffmem](https://github.com/DoctorTeeth/diffmem)\r\n- github: [https://github.com/carpedm20/NTM-tensorflow](https://github.com/carpedm20/NTM-tensorflow)\r\n- blog: [https://blog.aidangomez.ca/2016/05/16/The-Neural-Turing-Machine/](https://blog.aidangomez.ca/2016/05/16/The-Neural-Turing-Machine/)\r\n\r\n**Memory Networks**\r\n\r\n- intro: Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1410.3916](http://arxiv.org/abs/1410.3916)\r\n- github: [https://github.com/npow/MemNN](https://github.com/npow/MemNN)\r\n\r\n**End-To-End Memory Networks**\r\n\r\n- intro: Facebook AI Research\r\n- intro: Continuous version of memory extraction via softmax. \"Weakly supervised memory networks\"\r\n- arxiv: [http://arxiv.org/abs/1503.08895](http://arxiv.org/abs/1503.08895)\r\n- github: [https://github.com/facebook/MemNN](https://github.com/facebook/MemNN)\r\n- github: [https://github.com/vinhkhuc/MemN2N-babi-python](https://github.com/vinhkhuc/MemN2N-babi-python)\r\n- github: [https://github.com/npow/MemN2N](https://github.com/npow/MemN2N)\r\n- github: [https://github.com/domluna/memn2n](https://github.com/domluna/memn2n)\r\n- github(Tensorflow): [https://github.com/abhaikollara/MemN2N-Tensorflow](https://github.com/abhaikollara/MemN2N-Tensorflow)\r\n- video: [http://research.microsoft.com/apps/video/default.aspx?id=259920&r=1](http://research.microsoft.com/apps/video/default.aspx?id=259920&r=1)\r\n- video: [http://pan.baidu.com/s/1pKiGLzP](http://pan.baidu.com/s/1pKiGLzP)\r\n\r\n**Reinforcement Learning Neural Turing Machines - Revised**\r\n\r\n- arxiv: [http://arxiv.org/abs/1505.00521](http://arxiv.org/abs/1505.00521)\r\n- github: [https://github.com/ilyasu123/rlntm](https://github.com/ilyasu123/rlntm)\r\n\r\n- - -\r\n\r\n**Learning to Transduce with Unbounded Memory**\r\n\r\n- intro: Google DeepMind\r\n- arxiv: [http://arxiv.org/abs/1506.02516](http://arxiv.org/abs/1506.02516)\r\n\r\n**How to Code and Understand DeepMind's Neural Stack Machine**\r\n\r\n- blog: [https://iamtrask.github.io/2016/02/25/deepminds-neural-stack-machine/](https://iamtrask.github.io/2016/02/25/deepminds-neural-stack-machine/)\r\n- video tutorial: [http://pan.baidu.com/s/1qX0EGDe](http://pan.baidu.com/s/1qX0EGDe)\r\n\r\n- - -\r\n\r\n**Ask Me Anything: Dynamic Memory Networks for Natural Language Processing**\r\n\r\n- intro: Memory networks implemented via rnns and gated recurrent units (GRUs).\r\n- arxiv: [http://arxiv.org/abs/1506.07285](http://arxiv.org/abs/1506.07285)\r\n- blog(\"Implementing Dynamic memory networks\"): [http://yerevann.github.io//2016/02/05/implementing-dynamic-memory-networks/](http://yerevann.github.io//2016/02/05/implementing-dynamic-memory-networks/)\r\n- github(Python): [https://github.com/swstarlab/DynamicMemoryNetworks](https://github.com/swstarlab/DynamicMemoryNetworks)\r\n\r\n**Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model)**\r\n\r\n- intro: extensions for the Dynamic Memory Network (DMN)\r\n- arxiv: [https://arxiv.org/abs/1703.03939](https://arxiv.org/abs/1703.03939)\r\n- github: [https://github.com/rgsachin/DMTN](https://github.com/rgsachin/DMTN)\r\n\r\n**Structured Memory for Neural Turing Machines**\r\n\r\n- intro: IBM Watson\r\n- arxiv: [http://arxiv.org/abs/1510.03931](http://arxiv.org/abs/1510.03931)\r\n\r\n**Dynamic Memory Networks for Visual and Textual Question Answering**\r\n\r\n![](https://camo.githubusercontent.com/0f17be4fe54c583cf7b5ef5387ac363e0cd87f92/687474703a2f2f692e696d6775722e636f6d2f33304465504b682e706e67)\r\n\r\n- intro: MetaMind 2016\r\n- arxiv: [http://arxiv.org/abs/1603.01417](http://arxiv.org/abs/1603.01417)\r\n- slides: [http://slides.com/smerity/dmn-for-tqa-and-vqa-nvidia-gtc#/](http://slides.com/smerity/dmn-for-tqa-and-vqa-nvidia-gtc#/)\r\n- github: [https://github.com/therne/dmn-tensorflow](https://github.com/therne/dmn-tensorflow)\r\n- github(Theano): [https://github.com/ethancaballero/Improved-Dynamic-Memory-Networks-DMN-plus](https://github.com/ethancaballero/Improved-Dynamic-Memory-Networks-DMN-plus)\r\n- review: [https://www.technologyreview.com/s/600958/the-memory-trick-making-computers-seem-smarter/](https://www.technologyreview.com/s/600958/the-memory-trick-making-computers-seem-smarter/)\r\n- github(Tensorflow): [https://github.com/DeepRNN/visual_question_answering](https://github.com/DeepRNN/visual_question_answering)\r\n\r\n**Neural GPUs Learn Algorithms**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.08228](http://arxiv.org/abs/1511.08228)\r\n- github: [https://github.com/tensorflow/models/tree/master/neural_gpu](https://github.com/tensorflow/models/tree/master/neural_gpu)\r\n- github: [https://github.com/ikostrikov/torch-neural-gpu](https://github.com/ikostrikov/torch-neural-gpu)\r\n- github: [https://github.com/tristandeleu/neural-gpu](https://github.com/tristandeleu/neural-gpu)\r\n\r\n**Hierarchical Memory Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.07427](http://arxiv.org/abs/1605.07427)\r\n\r\n**Convolutional Residual Memory Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.05262](http://arxiv.org/abs/1606.05262)\r\n\r\n**NTM-Lasagne: A Library for Neural Turing Machines in Lasagne**\r\n\r\n- github: [https://github.com/snipsco/ntm-lasagne](https://github.com/snipsco/ntm-lasagne)\r\n- blog: [https://medium.com/snips-ai/ntm-lasagne-a-library-for-neural-turing-machines-in-lasagne-2cdce6837315#.96pnh1m6j](https://medium.com/snips-ai/ntm-lasagne-a-library-for-neural-turing-machines-in-lasagne-2cdce6837315#.96pnh1m6j)\r\n\r\n**Evolving Neural Turing Machines for Reward-based Learning**\r\n\r\n- homepage: [http://sebastianrisi.com/entm/](http://sebastianrisi.com/entm/)\r\n- paper: [http://sebastianrisi.com/wp-content/uploads/greve_gecco16.pdf](http://sebastianrisi.com/wp-content/uploads/greve_gecco16.pdf)\r\n- code: [https://www.dropbox.com/s/t019mwabw5nsnxf/neuralturingmachines-master.zip?dl=0](https://www.dropbox.com/s/t019mwabw5nsnxf/neuralturingmachines-master.zip?dl=0)\r\n\r\n**Hierarchical Memory Networks for Answer Selection on Unknown Words**\r\n\r\n- intro: COLING 2016\r\n- arxiv: [https://arxiv.org/abs/1609.08843](https://arxiv.org/abs/1609.08843)\r\n- github: [https://github.com/jacoxu/HMN4QA](https://github.com/jacoxu/HMN4QA)\r\n\r\n**Gated End-to-End Memory Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.04211](https://arxiv.org/abs/1610.04211)\r\n\r\n**Can Active Memory Replace Attention?**\r\n\r\n- intro: Google Brain\r\n- arxiv: [https://arxiv.org/abs/1610.08613](https://arxiv.org/abs/1610.08613)\r\n\r\n**A Taxonomy for Neural Memory Networks**\r\n\r\n- intro: University of Florida\r\n- arxiv: [https://arxiv.org/abs/1805.00327](https://arxiv.org/abs/1805.00327)\r\n\r\n# Papers\r\n\r\n**Globally Normalized Transition-Based Neural Networks**\r\n\r\n![](https://raw.githubusercontent.com/tensorflow/models/master/syntaxnet/looping-parser.gif)\r\n\r\n- intro: speech tagging, dependency parsing and sentence compression \r\n- arxiv: [http://arxiv.org/abs/1603.06042](http://arxiv.org/abs/1603.06042)\r\n- github(SyntaxNet): [https://github.com/tensorflow/models/tree/master/syntaxnet](https://github.com/tensorflow/models/tree/master/syntaxnet)\r\n\r\n**A Decomposable Attention Model for Natural Language Inference**\r\n\r\n- intro: EMNLP 2016\r\n- arxiv: [http://arxiv.org/abs/1606.01933](http://arxiv.org/abs/1606.01933)\r\n- github(Keras+spaCy): [https://github.com/explosion/spaCy/tree/master/examples/keras_parikh_entailment](https://github.com/explosion/spaCy/tree/master/examples/keras_parikh_entailment)\r\n\r\n**Improving Recurrent Neural Networks For Sequence Labelling**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.02555](http://arxiv.org/abs/1606.02555)\r\n\r\n**Recurrent Memory Networks for Language Modeling**\r\n\r\n- arixv: [http://arxiv.org/abs/1601.01272](http://arxiv.org/abs/1601.01272)\r\n- github: [https://github.com/ketranm/RMN](https://github.com/ketranm/RMN)\r\n\r\n**Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder**\r\n\r\n- intro: MIT Media Lab\r\n- arixv: [http://arxiv.org/abs/1607.07514](http://arxiv.org/abs/1607.07514)\r\n\r\n**Learning text representation using recurrent convolutional neural network with highway layers**\r\n\r\n- intro: Neu-IR '16 SIGIR Workshop on Neural Information Retrieval\r\n- arxiv: [http://arxiv.org/abs/1606.06905](http://arxiv.org/abs/1606.06905)\r\n- github: [https://github.com/wenying45/deep_learning_tutorial/tree/master/rcnn-hw](https://github.com/wenying45/deep_learning_tutorial/tree/master/rcnn-hw)\r\n\r\n**Ask the GRU: Multi-task Learning for Deep Text Recommendations**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.02116](http://arxiv.org/abs/1609.02116)\r\n\r\n**From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning**\r\n\r\n- intro: COLING 2016\r\n- arxiv: [https://arxiv.org/abs/1610.03342](https://arxiv.org/abs/1610.03342)\r\n\r\n**Visualizing Linguistic Shift**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.06478](https://arxiv.org/abs/1611.06478)\r\n\r\n**A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks**\r\n\r\n- intro: The University of Tokyo & Salesforce Research\r\n- arxiv: [https://arxiv.org/abs/1611.01587](https://arxiv.org/abs/1611.01587)\r\n\r\n**Deep Learning applied to NLP**\r\n\r\n[https://arxiv.org/abs/1703.03091](https://arxiv.org/abs/1703.03091)\r\n\r\n**Attention Is All You Need**\r\n\r\n- intro: Google Brain & Google Research & University of Toronto\r\n- intro: Just attention + positional encoding = state of the art\r\n- arxiv: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\r\n- github(Chainer): [https://github.com/soskek/attention_is_all_you_need](https://github.com/soskek/attention_is_all_you_need)\r\n\r\n**Recent Trends in Deep Learning Based Natural Language Processing**\r\n\r\n- intro: Beijing Institute of Technology & National University of Singapore & Nanyang Technological University\r\n- arxiv: [https://arxiv.org/abs/1708.02709](https://arxiv.org/abs/1708.02709)\r\n\r\n**HotFlip: White-Box Adversarial Examples for NLP**\r\n\r\n- intro: University of Oregon & Nanjing University\r\n- arxiv: [https://arxiv.org/abs/1712.06751](https://arxiv.org/abs/1712.06751)\r\n\r\n**No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling**\r\n\r\n- intro: ACL 2018\r\n- arxiv: [https://arxiv.org/abs/1804.09160](https://arxiv.org/abs/1804.09160)\r\n\r\n## Interesting Applications\r\n\r\n**Data-driven HR - Résumé Analysis Based on Natural Language Processing and Machine Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.05611](http://arxiv.org/abs/1606.05611)\r\n\r\n**sk_p: a neural program corrector for MOOCs**\r\n\r\n- intro: MIT\r\n- intro: Using seq2seq to fix buggy code submissions in MOOCs\r\n- arxiv: [http://arxiv.org/abs/1607.02902](http://arxiv.org/abs/1607.02902)\r\n\r\n**Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge**\r\n\r\n- intro: EMNLP 2016\r\n- intro: translating natural language queries into regular expressions which embody their meaning\r\n- arxiv: [http://arxiv.org/abs/1608.03000](http://arxiv.org/abs/1608.03000)\r\n\r\n**emoji2vec: Learning Emoji Representations from their Description**\r\n\r\n- intro: EMNLP 2016\r\n- arxiv: [http://arxiv.org/abs/1609.08359](http://arxiv.org/abs/1609.08359)\r\n\r\n**Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper)**\r\n\r\n- paper: [https://www.cs.jhu.edu/~jason/papers/eisner.spnlp16.pdf](https://www.cs.jhu.edu/~jason/papers/eisner.spnlp16.pdf)\r\n\r\n**Cruciform: Solving Crosswords with Natural Language Processing**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.02360](https://arxiv.org/abs/1611.02360)\r\n\r\n**Smart Reply: Automated Response Suggestion for Email**\r\n\r\n- intro: Google. KDD 2016\r\n- arxiv: [https://arxiv.org/abs/1606.04870](https://arxiv.org/abs/1606.04870)\r\n- notes: [https://blog.acolyer.org/2016/11/24/smart-reply-automated-response-suggestion-for-email/](https://blog.acolyer.org/2016/11/24/smart-reply-automated-response-suggestion-for-email/)\r\n\r\n**Deep Learning for RegEx**\r\n\r\n- intro: a winning submission of *Extraction of product attribute values* competition (CrowdAnalytix)\r\n- blog: [http://dlacombejr.github.io/2016/11/13/deep-learning-for-regex.html](http://dlacombejr.github.io/2016/11/13/deep-learning-for-regex.html)\r\n\r\n**Learning Python Code Suggestion with a Sparse Pointer Network**\r\n\r\n- intro: Learning to Auto-Complete using RNN Language Models\r\n- intro: University College London\r\n- arxiv: [https://arxiv.org/abs/1611.08307](https://arxiv.org/abs/1611.08307)\r\n- github: [https://github.com/uclmr/pycodesuggest](https://github.com/uclmr/pycodesuggest)\r\n\r\n**End-to-End Prediction of Buffer Overruns from Raw Source Code via Neural Memory Networks**\r\n\r\n[https://arxiv.org/abs/1703.02458](https://arxiv.org/abs/1703.02458)\r\n\r\n**Convolutional Sequence to Sequence Learning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1705.03122](https://arxiv.org/abs/1705.03122)\r\n- paper: [https://s3.amazonaws.com/fairseq/papers/convolutional-sequence-to-sequence-learning.pdf](https://s3.amazonaws.com/fairseq/papers/convolutional-sequence-to-sequence-learning.pdf)\r\n- github: [https://github.com/facebookresearch/fairseq](https://github.com/facebookresearch/fairseq)\r\n\r\n**DeepFix: Fixing Common C Language Errors by Deep Learning**\r\n\r\n- intro: AAAI 2017. Indian Institute of Science\r\n- project page: [http://www.iisc-seal.net/deepfix](http://www.iisc-seal.net/deepfix)\r\n- paper: [https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603/13921](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603/13921)\r\n- bitbucket: [https://bitbucket.org/iiscseal/deepfix](https://bitbucket.org/iiscseal/deepfix)\r\n\r\n**Hierarchically-Attentive RNN for Album Summarization and Storytelling**\r\n\r\n- intro: EMNLP 2017. UNC Chapel Hill\r\n- arxiv: [https://arxiv.org/abs/1708.02977](https://arxiv.org/abs/1708.02977)\r\n\r\n# Project\r\n\r\n**TheanoLM - An Extensible Toolkit for Neural Network Language Modeling**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.00942](http://arxiv.org/abs/1605.00942)\r\n- github: [https://github.com/senarvi/theanolm](https://github.com/senarvi/theanolm)\r\n\r\n**NLP-Caffe: natural language processing with Caffe**\r\n\r\n- github: [https://github.com/Russell91/nlpcaffe](https://github.com/Russell91/nlpcaffe)\r\n\r\n**DL4NLP: Deep Learning for Natural Language Processing**\r\n\r\n- github: [https://github.com/nokuno/dl4nlp](https://github.com/nokuno/dl4nlp)\r\n\r\n**Combining CNN and RNN for spoken language identification**\r\n\r\n- blog: [http://yerevann.github.io//2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/](http://yerevann.github.io//2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/)\r\n- github: [https://github.com/YerevaNN/Spoken-language-identification/tree/master/theano](https://github.com/YerevaNN/Spoken-language-identification/tree/master/theano)\r\n\r\n**Character-Aware Neural Language Models: LSTM language model with CNN over characters in TensorFlow**\r\n\r\n- github: [https://github.com/carpedm20/lstm-char-cnn-tensorflow](https://github.com/carpedm20/lstm-char-cnn-tensorflow)\r\n\r\n**Neural Relation Extraction with Selective Attention over Instances**\r\n\r\n- paper: [http://nlp.csai.tsinghua.edu.cn/~lzy/publications/acl2016_nre.pdf](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/acl2016_nre.pdf)\r\n- github: [https://github.com/thunlp/NRE](https://github.com/thunlp/NRE)\r\n\r\n**deep-simplification: Text simplification using RNNs**\r\n\r\n- intro: achieves a BLEU score of 61.14\r\n- github: [https://github.com/mbartoli/deep-simplification](https://github.com/mbartoli/deep-simplification)\r\n\r\n**lamtram: A toolkit for language and translation modeling using neural networks**\r\n\r\n- github: [https://github.com/neubig/lamtram](https://github.com/neubig/lamtram)\r\n\r\n**Lango: Language Lego**\r\n\r\n- intro: Lango is a natural language processing library for working with the building blocks of language.\r\n- github: [https://github.com/ayoungprogrammer/Lango](https://github.com/ayoungprogrammer/Lango)\r\n\r\n**Sequence-to-Sequence Learning with Attentional Neural Networks**\r\n\r\n- github(Torch): [https://github.com/harvardnlp/seq2seq-attn](https://github.com/harvardnlp/seq2seq-attn)\r\n\r\n**harvardnlp code**\r\n\r\n- intro: pen-source implementations of popular deep learning techniques with applications to NLP\r\n- homepage: [http://nlp.seas.harvard.edu/code/](http://nlp.seas.harvard.edu/code/)\r\n\r\n**Seq2seq: Sequence to Sequence Learning with Keras**\r\n\r\n![](https://camo.githubusercontent.com/242210d7d0151cae91107ee63bff364a860db5dd/687474703a2f2f6936342e74696e797069632e636f6d2f333031333674652e706e67)\r\n\r\n- github: [https://github.com/farizrahman4u/seq2seq](https://github.com/farizrahman4u/seq2seq)\r\n\r\n**debug seq2seq**\r\n\r\n- github: [https://github.com/nicolas-ivanov/debug_seq2seq](https://github.com/nicolas-ivanov/debug_seq2seq)\r\n\r\n**Recurrent & convolutional neural network modules**\r\n\r\n- intro: This repo contains Theano implementations of popular neural network components and optimization methods.\r\n- github: [https://github.com/taolei87/rcnn](https://github.com/taolei87/rcnn)\r\n\r\n# Datasets\r\n\r\n**Datasets for Natural Language Processing**\r\n\r\n- github: [https://github.com/karthikncode/nlp-datasets](https://github.com/karthikncode/nlp-datasets)\r\n\r\n# Blogs\r\n\r\n**How to read: Character level deep learning**\r\n\r\n![](https://raw.githubusercontent.com/offbit/offbit.github.io/master/assets/char-models/fullmodel.jpg)\r\n\r\n- blog: [https://offbit.github.io/how-to-read/](https://offbit.github.io/how-to-read/)\r\n- github: [https://github.com/offbit/char-models](https://github.com/offbit/char-models)\r\n\r\n**Heavy Metal and Natural Language Processing**\r\n\r\n- part 1: [http://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/](http://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/)\r\n\r\n**Sequence To Sequence Attention Models In PyCNN**\r\n\r\n[https://talbaumel.github.io/Neural+Attention+Mechanism.html](https://talbaumel.github.io/Neural+Attention+Mechanism.html)\r\n\r\n**Source Code Classification Using Deep Learning**\r\n\r\n![](http://blog.aylien.com/wp-content/uploads/2016/08/cnn_source_code_model.png)\r\n\r\n[http://blog.aylien.com/source-code-classification-using-deep-learning/](http://blog.aylien.com/source-code-classification-using-deep-learning/)\r\n\r\n**My Process for Learning Natural Language Processing with Deep Learning**\r\n\r\n[https://medium.com/@MichaelTeifel/my-process-for-learning-natural-language-processing-with-deep-learning-bd0a64a36086](https://medium.com/@MichaelTeifel/my-process-for-learning-natural-language-processing-with-deep-learning-bd0a64a36086)\r\n\r\n**Convolutional Methods for Text**\r\n\r\n[https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f](https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f)\r\n\r\n## Word2Vec\r\n\r\n**Word2Vec Tutorial - The Skip-Gram Model**\r\n\r\n[http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\r\n\r\n**Word2Vec Tutorial Part 2 - Negative Sampling**\r\n\r\n[http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\r\n\r\n**Word2Vec Resources**\r\n\r\n[http://mccormickml.com/2016/04/27/word2vec-resources/](http://mccormickml.com/2016/04/27/word2vec-resources/)\r\n\r\n# Demos\r\n\r\n**AskImage.org - Deep Learning for Answering Questions about Images**\r\n\r\n- homepage: [http://www.askimage.org/](http://www.askimage.org/)\r\n\r\n# Talks / Videos\r\n\r\n**Navigating Natural Language Using Reinforcement Learning**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=7s-erJbCkaY](https://www.youtube.com/watch?v=7s-erJbCkaY)\r\n\r\n# Resources\r\n\r\n**So, you need to understand language data? Open-source NLP software can help!**\r\n\r\n![](http://entopix.com/assets/white-paper/slide1.png)\r\n\r\n- blog: [http://entopix.com/so-you-need-to-understand-language-data-open-source-nlp-software-can-help.html](http://entopix.com/so-you-need-to-understand-language-data-open-source-nlp-software-can-help.html)\r\n\r\n**Curated list of resources on building bots**\r\n\r\n![](https://raw.githubusercontent.com/hackerkid/bots/master/bots3d.png)\r\n\r\n- github: [https://github.com/hackerkid/bots](https://github.com/hackerkid/bots)\r\n\r\n**Notes for deep learning on NLP**\r\n\r\n[https://medium.com/@frank_chung/notes-for-deep-learning-on-nlp-94ddfcb45723#.iouo0v7m7](https://medium.com/@frank_chung/notes-for-deep-learning-on-nlp-94ddfcb45723#.iouo0v7m7)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","title":"Object Counting"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Object Counting\ndate: 2015-10-09\n---\n\n# Object Counting\n\n**Towards perspective-free object counting with deep learning**\n\n- intro: ECCV 2016. Counting CNN and Hydra CNN\n- paper: [http://agamenon.tsc.uah.es/Investigacion/gram/publications/eccv2016-onoro.pdf](http://agamenon.tsc.uah.es/Investigacion/gram/publications/eccv2016-onoro.pdf)\n- github: [https://github.com/gramuah/ccnn](https://github.com/gramuah/ccnn)\n- poster: [http://www.eccv2016.org/files/posters/P-3B-26.pdf](http://www.eccv2016.org/files/posters/P-3B-26.pdf)\n\n**Using Convolutional Neural Networks to Count Palm Trees in Satellite Images**\n\n- arxiv: [https://arxiv.org/abs/1701.06462](https://arxiv.org/abs/1701.06462)\n\n**Count-ception: Counting by Fully Convolutional Redundant Counting**\n\n[https://arxiv.org/abs/1703.08710](https://arxiv.org/abs/1703.08710)\n\n**Counting Objects with Faster R-CNN**\n\n- blog: [https://softwaremill.com/counting-objects-with-faster-rcnn/](https://softwaremill.com/counting-objects-with-faster-rcnn/)\n- github: [https://github.com/softberries/keras-frcnn](https://github.com/softberries/keras-frcnn)\n\n**Drone-based Object Counting by Spatially Regularized Regional Proposal Network**\n\n[https://arxiv.org/abs/1707.05972](https://arxiv.org/abs/1707.05972)\n\n**FCN-rLSTM: Deep Spatio-Temporal Neural Networks for Vehicle Counting in City Cameras**\n\n- intro: ICCV 2017. CMU & Universidade de Lisboa\n- arxiv: [https://arxiv.org/abs/1707.09476](https://arxiv.org/abs/1707.09476)\n\n**Representation Learning by Learning to Count**\n\n- intro: ICCV 2017 oral\n- arxiv: [https://arxiv.org/abs/1708.06734](https://arxiv.org/abs/1708.06734)\n\n**Leaf Counting with Deep Convolutional and Deconvolutional Networks**\n\n- intro: ICCV 2017 Workshop on Computer Vision Problems in Plant Phenotyping\n- arxiv: [https://arxiv.org/abs/1708.07570](https://arxiv.org/abs/1708.07570)\n\n**Improving Object Counting with Heatmap Regulation**\n\n[https://arxiv.org/abs/1803.05494](https://arxiv.org/abs/1803.05494)\n\n**Learning Short-Cut Connections for Object Counting**\n\n- keywords: Gated U-Net (GU-Net)\n- arxiv: [https://arxiv.org/abs/1805.02919](https://arxiv.org/abs/1805.02919)\n\n**Object Counting with Small Datasets of Large Images**\n\n[https://arxiv.org/abs/1805.11123](https://arxiv.org/abs/1805.11123)\n\n**Counting with Focus for Free**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1903.12206](https://arxiv.org/abs/1903.12206)\n- github: [https://github.com/shizenglin/Counting-with-Focus-for-Free](https://github.com/shizenglin/Counting-with-Focus-for-Free)\n\n**Dilated-Scale-Aware Attention ConvNet For Multi-Class Object Counting**\n\n[https://arxiv.org/abs/2012.08149](https://arxiv.org/abs/2012.08149)\n\n**Object Counting: You Only Need to Look at One**\n\n- intro: Xi’an Jiaotong University\n- arxiv: [https://arxiv.org/abs/2112.05993](https://arxiv.org/abs/2112.05993)\n\n## Crowd Counting / Crowd Analysis\n\n**Large scale crowd analysis based on convolutional neural network**\n\n- paper: [http://www.sciencedirect.com/science/article/pii/S0031320315001259](http://www.sciencedirect.com/science/article/pii/S0031320315001259)\n\n**Deep People Counting in Extremely Dense Crowds**\n\n- intro: ACM 2015\n- paper: [http://yangliang.github.io/pdf/sp055u.pdf](http://yangliang.github.io/pdf/sp055u.pdf)\n\n**Crossing-line Crowd Counting with Two-phase Deep Neural Networks**\n\n- intro: ECCV 2016\n- paper: [http://www.ee.cuhk.edu.hk/~rzhao/project/crossline_eccv16/ZhaoLZWeccv16.pdf](http://www.ee.cuhk.edu.hk/~rzhao/project/crossline_eccv16/ZhaoLZWeccv16.pdf)\n- poster: [http://www.eccv2016.org/files/posters/P-3C-41.pdf](http://www.eccv2016.org/files/posters/P-3C-41.pdf)\n\n**Cross-scene Crowd Counting via Deep Convolutional Neural Networks**\n\n- intro: CVPR 2015\n- paper: [http://www.ee.cuhk.edu.hk/~xgwang/papers/zhangLWYcvpr15.pdf](http://www.ee.cuhk.edu.hk/~xgwang/papers/zhangLWYcvpr15.pdf)\n\n**Single-Image Crowd Counting via Multi-Column Convolutional Neural Network**\n\n- intro: CVPR 2016\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.pdf)\n- paper: [http://sist.shanghaitech.edu.cn/office/research/news/CVPR2016/paper/Single-Image%20Crowd%20Counting%20via%20Multi-Column%20Convolutional%20Neural%20Network.pdf](http://sist.shanghaitech.edu.cn/office/research/news/CVPR2016/paper/Single-Image%20Crowd%20Counting%20via%20Multi-Column%20Convolutional%20Neural%20Network.pdf)\n- dataset(pwd: p1rv): [http://pan.baidu.com/s/1gfyNBTh](http://pan.baidu.com/s/1gfyNBTh)\n- slides: [http://smartdsp.xmu.edu.cn/%E6%B1%87%E6%8A%A5pdf/crowd%20counting%E6%9E%97%E8%B4%A8%E9%94%90.pdf](http://smartdsp.xmu.edu.cn/%E6%B1%87%E6%8A%A5pdf/crowd%20counting%E6%9E%97%E8%B4%A8%E9%94%90.pdf)\n- github:[https://github.com/svishwa/crowdcount-mcnn](https://github.com/svishwa/crowdcount-mcnn)\n\n**CrowdNet: A Deep Convolutional Network for Dense Crowd Counting**\n\n- intro: ACM Multimedia (MM) 2016\n- arxiv: [http://arxiv.org/abs/1608.06197](http://arxiv.org/abs/1608.06197)\n- github(Caffe): [https://github.com/davideverona/deep-crowd-counting_crowdnet](https://github.com/davideverona/deep-crowd-counting_crowdnet)\n\n**Crowd Counting by Adapting Convolutional Neural Networks with Side Information**\n\n- arxiv: [https://arxiv.org/abs/1611.06748](https://arxiv.org/abs/1611.06748)\n\n**Fully Convolutional Crowd Counting On Highly Congested Scenes**\n\n- intro: VISAPP 2017\n- arxiv: [https://arxiv.org/abs/1612.00220](https://arxiv.org/abs/1612.00220)\n\n**Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction**\n\n- intro: AAAI 2017\n- project page: [https://www.microsoft.com/en-us/research/publication/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/](https://www.microsoft.com/en-us/research/publication/deep-spatio-temporal-residual-networks-for-citywide-crowd-flows-prediction/)\n- paper: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/ST-ResNet-AAAI17-Zhang.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/ST-ResNet-AAAI17-Zhang.pdf)\n- github: [https://github.com/lucktroy/DeepST/tree/master/scripts/papers/AAAI17](https://github.com/lucktroy/DeepST/tree/master/scripts/papers/AAAI17)\n- ppt: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/DeepST-crowd-prediction.pptx](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/DeepST-crowd-prediction.pptx)\n- system: [http://urbanflow.sigkdd.com.cn/](http://urbanflow.sigkdd.com.cn/)\n\n**Multi-scale Convolutional Neural Networks for Crowd Counting**\n\n- arxiv: [https://arxiv.org/abs/1702.02359](https://arxiv.org/abs/1702.02359)\n\n**Mixture of Counting CNNs: Adaptive Integration of CNNs Specialized to Specific Appearance for Crowd Counting**\n\n[https://arxiv.org/abs/1703.09393](https://arxiv.org/abs/1703.09393)\n\n**Beyond Counting: Comparisons of Density Maps for Crowd Analysis Tasks - Counting, Detection, and Tracking**\n\n[https://arxiv.org/abs/1705.10118](https://arxiv.org/abs/1705.10118)\n\n**ResnetCrowd: A Residual Deep Learning Architecture for Crowd Counting, Violent Behaviour Detection and Crowd Density Level Classification**\n\n- intro: AVSS 2017\n- arxiv: [https://arxiv.org/abs/1705.10698](https://arxiv.org/abs/1705.10698)\n\n**Image Crowd Counting Using Convolutional Neural Network and Markov Random Field**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1706.03725](https://arxiv.org/abs/1706.03725)\n\n**A Survey of Recent Advances in CNN-based Single Image Crowd Counting and Density Estimation**\n\n[https://arxiv.org/abs/1707.01202](https://arxiv.org/abs/1707.01202)\n\n**Spatiotemporal Modeling for Crowd Counting in Videos**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.07890](https://arxiv.org/abs/1707.07890)\n\n**CNN-based Cascaded Multi-task Learning of High-level Prior and Density Estimation for Crowd Counting**\n\n- intro: AVSS 2017 (14th International Conference on Advanced Video and Signal Based Surveillance)\n- arxiv: [https://arxiv.org/abs/1707.09605](https://arxiv.org/abs/1707.09605)\n\n**Switching Convolutional Neural Network for Crowd Counting**\n\n- intro: CVPR 2017. Indian Institute of Science\n- project page: [http://val.serc.iisc.ernet.in/crowdcnn/](http://val.serc.iisc.ernet.in/crowdcnn/)\n- arxiv: [https://arxiv.org/abs/1708.00199](https://arxiv.org/abs/1708.00199)\n- github: [https://github.com/val-iisc/crowd-counting-scnn](https://github.com/val-iisc/crowd-counting-scnn)\n\n**Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.00953](https://arxiv.org/abs/1708.00953)\n\n**Deep Spatial Regression Model for Image Crowd Counting**\n\n[https://arxiv.org/abs/1710.09757](https://arxiv.org/abs/1710.09757)\n\n**Crowd counting via scale-adaptive convolutional neural network**\n\n- intro: WACV 2-18. Tencent Youtu Lab\n- arxiv: [https://arxiv.org/abs/1711.04433](https://arxiv.org/abs/1711.04433)\n- github: [https://github.com/miao0913/SaCNN-CrowdCounting-Tencent_Youtu](https://github.com/miao0913/SaCNN-CrowdCounting-Tencent_Youtu)\n\n**DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1712.06679](https://arxiv.org/abs/1712.06679)\n\n**Structured Inhomogeneous Density Map Learning for Crowd Counting**\n\n[https://arxiv.org/abs/1801.06642](https://arxiv.org/abs/1801.06642)\n\n**Understanding Human Behaviors in Crowds by Imitating the Decision-Making Process**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1801.08391](https://arxiv.org/abs/1801.08391)\n\n**Leveraging Unlabeled Data for Crowd Counting by Learning to Rank**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.03095](https://arxiv.org/abs/1803.03095)\n\n**Crowd Counting via Adversarial Cross-Scale Consistency Pursuit**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Crowd_Counting_via_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Crowd_Counting_via_CVPR_2018_paper.pdf)\n- github: [https://github.com/Ling-Bao/ACSCP_cGAN](https://github.com/Ling-Bao/ACSCP_cGAN)\n\n**Crowd Counting with Deep Negative Correlation Learning**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdfhttp://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf)\n- github: [https://github.com/shizenglin/Deep-NCL](https://github.com/shizenglin/Deep-NCL)\n\n**An Aggregated Multicolumn Dilated Convolution Network for Perspective-Free Counting**\n\n- intro: CVPR 2018 Workshop On Visual Understanding of Humans in Crowd Scene\n- arxiv: [https://arxiv.org/abs/1804.07821](https://arxiv.org/abs/1804.07821)\n\n**A Deeply-Recursive Convolutional Network for Crowd Counting**\n\n- intro: Xiamen University\n- arxiv: [https://arxiv.org/abs/1805.05633](https://arxiv.org/abs/1805.05633)\n\n**Crowd Counting by Adaptively Fusing Predictions from an Image Pyramid**\n\n[https://arxiv.org/abs/1805.06115](https://arxiv.org/abs/1805.06115)\n\n**Attention to Head Locations for Crowd Counting**\n\n[https://arxiv.org/abs/1806.10287](https://arxiv.org/abs/1806.10287)\n\n**Crowd Counting with Density Adaption Networks**\n\n[https://arxiv.org/abs/1806.10040](https://arxiv.org/abs/1806.10040)\n\n**Perspective-Aware CNN For Crowd Counting**\n\n[https://arxiv.org/abs/1807.01989](https://arxiv.org/abs/1807.01989)\n\n**Crowd Counting using Deep Recurrent Spatial-Aware Network**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1807.00601](https://arxiv.org/abs/1807.00601)\n\n**Top-Down Feedback for Crowd Counting Convolutional Neural Network**\n\n[https://arxiv.org/abs/1807.08881](https://arxiv.org/abs/1807.08881)\n\n**Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.01050](https://arxiv.org/abs/1808.01050)\n\n**Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance**\n\n- arxiv: [https://arxiv.org/abs/1808.07456](https://arxiv.org/abs/1808.07456)\n- github: [https://github.com/siyuhuang/crowdcount-stackpool](https://github.com/siyuhuang/crowdcount-stackpool)\n\n**In Defense of Single-column Networks for Crowd Counting**\n\n[https://arxiv.org/abs/1808.06133](https://arxiv.org/abs/1808.06133)\n\n**Attentive Crowd Flow Machines**\n\n- intro: ACM MM, full paper\n- arxiv: [https://arxiv.org/abs/1809.00101](https://arxiv.org/abs/1809.00101)\n\n**Context-Aware Crowd Counting**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1811.10452](https://arxiv.org/abs/1811.10452)\n- github: [https://github.com/weizheliu/Context-Aware-Crowd-Counting](https://github.com/weizheliu/Context-Aware-Crowd-Counting)\n\n**ADCrowdNet: An Attention-injective Deformable Convolutional Network for Crowd Understanding**\n\n[https://arxiv.org/abs/1811.11968](https://arxiv.org/abs/1811.11968)\n\n**Learning from Synthetic Data for Crowd Counting in the Wild**\n\n- intro: CVPR 2019\n- project page: [https://gjy3035.github.io/GCC-CL/](https://gjy3035.github.io/GCC-CL/)\n- arxiv: [https://arxiv.org/abs/1903.03303](https://arxiv.org/abs/1903.03303)\n\n**Point in, Box out: Beyond Counting Persons in Crowds**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.01333](https://arxiv.org/abs/1904.01333)\n\n**Crowd Transformer Network**\n\n[https://arxiv.org/abs/1904.02774](https://arxiv.org/abs/1904.02774)\n\n**DENet: A Universal Network for Counting Crowd with Varying Densities and Scales**\n\n[https://arxiv.org/abs/1904.08056](https://arxiv.org/abs/1904.08056)\n\n**PCC Net: Perspective Crowd Counting via Spatial Convolutional Network**\n\n- intro: IEEE T-CSVT\n- arxiv: [https://arxiv.org/abs/1905.10085](https://arxiv.org/abs/1905.10085)\n- github: [https://github.com/gjy3035/PCC-Net](https://github.com/gjy3035/PCC-Net)\n\n**Dense Scale Network for Crowd Counting**\n\n[https://arxiv.org/abs/1906.09707](https://arxiv.org/abs/1906.09707)\n\n**Inverse Attention Guided Deep Crowd Counting Network**\n\n- intro: AVSS 2019\n- arxiv: [https://arxiv.org/abs/1907.01193](https://arxiv.org/abs/1907.01193)\n\n**Locality-constrained Spatial Transformer Network for Video Crowd Counting**\n\n- intro: ICME 2019 Oral\n- arxiv: [https://arxiv.org/abs/1907.07911](https://arxiv.org/abs/1907.07911)\n\n**HA-CCN: Hierarchical Attention-based Crowd Counting Network**\n\n- intro: TIP 2019\n- arxiv: [https://arxiv.org/abs/1907.10255](https://arxiv.org/abs/1907.10255)\n\n**Learn to Scale: Generating Multipolar Normalized Density Map for Crowd Counting**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1907.12428](https://arxiv.org/abs/1907.12428)**\n\n**Deep Density-aware Count Regressor**\n\n[https://arxiv.org/abs/1908.03314](https://arxiv.org/abs/1908.03314)\n\n**Bayesian Loss for Crowd Count Estimation with Point Supervision**\n\n- intro: ICCV 2019 oral\n- arxiv: [https://arxiv.org/abs/1908.03684](https://arxiv.org/abs/1908.03684)\n- github: [https://github.com/ZhihengCV/Bayesian-Crowd-Counting](https://github.com/ZhihengCV/Bayesian-Crowd-Counting)\n\n**Crowd Counting with Deep Structured Scale Integration Network**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.08692](https://arxiv.org/abs/1908.08692)\n\n**Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.10937](https://arxiv.org/abs/1908.10937)\n\n**Awesome Crowd Counting**\n\n[https://github.com/gjy3035/Awesome-Crowd-Counting](https://github.com/gjy3035/Awesome-Crowd-Counting)\n\n**Learning Spatial Awareness to Improve Crowd Counting**\n\n- intro: ICCV 2019 oral\n- intro: Southwest Jiaotong University & Carnegie Mellon University & Microsoft Research\n- keywords: SPatial Awareness Network (SPANet), Maximum Excess over Pixels (MEP) loss\n- arxiv: [https://arxiv.org/abs/1909.07057](https://arxiv.org/abs/1909.07057)\n\n**Perspective-Guided Convolution Networks for Crowd Counting**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.06966](https://arxiv.org/abs/1909.06966)\n- github: [https://github.com/Zhaoyi-Yan/PGCNet](https://github.com/Zhaoyi-Yan/PGCNet)\n\n**Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.12384](https://arxiv.org/abs/1910.12384)\n\n**Feature-aware Adaptation and Structured Density Alignment for Crowd Counting in Video Surveillance**\n\n[https://arxiv.org/abs/1912.03672](https://arxiv.org/abs/1912.03672)\n\n**AutoScale: Learning to Scale for Crowd Counting**\n\n[https://arxiv.org/abs/1912.09632](https://arxiv.org/abs/1912.09632)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","title":"OCR"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: OCR\ndate: 2015-10-09\n---\n\n# Papers\n\n**Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks**\n\n- intro: Google. Ian J. Goodfellow\n- arxiv: [https://arxiv.org/abs/1312.6082](https://arxiv.org/abs/1312.6082)\n\n**End-to-End Text Recognition with Convolutional Neural Networks**\n\n- paper: [http://www.cs.stanford.edu/~acoates/papers/wangwucoatesng_icpr2012.pdf](http://www.cs.stanford.edu/~acoates/papers/wangwucoatesng_icpr2012.pdf)\n- PhD thesis: [http://cs.stanford.edu/people/dwu4/HonorThesis.pdf](http://cs.stanford.edu/people/dwu4/HonorThesis.pdf)\n\n**Word Spotting and Recognition with Embedded Attributes**\n\n![](/assets/ocr-materials/Word_Spotting_and_Recognition_with_Embedded_Attributes.jpg)\n\n- paper: [http://ieeexplore.ieee.org.sci-hub.org/xpl/articleDetails.jsp?arnumber=6857995&filter%3DAND%28p_IS_Number%3A6940341%29](http://ieeexplore.ieee.org.sci-hub.org/xpl/articleDetails.jsp?arnumber=6857995&filter%3DAND%28p_IS_Number%3A6940341%29)\n\n**Reading Text in the Wild with Convolutional Neural Networks**\n\n![](http://www.robots.ox.ac.uk/~vgg/research/text/pipeline.png)\n\n- arxiv: [http://arxiv.org/abs/1412.1842](http://arxiv.org/abs/1412.1842)\n- homepage: [http://www.robots.ox.ac.uk/~vgg/publications/2016/Jaderberg16/](http://www.robots.ox.ac.uk/~vgg/publications/2016/Jaderberg16/)\n- demo: [http://zeus.robots.ox.ac.uk/textsearch/#/search/](http://zeus.robots.ox.ac.uk/textsearch/#/search/)\n- code: [http://www.robots.ox.ac.uk/~vgg/research/text/](http://www.robots.ox.ac.uk/~vgg/research/text/)\n\n**Deep structured output learning for unconstrained text recognition**\n\n- intro: \"propose an architecture consisting of a character sequence CNN and \nan N-gram encoding CNN which act on an input image in parallel and whose outputs are utilized\nalong with a CRF model to recognize the text content present within the image.\"\n- arxiv: [http://arxiv.org/abs/1412.5903](http://arxiv.org/abs/1412.5903)\n\n**Deep Features for Text Spotting**\n\n- paper: [http://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14/jaderberg14.pdf](http://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14/jaderberg14.pdf)\n- bitbucket: [https://bitbucket.org/jaderberg/eccv2014_textspotting](https://bitbucket.org/jaderberg/eccv2014_textspotting)\n- gitxiv: [http://gitxiv.com/posts/uB4y7QdD5XquEJ69c/deep-features-for-text-spotting](http://gitxiv.com/posts/uB4y7QdD5XquEJ69c/deep-features-for-text-spotting)\n\n**Reading Scene Text in Deep Convolutional Sequences**\n\n- intro: AAAI 2016\n- arxiv: [http://arxiv.org/abs/1506.04395](http://arxiv.org/abs/1506.04395)\n\n**DeepFont: Identify Your Font from An Image**\n\n- arxiv: [http://arxiv.org/abs/1507.03196](http://arxiv.org/abs/1507.03196)\n\n**An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition**\n\n- intro: Convolutional Recurrent Neural Network (CRNN)\n- arxiv: [http://arxiv.org/abs/1507.05717](http://arxiv.org/abs/1507.05717)\n- github: [https://github.com/bgshih/crnn](https://github.com/bgshih/crnn)\n- github: [https://github.com/meijieru/crnn.pytorch](https://github.com/meijieru/crnn.pytorch)\n\n**Recursive Recurrent Nets with Attention Modeling for OCR in the Wild**\n\n- arxiv: [http://arxiv.org/abs/1603.03101](http://arxiv.org/abs/1603.03101)\n\n**Writer-independent Feature Learning for Offline Signature Verification using Deep Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.00974](http://arxiv.org/abs/1604.00974)\n\n**DeepText: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images**\n\n- arxiv: [http://arxiv.org/abs/1605.07314](http://arxiv.org/abs/1605.07314)\n\n**End-to-End Interpretation of the French Street Name Signs Dataset**\n\n- paper: [http://link.springer.com/chapter/10.1007%2F978-3-319-46604-0_30](http://link.springer.com/chapter/10.1007%2F978-3-319-46604-0_30)\n- github: [https://github.com/tensorflow/models/tree/master/street](https://github.com/tensorflow/models/tree/master/street)\n\n**End-to-End Subtitle Detection and Recognition for Videos in East Asian Languages via CNN Ensemble with Near-Human-Level Performance**\n\n- arxiv: [https://arxiv.org/abs/1611.06159](https://arxiv.org/abs/1611.06159)\n\n**Smart Library: Identifying Books in a Library using Richly Supervised Deep Scene Text Reading**\n\n- arxiv: [https://arxiv.org/abs/1611.07385](https://arxiv.org/abs/1611.07385)\n\n**Improving Text Proposals for Scene Images with Fully Convolutional Networks**\n\n- intro: Universitat Autonoma de Barcelona (UAB) & University of Florence\n- intro: International Conference on Pattern Recognition (ICPR) - DLPR (Deep Learning for Pattern Recognition) workshop\n- arxiv: [https://arxiv.org/abs/1702.05089](https://arxiv.org/abs/1702.05089)\n\n**Scene Text Eraser**\n\n[https://arxiv.org/abs/1705.02772](https://arxiv.org/abs/1705.02772)\n\n**Attention-based Extraction of Structured Information from Street View Imagery**\n\n- intro: University College London & Google Inc\n- arxiv: [https://arxiv.org/abs/1704.03549](https://arxiv.org/abs/1704.03549)\n- github: [https://github.com/tensorflow/models/tree/master/attention_ocr](https://github.com/tensorflow/models/tree/master/attention_ocr)\n\n**Implicit Language Model in LSTM for OCR**\n\n[https://arxiv.org/abs/1805.09441](https://arxiv.org/abs/1805.09441)\n\n**Scene Text Magnifier**\n\n- intro: ICDAR 2019\n- arxiv: [https://arxiv.org/abs/1907.00693](https://arxiv.org/abs/1907.00693)\n\n# Text Detection\n\n**Object Proposals for Text Extraction in the Wild**\n\n- intro: ICDAR 2015\n- arxiv: [http://arxiv.org/abs/1509.02317](http://arxiv.org/abs/1509.02317)\n- github: [https://github.com/lluisgomez/TextProposals](https://github.com/lluisgomez/TextProposals)\n\n**Text-Attentional Convolutional Neural Networks for Scene Text Detection**\n\n- arxiv: [http://arxiv.org/abs/1510.03283](http://arxiv.org/abs/1510.03283)\n\n**Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network**\n\n- arxiv: [http://arxiv.org/abs/1603.09423](http://arxiv.org/abs/1603.09423)\n\n**Synthetic Data for Text Localisation in Natural Images**\n\n![](https://raw.githubusercontent.com/ankush-me/SynthText/master/samples.png)\n\n- intro: CVPR 2016\n- project page: [http://www.robots.ox.ac.uk/~vgg/data/scenetext/](http://www.robots.ox.ac.uk/~vgg/data/scenetext/)\n- arxiv: [http://arxiv.org/abs/1604.06646](http://arxiv.org/abs/1604.06646)\n- paper: [http://www.robots.ox.ac.uk/~vgg/data/scenetext/gupta16.pdf](http://www.robots.ox.ac.uk/~vgg/data/scenetext/gupta16.pdf)\n- github: [https://github.com/ankush-me/SynthText](https://github.com/ankush-me/SynthText)\n\n**Scene Text Detection via Holistic, Multi-Channel Prediction**\n\n- arxiv: [http://arxiv.org/abs/1606.09002](http://arxiv.org/abs/1606.09002)\n\n**Detecting Text in Natural Image with Connectionist Text Proposal Network**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1609.03605](http://arxiv.org/abs/1609.03605)\n- github(Caffe): [https://github.com/tianzhi0549/CTPN](https://github.com/tianzhi0549/CTPN)\n- github(CUDA8.0 support): [https://github.com/qingswu/CTPN](https://github.com/qingswu/CTPN)\n- demo: [http://textdet.com/](http://textdet.com/)\n- github(Tensorflow): [https://github.com/eragonruan/text-detection-ctpn](https://github.com/eragonruan/text-detection-ctpn)\n\n**TextBoxes: A Fast Text Detector with a Single Deep Neural Network**\n\n- intro: AAAI 2017\n- arxiv: [https://arxiv.org/abs/1611.06779](https://arxiv.org/abs/1611.06779)\n- github(Caffe): [https://github.com/MhLiao/TextBoxes](https://github.com/MhLiao/TextBoxes)\n- github: [https://github.com/xiaodiu2010/TextBoxes-TensorFlow](https://github.com/xiaodiu2010/TextBoxes-TensorFlow)\n\n**TextBoxes++: A Single-Shot Oriented Scene Text Detector**\n\n- intro: TIP 2018. University of Science and Technology(HUST)\n- arxiv: [https://arxiv.org/abs/1801.02765](https://arxiv.org/abs/1801.02765)\n- github(official, Caffe): [https://github.com/MhLiao/TextBoxes_plusplus](https://github.com/MhLiao/TextBoxes_plusplus)\n\n**Arbitrary-Oriented Scene Text Detection via Rotation Proposals**\n\n- intro: IEEE Transactions on Multimedia\n- keywords: RRPN\n- arxiv: [https://arxiv.org/abs/1703.01086](https://arxiv.org/abs/1703.01086)\n- github: [https://github.com/mjq11302010044/RRPN](https://github.com/mjq11302010044/RRPN)\n- github: [https://github.com/DetectionTeamUCAS/RRPN_Faster-RCNN_Tensorflow](https://github.com/DetectionTeamUCAS/RRPN_Faster-RCNN_Tensorflow)\n\n**Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection**\n\n- intro: CVPR 2017\n- intro: F-measure 70.64%, outperforming the existing state-of-the-art method with F-measure 63.76%\n- arxiv: [https://arxiv.org/abs/1703.01425](https://arxiv.org/abs/1703.01425)\n\n**Detecting Oriented Text in Natural Images by Linking Segments**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1703.06520](https://arxiv.org/abs/1703.06520)\n- github(Tensorflow): [https://github.com/dengdan/seglink](https://github.com/dengdan/seglink)\n\n**Deep Direct Regression for Multi-Oriented Scene Text Detection**\n\n- arxiv: [https://arxiv.org/abs/1703.08289](https://arxiv.org/abs/1703.08289)\n\n**Cascaded Segmentation-Detection Networks for Word-Level Text Spotting**\n\n[https://arxiv.org/abs/1704.00834](https://arxiv.org/abs/1704.00834)\n\n**Text-Detection-using-py-faster-rcnn-framework**\n\n- github: [https://github.com/jugg1024/Text-Detection-with-FRCN](https://github.com/jugg1024/Text-Detection-with-FRCN)\n\n**WordFence: Text Detection in Natural Images with Border Awareness**\n\n- intro: ICIP 2017\n- arcxiv: [https://arxiv.org/abs/1705.05483](https://arxiv.org/abs/1705.05483)\n\n**SSD-text detection: Text Detector**\n\n- intro: A modified SSD model for text detection\n- github: [https://github.com/oyxhust/ssd-text_detection](https://github.com/oyxhust/ssd-text_detection)\n\n**R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection**\n\n- intro: Samsung R&D Institute China\n- arxiv: [https://arxiv.org/abs/1706.09579](https://arxiv.org/abs/1706.09579)\n\n**R-PHOC: Segmentation-Free Word Spotting using CNN**\n\n- intro: ICDAR 2017\n- arxiv: [https://arxiv.org/abs/1707.01294](https://arxiv.org/abs/1707.01294)\n\n**Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.03985](https://arxiv.org/abs/1707.03985)\n\n**EAST: An Efficient and Accurate Scene Text Detector**\n\n- intro: CVPR 2017. Megvii\n- arxiv: [https://arxiv.org/abs/1704.03155](https://arxiv.org/abs/1704.03155)\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf)\n- github(Tensorflow): [https://github.com/argman/EAST](https://github.com/argman/EAST)\n\n**Deep Scene Text Detection with Connected Component Proposals**\n\n- intro: Amap Vision Lab, Alibaba Group\n- arxiv: [https://arxiv.org/abs/1708.05133](https://arxiv.org/abs/1708.05133)\n\n**Single Shot Text Detector with Regional Attention**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.00138](https://arxiv.org/abs/1709.00138)\n- github: [https://github.com/BestSonny/SSTD](https://github.com/BestSonny/SSTD)\n- code: [http://sstd.whuang.org](http://sstd.whuang.org)\n\n**Fused Text Segmentation Networks for Multi-oriented Scene Text Detection**\n\n[https://arxiv.org/abs/1709.03272](https://arxiv.org/abs/1709.03272)\n\n**Deep Residual Text Detection Network for Scene Text**\n\n- intro: IAPR International Conference on Document Analysis and Recognition (ICDAR) 2017. Samsung R&D Institute of China, Beijing\n- arxiv: [https://arxiv.org/abs/1711.04147](https://arxiv.org/abs/1711.04147)\n\n**Feature Enhancement Network: A Refined Scene Text Detector**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1711.04249](https://arxiv.org/abs/1711.04249)\n\n**ArbiText: Arbitrary-Oriented Text Detection in Unconstrained Scene**\n\n[https://arxiv.org/abs/1711.11249](https://arxiv.org/abs/1711.11249)\n\n**Detecting Curve Text in the Wild: New Dataset and New Solution**\n\n- arxiv: [https://arxiv.org/abs/1712.02170](https://arxiv.org/abs/1712.02170)\n- github: [https://github.com/Yuliang-Liu/Curve-Text-Detector](https://github.com/Yuliang-Liu/Curve-Text-Detector)\n\n**FOTS: Fast Oriented Text Spotting with a Unified Network**\n\n[https://arxiv.org/abs/1801.01671](https://arxiv.org/abs/1801.01671)\n\n**PixelLink: Detecting Scene Text via Instance Segmentation**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1801.01315](https://arxiv.org/abs/1801.01315)\n\n**PixelLink: Detecting Scene Text via Instance Segmentation**\n\n- intro: AAAI 2018. Zhejiang University & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1801.01315](https://arxiv.org/abs/1801.01315)\n\n**Sliding Line Point Regression for Shape Robust Scene Text Detection**\n\n[https://arxiv.org/abs/1801.09969](https://arxiv.org/abs/1801.09969)\n\n**Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1802.08948](https://arxiv.org/abs/1802.08948)\n\n**Single Shot TextSpotter with Explicit Alignment and Attention**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.03474](https://arxiv.org/abs/1803.03474)\n\n**Rotation-Sensitive Regression for Oriented Scene Text Detection**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.05265](https://arxiv.org/abs/1803.05265)\n\n**Detecting Multi-Oriented Text with Corner-based Region Proposals**\n\n- arxiv: [https://arxiv.org/abs/1804.02690](https://arxiv.org/abs/1804.02690)\n- github: [https://github.com/xhzdeng/crpn](https://github.com/xhzdeng/crpn)\n\n**An Anchor-Free Region Proposal Network for Faster R-CNN based Text Detection Approaches**\n\n[https://arxiv.org/abs/1804.09003](https://arxiv.org/abs/1804.09003)\n\n**IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection**\n\n- intro: IJCAI 2018. Alibaba Group\n- arxiv: [https://arxiv.org/abs/1805.01167](https://arxiv.org/abs/1805.01167)\n\n**Boosting up Scene Text Detectors with Guided CNN**\n\n[https://arxiv.org/abs/1805.04132](https://arxiv.org/abs/1805.04132)\n\n**Shape Robust Text Detection with Progressive Scale Expansion Network**\n\n- arxiv: [https://arxiv.org/abs/1806.02559](https://arxiv.org/abs/1806.02559)\n- github: [https://github.com/whai362/PSENet](https://github.com/whai362/PSENet)\n\n**A Single Shot Text Detector with Scale-adaptive Anchors**\n\n[https://arxiv.org/abs/1807.01884](https://arxiv.org/abs/1807.01884)\n\n**TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.01544](https://arxiv.org/abs/1807.01544)\n\n**Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes**\n\n- intro: ECCV 2018. Huazhong University of Science and Technology & Megvii (Face++) Technology\n- arxiv: [https://arxiv.org/abs/1807.02242](https://arxiv.org/abs/1807.02242)\n- github: [https://github.com/MhLiao/MaskTextSpotter](https://github.com/MhLiao/MaskTextSpotter)\n\n**Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.03547](https://arxiv.org/abs/1807.03547)\n\n**TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade**\n\n[https://arxiv.org/abs/1809.03050](https://arxiv.org/abs/1809.03050)\n\n**Correlation Propagation Networks for Scene Text Detection**\n\n[https://arxiv.org/abs/1810.00304](https://arxiv.org/abs/1810.00304)\n\n**Scene Text Detection with Supervised Pyramid Context Network**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.08605](https://arxiv.org/abs/1811.08605)\n\n**Improving Rotated Text Detection with Rotation Region Proposal Networks**\n\n[https://arxiv.org/abs/1811.07031](https://arxiv.org/abs/1811.07031)\n\n**Pixel-Anchor: A Fast Oriented Scene Text Detector with Combined Networks**\n\n[https://arxiv.org/abs/1811.07432](https://arxiv.org/abs/1811.07432)\n\n**Mask R-CNN with Pyramid Attention Network for Scene Text Detection**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1811.09058](https://arxiv.org/abs/1811.09058)\n\n**TextField: Learning A Deep Direction Field for Irregular Scene Text Detection**\n\n- intro: Huazhong University of Science and Technology (HUST) &  Alibaba Group\n- arxiv: [https://arxiv.org/abs/1812.01393](https://arxiv.org/abs/1812.01393)\n\n**Detecting Text in the Wild with Deep Character Embedding Network**\n\n- intro: ACCV 2018\n- intro: Baidu\n- arxiv: [https://arxiv.org/abs/1901.00363](https://arxiv.org/abs/1901.00363)\n\n**MSR: Multi-Scale Shape Regression for Scene Text Detection**\n\n[https://arxiv.org/abs/1901.02596](https://arxiv.org/abs/1901.02596)\n\n**Pyramid Mask Text Detector**\n\n- intro: SenseTime & Beihang University & CUHK\n- arxiv: [https://arxiv.org/abs/1903.11800](https://arxiv.org/abs/1903.11800)\n\n**Shape Robust Text Detection with Progressive Scale Expansion Network**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.12473](https://arxiv.org/abs/1903.12473)\n\n**Tightness-aware Evaluation Protocol for Scene Text Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.00813](https://arxiv.org/abs/1904.00813)\n- github: [https://github.com/Yuliang-Liu/TIoU-metric](https://github.com/Yuliang-Liu/TIoU-metric)\n\n**Character Region Awareness for Text Detection**\n\n- intro: CVPR 2019\n- keywords: CRAFT: Character-Region Awareness For Text detection\n- arxiv: [https://arxiv.org/abs/1904.01941](https://arxiv.org/abs/1904.01941)\n- github(official): [https://github.com/clovaai/CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch)\n\n**Towards End-to-End Text Spotting in Natural Scenes**\n\n- intro: An extension of the work \"Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks\", Proc. Int. Conf. Comp. Vision 2017\n- arxiv: [https://arxiv.org/abs/1906.06013](https://arxiv.org/abs/1906.06013)\n\n**A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning**\n\n- intro: ACM MM 2019\n- arxiv: [https://arxiv.org/abs/1908.05498](https://arxiv.org/abs/1908.05498)\n\n**Geometry Normalization Networks for Accurate Scene Text Detection**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.00794](https://arxiv.org/abs/1909.00794)\n\n**Real-time Scene Text Detection with Differentiable Binarization**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.08947](https://arxiv.org/abs/1911.08947)\n- github: [https://github.com/MhLiao/DB](https://github.com/MhLiao/DB)\n\n**TextTubes for Detecting Curved Text in the Wild**\n\n[https://arxiv.org/abs/1912.08990](https://arxiv.org/abs/1912.08990)\n\n**Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/2002.06820](https://arxiv.org/abs/2002.06820)\n\n**ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2002.10200](https://arxiv.org/abs/2002.10200)\n\n**DGST : Discriminator Guided Scene Text detector**\n\n[https://arxiv.org/abs/2002.12509](https://arxiv.org/abs/2002.12509)\n\n**MANGO: A Mask Attention Guided One-Stage Scene Text Spotter**\n\n- intro: AAAI 2021\n- arxiv: [https://arxiv.org/abs/2012.04350](https://arxiv.org/abs/2012.04350)\n\n**Vision-Language Pre-Training for Boosting Scene Text Detectors**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.13867](https://arxiv.org/abs/2204.13867)\n\n# Text Recognition\n\n**Sequence to sequence learning for unconstrained scene text recognition**\n\n- intro: master thesis\n- arxiv: [http://arxiv.org/abs/1607.06125](http://arxiv.org/abs/1607.06125)\n\n**Drawing and Recognizing Chinese Characters with Recurrent Neural Network**\n\n- arxiv: [https://arxiv.org/abs/1606.06539](https://arxiv.org/abs/1606.06539)\n\n**Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition**\n\n- intro: correct rates: Dataset-CASIA 97.10% and Dataset-ICDAR 97.15%\n- arxiv: [https://arxiv.org/abs/1610.02616](https://arxiv.org/abs/1610.02616)\n\n**Stroke Sequence-Dependent Deep Convolutional Neural Network for Online Handwritten Chinese Character Recognition**\n\n- arxiv: [https://arxiv.org/abs/1610.04057](https://arxiv.org/abs/1610.04057)\n\n**Visual attention models for scene text recognition**\n\n[https://arxiv.org/abs/1706.01487](https://arxiv.org/abs/1706.01487)\n\n**Focusing Attention: Towards Accurate Text Recognition in Natural Images**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.02054](https://arxiv.org/abs/1709.02054)\n\n**Scene Text Recognition with Sliding Convolutional Character Models**\n\n[https://arxiv.org/abs/1709.01727](https://arxiv.org/abs/1709.01727)\n\n**AdaDNNs: Adaptive Ensemble of Deep Neural Networks for Scene Text Recognition**\n\n[https://arxiv.org/abs/1710.03425](https://arxiv.org/abs/1710.03425)\n\n**A New Hybrid-parameter Recurrent Neural Networks for Online Handwritten Chinese Character Recognition**\n\n[https://arxiv.org/abs/1711.02809](https://arxiv.org/abs/1711.02809)\n\n**AON: Towards Arbitrarily-Oriented Text Recognition**\n\n- arxiv: [https://arxiv.org/abs/1711.04226](https://arxiv.org/abs/1711.04226)\n- github: [https://github.com/huizhang0110/AON](https://github.com/huizhang0110/AON)\n\n**Arbitrarily-Oriented Text Recognition**\n\n- intro: A method used in ICDAR 2017 word recognition competitions\n- arxiv: [https://arxiv.org/abs/1711.04226](https://arxiv.org/abs/1711.04226)\n\n**SEE: Towards Semi-Supervised End-to-End Scene Text Recognition**\n\n[https://arxiv.org/abs/1712.05404](https://arxiv.org/abs/1712.05404)\n\n**Edit Probability for Scene Text Recognition**\n\n- intro: Fudan University & Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1805.03384](https://arxiv.org/abs/1805.03384)\n\n**SCAN: Sliding Convolutional Attention Network for Scene Text Recognition**\n\n[https://arxiv.org/abs/1806.00578](https://arxiv.org/abs/1806.00578)\n\n**Adaptive Adversarial Attack on Scene Text Recognition**\n\n- intro: University of Florida\n- arxiv: [https://arxiv.org/abs/1807.03326](https://arxiv.org/abs/1807.03326)\n\n**ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification**\n\n[https://arxiv.org/abs/1812.05824](https://arxiv.org/abs/1812.05824)\n\n**A Multi-Object Rectified Attention Network for Scene Text Recognition**\n\n- intro: Pattern Recognition 2019\n- keywords: MORAN\n- arxiv: [https://arxiv.org/abs/1901.03003](https://arxiv.org/abs/1901.03003)\n\n**SAFE: Scale Aware Feature Encoder for Scene Text Recognition**\n\n- intro: ACCV 2018\n- arxiv: [https://arxiv.org/abs/1901.05770](https://arxiv.org/abs/1901.05770)\n\n**A Simple and Robust Convolutional-Attention Network for Irregular Text Recognition**\n\n[https://arxiv.org/abs/1904.01375](https://arxiv.org/abs/1904.01375)\n\n**FACLSTM: ConvLSTM with Focused Attention for Scene Text Recognition**\n\n[https://arxiv.org/abs/1904.09405](https://arxiv.org/abs/1904.09405)\n\n**Towards Accurate Scene Text Recognition with Semantic Reasoning Networks**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.12294](https://arxiv.org/abs/2003.12294)\n\n**FedOCR: Communication-Efficient Federated Learning for Scene Text Recognition**\n\n- intro: Huazhong University of Science and Technology & Meituan-Dianping Group\n- arxiv: [https://arxiv.org/abs/2007.11462](https://arxiv.org/abs/2007.11462)\n\n# Text Spotting & Text Detection + Recognition\n\n**STN-OCR: A single Neural Network for Text Detection and Text Recognition**\n\n- arxiv: [https://arxiv.org/abs/1707.08831](https://arxiv.org/abs/1707.08831)\n- github(MXNet): [https://github.com/Bartzi/stn-ocr](https://github.com/Bartzi/stn-ocr)\n\n**Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework**\n\n- intro: ICCV 2017\n- arxiv: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf)\n\n**FOTS: Fast Oriented Text Spotting with a Unified Network**\n\n[https://arxiv.org/abs/1801.01671](https://arxiv.org/abs/1801.01671)\n\n**Single Shot TextSpotter with Explicit Alignment and Attention**\n\n**An end-to-end TextSpotter with Explicit Alignment and Attention**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.03474](https://arxiv.org/abs/1803.03474)\n- github(official, Caffe): [https://github.com/tonghe90/textspotter](https://github.com/tonghe90/textspotter)\n\n**Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.03021](https://arxiv.org/abs/1807.03021)\n- github: [https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scenes](https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scenes)\n\n**Scene Text Detection and Recognition: The Deep Learning Era**\n\n- arxiv: [https://arxiv.org/abs/1811.04256](https://arxiv.org/abs/1811.04256)\n- gihtub: [https://github.com/Jyouhou/SceneTextPapers](https://github.com/Jyouhou/SceneTextPapers)\n\n**A Novel Integrated Framework for Learning both Text Detection and Recognition**\n\n- intro: Alibaba\n- arxiv: [https://arxiv.org/abs/1811.08611](https://arxiv.org/abs/1811.08611)\n\n**Efficient Video Scene Text Spotting: Unifying Detection, Tracking, and Recognition**\n\n- intro: Zhejiang University & Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1903.03299](https://arxiv.org/abs/1903.03299)\n\n**A Multitask Network for Localization and Recognition of Text in Images**\n\n- intro: ICDAR 2019\n- arxiv: [https://arxiv.org/abs/1906.09266](https://arxiv.org/abs/1906.09266)\n\n**GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1907.09653](https://arxiv.org/abs/1907.09653)\n\n**Convolutional Character Networks**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.07954](https://arxiv.org/abs/1910.07954)\n- github: [https://github.com/MalongTech/research-charnet](https://github.com/MalongTech/research-charnet)\n\n**RoadText-1K: Text Detection & Recognition Dataset for Driving Videos**\n\n- intro: ICRA 2020\n- project page: [http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k](http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k)\n- arxiv: [https://arxiv.org/abs/2005.09496](https://arxiv.org/abs/2005.09496)\n\n**SPTS: Single-Point Text Spotting**\n\n- intro: Chinese University of Hong Kong & South China University of Technology & University of Adelaide & ByteDance Inc. & Huawei Technologies & Zhejiang University\n- arxiv: [https://arxiv.org/abs/2112.07917](https://arxiv.org/abs/2112.07917)\n\n**Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer**\n\n- intro: AWS AI Labs\n- arxiv: [https://arxiv.org/abs/2202.05508](https://arxiv.org/abs/2202.05508)\n\n**SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition**\n\n- intro: CVPR 2022\n- intro: South China University of Technology & Chinese University of Hong Kong 3Huawei Cloud AI & IntSig Information Co., Ltd & Peng Cheng Laboratory\n- arxiv: [https://arxiv.org/abs/2203.10209](https://arxiv.org/abs/2203.10209)\n- github: [https://github.com/mxin262/SwinTextSpotter](https://github.com/mxin262/SwinTextSpotter)\n\n**End-to-End Video Text Spotting with Transformer**\n\n- intro: Zhejiang University & Kuaishou Technology & Beijing Institute of Technology & Beijing University of Posts and Telecommunications & The University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2203.10539](https://arxiv.org/abs/2203.10539)\n- github: [https://github.com/weijiawu/TransDETR](https://github.com/weijiawu/TransDETR)\n\n**Text Spotting Transformers**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.01918](https://arxiv.org/abs/2204.01918)\n\n**Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting**\n\n- intro: ECCV 2022\n- intro: Zhejiang University & Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/2207.06694](https://arxiv.org/abs/2207.06694)\n- github: [https://github.com/hikopensource/DAVAR-Lab-OCR/](https://github.com/hikopensource/DAVAR-Lab-OCR/)\n\n# Breaking Captcha\n\n**Using deep learning to break a Captcha system**\n\n- intro: \"Using Torch code to break simplecaptcha with 92% accuracy\"\n- blog: [https://deepmlblog.wordpress.com/2016/01/03/how-to-break-a-captcha-system/](https://deepmlblog.wordpress.com/2016/01/03/how-to-break-a-captcha-system/)\n- github: [https://github.com/arunpatala/captcha](https://github.com/arunpatala/captcha)\n\n**Breaking reddit captcha with 96% accuracy**\n\n- blog: [https://deepmlblog.wordpress.com/2016/01/05/breaking-reddit-captcha-with-96-accuracy/](https://deepmlblog.wordpress.com/2016/01/05/breaking-reddit-captcha-with-96-accuracy/)\n- github: [https://github.com/arunpatala/reddit.captcha](https://github.com/arunpatala/reddit.captcha)\n\n**I’m not a human: Breaking the Google reCAPTCHA**\n\n- paper: [https://www.blackhat.com/docs/asia-16/materials/asia-16-Sivakorn-Im-Not-a-Human-Breaking-the-Google-reCAPTCHA-wp.pdf](https://www.blackhat.com/docs/asia-16/materials/asia-16-Sivakorn-Im-Not-a-Human-Breaking-the-Google-reCAPTCHA-wp.pdf)\n\n**Neural Net CAPTCHA Cracker**\n\n- slides: [http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring15/geetika/CS298%20Slides%20-%20PDF](http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring15/geetika/CS298%20Slides%20-%20PDF)\n- github: [https://github.com/bgeetika/Captcha-Decoder](https://github.com/bgeetika/Captcha-Decoder)\n- demo: [http://cp-training.appspot.com/](http://cp-training.appspot.com/)\n\n**Recurrent neural networks for decoding CAPTCHAS**\n\n- blog: [https://deepmlblog.wordpress.com/2016/01/12/recurrent-neural-networks-for-decoding-captchas/](https://deepmlblog.wordpress.com/2016/01/12/recurrent-neural-networks-for-decoding-captchas/)\n- demo: [http://simplecaptcha.sourceforge.net/](http://simplecaptcha.sourceforge.net/)\n- code: [http://sourceforge.net/projects/simplecaptcha/](http://sourceforge.net/projects/simplecaptcha/)\n\n**Reading irctc captchas with 95% accuracy using deep learning**\n\n- github: [https://github.com/arunpatala/captcha.irctc](https://github.com/arunpatala/captcha.irctc)\n\n**端到端的OCR：基于CNN的实现**\n\n- blog: [http://blog.xlvector.net/2016-05/mxnet-ocr-cnn/](http://blog.xlvector.net/2016-05/mxnet-ocr-cnn/)\n\n**I Am Robot: (Deep) Learning to Break Semantic Image CAPTCHAs**\n\n- intro: automatically solving 70.78% of the image reCaptchachallenges, \nwhile requiring only 19 seconds per challenge. \napply to the Facebook image captcha and achieve an accuracy of 83.5%\n- paper: [http://www.cs.columbia.edu/~polakis/papers/sivakorn_eurosp16.pdf](http://www.cs.columbia.edu/~polakis/papers/sivakorn_eurosp16.pdf)\n\n**SimGAN-Captcha**\n\n- intro: Solve captcha without manually labeling a training set \n- github: [https://github.com/rickyhan/SimGAN-Captcha](https://github.com/rickyhan/SimGAN-Captcha)\n\n# Handwritten Recognition\n\n**High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps**\n\n- arxiv: [http://arxiv.org/abs/1505.04925](http://arxiv.org/abs/1505.04925)\n- github: [https://github.com/zhongzhuoyao/HCCR-GoogLeNet](https://github.com/zhongzhuoyao/HCCR-GoogLeNet)\n\n**Recognize your handwritten numbers**\n\n![](https://cdn-images-1.medium.com/max/800/1*YGT2w66tVNIgdiK7hEPSTQ.png)\n\n[https://medium.com/@o.kroeger/recognize-your-handwritten-numbers-3f007cbe46ff#.jllz62xgu](https://medium.com/@o.kroeger/recognize-your-handwritten-numbers-3f007cbe46ff#.jllz62xgu)\n\n**Handwritten Digit Recognition using Convolutional Neural Networks in Python with Keras**\n\n- blog: [http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/](http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/)\n\n**MNIST Handwritten Digit Classifier**\n\n- github: [https://github.com/karandesai-96/digit-classifier](https://github.com/karandesai-96/digit-classifier)\n\n**如何用卷积神经网络CNN识别手写数字集？**\n\n- blog: [http://www.cnblogs.com/charlotte77/p/5671136.html](http://www.cnblogs.com/charlotte77/p/5671136.html)\n\n**LeNet – Convolutional Neural Network in Python**\n\n- blog: [http://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/](http://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/)\n\n**Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention**\n\n- arxiv: [http://arxiv.org/abs/1604.03286](http://arxiv.org/abs/1604.03286)\n\n**MLPaint: the Real-Time Handwritten Digit Recognizer**\n\n![](http://blog.mldb.ai/img/mlpaint_digits_thumb.gif)\n\n- blog: [http://blog.mldb.ai/blog/posts/2016/09/mlpaint/](http://blog.mldb.ai/blog/posts/2016/09/mlpaint/)\n- github: [https://github.com/mldbai/mlpaint](https://github.com/mldbai/mlpaint)\n- demo: [https://docs.mldb.ai/ipy/notebooks/_demos/_latest/Image%20Processing%20with%20Convolutions.html](https://docs.mldb.ai/ipy/notebooks/_demos/_latest/Image%20Processing%20with%20Convolutions.html)\n\n**Training a Computer to Recognize Your Handwriting**\n\n[https://medium.com/@annalyzin/training-a-computer-to-recognize-your-handwriting-24b808fb584#.gd4pb9jk2](https://medium.com/@annalyzin/training-a-computer-to-recognize-your-handwriting-24b808fb584#.gd4pb9jk2)\n\n**Using TensorFlow to create your own handwriting recognition engine**\n\n- blog: [https://niektemme.com/2016/02/21/tensorflow-handwriting/](https://niektemme.com/2016/02/21/tensorflow-handwriting/)\n- github: [https://github.com/niektemme/tensorflow-mnist-predict/](https://github.com/niektemme/tensorflow-mnist-predict/)\n\n**Building a Deep Handwritten Digits Classifier using Microsoft Cognitive Toolkit**\n\n- blog: [https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69#.c3h6o7oxf](https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69#.c3h6o7oxf)\n- github: [https://github.com/tuzzer/ai-gym/blob/a97936619cf56b5ed43329c6fa13f7e26b1d46b8/MNIST/minist_softmax_cntk.py](https://github.com/tuzzer/ai-gym/blob/a97936619cf56b5ed43329c6fa13f7e26b1d46b8/MNIST/minist_softmax_cntk.py)\n\n**Hand Writing Recognition Using Convolutional Neural Networks**\n\n- intro: This CNN-based model for recognition of hand written digits attains a validation accuracy of 99.2% after training for 12 epochs. \nIts trained on the MNIST dataset on Kaggle.\n- github: [https://github.com/ayushoriginal/HandWritingRecognition-CNN](https://github.com/ayushoriginal/HandWritingRecognition-CNN)\n\n**Design of a Very Compact CNN Classifier for Online Handwritten Chinese Character Recognition Using DropWeight and Global Pooling**\n\n- intro: 0.57 MB, performance is decreased only by 0.91%.\n- arxiv: [https://arxiv.org/abs/1705.05207](https://arxiv.org/abs/1705.05207)\n\n**Handwritten digit string recognition by combination of residual network and RNN-CTC**\n\n[https://arxiv.org/abs/1710.03112](https://arxiv.org/abs/1710.03112)\n\n# Plate Recognition\n\n**Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs**\n\n- arxiv: [http://arxiv.org/abs/1601.05610](http://arxiv.org/abs/1601.05610)\n\n**Number plate recognition with Tensorflow**\n\n![](http://matthewearl.github.io/assets/cnn-anpr/topology.svg)\n\n- blog: [http://matthewearl.github.io/2016/05/06/cnn-anpr/](http://matthewearl.github.io/2016/05/06/cnn-anpr/)\n- github(Deep ANPR): [https://github.com/matthewearl/deep-anpr](https://github.com/matthewearl/deep-anpr)\n\n**end-to-end-for-plate-recognition**\n\n- github: [https://github.com/szad670401/end-to-end-for-chinese-plate-recognition](https://github.com/szad670401/end-to-end-for-chinese-plate-recognition)\n\n**Segmentation-free Vehicle License Plate Recognition using ConvNet-RNN**\n\n- intro: International Workshop on Advanced Image Technology, January, 8-10, 2017. Penang, Malaysia. Proceeding IWAIT2017\n- arxiv: [https://arxiv.org/abs/1701.06439](https://arxiv.org/abs/1701.06439)\n\n**License Plate Detection and Recognition Using Deeply Learned Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1703.07330](https://arxiv.org/abs/1703.07330)\n- api: [https://www.sighthound.com/products/cloud](https://www.sighthound.com/products/cloud)\n\n**Adversarial Generation of Training Examples for Vehicle License Plate Recognition**\n\n[https://arxiv.org/abs/1707.03124](https://arxiv.org/abs/1707.03124)\n\n**Towards End-to-End Car License Plates Detection and Recognition with Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1709.08828](https://arxiv.org/abs/1709.08828)\n\n**Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline**\n\n- paper: [http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf](http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf)\n- github: [https://github.com/detectRecog/CCPD](https://github.com/detectRecog/CCPD)\n- dataset: [https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view](https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view)\n\n**High Accuracy Chinese Plate Recognition Framework**\n\n- intro: 基于深度学习高性能中文车牌识别 High Performance Chinese License Plate Recognition Framework.\n- gihtub: [https://github.com/zeusees/HyperLPR](https://github.com/zeusees/HyperLPR)\n\n**LPRNet: License Plate Recognition via Deep Neural Networks**\n\n- intrp=o: Intel IOTG Computer Vision Group\n- intro: works in real-time with recognition accuracy up to 95% for Chinese license plates: \n3 ms/plate on nVIDIAR GeForceTMGTX 1080 and 1.3 ms/plate on IntelR CoreTMi7-6700K CPU.\n- arxiv: [https://arxiv.org/abs/1806.10447](https://arxiv.org/abs/1806.10447)\n\n**How many labeled license plates are needed?**\n\n- intro: Chinese Conference on Pattern Recognition and Computer Vision\n- arxiv: [https://arxiv.org/abs/1808.08410](https://arxiv.org/abs/1808.08410)\n\n**An End-to-End Neural Network for Multi-line License Plate Recognition**\n\n- intro: ICPR 2018\n- paper: [https://sci-hub.se/10.1109/ICPR.2018.8546200#](https://sci-hub.se/10.1109/ICPR.2018.8546200#)\n- github: [https://github.com/deeplearningshare/multi-line-plate-recognition](https://github.com/deeplearningshare/multi-line-plate-recognition)\n\n# Blogs\n\n**Applying OCR Technology for Receipt Recognition**\n\n![](http://rnd.azoft.com/wp-content/uploads/2016/04/applying-ocr-technology-for-receipt-recognition.png)\n\n- blog: [http://rnd.azoft.com/applying-ocr-technology-receipt-recognition/](http://rnd.azoft.com/applying-ocr-technology-receipt-recognition/)\n- mirror: [http://pan.baidu.com/s/1qXQBQiC](http://pan.baidu.com/s/1qXQBQiC)\n\n**Hacking MNIST in 30 lines of Python**\n\n- blog: [http://jrusev.github.io/post/hacking-mnist/](http://jrusev.github.io/post/hacking-mnist/)\n- github: [https://github.com/jrusev/simple-neural-networks](https://github.com/jrusev/simple-neural-networks)\n\n**Optical Character Recognition Using One-Shot Learning, RNN, and TensorFlow**\n\n[https://blog.altoros.com/optical-character-recognition-using-one-shot-learning-rnn-and-tensorflow.html](https://blog.altoros.com/optical-character-recognition-using-one-shot-learning-rnn-and-tensorflow.html)\n\n**Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning**\n\n[https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/](https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/)\n\n# Projects\n\n**ocropy: Python-based tools for document analysis and OCR**\n\n- github: [https://github.com/tmbdev/ocropy](https://github.com/tmbdev/ocropy)\n\n**Extracting text from an image using Ocropus**\n\n- blog: [http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html](http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html)\n\n**CLSTM : A small C++ implementation of LSTM networks, focused on OCR**\n\n- github: [https://github.com/tmbdev/clstm](https://github.com/tmbdev/clstm)\n\n**OCR text recognition using tensorflow with attention**\n\n- github: [https://github.com/pannous/caffe-ocr](https://github.com/pannous/caffe-ocr)\n- github: [https://github.com/pannous/tensorflow-ocr](https://github.com/pannous/tensorflow-ocr)\n\n**Digit Recognition via CNN: digital meter numbers detection**\n\n![](https://camo.githubusercontent.com/bbdd1924ed88618f11cfe2f302a624591122d666/687474703a2f2f37786e37777a2e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f64696769742e6a7067)\n\n- github(caffe): [https://github.com/SHUCV/digit](https://github.com/SHUCV/digit)\n\n**Attention-OCR: Visual Attention based OCR**\n\n![](https://camo.githubusercontent.com/70902af6f701308b84854eb5f2f46f0729288a84/687474703a2f2f63732e636d752e6564752f25374579756e7469616e642f4f43522d322e6a7067)\n\n- github: [https://github.com/da03/Attention-OCR](https://github.com/da03/Attention-OCR)\n\n**umaru: An OCR-system based on torch using the technique of LSTM/GRU-RNN, CTC and referred to the works of rnnlib and clstm**\n\n- github: [https://github.com/edward-zhu/umaru](https://github.com/edward-zhu/umaru)\n\n**Tesseract.js: Pure Javascript OCR for 62 Languages**\n\n![](https://raw.githubusercontent.com/naptha/tesseract.js/master/demo.gif)\n\n- homepage: [http://tesseract.projectnaptha.com/](http://tesseract.projectnaptha.com/)\n- github: [https://github.com/naptha/tesseract.js](https://github.com/naptha/tesseract.js)\n\n**DeepHCCR: Offline Handwritten Chinese Character Recognition based on GoogLeNet and AlexNet (With CaffeModel)**\n\n- github: [https://github.com/chongyangtao/DeepHCCR](https://github.com/chongyangtao/DeepHCCR)\n\n**deep ocr: make a better chinese character recognition OCR than tesseract**\n\n[https://github.com/JinpengLI/deep_ocr](https://github.com/JinpengLI/deep_ocr)\n\n**Practical Deep OCR for scene text using CTPN + CRNN**\n\n[https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/blob/master/notebooks/OCR/readme.md](https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/blob/master/notebooks/OCR/readme.md)\n\n**Tensorflow-based CNN+LSTM trained with CTC-loss for OCR**\n\n[https://github.com//weinman/cnn_lstm_ctc_ocr](https://github.com//weinman/cnn_lstm_ctc_ocr)\n\n**SSD_scene-text-detection**\n\n- github: [https://github.com//chenxinpeng/SSD_scene_text_detection](https://github.com//chenxinpeng/SSD_scene_text_detection)\n- blog: [http://blog.csdn.net/u010167269/article/details/52563573](http://blog.csdn.net/u010167269/article/details/52563573)\n\n# Videos\n\n**LSTMs for OCR**\n\n- youtube: [https://www.youtube.com/watch?v=5vW8faXvnrc](https://www.youtube.com/watch?v=5vW8faXvnrc)\n\n# Resources\n\n**Deep Learning for OCR**\n\n[https://github.com/hs105/Deep-Learning-for-OCR](https://github.com/hs105/Deep-Learning-for-OCR)\n\n**Scene Text Localization & Recognition Resources**\n\n- intro: A curated list of resources dedicated to scene text localization and recognition\n- github: [https://github.com/chongyangtao/Awesome-Scene-Text-Recognition](https://github.com/chongyangtao/Awesome-Scene-Text-Recognition)\n\n**Scene Text Localization & Recognition Resources**\n\n- intro: 图像文本位置感知与识别的论文资源汇总\n- github: [https://github.com/whitelok/image-text-localization-recognition/blob/master/README.zh-cn.md](https://github.com/whitelok/image-text-localization-recognition/blob/master/README.zh-cn.md)\n\n**awesome-ocr: A curated list of promising OCR resources**\n\n[https://github.com/wanghaisheng/awesome-ocr](https://github.com/wanghaisheng/awesome-ocr)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","title":"Optical Flow"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Optical Flow\ndate: 2015-10-09\n---\n\n# Papers\n\n**FlowNet: Learning Optical Flow with Convolutional Networks**\n\n- intro: \"competitive accuracy at frame rates of 5 to 10 fps\"\n- project page: [http://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/](http://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/)\n- arxiv: [https://arxiv.org/abs/1504.06852](https://arxiv.org/abs/1504.06852)\n- github: [https://github.com/ClementPinard/FlowNetTorch](https://github.com/ClementPinard/FlowNetTorch)\n- github: [https://github.com/ClementPinard/FlowNetPytorch](https://github.com/ClementPinard/FlowNetPytorch)\n\n**FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks**\n\n- intro: CVPR 2017\n- project page: [http://lmb.informatik.uni-freiburg.de/Publications/2016/IMKDB16/](http://lmb.informatik.uni-freiburg.de/Publications/2016/IMKDB16/)\n- arxiv: [https://arxiv.org/abs/1612.01925](https://arxiv.org/abs/1612.01925)\n- github(Caffe): [https://github.com/lmb-freiburg/flownet2](https://github.com/lmb-freiburg/flownet2)\n- github: [https://github.com//NVIDIA/flownet2-pytorch](https://github.com//NVIDIA/flownet2-pytorch)\n- video: [http://lmb.informatik.uni-freiburg.de/Publications/2016/IMKDB16/](http://lmb.informatik.uni-freiburg.de/Publications/2016/IMKDB16/)\n\n**Optical Flow Estimation using a Spatial Pyramid Network**\n\n- arxiv: [https://arxiv.org/abs/1611.00850](https://arxiv.org/abs/1611.00850)\n\n**Guided Optical Flow Learning**\n\n- arxiv: [https://arxiv.org/abs/1702.02295](https://arxiv.org/abs/1702.02295)\n\n**PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume**\n\n- intro: CVPR 2018\n- project page: [https://research.nvidia.com/publication/2018-02_PWC-Net:-CNNs-for](https://research.nvidia.com/publication/2018-02_PWC-Net:-CNNs-for)\n- arxiv: [https://arxiv.org/abs/1709.02371](https://arxiv.org/abs/1709.02371)\n- github: [https://github.com/NVlabs/PWC-Net](https://github.com/NVlabs/PWC-Net)\n\n**Occlusion Aware Unsupervised Learning of Optical Flow**\n\n[https://arxiv.org/abs/1711.05890](https://arxiv.org/abs/1711.05890)\n\n**Joint Coarse-And-Fine Reasoning for Deep Optical Flow**\n\n- intro: ICIP 2017\n- arxiv: [https://arxiv.org/abs/1808.07416](https://arxiv.org/abs/1808.07416)\n\n**UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1711.07837](https://arxiv.org/abs/1711.07837)\n\n**Learning Optical Flow via Dilated Networks and Occlusion Reasoning**\n\n- intro: ICIP 2018\n- arxiv: [https://arxiv.org/abs/1805.02733](https://arxiv.org/abs/1805.02733)\n\n**LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation**\n\n- intro: CVPR 2018 (spotlight)\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/](http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/)\n- arxiv: [https://arxiv.org/abs/1805.07036](https://arxiv.org/abs/1805.07036)\n- github: [https://github.com/twhui/LiteFlowNet](https://github.com/twhui/LiteFlowNet)\n\n**ProFlow: Learning to Predict Optical Flow**\n\n[https://arxiv.org/abs/1806.00800](https://arxiv.org/abs/1806.00800)\n\n**Learning Human Optical Flow**\n\n[https://arxiv.org/abs/1806.05666](https://arxiv.org/abs/1806.05666)\n\n**DDFlow: Learning Optical Flow with Unlabeled Data Distillation**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1902.09145](https://arxiv.org/abs/1902.09145)\n\n**Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.05290](https://arxiv.org/abs/1904.05290)\n\n**Attacking Optical Flow**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.10053](https://arxiv.org/abs/1910.10053)\n\n**Learning optical flow from still images**\n\n- intro: CVPR 2021\n- intro: University of Bologna\n- project page: [https://mattpoggi.github.io/projects/cvpr2021aleotti/](https://mattpoggi.github.io/projects/cvpr2021aleotti/)\n- arxiv: [https://arxiv.org/abs/2104.03965](https://arxiv.org/abs/2104.03965)\n- github: [https://github.com/mattpoggi/depthstillation](https://github.com/mattpoggi/depthstillation)\n\n**Sensor-Guided Optical Flow**\n\n- intro: ICCV 2021\n- intro: University of Bologna, Italy\n- arxiv: [https://arxiv.org/abs/2109.15321](https://arxiv.org/abs/2109.15321)\n\n**FlowFormer: A Transformer Architecture for Optical Flow**\n\n- project page: [https://drinkingcoder.github.io/publication/flowformer/](https://drinkingcoder.github.io/publication/flowformer/)\n- arxiv: [https://arxiv.org/abs/2203.16194](https://arxiv.org/abs/2203.16194)\n\n**CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.16896](https://arxiv.org/abs/2203.16896)\n- github: [https://github.com/askerlee/craft](https://github.com/askerlee/craft)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","title":"Deep Learning Applications"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Deep Learning Applications\ndate: 2015-10-09\n---\n\n# Papers\n\n**DeepPose: Human Pose Estimation via Deep Neural Networks**\n\n- intro: CVPR 2014\n- arxiv: [http://arxiv.org/abs/1312.4659](http://arxiv.org/abs/1312.4659)\n- slides: [http://140.122.184.143/paperlinks/Slides/DeepPose_HumanPose_Estimation_via_Deep_Neural_Networks.pptx](http://140.122.184.143/paperlinks/Slides/DeepPose_HumanPose_Estimation_via_Deep_Neural_Networks.pptx)\n- github: [https://github.com/asanakoy/deeppose_tf](https://github.com/asanakoy/deeppose_tf)\n\n**Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network**\n\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/papers/LI_Heterogeneous_Multi-task_Learning_2014_CVPR_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/papers/LI_Heterogeneous_Multi-task_Learning_2014_CVPR_paper.pdf)\n\n**Flowing ConvNets for Human Pose Estimation in Videos**\n\n- arxiv: [http://arxiv.org/abs/1506.02897](http://arxiv.org/abs/1506.02897)\n- homepage: [http://www.robots.ox.ac.uk/~vgg/software/cnn_heatmap/](http://www.robots.ox.ac.uk/~vgg/software/cnn_heatmap/)\n- github: [https://github.com/tpfister/caffe-heatmap](https://github.com/tpfister/caffe-heatmap)\n\n**Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video**\n\n![](https://fling.seas.upenn.edu/~xiaowz/dynamic/wordpress/wp-content/uploads/2016/01/overview.png)\n\n- arxiv: [http://arxiv.org/abs/1511.09439](http://arxiv.org/abs/1511.09439)\n- project page: [https://fling.seas.upenn.edu/~xiaowz/dynamic/wordpress/monocular-human-pose/](https://fling.seas.upenn.edu/~xiaowz/dynamic/wordpress/monocular-human-pose/)\n- video: [http://weibo.com/p/230444264a8772b7fff71cd23e40b8a88dcaad](http://weibo.com/p/230444264a8772b7fff71cd23e40b8a88dcaad)\n\n**Structured Feature Learning for Pose Estimation**\n\n- arxiv: [http://arxiv.org/abs/1603.09065](http://arxiv.org/abs/1603.09065)\n- homepage: [http://www.ee.cuhk.edu.hk/~xgwang/projectpage_structured_feature_pose.html](http://www.ee.cuhk.edu.hk/~xgwang/projectpage_structured_feature_pose.html)\n\n## CPM\n\n**Convolutional Pose Machines**\n\n- intro: Convolutional Pose Machines(CPMs)\n- arxiv: [http://arxiv.org/abs/1602.00134](http://arxiv.org/abs/1602.00134)\n- github: [https://github.com/shihenw/convolutional-pose-machines-release](https://github.com/shihenw/convolutional-pose-machines-release)\n- github(PyTorch): [https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation)\n- github: [https://github.com/timctho/convolutional-pose-machines-tensorflow](https://github.com/timctho/convolutional-pose-machines-tensorflow)\n\n**Stacked Hourglass Networks for Human Pose Estimation**\n\n- homepage: [http://www-personal.umich.edu/~alnewell/pose/](http://www-personal.umich.edu/~alnewell/pose/)\n- arxiv: [http://arxiv.org/abs/1603.06937](http://arxiv.org/abs/1603.06937)\n- github: [https://github.com/anewell/pose-hg-train](https://github.com/anewell/pose-hg-train)\n- demo: [https://github.com/anewell/pose-hg-demo](https://github.com/anewell/pose-hg-demo)\n\n**Chained Predictions Using Convolutional Neural Networks**\n\n- intro: EECV 2016\n- keywords: CNN, structured prediction, RNN, human pose estimation\n- arxiv: [http://arxiv.org/abs/1605.02346](http://arxiv.org/abs/1605.02346)\n\n**DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model**\n\n- arxiv: [http://arxiv.org/abs/1605.03170](http://arxiv.org/abs/1605.03170)\n- github: [https://github.com/eldar/deepcut-cnn](https://github.com/eldar/deepcut-cnn)\n\n**Real-time Human Pose Estimation from Video with Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1609.07420](http://arxiv.org/abs/1609.07420)\n\n**Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields**\n\n- intro: CVPR 2017 Oral\n- keywords: Part Confidence Maps, Part Affinity Fields & Bipartite Matching & Part Association\n- arxiv: [https://arxiv.org/abs/1611.08050](https://arxiv.org/abs/1611.08050)\n- video: [https://www.youtube.com/watch?v=pW6nZXeWlGM&feature=youtu.be](https://www.youtube.com/watch?v=pW6nZXeWlGM&feature=youtu.be)\n- slides: [http://image-net.org/challenges/talks/2016/Multi-person%20pose%20estimation-CMU.pdf](http://image-net.org/challenges/talks/2016/Multi-person%20pose%20estimation-CMU.pdf)\n- github: [https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation)\n\n**OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields**\n\n- intro: Journal version\n- arxiv: [https://arxiv.org/abs/1812.08008](https://arxiv.org/abs/1812.08008)\n\n**Towards Accurate Multi-person Pose Estimation in the Wild**\n\n- intro: Google\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1701.01779](https://arxiv.org/abs/1701.01779)\n\n**Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources**\n\n- intro: ICCV 2017 Oral\n- project page: [https://www.adrianbulat.com/binary-cnn-landmarks](https://www.adrianbulat.com/binary-cnn-landmarks)\n- arxiv: [https://www.arxiv.org/abs/1703.00862](https://www.arxiv.org/abs/1703.00862)\n\n**Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation**\n\n- arxiv: [https://arxiv.org/abs/1705.00389](https://arxiv.org/abs/1705.00389)\n- video: [http://v.qq.com/x/page/c039862eira.html](http://v.qq.com/x/page/c039862eira.html)\n- video: [http://v.qq.com/x/page/f0398zcvkl5.html](http://v.qq.com/x/page/f0398zcvkl5.html)\n- video: [http://v.qq.com/x/page/w0398ei9m1r.html](http://v.qq.com/x/page/w0398ei9m1r.html)\n\n**A simple yet effective baseline for 3d human pose estimation**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1705.03098](https://arxiv.org/abs/1705.03098)\n- github: [https://github.com/una-dinosauria/3d-pose-baseline](https://github.com/una-dinosauria/3d-pose-baseline)\n\n**Human Pose Detection Mining Body Language from Videos**\n\n- blog: [https://medium.com/@samim/human-pose-detection-51268e95ddc2](https://medium.com/@samim/human-pose-detection-51268e95ddc2)\n\n**OpenPose: A Real-Time Multi-Person Keypoint Detection And Multi-Threading C++ Library**\n\n- intro: OpenPose is a library for real-time multi-person keypoint detection and multi-threading written in C++ using OpenCV and Caffe\n- github: [https://github.com/CMU-Perceptual-Computing-Lab/openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)\n\n**Learning Feature Pyramids for Human Pose Estimation**\n\n- arxiv: [https://arxiv.org/abs/1708.01101](https://arxiv.org/abs/1708.01101)\n- github: [https://github.com/bearpaw/PyraNet](https://github.com/bearpaw/PyraNet)\n\n**Multi-Context Attention for Human Pose Estimation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1702.07432](https://arxiv.org/abs/1702.07432)\n- github(Torch): [https://github.com/bearpaw/pose-attention](https://github.com/bearpaw/pose-attention)\n\n**Human Pose Estimation with TensorFlow**\n\n[https://github.com/eldar/pose-tensorflow](https://github.com/eldar/pose-tensorflow)\n\n**Cascaded Pyramid Network for Multi-Person Pose Estimation**\n\n- intro: CVPR 2018. Tsinghua University & HuaZhong Univerisity of Science and Technology & Megvii Inc\n- arxiv: [https://arxiv.org/abs/1711.07319](https://arxiv.org/abs/1711.07319)\n- github(official): [https://github.com/chenyilun95/tf-cpn](https://github.com/chenyilun95/tf-cpn)\n- github: [https://github.com/GengDavid/pytorch-cpn](https://github.com/GengDavid/pytorch-cpn)\n\n**LSTM Pose Machines**\n\n- intro: CVPR 2018. SenseTime Research & Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/1712.06316](https://arxiv.org/abs/1712.06316)\n- github(Caffe, officical): [https://github.com/lawy623/LSTM_Pose_Machines](https://github.com/lawy623/LSTM_Pose_Machines)\n\n**DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild**\n\n- intro: CVPR 2017\n- project page: [http://alpguler.com/DenseReg.html](http://alpguler.com/DenseReg.html)\n- arxiv: [https://arxiv.org/abs/1612.01202](https://arxiv.org/abs/1612.01202)\n- github: [https://github.com/ralpguler/DenseReg](https://github.com/ralpguler/DenseReg)\n\n**DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild**\n\n[https://arxiv.org/abs/1803.02188](https://arxiv.org/abs/1803.02188)\n\n**DensePose: Dense Human Pose Estimation In The Wild**\n\n- intro: CVPR 2018. INRIA & Facebook AI Research\n- project page: [http://densepose.org/](http://densepose.org/)\n- arxiv: [https://arxiv.org/abs/1802.00434](https://arxiv.org/abs/1802.00434)\n- github(CaffeO2): [https://github.com/facebookresearch/DensePose](https://github.com/facebookresearch/DensePose)\n\n**LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images**\n\n- intro: journal version of the CVPR 2017 paper\n- arxiv: [https://arxiv.org/abs/1803.00455](https://arxiv.org/abs/1803.00455)\n\n**Deep Pose Consensus Networks**\n\n[https://arxiv.org/abs/1803.08190](https://arxiv.org/abs/1803.08190)\n\n**3D Human Pose Estimation in the Wild by Adversarial Learning**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.09722](https://arxiv.org/abs/1803.09722)\n\n**Multi-Scale Structure-Aware Network for Human Pose Estimation**\n\n[https://arxiv.org/abs/1803.09894](https://arxiv.org/abs/1803.09894)\n\n**Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation**\n\n- intro: IJCAI 2018 oral. Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1804.06055](https://arxiv.org/abs/1804.06055)\n\n**Learning to Refine Human Pose Estimation**\n\n- intro: CVPRW (2018). Workshop: Visual Understanding of Humans in Crowd Scene and the 2nd Look Into Person Challenge (VUHCS-LIP)\n- arxiv: [https://arxiv.org/abs/1804.07909](https://arxiv.org/abs/1804.07909)\n\n**3D Human Pose Estimation with Relational Networks**\n\n[https://arxiv.org/abs/1805.08961](https://arxiv.org/abs/1805.08961)\n\n**Jointly Optimize Data Augmentation and Network Training: Adversarial Data Augmentation in Human Pose Estimation**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1805.09707](https://arxiv.org/abs/1805.09707)\n\n## AlphaPose\n\n**RMPE: Regional Multi-person Pose Estimation**\n\n- intro: ICCV 2017\n- project page: [https://fang-haoshu.github.io/publications/rmpe/](https://fang-haoshu.github.io/publications/rmpe/)\n- arxiv: [https://arxiv.org/abs/1612.00137](https://arxiv.org/abs/1612.00137)\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Fang_RMPE_Regional_Multi-Person_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Fang_RMPE_Regional_Multi-Person_ICCV_2017_paper.pdf)\n- github(Caffe, official): [https://github.com/MVIG-SJTU/RMPE](https://github.com/MVIG-SJTU/RMPE)\n- github: [https://github.com/Fang-Haoshu/RMPE](https://github.com/Fang-Haoshu/RMPE)\n\n**Pose Flow: Efficient Online Pose Tracking**\n\n[https://arxiv.org/abs/1802.00977](https://arxiv.org/abs/1802.00977)\n\n**AlphaPose: Multi-Person Pose Estimation System**\n\n- intro: an accurate multi-person pose estimation system\n- project page: [http://www.mvig.org/research/alphapose.html](http://www.mvig.org/research/alphapose.html)\n\n- - -\n\n**Computing CNN Loss and Gradients for Pose Estimation with Riemannian Geometry**\n\n[https://arxiv.org/abs/1805.01026](https://arxiv.org/abs/1805.01026)\n\n**Bi-directional Graph Structure Information Model for Multi-Person Pose Estimation**\n\n[https://arxiv.org/abs/1805.00603](https://arxiv.org/abs/1805.00603)\n\n**MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network**\n\n- intro: ECCV 2018. Middle East Technical University\n- keywords: Pose Residual Network (PRN), person detection, keypoint detection, person segmentation and pose estimation\n- arxiv: [https://arxiv.org/abs/1807.04067](https://arxiv.org/abs/1807.04067)\n- github: [https://github.com/mkocabas/pose-residual-network](https://github.com/mkocabas/pose-residual-network)\n\n**Deep Autoencoder for Combined Human Pose Estimation and body Model Upscaling**\n\n[https://arxiv.org/abs/1807.01511](https://arxiv.org/abs/1807.01511)\n\n**Learning Human Poses from Actions**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.09075](https://arxiv.org/abs/1807.09075)\n\n**Multi-Scale Supervised Network for Human Pose Estimation**\n\n- intro: ICIP 2018\n- arxiv: [https://arxiv.org/abs/1808.01623](https://arxiv.org/abs/1808.01623)\n\n**CU-Net: Coupled U-Nets**\n\n- intro: BMVC 2018 (Oral)\n- arxiv: [https://arxiv.org/abs/1808.06521](https://arxiv.org/abs/1808.06521)\n\n**Multi-Domain Pose Network for Multi-Person Pose Estimation and Tracking**\n\n[https://arxiv.org/abs/1810.08338](https://arxiv.org/abs/1810.08338)\n\n**Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation**\n\n- intro: ICCV 2017\n- project page: [http://www.vision.caltech.edu/~mronchi/projects/PoseErrorDiagnosis/](http://www.vision.caltech.edu/~mronchi/projects/PoseErrorDiagnosis/)\n- arxiv: [https://arxiv.org/abs/1707.05388](https://arxiv.org/abs/1707.05388)\n- github: [https://github.com/matteorr/coco-analyze](https://github.com/matteorr/coco-analyze)\n\n**Improving Multi-Person Pose Estimation using Label Correction**\n\n[https://arxiv.org/abs/1811.03331](https://arxiv.org/abs/1811.03331)\n\n**Fast Human Pose Estimation**\n\n- intro: Fast Pose Distillation (FPD)\n- arxiv: [https://arxiv.org/abs/1811.05419](https://arxiv.org/abs/1811.05419)\n\n**PoseFix: Model-agnostic General Human Pose Refinement Network**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1812.03595](https://arxiv.org/abs/1812.03595)\n- github(TensorFlow): [https://github.com/mks0601/PoseFix_RELEASE](https://github.com/mks0601/PoseFix_RELEASE)\n\n**Rethinking on Multi-Stage Networks for Human Pose Estimation**\n\n- intro: Megvii Inc. (Face++) & Shanghai Jiao Tong University & Beihang University & Beijing University of Posts and Telecommunications\n- arxiv: [https://arxiv.org/abs/1901.00148](https://arxiv.org/abs/1901.00148)\n- github: [https://github.com/fenglinglwb/MSPN](https://github.com/fenglinglwb/MSPN)\n\n**Deep High-Resolution Representation Learning for Human Pose Estimation**\n\n- intro: CVPR 2019\n- intro: University of Science and Technology of China & Microsoft Research Asia\n- keywords: HRNet\n- arxiv: [https://arxiv.org/abs/1902.09212](https://arxiv.org/abs/1902.09212)\n- project page: [https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html](https://jingdongwang2017.github.io/Projects/HRNet/PoseEstimation.html)\n- github(official): [https://github.com/leoxiaobin/deep-high-resolution-net.pytorch](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch)\n\n**A Context-and-Spatial Aware Network for Multi-Person Pose Estimation**\n\n[https://arxiv.org/abs/1905.05355](https://arxiv.org/abs/1905.05355)\n\n**FastPose: Towards Real-time Pose Estimation and Tracking via Scale-normalized Multi-task Networks**\n\n- intro: Chinese Academy of Sciences & BUPT & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1908.06290](https://arxiv.org/abs/1908.06290)\n\n**Single-Stage Multi-Person Pose Machines**\n\n- intro: ICCV 2019\n- intro: Yitu Technology\n- arxiv: [https://arxiv.org/abs/1908.09220](https://arxiv.org/abs/1908.09220)\n\n**Single-Network Whole-Body Pose Estimation**\n\n- intro: ICCV 2019\n- project page: [https://github.com/CMU-Perceptual-Computing-Lab/openpose_train](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train)\n- arxiv: [https://arxiv.org/abs/1909.13423](https://arxiv.org/abs/1909.13423)\n\n**NADS-Net: A Nimble Architecture for Driver and Seat Belt Detection via Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1910.03695](https://arxiv.org/abs/1910.03695)\n\n**Distribution-Aware Coordinate Representation for Human Pose Estimation**\n\n- intro: CVPR 2020\n- keywords: Distribution-Aware coordinate Representation of Keypoint (DARK)\n- intro: Results on the COCO keypoint detection challenge: 78.9% AP on the test-dev set (Top-1 in the leaderbord by 12 Oct 2019) and 76.4% AP on the test-challenge set.\n- project page: [https://ilovepose.github.io/coco/](https://ilovepose.github.io/coco/)\n- arxiv: [https://arxiv.org/abs/1910.06278](https://arxiv.org/abs/1910.06278)\n- github: [https://github.com/ilovepose/DarkPose](https://github.com/ilovepose/DarkPose)\n\n**TRB: A Novel Triplet Representation for Understanding 2D Human Body**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.11535](https://arxiv.org/abs/1910.11535)\n\n**Chirality Nets for Human Pose Regression**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1911.00029](https://arxiv.org/abs/1911.00029)\n\n**Conservative Wasserstein Training for Pose Estimation**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1911.00962](https://arxiv.org/abs/1911.00962)\n\n**DirectPose: Direct End-to-End Multi-Person Pose Estimation**\n\n- intro: The University of Adelaide\n- keywords: Keypoint Alignment (KPAlign)\n- arxiv: [https://arxiv.org/abs/1911.07451](https://arxiv.org/abs/1911.07451)\n\n**The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation**\n\n- intro: CVPR 2020\n- intro: XForwardAI Technology Co.,Ltd & Tsinghua University\n- arxiv: [https://arxiv.org/abs/1911.07524](https://arxiv.org/abs/1911.07524)\n- github: [https://github.com/HuangJunJie2017/UDP-Pose](https://github.com/HuangJunJie2017/UDP-Pose)\n\n**Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.10529](https://arxiv.org/abs/1911.10529)\n- github: [https://github.com/hellojialee/Improved-Body-Parts](https://github.com/hellojialee/Improved-Body-Parts)\n\n**HintPose**\n\n- intro: Joint COCO and Mapillary Workshop at ICCV 2019: Keypoint Detection Challenge Track\n- arxiv: [https://arxiv.org/abs/2003.02170](https://arxiv.org/abs/2003.02170)\n\n**How to Train Your Robust Human Pose Estimator: Pay Attention to the Constraint Cue**\n\n- intro: XForwardAI Technology Co.,Ltd & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2008.07139](https://arxiv.org/abs/2008.07139)\n\n**CoKe: Localized Contrastive Learning for Robust Keypoint Detection**\n\n- intro: Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/2009.14115](https://arxiv.org/abs/2009.14115)\n\n**View-Invariant, Occlusion-Robust Probabilistic Embedding for Human Pose**\n\n- intro: Google Research & California Institute of Technology & Rutgers University\n- arxiv: [https://arxiv.org/abs/2010.13321](https://arxiv.org/abs/2010.13321)\n- gtihub: [https://github.com/google-research/google-research/tree/master/poem](https://github.com/google-research/google-research/tree/master/poem)\n\n**An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation**\n\n- intro: Peking University & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/2011.12498](https://arxiv.org/abs/2011.12498)\n\n**EfficientPose: Efficient Human Pose Estimation with Neural Architecture Search**\n\n[https://arxiv.org/abs/2012.07086](https://arxiv.org/abs/2012.07086)\n\n**TransPose: Towards Explainable Human Pose Estimation by Transformer**\n\n- intro: Southeast University\n- arxiv: [https://arxiv.org/abs/2012.14214](https://arxiv.org/abs/2012.14214)\n- github: [https://github.com/yangsenius/TransPose](https://github.com/yangsenius/TransPose)\n\n**Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation**\n\n[https://arxiv.org/abs/2012.15175](https://arxiv.org/abs/2012.15175)\n\n**Multi-Hypothesis Pose Networks: Rethinking Top-Down Pose Estimation**\n\n[https://arxiv.org/abs/2101.11223](https://arxiv.org/abs/2101.11223)\n\n**OmniPose: A Multi-Scale Framework for Multi-Person Pose Estimation**\n\n[https://arxiv.org/abs/2103.10180](https://arxiv.org/abs/2103.10180)\n\n**End-to-End Trainable Multi-Instance Pose Estimation with Transformers**\n\n- intro: Swiss Federal Institute of Technology (EPFL)\n- arxiv: [https://arxiv.org/abs/2103.12115](https://arxiv.org/abs/2103.12115)\n\n**TFPose: Direct Human Pose Estimation with Transformers**\n\n- intro: The University of Adelaide & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2103.15320](https://arxiv.org/abs/2103.15320)\n\n**TokenPose: Learning Keypoint Tokens for Human Pose Estimation**\n\n- intro: MEGVII Technology & Tsinghua University & Southeast University & Peng Cheng Laboratory\n- arxiv: [https://arxiv.org/abs/2104.03516](https://arxiv.org/abs/2104.03516)\n\n**Pose Recognition with Cascade Transformers**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.06976](https://arxiv.org/abs/2104.06976)\n- github: [https://github.com/mlpc-ucsd/PRTR](https://github.com/mlpc-ucsd/PRTR)\n\n**Is 2D Heatmap Representation Even Necessary for Human Pose Estimation?**\n\n- intro: Tsinghua University & MEGVII Technology & Southeast University & Peng Cheng Laboratory\n- arxiv: [https://arxiv.org/abs/2107.03332](https://arxiv.org/abs/2107.03332)\n- github: [https://github.com/leeyegy/SimDR](https://github.com/leeyegy/SimDR)\n\n**InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation**\n\n- intro: ACM MM 2021\n- arxiv: [https://arxiv.org/abs/2107.08982](https://arxiv.org/abs/2107.08982)\n\n**Adaptive Dilated Convolution For Human Pose Estimation**\n\n- intro: Megvii & UCAS & CRIPAC & NLPR & CASIA\n- arxiv: [https://arxiv.org/abs/2107.10477](https://arxiv.org/abs/2107.10477)\n\n**PoseDet: Fast Multi-Person Pose Estimation Using Pose Embedding**\n\n- intro: Tsinghua University & Northwestern University\n- arxiv: [https://arxiv.org/abs/2107.10466](https://arxiv.org/abs/2107.10466)\n\n**Online Knowledge Distillation for Efficient Pose Estimation**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.02092](https://arxiv.org/abs/2108.02092)\n\n**SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos**\n\n- intro: The Chinese University of Hong Kong & Sensetime Group Ltd. &Shanghai Jiao Tong University & Nanyang Technological University\n- project page: [https://ailingzeng.site/smoothnet](https://ailingzeng.site/smoothnet)\n- arxiv: [https://arxiv.org/abs/2112.13715](https://arxiv.org/abs/2112.13715)\n\n**AdaptivePose: Human Parts as Adaptive Points**\n\n- intro: AAAI 2022\n- intro: Beijing University of Posts and Telecommunications & ByteDance Inc. & Tsinghua University &  Horizon Robotics\n- arxiv: [https://arxiv.org/abs/2112.13635](https://arxiv.org/abs/2112.13635)\n\n**Learning Quality-aware Representation for Multi-person Pose Regression**\n\n- intro: AAAI 2022\n- intro: Beijing University of Posts and Telecommunications & ByteDance Inc. & Tsinghua University &  Horizon Robotics\n- arxiv: [https://arxiv.org/abs/2201.01087](https://arxiv.org/abs/2201.01087)\n\n**Recognition of Freely Selected Keypoints on Human Limbs**\n\n- intro: CVPR 2022 Workshops\n- arxiv: [https://arxiv.org/abs/2204.06326](https://arxiv.org/abs/2204.06326)\n\n**YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss**\n\n- intro: Texas Instruments Inc\n- arxiv: [https://arxiv.org/abs/2204.06806](https://arxiv.org/abs/2204.06806)\n- github: [https://github.com/TexasInstruments/edgeai-yolov5](https://github.com/TexasInstruments/edgeai-yolov5)\n- github: [https://github.com/TexasInstruments/edgeai-yolox](https://github.com/TexasInstruments/edgeai-yolox)\n\n**Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation**\n\n- intro: Tsinghua University & CMU & MIT\n- arxiv: [https://arxiv.org/abs/2205.01271](https://arxiv.org/abs/2205.01271)\n- github: [https://github.com/mit-han-lab/litepose](https://github.com/mit-han-lab/litepose)\n\n# Regression-based Method\n\n**Integral Human Pose Regression**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1711.08229](https://arxiv.org/abs/1711.08229)\n- slides: [https://jimmysuen.github.io/slides/xiaosun_integral_human_pose_regression.pptx](https://jimmysuen.github.io/slides/xiaosun_integral_human_pose_regression.pptx)\n- github: [https://github.com/JimmySuen/integral-human-pose](https://github.com/JimmySuen/integral-human-pose)\n\n**Human Pose Regression with Residual Log-likelihood Estimation**\n\n- intro: ICCV 2021 Oral\n- intro: Shanghai Jiao Tong University & The Chinese University of Hong Kong & SenseTime Research\n- arxiv: [https://arxiv.org/abs/2107.11291](https://arxiv.org/abs/2107.11291)\n- github: [https://github.com/Jeff-sjtu/res-loglikelihood-regression](https://github.com/Jeff-sjtu/res-loglikelihood-regression)\n\n**Poseur: Direct Human Pose Regression with Transformers**\n\n- intro: The University of Adelaide & Alibaba Damo Academy & Zhejiang University\n- arxiv: [https://arxiv.org/abs/2201.07412](https://arxiv.org/abs/2201.07412)\n\n**Location-free Human Pose Estimation**\n\n- intro: Beijing Jiaotong University & Tencent Youtu Lab\n- arxiv: [https://arxiv.org/abs/2205.12619](https://arxiv.org/abs/2205.12619)\n\n# Top-Down\n\n**Point-Set Anchors for Object Detection, Instance Segmentation and Pose Estimation**\n\n- intro: ECCV 2020\n- intro: MSRA & Peking University\n- arxiv: [https://arxiv.org/abs/2007.02846](https://arxiv.org/abs/2007.02846)\n- github: [https://github.com/FangyunWei/PointSetAnchor](https://github.com/FangyunWei/PointSetAnchor)\n\n# Bottom-Up\n\n**PifPaf: Composite Fields for Human Pose Estimation**\n\n- intro: CVPR 2019\n- intro: EPFL VITA lab\n- keywords: Part Intensity Field (PIF), Part Association Field (PAF)\n- arxiv: [https://arxiv.org/abs/1903.06593](https://arxiv.org/abs/1903.06593)\n\n**OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association**\n\n- project: [https://openpifpaf.github.io/intro.html](https://openpifpaf.github.io/intro.html)\n- intro: [https://arxiv.org/abs/2103.02440](https://arxiv.org/abs/2103.02440)\n- github: [https://github.com/openpifpaf/openpifpaf](https://github.com/openpifpaf/openpifpaf)\n\n**Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression**\n\n- intro: CVPR 2021\n- keywords: DEKR\n- intro: University of Science and Technology of China & University of Chinese Academy of Sciences & Microsof\n- arxiv: [https://arxiv.org/abs/2104.02300](https://arxiv.org/abs/2104.02300)\n- github: [https://github.com/HRNet/DEKR](https://github.com/HRNet/DEKR)\n\n**DeepSportLab: a Unified Framework for Ball Detection, Player Instance Segmentation and Pose Estimation in Team Sports Scenes**\n\n- intro: BMVC 2021\n- arxiv: [https://arxiv.org/abs/2112.00627](https://arxiv.org/abs/2112.00627)\n- github: [https://github.com/ispgroupucl/DeepSportLab](https://github.com/ispgroupucl/DeepSportLab)\n\n**Learning Local-Global Contextual Adaptation for Fully End-to-End Bottom-Up Human Pose Estimation**\n\n- intro: Wuhan University & North Carolina State University\n- arxiv: [https://arxiv.org/abs/2109.03622](https://arxiv.org/abs/2109.03622)\n\n**Keypoint Communities**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2110.00988](https://arxiv.org/abs/2110.00988)\n\n**The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation**\n\n- intro: ICCV 2021\n- intro: Technical University of Munich\n- arxiv: [https://arxiv.org/abs/2110.05132](https://arxiv.org/abs/2110.05132)\n\n**Self-Supervision and Spatial-Sequential Attention Based Loss for Multi-Person Pose Estimation**\n\n[https://arxiv.org/abs/2110.10734](https://arxiv.org/abs/2110.10734)\n\n**Learning Local-Global Contextual Adaptation for Multi-Person Pose Estimation**\n\n- intro: CVPR 2022\n- intro: Wuhan University, NC State University\n- arxiv: [https://arxiv.org/abs/2109.03622](https://arxiv.org/abs/2109.03622)\n\n**I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation**\n\n- intro: Xiamen University & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/2206.10892](https://arxiv.org/abs/2206.10892)\n\n**End-to-End Multi-Person Pose Estimation with Transformers**\n\n- intro: CVPR 2022 Oral\n- paper: [https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_End-to-End_Multi-Person_Pose_Estimation_With_Transformers_CVPR_2022_paper.pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_End-to-End_Multi-Person_Pose_Estimation_With_Transformers_CVPR_2022_paper.pdf)\n- github: [https://github.com/hikvision-research/opera/tree/main/configs/petr](https://github.com/hikvision-research/opera/tree/main/configs/petr)\n\n# Hand Pose\n\n**Model-based Deep Hand Pose Estimation**\n\n- paper: [http://xingyizhou.xyz/zhou2016model.pdf](http://xingyizhou.xyz/zhou2016model.pdf)\n- github: [https://github.com/tenstep/DeepModel](https://github.com/tenstep/DeepModel)\n\n**Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation**\n\n- arxiv: [https://arxiv.org/abs/1702.02447](https://arxiv.org/abs/1702.02447)\n\n**Crossing Nets: Combining GANs and VAEs with a Shared Latent Space for Hand Pose Estimation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1702.03431](https://arxiv.org/abs/1702.03431)\n\n**Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation**\n\n- intro: 3DV 2021 Oral\n- arxiv: [https://arxiv.org/abs/2107.00434](https://arxiv.org/abs/2107.00434)\n- github: [https://github.com/zc-alexfan/digit-interacting](https://github.com/zc-alexfan/digit-interacting)\n\n# 3D Pose\n\n**Can 3D Pose be Learned from 2D Projections Alone?**\n\n- intro: ECCV 2018 workshop\n- arxiv: [https://arxiv.org/abs/1808.07182](https://arxiv.org/abs/1808.07182)\n\n**Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views**\n\n- project page: [https://zju-3dv.github.io/mvpose/](https://zju-3dv.github.io/mvpose/)\n- arxiv: [https://arxiv.org/abs/1901.04111](https://arxiv.org/abs/1901.04111)\n- github: [https://github.com/zju-3dv/mvpose](https://github.com/zju-3dv/mvpose)\n\n**3D Human Pose Machines with Self-supervised Learning**\n\n- intro: T-PAMI 2019\n- project page: [http://www.sysu-hcp.net/3d_pose_ssl/](http://www.sysu-hcp.net/3d_pose_ssl/)\n- arxiv: [https://arxiv.org/abs/1901.03798](https://arxiv.org/abs/1901.03798)\n- github: [https://github.com/chanyn/3Dpose_ssl](https://github.com/chanyn/3Dpose_ssl)\n\n**Feature Boosting Network For 3D Pose Estimation**\n\n- intro: Nanyang Technological University & Chalmers University of Technology & Peking University & Alibaba Group\n- arxiv: [https://arxiv.org/abs/1901.04877](https://arxiv.org/abs/1901.04877)\n\n**View Invariant 3D Human Pose Estimation**\n\n- intro: MSRA & USTC\n- arxiv: [https://arxiv.org/abs/1901.10841](https://arxiv.org/abs/1901.10841)\n\n**3D Human Pose Estimation from Deep Multi-View 2D Pose**\n\n[https://arxiv.org/abs/1902.02841](https://arxiv.org/abs/1902.02841)\n\n**RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1902.09868](https://arxiv.org/abs/1902.09868)\n\n**DenseBody: Directly Regressing Dense 3D Human Pose and Shape From a Single Color Image**\n\n- intro: Cloudwalk & Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/1903.10153](https://arxiv.org/abs/1903.10153)\n\n**Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1907.11346](https://arxiv.org/abs/1907.11346)\n- github: [https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE](https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE)\n\n**Lightweight 3D Human Pose Estimation Network Training Using Teacher-Student Learning**\n\n[https://arxiv.org/abs/2001.05097](https://arxiv.org/abs/2001.05097)\n\n**Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS**\n\n- intro: CVPR 2020\n- intro: Tsinghua University & AiFi Inc.\n- arxiv: [https://arxiv.org/abs/2003.03972](https://arxiv.org/abs/2003.03972)\n\n**Skeletor: Skeletal Transformers for Robust Body-Pose Estimation**\n\n- intro: University of Surrey\n- arxiv: [https://arxiv.org/abs/2104.11712](https://arxiv.org/abs/2104.11712)\n\n**UltraPose: Synthesizing Dense Pose with 1 Billion Points by Human-body Decoupling 3D Model**\n\n- intro: ICCV 2021\n- intro: Beijing Momo Technology Co., Ltd. & Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/2110.15267](https://arxiv.org/abs/2110.15267)\n- github: [https://github.com/MomoAILab/ultrapose](https://github.com/MomoAILab/ultrapose)\n\n**Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation**\n\n- intro: CVPR 2022\n- intro: Beihang University & Meitu Inc.\n- arxiv: [https://arxiv.org/abs/2203.07697](https://arxiv.org/abs/2203.07697)\n\n**DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation**\n\n- intro: The Chinese University of Hong Kong & Sensetime Group Ltd. &  Shanghai Artificial Intelligence Laboratory\n- project page: [https://ailingzeng.site/deciwatch](https://ailingzeng.site/deciwatch)\n- arxiv: [https://arxiv.org/abs/2203.08713](https://arxiv.org/abs/2203.08713)\n\n# 3D Car keypoints Detection\n\n**Occlusion-Net: 2D/3D Occluded Keypoint Localization Using Graph Networks**\n\n- intro: CVPR 2019\n- paper: [http://openaccess.thecvf.com/content_CVPR_2019/papers/Reddy_Occlusion-Net_2D3D_Occluded_Keypoint_Localization_Using_Graph_Networks_CVPR_2019_paper.pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Reddy_Occlusion-Net_2D3D_Occluded_Keypoint_Localization_Using_Graph_Networks_CVPR_2019_paper.pdf)\n- github: [https://github.com/dineshreddy91/Occlusion_Net](https://github.com/dineshreddy91/Occlusion_Net)\n\n**RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving**\n\n- intro: Chinese Academy of Sciences & University of Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/2001.03343](https://arxiv.org/abs/2001.03343)\n- github: [https://github.com/Banconxuan/RTM3D](https://github.com/Banconxuan/RTM3D)\n\n**SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation**\n\n- intro: 1ZongMu Tech & TU/e\n- arxiv: [https://arxiv.org/abs/2002.10111](https://arxiv.org/abs/2002.10111)\n\n# Pose Estimation and Action Recognition\n\n**2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1802.09232](https://arxiv.org/abs/1802.09232)\n\n**Multi-task Deep Learning for Real-Time 3D Human Pose Estimation and Action Recognition**\n\n- arxiv: [https://arxiv.org/abs/1912.08077](https://arxiv.org/abs/1912.08077)\n- github: [https://github.com/dluvizon/deephar](https://github.com/dluvizon/deephar)\n\n**Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition**\n\n- intro: ICCV 2021 poster\n- intro: Stanford University & Adobe Research\n- arxiv: [https://arxiv.org/abs/2109.01305](https://arxiv.org/abs/2109.01305)\n\n**Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition**\n\n- intro: ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)\n- arxiv: [https://arxiv.org/abs/2202.11374](https://arxiv.org/abs/2202.11374)\n\n**Revisiting Skeleton-based Action Recognition**\n\n- intro: CVPR 2022 Oral\n- keywords: PoseConv3D\n- arxiv: [https://arxiv.org/abs/2104.13586](https://arxiv.org/abs/2104.13586)\n- github: [https://github.com/kennymckormick/pyskl](https://github.com/kennymckormick/pyskl)\n- github: [https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/README.md](https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/README.md)\n\n# Pose Tracking\n\n**Detect-and-Track: Efficient Pose Estimation in Videos**\n\n- intro: CVPR 2018. CMU & Facebook & Dartmouth\n- intro: Ranked first in ICCV 2017 PoseTrack challenge (keypoint tracking in videos)\n- project page: [https://rohitgirdhar.github.io/DetectAndTrack/](https://rohitgirdhar.github.io/DetectAndTrack/)\n- arxiv: [https://arxiv.org/abs/1712.09184](https://arxiv.org/abs/1712.09184)\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Girdhar_Detect-and-Track_Efficient_Pose_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Girdhar_Detect-and-Track_Efficient_Pose_CVPR_2018_paper.pdf)\n- github: [https://github.com/facebookresearch/DetectAndTrack/](https://github.com/facebookresearch/DetectAndTrack/)\n\n**Simple Baselines for Human Pose Estimation and Tracking**\n\n- intro: ECCV 2018\n- intro: MSRA\n- keywords: optical flow based pose propagation and similarity measurement\n- arxiv: [https://arxiv.org/abs/1804.06208](https://arxiv.org/abs/1804.06208)\n- github(official): [https://github.com/Microsoft/human-pose-estimation.pytorch](https://github.com/Microsoft/human-pose-estimation.pytorch)\n\n**A Top-down Approach to Articulated Human Pose Estimation and Tracking**\n\n- intro: JD.com Silicon Valley Research Center\n- intro: ECCVW 2018. Workshop: 2nd PoseTrack Challenge\n- arxiv: [https://arxiv.org/abs/1901.07680](https://arxiv.org/abs/1901.07680)\n\n**15 Keypoints Is All You Need**\n\n- intro: Brown University & NEC Labs America\n- arxiv: [https://arxiv.org/abs/1912.02323](https://arxiv.org/abs/1912.02323)\n\n**Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking**\n\n- intro: CVPR 2021\n- intro: Stevens Institute of Technology & Wormpex AI Research & National University of Singapore\n- arxiv: [https://arxiv.org/abs/2106.03772](https://arxiv.org/abs/2106.03772)\n\n# Object Pose Estimation\n\n**Real-Time Object Pose Estimation with Pose Interpreter Networks**\n\n- intro: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2018)\n- arxiv: [https://arxiv.org/abs/1808.01099](https://arxiv.org/abs/1808.01099)\n- github: [https://github.com/jimmyyhwu/pose-interpreter-networks](https://github.com/jimmyyhwu/pose-interpreter-networks)\n\n# Projects\n\n**MobilePose: Single Person Pose Estimation for Mobile Device**\n\n- intro: a Tiny PyTorch implementation of single person 2D pose estimation framework\n- github: [https://github.com/YuliangXiu/MobilePose-pytorch](https://github.com/YuliangXiu/MobilePose-pytorch)\n github: [https://github.com/MVIG-SJTU/AlphaPose](https://github.com/MVIG-SJTU/AlphaPose)\n\n**PyTorch-Pose: A PyTorch toolkit for 2D Human Pose Estimation**\n\n- intro: a PyTorch implementation of the general pipeline for 2D single human pose estimation.\n- github: [https://github.com/bearpaw/pytorch-pose](https://github.com/bearpaw/pytorch-pose)\n\n**Hourglass, DHN and CPN model in TensorFlow for 2018-FashionAI Key Points Detection of Apparel at TianChi**\n\n- intro: Full pipeline for TianChi FashionAI clothes keypoints detection compitetion in TensorFlow\n- github: [https://github.com/HiKapok/tf.fashionAI](https://github.com/HiKapok/tf.fashionAI)\n\n**FashionAI: KeyPoint Detection Challenge in Keras**\n\n- intro: Code for TianChi 2018 FashionAI Cloth KeyPoint Detection Challenge\n- github: [https://github.com/yuanyuanli85/FashionAI_KeyPoint_Detection_Challenge_Keras](https://github.com/yuanyuanli85/FashionAI_KeyPoint_Detection_Challenge_Keras)\n\n# Challenge\n\n**POSETRACK CHALLENGE: ARTICULATED PEOPLE TRACKING IN THE WILD**\n\n[https://posetrack.net/workshops/eccv2018/#challenges](https://posetrack.net/workshops/eccv2018/#challenges)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","title":"Classification / Recognition"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Classification / Recognition\ndate: 2015-10-09\n---\n\n# Papers\n\n**DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition**\n\n- auothor: Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell\n- arxiv: [http://arxiv.org/abs/1310.1531](http://arxiv.org/abs/1310.1531)\n\n**CNN Features off-the-shelf: an Astounding Baseline for Recognition**\n\n- intro: CVPR 2014\n- arxiv: [http://arxiv.org/abs/1403.6382](http://arxiv.org/abs/1403.6382)\n\n**HD-CNN: Hierarchical Deep Convolutional Neural Network for Image Classification**\n\n**HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition**\n\n- intro: ICCV 2015\n- intro: introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy\n- project page: [https://sites.google.com/site/homepagezhichengyan/home/hdcnn](https://sites.google.com/site/homepagezhichengyan/home/hdcnn)\n- arxiv: [https://arxiv.org/abs/1410.0736](https://arxiv.org/abs/1410.0736)\n- code: [https://sites.google.com/site/homepagezhichengyan/home/hdcnn/code](https://sites.google.com/site/homepagezhichengyan/home/hdcnn/code)\n- github: [https://github.com/stephenyan1231/caffe-public/tree/hdcnn](https://github.com/stephenyan1231/caffe-public/tree/hdcnn)\n\n**Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification**\n\n- intro: ImageNet top-5 error: 4.94%\n- arxiv: [http://arxiv.org/abs/1502.01852](http://arxiv.org/abs/1502.01852)\n- notes: [http://blog.csdn.net/happynear/article/details/45440811](http://blog.csdn.net/happynear/article/details/45440811)\n\n**Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1511.05520](http://arxiv.org/abs/1511.05520)\n- github: [https://github.com/glennq/instrument-recognition](https://github.com/glennq/instrument-recognition)\n\n**Deep Convolutional Networks on the Pitch Spiral for Musical Instrument Recognition**\n\n- paper: [https://github.com/lostanlen/ismir2016/blob/master/paper/lostanlen_ismir2016.pdf](https://github.com/lostanlen/ismir2016/blob/master/paper/lostanlen_ismir2016.pdf)\n- github: [https://github.com/lostanlen/ismir2016](https://github.com/lostanlen/ismir2016)\n\n**Humans and deep networks largely agree on which kinds of variation make object recognition harder**\n\n- arxiv: [http://arxiv.org/abs/1604.06486](http://arxiv.org/abs/1604.06486)\n- review: [https://www.technologyreview.com/s/601387/why-machine-vision-is-flawed-in-the-same-way-as-human-vision/](https://www.technologyreview.com/s/601387/why-machine-vision-is-flawed-in-the-same-way-as-human-vision/)\n\n**FusionNet: 3D Object Classification Using Multiple Data Representations**\n\n- arxiv: [https://arxiv.org/abs/1607.05695](https://arxiv.org/abs/1607.05695)\n\n**From image recognition to object recognition**\n\n- blog: [https://www.oreilly.com/ideas/from-image-recognition-to-object-recognition](https://www.oreilly.com/ideas/from-image-recognition-to-object-recognition)\n\n**Deep FisherNet for Object Classification**\n\n- arxiv: [http://arxiv.org/abs/1608.00182](http://arxiv.org/abs/1608.00182)\n\n**Factorized Bilinear Models for Image Recognition**\n\n- intro: TuSimple\n- arxiv: [https://arxiv.org/abs/1611.05709](https://arxiv.org/abs/1611.05709)\n- github(MXNet): [https://github.com/lyttonhao/Factorized-Bilinear-Network](https://github.com/lyttonhao/Factorized-Bilinear-Network)\n\n**Hyperspectral CNN Classification with Limited Training Samples**\n\n- arxiv: [https://arxiv.org/abs/1611.09007](https://arxiv.org/abs/1611.09007)\n\n**The More You Know: Using Knowledge Graphs for Image Classification**\n\n- intro: CMU. GSNN\n- arxiv: [https://arxiv.org/abs/1612.04844](https://arxiv.org/abs/1612.04844)\n\n**MaxMin Convolutional Neural Networks for Image Classification**\n\n- paper: [http://webia.lip6.fr/~thomen/papers/Blot_ICIP_2016.pdf](http://webia.lip6.fr/~thomen/papers/Blot_ICIP_2016.pdf)\n- github: [https://github.com/karandesai-96/maxmin-cnn](https://github.com/karandesai-96/maxmin-cnn)\n\n**Cost-Effective Active Learning for Deep Image Classification**\n\n- intro: TCSVT 2016\n- intro: Sun Yat-sen University & Guangzhou University\n- arxiv: [https://arxiv.org/abs/1701.03551](https://arxiv.org/abs/1701.03551)\n\n**Deep Collaborative Learning for Visual Recognition**\n\n[https://www.arxiv.org/abs/1703.01229](https://www.arxiv.org/abs/1703.01229)\n\n**Convolutional Low-Resolution Fine-Grained Classification**\n\n[https://arxiv.org/abs/1703.05393](https://arxiv.org/abs/1703.05393)\n\n**Multi-Scale Dense Networks for Resource Efficient Image Classification**\n\n- intro: Cornell University & Fudan University & Tsinghua University & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1703.09844](https://arxiv.org/abs/1703.09844)\n- github: [https://github.com//gaohuang/MSDNet](https://github.com//gaohuang/MSDNet)\n\n**Deep Mixture of Diverse Experts for Large-Scale Visual Recognition**\n\n[https://arxiv.org/abs/1706.07901](https://arxiv.org/abs/1706.07901)\n\n**Sunrise or Sunset: Selective Comparison Learning for Subtle Attribute Recognition**\n\n- intro: BMVC 2017\n- arxiv: [https://arxiv.org/abs/1707.06335](https://arxiv.org/abs/1707.06335)\n\n**Why Do Deep Neural Networks Still Not Recognize These Images?: A Qualitative Analysis on Failure Cases of ImageNet Classification**\n\n- intro: Poster presented at CVPR 2017 Scene Understanding Workshop\n- arxiv: [https://arxiv.org/abs/1709.03439](https://arxiv.org/abs/1709.03439)\n\n**B-CNN: Branch Convolutional Neural Network for Hierarchical Classification**\n\n[https://arxiv.org/abs/1709.09890](https://arxiv.org/abs/1709.09890)\n\n**Learning Transferable Architectures for Scalable Image Recognition**\n\n- intro: Google Brain\n- keywords: Neural Architecture Search\n- arxiv: [https://arxiv.org/abs/1707.07012](https://arxiv.org/abs/1707.07012)\n\n**AOGNets: Deep AND-OR Grammar Networks for Visual Recognition**\n\n[https://arxiv.org/abs/1711.05847](https://arxiv.org/abs/1711.05847)\n\n**Knowledge Concentration: Learning 100K Object Classifiers in a Single CNN**\n\n- intro: University of Southern California & Google Research\n- arxiv: [https://arxiv.org/abs/1711.07607](https://arxiv.org/abs/1711.07607)\n\n**Between-class Learning for Image Classification**\n\n- intro: The University of Tokyo & RIKEN\n- arxiv: [https://arxiv.org/abs/1711.10284](https://arxiv.org/abs/1711.10284)\n\n**Efficient Traffic-Sign Recognition with Scale-aware CNN**\n\n- intro: BMVC 2017\n- arxiv: [https://arxiv.org/abs/1805.12289](https://arxiv.org/abs/1805.12289)\n\n**Co-domain Embedding using Deep Quadruplet Networks for Unseen Traffic Sign Recognition**\n\n- intro: AAAI 2018\n- arix:v[https://arxiv.org/abs/1712.01907](https://arxiv.org/abs/1712.01907)\n\n**µNet: A Highly Compact Deep Convolutional Neural Network Architecture for Real-time Embedded Traffic Sign Classification**\n\n[https://arxiv.org/abs/1804.00497](https://arxiv.org/abs/1804.00497)\n\n**Deep Predictive Coding Network for Object Recognition**\n\n[https://arxiv.org/abs/1802.04762](https://arxiv.org/abs/1802.04762)\n\n**Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs**\n\n- intro: CVPR 2018. The Robotics Institute, Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/1803.08035](https://arxiv.org/abs/1803.08035)\n\n**Attention-based Pyramid Aggregation Network for Visual Place Recognition**\n\n- intro: ACM MM 2018\n- arxiv: [https://arxiv.org/abs/1808.00288](https://arxiv.org/abs/1808.00288)\n\n**How do Convolutional Neural Networks Learn Design?**\n\n- intro: ICPR 2018\n- arxiv: [https://arxiv.org/abs/1808.08402](https://arxiv.org/abs/1808.08402)\n\n**Making Classification Competitive for Deep Metric Learning**\n\n[https://arxiv.org/abs/1811.12649](https://arxiv.org/abs/1811.12649)\n\n**In Defense of the Triplet Loss for Visual Recognition**\n\n- intro: University of Maryland & Honda Research Institute\n- arxiv: [https://arxiv.org/abs/1901.08616](https://arxiv.org/abs/1901.08616)\n\n**All You Need is a Few Shifts: Designing Efficient Convolutional Neural Networks for Image Classification**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.05285](https://arxiv.org/abs/1903.05285)\n\n**Deep CNN-based Multi-task Learning for Open-Set Recognition**\n\n[https://arxiv.org/abs/1903.03161](https://arxiv.org/abs/1903.03161)\n\n**Squared Earth Mover's Distance-based Loss for Training Deep Neural Networks**\n\n[https://arxiv.org/abs/1611.05916](https://arxiv.org/abs/1611.05916)\n\n**Large-Scale Long-Tailed Recognition in an Open World**\n\n- intro: CVPR 2019 oral\n- intro: CUHK & UC Berkeley / ICSI\n- project page: [https://liuziwei7.github.io/projects/LongTail.html](https://liuziwei7.github.io/projects/LongTail.html)\n- arxiv: [https://arxiv.org/abs/1904.05160](https://arxiv.org/abs/1904.05160)\n\n**An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**\n\n- intro: Google Research, Brain Team\n- arxiv: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)\n- github: [https://github.com/google-research/vision_transformer](https://github.com/google-research/vision_transformer)\n\n**High-Performance Large-Scale Image Recognition Without Normalization**\n\n- intro: NFNet\n- arxiv: [https://arxiv.org/abs/2102.06171](https://arxiv.org/abs/2102.06171)\n- github: [https://github.com/deepmind/deepmind-research/tree/master/nfnets](https://github.com/deepmind/deepmind-research/tree/master/nfnets)\n\n# Massive Classification\n\n**Accelerated Training for Massive Classification via Dynamic Class Selection**\n\n- intro: AAAI 2018. CUHK & SenseTime\n- keywords: HF-Softmax\n- arxiv: [https://arxiv.org/abs/1801.01687](https://arxiv.org/abs/1801.01687)\n- github: [https://github.com/yl-1993/hfsoftmax](https://github.com/yl-1993/hfsoftmax)\n\n# Multi-object Recognition\n\n**Multiple Object Recognition with Visual Attention**\n\n- keyword: deep recurrent neural network, reinforcement learning\n- arxiv: [https://arxiv.org/abs/1412.7755](https://arxiv.org/abs/1412.7755)\n- github: [https://github.com/jrbtaylor/visual-attention](https://github.com/jrbtaylor/visual-attention)\n\n**Multiple Instance Learning Convolutional Neural Networks for Object Recognition**\n\n- intro: ICPR 2016 Oral\n- arxiv: [https://arxiv.org/abs/1610.03155](https://arxiv.org/abs/1610.03155)\n\n# Multi-Label Classification\n\n**Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification**\n\n- intro: CVPR 2017\n- intro: University of Science and Technology of China & CUHK\n- arxiv: [https://arxiv.org/abs/1702.05891](https://arxiv.org/abs/1702.05891)\n- github(official. Caffe): [https://github.com/zhufengx/SRN_multilabel/](https://github.com/zhufengx/SRN_multilabel/)\n\n**Order-Free RNN with Visual Attention for Multi-Label Classification**\n\n[https://arxiv.org/abs/1707.05495](https://arxiv.org/abs/1707.05495)\n\n**Learning Social Image Embedding with Deep Multimodal Attention Networks**\n\n- intro: Beihang University & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1710.06582](https://arxiv.org/abs/1710.06582)\n\n**Multi-label Image Recognition by Recurrently Discovering Attentional Regions**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1711.02816](https://arxiv.org/abs/1711.02816)\n\n**Recurrent Attentional Reinforcement Learning for Multi-label Image Recognition**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1712.07465](https://arxiv.org/abs/1712.07465)\n\n**A Baseline for Multi-Label Image Classification Using Ensemble Deep CNN**\n\n[https://arxiv.org/abs/1811.08412](https://arxiv.org/abs/1811.08412)\n\n**Multi-class Classification without Multi-class Labels**\n\n- intro: ICLR 2019\n- arxiv: [https://arxiv.org/abs/1901.00544](https://arxiv.org/abs/1901.00544)\n\nL**earning a Deep ConvNet for Multi-label Classification with Partial Labels**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1902.09720](https://arxiv.org/abs/1902.09720)\n\n**Multi-Label Image Recognition with Graph Convolutional Networks**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.03582](https://arxiv.org/abs/1904.03582)\n- github: [https://github.com/chenzhaomin123/ML_GCN](https://github.com/chenzhaomin123/ML_GCN)\n\n**General Multi-label Image Classification with Transformers**\n\n- intro: University of Virginia\n- arxiv: [https://arxiv.org/abs/2011.14027](https://arxiv.org/abs/2011.14027)\n\n# Person Recognition\n\n**Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues**\n\n- intro: UC Berkeley & Facebook AI Research\n- keywords: People In Photo Albums (PIPA) dataset, Pose Invariant PErson Recognition (PIPER)\n- project page: [https://people.eecs.berkeley.edu/~nzhang/piper.html](https://people.eecs.berkeley.edu/~nzhang/piper.html)\n- arxiv: [https://arxiv.org/abs/1501.05703](https://arxiv.org/abs/1501.05703)\n\n## COCO_v1\n\n**Learning Deep Features via Congenerous Cosine Loss for Person Recognition**\n\n- keywords: COCO loss\n- arxiv: [https://arxiv.org/abs/1702.06890](https://arxiv.org/abs/1702.06890)\n- github: [https://github.com/sciencefans/coco_loss](https://github.com/sciencefans/coco_loss)\n\n**Pose-Aware Person Recognition**\n\n- intro: CVIT & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1705.10120](https://arxiv.org/abs/1705.10120)\n\n## COCO_v2\n\n**Rethinking Feature Discrimination and Polymerization for Large-scale Recognition**\n\n- intro: NIPS 2017 Deep Learning Workshop\n- keywords: COCO loss\n- arxiv: [https://arxiv.org/abs/1710.00870](https://arxiv.org/abs/1710.00870)\n- github: [https://github.com/sciencefans/coco_loss](https://github.com/sciencefans/coco_loss)\n\n**Person Recognition in Social Media Photos**\n\n[https://arxiv.org/abs/1710.03224](https://arxiv.org/abs/1710.03224)\n\n**Unifying Identification and Context Learning for Person Recognition**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Unifying_Identification_and_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Unifying_Identification_and_CVPR_2018_paper.pdf)\n\n# Fine-grained Recognition\n\n**Bilinear CNN Models for Fine-grained Visual Recognition**\n\n![](http://people.cs.umass.edu/~smaji/picon/bcnn.png)\n\n- intro: ICCV 2015\n- homepage: [http://vis-www.cs.umass.edu/bcnn/](http://vis-www.cs.umass.edu/bcnn/)\n- paper: [http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf](http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf)\n- arxiv: [http://arxiv.org/abs/1504.07889](http://arxiv.org/abs/1504.07889)\n- bitbucket: [https://bitbucket.org/tsungyu/bcnn.git](https://bitbucket.org/tsungyu/bcnn.git)\n\n**Fine-grained Image Classification by Exploring Bipartite-Graph Labels**\n\n![](http://www.f-zhou.com/fg/over.png)\n\n- intro: CVPR 2016\n- project page: [http://www.f-zhou.com/fg.html](http://www.f-zhou.com/fg.html)\n- arxiv: [http://arxiv.org/abs/1512.02665](http://arxiv.org/abs/1512.02665)\n- demo: [http://www.f-zhou.com/fg_demo/](http://www.f-zhou.com/fg_demo/)\n\n**Embedding Label Structures for Fine-Grained Feature Representation**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1512.02895](http://arxiv.org/abs/1512.02895)\n- paper: [http://webpages.uncc.edu/~szhang16/paper/CVPR16_structured_labels.pdf](http://webpages.uncc.edu/~szhang16/paper/CVPR16_structured_labels.pdf)\n\n**Fine-grained Categorization and Dataset Bootstrapping using Deep Metric Learning with Humans in the Loop**\n\n- arxiv: [http://arxiv.org/abs/1512.05227](http://arxiv.org/abs/1512.05227)\n\n**Fully Convolutional Attention Localization Networks: Efficient Attention Localization for Fine-Grained Recognition**\n\n- arxiv: [http://arxiv.org/abs/1603.06765](http://arxiv.org/abs/1603.06765)\n\n**Localizing by Describing: Attribute-Guided Attention Localization for Fine-Grained Recognition**\n\n- arxiv: [https://arxiv.org/abs/1605.06217](https://arxiv.org/abs/1605.06217)\n\n**Learning Deep Representations of Fine-grained Visual Descriptions**\n\n![](https://raw.githubusercontent.com/reedscot/cvpr2016/master/images/description_embedding.jpg)\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1605.05395](http://arxiv.org/abs/1605.05395)\n- github: [https://github.com/reedscot/cvpr2016](https://github.com/reedscot/cvpr2016)\n\n**IDNet: Smartphone-based Gait Recognition with Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1606.03238](http://arxiv.org/abs/1606.03238)\n\n**Picking Deep Filter Responses for Fine-grained Image Recognition**\n\n- intro: CVPR 2016\n\n**SPDA-CNN: Unifying Semantic Part Detection and Abstraction for Fine-grained Recognition**\n\n- intro: CVPR 2016\n\n**Part-Stacked CNN for Fine-Grained Visual Categorization**\n\n- intro: CVPR 2016\n\n**Fine-grained Recognition in the Noisy Wild: Sensitivity Analysis of Convolutional Neural Networks Approaches**\n\n- intro: BMVC 2016\n- arxiv: [https://arxiv.org/abs/1610.06756](https://arxiv.org/abs/1610.06756)\n\n**Low-rank Bilinear Pooling for Fine-Grained Classification**\n\n- intro: CVPR 2017\n- project page: [http://www.ics.uci.edu/~skong2/lr_bilinear.html](http://www.ics.uci.edu/~skong2/lr_bilinear.html)\n- arxiv: [https://arxiv.org/abs/1611.05109](https://arxiv.org/abs/1611.05109)\n- github: [https://github.com/aimerykong/Low-Rank-Bilinear-Pooling](https://github.com/aimerykong/Low-Rank-Bilinear-Pooling)\n\n**细粒度图像分析**\n\n- intro: by 吴建鑫, NJU. VALSE 2017 Annual Progress Review Series\n- slides: [http://mac.xmu.edu.cn/valse2017/ppt/APR/wjx_APR.pdf](http://mac.xmu.edu.cn/valse2017/ppt/APR/wjx_APR.pdf)\n\n**Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition**\n\n- intro: CVPR 2017\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf)\n\n**Fine-grained Recognition in the Wild: A Multi-Task Domain Adaptation Approach**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.02476](https://arxiv.org/abs/1709.02476)\n\n**Where to Focus: Deep Attention-based Spatially Recurrent Bilinear Networks for Fine-Grained Visual Recognition**\n\n[https://arxiv.org/abs/1709.05769](https://arxiv.org/abs/1709.05769)\n\n**Learning Multi-Attention Convolutional Neural Network for Fine-Grained Image Recognition**\n\n- introL ICCV 2017\n- keywords: MA-CNN\n- intro: University of Science and Technology of China & Microsoft Research & University of Rochester\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Zheng_Learning_Multi-Attention_Convolutional_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zheng_Learning_Multi-Attention_Convolutional_ICCV_2017_paper.pdf)\n\n**TransFG: A Transformer Architecture for Fine-grained Recognition**\n\n- intro: Johns Hopkins University & ByteDance Inc.\n- arxiv: [https://arxiv.org/abs/2103.07976](https://arxiv.org/abs/2103.07976)\n\n# Food Recognition\n\n**DeepFood: Deep Learning-Based Food Image Recognition for Computer-Aided Dietary Assessment**\n\n- arxiv: [http://arxiv.org/abs/1606.05675](http://arxiv.org/abs/1606.05675)\n- github: [https://github.com/deercoder/DeepFood](https://github.com/deercoder/DeepFood)\n\n**Im2Calories: towards an automated mobile vision food diary**\n\n- intro: recognize the contents of your meal from a single image, then predict its nutritional contents, such as calories\n- paper: [http://www.cs.ubc.ca/~murphyk/Papers/im2calories_iccv15.pdf](http://www.cs.ubc.ca/~murphyk/Papers/im2calories_iccv15.pdf)\n\n**Food Image Recognition by Using Convolutional Neural Networks (CNNs)**\n\n- arxiv: [https://arxiv.org/abs/1612.00983](https://arxiv.org/abs/1612.00983)\n\n**Wide-Slice Residual Networks for Food Recognition**\n\n- arxiv: [https://arxiv.org/abs/1612.06543](https://arxiv.org/abs/1612.06543)\n\n**Food Classification with Deep Learning in Keras / Tensorflow**\n\n- blog: [http://blog.stratospark.com/deep-learning-applied-food-classification-deep-learning-keras.html](http://blog.stratospark.com/deep-learning-applied-food-classification-deep-learning-keras.html)\n- github: [https://github.com/stratospark/food-101-keras](https://github.com/stratospark/food-101-keras)\n\n**ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition**\n\n[https://arxiv.org/abs/1705.02743](https://arxiv.org/abs/1705.02743)\n\n**Computer vision-based food calorie estimation: dataset, method, and experiment**\n\n[https://arxiv.org/abs/1705.07632](https://arxiv.org/abs/1705.07632)\n\n**Deep Learning-Based Food Calorie Estimation Method in Dietary Assessment**\n\n[https://arxiv.org/abs/1706.04062](https://arxiv.org/abs/1706.04062)\n\n**Food Ingredients Recognition through Multi-label Learning**\n\n[https://arxiv.org/abs/1707.08816](https://arxiv.org/abs/1707.08816)\n\n**FoodNet: Recognizing Foods Using Ensemble of Deep Networks**\n\n- intro: IEEE Signal Processing Letters\n- arxiv: [https://arxiv.org/abs/1709.09429](https://arxiv.org/abs/1709.09429)\n\n**Food recognition and recipe analysis: integrating visual content, context and external knowledge**\n\n[https://arxiv.org/abs/1801.07230](https://arxiv.org/abs/1801.07230)\n\n# Attribute Recognition\n\n**Multi-task CNN Model for Attribute Prediction**\n\n- intro: ieee transaction paper\n- arxiv: [https://arxiv.org/abs/1601.00400](https://arxiv.org/abs/1601.00400)\n\n**Attributes for Improved Attributes: A Multi-Task Network for Attribute Classification**\n\n[https://arxiv.org/abs/1604.07360](https://arxiv.org/abs/1604.07360)\n\n**Generative Adversarial Models for People Attribute Recognition in Surveillance**\n\n- intro: AVSS 2017 oral\n- arxiv: [https://arxiv.org/abs/1707.02240](https://arxiv.org/abs/1707.02240)\n\n**Attribute Recognition by Joint Recurrent Learning of Context and Correlation**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.08553](https://arxiv.org/abs/1709.08553)\n\n**Multi-label Object Attribute Classification using a Convolutional Neural Network**\n\n[https://arxiv.org/abs/1811.04309](https://arxiv.org/abs/1811.04309)\n\n# Pedestrian Attribute Recognition / Person Attribute Recognition\n\n**Multi-attribute Learning for Pedestrian Attribute Recognition in Surveillance Scenarios**\n\n- intro: ACPR 2015\n- keywords: DeepSAR / DeepMAR\n- paper: [http://or.nsfc.gov.cn/bitstream/00001903-5/417802/1/1000014103914.pdf](http://or.nsfc.gov.cn/bitstream/00001903-5/417802/1/1000014103914.pdf)\n- github: [https://github.com/kyu-sz/DeepMAR_deploy](https://github.com/kyu-sz/DeepMAR_deploy)\n- github: [https://github.com/dangweili/pedestrian-attribute-recognition-pytorch](https://github.com/dangweili/pedestrian-attribute-recognition-pytorch)\n\n**Pedestrian Attribute Recognition At Far Distance**\n\n- intro: ACM MM 2014\n- paper: [http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf](http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf)\n\n**Person Attribute Recognition with a Jointly-trained Holistic CNN Model**\n\n- intro: ICCV 2015\n- keywords: Parse27k\n- arxiv: [https://www.vision.rwth-aachen.de/media/papers/sudowe_spitzer_leibe_ICCV_LaP_2015.pdf](https://www.vision.rwth-aachen.de/media/papers/sudowe_spitzer_leibe_ICCV_LaP_2015.pdf)\n\n**Human Attribute Recognition by Deep Hierarchical Contexts**\n\n- intro: ECCV 2016\n- paper: [http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf](http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2016_human.pdf)\n\n**Robust Pedestrian Attribute Recognition for an Unbalanced Dataset using Mini-batch Training with Rarity Rate**\n\n- intro: Intelligent Vehicles Symposium 2016\n- intro: Chubu University & Nagoya University, Japan\n- paper: [http://www.vision.cs.chubu.ac.jp/MPRG/C_group/C081_fukui2016.pdf](http://www.vision.cs.chubu.ac.jp/MPRG/C_group/C081_fukui2016.pdf)\n\n**Weakly-supervised Learning of Mid-level Features for Pedestrian Attribute Recognition and Localization**\n\n- arxiv: [https://arxiv.org/abs/1611.05603](https://arxiv.org/abs/1611.05603)\n- github: [https://github.com/kyu-sz/WPAL-network](https://github.com/kyu-sz/WPAL-network)\n\n**Deep View-Sensitive Pedestrian Attribute Inference in an end-to-end Model**\n\n- intro: BMVC 2017\n- keywords: PETA, RAP and WIDER\n- arxiv: [https://arxiv.org/abs/1707.06089](https://arxiv.org/abs/1707.06089)\n- github: [https://github.com/asc-kit/vespa](https://github.com/asc-kit/vespa)\n\n**HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis**\n\n- intro: ICCV 2017\n- intro: CUHK & SenseTime\n- keywords: multi-directional attention (MDA)\n- arxiv: [https://arxiv.org/abs/1709.09930](https://arxiv.org/abs/1709.09930)\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_HydraPlus-Net_Attentive_Deep_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_HydraPlus-Net_Attentive_Deep_ICCV_2017_paper.pdf)\n- github: [https://github.com/xh-liu/HydraPlus-Net](https://github.com/xh-liu/HydraPlus-Net)\n\n**Deep Imbalanced Attribute Classification using Visual Attention Aggregation**\n\n- intro: ECCV 2018\n- intro: University of Houston\n- arxiv: [https://arxiv.org/abs/1807.03903](https://arxiv.org/abs/1807.03903)\n\n**Localization Guided Learning for Pedestrian Attribute Recognition**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1808.09102](https://arxiv.org/abs/1808.09102)\n\n**Grouping Attribute Recognition for Pedestrian with Joint Recurrent Learning**\n\n- intro: IJCAI 2018\n- paper: [https://www.ijcai.org/proceedings/2018/0441.pdf](https://www.ijcai.org/proceedings/2018/0441.pdf)\n\n**Sequence-based Person Attribute Recognition with Joint CTC-Attention Model**\n\n- keywords: joint CTC-Attention model (JCM), s connectionist temporal classification (CTC)\n- arxiv: [https://arxiv.org/abs/1811.08115](https://arxiv.org/abs/1811.08115)\n\n**The Deeper, the Better: Analysis of Person Attributes Recognition**\n\n[https://arxiv.org/abs/1901.03756](https://arxiv.org/abs/1901.03756)\n\n**Video-Based Pedestrian Attribute Recognition**\n\n[https://arxiv.org/abs/1901.05742](https://arxiv.org/abs/1901.05742)\n\n**Pedestrian Attribute Recognition: A Survey**\n\n- intro: Anhui University\n- project page: [https://sites.google.com/view/ahu-pedestrianattributes/](https://sites.google.com/view/ahu-pedestrianattributes/)\n- arxiv: [https://arxiv.org/abs/1901.07474](https://arxiv.org/abs/1901.07474)\n\n**Papers with code: Pedestrian Attribute Recognition**\n\n[https://paperswithcode.com/task/pedestrian-attribute-recognition/codeless](https://paperswithcode.com/task/pedestrian-attribute-recognition/codeless)\n\n**Pedestrian-Attribute-Recognition-Paper-List**\n\n[https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List](https://github.com/wangxiao5791509/Pedestrian-Attribute-Recognition-Paper-List)\n\n**Attribute Aware Pooling for Pedestrian Attribute Recognition**\n\n- intro: IJCAI 2019\n- intro: Huawei Noah’s Ark Lab & University of Sydney\n- arxiv: [https://arxiv.org/abs/1907.11837](https://arxiv.org/abs/1907.11837)\n\n**Distraction-Aware Feature Learning for Human Attribute Recognition via Coarse-to-Fine Attention Mechanism**\n\n- intro: AAAI 2020 oral\n- arxiv: [https://arxiv.org/abs/1911.11351](https://arxiv.org/abs/1911.11351)\n\n**Rethinking of Pedestrian Attribute Recognition: Realistic Datasets with Efficient Method**\n\n- intro: University of Chinese Academy of Sciences & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/2005.11909](https://arxiv.org/abs/2005.11909)\n- github(official, Pytorch): [https://github.com/valencebond/Strong_Baseline_of_Pedestrian_Attribute_Recognition](https://github.com/valencebond/Strong_Baseline_of_Pedestrian_Attribute_Recognition)\n\n**Hierarchical Feature Embedding for Attribute Recognition**\n\n- intro: SenseTime Group Limited & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2005.11576](https://arxiv.org/abs/2005.11576)\n\n# Clothes Recognition\n\n**DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations**\n\n![](http://personal.ie.cuhk.edu.hk/~lz013/projects/deepfashion/intro.png)\n\n- intro: CVPR 2016\n- keywords: FashionNet\n- project page: [http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html](http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf)\n\n**Multi-Task Curriculum Transfer Deep Learning of Clothing Attributes**\n\n- arxiv: [https://arxiv.org/abs/1610.03670](https://arxiv.org/abs/1610.03670)\n\n# Star-galaxy Classification\n\n**Star-galaxy Classification Using Deep Convolutional Neural Networks**\n\n- intro: MNRAS\n- arxiv: [http://arxiv.org/abs/1608.04369](http://arxiv.org/abs/1608.04369)\n- github: [https://github.com/EdwardJKim/dl4astro](https://github.com/EdwardJKim/dl4astro)\n\n# Logo Recognition\n\n**Deep Learning for Logo Recognition**\n\n- arxiv: [https://arxiv.org/abs/1701.02620](https://arxiv.org/abs/1701.02620)\n\n# Plant Classification\n\n**Large-Scale Plant Classification with Deep Neural Networks**\n\n- intro: Published at Proocedings of ACM Computing Frontiers Conference 2017\n- arxiv: [https://arxiv.org/abs/1706.03736](https://arxiv.org/abs/1706.03736)\n\n# Scene Recognition / Scene Classification\n\n**Learning Deep Features for Scene Recognition using Places Database**\n\n- paper: [http://places.csail.mit.edu/places_NIPS14.pdf](http://places.csail.mit.edu/places_NIPS14.pdf)\n- gihtub: [https://github.com/metalbubble/places365](https://github.com/metalbubble/places365)\n\n**Using neon for Scene Recognition: Mini-Places2**\n\n- intro: This is an implementation of the deep residual network used for \n[Mini-Places2](http://6.869.csail.mit.edu/fa15/project.html) as described in \n[He et. al., \"Deep Residual Learning for Image Recognition\"](http://arxiv.org/abs/1512.03385).\n- blog: [http://www.nervanasys.com/using-neon-for-scene-recognition-mini-places2/](http://www.nervanasys.com/using-neon-for-scene-recognition-mini-places2/)\n- github: [https://github.com/hunterlang/mpmz](https://github.com/hunterlang/mpmz)\n\n**Scene Classification with Inception-7**\n\n- slides: [http://lsun.cs.princeton.edu/slides/Christian.pdf](http://lsun.cs.princeton.edu/slides/Christian.pdf)\n\n**Semantic Clustering for Robust Fine-Grained Scene Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.07614](http://arxiv.org/abs/1607.07614)\n\n**Scene recognition with CNNs: objects, scales and dataset bias**\n\n- intro: CVPR 2016\n- arxiv: [https://arxiv.org/abs/1801.06867](https://arxiv.org/abs/1801.06867)\n\n## Leaderboard\n\n**Leaderboard of Places Database**\n\n- intro: currently rank1: Qian Zhang(Beijing Samsung Telecom R&D Center), 0.6410@top1, 0.9065@top5\n- homepage: [http://places.csail.mit.edu/user/leaderboard.php](http://places.csail.mit.edu/user/leaderboard.php)\n\n# Blogs\n\n**What is the class of this image ? - Discover the current state of the art in objects classification**\n\n- intro: \"Discover the current state of the art in objects classification.\"\n- intro: MNIST, CIFAR-10, CIFAR-100, STL-10, SVHN, ILSVRC2012 task 1\n- blog: [http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)\n\n**Object Recognition with Convolutional Neural Networks in the Keras Deep Learning Library**\n\n[http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/](http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/)\n\n**The Effect of Resolution on Deep Neural Network Image Classification Accuracy**\n\n![](https://cdn-images-1.medium.com/max/800/1*0SBEYOChCOMbqnGG34xSxw.png)\n\n[https://medium.com/the-downlinq/the-effect-of-resolution-on-deep-neural-network-image-classification-accuracy-d1338e2782c5#.em5rk991r](https://medium.com/the-downlinq/the-effect-of-resolution-on-deep-neural-network-image-classification-accuracy-d1338e2782c5#.em5rk991r)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","title":"Recommendation System"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Recommendation System\r\ndate: 2015-10-09\r\n---\r\n\r\n# Tutorials\r\n\r\n**Making a Contextual Recommendation Engine**\r\n\r\n- intro: by Muktabh Mayank\r\n- youtube: [https://www.youtube.com/watch?v=ToTyNF9kXkk&hd=1http://weibo.com/1402400261/profile?topnav=1&wvr=6](https://www.youtube.com/watch?v=ToTyNF9kXkk&hd=1http://weibo.com/1402400261/profile?topnav=1&wvr=6)\r\n- video: [http://pan.baidu.com/s/1eQFFVns](http://pan.baidu.com/s/1eQFFVns)\r\n\r\n# Papers\r\n\r\n**Collaborative Deep Learning for Recommender Systems**\r\n\r\n- arxiv: [https://arxiv.org/abs/1409.2944](https://arxiv.org/abs/1409.2944)\r\n- paper: [http://www.wanghao.in/paper/KDD15_CDL.pdf](http://www.wanghao.in/paper/KDD15_CDL.pdf)\r\n\r\n**Image-based recommendations on styles and substitutes**\r\n\r\n- paper: [http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf](http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf)\r\n- code: [http://cseweb.ucsd.edu/~jmcauley/code/imageGraph.tar.gz](http://cseweb.ucsd.edu/~jmcauley/code/imageGraph.tar.gz)\r\n- data: [http://jmcauley.ucsd.edu/data/amazon/](http://jmcauley.ucsd.edu/data/amazon/)\r\n\r\n**A Complex Network Approach for Collaborative Recommendation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1510.00585v1](http://arxiv.org/abs/1510.00585v1)\r\n\r\n**Session-based Recommendations with Recurrent Neural Networks**\r\n\r\n- intro: ICLR 2016\r\n- arxiv: [http://arxiv.org/abs/1511.06939](http://arxiv.org/abs/1511.06939)\r\n- github: [https://github.com/hidasib/GRU4Rec](https://github.com/hidasib/GRU4Rec)\r\n\r\n**Item2Vec: Neural Item Embedding for Collaborative Filtering**\r\n\r\n- arxiv: [https://arxiv.org/abs/1603.04259](https://arxiv.org/abs/1603.04259)\r\n\r\n**Wide & Deep Learning for Recommender Systems**\r\n\r\n- intro: Google Research\r\n- arxiv: [http://arxiv.org/abs/1606.07792](http://arxiv.org/abs/1606.07792)\r\n- blog: [https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html](https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)\r\n\r\n**Hybrid Recommender System based on Autoencoders**\r\n\r\n- arxiv: [https://arxiv.org/abs/1606.07659](https://arxiv.org/abs/1606.07659)\r\n- github: [https://github.com/fstrub95/Autoencoders_cf](https://github.com/fstrub95/Autoencoders_cf)\r\n- notes: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2016/06/hybrid-recommender-system-based-on-autoencoders.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/papernotes/2016/06/hybrid-recommender-system-based-on-autoencoders.md)\r\n\r\n**Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations**\r\n\r\n- paper: [https://alexiskz.files.wordpress.com/2016/06/feature-rnn-paper1.pdf](https://alexiskz.files.wordpress.com/2016/06/feature-rnn-paper1.pdf)\r\n\r\n**Collaborative Filtering with Recurrent Neural Networks**\r\n\r\n- keywords: LSTM, movie recommendation\r\n- arixv: [http://arxiv.org/abs/1608.07400](http://arxiv.org/abs/1608.07400)\r\n\r\n**Deep Neural Networks for YouTube Recommendations**\r\n\r\n- intro: RECSYS 2016. Google\r\n- paper: [http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf)\r\n- summary: [https://blog.acolyer.org/2016/09/19/deep-neural-networks-for-youtube-recommendations/](https://blog.acolyer.org/2016/09/19/deep-neural-networks-for-youtube-recommendations/)\r\n\r\n**Photo Filter Recommendation by Category-Aware Aesthetic Learning**\r\n\r\n- intro: Filter Aesthetic Comparison Dataset (FACD): 28,000 filtered images and 42,240 reliable image pairs with aesthetic comparison annotations\r\n- arxiv: [http://arxiv.org/abs/1608.05339](http://arxiv.org/abs/1608.05339)\r\n\r\n**Convolutional Matrix Factorization for Document Context-Aware Recommendation**\r\n\r\n![](http://dm.postech.ac.kr/~cartopy/img/ConvMF_PGM_CNN.png)\r\n\r\n- project page: [http://dm.postech.ac.kr/~cartopy/ConvMF/](http://dm.postech.ac.kr/~cartopy/ConvMF/)\r\n- paper: [http://dl.acm.org/citation.cfm?id=2959165](http://dl.acm.org/citation.cfm?id=2959165)\r\n\r\n**Deep learning for audio-based music recommendation**\r\n\r\n- slides: [https://docs.google.com/presentation/d/1CRSAs2WOKo5mFhh5Iu-xkDfyJsg_NDL1r5dRtj6_aHo/edit#slide=id.p](https://docs.google.com/presentation/d/1CRSAs2WOKo5mFhh5Iu-xkDfyJsg_NDL1r5dRtj6_aHo/edit#slide=id.p)\r\n- mirror: [https://pan.baidu.com/s/1o8NaMPs](https://pan.baidu.com/s/1o8NaMPs)\r\n\r\n**Ask the GRU: Multi-Task Learning for Deep Text Recommendations**\r\n\r\n- arxiv: [https://arxiv.org/abs/1609.02116](https://arxiv.org/abs/1609.02116)\r\n\r\n**Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks**\r\n\r\n- intro: NIPS 2016\r\n- arxiv: [https://arxiv.org/abs/1611.00454](https://arxiv.org/abs/1611.00454)\r\n\r\n**Recurrent Recommender Networks**\r\n\r\n- intro: University of Texas at Austin & Google Research & CMU & LinkedIn\r\n- paper: [http://alexbeutel.com/papers/rrn_wsdm2017.pdf](http://alexbeutel.com/papers/rrn_wsdm2017.pdf)\r\n\r\n**Deep Learning based Large Scale Visual Recommendation and Search for E-Commerce**\r\n\r\n- intro: Visnet. Flipkart's visual search and recommendation system\r\n- arxiv: [https://arxiv.org/abs/1703.02344](https://arxiv.org/abs/1703.02344)\r\n- github: [https://github.com/flipkart-incubator/fk-visual-search](https://github.com/flipkart-incubator/fk-visual-search)\r\n\r\n**What Your Image Reveals: Exploiting Visual Contents for Point-of-Interest Recommendation**\r\n\r\n- intro: Arizona State University & Michigan State University\r\n- intro: Point-of-Interest (POI)\r\n- paper: [http://www.public.asu.edu/~swang187/publications/VPOI.pdf](http://www.public.asu.edu/~swang187/publications/VPOI.pdf)\r\n\r\n**Recurrent Neural Networks with Top-k Gains for Session-based Recommendations**\r\n\r\n- intro: Gravity R&D & Telefonica Research\r\n- arxiv: [https://arxiv.org/abs/1706.03847](https://arxiv.org/abs/1706.03847)\r\n- github: [https://github.com/hidasib/GRU4Rec](https://github.com/hidasib/GRU4Rec)\r\n\r\n**On Sampling Strategies for Neural Network-based Collaborative Filtering**\r\n\r\n- intro: KDD 2017. University of California, Los Angeles & Yahoo! Research & Etsy Inc\r\n- arxiv: [https://arxiv.org/abs/1706.07881](https://arxiv.org/abs/1706.07881)\r\n\r\n**Deep Learning based Recommender System: A Survey and New Perspectives**\r\n\r\n- intro: University of New South Wales & Nanyang Technological University\r\n- arxiv: [https://arxiv.org/abs/1707.07435](https://arxiv.org/abs/1707.07435)\r\n\r\n**Training Deep AutoEncoders for Collaborative Filtering**\r\n\r\n- arxiv: [https://arxiv.org/abs/1708.01715](https://arxiv.org/abs/1708.01715)\r\n- github: [https://github.com/NVIDIA/DeepRecommender](https://github.com/NVIDIA/DeepRecommender)\r\n\r\n**Deep Collaborative Autoencoder for Recommender Systems: A Unified Framework for Explicit and Implicit Feedback**\r\n\r\n- intro: Zhejiang University\r\n- arxiv: [https://arxiv.org/abs/1712.09043](https://arxiv.org/abs/1712.09043)\r\n\r\n**Deep Reinforcement Learning for List-wise Recommendations**\r\n\r\n- intro: Michigan State University & Data Science Lab\r\n- arxiv: [https://arxiv.org/abs/1801.00209](https://arxiv.org/abs/1801.00209)\r\n\r\n**Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning**\r\n\r\n- intro: Michigan State University & JD.com\r\n- arxiv: [https://arxiv.org/abs/1802.06501](https://arxiv.org/abs/1802.06501)\r\n\r\n**Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding**\r\n\r\n- intro: WSDM 2018. Simon Fraser University\r\n- arxiv: [https://arxiv.org/abs/1809.07426](https://arxiv.org/abs/1809.07426)\r\n- github(Matlab+MatcConvNet): [https://github.com/graytowne/caser](https://github.com/graytowne/caser)\r\n\r\n# Slides\r\n\r\n**Deep learning for music recommendation**\r\n\r\n- sldies: [http://pan.baidu.com/s/1skriMJj](http://pan.baidu.com/s/1skriMJj)\r\n\r\n**Deep learning for music recommendation and generation**\r\n\r\n- slides: [https://docs.google.com/presentation/d/1AIotiiAp_528R90ll8j-Kc2EsRk2Oxc1poRgPXnEH8Y/edit](https://docs.google.com/presentation/d/1AIotiiAp_528R90ll8j-Kc2EsRk2Oxc1poRgPXnEH8Y/edit)\r\n- mirror: [https://pan.baidu.com/s/1czQQNO](https://pan.baidu.com/s/1czQQNO)\r\n\r\n# Blogs\r\n\r\n**Recommending music on Spotify with deep learning**\r\n\r\n[http://benanne.github.io/2014/08/05/spotify-cnns.html](http://benanne.github.io/2014/08/05/spotify-cnns.html)\r\n\r\n**Generating Recommendations at Amazon Scale with Apache Spark and Amazon DSSTNE**\r\n\r\n![](https://cdn.amazonblogs.com/bigdata_awsblog/images/DSSTNE_Image_1aa.PNG)\r\n\r\n[http://blogs.aws.amazon.com/bigdata/post/TxGEL8IJ0CAXTK/Generating-Recommendations-at-Amazon-Scale-with-Apache-Spark-and-Amazon-DSSTNE](http://blogs.aws.amazon.com/bigdata/post/TxGEL8IJ0CAXTK/Generating-Recommendations-at-Amazon-Scale-with-Apache-Spark-and-Amazon-DSSTNE)\r\n\r\n**Recommending movies with deep learning**\r\n\r\n- blog: [http://blog.richardweiss.org/2016/09/25/movie-embeddings.html](http://blog.richardweiss.org/2016/09/25/movie-embeddings.html)\r\n- ipn: [https://github.com/ririw/ririw.github.io/blob/master/assets/Recommending%20movies.ipynb](https://github.com/ririw/ririw.github.io/blob/master/assets/Recommending%20movies.ipynb)\r\n\r\n**Deep Learning Helps iHeartRadio Personalize Music Recommendations**\r\n\r\n- blog: [https://news.developer.nvidia.com/deep-learning-helps-iheartradio-personalize-music-recommendations/](https://news.developer.nvidia.com/deep-learning-helps-iheartradio-personalize-music-recommendations/)\r\n\r\n**Applying deep learning to Related Pins**\r\n\r\n- intro: Pinterest\r\n- blog: [https://engineering.pinterest.com/blog/applying-deep-learning-related-pins](https://engineering.pinterest.com/blog/applying-deep-learning-related-pins)\r\n\r\n**Recommendation System Algorithms: Main existing recommendation engines and how they work**\r\n\r\n[https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3](https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3)\r\n\r\n**Building a Music Recommender with Deep Learning**\r\n\r\n- intro: Music recommender using deep learning with Keras and TensorFlow\r\n- blog: [http://mattmurray.net/building-a-music-recommender-with-deep-learning/](http://mattmurray.net/building-a-music-recommender-with-deep-learning/)\r\n- github: [https://github.com/mattmurray/music_recommender](https://github.com/mattmurray/music_recommender)\r\n\r\n# Projects\r\n\r\n**NNRec: Neural models for Collaborative Filtering**\r\n\r\n- intro: Source code for, AutoRec, an autoencoder based model for collaborative filtering. \r\nThis package also includes implementation of RBM based collaborative filtering model(RBM-CF).\r\n- github: [https://github.com/mesuvash/NNRec](https://github.com/mesuvash/NNRec)\r\n\r\n**Deep learning recommend system with TensorFlow**\r\n\r\n- intro: a general project to walk through the proceses of using TensorFlow\r\n- github: [https://github.com/tobegit3hub/deep_recommend_system](https://github.com/tobegit3hub/deep_recommend_system)\r\n\r\n**Deep Learning Recommender System**\r\n\r\n- github: [https://github.com/freegraphics/MIDS](https://github.com/freegraphics/MIDS)\r\n\r\n**Keras Implementation of Recommender Systems**\r\n\r\n[https://github.com/sonyisme/keras-recommendation](https://github.com/sonyisme/keras-recommendation)\r\n\r\n# Videos\r\n\r\n**Deep Learning for Recommender Systems**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=KZ7bcfYGuxw](https://www.youtube.com/watch?v=KZ7bcfYGuxw)\r\n\r\n**Using MXNet for Recommendation Modeling at Scale**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=cftJAuwKWkA](https://www.youtube.com/watch?v=cftJAuwKWkA)\r\n- mirror: [https://pan.baidu.com/s/1kVsdrmR](https://pan.baidu.com/s/1kVsdrmR)\r\n\r\n# Resources\r\n\r\n**Recommender Systems with Deep Learning**\r\n\r\n[https://amundtveit.com/2016/11/20/recommender-systems-with-deep-learning/](https://amundtveit.com/2016/11/20/recommender-systems-with-deep-learning/)\r\n\r\n**Deep-Learning-for-Recommendation-Systems**\r\n\r\n- intro: This repository contains Deep Learning based articles , paper and repositories for Recommender Systems\r\n- github: [https://github.com/robi56/Deep-Learning-for-Recommendation-Systems](https://github.com/robi56/Deep-Learning-for-Recommendation-Systems)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","title":"Image Retrieval"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Image Retrieval\ndate: 2015-10-09\n---\n\n# Papers\n\n**Using Very Deep Autoencoders for Content-Based Image Retrieval**\n\n- intro: ESANN 2011. Alex Krizhevsky, and Geoffrey E. Hinton\n- paper: [https://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf](https://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf)\n- paper: [http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf](http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf)\n\n**Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data**\n\n- arxiv: [http://arxiv.org/abs/1312.4740](http://arxiv.org/abs/1312.4740)\n- paper: [http://legacy.openreview.net/document/90fc8dad-ad02-4ddc-ab06-e7b55706869d#90fc8dad-ad02-4ddc-ab06-e7b55706869d](http://legacy.openreview.net/document/90fc8dad-ad02-4ddc-ab06-e7b55706869d#90fc8dad-ad02-4ddc-ab06-e7b55706869d)\n\n**Neural Codes for Image Retrieval**\n\n![](http://sites.skoltech.ru/app/data/uploads/sites/25/2014/11/example-e1404721339557.png)\n\n- intro: ECCV 2014\n- project page: [http://sites.skoltech.ru/compvision/projects/neuralcodes/](http://sites.skoltech.ru/compvision/projects/neuralcodes/)\n- arxiv: [http://arxiv.org/abs/1404.1777](http://arxiv.org/abs/1404.1777)\n- github: [https://github.com/arbabenko/Spoc](https://github.com/arbabenko/Spoc)\n\n**Efficient On-the-fly Category Retrieval using ConvNets and GPUs**\n\n- arxiv: [http://arxiv.org/abs/1407.4764](http://arxiv.org/abs/1407.4764)\n\n**Learning visual similarity for product design with convolutional neural networks**\n\n- intro: SIGGRAPH 2015\n- paper: [http://www.cs.cornell.edu/~kb/publications/SIG15ProductNet.pdf](http://www.cs.cornell.edu/~kb/publications/SIG15ProductNet.pdf)\n- paper: [http://dl.acm.org.sci-hub.cc/citation.cfm?doid=2809654.2766959](http://dl.acm.org.sci-hub.cc/citation.cfm?doid=2809654.2766959)\n\n**Exploiting Local Features from Deep Networks for Image Retrieval**\n\n- intro: CVPR DeepVision Workshop 2015\n- arxiv: [https://arxiv.org/abs/1504.05133](https://arxiv.org/abs/1504.05133)\n\n**Visual Search at Pinterest**\n\n- intro: in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge and Discovery and Data Mining, 2015\n- arxiv: [http://arxiv.org/abs/1505.07647](http://arxiv.org/abs/1505.07647)\n- blog: [https://engineering.pinterest.com/blog/introducing-new-way-visually-search-pinterest](https://engineering.pinterest.com/blog/introducing-new-way-visually-search-pinterest)\n\n**Aggregating Deep Convolutional Features for Image Retrieval**\n\n- intro: ICCV 2015\n- intro: Sum pooing\n- arxiv: [http://arxiv.org/abs/1510.07493](http://arxiv.org/abs/1510.07493)\n\n**Particular object retrieval with integral max-pooling of CNN activations**\n\n- intro: use max-pooling to aggregate the deep descriptors, R-MAC (regional maximum activation of convolutions)\n- arxiv: [https://arxiv.org/abs/1511.05879](https://arxiv.org/abs/1511.05879)\n\n**Group Invariant Deep Representations for Image Instance Retrieval**\n\n- arxiv: [http://arxiv.org/abs/1601.02093](http://arxiv.org/abs/1601.02093)\n\n**Where to Buy It: Matching Street Clothing Photos in Online Shops**\n\n![](http://www.tamaraberg.com/street2shop/header.jpg)\n\n- intro: ICCV 2015\n- hmepage: [http://www.tamaraberg.com/street2shop/](http://www.tamaraberg.com/street2shop/)\n- paper: [http://www.tamaraberg.com/papers/street2shop.pdf](http://www.tamaraberg.com/papers/street2shop.pdf)\n- paper: [http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Kiapour_Where_to_Buy_ICCV_2015_paper.html](http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Kiapour_Where_to_Buy_ICCV_2015_paper.html)\n\n**Natural Language Object Retrieval**\n\n![](http://ronghanghu.com/wp-content/uploads/method-900x353.png)\n\n- intro: CVPR 2015\n- homepage: [http://ronghanghu.com/text_obj_retrieval/](http://ronghanghu.com/text_obj_retrieval/)\n- arxiv: [http://arxiv.org/abs/1511.04164](http://arxiv.org/abs/1511.04164)\n- slides: [http://ronghanghu.com/slides/cvpr16_text_obj_retrieval_slides.pdf](http://ronghanghu.com/slides/cvpr16_text_obj_retrieval_slides.pdf)\n- github: [https://github.com/ronghanghu/natural-language-object-retrieval](https://github.com/ronghanghu/natural-language-object-retrieval)\n- github: [https://github.com/andrewliao11/Natural-Language-Object-Retrieval-tensorflow](https://github.com/andrewliao11/Natural-Language-Object-Retrieval-tensorflow)\n\n**Deep Image Retrieval: Learning global representations for image search**\n\n- intro: ECCV 2016\n- project page: [http://www.xrce.xerox.com/Research-Development/Computer-Vision/Learning-Visual-Representations/Deep-Image-Retrieval](http://www.xrce.xerox.com/Research-Development/Computer-Vision/Learning-Visual-Representations/Deep-Image-Retrieval)\n- arxiv: [https://arxiv.org/abs/1604.01325](https://arxiv.org/abs/1604.01325)\n- slides: [http://www.slideshare.net/xavigiro/deep-image-retrieval-learning-global-representations-for-image-search](http://www.slideshare.net/xavigiro/deep-image-retrieval-learning-global-representations-for-image-search)\n\n**End-to-end Learning of Deep Visual Representations for Image Retrieval**\n\n- intro: IJCV 2017. Extended version of our ECCV2016 paper \"Deep Image Retrieval: Learning global representations for image search\"\n- project page: [http://www.xrce.xerox.com/Research-Development/Computer-Vision/Learning-Visual-Representations/Deep-Image-Retrieval](http://www.xrce.xerox.com/Research-Development/Computer-Vision/Learning-Visual-Representations/Deep-Image-Retrieval)\n- arxiv: [https://arxiv.org/abs/1610.07940](https://arxiv.org/abs/1610.07940)\n\n**Bags of Local Convolutional Features for Scalable Instance Search**\n\n- intro: ICMR 2016. Best Poster Award at ICMR 2016.\n- project page: [https://imatge-upc.github.io/retrieval-2016-icmr/](https://imatge-upc.github.io/retrieval-2016-icmr/)\n- arxiv: [https://arxiv.org/abs/1604.04653](https://arxiv.org/abs/1604.04653)\n- github: [https://github.com/imatge-upc/retrieval-2016-icmr](https://github.com/imatge-upc/retrieval-2016-icmr)\n- slides: [http://www.slideshare.net/xavigiro/convolutional-features-for-instance-search](http://www.slideshare.net/xavigiro/convolutional-features-for-instance-search)\n\n**Faster R-CNN Features for Instance Search**\n\n- intro: DeepVision Workshop in CVPR 2016\n- homepage: [http://imatge-upc.github.io/retrieval-2016-deepvision/](http://imatge-upc.github.io/retrieval-2016-deepvision/)\n- arxiv: [http://arxiv.org/abs/1604.08893](http://arxiv.org/abs/1604.08893)\n- github: [https://github.com/imatge-upc/retrieval-2016-deepvision](https://github.com/imatge-upc/retrieval-2016-deepvision)\n\n**Where to Focus: Query Adaptive Matching for Instance Retrieval Using Convolutional Feature Maps**\n\n- intro: query adaptive matching (QAM), Feature Map Pooling, Overlapped Spatial Pyramid Pooling (OSPP)\n- arxiv: [https://arxiv.org/abs/1606.06811](https://arxiv.org/abs/1606.06811)\n\n**Adversarial Training For Sketch Retrieval**\n\n- arxiv: [http://arxiv.org/abs/1607.02748](http://arxiv.org/abs/1607.02748)\n\n**Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks**\n\n- intro: CVPR 2016. DeepBit\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Learning_Compact_Binary_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Learning_Compact_Binary_CVPR_2016_paper.pdf)\n- github: [https://github.com/kevinlin311tw/cvpr16-deepbit](https://github.com/kevinlin311tw/cvpr16-deepbit)\n\n**Fast Training of Triplet-based Deep Binary Embedding Networks**\n\n- intro: CVPR 2016\n- arxiv: [https://arxiv.org/abs/1603.02844](https://arxiv.org/abs/1603.02844)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhuang_Fast_Training_of_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhuang_Fast_Training_of_CVPR_2016_paper.pdf)\n- bitbucket(official): [https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks](https://bitbucket.org/jingruixiaozhuang/fast-training-of-triplet-based-deep-binary-embedding-networks)\n\n**Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles**\n\n- intro: CVPR 2016\n- intro: vehicle re-identification, vehicle retrieval. coupled clusters loss\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Relative_Distance_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Relative_Distance_CVPR_2016_paper.pdf)\n\n**DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations**\n\n![](http://personal.ie.cuhk.edu.hk/~lz013/projects/deepfashion/intro.png)\n\n- intro: CVPR 2016. FashionNet\n- project page: [http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html](http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf)\n\n**CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples**\n\n![](http://ptak.felk.cvut.cz/personal/radenfil/siamac/siamac.png)\n\n- intro: ECCV 2016\n- project page(paper+code+data): [http://cmp.felk.cvut.cz/~radenfil/projects/siamac.html](http://cmp.felk.cvut.cz/~radenfil/projects/siamac.html)\n- arxiv: [https://arxiv.org/abs/1604.02426](https://arxiv.org/abs/1604.02426)\n- paper: [http://cmp.felk.cvut.cz/~radenfil/publications/Radenovic-ECCV16.pdf](http://cmp.felk.cvut.cz/~radenfil/publications/Radenovic-ECCV16.pdf)\n- code(Matlab): [http://ptak.felk.cvut.cz/personal/radenfil/siamac/siaMAC_code.tar.gz](http://ptak.felk.cvut.cz/personal/radenfil/siamac/siaMAC_code.tar.gz)\n\n**PicHunt: Social Media Image Retrieval for Improved Law Enforcement**\n\n- arxiv: [http://arxiv.org/abs/1608.00905](http://arxiv.org/abs/1608.00905)\n\n**SIFT Meets CNN: A Decade Survey of Instance Retrieval**\n\n- arxiv: [http://arxiv.org/abs/1608.01807](http://arxiv.org/abs/1608.01807)\n\n**The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies**\n\n- project page: [http://sketchy.eye.gatech.edu/](http://sketchy.eye.gatech.edu/)\n- paper: [http://www.cc.gatech.edu/~hays/tmp/sketchy-database.pdf](http://www.cc.gatech.edu/~hays/tmp/sketchy-database.pdf)\n- github: [https://github.com/janesjanes/sketchy](https://github.com/janesjanes/sketchy)\n\n**What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?**\n\n- arxiv: [https://arxiv.org/abs/1611.01640](https://arxiv.org/abs/1611.01640)\n\n**Image Retrieval with Deep Local Features and Attention-based Keypoints**\n\n- arxiv: [https://arxiv.org/abs/1612.05478](https://arxiv.org/abs/1612.05478)\n\n**Internet-Based Image Retrieval Using End-to-End Trained Deep Distributions**\n\n- arxiv: [https://arxiv.org/abs/1612.07697](https://arxiv.org/abs/1612.07697)\n\n**Compression of Deep Neural Networks for Image Instance Retrieval**\n\n- intro: DCC 2017\n- arxiv: [https://arxiv.org/abs/1701.04923](https://arxiv.org/abs/1701.04923)\n\n**Effective Multi-Query Expansions: Collaborative Deep Networks for Robust Landmark Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1701.05003](https://arxiv.org/abs/1701.05003)\n\n**Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1702.00338](https://arxiv.org/abs/1702.00338)\n\n**Deep Geometric Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1702.06383](https://arxiv.org/abs/1702.06383)\n\n**Context Aware Query Image Representation for Particular Object Retrieval**\n\n[https://www.arxiv.org/abs/1703.01226](https://www.arxiv.org/abs/1703.01226)\n\n**An End-to-End Approach to Natural Language Object Retrieval via Context-Aware Deep Reinforcement Learning**\n\n[https://arxiv.org/abs/1703.07579](https://arxiv.org/abs/1703.07579)\n\n**AMC: Attention guided Multi-modal Correlation Learning for Image Search**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.00763](https://arxiv.org/abs/1704.00763)\n- github: [https://github.com/kanchen-usc/amc_att](https://github.com/kanchen-usc/amc_att)\n\n**Video2Shop: Exactly Matching Clothes in Videos to Online Shopping Images**\n\n- intro: CVPR 2017\n- keywrods: AsymNet\n- arxiv: [https://arxiv.org/abs/1804.05287](https://arxiv.org/abs/1804.05287)\n\n**Deep image representations using caption generators**\n\n- intro: ICME 2017\n- arxiv: [https://arxiv.org/abs/1705.09142](https://arxiv.org/abs/1705.09142)\n\n**Visual Search at eBay**\n\n- intro: 23rd SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2017\n- arxiv: [https://arxiv.org/abs/1706.03154](https://arxiv.org/abs/1706.03154)\n\n**Sampling Matters in Deep Embedding Learning**\n\n- intro: UT Austin & A9/Amazon\n- keywords: distance weighted sampling\n- arxiv: [https://arxiv.org/abs/1706.07567](https://arxiv.org/abs/1706.07567)\n\n**One-Shot Fine-Grained Instance Retrieval**\n\n- intro: ACM MM 2017\n- arxiv: [https://arxiv.org/abs/1707.00811](https://arxiv.org/abs/1707.00811)\n\n**Selective Deep Convolutional Features for Image Retrieval**\n\n- intro: ACM MM 2017\n- arxiv: [https://arxiv.org/abs/1707.00809](https://arxiv.org/abs/1707.00809)\n\n**Class-Weighted Convolutional Features for Visual Instance Search**\n\n- intro: BMVC 2017. Universitat Politecnica de Catalunya Barcelona & CSIRO\n- project page: [http://imatge-upc.github.io/retrieval-2017-cam/](http://imatge-upc.github.io/retrieval-2017-cam/)\n- arxiv: [https://arxiv.org/abs/1707.02581](https://arxiv.org/abs/1707.02581)\n- github: [https://github.com/imatge-upc/retrieval-2017-cam](https://github.com/imatge-upc/retrieval-2017-cam)\n\n**Learning a Repression Network for Precise Vehicle Search**\n\n[https://arxiv.org/abs/1708.02386](https://arxiv.org/abs/1708.02386)\n\n**SUBIC: A supervised, structured binary code for image search**\n\n- intro: ICCV 2017 (Spotlight). Technicolor & INRIA Rennes & Amazon\n- arxiv: [https://arxiv.org/abs/1708.02932](https://arxiv.org/abs/1708.02932)\n\n**Pruning Convolutional Neural Networks for Image Instance Retrieval**\n\n[https://arxiv.org/abs/1707.05455](https://arxiv.org/abs/1707.05455)\n\n**Image2song: Song Retrieval via Bridging Image Content and Lyric Words**\n\n- intro: ICCV 2017. Chinese Academy of Sciences & Northwestern Polytechnical University\n- arxiv: [https://arxiv.org/abs/1708.05851](https://arxiv.org/abs/1708.05851)\n\n**Region-Based Image Retrieval Revisited**\n\n- intro: ACM Multimedia 2017 (Oral)\n- arxiv: [https://arxiv.org/abs/1709.09106](https://arxiv.org/abs/1709.09106)\n\n**Beyond Part Models: Person Retrieval with Refined Part Pooling**\n\n[https://arxiv.org/abs/1711.09349](https://arxiv.org/abs/1711.09349)\n\n**Query-Adaptive R-CNN for Open-Vocabulary Object Detection and Retrieval**\n\n[https://arxiv.org/abs/1711.09509](https://arxiv.org/abs/1711.09509)\n\n**Saliency Weighted Convolutional Features for Instance Search**\n\n- intro: Dublin City University & Universitat Politecnica de Catalunya\n- keywords: local convolutional features (BLCF), human visual attention models (saliency)\n- project page: [https://imatge-upc.github.io/salbow/](https://imatge-upc.github.io/salbow/)\n- arxiv: [https://arxiv.org/abs/1711.10795](https://arxiv.org/abs/1711.10795)\n- github: [https://arxiv.org/abs/1711.10795](https://arxiv.org/abs/1711.10795)\n\n**DeepStyle: Multimodal Search Engine for Fashion and Interior Design**\n\n[https://arxiv.org/abs/1801.03002](https://arxiv.org/abs/1801.03002)\n\n**From Selective Deep Convolutional Features to Compact Binary Representations for Image Retrieval**\n\n[https://arxiv.org/abs/1802.02899](https://arxiv.org/abs/1802.02899)\n\n**Web-Scale Responsive Visual Search at Bing**\n\n- intro: Microsoft\n- arxiv: [https://arxiv.org/abs/1802.04914](https://arxiv.org/abs/1802.04914)\n\n**Approximate Query Matching for Image Retrieval**\n\n[https://arxiv.org/abs/1803.05401](https://arxiv.org/abs/1803.05401)\n\n**Object Captioning and Retrieval with Natural Language**\n\n[https://arxiv.org/abs/1803.06152](https://arxiv.org/abs/1803.06152)\n\n**Triplet-Center Loss for Multi-View 3D Object Retrieval**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.06189](https://arxiv.org/abs/1803.06189)\n\n**Collaborative Multi-modal deep learning for the personalized product retrieval in Facebook Marketplace**\n\n- intro: Facebook\n= arxiv: [https://arxiv.org/abs/1805.12312](https://arxiv.org/abs/1805.12312)\n\n**DeepFirearm: Learning Discriminative Feature Representation for Fine-grained Firearm Retrieval**\n\n- intro: ICPR 2018\n- arxiv: [https://arxiv.org/abs/1806.02984](https://arxiv.org/abs/1806.02984)\n- github: [https://github.com/jdhao/deep_firearm](https://github.com/jdhao/deep_firearm)\n\n**Instance Search via Instance Level Segmentation and Feature Representation**\n\n[https://arxiv.org/abs/1806.03576](https://arxiv.org/abs/1806.03576)\n\n**Deep Feature Aggregation with Heat Diffusion for Image Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1805.08587](https://arxiv.org/abs/1805.08587)\n- github: [https://github.com/pangsm0415/HeW](https://github.com/pangsm0415/HeW)\n\n**Single Shot Scene Text Retrieval**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.09044](https://arxiv.org/abs/1808.09044)\n\n**Learning Embeddings for Product Visual Search with Triplet Loss and Online Sampling**\n\n- intro: Yahoo Research\n- arxiv: [https://arxiv.org/abs/1810.04652](https://arxiv.org/abs/1810.04652)\n\n**Attention-aware Generalized Mean Pooling for Image Retrieval**\n\n[https://arxiv.org/abs/1811.00202](https://arxiv.org/abs/1811.00202)\n\n**Hierarchy-based Image Embeddings for Semantic Image Retrieval**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1809.09924](https://arxiv.org/abs/1809.09924)\n- github: [https://github.com/cvjena/semantic-embeddings](https://github.com/cvjena/semantic-embeddings)\n\n**Mean Local Group Average Precision (mLGAP): A New Performance Metric for Hashing-based Retrieval**\n\n[https://arxiv.org/abs/1811.09763](https://arxiv.org/abs/1811.09763)\n\n**Instance-level Sketch-based Retrieval by Deep Triplet Classification Siamese Network**\n\n[https://arxiv.org/abs/1811.11375](https://arxiv.org/abs/1811.11375)\n\n**Detect-to-Retrieve: Efficient Regional Aggregation for Image Search**\n\n- intro: University of Cambridge & Google AI\n- arxiv: [https://arxiv.org/abs/1812.01584](https://arxiv.org/abs/1812.01584)\n\n**Learning with Average Precision: Training Image Retrieval with a Listwise Loss**\n\n- intro: NAVER LABS Europe\n- arxiv: [https://arxiv.org/abs/1906.07589](https://arxiv.org/abs/1906.07589)\n\n**A Benchmark on Tricks for Large-scale Image Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1907.11854](https://arxiv.org/abs/1907.11854)\n\n**Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.12163](https://arxiv.org/abs/2007.12163)\n\n**Keypoint-Aligned Embeddings for Image Retrieval and Re-identification**\n\n- intro: WACV 2021\n- arxiv: [https://arxiv.org/abs/2008.11368](https://arxiv.org/abs/2008.11368)\n\n**Tasks Integrated Networks: Joint Detection and Retrieval for Image Search**\n\n[https://arxiv.org/abs/2009.01438](https://arxiv.org/abs/2009.01438)\n\n**Instance-level Image Retrieval using Reranking Transformers**\n\n- intro: University of Virginia & eBay Computer Vision\n- arxiv: [https://arxiv.org/abs/2103.12236](https://arxiv.org/abs/2103.12236)\n\n# Hashing\n\n**Supervised Hashing for Image Retrieval via Image Representation Learning**\n\n- intro: AAAI 2014. Sun Yat-Sen University & National University of Singapore\n- keywords: CNNH (Convolutional Neural Network Hashing)\n- paper: [www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8137/8861](www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/download/8137/8861)\n- slides: [https://pdfs.semanticscholar.org/f633/8f23860f9c4808586bbc7e8907d33836147f.pdf](https://pdfs.semanticscholar.org/f633/8f23860f9c4808586bbc7e8907d33836147f.pdf)\n\n**Simultaneous Feature Learning and Hash Coding with Deep Neural Networks**\n\n- intro: CVPR 2015. Sun Yat-Sen University & National University of Singapore\n- keywords: NINH (NIN Hashing), DNNH (Deep Neural Network Hashing)\n- arxiv: [https://arxiv.org/abs/1504.03410](https://arxiv.org/abs/1504.03410)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lai_Simultaneous_Feature_Learning_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lai_Simultaneous_Feature_Learning_2015_CVPR_paper.pdf)\n\n**Hashing by Deep Learning**\n\n- intro: IBM T. J. Watson Research Center\n- paper: [http://www.ee.columbia.edu/~wliu/WeiLiu_DLHash.pdf](http://www.ee.columbia.edu/~wliu/WeiLiu_DLHash.pdf)\n\n**Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval**\n\n- intro: CVPR 2015. DSRH (Deep Semantic Ranking Hashing)\n- arxiv: [http://arxiv.org/abs/1501.06272](http://arxiv.org/abs/1501.06272)\n\n**Deep Learning of Binary Hash Codes for Fast Image Retrieval**\n\n- intro: CVPR Workshop 2015\n- keywords: MNIST, CIFAR-10, Yahoo-1M. DLBHC (Deep Learning of Binary Hash Codes)\n- paper: [http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvprw15.pdf](http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvprw15.pdf)\n- github: [https://github.com/kevinlin311tw/caffe-cvprw15](https://github.com/kevinlin311tw/caffe-cvprw15)\n\n**Supervised Learning of Semantics-Preserving Hashing via Deep Neural Networks for Large-Scale Image Search**\n\n- intro: SSDH\n- arxiv: [http://arxiv.org/abs/1507.00101](http://arxiv.org/abs/1507.00101)\n- github: [https://github.com/kevinlin311tw/Caffe-DeepBinaryCode](https://github.com/kevinlin311tw/Caffe-DeepBinaryCode)\n\n**Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image Retrieval and Person Re-identification**\n\n- intro: IEEE Transactions on Image Processing 2015\n- keywords: DRSCH (Deep Regularized Similarity Comparison Hashing)\n- project page: [http://vision.sysu.edu.cn/projects/deephashing/](http://vision.sysu.edu.cn/projects/deephashing/)\n- arxiv: [https://arxiv.org/abs/1508.04535](https://arxiv.org/abs/1508.04535)\n- github: [https://github.com/ruixuejianfei/BitScalableDeepHash](https://github.com/ruixuejianfei/BitScalableDeepHash)\n\n**Deep Supervised Hashing for Fast Image Retrieval**\n\n- intro: CVPR 2016\n- keywords: DSH (Deep Supervised Hashing)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.pdf)\n- paper: [http://www.jdl.ac.cn/doc/2011/201711214443668218_deep%20supervised%20hashing%20for%20fast%20image%20retrieval_cvpr2016.pdf](http://www.jdl.ac.cn/doc/2011/201711214443668218_deep%20supervised%20hashing%20for%20fast%20image%20retrieval_cvpr2016.pdf)\n- github: [https://github.com/lhmRyan/deep-supervised-hashing-DSH](https://github.com/lhmRyan/deep-supervised-hashing-DSH)\n\n**Deep Hashing Network for Efficient Similarity Retrieval**\n\n- intro: AAAI 2016\n- paper: [http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12039](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12039)\n\n**Feature Learning based Deep Supervised Hashing with Pairwise Labels**\n\n- intro: IJCAI 2016\n- arxiv: [https://arxiv.org/abs/1511.03855](https://arxiv.org/abs/1511.03855)\n- paper: [https://www.ijcai.org/Proceedings/16/Papers/245.pdf](https://www.ijcai.org/Proceedings/16/Papers/245.pdf)\n- paper: [https://cs.nju.edu.cn/lwj/paper/IJCAI16_DPSH.pdf](https://cs.nju.edu.cn/lwj/paper/IJCAI16_DPSH.pdf)\n- code: [http://cs.nju.edu.cn/lwj/code/DPSH_code.rar](http://cs.nju.edu.cn/lwj/code/DPSH_code.rar)\n\n**Deep Cross-Modal Hashing**\n\n[https://arxiv.org/abs/1602.02255](https://arxiv.org/abs/1602.02255)\n\n**Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval**\n\n[https://arxiv.org/abs/1804.11013](https://arxiv.org/abs/1804.11013)\n\n**SSDH: Semi-supervised Deep Hashing for Large Scale Image Retrieval**\n\n- arxiv: [http://arxiv.org/abs/1607.08477](http://arxiv.org/abs/1607.08477)\n\n**Deep Semantic-Preserving and Ranking-Based Hashing for Image Retrieval**\n\n- intro: Microsoft\n- paper: [http://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/Deep-Semantic-Preserving-and-Ranking-Based-Hashing-for-Image-Retrieval.pdf](http://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/Deep-Semantic-Preserving-and-Ranking-Based-Hashing-for-Image-Retrieval.pdf)\n\n**Deep Hashing: A Joint Approach for Image Signature Learning**\n\n- arxiv: [http://arxiv.org/abs/1608.03658](http://arxiv.org/abs/1608.03658)\n\n**Transitive Hashing Network for Heterogeneous Multimedia Retrieval**\n\n- intro: state of the art on NUS-WIDE, ImageNet-YahooQA\n- arxiv: [http://arxiv.org/abs/1608.04307](http://arxiv.org/abs/1608.04307)\n\n**Deep Residual Hashing**\n\n- arxiv: [https://arxiv.org/abs/1612.05400](https://arxiv.org/abs/1612.05400)\n\n**Deep Region Hashing for Efficient Large-scale Instance Search from Images**\n\n- intro: Columbia University & University of Electronic Science and Technology of China\n- arxiv: [https://arxiv.org/abs/1701.07901](https://arxiv.org/abs/1701.07901)\n\n**HashNet: Deep Learning to Hash by Continuation**\n\n- intro: ICCV 2017. Tsinghua University\n- arxiv: [https://arxiv.org/abs/1702.00758](https://arxiv.org/abs/1702.00758)\n- github: [https://github.com/thuml/HashNet](https://github.com/thuml/HashNet)\n\n**Unsupervised Triplet Hashing for Fast Image Retrieval**\n\n- arxiv: [https://www.arxiv.org/abs/1702.08798](https://www.arxiv.org/abs/1702.08798)\n\n**Deep Sketch Hashing: Fast Free-hand Sketch-Based Image Retrieval**\n\n- intro: CVPR 2017 spotlight paper\n- arxiv: [https://arxiv.org/abs/1703.05605](https://arxiv.org/abs/1703.05605)\n\n**Learning Robust Hash Codes for Multiple Instance Image Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1703.05724](https://arxiv.org/abs/1703.05724)\n\n**Simultaneous Feature Aggregating and Hashing for Large-scale Image Search**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.00860](https://arxiv.org/abs/1704.00860)\n\n**Learning to Hash**\n\n- blog: [https://cs.nju.edu.cn/lwj/L2H.html](https://cs.nju.edu.cn/lwj/L2H.html)\n\n**Hashing as Tie-Aware Learning to Rank**\n\n[https://arxiv.org/abs/1705.08562](https://arxiv.org/abs/1705.08562)\n\n**Deep Hashing Network for Unsupervised Domain Adaptation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1706.07522](https://arxiv.org/abs/1706.07522)\n- github(MatConvNet): [https://github.com/hemanthdv/da-hash](https://github.com/hemanthdv/da-hash)\n\n**Deep Binary Reconstruction for Cross-modal Hashing**\n\n- intro: ACM Multimedia 2017\n- arxiv: [https://arxiv.org/abs/1708.05127](https://arxiv.org/abs/1708.05127)\n\n**A Revisit on Deep Hashings for Large-scale Content Based Image Retrieval**\n\n- intro: Zhejiang University\n- arixv: [https://arxiv.org/abs/1711.06016](https://arxiv.org/abs/1711.06016)\n\n**The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching**\n\n- keywords: finegrained sketch-based image retrieval (FG-SBIR) and Person Re-identification (person ReID)\n- arxiv: [https://arxiv.org/abs/1711.08106](https://arxiv.org/abs/1711.08106)\n\n**ForestHash: Semantic Hashing With Shallow Random Forests and Tiny Convolutional Networks**\n\n[https://arxiv.org/abs/1711.08364](https://arxiv.org/abs/1711.08364)\n\n**Supervised Hashing with End-to-End Binary Deep Neural Network**\n\n[https://arxiv.org/abs/1711.08901](https://arxiv.org/abs/1711.08901)\n\n**Transfer Adversarial Hashing for Hamming Space Retrieval**\n\n[https://arxiv.org/abs/1712.04616](https://arxiv.org/abs/1712.04616)\n\n**Dual Asymmetric Deep Hashing Learning**\n\n[https://arxiv.org/abs/1801.08360](https://arxiv.org/abs/1801.08360)\n\n**Attribute-Guided Network for Cross-Modal Zero-Shot Hashing**\n\n[https://arxiv.org/abs/1802.01943](https://arxiv.org/abs/1802.01943)\n\n**Deep Reinforcement Learning for Image Hashing**\n\n[https://arxiv.org/abs/1802.02904](https://arxiv.org/abs/1802.02904)\n\n**Hashing with Mutual Information**\n\n[https://arxiv.org/abs/1803.00974](https://arxiv.org/abs/1803.00974)\n\n**Zero-Shot Sketch-Image Hashing**\n\n- intro: CVPR 2018 spotlight\n- arxiv: [https://arxiv.org/abs/1803.02284](https://arxiv.org/abs/1803.02284)\n\n**Instance Similarity Deep Hashing for Multi-Label Image Retrieval**\n\n[https://arxiv.org/abs/1803.02987](https://arxiv.org/abs/1803.02987)\n\n**Deep Class-Wise Hashing: Semantics-Preserving Hashing via Class-wise Loss**\n\n- intro: City University of Hong Kong\n- arxiv: [https://arxiv.org/abs/1803.04137](https://arxiv.org/abs/1803.04137)\n\n**Unsupervised Semantic Deep Hashing**\n\n[https://arxiv.org/abs/1803.06911](https://arxiv.org/abs/1803.06911)\n\n**SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.01401](https://arxiv.org/abs/1804.01401)\n\n**Improving Deep Binary Embedding Networks by Order-aware Reweighting of Triplets**\n\n- intro: Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/1804.06061](https://arxiv.org/abs/1804.06061)\n\n**Deep Semantic Hashing with Generative Adversarial Networks**\n\n- intro: SIGIR 2017 Oral\n- arxiv: [https://arxiv.org/abs/1804.08275](https://arxiv.org/abs/1804.08275)\n\n**Deep Ordinal Hashing with Spatial Attention**\n\n[https://arxiv.org/abs/1805.02459](https://arxiv.org/abs/1805.02459)\n\n**Efficient end-to-end learning for quantizable representations**\n\n- intro: ICML 2018. Seoul National University\n- arxiv: [https://arxiv.org/abs/1805.05809](https://arxiv.org/abs/1805.05809)\n- github: [https://github.com/maestrojeong/Deep-Hash-Table-ICML18](https://github.com/maestrojeong/Deep-Hash-Table-ICML18)\n\n**Unsupervised Deep Image Hashing through Tag Embeddings**\n\n[https://arxiv.org/abs/1806.05804](https://arxiv.org/abs/1806.05804)\n\n**Adversarial Learning for Fine-grained Image Search**\n\n[https://arxiv.org/abs/1807.02247](https://arxiv.org/abs/1807.02247)\n\n**Error Correction Maximization for Deep Image Hashing**\n\n[https://arxiv.org/abs/1808.01942](https://arxiv.org/abs/1808.01942)\n\n**Deep Priority Hashing**\n\n- intro: ACM MM 2018 Poster\n- arxiv: [https://arxiv.org/abs/1809.01238](https://arxiv.org/abs/1809.01238)\n\n**Neurons Merging Layer: Towards Progressive Redundancy Reduction for Deep Supervised Hashing**\n\n[https://arxiv.org/abs/1809.02302](https://arxiv.org/abs/1809.02302)\n\n**Deep LDA Hashing**\n\n[https://arxiv.org/abs/1810.03402](https://arxiv.org/abs/1810.03402)\n\n**Deep Triplet Quantization**\n\n- intro: ACM Multimedia 2018 oral\n- arxiv: [https://arxiv.org/abs/1902.00153](https://arxiv.org/abs/1902.00153)\n\n**SADIH: Semantic-Aware DIscrete Hashing**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1904.01739](https://arxiv.org/abs/1904.01739)\n\n**Feature Pyramid Hashing**\n\n[https://arxiv.org/abs/1904.02325](https://arxiv.org/abs/1904.02325)\n\n**Global Hashing System for Fast Image Search**\n\n[https://arxiv.org/abs/1904.08685](https://arxiv.org/abs/1904.08685)\n\n**Self-Distilled Hashing for Deep Image Retrieval**\n\n- intro: Seoul National University & NAVER/LINE Vision\n- arxiv: [https://arxiv.org/abs/2112.08816](https://arxiv.org/abs/2112.08816)\n\n# Cross Modal Retrieval\n\n**Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network**\n\n- intro: ICCV 2015\n- intro: DARN, cross-entropy loss, triplet loss\n- arxiv: [http://arxiv.org/abs/1505.07922](http://arxiv.org/abs/1505.07922)\n\n**Deep Learning for Content-Based, Cross-Modal Retrieval of Videos and Music**\n\n- arxiv: [https://arxiv.org/abs/1704.06761](https://arxiv.org/abs/1704.06761)\n- supplementary: [https://youtu.be/ZyINqDMo3Fg](https://youtu.be/ZyINqDMo3Fg)\n\n**Deep Binaries: Encoding Semantic-Rich Cues for Efficient Textual-Visual Cross Retrieval**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.02531](https://arxiv.org/abs/1708.02531)\n\n**MHTN: Modal-adversarial Hybrid Transfer Network for Cross-modal Retrieval**\n\n[https://arxiv.org/abs/1708.04308](https://arxiv.org/abs/1708.04308)\n\n**Cross-Domain Image Retrieval with Attention Modeling**\n\n[https://arxiv.org/abs/1709.01784](https://arxiv.org/abs/1709.01784)\n\n**Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models**\n\n[https://arxiv.org/abs/1711.06420](https://arxiv.org/abs/1711.06420)\n\n**HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal Retrieval**\n\n[https://arxiv.org/abs/1711.09347](https://arxiv.org/abs/1711.09347)\n\n**Objects that Sound**\n\n- intro: DeepMind, VGG\n- arxiv: [https://arxiv.org/abs/1712.06651](https://arxiv.org/abs/1712.06651)\n\n**Cross-modal Embeddings for Video and Audio Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1801.02200](https://arxiv.org/abs/1801.02200)\n- github: [https://github.com/surisdi/youtube-8m](https://github.com/surisdi/youtube-8m)\n\n**Learnable PINs: Cross-Modal Embeddings for Person Identity**\n\n- intro: VGG\n- arxiv: [https://arxiv.org/abs/1805.00833](https://arxiv.org/abs/1805.00833)\n\n**Revisiting Cross Modal Retrieval**\n\n- intro: ECCVW (MULA 2018)\n- arxiv: [https://arxiv.org/abs/1807.07364](https://arxiv.org/abs/1807.07364)\n\n## Projects\n\n**HABIR哈希图像检索工具箱**\n\n- intro: Various hashing methods for image retrieval and serves as the baselines\n- blog: [http://yongyuan.name/habir/](http://yongyuan.name/habir/)\n- github: [https://github.com/willard-yuan/hashing-baseline-for-image-retrieval](https://github.com/willard-yuan/hashing-baseline-for-image-retrieval)\n\n# Video Indexing / Retrieval\n\n**Face Video Retrieval via Deep Learning of Binary Hash Representations**\n\n- paper: [https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11893/12117](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11893/12117)\n\n**Deep Learning Based Semantic Video Indexing and Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1601.07754](https://arxiv.org/abs/1601.07754)\n\n**Learning Joint Representations of Videos and Sentences with Web Image Search**\n\n- intro: 4th Workshop on Web-scale Vision and Social Media (VSM), ECCV 2016\n- arxiv: [http://arxiv.org/abs/1608.02367](http://arxiv.org/abs/1608.02367)\n\n**Multi-View Product Image Search Using ConvNets Features**\n\n- arxiv: [http://arxiv.org/abs/1608.03462](http://arxiv.org/abs/1608.03462)\n\n**Generalisation and Sharing in Triplet Convnets for Sketch based Visual Search**\n\n- arxiv: [https://arxiv.org/abs/1611.05301](https://arxiv.org/abs/1611.05301)\n\n**Binary Subspace Coding for Query-by-Image Video Retrieval**\n\n- arxiv: [https://arxiv.org/abs/1612.01657](https://arxiv.org/abs/1612.01657)\n\n**Action Search: Learning to Search for Human Activities in Untrimmed Videos**\n\n[https://arxiv.org/abs/1706.04269](https://arxiv.org/abs/1706.04269)\n\n**Deep Supervised Hashing with Triplet Labels**\n\n- intro: ACCV 2016\n- arxiv: [https://arxiv.org/abs/1612.03900](https://arxiv.org/abs/1612.03900)\n\n**Supervised Deep Hashing for Hierarchical Labeled Data**\n\n- arxiv: [https://arxiv.org/abs/1704.02088](https://arxiv.org/abs/1704.02088)\n\n**Localizing Moments in Video with Natural Language**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.01641](https://arxiv.org/abs/1708.01641)\n\n**Dress like a Star: Retrieving Fashion Products from Videos**\n\n- intro: Aston University\n- arxiv: [https://arxiv.org/abs/1710.07198](https://arxiv.org/abs/1710.07198)\n\n**Deep Hashing with Category Mask for Fast Video Retrieval**\n\n[https://arxiv.org/abs/1712.08315](https://arxiv.org/abs/1712.08315)\n\n**Focus: Querying Large Video Datasets with Low Latency and Low Cost**\n\n[https://arxiv.org/abs/1801.03493](https://arxiv.org/abs/1801.03493)\n\n**Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning**\n\n- intro: Boston University, University of British Columbia\n- arxiv: [https://arxiv.org/abs/1804.05113](https://arxiv.org/abs/1804.05113)\n\n# Learning to Rank\n\n**Simple to Complex Cross-modal Learning to Rank**\n\n- intro: Xi’an Jiaotong University & University of Technology Sydney & National University of Singapore & CMU\n- arxiv: [https://arxiv.org/abs/1702.01229](https://arxiv.org/abs/1702.01229)\n\n**SoDeep: a Sorting Deep net to learn ranking loss surrogates**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.04272](https://arxiv.org/abs/1904.04272)\n- github: [https://github.com/technicolor-research/sodeep](https://github.com/technicolor-research/sodeep)\n\n# Deep Metric Learning\n\n**Deep metric learning using Triplet network**\n\n- arxiv: [https://arxiv.org/abs/1412.6622](https://arxiv.org/abs/1412.6622)\n- slides: [http://tce.technion.ac.il/wp-content/uploads/sites/8/2016/01/Elad-Hofer.pdf](http://tce.technion.ac.il/wp-content/uploads/sites/8/2016/01/Elad-Hofer.pdf)\n- github: [https://github.com/eladhoffer/TripletNet](https://github.com/eladhoffer/TripletNet)\n\n**Improved Deep Metric Learning with Multi-class N-pair Loss Objective**\n\n- intro: NIPS 2016\n- arxiv: [http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf](http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf)\n\n**Metric Learning with Adaptive Density Discrimination**\n\n- intro: ICLR 2016. Facebook AI Research & UC Berkeley\n- arxiv: [https://arxiv.org/abs/1511.05939](https://arxiv.org/abs/1511.05939)\n- github: [https://github.com/pumpikano/tf-magnet-loss](https://github.com/pumpikano/tf-magnet-loss)\n- github: [https://github.com/vithursant/MagnetLoss-PyTorch/](https://github.com/vithursant/MagnetLoss-PyTorch/)\n\n**Hard-Aware Deeply Cascaded Embedding**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1611.05720](https://arxiv.org/abs/1611.05720)\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf)\n- github: [https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaded-Embedding_release](https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaded-Embedding_release)\n- github: [https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaed-Embedding](https://github.com/PkuRainBow/Hard-Aware-Deeply-Cascaed-Embedding)\n\n**Learnable Structured Clustering Framework for Deep Metric Learning**\n\n- arxiv: [https://arxiv.org/abs/1612.01213](https://arxiv.org/abs/1612.01213)\n\n**Deep Metric Learning via Lifted Structured Feature Embedding**\n\n- intro: CVPR 2016\n- project page(code+data): [http://cvgl.stanford.edu/projects/lifted_struct/](http://cvgl.stanford.edu/projects/lifted_struct/)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Song_Deep_Metric_Learning_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Song_Deep_Metric_Learning_CVPR_2016_paper.pdf)\n- paper: [http://cvgl.stanford.edu/papers/song_cvpr16.pdf](http://cvgl.stanford.edu/papers/song_cvpr16.pdf)\n- github: [https://github.com/rksltnl/Deep-Metric-Learning-CVPR16](https://github.com/rksltnl/Deep-Metric-Learning-CVPR16)\n- github: [https://github.com/rksltnl/Caffe-Deep-Metric-Learning-CVPR16](https://github.com/rksltnl/Caffe-Deep-Metric-Learning-CVPR16)\n- dataset: [ftp://cs.stanford.edu/cs/cvgl/Stanford_Online_Products.zip](ftp://cs.stanford.edu/cs/cvgl/Stanford_Online_Products.zip)\n\n**Cross-modal Deep Metric Learning with Multi-task Regularization**\n\n- intro: ICME 2017\n- arxiv: [https://arxiv.org/abs/1703.07026](https://arxiv.org/abs/1703.07026)\n\n**Smart Mining for Deep Metric Learning**\n\n[https://arxiv.org/abs/1704.01285](https://arxiv.org/abs/1704.01285)\n\n**DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer**\n\n- intro: TuSimple\n- keywords: pedestrian re-identification\n- arxiv: [https://arxiv.org/abs/1707.01220](https://arxiv.org/abs/1707.01220)\n\n**Deep Metric Learning with Angular Loss**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.01682](https://arxiv.org/abs/1708.01682)\n\n**Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly**\n\n[https://arxiv.org/abs/1801.04815](https://arxiv.org/abs/1801.04815)\n\n**Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval**\n\n[https://arxiv.org/abs/1802.09662](https://arxiv.org/abs/1802.09662)\n\n**Generalization in Metric Learning: Should the Embedding Layer be the Embedding Layer?**\n\n- intro: Georgia Tech\n- keywords: Cars-196, CUB-200-2011 and Stanford Online Product\n- arxiv: [https://arxiv.org/abs/1803.03310](https://arxiv.org/abs/1803.03310)\n\n**Deep Metric Learning**\n\n- github(PyTorch): [https://github.com/bnulihaixia/Deep_metric](https://github.com/bnulihaixia/Deep_metric)\n\n**Attention-based Ensemble for Deep Metric Learning**\n\n[https://arxiv.org/abs/1804.00382](https://arxiv.org/abs/1804.00382)\n\n**Online Deep Metric Learning**\n\n[https://arxiv.org/abs/1805.05510](https://arxiv.org/abs/1805.05510)\n\n**Deep Randomized Ensembles for Metric Learning**\n\n- arxiv: [https://arxiv.org/abs/1808.04469](https://arxiv.org/abs/1808.04469)\n- github: [https://github.com/littleredxh/DREML](https://github.com/littleredxh/DREML)\n\n**Deep Metric Learning with Hierarchical Triplet Loss**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1810.06951](https://arxiv.org/abs/1810.06951)\n\n**Ranked List Loss for Deep Metric Learning**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.03238](https://arxiv.org/abs/1903.03238)\n\n**Hardness-Aware Deep Metric Learning**\n\n- intro: CVPR 2019 Oral\n- arxiv: [https://arxiv.org/abs/1903.05503](https://arxiv.org/abs/1903.05503)\n- github(official, Tensorflow): [https://github.com/wzzheng/HDML](https://github.com/wzzheng/HDML)\n\n**Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.02616](https://arxiv.org/abs/1904.02616)\n\n**Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning**\n\n- arxiv: [https://arxiv.org/abs/1904.06627](https://arxiv.org/abs/1904.06627)\n- github: [https://github.com/MalongTech/research-ms-loss](https://github.com/MalongTech/research-ms-loss)\n\n**Deep Metric Learning Beyond Binary Supervision**\n\n- intro: CVPR 2019 oral\n- arxiv: [https://arxiv.org/abs/1904.09626](https://arxiv.org/abs/1904.09626)\n\n**SoftTriple Loss: Deep Metric Learning Without Triplet Sampling**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.05235](https://arxiv.org/abs/1909.05235)\n\n**The Group Loss for Deep Metric Learning**\n\n[https://arxiv.org/abs/1912.00385](https://arxiv.org/abs/1912.00385)\n\n**Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning**\n\n- intro: CVPR 2020\n- intro: NAVER Corp.\n- arxiv: [https://arxiv.org/abs/2003.02546](https://arxiv.org/abs/2003.02546)\n\n**Proxy Anchor Loss for Deep Metric Learning**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.13911](https://arxiv.org/abs/2003.13911)\n- github(official, Pytorch): [https://github.com/tjddus9597/Proxy-Anchor-CVPR2020](https://github.com/tjddus9597/Proxy-Anchor-CVPR2020)\n\n**Spherical Feature Transform for Deep Metric Learning**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.01469](https://arxiv.org/abs/2008.01469)\n\n**Diversified Mutual Learning for Deep Metric Learning**\n\n- intro: ECCV Workshop 2020\n- arxiv: [https://arxiv.org/abs/2009.04170](https://arxiv.org/abs/2009.04170)\n\n**Deep Metric Learning with Spherical Embedding**\n\n- intro: NeurIPS 2020\n- arxiv: [https://arxiv.org/abs/2011.02785](https://arxiv.org/abs/2011.02785)\n- github(Pytorch):[https://github.com/Dyfine/SphericalEmbedding](https://github.com/Dyfine/SphericalEmbedding)\n\n**Learning Intra-Batch Connections for Deep Metric Learning**\n\n[https://arxiv.org/abs/2102.07753](https://arxiv.org/abs/2102.07753)\n\n**LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.09335](https://arxiv.org/abs/2108.09335)\n\n# Talks / Slides\n\n**TiefVision: end-to-end image similarity search engine**\n\n- intro: It covers image classification, image location ( OverFeat ) and image similarity ( Deep Ranking).\n- slides: [https://docs.google.com/presentation/d/16hrXJhOzkbmla9AL7JCreCuBsa5L80gm71Pfrjo7F9Y/edit#slide=id.p](https://docs.google.com/presentation/d/16hrXJhOzkbmla9AL7JCreCuBsa5L80gm71Pfrjo7F9Y/edit#slide=id.p)\n- github: [https://github.com/paucarre/tiefvision](https://github.com/paucarre/tiefvision)\n\n# Projects\n\n**PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by Deep Convolutional Neural Networks**\n\n- intro: Open source deep learning based image retrieval toolbox based on PyTorch\n- arxiv: [https://arxiv.org/abs/2005.02154](https://arxiv.org/abs/2005.02154)\n- github: [https://github.com/PyRetri/PyRetri](https://github.com/PyRetri/PyRetri)\n\n**图像检索：CNN卷积神经网络与实战**\n\n**CNN for Image Retrieval**\n\n- blog: [http://yongyuan.name/blog/CBIR-CNN-and-practice.html](http://yongyuan.name/blog/CBIR-CNN-and-practice.html)\n- github: [https://github.com/willard-yuan/CNN-for-Image-Retrieval](https://github.com/willard-yuan/CNN-for-Image-Retrieval)\n- demo: [http://yongyuan.name/pic/](http://yongyuan.name/pic/)\n\n**Visual Search Server**\n\n![](https://raw.githubusercontent.com/AKSHAYUBHAT/VisualSearchServer/master/appcode/static/alpha3.png)\n\n- intro: A simple implementation of Visual Search using features extracted from Tensorflow inception model and Approximate Nearest Neighbors \n- github: [https://github.com/AKSHAYUBHAT/VisualSearchServer](https://github.com/AKSHAYUBHAT/VisualSearchServer)\n\n**Vehicle Retrieval: vehicle image retrieval using k CNNs ensemble method**\n\n- intro: ranked 1st and won the special prize in the final of \nthe 3rd National Gradute Contest on Smart-CIty Technology and Creative Design, China\n- project page: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)\n- github: [https://github.com/iamhankai/vehicle-retrieval-kCNNs](https://github.com/iamhankai/vehicle-retrieval-kCNNs)\n\n**A visual search engine based on Elasticsearch and Tensorflow**\n\n- keywords: faster r-cnn\n- github: [https://github.com/tuan3w/visual_search](https://github.com/tuan3w/visual_search)\n\n**Siamese and triplet networks with online pair/triplet mining in PyTorch**\n\n[https://github.com/adambielski/siamese-triplet](https://github.com/adambielski/siamese-triplet)\n\n**Triplet Loss and Online Triplet Mining in TensorFlow**\n\n- blog: [https://omoindrot.github.io/triplet-loss](https://omoindrot.github.io/triplet-loss)\n- gtihub: [https://github.com/omoindrot/tensorflow-triplet-loss](https://github.com/omoindrot/tensorflow-triplet-loss)\n\n# Blogs\n\n**Where can I buy a chair like that? – This app will tell you**\n\n![](http://www.news.cornell.edu/sites/chronicle.cornell/files/GrokStyleApp.png?itok=3jd_S2R7)\n\n- blog: [http://www.news.cornell.edu/stories/2016/08/where-can-i-buy-chair-app-will-tell-you](http://www.news.cornell.edu/stories/2016/08/where-can-i-buy-chair-app-will-tell-you)\n\n**Using Sketches to Search for Products Online**\n\n![](http://sketchx.eecs.qmul.ac.uk/wp-content/uploads/sites/27/2016/04/slider_template_cvpr4-1.jpg)\n\n- homepage: [http://sketchx.eecs.qmul.ac.uk/](http://sketchx.eecs.qmul.ac.uk/)\n- blog: [https://news.developer.nvidia.com/using-sketches-to-search-for-products-online/](https://news.developer.nvidia.com/using-sketches-to-search-for-products-online/)\n\n# Tutorials\n\n**Deep Image Retrieval: Learning global representations for image search**\n\n- youtube: [https://www.youtube.com/watch?v=yT52xDML6ys](https://www.youtube.com/watch?v=yT52xDML6ys)\n\n**Image Instance Retrieval: Overview of state-of-the-art**\n\n- youtube: [https://www.youtube.com/watch?v=EYq-rpaZn1o](https://www.youtube.com/watch?v=EYq-rpaZn1o)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","title":"Reinforcement Learning"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Reinforcement Learning\r\ndate: 2015-10-09\r\n---\r\n\r\n# Tutorials\r\n\r\n**Demystifying Deep Reinforcement Learning (Part1)**\r\n\r\n[http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/](http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/)\r\n\r\n**Deep Reinforcement Learning With Neon (Part2)**\r\n\r\n[http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/](http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/)\r\n\r\n**Deep Reinforcement Learning**\r\n\r\n- intro: David Silver, Google DeepMind\r\n- slides: [http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf)\r\n- mirror: [http://pan.baidu.com/s/1qWBOJGo](http://pan.baidu.com/s/1qWBOJGo)\r\n\r\n**Deep Reinforcement Learning**\r\n\r\n- intro:  MLSS 2016. John Schulman[UC Berkeley]\r\n- homepage: [http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/index.html](http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/index.html)\r\n- slides: [http://pan.baidu.com/s/1jIatusA#path=%252F](http://pan.baidu.com/s/1jIatusA#path=%252F)\r\n\r\n**Deep Reinforcement Learning: Pong from Pixels**\r\n\r\n![](http://karpathy.github.io/assets/rl/preview.jpeg)\r\n\r\n- intro: Andrej Karpathy\r\n- blog: [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)\r\n- gist: [https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\r\n\r\n**Deep Reinforcement Learning**\r\n\r\n- instructor: David Silver. RLDM 2015\r\n- video: [http://videolectures.net/rldm2015_silver_reinforcement_learning/](http://videolectures.net/rldm2015_silver_reinforcement_learning/)\r\n\r\n**Deep Reinforcement Learning**\r\n\r\n- intro: David Silver [Google DeepMind]\r\n- video: [http://techtalks.tv/talks/deep-reinforcement-learning/62360/](http://techtalks.tv/talks/deep-reinforcement-learning/62360/)\r\n- slides: [http://hunch.net/~beygel/deep_rl_tutorial.pdf](http://hunch.net/~beygel/deep_rl_tutorial.pdf)\r\n\r\n**The Nuts and Bolts of Deep RL Research**\r\n\r\n- intro: NIPS 2016, John Schulman, OpenAI\r\n- slides: [http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf](http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf)\r\n- mirror: [https://pan.baidu.com/s/1kVkBLkF](https://pan.baidu.com/s/1kVkBLkF)\r\n\r\n**ML Tutorial: Modern Reinforcement Learning and Video Games**\r\n\r\n- intro: by Marc Bellemare [DeepMind]\r\n- youtube: [https://www.youtube.com/watch?v=WuFMrk3ZbkE](https://www.youtube.com/watch?v=WuFMrk3ZbkE)\r\n- mirror: [https://www.bilibili.com/video/av17360035/](https://www.bilibili.com/video/av17360035/)\r\n\r\n**Reinforcement learning explained**\r\n\r\n- blog: [https://www.oreilly.com/ideas/reinforcement-learning-explained](https://www.oreilly.com/ideas/reinforcement-learning-explained)\r\n\r\n**Beginner's guide to Reinforcement Learning & its implementation in Python**\r\n\r\n[https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)\r\n\r\n**Reinforcement Learning on the Web**\r\n\r\n- intro: Andrej Karpathy\r\n- slides: [https://docs.google.com/presentation/d/1lcYrN56V2_SuX1rSmpzOUeMnheF6Jsu33-MsvLW9O_4/edit#slide=id.p](https://docs.google.com/presentation/d/1lcYrN56V2_SuX1rSmpzOUeMnheF6Jsu33-MsvLW9O_4/edit#slide=id.p)\r\n- slides: [http://alpha.openai.com/ak_rework_2017.pdf](http://alpha.openai.com/ak_rework_2017.pdf)\r\n\r\n**Deep Q Learning with Keras and Gym**\r\n\r\n- blog: [https://keon.io/rl/deep-q-learning-with-keras-and-gym/](https://keon.io/rl/deep-q-learning-with-keras-and-gym/)\r\n- github: [https://github.com/keon/deep-q-learning](https://github.com/keon/deep-q-learning)\r\n\r\n**“Deep Reinforcement Learning, Decision Making, and Control**\r\n\r\n- intro: ICML 2017 Tutorial\r\n- slides: [https://sites.google.com/view/icml17deeprl](https://sites.google.com/view/icml17deeprl)\r\n\r\n**A Tour of Reinforcement Learning: The View from Continuous Control**\r\n\r\n- intro: by Benjamin Recht, UC Berkeley\r\n- slides: [https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/Recht_ICML_Control-RL_tutorial.pdf](https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/Recht_ICML_Control-RL_tutorial.pdf)\r\n\r\n**An Introduction to Deep Reinforcement Learning**\r\n\r\n- intro: McGill University & Google Brain\r\n- arxiv: [https://arxiv.org/abs/1811.12560](https://arxiv.org/abs/1811.12560)\r\n\r\n## Simple Reinforcement Learning with Tensorflow\r\n\r\n**Part 0: Q-Learning with Tables and Neural Networks**\r\n[https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.oo105wa2t](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.oo105wa2t)\r\n\r\n**Part 1 - Two-armed Bandit**\r\n\r\n[https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.tk89k51ob](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.tk89k51ob)\r\n\r\n**Part 2 - Policy-based Agents**\r\n\r\n[https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.n2wytg9q0](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.n2wytg9q0)\r\n\r\n**Part 3 - Model-Based RL**\r\n[https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.742i2yj6p](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.742i2yj6p)\r\n\r\n**Part 4: Deep Q-Networks and Beyond**\r\n[https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.jox069crz](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.jox069crz)\r\n\r\n**Part 5: Visualizing an Agent’s Thoughts and Actions**\r\n[https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.pluh6cygm](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.pluh6cygm)\r\n\r\n**Part 6: Partial Observability and Deep Recurrent Q-Networks**\r\n\r\n- blog: [https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.3se46qkzy](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.3se46qkzy)\r\n- github: [https://gist.github.com/awjuliani/35d2ab3409fc818011b6519f0f1629df](https://gist.github.com/awjuliani/35d2ab3409fc818011b6519f0f1629df)\r\n\r\n**Part 7: Action-Selection Strategies for Exploration**\r\n\r\n- blog: [https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.8mcaa5nbe](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.8mcaa5nbe)\r\n- demo: [https://awjuliani.github.io/exploration/index.html](https://awjuliani.github.io/exploration/index.html)\r\n\r\n**Dissecting Reinforcement Learning**\r\n\r\n- part 1: [https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html)\r\n- part 2: [https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html](https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html)\r\n- part 3: [https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html](https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html)\r\n- github: [https://github.com/mpatacchiola/dissecting-reinforcement-learning](https://github.com/mpatacchiola/dissecting-reinforcement-learning)\r\n\r\n**REINFORCE tutorial**\r\n\r\n- intro: A small collection of code snippets and notes explaining the foundations of the REINFORCE algorithm.\r\n- github: [https://github.com/mathias-madsen/reinforce_tutorial](https://github.com/mathias-madsen/reinforce_tutorial)\r\n\r\n**Deep Q-Learning Recap**\r\n\r\n[http://blog.davidqiu.com/Research/%5B%20Recap%20%5D%20Deep%20Q-Learning%20Recap/](http://blog.davidqiu.com/Research/%5B%20Recap%20%5D%20Deep%20Q-Learning%20Recap/)\r\n\r\n**Introduction to Reinforcement Learning**\r\n\r\n- intro: Joelle Pineau [McGill University]\r\n- video: [http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/](http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/)\r\n- slides: [http://videolectures.net/site/normal_dl/tag=1051677/deeplearning2016_pineau_reinforcement_learning_01.pdf](http://videolectures.net/site/normal_dl/tag=1051677/deeplearning2016_pineau_reinforcement_learning_01.pdf)\r\n\r\n# Courses\r\n\r\n**Advanced Topics: RL**\r\n\r\n**UCL Course on RL**\r\n\r\n- instructors: David Silver (Google DeepMind, AlphaGo)\r\n- homepage: [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\r\n- youtube: [https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\r\n- video: [http://pan.baidu.com/s/1bnWGuIz/](http://pan.baidu.com/s/1bnWGuIz/)\r\n- assignment: [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf)\r\n\r\n**CS 294: Deep Reinforcement Learning, Fall 2017**\r\n\r\n- instructor: Sergey Levine\r\n- homepage: [http://rll.berkeley.edu/deeprlcourse/](http://rll.berkeley.edu/deeprlcourse/)\r\n- youtube: [https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3](https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3)\r\n- bilibili: [https://www.bilibili.com/video/av21501169/](https://www.bilibili.com/video/av21501169/)\r\n\r\n**CS 294: Deep Reinforcement Learning, Spring 2017**\r\n\r\n- course page: [http://rll.berkeley.edu/deeprlcoursesp17/](http://rll.berkeley.edu/deeprlcoursesp17/)\r\n- github: [https://github.com//txizzle/drl](https://github.com//txizzle/drl)\r\n\r\n**Berkeley CS 294: Deep Reinforcement Learning**\r\n\r\n- instructors: John Schulman, Pieter Abbeel\r\n- homepage: [http://rll.berkeley.edu/deeprlcourse/](http://rll.berkeley.edu/deeprlcourse/)\r\n- youtube: [https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX](https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX)\r\n- mirror: [https://pan.baidu.com/s/1hsQcm1Y](https://pan.baidu.com/s/1hsQcm1Y)\r\n\r\n**(Udacity) Reinforcement Learning - Offered at Georgia Tech as CS 8803**\r\n\r\n- instructor: Charles Isbell, Michael Littman\r\n- homepage: [https://www.udacity.com/course/reinforcement-learning--ud600](https://www.udacity.com/course/reinforcement-learning--ud600)\r\n- homepage: [https://classroom.udacity.com/courses/ud820/lessons/684808907/concepts/6512308530923](https://classroom.udacity.com/courses/ud820/lessons/684808907/concepts/6512308530923)\r\n\r\n**CS229 Lecture notes Part XIII: Reinforcement Learning and Control**\r\n\r\n- intro: Andrew Ng\r\n- lecture notes: [http://cs229.stanford.edu/notes/cs229-notes12.pdf](http://cs229.stanford.edu/notes/cs229-notes12.pdf)\r\n\r\n**Practical_RL: A course in reinforcement learning in the wild**\r\n\r\n- github: [https://github.com/yandexdataschool/Practical_RL](https://github.com/yandexdataschool/Practical_RL)\r\n\r\n**Reinforcement Learning (COMP-762) Winter 2017**\r\n\r\n- course page: [http://www.cs.mcgill.ca/~dprecup/courses/rl.html](http://www.cs.mcgill.ca/~dprecup/courses/rl.html)\r\n- lectures: [http://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html](http://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html)\r\n\r\n**Deep RL Bootcamp - 26-27 August 2017 | Berkeley CA**\r\n\r\n- lectures: [https://sites.google.com/view/deep-rl-bootcamp/lectures](https://sites.google.com/view/deep-rl-bootcamp/lectures)\r\n- video: [https://www.bilibili.com/video/av15568836/](https://www.bilibili.com/video/av15568836/)\r\n\r\n**CMPUT 366: Intelligent Systems and CMPUT 609: Reinforcement Learning & Artificial Intelligence**\r\n\r\n- intro: by Rich Sutton, Adam White\r\n- lecture video: [https://drive.google.com/drive/folders/0B3w765rOKuKAMG9lbmRacFdsLWM?direction=a](https://drive.google.com/drive/folders/0B3w765rOKuKAMG9lbmRacFdsLWM?direction=a)\r\n\r\n**Deep Reinforcement Learning and Control (Spring 2017, CMU 10703)**\r\n\r\n- instructors: Katerina Fragkiadaki, Ruslan Satakhutdinov\r\n- homepage: [https://katefvision.github.io/](https://katefvision.github.io/)\r\n- video: [https://www.youtube.com/playlist?list=PLpIxOj-HnDsNPFdu2UqCu2McJKHs-eWXv](https://www.youtube.com/playlist?list=PLpIxOj-HnDsNPFdu2UqCu2McJKHs-eWXv)\r\n- mirror: [https://www.bilibili.com/video/av18865689/](https://www.bilibili.com/video/av18865689/)\r\n\r\n**Advanced Deep Learning & Reinforcement Learning**\r\n\r\n- intro: DeepMind\r\n- youtube: [https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs](https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)\r\n- bilibili: [https://www.bilibili.com/video/av36621866/](https://www.bilibili.com/video/av36621866/)\r\n- github: [https://github.com/RylanSchaeffer/ucl-adv-dl-rl](https://github.com/RylanSchaeffer/ucl-adv-dl-rl)\r\n\r\n# Papers\r\n\r\n**Playing Atari with Deep Reinforcement Learning**\r\n\r\n- intro: Google DeepMind. NIPS Deep Learning Workshop 2013\r\n- arxiv: [http://arxiv.org/abs/1312.5602](http://arxiv.org/abs/1312.5602)\r\n- github: [https://github.com/kristjankorjus/Replicating-DeepMind](https://github.com/kristjankorjus/Replicating-DeepMind)\r\n- demo: [http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html](http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html)\r\n- github: [https://github.com/Kaixhin/Atari](https://github.com/Kaixhin/Atari)\r\n- github(Tensorflow): [https://github.com/gliese581gg/DQN_tensorflow](https://github.com/gliese581gg/DQN_tensorflow)\r\n- summary: [https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md](https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md)\r\n\r\n**Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning**\r\n\r\n- intro: NIPS 2014\r\n- keywords: DQN, MCTS\r\n- paper: [http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging](http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging)\r\n- paper: [https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf](https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf)\r\n\r\n**Replicating the Paper “Playing Atari with Deep Reinforcement Learning”**\r\n\r\n- intro: University of Tartu\r\n- technical report: [https://courses.cs.ut.ee/MTAT.03.291/2014_spring/uploads/Main/Replicating%20DeepMind.pdf](https://courses.cs.ut.ee/MTAT.03.291/2014_spring/uploads/Main/Replicating%20DeepMind.pdf)\r\n\r\n**A Tutorial for Reinforcement Learning**\r\n\r\n- paper: [http://web.mst.edu/~gosavia/tutorial.pdf](http://web.mst.edu/~gosavia/tutorial.pdf)\r\n- code(C): [http://web.mst.edu/~gosavia/bookcodes.html](http://web.mst.edu/~gosavia/bookcodes.html)\r\n- code(Matlab): [http://web.mst.edu/~gosavia/mrrl_website.html](http://web.mst.edu/~gosavia/mrrl_website.html)\r\n\r\n**Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models**\r\n\r\n- arxiv: [http://arxiv.org/abs/1507.00814](http://arxiv.org/abs/1507.00814)\r\n- notes: [https://www.evernote.com/shard/s189/sh/a4262b84-a322-4f77-9a76-569278be84af/b8c3e146a76ca3853f560bb03b60a481](https://www.evernote.com/shard/s189/sh/a4262b84-a322-4f77-9a76-569278be84af/b8c3e146a76ca3853f560bb03b60a481)\r\n\r\n**Massively Parallel Methods for Deep Reinforcement Learning**\r\n\r\n- intro: ICML 2015. DeepMind\r\n- keywords: DQN, Gorila\r\n- arxiv: [https://arxiv.org/abs/1507.04296](https://arxiv.org/abs/1507.04296)\r\n\r\n**Action-Conditional Video Prediction using Deep Networks in Atari Games**\r\n\r\n- homepage: [https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction](https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction)\r\n- arxiv: [http://arxiv.org/abs/1507.08750](http://arxiv.org/abs/1507.08750)\r\n- github: [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)\r\n- video: [http://video.weibo.com/show?fid=1034:98062f3d83e41da6faa99cde5aa1ac97](http://video.weibo.com/show?fid=1034:98062f3d83e41da6faa99cde5aa1ac97)\r\n\r\n**Deep Recurrent Q-Learning for Partially Observable MDPs**\r\n\r\n- intro: AAAI 2015\r\n- arxiv: [https://arxiv.org/abs/1507.06527](https://arxiv.org/abs/1507.06527)\r\n\r\n**Continuous control with deep reinforcement learning**\r\n\r\n- intro: Google DeepMind\r\n- arxiv: [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)\r\n- github: [https://github.com/iassael/torch-policy-gradient](https://github.com/iassael/torch-policy-gradient)\r\n- github: [https://github.com/stevenpjg/ddpg-aigym](https://github.com/stevenpjg/ddpg-aigym)\r\n- github(TensorFlow + OpenAI Gym): [https://github.com/SimonRamstedt/ddpg](https://github.com/SimonRamstedt/ddpg)\r\n\r\n**Benchmarking for Bayesian Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1509.04064](http://arxiv.org/abs/1509.04064)\r\n- code: [https://github.com/mcastron/BBRL/](https://github.com/mcastron/BBRL/)\r\n- reading: [http://blogs.ulg.ac.be/damien-ernst/benchmarking-for-bayesian-reinforcement-learning/](http://blogs.ulg.ac.be/damien-ernst/benchmarking-for-bayesian-reinforcement-learning/)\r\n\r\n**Deep Reinforcement Learning with Double Q-learning**\r\n\r\n- intro: AAAI 2016\r\n- arxiv: [https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)\r\n\r\n**Giraffe: Using Deep Reinforcement Learning to Play Chess**\r\n\r\n- arxiv: [http://arxiv.org/abs/1509.01549](http://arxiv.org/abs/1509.01549)\r\n\r\n**Human-level control through deep reinforcement learning**\r\n\r\n![](/assets/reinforcement-learning-materials/DeepMind_Atari_Deep_Q_Learner-breakout.gif)\r\n\r\n- intro: Google DeepMind. 2015 Nature\r\n- paper: [http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D](http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D)\r\n- paper: [http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf](http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\r\n- github(Lua/Torch): [https://github.com/deepmind/dqn](https://github.com/deepmind/dqn)\r\n- mirror: [http://pan.baidu.com/s/1kTiwzOF](http://pan.baidu.com/s/1kTiwzOF)\r\n- code: [https://sites.google.com/a/deepmind.com/dqn/](https://sites.google.com/a/deepmind.com/dqn/)\r\n- youtube: [https://www.youtube.com/watch?v=V2wzkPmiB_A](https://www.youtube.com/watch?v=V2wzkPmiB_A)\r\n- github: [https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner](https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner)\r\n- github: [https://github.com/tambetm/simple_dqn](https://github.com/tambetm/simple_dqn)\r\n- github: [https://github.com/devsisters/DQN-tensorflow](https://github.com/devsisters/DQN-tensorflow)\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/2x4yy1/google_deepmind_nature_paper_humanlevel_control](https://www.reddit.com/r/MachineLearning/comments/2x4yy1/google_deepmind_nature_paper_humanlevel_control)\r\n\r\n**Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models**\r\n\r\n- arxiv: [http://arxiv.org/abs/1510.02173](http://arxiv.org/abs/1510.02173)\r\n\r\n**Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning**\r\n\r\n- intro: Google DeepMind\r\n- arxiv: [http://arxiv.org/abs/1509.08731](http://arxiv.org/abs/1509.08731)\r\n- notes: [https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889](https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889)\r\n\r\n**Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning**\r\n\r\n- intro: ICLR 2016\r\n- arxiv: [http://arxiv.org/abs/1511.06342](http://arxiv.org/abs/1511.06342)\r\n- github: [https://github.com/eparisotto/ActorMimic](https://github.com/eparisotto/ActorMimic)\r\n\r\n**MazeBase: A Sandbox for Learning from Games**\r\n\r\n- intro: New York University & Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1511.07401](http://arxiv.org/abs/1511.07401)\r\n\r\n**Learning Simple Algorithms from Examples**\r\n\r\n- intro: New York University & Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1511.07275](http://arxiv.org/abs/1511.07275)\r\n- github: [https://github.com/wojzaremba/algorithm-learning](https://github.com/wojzaremba/algorithm-learning)\r\n\r\n**Learning Algorithms from Data**\r\n\r\n- PhD thesis: [http://www.cs.nyu.edu/media/publications/zaremba_wojciech.pdf](http://www.cs.nyu.edu/media/publications/zaremba_wojciech.pdf)\r\n- github: [https://github.com/wojzaremba/algorithm-learning](https://github.com/wojzaremba/algorithm-learning)\r\n\r\n**Multiagent Cooperation and Competition with Deep Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.08779](http://arxiv.org/abs/1511.08779)\r\n- github: [https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player](https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player)\r\n\r\n**Active Object Localization with Deep Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.06015](http://arxiv.org/abs/1511.06015)\r\n\r\n**Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.01124](http://arxiv.org/abs/1512.01124)\r\n\r\n**How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.02011](http://arxiv.org/abs/1512.02011)\r\n\r\n**State of the Art Control of Atari Games Using Shallow Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.01563](http://arxiv.org/abs/1512.01563)\r\n\r\n**Angrier Birds: Bayesian reinforcement learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.01297](http://arxiv.org/abs/1601.01297)\r\n- github: [https://github.com/imanolarrieta/angrybirds](https://github.com/imanolarrieta/angrybirds)\r\n- gitxiv: [http://gitxiv.com/posts/Nr2N7j4YrR4gnCYK9/angrier-birds-bayesian-reinforcement-learning](http://gitxiv.com/posts/Nr2N7j4YrR4gnCYK9/angrier-birds-bayesian-reinforcement-learning)\r\n\r\n**Prioritized Experience Replay**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.05952](http://arxiv.org/abs/1511.05952)\r\n\r\n**Dueling Network Architectures for Deep Reinforcement Learning**\r\n\r\n- intro: ICML 2016 best paper\r\n- arxiv: [http://arxiv.org/abs/1511.06581](http://arxiv.org/abs/1511.06581)\r\n- notes: [https://hadovanhasselt.wordpress.com/2016/06/20/best-paper-at-icml-dueling-network-architectures-for-deep-reinforcement-learning/](https://hadovanhasselt.wordpress.com/2016/06/20/best-paper-at-icml-dueling-network-architectures-for-deep-reinforcement-learning/)\r\n\r\n**Asynchronous Methods for Deep Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.01783](http://arxiv.org/abs/1602.01783)\r\n- github(Tensorflow): [https://github.com/traai/async-deep-rl](https://github.com/traai/async-deep-rl)\r\n- github(Tensorflow+Keras+OpenAI Gym): [https://github.com/coreylynch/async-rl](https://github.com/coreylynch/async-rl)\r\n- github(Tensorflow): [https://github.com/devsisters/async-rl-tensorflow](https://github.com/devsisters/async-rl-tensorflow)\r\n- github(PyTorch): [https://github.com/ikostrikov/pytorch-a3c](https://github.com/ikostrikov/pytorch-a3c)\r\n- notes: [https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/](https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/)\r\n\r\n**Graying the black box: Understanding DQNs**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.02658](http://arxiv.org/abs/1602.02658)\r\n\r\n**Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.02672](http://arxiv.org/abs/1602.02672)\r\n\r\n**Value Iteration Networks**\r\n\r\n![](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%204.58.42%20PM.png)\r\n\r\n- intro: NIPS 2016, Best Paper Award. University of California, Berkeley\r\n- arxiv: [http://arxiv.org/abs/1602.02867](http://arxiv.org/abs/1602.02867)\r\n- github(official, Theano): [https://github.com/avivt/VIN](https://github.com/avivt/VIN)\r\n- github: [https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks](https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks)\r\n- github: [https://github.com/onlytailei/PyTorch-value-iteration-networks](https://github.com/onlytailei/PyTorch-value-iteration-networks)\r\n- github: [https://github.com/kentsommer/pytorch-value-iteration-networks](https://github.com/kentsommer/pytorch-value-iteration-networks)\r\n- github: [https://github.com/neka-nat/vin-keras](https://github.com/neka-nat/vin-keras)\r\n- notes(by Andrej Karpathy): [https://github.com/karpathy/paper-notes/blob/master/vin.md](https://github.com/karpathy/paper-notes/blob/master/vin.md)\r\n\r\n**Insights in Reinforcement Learning**\r\n\r\n- intro: MSc thesis\r\n- mirror: [http://pan.baidu.com/s/1bn51BYJ](http://pan.baidu.com/s/1bn51BYJ)\r\n\r\n**Using Deep Q-Learning to Control Optimization Hyperparameters**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.04062](http://arxiv.org/abs/1602.04062)\r\n\r\n**Continuous Deep Q-Learning with Model-based Acceleration**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.00748](http://arxiv.org/abs/1603.00748)\r\n\r\n**Deep Reinforcement Learning from Self-Play in Imperfect-Information Games**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.01121](http://arxiv.org/abs/1603.01121)\r\n\r\n**Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation**\r\n\r\n- intro: MIT\r\n- arxiv: [https://arxiv.org/abs/1604.06057](https://arxiv.org/abs/1604.06057)\r\n- github: [https://github.com/EthanMacdonald/h-DQN](https://github.com/EthanMacdonald/h-DQN)\r\n\r\n**Benchmarking Deep Reinforcement Learning for Continuous Control**\r\n\r\n- arxiv: [http://arxiv.org/abs/1604.06778](http://arxiv.org/abs/1604.06778)\r\n- github: [https://github.com/rllab/rllab](https://github.com/rllab/rllab)\r\n- doc: [https://rllab.readthedocs.org/en/latest/](https://rllab.readthedocs.org/en/latest/)\r\n\r\n**Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning**\r\n\r\n![](http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/dog_teaser.png)\r\n\r\n- homepage: [http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html](http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html)\r\n- paper: [http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf](http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf)\r\n- github: [https://github.com/xbpeng/DeepTerrainRL](https://github.com/xbpeng/DeepTerrainRL)\r\n\r\n**Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.05359](http://arxiv.org/abs/1605.05359)\r\n\r\n**Deep Successor Reinforcement Learning (MIT)**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.02396](http://arxiv.org/abs/1606.02396)\r\n- github: [https://github.com/Ardavans/DSR](https://github.com/Ardavans/DSR)\r\n\r\n**Learning to Communicate with Deep Multi-Agent Reinforcement Learning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1605.06676](https://arxiv.org/abs/1605.06676)\r\n- github: [https://github.com/iassael/learning-to-communicate](https://github.com/iassael/learning-to-communicate)\r\n\r\n**Deep Reinforcement Learning with Regularized Convolutional Neural Fitted Q Iteration**\r\n**RC-NFQ: Regularized Convolutional Neural Fitted Q Iteration**\r\n\r\n- intro: A batch algorithm for deep reinforcement learning. \r\nIncorporates dropout regularization and convolutional neural networks with a separate target Q network.\r\n- paper: [http://machineintelligence.org/papers/rc-nfq.pdf](http://machineintelligence.org/papers/rc-nfq.pdf)\r\n- github: [https://github.com/cosmoharrigan/rc-nfq](https://github.com/cosmoharrigan/rc-nfq)\r\n\r\n**Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks**\r\n\r\n- intro: Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1609.02993](http://arxiv.org/abs/1609.02993)\r\n\r\n**Bayesian Reinforcement Learning: A Survey**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.04436](http://arxiv.org/abs/1609.04436)\r\n\r\n**Playing FPS Games with Deep Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.05521](http://arxiv.org/abs/1609.05521)\r\n- demo: [https://www.youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT](https://www.youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT)\r\n- notes: [https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/](https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/)\r\n\r\n**Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning with Stochastic Initial States**\r\n\r\n- intro: University of Washington & UC Berkeley\r\n- arxiv: [https://arxiv.org/abs/1610.01112](https://arxiv.org/abs/1610.01112)\r\n\r\n**Utilization of Deep Reinforcement Learning for saccadic-based object visual search**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.06492](https://arxiv.org/abs/1610.06492)\r\n\r\n**Learning to Navigate in Complex Environments**\r\n\r\n- intro: Google DeepMind\r\n- arxiv: [https://arxiv.org/abs/1611.03673](https://arxiv.org/abs/1611.03673)\r\n- github: [https://github.com/deepmind/lab](https://github.com/deepmind/lab)\r\n- youtube: [https://www.youtube.com/watch?v=lNoaTyMZsWI](https://www.youtube.com/watch?v=lNoaTyMZsWI)\r\n\r\n**Reinforcement Learning with Unsupervised Auxiliary Tasks**\r\n\r\n- intro: DeepMind. ICLR 2017 oral\r\n- arxiv: [https://arxiv.org/abs/1611.05397](https://arxiv.org/abs/1611.05397)\r\n\r\n**Learning to reinforcement learn**\r\n\r\n- intro: DeepMind\r\n- arxiv: [https://arxiv.org/abs/1611.05763](https://arxiv.org/abs/1611.05763)\r\n\r\n**A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games**\r\n\r\n- intro: Graduate Training Center of Neuroscience & MSR\r\n- arxiv: [https://arxiv.org/abs/1611.07078](https://arxiv.org/abs/1611.07078)\r\n\r\n**Exploration for Multi-task Reinforcement Learning with Deep Generative Models**\r\n\r\n- intro: NIPS Deep Reinforcement Learning Workshop 2016\r\n- arxiv: [https://arxiv.org/abs/1611.09894](https://arxiv.org/abs/1611.09894)\r\n\r\n**Neural Combinatorial Optimization with Reinforcement Learning**\r\n\r\n- intro: Google Brain\r\n- keywords: traveling salesman problem (TSP)\r\n- arxiv: [https://arxiv.org/abs/1611.09940](https://arxiv.org/abs/1611.09940)\r\n\r\n**Loss is its own Reward: Self-Supervision for Reinforcement Learning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.07307](https://arxiv.org/abs/1612.07307)\r\n\r\n**Reinforcement Learning Using Quantum Boltzmann Machines**\r\n\r\n- intro: 1QB Information Technologies (1QBit)\r\n- arxiv: [https://arxiv.org/abs/1612.05695](https://arxiv.org/abs/1612.05695)\r\n\r\n**Deep Reinforcement Learning applied to the game Bubble Shooter**\r\n\r\n- bachelor thesis: [https://staff.fnwi.uva.nl/b.bredeweg/pdf/BSc/20152016/Samson.pdf](https://staff.fnwi.uva.nl/b.bredeweg/pdf/BSc/20152016/Samson.pdf)\r\n- github: [https://github.com/laurenssam/AlphaBubble](https://github.com/laurenssam/AlphaBubble)\r\n- demo: [https://www.youtube.com/watch?v=DPAKFenNgbs](https://www.youtube.com/watch?v=DPAKFenNgbs)\r\n\r\n**Deep Reinforcement Learning: An Overview**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.07274](https://arxiv.org/abs/1701.07274)\r\n\r\n**Robust Adversarial Reinforcement Learning**\r\n\r\n- intro: CMU & Google Brain & Google Research\r\n- arxiv: [https://arxiv.org/abs/1703.02702](https://arxiv.org/abs/1703.02702)\r\n\r\n**Beating Atari with Natural Language Guided Reinforcement Learning**\r\n\r\n- intro: Stanford University\r\n- arxiv: [https://arxiv.org/abs/1704.05539](https://arxiv.org/abs/1704.05539)\r\n\r\n**Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning**\r\n\r\n- intro: Imperial College London\r\n- arxiv: [https://arxiv.org/abs/1705.06769](https://arxiv.org/abs/1705.06769)\r\n- github: [https://github.com/Nat-D/FeatureControlHRL](https://github.com/Nat-D/FeatureControlHRL)\r\n\r\n**Distral: Robust Multitask Reinforcement Learning**\r\n\r\n- intro: DeepMind\r\n- keywords: Distill, transfer learning\r\n- arxiv: [https://arxiv.org/abs/1707.04175](https://arxiv.org/abs/1707.04175)\r\n\r\n**Deep Reinforcement Learning: Framework, Applications, and Embedded Implementations**\r\n\r\n- intro: Syracuse University & University of California, Riverside\r\n- arxiv: [https://arxiv.org/abs/1710.03792](https://arxiv.org/abs/1710.03792)\r\n\r\n**Robust Deep Reinforcement Learning with Adversarial Attacks**\r\n\r\n[https://arxiv.org/abs/1712.03632](https://arxiv.org/abs/1712.03632)\r\n\r\n**Variational Deep Q Network**\r\n\r\n- intro: Second workshop on Bayesian Deep Learning (NIPS 2017). Columbia University\r\n- arxiv: [https://arxiv.org/abs/1711.11225](https://arxiv.org/abs/1711.11225)\r\n\r\n**On Monte Carlo Tree Search and Reinforcement Learning**\r\n\r\n[https://www.jair.org/media/5507/live-5507-10333-jair.pdf](https://www.jair.org/media/5507/live-5507-10333-jair.pdf)\r\n\r\n**Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes**\r\n\r\n- intro: deepsense.ai & Intel & Polish Academy of Sciences\r\n- arxiv: [https://arxiv.org/abs/1801.02852](https://arxiv.org/abs/1801.02852)\r\n- gihtub: [https://github.com//anonymous-author1/DDRL](https://github.com//anonymous-author1/DDRL)\r\n\r\n**GAN Q-learning**\r\n\r\n[https://arxiv.org/abs/1805.04874](https://arxiv.org/abs/1805.04874)\r\n\r\n**Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents**\r\n\r\n- intro: Visual Geometry Group, University of Oxford & Element AI & Polytechnique Montreal, Mila & Canada CIFAR AI Chair\r\n- arxiv: [https://arxiv.org/abs/1904.01318](https://arxiv.org/abs/1904.01318)\r\n\r\n## Surveys\r\n\r\n**Reinforcement Learning: A Survey**\r\n\r\n- intro: JAIR 1996\r\n- project page: [http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/rl-survey.html](http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/rl-survey.html)\r\n- arxiv: [http://arxiv.org/abs/cs/9605103](http://arxiv.org/abs/cs/9605103)\r\n\r\n**A Brief Survey of Deep Reinforcement Learning**\r\n\r\n- intro: IEEE Signal Processing Magazine, Special Issue on Deep Learning for Image Understanding\r\n- intro: Imperial College London & Arizona State University\r\n- arxiv: [https://arxiv.org/abs/1708.05866](https://arxiv.org/abs/1708.05866)\r\n\r\n## Playing Doom\r\n\r\n**ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning**\r\n\r\n![](http://vizdoom.cs.put.edu.pl/user/pages/01.home/depthbuffer.png)\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.02097](http://arxiv.org/abs/1605.02097)\r\n- github: [https://github.com/Marqt/ViZDoom](https://github.com/Marqt/ViZDoom)\r\n- homepage: [http://vizdoom.cs.put.edu.pl/](http://vizdoom.cs.put.edu.pl/)\r\n- tutorial: [http://vizdoom.cs.put.edu.pl/tutorial](http://vizdoom.cs.put.edu.pl/tutorial)\r\n\r\n**Deep Reinforcement Learning From Raw Pixels in Doom**\r\n\r\n- intro: Bachelor's thesis\r\n- arxiv: [https://arxiv.org/abs/1610.02164](https://arxiv.org/abs/1610.02164)\r\n\r\n**Playing Doom with SLAM-Augmented Deep Reinforcement Learning**\r\n\r\n- intro: University of Oxford\r\n- arxiv: [https://arxiv.org/abs/1612.00380](https://arxiv.org/abs/1612.00380)\r\n\r\n**Reinforcement Learning via Recurrent Convolutional Neural Networks**\r\n\r\n- intro: ICPR 2016\r\n- arxiv: [https://arxiv.org/abs/1701.02392](https://arxiv.org/abs/1701.02392)\r\n- github: [https://github.com/tanmayshankar/RCNN_MDP](https://github.com/tanmayshankar/RCNN_MDP)\r\n\r\n**Shallow Updates for Deep Reinforcement Learning**\r\n\r\n- intro: The Technion & UC Berkeley\r\n- arxiv: [https://arxiv.org/abs/1705.07461](https://arxiv.org/abs/1705.07461)\r\n- github(Official): [https://github.com/Shallow-Updates-for-Deep-RL/Shallow_Updates_for_Deep_RL](https://github.com/Shallow-Updates-for-Deep-RL/Shallow_Updates_for_Deep_RL)\r\n\r\n# Projects\r\n\r\n**TorchQLearning**\r\n\r\n![](https://raw.githubusercontent.com/SeanNaren/TorchQLearningExample/master/images/torchplayscatch.gif)\r\n\r\n- github: [https://github.com/SeanNaren/TorchQLearningExample](https://github.com/SeanNaren/TorchQLearningExample)\r\n\r\n**General_Deep_Q_RL: General deep Q learning framework**\r\n\r\n- github: [https://github.com/VinF/General_Deep_Q_RL](https://github.com/VinF/General_Deep_Q_RL)\r\n- wiki: [https://github.com/VinF/General_Deep_Q_RL/wiki](https://github.com/VinF/General_Deep_Q_RL/wiki)\r\n\r\n**Snake: Toy example of deep reinforcement model playing the game of snake**\r\n\r\n![](https://raw.githubusercontent.com/bitwise-ben/Snake/master/images/snake.gif)\r\n\r\n- github: [https://github.com/bitwise-ben/Snake](https://github.com/bitwise-ben/Snake)\r\n\r\n**Using Deep Q Networks to Learn Video Game Strategies**\r\n\r\n- github: [https://github.com/asrivat1/DeepLearningVideoGames](https://github.com/asrivat1/DeepLearningVideoGames)\r\n\r\n**qlearning4k: Q-learning for Keras**\r\n\r\n![](https://raw.githubusercontent.com/farizrahman4u/qlearning4k/master/gifs/catch.gif)\r\n![](https://raw.githubusercontent.com/farizrahman4u/qlearning4k/master/gifs/snake.gif)\r\n\r\n- intro: \"Qlearning4k is a reinforcement learning add-on for the python deep learning library Keras. \r\nIts simple, and is ideal for rapid prototyping.\"\r\n- github: [https://github.com/farizrahman4u/qlearning4k](https://github.com/farizrahman4u/qlearning4k)\r\n\r\n**rlenvs: Reinforcement learning environments for Torch7, inspired by RL-Glue**\r\n\r\n- github: [https://github.com/Kaixhin/rlenvs](https://github.com/Kaixhin/rlenvs)\r\n\r\n**deep_rl_ale: An implementation of Deep Reinforcement Learning / Deep Q-Networks for Atari games in TensorFlow**\r\n\r\n- github: [https://github.com/Jabberwockyll/deep_rl_ale](https://github.com/Jabberwockyll/deep_rl_ale)\r\n\r\n**Chimp: General purpose framework for deep reinforcement learning**\r\n\r\n- github: [https://github.com/sisl/Chimp](https://github.com/sisl/Chimp)\r\n\r\n**Deep Q Learning for ATARI using Tensorflow**\r\n\r\n- github: [https://github.com/mrkulk/deepQN_tensorflow](https://github.com/mrkulk/deepQN_tensorflow)\r\n\r\n**DeepQLearning: A powerful machine learning algorithm utilizing Q-Learning and Neural Networks, implemented using Torch and Lua.**\r\n\r\n- github: [https://github.com/blakeMilner/DeepQLearning](https://github.com/blakeMilner/DeepQLearning)\r\n\r\n**OpenAI Gym: A toolkit for developing and comparing reinforcement learning algorithms**\r\n\r\n- homepage: [https://gym.openai.com/](https://gym.openai.com/)\r\n- github: [https://github.com/openai/gym](https://github.com/openai/gym)\r\n\r\n**DeeR: DEEp Reinforcement learning framework**\r\n\r\n- github: [https://github.com/VinF/deer/](https://github.com/VinF/deer/)\r\n- docs: [http://deer.readthedocs.io/en/latest/](http://deer.readthedocs.io/en/latest/)\r\n\r\n**KeRLym: A Deep Reinforcement Learning Toolbox in Keras**\r\n\r\n![](https://raw.githubusercontent.com/osh/kerlym/master/examples/example.png)\r\n\r\n- homepage: [https://oshearesearch.com/index.php/2016/06/14/kerlym-a-deep-reinforcement-learning-toolbox-in-keras/](https://oshearesearch.com/index.php/2016/06/14/kerlym-a-deep-reinforcement-learning-toolbox-in-keras/)\r\n- github: [https://github.com/osh/kerlym](https://github.com/osh/kerlym)\r\n\r\n**Pack of Drones: Layered reinforcement learning for complex behaviors**\r\n\r\n- github: [https://github.com/MickyDowns/deep-theano-rnn-lstm-car](https://github.com/MickyDowns/deep-theano-rnn-lstm-car)\r\n- youtube: [https://www.youtube.com/watch?v=WrLRGzbfeZc](https://www.youtube.com/watch?v=WrLRGzbfeZc)\r\n\r\n**RL Helicopter Game: Q-Learning and DQN Reinforcement Learning to play the Helicopter Game - Keras based!**\r\n\r\n- project page: [http://dandxy89.github.io/rf_helicopter/](http://dandxy89.github.io/rf_helicopter/)\r\n- github: [https://github.com/dandxy89/rf_helicopter](https://github.com/dandxy89/rf_helicopter)\r\n\r\n**Playing Mario with Deep Reinforcement Learning**\r\n\r\n- github: [https://github.com/aleju/mario-ai](https://github.com/aleju/mario-ai)\r\n\r\n**Deep Attention Recurrent Q-Network**\r\n\r\n- intro: Deep Reinforcement Learning Workshop, NIPS 2015. DeepHack Game\r\n- arxiv: [https://arxiv.org/abs/1512.01693](https://arxiv.org/abs/1512.01693)\r\n- github: [https://github.com/5vision/DARQN](https://github.com/5vision/DARQN)\r\n\r\n**Deep Reinforcement Learning in TensorFlow**\r\n\r\n- intro: TensorFlow implementation of Deep Reinforcement Learning papers\r\n- github: [https://github.com/carpedm20/deep-rl-tensorflow](https://github.com/carpedm20/deep-rl-tensorflow)\r\n\r\n**rltorch: A RL package for Torch that can also be used with openai gym**\r\n\r\n- github: [https://github.com/ludc/rltorch](https://github.com/ludc/rltorch)\r\n\r\n**deep_q_rl: Theano-based implementation of Deep Q-learning**\r\n\r\n- github: [https://github.com/spragunr/deep_q_rl](https://github.com/spragunr/deep_q_rl)\r\n\r\n**Reinforcement-trading**\r\n\r\n- intro: This project uses reinforcement learning on stock market and agent tries to learn trading. \r\nThe goal is to check if the agent can learn to read tape. \r\nThe project is dedicated to hero in life great Jesse Livermore.\r\n- github: [https://github.com/deependersingla/deep_trader](https://github.com/deependersingla/deep_trader)\r\n\r\n**dist-dqn：Distributed Reinforcement Learning using Deep Q-Network in TensorFlow**\r\n\r\n- github: [https://github.com/viswanathgs/dist-dqn](https://github.com/viswanathgs/dist-dqn)\r\n\r\n**Deep Reinforcement Learning for Keras**\r\n\r\n- github: [https://github.com/matthiasplappert/keras-rl](https://github.com/matthiasplappert/keras-rl)\r\n\r\n**RL4J: Reinforcement Learning for the JVM**\r\n\r\n- intro: Reinforcement learning framework integrated with deeplearning4j.\r\n- github: [https://github.com/deeplearning4j/rl4j](https://github.com/deeplearning4j/rl4j)\r\n\r\n**Teaching Your Computer To Play Super Mario Bros. – A Fork of the Google DeepMind Atari Machine Learning Project**\r\n\r\n- blog: [http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/](http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/)\r\n- github: [https://github.com/ehrenbrav/DeepQNetwork](https://github.com/ehrenbrav/DeepQNetwork)\r\n\r\n**dprl: Deep reinforcement learning package for torch7**\r\n\r\n- github: [https://github.com/PoHsunSu/dprl](https://github.com/PoHsunSu/dprl)\r\n\r\n**Reinforcement Learning for Torch: Introducing torch-twrl**\r\n\r\n- blog: [https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl](https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl)\r\n- github: [https://github.com/twitter/torch-twrl](https://github.com/twitter/torch-twrl)\r\n\r\n**Alpha Toe - Using Deep learning to master Tic-Tac-Toe - Daniel Slater**\r\n\r\n- blog: [http://www.danielslater.net/2016/10/alphatoe.html](http://www.danielslater.net/2016/10/alphatoe.html)\r\n- youtube: [https://www.youtube.com/watch?v=Meb5hApAnj4](https://www.youtube.com/watch?v=Meb5hApAnj4)\r\n- github: [https://github.com/DanielSlater/AlphaToe](https://github.com/DanielSlater/AlphaToe)\r\n\r\n**Tensorflow-Reinforce: Implementation of Reinforcement Learning Models in Tensorflow**\r\n\r\n- github: [https://github.com/yukezhu/tensorflow-reinforce](https://github.com/yukezhu/tensorflow-reinforce)\r\n\r\n**deep RL hacking on minecraft with malmo**\r\n\r\n- github: [https://github.com/matpalm/malmomo](https://github.com/matpalm/malmomo)\r\n\r\n**ReinforcementLearning**\r\n\r\n- intro: MC control, Q-learning, SARSA, Cross Entropy Method\r\n- github: [https://github.com/janivanecky/ReinforcementLearning](https://github.com/janivanecky/ReinforcementLearning)\r\n\r\n**markovjs: Reinforcement Learning in JavaScript**\r\n\r\n- github: [https://github.com/lsunsi/markovjs](https://github.com/lsunsi/markovjs)\r\n\r\n**Deep Q: Deep reinforcement learning with TensorFlow**\r\n\r\n- github: [https://github.com/tobegit3hub/deep_q](https://github.com/tobegit3hub/deep_q)\r\n\r\n**Deep Q-Learning Network in pytorch**\r\n\r\n[https://github.com/transedward/pytorch-dqn](https://github.com/transedward/pytorch-dqn)\r\n\r\n**Tensorflow-RL: Implementations of deep RL papers and random experimentation**\r\n\r\n[https://github.com/steveKapturowski/tensorflow-rl](https://github.com/steveKapturowski/tensorflow-rl)\r\n\r\n**Minimal and Clean Reinforcement Learning Examples**\r\n\r\n[https://github.com/rlcode/reinforcement-learning](https://github.com/rlcode/reinforcement-learning)\r\n\r\n**DeepRL: Highly modularized implementation of popular deep RL algorithms by PyTorch**\r\n\r\n[https://github.com/ShangtongZhang/DeepRL](https://github.com/ShangtongZhang/DeepRL)\r\n\r\n## Autonomous vehicle navigation\r\n\r\n**Self-Driving-Car-AI**\r\n\r\n- intro: A simple self-driving car AI python script using the deep Q-learning algorithm\r\n- github: [https://github.com//JianyangZhang/Self-Driving-Car-AI](https://github.com//JianyangZhang/Self-Driving-Car-AI)\r\n\r\n**Autonomous vehicle navigation based on Deep Reinforcement Learning**\r\n\r\n[https://github.com//kaihuchen/DRL-AutonomousVehicles](https://github.com//kaihuchen/DRL-AutonomousVehicles)\r\n\r\n**Car Racing using Reinforcement Learning**\r\n\r\n- intro: Stanford University\r\n- paper: [https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf](https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf)\r\n\r\n## Play Flappy Bird\r\n\r\n**Using Deep Q-Network to Learn How To Play Flappy Bird**\r\n\r\n- github: [https://github.com/yenchenlin/DeepLearningFlappyBird](https://github.com/yenchenlin/DeepLearningFlappyBird)\r\n\r\n**Playing Flappy Bird Using Deep Reinforcement Learning (Based on Deep Q Learning DQN using Tensorflow)**\r\n\r\n- blog: [http://blog.csdn.net/songrotek/article/details/50951537](http://blog.csdn.net/songrotek/article/details/50951537)\r\n- github: [https://github.com/songrotek/DRL-FlappyBird](https://github.com/songrotek/DRL-FlappyBird)\r\n\r\n**Playing Flappy Bird Using Deep Reinforcement Learning (Based on Deep Q Learning DQN)**\r\n\r\n- github: [https://github.com/li-haoran/DRL-FlappyBird](https://github.com/li-haoran/DRL-FlappyBird)\r\n\r\n**MXNET-Scala Playing Flappy Bird Using Deep Reinforcement Learning**\r\n\r\n- github: [https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/DRLFlappyBird](https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/DRLFlappyBird)\r\n\r\n**Flappy Bird Bot using Reinforcement Learning in Python**\r\n\r\n- github: [https://github.com/chncyhn/flappybird-qlearning-bot](https://github.com/chncyhn/flappybird-qlearning-bot)\r\n\r\n**Using Keras and Deep Q-Network to Play FlappyBird**\r\n\r\n- blog: [https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html](https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html)\r\n- github: [https://github.com/yanpanlau/Keras-FlappyBird](https://github.com/yanpanlau/Keras-FlappyBird)\r\n\r\n# Pong\r\n\r\n**Building a Pong playing AI in just 1 hour(plus 4 days training...)**\r\n\r\n- sildes: [https://speakerdeck.com/danielslater/building-a-pong-ai](https://speakerdeck.com/danielslater/building-a-pong-ai)\r\n- github: [https://github.com/DanielSlater/PyDataLondon2016](https://github.com/DanielSlater/PyDataLondon2016)\r\n- youtube: [https://www.youtube.com/watch?v=n8NdT_3y9oY](https://www.youtube.com/watch?v=n8NdT_3y9oY)\r\n\r\n**Pong Neural Network(LIVE)**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=Hqf__FlRlzg](https://www.youtube.com/watch?v=Hqf__FlRlzg)\r\n- github: [https://github.com/llSourcell/pong_neural_network_live](https://github.com/llSourcell/pong_neural_network_live)\r\n\r\n## Tips and Tricks\r\n\r\n**DeepRLHacks**\r\n\r\n- intro: The Nuts and Bolts of Deep RL Research\r\n- github: [https://github.com/williamFalcon/DeepRLHacks](https://github.com/williamFalcon/DeepRLHacks)\r\n\r\n# Library\r\n\r\n**BURLAP: Brown-UMBC Reinforcement Learning and Planning (BURLAP) java code library**\r\n\r\n- intro: for the use and development of single or multi-agent planning and learning algorithms and domains to accompany them\r\n- homepage: [http://burlap.cs.brown.edu/](http://burlap.cs.brown.edu/)\r\n\r\n**AgentNet: Deep Reinforcement Learning library for humans**\r\n\r\n![](https://camo.githubusercontent.com/5593b2c8184c4bc08372f919063e826d9bcc2c67/687474703a2f2f7333332e706f7374696d672e6f72672f79747836336b7763762f7768617469735f6167656e746e65745f706e672e706e67)\r\n\r\n- intro: A lightweight library to build and train deep reinforcement learning and custom recurrent networks using Theano+Lasagne \r\n- github: [https://github.com/yandexdataschool/AgentNet](https://github.com/yandexdataschool/AgentNet)\r\n\r\n**Atari Multitask & Transfer Learning Benchmark (AMTLB)**\r\n\r\n- intro: Atari gauntlet for RL agents\r\n- project page: [http://ai-on.org/projects/multitask-and-transfer-learning.html](http://ai-on.org/projects/multitask-and-transfer-learning.html)\r\n- github: [https://github.com/deontologician/atari_multitask](https://github.com/deontologician/atari_multitask)\r\n\r\n**Coach: a python reinforcement learning research framework containing implementation of many state-of-the-art algorithms**\r\n\r\n- intro: Reinforcement Learning Coach by Intel® Nervana™ enables easy experimentation with state of the art Reinforcement Learning algorithms\r\n- homepage: [http://coach.nervanasys.com/](http://coach.nervanasys.com/)\r\n- github: [https://github.com/NervanaSystems/coach](https://github.com/NervanaSystems/coach)\r\n\r\n# Blogs\r\n\r\n**Reinforcement learning’s foundational flaw**\r\n\r\n[https://thegradient.pub/why-rl-is-flawed/](https://thegradient.pub/why-rl-is-flawed/)\r\n\r\n**A Short Introduction To Some Reinforcement Learning Algorithms**\r\n\r\n[http://webdocs.cs.ualberta.ca/~vanhasse/rl_algs/rl_algs.html](http://webdocs.cs.ualberta.ca/~vanhasse/rl_algs/rl_algs.html)\r\n\r\n**A Painless Q-Learning Tutorial**\r\n\r\n[http://mnemstudio.org/path-finding-q-learning-tutorial.htm](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)\r\n\r\n- - -\r\n\r\n**Reinforcement Learning - Part 1**\r\n\r\n[http://outlace.com/Reinforcement-Learning-Part-1/](http://outlace.com/Reinforcement-Learning-Part-1/)\r\n\r\n**Reinforcement Learning - Monte Carlo Methods**\r\n\r\n[http://outlace.com/Reinforcement-Learning-Part-2/](http://outlace.com/Reinforcement-Learning-Part-2/)\r\n\r\n**Q-learning with Neural Networks**\r\n\r\n[http://outlace.com/Reinforcement-Learning-Part-3/](http://outlace.com/Reinforcement-Learning-Part-3/)\r\n\r\n- - -\r\n\r\n**Guest Post (Part I): Demystifying Deep Reinforcement Learning**\r\n\r\n[http://www.nervanasys.com/demystifying-deep-reinforcement-learning/](http://www.nervanasys.com/demystifying-deep-reinforcement-learning/)\r\n\r\n**Using reinforcement learning in Python to teach a virtual car to avoid obstacles: An experiment in Q-learning, neural networks and Pygame.**\r\n\r\n![](https://cdn-images-1.medium.com/max/800/1*Ht0KPSlYVsLUp-wqr-ab7Q.png)\r\n\r\n- blog: [https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.p8ug6snri](https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.p8ug6snri)\r\n- github: [https://github.com/harvitronix/reinforcement-learning-car](https://github.com/harvitronix/reinforcement-learning-car)\r\n\r\n**Reinforcement learning in Python to teach a virtual car to avoid obstacles — part 2**\r\n\r\n[https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.i0o643m1h](https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.i0o643m1h)\r\n\r\n**Some Reinforcement Learning Algorithms in Python, C++**\r\n\r\n- pan: [http://pan.baidu.com/s/1mhcYf3M#path=%252FImplementations%2520of%2520Some%2520Reinforcement%2520Learning%2520Algorithms](http://pan.baidu.com/s/1mhcYf3M#path=%252FImplementations%2520of%2520Some%2520Reinforcement%2520Learning%2520Algorithms)\r\n\r\n**learning to do laps with reinforcement learning and neural nets**\r\n\r\n![](http://matpalm.com/blog/imgs/2016/drivebot/walk.gif)\r\n![](http://matpalm.com/blog/imgs/2016/drivebot/runt.jpg)\r\n\r\n- blog: [http://matpalm.com/blog/drivebot/](http://matpalm.com/blog/drivebot/)\r\n- github: [https://github.com/matpalm/drivebot](https://github.com/matpalm/drivebot)\r\n\r\n**Get a taste of reinforcement learning — implement a tic tac toe agent**\r\n\r\n![](https://cdn-images-1.medium.com/max/800/1*Ntrov1dzaerfesi9vRKdow.gif)\r\n\r\n[https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4#.59bx71a2h](https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4#.59bx71a2h)\r\n\r\n**Best reinforcement learning libraries?**\r\n\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/](https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/)\r\n\r\n**Super Simple Reinforcement Learning Tutorial**\r\n\r\n- part 1: [https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149)\r\n- part 2: [https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.dyhxww1u6](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.dyhxww1u6)\r\n- part 3: [https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.r4c7i7tjq](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.r4c7i7tjq)\r\n- gist: [https://gist.github.com/awjuliani/16608e1c4968baaa692b9b8c7dd94d04](https://gist.github.com/awjuliani/16608e1c4968baaa692b9b8c7dd94d04)\r\n\r\n**Reinforcement Learning in Python**\r\n\r\n- github: [https://github.com/NathanEpstein/pydata-reinforce](https://github.com/NathanEpstein/pydata-reinforce)\r\n\r\n**The Skynet Salesman**\r\n\r\n![](http://multithreaded.stitchfix.com/assets/posts/2016-07-18-skynet-salesman/np4_5_w.gif)\r\n\r\n- keyworkds: traveling salesman problem (TSP), deep Q learning\r\n- blog: [http://multithreaded.stitchfix.com/blog/2016/07/21/skynet-salesman/](http://multithreaded.stitchfix.com/blog/2016/07/21/skynet-salesman/)\r\n- github: [https://github.com/jn2clark/ReinforcementLearning/tree/master/DeepQ](https://github.com/jn2clark/ReinforcementLearning/tree/master/DeepQ)\r\n\r\n**Apprenticeship learning using Inverse Reinforcement Learning**\r\n\r\n![](https://jangirrishabh.github.io/assets/IRL/irl_des.png)\r\n\r\n- blog: [https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/](https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/)\r\n- github: [https://github.com/jangirrishabh/toyCarIRL](https://github.com/jangirrishabh/toyCarIRL)\r\n\r\n**Reinforcement Learning and DQN, learning to play from pixels**\r\n\r\n- blog: [https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html](https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html)\r\n\r\n**Deep Learning in a Nutshell: Reinforcement Learning**\r\n\r\n[https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/)\r\n\r\n**Write an AI to win at Pong from scratch with Reinforcement Learning**\r\n\r\n[https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0#.n1pgn9chr](https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0#.n1pgn9chr)\r\n\r\n**Learning Reinforcement Learning (with Code, Exercises and Solutions)**\r\n\r\n- blog: [http://www.wildml.com/2016/10/learning-reinforcement-learning/](http://www.wildml.com/2016/10/learning-reinforcement-learning/)\r\n- github: [https://github.com/dennybritz/reinforcement-learning](https://github.com/dennybritz/reinforcement-learning)\r\n\r\n**Deep Reinforcement Learning: Playing a Racing Game**\r\n\r\n[https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html](https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html)\r\n\r\n**Experimenting with Reinforcement Learning and Active Inference**\r\n\r\n- blog: [http://www.araya.org/archives/955](http://www.araya.org/archives/955)\r\n- github: [https://github.com/arayabrain/BinarySearchLSTM](https://github.com/arayabrain/BinarySearchLSTM)\r\n\r\n**Deep reinforcement learning, battleship**\r\n\r\n- blog: [http://efavdb.com/battleship/](http://efavdb.com/battleship/)\r\n- github: [https://github.com/EFavDB/battleship](https://github.com/EFavDB/battleship)\r\n\r\n**Deep Learning Research Review Week 2: Reinforcement Learning**\r\n\r\n[https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning)\r\n\r\n**Reinforcement Learning: Artificial Intelligence in Game Playing**\r\n\r\n[https://medium.com/@pavelkordik/reinforcement-learning-the-hardest-part-of-machine-learning-b667a22995ca#.jjiitflok](https://medium.com/@pavelkordik/reinforcement-learning-the-hardest-part-of-machine-learning-b667a22995ca#.jjiitflok)\r\n\r\n**Artificial Intelligence’s Next Big Step: Reinforcement Learning**\r\n\r\n[http://thenewstack.io/reinforcement-learning-ready-real-world/](http://thenewstack.io/reinforcement-learning-ready-real-world/)\r\n\r\n## Let’s make a DQN\r\n\r\n**Let’s make a DQN**\r\n\r\n- Theory: [https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/](https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/)\r\n- Implementation: [https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/](https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/)\r\n- Debugging: [https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/](https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/)\r\n- Full DQN: [https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/](https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/)\r\n- github: [https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py](https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py)\r\n\r\n# Books\r\n\r\n**Reinforcement Learning: State-of-the-Art**\r\n\r\n- intro: \"The main goal of this book is to present an up-to-date series of survey articles \r\non the main contemporary sub-fields of reinforcement learning. \r\nThis includes surveys on partially observable environments, hierarchical task decompositions, \r\nrelational knowledge representation and predictive state representations. \r\nFurthermore, topics such as transfer, evolutionary methods and continuous spaces in reinforcement learning are surveyed. \r\nIn addition, several chapters review reinforcement learning methods in robotics, in games, and in computational neuroscience. \r\nIn total seventeen different subfields are presented by mostly young experts in those areas, \r\nand together they truly represent a state-of-the-art of current reinforcement learning research.\"\r\n- book: [http://www.springer.com/gp/book/9783642276446#](http://www.springer.com/gp/book/9783642276446#)\r\n\r\n**Reinforcement Learning: An Introduction**\r\n\r\n- github: [https://github.com/Mononofu/reinforcement-learning](https://github.com/Mononofu/reinforcement-learning)\r\n- homepage: [http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html](http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)\r\n- course: [http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html](http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html)\r\n- book(1st edition): [http://pan.baidu.com/s/1jkaMq](http://pan.baidu.com/s/1jkaMq)\r\n- book(2rd edition): [http://pan.baidu.com/s/1dDnNEnR](http://pan.baidu.com/s/1dDnNEnR)\r\n\r\n**Reinforcement Learning: An Introduction (Second edition, Draft)**\r\n\r\n- book: [https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)\r\n- mirror: [https://pan.baidu.com/s/1slrMYkP](https://pan.baidu.com/s/1slrMYkP)\r\n- github: [https://github.com/ShangtongZhang/reinforcement-learning-an-introduction](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)\r\n\r\n**The Self Learning Quant**\r\n\r\n- intro: explain and show the concept of self reinforcement learning combined with a neural network\r\n- blog: [https://medium.com/@danielzakrisson/the-self-learning-quant-d3329fcc9915#.9lsa5rh3e](https://medium.com/@danielzakrisson/the-self-learning-quant-d3329fcc9915#.9lsa5rh3e)\r\n- gihtub: [https://github.com/danielzak/sl-quant](https://github.com/danielzak/sl-quant)\r\n\r\n**Reinforcement Learning: An Introduction**\r\n\r\n- author: Richard S. Sutton and Andrew G. Barto\r\n- book: [https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html)\r\n- solutions: [https://github.com/btaba/intro-to-rl](https://github.com/btaba/intro-to-rl)\r\n\r\n# Resources\r\n\r\n**Deep Reinforcement Learning Papers**\r\n\r\n[https://github.com/junhyukoh/deep-reinforcement-learning-papers](https://github.com/junhyukoh/deep-reinforcement-learning-papers)\r\n\r\n**Awesome Reinforcement Learning**\r\n\r\n- website: [http://aikorea.org/awesome-rl/?utm_content=buffer5d0f3&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer#online-demos](http://aikorea.org/awesome-rl/?utm_content=buffer5d0f3&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer#online-demos)\r\n- github: [https://github.com/aikorea/awesome-rl](https://github.com/aikorea/awesome-rl)\r\n\r\n**Deep Reinforcement Learning Papers**\r\n\r\n- github: [https://github.com/muupan/deep-reinforcement-learning-papers](https://github.com/muupan/deep-reinforcement-learning-papers)\r\n\r\n**Deep Reinforcement Learning 深度增强学习资源**\r\n\r\n- blog: [https://zhuanlan.zhihu.com/p/20885568](https://zhuanlan.zhihu.com/p/20885568)\r\n\r\n**deep-reinforcement-learning-networks: A list of deep neural network architectures for reinforcement learning tasks**\r\n\r\n- github: [https://github.com/5vision/deep-reinforcement-learning-networks](https://github.com/5vision/deep-reinforcement-learning-networks)\r\n\r\n**Deep Reinforcement Learning survey**\r\n\r\n- github: [https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey](https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey)\r\n\r\n**Studying Reinforcement Learning Guide**\r\n\r\n- github: [https://github.com/0bserver07/Study-Reinforcement-Learning](https://github.com/0bserver07/Study-Reinforcement-Learning)\r\n\r\n# Reading and Questions\r\n\r\n**What are the best books about reinforcement learning?**\r\n\r\n[https://www.quora.com/What-are-the-best-books-about-reinforcement-learning](https://www.quora.com/What-are-the-best-books-about-reinforcement-learning)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","title":"RNN and LSTM"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: RNN and LSTM\r\ndate: 2015-10-09\r\n---\r\n\r\n# Types of RNN\r\n\r\n1) Plain Tanh Recurrent Nerual Networks\r\n\r\n2) Gated Recurrent Neural Networks (GRU)\r\n\r\n3) Long Short-Term Memory (LSTM)\r\n\r\n# Tutorials\r\n\r\n**The Unreasonable Effectiveness of Recurrent Neural Networks**\r\n\r\n- blog: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\r\n\r\n**Understanding LSTM Networks**\r\n\r\n- blog: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\r\n- blog(zh): [http://www.jianshu.com/p/9dc9f41f0b29](http://www.jianshu.com/p/9dc9f41f0b29)\r\n\r\n**A Beginner’s Guide to Recurrent Networks and LSTMs**\r\n\r\n[http://deeplearning4j.org/lstm.html](http://deeplearning4j.org/lstm.html)\r\n\r\n**A Deep Dive into Recurrent Neural Nets**\r\n\r\n[http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/](http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/)\r\n\r\n**Exploring LSTMs**\r\n\r\n[http://blog.echen.me/2017/05/30/exploring-lstms/](http://blog.echen.me/2017/05/30/exploring-lstms/)\r\n\r\n**A tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \"echo state network\" approach**\r\n\r\n- paper: [http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf)\r\n- slides: [http://deeplearning.cs.cmu.edu/notes/shaoweiwang.pdf](http://deeplearning.cs.cmu.edu/notes/shaoweiwang.pdf)\r\n\r\n**Long Short-Term Memory: Tutorial on LSTM Recurrent Networks**\r\n\r\n[http://people.idsia.ch/~juergen/lstm/index.htm](http://people.idsia.ch/~juergen/lstm/index.htm)\r\n\r\n**LSTM implementation explained**\r\n\r\n[http://apaszke.github.io/lstm-explained.html](http://apaszke.github.io/lstm-explained.html)\r\n\r\n**Recurrent Neural Networks Tutorial**\r\n\r\n- Part 1(Introduction to RNNs): [http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\r\n- Part 2(Implementing a RNN using Python and Theano): [http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)\r\n- Part 3(Understanding the Backpropagation Through Time (BPTT) algorithm): [http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)\r\n- Part 4(Implementing a GRU/LSTM RNN): [http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\r\n\r\n**Recurrent Neural Networks in DL4J**\r\n\r\n[http://deeplearning4j.org/usingrnns.html](http://deeplearning4j.org/usingrnns.html)\r\n\r\n**Learning RNN Hierarchies**\r\n\r\n![](https://cloud.githubusercontent.com/assets/8753078/11612806/46e59834-9c2e-11e5-8309-7a93aa72383c.png)\r\n\r\n- github: [https://github.com/pranv/lrh/blob/master/about.md](https://github.com/pranv/lrh/blob/master/about.md)\r\n\r\n**Element-Research Torch RNN Tutorial for recurrent neural nets : let's predict time series with a laptop GPU**\r\n\r\n- blog: [https://christopher5106.github.io/deep/learning/2016/07/14/element-research-torch-rnn-tutorial.html](https://christopher5106.github.io/deep/learning/2016/07/14/element-research-torch-rnn-tutorial.html)\r\n\r\n**RNNs in Tensorflow, a Practical Guide and Undocumented Features**\r\n\r\n- blog: [http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)\r\n\r\n**Learning about LSTMs using Torch**\r\n\r\n- blog: [http://kbullaughey.github.io/lstm-play/](http://kbullaughey.github.io/lstm-play/)\r\n- github: [https://github.com/kbullaughey/lstm-play](https://github.com/kbullaughey/lstm-play)\r\n\r\n**Build a Neural Network (LIVE)**\r\n\r\n- intro: LSTM\r\n- youtube: [https://www.youtube.com/watch?v=KvoZU-ItDiE](https://www.youtube.com/watch?v=KvoZU-ItDiE)\r\n- mirror: [https://pan.baidu.com/s/1i4KoumL](https://pan.baidu.com/s/1i4KoumL)\r\n- github: [https://github.com/llSourcell/build_a_neural_net_live](https://github.com/llSourcell/build_a_neural_net_live)\r\n\r\n**Deriving LSTM Gradient for Backpropagation**\r\n\r\n[http://wiseodd.github.io/techblog/2016/08/12/lstm-backprop/](http://wiseodd.github.io/techblog/2016/08/12/lstm-backprop/)\r\n\r\n**TensorFlow RNN Tutorial**\r\n\r\n[https://svds.com/tensorflow-rnn-tutorial/](https://svds.com/tensorflow-rnn-tutorial/)\r\n\r\n**RNN Training Tips and Tricks**\r\n\r\n[https://github.com/karpathy/char-rnn#tips-and-tricks](https://github.com/karpathy/char-rnn#tips-and-tricks)\r\n\r\n**Tips for Training Recurrent Neural Networks**\r\n\r\n[http://danijar.com/tips-for-training-recurrent-neural-networks/](http://danijar.com/tips-for-training-recurrent-neural-networks/)\r\n\r\n**A Tour of Recurrent Neural Network Algorithms for Deep Learning**\r\n\r\n[http://machinelearningmastery.com/recurrent-neural-network-algorithms-for-deep-learning/](http://machinelearningmastery.com/recurrent-neural-network-algorithms-for-deep-learning/)\r\n\r\n**Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks**\r\n\r\n[https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/)\r\n\r\n**Essentials of Deep Learning : Introduction to Long Short Term Memory**\r\n\r\n[https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/)\r\n\r\n## How to build a Recurrent Neural Network in TensorFlow\r\n\r\n**How to build a Recurrent Neural Network in TensorFlow (1/7)**\r\n\r\n[https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767#.2vozogqf7](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767#.2vozogqf7)\r\n\r\n**Using the RNN API in TensorFlow (2/7)**\r\n\r\n[https://medium.com/@erikhallstrm/tensorflow-rnn-api-2bb31821b185#.h0ycrjuo3](https://medium.com/@erikhallstrm/tensorflow-rnn-api-2bb31821b185#.h0ycrjuo3)\r\n\r\n**Using the LSTM API in TensorFlow (3/7)**\r\n\r\n[https://medium.com/@erikhallstrm/using-the-tensorflow-lstm-api-3-7-5f2b97ca6b73#.k7aciqaxn](https://medium.com/@erikhallstrm/using-the-tensorflow-lstm-api-3-7-5f2b97ca6b73#.k7aciqaxn)\r\n\r\n**Using the Multilayered LSTM API in TensorFlow (4/7)**\r\n\r\n[https://medium.com/@erikhallstrm/using-the-tensorflow-multilayered-lstm-api-f6e7da7bbe40#.dj7dy92m5](https://medium.com/@erikhallstrm/using-the-tensorflow-multilayered-lstm-api-f6e7da7bbe40#.dj7dy92m5)\r\n\r\n**Using the DynamicRNN API in TensorFlow (5/7)**\r\n\r\n[https://medium.com/@erikhallstrm/using-the-dynamicrnn-api-in-tensorflow-7237aba7f7ea#.49qw259ks](https://medium.com/@erikhallstrm/using-the-dynamicrnn-api-in-tensorflow-7237aba7f7ea#.49qw259ks)\r\n\r\n**Using the Dropout API in TensorFlow (6/7)**\r\n\r\n[https://medium.com/@erikhallstrm/using-the-dropout-api-in-tensorflow-2b2e6561dfeb#.a7mc3o9aq](https://medium.com/@erikhallstrm/using-the-dropout-api-in-tensorflow-2b2e6561dfeb#.a7mc3o9aq)\r\n\r\n## Unfolding RNNs\r\n\r\n**Unfolding RNNs: RNN : Concepts and Architectures**\r\n\r\n- blog: [http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/](http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/)\r\n\r\n**Unfolding RNNs II: Vanilla, GRU, LSTM RNNs from scratch in Tensorflow**\r\n\r\n- blog: [http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/](http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/)\r\n- github: [https://github.com/suriyadeepan/rnn-from-scratch](https://github.com/suriyadeepan/rnn-from-scratch)\r\n\r\n# Train RNN\r\n\r\n**On the difficulty of training Recurrent Neural Networks**\r\n\r\n- author: Razvan Pascanu, Tomas Mikolov, Yoshua Bengio\r\n- arxiv: [http://arxiv.org/abs/1211.5063](http://arxiv.org/abs/1211.5063)\r\n- video talks: [http://techtalks.tv/talks/on-the-difficulty-of-training-recurrent-neural-networks/58134/](http://techtalks.tv/talks/on-the-difficulty-of-training-recurrent-neural-networks/58134/)\r\n\r\n**A Simple Way to Initialize Recurrent Networks of Rectified Linear Units**\r\n\r\n- arxiv: [http://arxiv.org/abs/1504.00941](http://arxiv.org/abs/1504.00941)\r\n- gitxiv: [http://gitxiv.com/posts/7j5JXvP3kn5Jf8Waj/irnn-experiment-with-pixel-by-pixel-sequential-mnist](http://gitxiv.com/posts/7j5JXvP3kn5Jf8Waj/irnn-experiment-with-pixel-by-pixel-sequential-mnist)\r\n- github: [https://github.com/fchollet/keras/blob/master/examples/mnist_irnn.py](https://github.com/fchollet/keras/blob/master/examples/mnist_irnn.py)\r\n- github: [https://gist.github.com/GabrielPereyra/353499f2e6e407883b32](https://gist.github.com/GabrielPereyra/353499f2e6e407883b32)\r\n- blog(\"Implementing Recurrent Neural Net using chainer!\"): [http://t-satoshi.blogspot.jp/2015/06/implementing-recurrent-neural-net-using.html](http://t-satoshi.blogspot.jp/2015/06/implementing-recurrent-neural-net-using.html)\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/31rinf/150400941_a_simple_way_to_initialize_recurrent/](https://www.reddit.com/r/MachineLearning/comments/31rinf/150400941_a_simple_way_to_initialize_recurrent/)\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/32tgvw/has_anyone_been_able_to_reproduce_the_results_in/](https://www.reddit.com/r/MachineLearning/comments/32tgvw/has_anyone_been_able_to_reproduce_the_results_in/)\r\n\r\n**Batch Normalized Recurrent Neural Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1510.01378](https://arxiv.org/abs/1510.01378)\r\n\r\n**Sequence Level Training with Recurrent Neural Networks**\r\n\r\n- intro: ICLR 2016\r\n- arxiv: [http://arxiv.org/abs/1511.06732](http://arxiv.org/abs/1511.06732)\r\n- github: [https://github.com/facebookresearch/MIXER](https://github.com/facebookresearch/MIXER)\r\n- notes: [https://www.evernote.com/shard/s189/sh/ada01a82-70a9-48d4-985c-20492ab91e84/8da92be19e704996dc2b929473abed46](https://www.evernote.com/shard/s189/sh/ada01a82-70a9-48d4-985c-20492ab91e84/8da92be19e704996dc2b929473abed46)\r\n\r\n**Training Recurrent Neural Networks (PhD thesis)**\r\n\r\n- atuhor: Ilya Sutskever\r\n- thesis: [https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)\r\n\r\n**Deep learning for control using augmented Hessian-free optimization**\r\n\r\n- blog: [https://studywolf.wordpress.com/2016/04/04/deep-learning-for-control-using-augmented-hessian-free-optimization/](https://studywolf.wordpress.com/2016/04/04/deep-learning-for-control-using-augmented-hessian-free-optimization/)\r\n- github: [https://github.com/studywolf/blog/blob/master/train_AHF/train_hf.py](https://github.com/studywolf/blog/blob/master/train_AHF/train_hf.py)\r\n\r\n- - -\r\n\r\n**Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.08118](http://arxiv.org/abs/1602.08118)\r\n\r\n**Recurrent Batch Normalization**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.09025](http://arxiv.org/abs/1603.09025)\r\n- github: [https://github.com/iassael/torch-bnlstm](https://github.com/iassael/torch-bnlstm)\r\n- github: [https://github.com/cooijmanstim/recurrent-batch-normalization](https://github.com/cooijmanstim/recurrent-batch-normalization)\r\n- github(\"LSTM with Batch Normalization\"): [https://github.com/fchollet/keras/pull/2183](https://github.com/fchollet/keras/pull/2183)\r\n- github: [https://github.com/jihunchoi/recurrent-batch-normalization-pytorch](https://github.com/jihunchoi/recurrent-batch-normalization-pytorch)\r\n- notes: [http://www.shortscience.org/paper?bibtexKey=journals/corr/CooijmansBLC16](http://www.shortscience.org/paper?bibtexKey=journals/corr/CooijmansBLC16)\r\n\r\n**Batch normalized LSTM for Tensorflow**\r\n\r\n- blog: [http://olavnymoen.com/2016/07/07/rnn-batch-normalization](http://olavnymoen.com/2016/07/07/rnn-batch-normalization)\r\n- github: [https://github.com/OlavHN/bnlstm](https://github.com/OlavHN/bnlstm)\r\n\r\n**Optimizing Performance of Recurrent Neural Networks on GPUs**\r\n\r\n- arxiv: [http://arxiv.org/abs/1604.01946](http://arxiv.org/abs/1604.01946)\r\n- github: [https://github.com/parallel-forall/code-samples/blob/master/posts/rnn/LSTM.cu](https://github.com/parallel-forall/code-samples/blob/master/posts/rnn/LSTM.cu)\r\n\r\n**Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.07154](http://arxiv.org/abs/1605.07154)\r\n\r\n**Explaining and illustrating orthogonal initialization for recurrent neural networks**\r\n\r\n- blog: [http://smerity.com/articles/2016/orthogonal_init.html](http://smerity.com/articles/2016/orthogonal_init.html)\r\n\r\n**Professor Forcing: A New Algorithm for Training Recurrent Networks**\r\n\r\n- intro: NIPS 2016\r\n- arxiv: [https://arxiv.org/abs/1610.09038](https://arxiv.org/abs/1610.09038)\r\n- github: [https://github.com/anirudh9119/LM_GANS](https://github.com/anirudh9119/LM_GANS)\r\n\r\n**Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences**\r\n\r\n- intro: Selected for an oral presentation at NIPS, 2016. University of Zurich and ETH Zurich\r\n- arxiv: [https://arxiv.org/abs/1610.09513](https://arxiv.org/abs/1610.09513)\r\n- github: [https://github.com/dannyneil/public_plstm](https://github.com/dannyneil/public_plstm)\r\n- github: [https://github.com/Enny1991/PLSTM](https://github.com/Enny1991/PLSTM)\r\n- github: [https://github.com/philipperemy/tensorflow-phased-lstm](https://github.com/philipperemy/tensorflow-phased-lstm)\r\n- github: [https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/PhasedLSTMCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/PhasedLSTMCell)\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/5bmfw5/r_phased_lstm_accelerating_recurrent_network/](https://www.reddit.com/r/MachineLearning/comments/5bmfw5/r_phased_lstm_accelerating_recurrent_network/)\r\n\r\n**Tuning Recurrent Neural Networks with Reinforcement Learning (RL Tuner)**\r\n\r\n- paper: [http://openreview.net/pdf?id=BJ8fyHceg](http://openreview.net/pdf?id=BJ8fyHceg)\r\n- blog: [https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning/](https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning/)\r\n- github: [https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner](https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner)\r\n\r\n**Capacity and Trainability in Recurrent Neural Networks**\r\n\r\n- intro: Google Brain\r\n- arxiv: [https://arxiv.org/abs/1611.09913](https://arxiv.org/abs/1611.09913)\r\n\r\n**Large-Batch Training for LSTM and Beyond**\r\n\r\n- intro: UC Berkeley & UCLA & Google\r\n- paper: [https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-138.pdf](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-138.pdf)\r\n\r\n# Learn To Execute Programs\r\n\r\n**Learning to Execute**\r\n\r\n- arxiv: [http://arxiv.org/abs/1410.4615](http://arxiv.org/abs/1410.4615)\r\n- github: [https://github.com/wojciechz/learning_to_execute](https://github.com/wojciechz/learning_to_execute)\r\n- github(Tensorflow): [https://github.com/raindeer/seq2seq_experiments](https://github.com/raindeer/seq2seq_experiments)\r\n\r\n**Neural Programmer-Interpreters**\r\n\r\n<img src=\"/assets/dl-materials/rnn_lstm/NPI/add.gif\" />\r\n\r\n<img src=\"/assets/dl-materials/rnn_lstm/NPI/cars.gif\" />\r\n\r\n<img src=\"/assets/dl-materials/rnn_lstm/NPI/sort_full.gif\" />\r\n\r\n- intro:  Google DeepMind. ICLR 2016 Best Paper\r\n- arxiv: [http://arxiv.org/abs/1511.06279](http://arxiv.org/abs/1511.06279)\r\n- project page: [http://www-personal.umic (Google DeepMind. ICLR 2016 Best Paper)h.edu/~reedscot/iclr_project.html](http://www-personal.umich.edu/~reedscot/iclr_project.html)\r\n- github: [https://github.com/mokemokechicken/keras_npi](https://github.com/mokemokechicken/keras_npi)\r\n\r\n**A Programmer-Interpreter Neural Network Architecture for Prefrontal Cognitive Control**\r\n\r\n- paper: [https://www.researchgate.net/publication/273912337_A_ProgrammerInterpreter_Neural_Network_Architecture_for_Prefrontal_Cognitive_Control](https://www.researchgate.net/publication/273912337_A_ProgrammerInterpreter_Neural_Network_Architecture_for_Prefrontal_Cognitive_Control)\r\n\r\n**Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.05875](http://arxiv.org/abs/1602.05875)\r\n\r\n**Neural Random-Access Machines**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.06392](http://arxiv.org/abs/1511.06392)\r\n\r\n# Attention Models\r\n\r\n**Recurrent Models of Visual Attention**\r\n\r\n- intro: Google DeepMind. NIPS 2014\r\n- arxiv: [http://arxiv.org/abs/1406.6247](http://arxiv.org/abs/1406.6247)\r\n- data: [https://github.com/deepmind/mnist-cluttered](https://github.com/deepmind/mnist-cluttered)\r\n- github: [https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua)\r\n\r\n**Recurrent Model of Visual Attention**\r\n\r\n- intro: Google DeepMind\r\n- paper: [http://arxiv.org/abs/1406.6247](http://arxiv.org/abs/1406.6247)\r\n- gitxiv: [http://gitxiv.com/posts/ZEobCXSh23DE8a8mo/recurrent-models-of-visual-attention](http://gitxiv.com/posts/ZEobCXSh23DE8a8mo/recurrent-models-of-visual-attention)\r\n- blog: [http://torch.ch/blog/2015/09/21/rmva.html](http://torch.ch/blog/2015/09/21/rmva.html)\r\n- github: [https://github.com/Element-Research/rnn/blob/master/scripts/evaluate-rva.lua](https://github.com/Element-Research/rnn/blob/master/scripts/evaluate-rva.lua)\r\n\r\n**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**\r\n\r\n- arxiv: [http://arxiv.org/abs/1502.03044](http://arxiv.org/abs/1502.03044)\r\n- github: [https://github.com/kelvinxu/arctic-captions](https://github.com/kelvinxu/arctic-captions)\r\n\r\n**A Neural Attention Model for Abstractive Sentence Summarization**\r\n\r\n- intro: EMNLP 2015. Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1509.00685](http://arxiv.org/abs/1509.00685)\r\n- github: [https://github.com/facebook/NAMAS](https://github.com/facebook/NAMAS)\r\n\r\n**Effective Approaches to Attention-based Neural Machine Translation**\r\n\r\n- intro: EMNLP 2015\r\n- paper: [http://nlp.stanford.edu/pubs/emnlp15_attn.pdf](http://nlp.stanford.edu/pubs/emnlp15_attn.pdf)\r\n- project: [http://nlp.stanford.edu/projects/nmt/](http://nlp.stanford.edu/projects/nmt/)\r\n- github: [https://github.com/lmthang/nmt.matlab](https://github.com/lmthang/nmt.matlab)\r\n\r\n**Generating Images from Captions with Attention**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.02793](http://arxiv.org/abs/1511.02793)\r\n- github: [https://github.com/emansim/text2image](https://github.com/emansim/text2image)\r\n- demo: [http://www.cs.toronto.edu/~emansim/cap2im.html](http://www.cs.toronto.edu/~emansim/cap2im.html)\r\n\r\n**Attention and Memory in Deep Learning and NLP**\r\n\r\n- blog: [http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)\r\n\r\n**Survey on the attention based RNN model and its applications in computer vision**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.06823](http://arxiv.org/abs/1601.06823)\r\n\r\n**Attention in Long Short-Term Memory Recurrent Neural Networks**\r\n\r\n- blog: [http://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/](http://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/)\r\n\r\n**How to Visualize Your Recurrent Neural Network with Attention in Keras**\r\n\r\n- blog: [https://medium.com/datalogue/attention-in-keras-1892773a4f22](https://medium.com/datalogue/attention-in-keras-1892773a4f22)\r\n- github: [https://github.com/datalogue/keras-attention](https://github.com/datalogue/keras-attention)\r\n\r\n# Papers\r\n\r\n**Generating Sequences With Recurrent Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1308.0850](http://arxiv.org/abs/1308.0850)\r\n- github: [https://github.com/hardmaru/write-rnn-tensorflow](https://github.com/hardmaru/write-rnn-tensorflow)\r\n- github: [https://github.com/szcom/rnnlib](https://github.com/szcom/rnnlib)\r\n- blog: [http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/](http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/)\r\n\r\n**A Clockwork RNN**\r\n\r\n- arxiv: [https://arxiv.org/abs/1402.3511](https://arxiv.org/abs/1402.3511)\r\n- github: [https://github.com/makistsantekidis/clockworkrnn](https://github.com/makistsantekidis/clockworkrnn)\r\n- github: [https://github.com/zergylord/ClockworkRNN](https://github.com/zergylord/ClockworkRNN)\r\n\r\n**Unsupervised Learning of Video Representations using LSTMs**\r\n\r\n- intro: ICML 2015\r\n- project page: [http://www.cs.toronto.edu/~nitish/unsupervised_video/](http://www.cs.toronto.edu/~nitish/unsupervised_video/)\r\n- arxiv: [http://arxiv.org/abs/1502.04681](http://arxiv.org/abs/1502.04681)\r\n- code: [http://www.cs.toronto.edu/~nitish/unsupervised_video/unsup_video_lstm.tar.gz](http://www.cs.toronto.edu/~nitish/unsupervised_video/unsup_video_lstm.tar.gz)\r\n- github: [https://github.com/emansim/unsupervised-videos](https://github.com/emansim/unsupervised-videos)\r\n\r\n**An Empirical Exploration of Recurrent Network Architectures**\r\n\r\n- paper: [http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\r\n\r\n**Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks**\r\n\r\n- intro: ACL 2015. Tree RNNs aka Recursive Neural Networks\r\n- arxiv: [https://arxiv.org/abs/1503.00075](https://arxiv.org/abs/1503.00075)\r\n- slides: [http://lit.eecs.umich.edu/wp-content/uploads/2015/10/tree-lstms.pptx](http://lit.eecs.umich.edu/wp-content/uploads/2015/10/tree-lstms.pptx)\r\n- gitxiv: [http://www.gitxiv.com/posts/esrArT2iLmSfNRrto/tree-structured-long-short-term-memory-networks](http://www.gitxiv.com/posts/esrArT2iLmSfNRrto/tree-structured-long-short-term-memory-networks)\r\n- github: [https://github.com/stanfordnlp/treelstm](https://github.com/stanfordnlp/treelstm)\r\n- github: [https://github.com/ofirnachum/tree_rnn](https://github.com/ofirnachum/tree_rnn)\r\n\r\n**LSTM: A Search Space Odyssey**\r\n\r\n- arxiv: [http://arxiv.org/abs/1503.04069](http://arxiv.org/abs/1503.04069)\r\n- notes: [https://www.evernote.com/shard/s189/sh/48da42c5-8106-4f0d-b835-c203466bfac4/50d7a3c9a961aefd937fae3eebc6f540](https://www.evernote.com/shard/s189/sh/48da42c5-8106-4f0d-b835-c203466bfac4/50d7a3c9a961aefd937fae3eebc6f540)\r\n- blog(\"Dissecting the LSTM\"): [https://medium.com/jim-fleming/implementing-lstm-a-search-space-odyssey-7d50c3bacf93#.crg8pztop](https://medium.com/jim-fleming/implementing-lstm-a-search-space-odyssey-7d50c3bacf93#.crg8pztop)\r\n- github: [https://github.com/jimfleming/lstm_search](https://github.com/jimfleming/lstm_search)\r\n\r\n**Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets**\r\n\r\n- arxiv: [http://arxiv.org/abs/1503.01007](http://arxiv.org/abs/1503.01007)\r\n- github: [https://github.com/facebook/Stack-RNN](https://github.com/facebook/Stack-RNN)\r\n\r\n**A Critical Review of Recurrent Neural Networks for Sequence Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.00019](http://arxiv.org/abs/1506.00019)\r\n- review: [http://blog.terminal.com/a-thorough-and-readable-review-on-rnns/](http://blog.terminal.com/a-thorough-and-readable-review-on-rnns/)\r\n\r\n**Visualizing and Understanding Recurrent Networks**\r\n\r\n- intro: ICLR 2016. Andrej Karpathy, Justin Johnson, Fei-Fei Li\r\n- arxiv: [http://arxiv.org/abs/1506.02078](http://arxiv.org/abs/1506.02078)\r\n- slides: [http://www.robots.ox.ac.uk/~seminars/seminars/Extra/2015_07_06_AndrejKarpathy.pdf](http://www.robots.ox.ac.uk/~seminars/seminars/Extra/2015_07_06_AndrejKarpathy.pdf)\r\n- github: [https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)\r\n\r\n**Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks**\r\n\r\n- intro: Winner of MSCOCO image captioning challenge, 2015\r\n- arxiv: [http://arxiv.org/abs/1506.03099](http://arxiv.org/abs/1506.03099)\r\n\r\n**Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting**\r\n\r\n- arxiv: [https://arxiv.org/abs/1506.04214](https://arxiv.org/abs/1506.04214)\r\n- github: [https://github.com/loliverhennigh/Convolutional-LSTM-in-Tensorflow](https://github.com/loliverhennigh/Convolutional-LSTM-in-Tensorflow)\r\n\r\n**Grid Long Short-Term Memory**\r\n\r\n- arxiv: [http://arxiv.org/abs/1507.01526](http://arxiv.org/abs/1507.01526)\r\n- github(Torch7): [https://github.com/coreylynch/grid-lstm/](https://github.com/coreylynch/grid-lstm/)\r\n\r\n**Depth-Gated LSTM**\r\n\r\n- arxiv: [http://arxiv.org/abs/1508.03790](http://arxiv.org/abs/1508.03790)\r\n- github: [GitHub(dglstm.h+dglstm.cc)](https://github.com/kaishengyao/cnn/tree/master/cnn)\r\n\r\n**Deep Knowledge Tracing**\r\n\r\n- paper: [https://web.stanford.edu/~cpiech/bio/papers/deepKnowledgeTracing.pdf](https://web.stanford.edu/~cpiech/bio/papers/deepKnowledgeTracing.pdf)\r\n- github: [https://github.com/chrispiech/DeepKnowledgeTracing](https://github.com/chrispiech/DeepKnowledgeTracing)\r\n\r\n**Top-down Tree Long Short-Term Memory Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.00060](http://arxiv.org/abs/1511.00060)\r\n- github: [https://github.com/XingxingZhang/td-treelstm](https://github.com/XingxingZhang/td-treelstm)\r\n\r\n**Improving performance of recurrent neural network with relu nonlinearity**\r\n\r\n- intro: ICLR 2016\r\n- arxiv: [https://arxiv.org/abs/1511.03771](https://arxiv.org/abs/1511.03771)\r\n\r\n**Alternative structures for character-level RNNs**\r\n\r\n- intro: INRIA & Facebook AI Research. ICLR 2016\r\n- arxiv: [http://arxiv.org/abs/1511.06303](http://arxiv.org/abs/1511.06303)\r\n- github: [https://github.com/facebook/Conditional-character-based-RNN](https://github.com/facebook/Conditional-character-based-RNN)\r\n\r\n**Long Short-Term Memory-Networks for Machine Reading**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.06733](http://arxiv.org/abs/1601.06733)\r\n- github: [https://github.com/cheng6076/SNLI-attention](https://github.com/cheng6076/SNLI-attention)\r\n\r\n**Lipreading with Long Short-Term Memory**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.08188](http://arxiv.org/abs/1601.08188)\r\n\r\n**Associative Long Short-Term Memory**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.03032](http://arxiv.org/abs/1602.03032)\r\n- github: [https://github.com/mohammadpz/Associative_LSTM](https://github.com/mohammadpz/Associative_LSTM)\r\n\r\n**Representation of linguistic form and function in recurrent neural networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.08952](http://arxiv.org/abs/1602.08952)\r\n\r\n**Architectural Complexity Measures of Recurrent Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.08210](http://arxiv.org/abs/1602.08210)\r\n\r\n**Easy-First Dependency Parsing with Hierarchical Tree LSTMs**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.00375](http://arxiv.org/abs/1603.00375)\r\n\r\n**Training Input-Output Recurrent Neural Networks through Spectral Methods**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.00954](http://arxiv.org/abs/1603.00954)\r\n\r\n**Sequential Neural Models with Stochastic Layers**\r\n\r\n- arxiv: [https://arxiv.org/abs/1605.07571](https://arxiv.org/abs/1605.07571)\r\n- github: [https://github.com/marcofraccaro/srnn](https://github.com/marcofraccaro/srnn)\r\n\r\n**Neural networks with differentiable structure**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.06216](http://arxiv.org/abs/1606.06216)\r\n- github: [https://github.com/ThomasMiconi/DiffRNN](https://github.com/ThomasMiconi/DiffRNN)\r\n\r\n**What You Get Is What You See: A Visual Markup Decompiler**\r\n\r\n![](https://camo.githubusercontent.com/d5c6c528cdb25b504b1de298bc34d7109de06aea/687474703a2f2f6c73746d2e736561732e686172766172642e6564752f6c617465782f6e6574776f726b2e706e67)\r\n\r\n- project page: [http://lstm.seas.harvard.edu/latex/](http://lstm.seas.harvard.edu/latex/)\r\n- arxiv: [http://arxiv.org/abs/1609.04938](http://arxiv.org/abs/1609.04938)\r\n- github: [https://github.com/harvardnlp/im2markup](https://github.com/harvardnlp/im2markup)\r\n- github(Tensorflow): [https://github.com/ssampang/im2latex](https://github.com/ssampang/im2latex)\r\n- github: [https://github.com/opennmt/im2text](https://github.com/opennmt/im2text)\r\n- github: [https://github.com/ritheshkumar95/im2latex-tensorflow](https://github.com/ritheshkumar95/im2latex-tensorflow)\r\n\r\n**Hybrid computing using a neural network with dynamic external memory**\r\n\r\n- intro: Nature 2016\r\n- keywords: Differentiable Neural Computer (DNC)\r\n[https://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz](https://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz)\r\n- github: [https://github.com/deepmind/dnc](https://github.com/deepmind/dnc)\r\n\r\n**Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks**\r\n\r\n- project page: [https://imatge-upc.github.io/skiprnn-2017-telecombcn/](https://imatge-upc.github.io/skiprnn-2017-telecombcn/)\r\n- arxiv: [https://arxiv.org/abs/1708.06834](https://arxiv.org/abs/1708.06834)\r\n\r\n**Dilated Recurrent Neural Networks**\r\n\r\n- intro: NIPS 2017. IBM & University of Illinois at Urbana-Champaign\r\n- keywords: DilatedRNN\r\n- arxiv: [https://arxiv.org/abs/1710.02224](https://arxiv.org/abs/1710.02224)\r\n- github(Tensorflow): [https://github.com/code-terminator/DilatedRNN](https://github.com/code-terminator/DilatedRNN)\r\n[https://github.com/zalandoresearch/pt-dilate-rnn](https://github.com/zalandoresearch/pt-dilate-rnn)\r\n\r\n**Excitation Backprop for RNNs**\r\n\r\n[https://arxiv.org/abs/1711.06778](https://arxiv.org/abs/1711.06778)\r\n\r\n**Recurrent Relational Networks for Complex Relational Reasoning**\r\n\r\n- project page: [https://rasmusbergpalm.github.io/recurrent-relational-networks/](https://rasmusbergpalm.github.io/recurrent-relational-networks/)\r\n- arxiv: [https://arxiv.org/abs/1711.08028](https://arxiv.org/abs/1711.08028)\r\n- github: [https://github.com//rasmusbergpalm/recurrent-relational-networks](https://github.com//rasmusbergpalm/recurrent-relational-networks)\r\n\r\n**Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition**\r\n\r\n- intro: University of Electronic Science and Technology of China & Brown University & University of Utah & XJERA LABS PTE.LTD\r\n- arxiv: [https://arxiv.org/abs/1712.05134](https://arxiv.org/abs/1712.05134)\r\n\r\n## LSTMVis\r\n\r\n**Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks**\r\n\r\n![](https://raw.githubusercontent.com/HendrikStrobelt/LSTMVis/master/docs/img/teaser_V2_small.png)\r\n\r\n- homepage: [http://lstm.seas.harvard.edu/](http://lstm.seas.harvard.edu/)\r\n- demo: [http://lstm.seas.harvard.edu/client/index.html](http://lstm.seas.harvard.edu/client/index.html)\r\n- arxiv: [https://arxiv.org/abs/1606.07461](https://arxiv.org/abs/1606.07461)\r\n- github: [https://github.com/HendrikStrobelt/LSTMVis](https://github.com/HendrikStrobelt/LSTMVis)\r\n\r\n**Recurrent Memory Array Structures**\r\n\r\n- arxiv: [https://arxiv.org/abs/1607.03085](https://arxiv.org/abs/1607.03085)\r\n- github: [https://github.com/krocki/ArrayLSTM](https://github.com/krocki/ArrayLSTM)\r\n\r\n**Recurrent Highway Networks**\r\n\r\n- author: Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber\r\n- arxiv: [http://arxiv.org/abs/1607.03474](http://arxiv.org/abs/1607.03474)\r\n- github(Tensorflow+Torch): [https://github.com/julian121266/RecurrentHighwayNetworks/](https://github.com/julian121266/RecurrentHighwayNetworks/)\r\n\r\n**DeepSoft: A vision for a deep model of software**\r\n\r\n- arxiv: [http://arxiv.org/abs/1608.00092](http://arxiv.org/abs/1608.00092)\r\n\r\n**Recurrent Neural Networks With Limited Numerical Precision**\r\n\r\n- arxiv: [http://arxiv.org/abs/1608.06902](http://arxiv.org/abs/1608.06902)\r\n\r\n**Hierarchical Multiscale Recurrent Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.01704](http://arxiv.org/abs/1609.01704)\r\n- notes: [https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/hm-rnn.md](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/hm-rnn.md)\r\n- notes: [https://medium.com/@jimfleming/notes-on-hierarchical-multiscale-recurrent-neural-networks-7362532f3b64#.pag4kund0](https://medium.com/@jimfleming/notes-on-hierarchical-multiscale-recurrent-neural-networks-7362532f3b64#.pag4kund0)\r\n\r\n## LightRNN\r\n\r\n**LightRNN: Memory and Computation-Efficient Recurrent Neural Networks**\r\n\r\n- intro: NIPS 2016\r\n- arxiv: [https://arxiv.org/abs/1610.09893](https://arxiv.org/abs/1610.09893)\r\n\r\n**Full-Capacity Unitary Recurrent Neural Networks**\r\n\r\n- intro: NIPS 2016\r\n- arxiv: [https://arxiv.org/abs/1611.00035](https://arxiv.org/abs/1611.00035)\r\n- github: [https://github.com/stwisdom/urnn](https://github.com/stwisdom/urnn)\r\n\r\n**DeepCoder: Learning to Write Programs**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.01989](https://arxiv.org/abs/1611.01989)\r\n\r\n**shuttleNet: A biologically-inspired RNN with loop connection and parameter sharing**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.05216](https://arxiv.org/abs/1611.05216)\r\n\r\n**Tracking the World State with Recurrent Entity Networks**\r\n\r\n- intro: Facebook AI Research\r\n- arxiv: [https://arxiv.org/abs/1612.03969](https://arxiv.org/abs/1612.03969)\r\n- github(Official): [https://github.com/facebook/MemNN/tree/master/EntNet-babi](https://github.com/facebook/MemNN/tree/master/EntNet-babi)\r\n\r\n**Robust LSTM-Autoencoders for Face De-Occlusion in the Wild**\r\n\r\n- intro: National University of Singapore & Peking University\r\n- arxiv: [https://arxiv.org/abs/1612.08534](https://arxiv.org/abs/1612.08534)\r\n\r\n**Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.03441](https://arxiv.org/abs/1701.03441)\r\n- github: [https://github.com/jingweimo/Modified-LSTM](https://github.com/jingweimo/Modified-LSTM)\r\n\r\n**The Statistical Recurrent Unit**\r\n\r\n- intro: CMU\r\n- arxiv: [https://arxiv.org/abs/1703.00381](https://arxiv.org/abs/1703.00381)\r\n\r\n**Factorization tricks for LSTM networks**\r\n\r\n- intro: ICLR 2017 Workshop\r\n- arxiv: [https://arxiv.org/abs/1703.10722](https://arxiv.org/abs/1703.10722)\r\n- github: [https://github.com/okuchaiev/f-lm](https://github.com/okuchaiev/f-lm)\r\n\r\n**Bayesian Recurrent Neural Networks**\r\n\r\n- intro: UC Berkeley\r\n- arxiv: [https://arxiv.org/abs/1704.02798](https://arxiv.org/abs/1704.02798)\r\n- github: [https://github.com/mirceamironenco/BayesianRecurrentNN](https://github.com/mirceamironenco/BayesianRecurrentNN)\r\n\r\n**Fast-Slow Recurrent Neural Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1705.08639](https://arxiv.org/abs/1705.08639)\r\n- github: [https://github.com/amujika/Fast-Slow-LSTM](https://github.com/amujika/Fast-Slow-LSTM)\r\n\r\n**Visualizing LSTM decisions**\r\n\r\n[https://arxiv.org/abs/1705.08153](https://arxiv.org/abs/1705.08153)\r\n\r\n**Recurrent Additive Networks**\r\n\r\n- intro: [University of Washington & Allen Institute for Artificial Intelligence\r\n- arxiv: [https://arxiv.org/abs/1705.07393](https://arxiv.org/abs/1705.07393)\r\n- paper: [http://www.kentonl.com/pub/llz.2017.pdf](http://www.kentonl.com/pub/llz.2017.pdf)\r\n- github(PyTorch): [https://github.com/bheinzerling/ran](https://github.com/bheinzerling/ran)\r\n\r\n**Recent Advances in Recurrent Neural Networks**\r\n\r\n- intro: University of Toronto & University of Waterloo\r\n- arxiv: [https://arxiv.org/abs/1801.01078](https://arxiv.org/abs/1801.01078)\r\n\r\n**Grow and Prune Compact, Fast, and Accurate LSTMs**\r\n\r\n[https://arxiv.org/abs/1805.11797](https://arxiv.org/abs/1805.11797)\r\n\r\n# Projects\r\n\r\n**NeuralTalk (Deprecated): a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences**\r\n\r\n- github: [https://github.com/karpathy/neuraltalk](https://github.com/karpathy/neuraltalk)\r\n\r\n**NeuralTalk2: Efficient Image Captioning code in Torch, runs on GPU**\r\n\r\n- github: [https://github.com/karpathy/neuraltalk2](https://github.com/karpathy/neuraltalk2)\r\n\r\n**char-rnn in Blocks**\r\n\r\n- github: [https://github.com/johnarevalo/blocks-char-rnn](https://github.com/johnarevalo/blocks-char-rnn)\r\n\r\n**Project: pycaffe-recurrent**\r\n\r\n- code: [https://github.com/kuprel/pycaffe-recurrent/](https://github.com/kuprel/pycaffe-recurrent/)\r\n\r\n**Using neural networks for password cracking**\r\n\r\n- blog: [https://0day.work/using-neural-networks-for-password-cracking/](https://0day.work/using-neural-networks-for-password-cracking/)\r\n- github: [https://github.com/gehaxelt/RNN-Passwords](https://github.com/gehaxelt/RNN-Passwords)\r\n\r\n**torch-rnn: Efficient, reusable RNNs and LSTMs for torch**\r\n\r\n- github: [https://github.com/jcjohnson/torch-rnn](https://github.com/jcjohnson/torch-rnn)\r\n\r\n**Deploying a model trained with GPU in Torch into JavaScript, for everyone to use**\r\n\r\n- blog: [http://testuggine.ninja/blog/torch-conversion](http://testuggine.ninja/blog/torch-conversion)\r\n- demo: [http://testuggine.ninja/DRUMPF-9000/](http://testuggine.ninja/DRUMPF-9000/)\r\n- github: [https://github.com/Darktex/char-rnn](https://github.com/Darktex/char-rnn)\r\n\r\n**LSTM implementation on Caffe**\r\n\r\n- github: [https://github.com/junhyukoh/caffe-lstm](https://github.com/junhyukoh/caffe-lstm)\r\n\r\n**JNN: Java Neural Network Library**\r\n\r\n- intro: C2W model, LSTM-based Language Model, LSTM-based Part-Of-Speech-Tagger Model\r\n- github: [https://github.com/wlin12/JNN](https://github.com/wlin12/JNN)\r\n\r\n**LSTM-Autoencoder: Seq2Seq LSTM Autoencoder**\r\n\r\n- github: [https://github.com/cheng6076/LSTM-Autoencoder](https://github.com/cheng6076/LSTM-Autoencoder)\r\n\r\n**RNN Language Model Variations**\r\n\r\n- intro: Standard LSTM, Gated Feedback LSTM, 1D-Grid LSTM\r\n- github: [https://github.com/cheng6076/mlm](https://github.com/cheng6076/mlm)\r\n\r\n**keras-extra: Extra Layers for Keras to connect CNN with RNN**\r\n\r\n- github: [https://github.com/anayebi/keras-extra](https://github.com/anayebi/keras-extra)\r\n\r\n**Dynamic Vanilla RNN, GRU, LSTM,2layer Stacked LSTM with Tensorflow Higher Order Ops**\r\n\r\n- github: [https://github.com/KnHuq/Dynamic_RNN_Tensorflow](https://github.com/KnHuq/Dynamic_RNN_Tensorflow)\r\n\r\n**PRNN: A fast implementation of recurrent neural network layers in CUDA**\r\n\r\n- intro: Baidu Research\r\n- blog: [https://svail.github.io/persistent_rnns/](https://svail.github.io/persistent_rnns/)\r\n- github: [https://github.com/baidu-research/persistent-rnn](https://github.com/baidu-research/persistent-rnn)\r\n\r\n**min-char-rnn: Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy**\r\n\r\n- github: [https://github.com/weixsong/min-char-rnn](https://github.com/weixsong/min-char-rnn)\r\n\r\n**rnn: Recurrent Neural Network library for Torch7's nn**\r\n\r\n- github: [https://github.com/Element-Research/rnn](https://github.com/Element-Research/rnn)\r\n\r\n**word-rnn-tensorflow: Multi-layer Recurrent Neural Networks (LSTM, RNN) for word-level language models in Python using TensorFlow**\r\n\r\n- github: [https://github.com/hunkim/word-rnn-tensorflow](https://github.com/hunkim/word-rnn-tensorflow)\r\n\r\n**tf-char-rnn: Tensorflow implementation of char-rnn**\r\n\r\n- github: [https://github.com/shagunsodhani/tf-char-rnn](https://github.com/shagunsodhani/tf-char-rnn)\r\n\r\n**translit-rnn: Automatic transliteration with LSTM**\r\n\r\n- blog: [http://yerevann.github.io/2016/09/09/automatic-transliteration-with-lstm/](http://yerevann.github.io/2016/09/09/automatic-transliteration-with-lstm/)\r\n- github: [https://github.com/YerevaNN/translit-rnn](https://github.com/YerevaNN/translit-rnn)\r\n\r\n**tf_lstm.py: Simple implementation of LSTM in Tensorflow in 50 lines (+ 130 lines of data generation and comments)**\r\n\r\n- gist: [https://gist.github.com/nivwusquorum/b18ce332bde37e156034e5d3f60f8a23](https://gist.github.com/nivwusquorum/b18ce332bde37e156034e5d3f60f8a23)\r\n\r\n**Handwriting generating with RNN**\r\n\r\n- github: [https://github.com/Arn-O/kadenze-deep-creative-apps/blob/master/final-project/glyphs-rnn.ipynb](https://github.com/Arn-O/kadenze-deep-creative-apps/blob/master/final-project/glyphs-rnn.ipynb)\r\n\r\n**RecNet - Recurrent Neural Network Framework**\r\n\r\n- github: [https://github.com/joergfranke/recnet](https://github.com/joergfranke/recnet)\r\n\r\n# Blogs\r\n\r\n**Survey on Attention-based Models Applied in NLP**\r\n\r\n[http://yanran.li/peppypapers/2015/10/07/survey-attention-model-1.html](http://yanran.li/peppypapers/2015/10/07/survey-attention-model-1.html)\r\n\r\n**Survey on Advanced Attention-based Models**\r\n\r\n[http://yanran.li/peppypapers/2015/10/07/survey-attention-model-2.html](http://yanran.li/peppypapers/2015/10/07/survey-attention-model-2.html)\r\n\r\n**Online Representation Learning in Recurrent Neural Language Models**\r\n\r\n[http://www.marekrei.com/blog/online-representation-learning-in-recurrent-neural-language-models/](http://www.marekrei.com/blog/online-representation-learning-in-recurrent-neural-language-models/)\r\n\r\n**Fun with Recurrent Neural Nets: One More Dive into CNTK and TensorFlow**\r\n\r\n[http://esciencegroup.com/2016/03/04/fun-with-recurrent-neural-nets-one-more-dive-into-cntk-and-tensorflow/](http://esciencegroup.com/2016/03/04/fun-with-recurrent-neural-nets-one-more-dive-into-cntk-and-tensorflow/)\r\n\r\n**Materials to understand LSTM**\r\n\r\n[https://medium.com/@shiyan/materials-to-understand-lstm-34387d6454c1#.4mt3bzoau](https://medium.com/@shiyan/materials-to-understand-lstm-34387d6454c1#.4mt3bzoau)\r\n\r\n**Understanding LSTM and its diagrams**\r\n\r\n:star::star::star::star::star:\r\n\r\n- blog: [https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714](https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714)\r\n- slides: [https://github.com/shi-yan/FreeWill/blob/master/Docs/Diagrams/lstm_diagram.pptx](https://github.com/shi-yan/FreeWill/blob/master/Docs/Diagrams/lstm_diagram.pptx)\r\n\r\n**Persistent RNNs: 30 times faster RNN layers at small mini-batch sizes**\r\n\r\n**Persistent RNNs: Stashing Recurrent Weights On-Chip**\r\n\r\n- intro: Greg Diamos, Baidu Silicon Valley AI Lab\r\n- paper: [http://jmlr.org/proceedings/papers/v48/diamos16.pdf](http://jmlr.org/proceedings/papers/v48/diamos16.pdf)\r\n- blog: [http://svail.github.io/persistent_rnns/](http://svail.github.io/persistent_rnns/)\r\n- slides: [http://on-demand.gputechconf.com/gtc/2016/presentation/s6673-greg-diamos-persisten-rnns.pdf](http://on-demand.gputechconf.com/gtc/2016/presentation/s6673-greg-diamos-persisten-rnns.pdf)\r\n\r\n**All of Recurrent Neural Networks**\r\n\r\n[https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e#.q4s02elqg](https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e#.q4s02elqg)\r\n\r\n**Rolling and Unrolling RNNs**\r\n\r\n[https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/](https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/)\r\n\r\n**Sequence prediction using recurrent neural networks(LSTM) with TensorFlow: LSTM regression using TensorFlow**\r\n\r\n- blog: [http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html](http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html)\r\n- github: [https://github.com/mouradmourafiq/tensorflow-lstm-regression](https://github.com/mouradmourafiq/tensorflow-lstm-regression)\r\n\r\n**LSTMs**\r\n\r\n![](https://shapeofdata.files.wordpress.com/2016/06/lstm.png?w=640)\r\n\r\n- blog: [https://shapeofdata.wordpress.com/2016/06/04/lstms/](https://shapeofdata.wordpress.com/2016/06/04/lstms/)\r\n\r\n**Machines and Magic: Teaching Computers to Write Harry Potter**\r\n\r\n- blog: [https://medium.com/@joycex99/machines-and-magic-teaching-computers-to-write-harry-potter-37839954f252#.4fxemal9t](https://medium.com/@joycex99/machines-and-magic-teaching-computers-to-write-harry-potter-37839954f252#.4fxemal9t)\r\n- github: [https://github.com/joycex99/hp-word-model](https://github.com/joycex99/hp-word-model)\r\n\r\n**Crash Course in Recurrent Neural Networks for Deep Learning**\r\n\r\n[http://machinelearningmastery.com/crash-course-recurrent-neural-networks-deep-learning/](http://machinelearningmastery.com/crash-course-recurrent-neural-networks-deep-learning/)\r\n\r\n**Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras**\r\n\r\n[http://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/](http://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)\r\n\r\n**Recurrent Neural Networks in Tensorflow**\r\n\r\n- part I: [http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html](http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html)\r\n- part II: [http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html)\r\n\r\n**Written Memories: Understanding, Deriving and Extending the LSTM**\r\n\r\n[http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html](http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)\r\n\r\n**Attention and Augmented Recurrent Neural Networks**\r\n\r\n- blog: [http://distill.pub/2016/augmented-rnns/](http://distill.pub/2016/augmented-rnns/)\r\n- github: [https://github.com/distillpub/post--augmented-rnns](https://github.com/distillpub/post--augmented-rnns)\r\n\r\n**Interpreting and Visualizing Neural Networks for Text Processing**\r\n\r\n[https://civisanalytics.com/blog/data-science/2016/09/22/neural-network-visualization/](https://civisanalytics.com/blog/data-science/2016/09/22/neural-network-visualization/)\r\n\r\n**A simple design pattern for recurrent deep learning in TensorFlow**\r\n\r\n- blog: [https://medium.com/@devnag/a-simple-design-pattern-for-recurrent-deep-learning-in-tensorflow-37aba4e2fd6b#.homq9zsyr](https://medium.com/@devnag/a-simple-design-pattern-for-recurrent-deep-learning-in-tensorflow-37aba4e2fd6b#.homq9zsyr)\r\n- github: [https://github.com/devnag/tensorflow-bptt](https://github.com/devnag/tensorflow-bptt)\r\n\r\n**RNN Spelling Correction: To crack a nut with a sledgehammer**\r\n\r\n- blog: [https://medium.com/@yaoyaowd/rnn-spelling-correction-to-crack-a-nut-with-a-sledgehammer-7f5aa442c08c#.mc2ycyfda](https://medium.com/@yaoyaowd/rnn-spelling-correction-to-crack-a-nut-with-a-sledgehammer-7f5aa442c08c#.mc2ycyfda)\r\n\r\n**Recurrent Neural Network Gradients, and Lessons Learned Therein**\r\n\r\n- blog: [http://willwolf.io/en/2016/10/13/recurrent-neural-network-gradients-and-lessons-learned-therein/](http://willwolf.io/en/2016/10/13/recurrent-neural-network-gradients-and-lessons-learned-therein/)\r\n\r\n**A noob’s guide to implementing RNN-LSTM using Tensorflow**\r\n\r\n[http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/](http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/)\r\n\r\n**Non-Zero Initial States for Recurrent Neural Networks**\r\n\r\n- blog: [http://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html](http://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html)\r\n\r\n**Interpreting neurons in an LSTM network**\r\n\r\n[http://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-LSTM-network/](http://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-LSTM-network/)\r\n\r\n## Optimizing RNN (Baidu Silicon Valley AI Lab)\r\n\r\n**Optimizing RNN performance**\r\n\r\n- blog: [http://svail.github.io/rnn_perf/](http://svail.github.io/rnn_perf/)\r\n\r\n**Optimizing RNNs with Differentiable Graphs**\r\n\r\n- blog: [http://svail.github.io/diff_graphs/](http://svail.github.io/diff_graphs/)\r\n- notes: [http://research.baidu.com/svail-tech-notes-optimizing-rnns-differentiable-graphs/](http://research.baidu.com/svail-tech-notes-optimizing-rnns-differentiable-graphs/)\r\n\r\n# Resources\r\n\r\n**Awesome Recurrent Neural Networks - A curated list of resources dedicated to RNN**\r\n\r\n- homepage: [http://jiwonkim.org/awesome-rnn/](http://jiwonkim.org/awesome-rnn/)\r\n- github: [https://github.com/kjw0612/awesome-rnn](https://github.com/kjw0612/awesome-rnn)\r\n\r\n**Jürgen Schmidhuber's page on Recurrent Neural Networks**\r\n\r\n[http://people.idsia.ch/~juergen/rnn.html](http://people.idsia.ch/~juergen/rnn.html)\r\n\r\n# Reading and Questions\r\n\r\n**Are there any Recurrent convolutional neural network network implementations out there ?**\r\n\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/4chu3y/are_there_any_recurrent_convolutional_neural/](https://www.reddit.com/r/MachineLearning/comments/4chu3y/are_there_any_recurrent_convolutional_neural/)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","title":"Style Transfer"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Style Transfer\r\ndate: 2015-10-09\r\n---\r\n\r\n# Neural Art\r\n\r\n**A Neural Algorithm of Artistic Style**\r\n\r\n![](/assets/fun_with_dl/a_nerual_algorithm_of_artistic_style.jpg)\r\n\r\n- arxiv: [http://arxiv.org/abs/1508.06576](http://arxiv.org/abs/1508.06576)\r\n- gitxiv: [http://gitxiv.com/posts/jG46ukGod8R7Rdtud/a-neural-algorithm-of-artistic-style](http://gitxiv.com/posts/jG46ukGod8R7Rdtud/a-neural-algorithm-of-artistic-style)\r\n- github: [https://github.com/kaishengtai/neuralart](https://github.com/kaishengtai/neuralart)\r\n- github: [https://github.com/jcjohnson/neural-style](https://github.com/jcjohnson/neural-style)\r\n- github: [https://github.com/andersbll/neural_artistic_style](https://github.com/andersbll/neural_artistic_style)\r\n- ipn: [http://nbviewer.ipython.org/github/Lasagne/Recipes/blob/master/examples/styletransfer/Art%20Style%20Transfer.ipynb](http://nbviewer.ipython.org/github/Lasagne/Recipes/blob/master/examples/styletransfer/Art%20Style%20Transfer.ipynb)\r\n- github: [https://github.com/mbartoli/neural-animation](https://github.com/mbartoli/neural-animation)\r\n- github: [https://github.com/memisevic/artify](https://github.com/memisevic/artify)\r\n- github: [https://github.com/mattya/chainer-gogh](https://github.com/mattya/chainer-gogh)\r\n- github(TensorFlow): [https://github.com/anishathalye/neural-style](https://github.com/anishathalye/neural-style)\r\n- github: [https://github.com/woodrush/neural-art-tf](https://github.com/woodrush/neural-art-tf)\r\n- github: [https://github.com/dmlc/mxnet/tree/master/example/neural-style](https://github.com/dmlc/mxnet/tree/master/example/neural-style)\r\n- demo: [http://deepart.io/](http://deepart.io/)\r\n- github: [https://github.com/Teaonly/easyStyle](https://github.com/Teaonly/easyStyle)\r\n- github: [https://github.com/ckmarkoh/neuralart_tensorflow](https://github.com/ckmarkoh/neuralart_tensorflow)\r\n- github: [https://github.com/fzliu/style-transfer](https://github.com/fzliu/style-transfer)\r\n- github: [https://github.com/titu1994/Neural-Style-Transfer](https://github.com/titu1994/Neural-Style-Transfer)\r\n- github: [https://github.com/saikatbsk/Vincent-AI-Artist](https://github.com/saikatbsk/Vincent-AI-Artist)\r\n- github: [https://github.com/zhaw/neural_style](https://github.com/zhaw/neural_style)\r\n\r\n**Image Style Transfer Using Convolutional Neural Networks**\r\n\r\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\r\n\r\n**Artificial Startup Style: Neural art about startup fashion**\r\n\r\n![](https://cdn-images-1.medium.com/max/800/1*n2cmWDB42iUij8TCil9yLA.png)\r\n\r\n- blog: [https://medium.com/data-engineering/artificial-startup-style-437f6090b1f7#.8u06gq42e](https://medium.com/data-engineering/artificial-startup-style-437f6090b1f7#.8u06gq42e)\r\n\r\n**From Pixels to Paragraphs: How artistic experiments with deep learning guard us from hype**\r\n\r\n- blog: [https://medium.com/@genekogan/from-pixels-to-paragraphs-eb2763da0e9b#.er3djn9z9](https://medium.com/@genekogan/from-pixels-to-paragraphs-eb2763da0e9b#.er3djn9z9)\r\n\r\n**Experiments with style transfer**\r\n\r\n- website: [http://mtyka.github.io/code/2015/10/02/experiments-with-style-transfer.html](http://mtyka.github.io/code/2015/10/02/experiments-with-style-transfer.html)\r\n\r\n**Style Transfer for Headshot Portraits (SIGGRAPH 2014)**\r\n\r\n![](https://people.csail.mit.edu/yichangshih/portrait_web/teaser.jpg)\r\n\r\n- project: [https://people.csail.mit.edu/yichangshih/portrait_web/](https://people.csail.mit.edu/yichangshih/portrait_web/)\r\n\r\n**Teaching recurrent Neural Networks about Monet**\r\n\r\n![](/assets/fun_with_dl/keras_monet_output_sample.png)\r\n\r\n- blog: [http://blog.manugarri.com/teaching-recurrent-neural-networks-about-monet/](http://blog.manugarri.com/teaching-recurrent-neural-networks-about-monet/)\r\n- github: [https://github.com/manugarri/keras_monet](https://github.com/manugarri/keras_monet)\r\n\r\n**Content Aware Neural Style Transfer**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.04568](http://arxiv.org/abs/1601.04568)\r\n\r\n**Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis**\r\n\r\n<img src=\"https://raw.githubusercontent.com/chuanli11/CNNMRF/master/data/examples/content.jpg\" width=\"150\" />\r\n<img src=\"https://raw.githubusercontent.com/chuanli11/CNNMRF/master/data/examples/style.jpg\" width=\"150\" />\r\n<img src=\"https://raw.githubusercontent.com/chuanli11/CNNMRF/master/data/examples/Interpolation/3_balanced.png\" width=\"150\" />\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.04589](http://arxiv.org/abs/1601.04589)\r\n- github: [https://github.com/chuanli11/CNNMRF](https://github.com/chuanli11/CNNMRF)\r\n\r\n**Stylenet: Neural Network with Style Synthesis**\r\n\r\n- github: [https://github.com/machrisaa/stylenet](https://github.com/machrisaa/stylenet)\r\n\r\n**Ostagram**\r\n\r\n- intro: This program presents web-service for algorithm combining the content of one image \r\nwith the style of another image using convolutional neural networks\r\n- github: [https://github.com/SergeyMorugin/ostagram](https://github.com/SergeyMorugin/ostagram)\r\n\r\n**Exploring the Neural Algorithm of Artistic Style**\r\n\r\n- intro: A short class project report\r\n- arxiv: [http://arxiv.org/abs/1602.07188](http://arxiv.org/abs/1602.07188)\r\n\r\n**Perceptual Losses for Real-Time Style Transfer and Super-Resolution**\r\n\r\n![](https://github.com/jcjohnson/fast-neural-style/blob/master/images/webcam.gif?raw=true)\r\n\r\n- intro: Justin Johnson, Alexandre Alahi, Li Fei-Fei. ECCV 2016\r\n- arxiv: [http://arxiv.org/abs/1603.08155](http://arxiv.org/abs/1603.08155)\r\n- github: [https://github.com/jcjohnson/fast-neural-style](https://github.com/jcjohnson/fast-neural-style)\r\n- github: [https://github.com/yusuketomoto/chainer-fast-neuralstyle](https://github.com/yusuketomoto/chainer-fast-neuralstyle)\r\n- github: [https://github.com/awentzonline/keras-rtst](https://github.com/awentzonline/keras-rtst)\r\n- github(Keras): [https://github.com/titu1994/Fast-Neural-Style](https://github.com/titu1994/Fast-Neural-Style)\r\n- gtihub(Tensorflow): [https://github.com/junrushao1994/fast-neural-style.tf] (https://github.com/junrushao1994/fast-neural-style.tf)\r\n- github(PyTorch): [https://github.com/bengxy/FastNeuralStyle](https://github.com/bengxy/FastNeuralStyle)\r\n\r\n**Image transformation networks with fancy loss functions**\r\n\r\n- intro: Fast neural style in tensorflow based on [http://arxiv.org/abs/1603.08155](http://arxiv.org/abs/1603.08155)\r\n- blog: [http://olavnymoen.com/2016/07/07/image-transformation-network](http://olavnymoen.com/2016/07/07/image-transformation-network)\r\n- github: [https://github.com/OlavHN/fast-neural-style](https://github.com/OlavHN/fast-neural-style)\r\n\r\n**Improving the Neural Algorithm of Artistic Style**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.04603](http://arxiv.org/abs/1605.04603)\r\n\r\n**CubistMirror: an openframeworks app which repeatedly applies real-time style transfer on a webcam**\r\n\r\n![](https://raw.githubusercontent.com/genekogan/CubistMirror/master/photos/cubist_mirror_1.jpg)\r\n\r\n- github: [https://github.com/genekogan/CubistMirror](https://github.com/genekogan/CubistMirror)\r\n\r\n**Transfer Style But Not Color**\r\n\r\n![](https://raw.githubusercontent.com/pavelgonchar/color-independent-style-transfer/master/results-ny.jpg)\r\n\r\n- blog: [http://blog.deepart.io/2016/06/04/color-independent-style-transfer/](http://blog.deepart.io/2016/06/04/color-independent-style-transfer/)\r\n- github: [https://github.com/pavelgonchar/color-independent-style-transfer](https://github.com/pavelgonchar/color-independent-style-transfer)\r\n\r\n**neural-art-mini: Lightweight version of mxnet neural art implementation**\r\n\r\n- intro: Lightweight version of mxnet neural art implementation using ~4.8M SqueezeNet model. \r\nCompressed model is less than 500KB\r\n- github: [https://github.com/pavelgonchar/neural-art-mini](https://github.com/pavelgonchar/neural-art-mini)\r\n\r\n**Preserving Color in Neural Artistic Style Transfer**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.05897](http://arxiv.org/abs/1606.05897)\r\n\r\n**End to End Neural Art with Generative Models**\r\n\r\n![](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/art/net.png)\r\n\r\n- blog: [http://dmlc.ml/mxnet/2016/06/20/end-to-end-neural-style.html](http://dmlc.ml/mxnet/2016/06/20/end-to-end-neural-style.html)\r\n- github: [https://github.com/dmlc/mxnet/tree/master/example/neural-style](https://github.com/dmlc/mxnet/tree/master/example/neural-style)\r\n\r\n**Neural Style Explained**\r\n\r\n- blog: [http://kvfrans.com/neural-style-explained/](http://kvfrans.com/neural-style-explained/)\r\n- github: [https://github.com/kvfrans/neural-style](https://github.com/kvfrans/neural-style)\r\n\r\n**Texture Networks: Feed-forward Synthesis of Textures and Stylized Images**\r\n\r\n- intro: IMCL 2016\r\n- arxiv: [http://arxiv.org/abs/1603.03417](http://arxiv.org/abs/1603.03417)\r\n- github: [https://github.com/DmitryUlyanov/texture_nets](https://github.com/DmitryUlyanov/texture_nets)\r\n- notes: [https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/](https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/)\r\n- github: [https://github.com/tgyg-jegli/tf_texture_net](https://github.com/tgyg-jegli/tf_texture_net)\r\n\r\n**Learning Typographic Style**\r\n\r\n- arxiv: [https://arxiv.org/abs/1603.04000](https://arxiv.org/abs/1603.04000)\r\n\r\n**Instance Normalization: The Missing Ingredient for Fast Stylization**\r\n\r\n- arxiv: [http://arxiv.org/abs/1607.08022](http://arxiv.org/abs/1607.08022)\r\n\r\n**Painting style transfer for head portraits using convolutional neural networks**\r\n\r\n- paper: [http://dl.acm.org/citation.cfm?id=2925968](http://dl.acm.org/citation.cfm?id=2925968)\r\n- sci-hub: [http://dl.acm.org.sci-hub.cc/citation.cfm?doid=2897824.2925968](http://dl.acm.org.sci-hub.cc/citation.cfm?doid=2897824.2925968)\r\n\r\n**Style-Transfer via Texture-Synthesis**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.03057](http://arxiv.org/abs/1609.03057)\r\n\r\n**neural-style-tf: TensorFlow implementation of Neural Style**\r\n\r\n- github: [https://github.com/cysmith/neural-style-tf](https://github.com/cysmith/neural-style-tf)\r\n\r\n**Deep Convolutional Networks as Models of Generalization and Blending Within Visual Creativity**\r\n\r\n- intro: In Proceedings of the 7th International Conference on Computational Creativity. Palo Alto: Association for the Advancement of Artificial Intelligence (AAAI) Press (2016)\r\n- arxiv: [https://arxiv.org/abs/1610.02478](https://arxiv.org/abs/1610.02478)\r\n\r\n**A Learned Representation For Artistic Style**\r\n\r\n- intro: Google Brain\r\n- arxiv: [https://arxiv.org/abs/1610.07629](https://arxiv.org/abs/1610.07629)\r\n- blog: [https://research.googleblog.com/2016/10/supercharging-style-transfer.html](https://research.googleblog.com/2016/10/supercharging-style-transfer.html)\r\n- github: [https://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization](https://github.com/tensorflow/magenta/tree/master/magenta/models/image_stylization)\r\n- github(Tensorflow): [https://github.com/Heumi/Fast_Multi_Style_Transfer-tf](https://github.com/Heumi/Fast_Multi_Style_Transfer-tf)\r\n\r\n**How to Fake It As an Artist with Docker, AWS and Deep Learning**\r\n\r\n- blog: [https://medium.com/@lherrera/how-to-fake-it-as-an-artist-with-docker-aws-and-deep-learning-6d42f4acd890#.eq9gcgkzh](https://medium.com/@lherrera/how-to-fake-it-as-an-artist-with-docker-aws-and-deep-learning-6d42f4acd890#.eq9gcgkzh)\r\n\r\n**Multistyle Pastiche Generator**\r\n\r\n- blog: [https://magenta.tensorflow.org/2016/11/01/multistyle-pastiche-generator/](https://magenta.tensorflow.org/2016/11/01/multistyle-pastiche-generator/)\r\n\r\n**Fast Style Transfer in TensorFlow**\r\n\r\n- intro: Video Stylization, Image Stylization\r\n- github: [https://github.com/lengstrom/fast-style-transfer](https://github.com/lengstrom/fast-style-transfer)\r\n\r\n**Neural Style Transfer For Chinese Fonts**\r\n\r\n![](https://raw.githubusercontent.com/kaonashi-tyc/Rewrite/master/images/mixed_font.gif)\r\n\r\n- github: [https://github.com/kaonashi-tyc/Rewrite](https://github.com/kaonashi-tyc/Rewrite)\r\n\r\n**Neural Style Representations and the Large-Scale Classification of Artistic Style**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.05368](https://arxiv.org/abs/1611.05368)\r\n\r\n**Controlling Perceptual Factors in Neural Style Transfer**\r\n\r\n- intro: University of Tubingen & Adobe Research\r\n- arxiv: [https://arxiv.org/abs/1611.07865](https://arxiv.org/abs/1611.07865)\r\n\r\n**Awesome Typography: Statistics-Based Text Effects Transfer**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.09026](https://arxiv.org/abs/1611.09026)\r\n\r\n**Fast Patch-based Style Transfer of Arbitrary Style**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.04337](https://arxiv.org/abs/1612.04337)\r\n- github: [https://github.com/rtqichen/style-swap](https://github.com/rtqichen/style-swap)\r\n\r\n**Demystifying Neural Style Transfer**\r\n\r\n- intro: IJCAI 2017\r\n- intro: Peking University & TuSimple\r\n- arxiv: [https://arxiv.org/abs/1701.01036](https://arxiv.org/abs/1701.01036)\r\n- github: [https://github.com/lyttonhao/Neural-Style-MMD](https://github.com/lyttonhao/Neural-Style-MMD)\r\n\r\n**Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware Semantic Segmentation**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.02357](https://arxiv.org/abs/1701.02357)\r\n\r\n**Bringing Impressionism to Life with Neural Style Transfer in Come Swim**\r\n\r\n- intro: a case study of how Neural Style Transfer can be used in a movie production context\r\n- keywords: Kristen Stewart !\r\n- arxiv: [https://arxiv.org/abs/1701.04928](https://arxiv.org/abs/1701.04928)\r\n\r\n**Pytorch tutorials for Neural Style transfert**\r\n\r\n- github: [https://github.com/alexis-jacq/Pytorch-Tutorials](https://github.com/alexis-jacq/Pytorch-Tutorials)\r\n\r\n**Stable and Controllable Neural Texture Synthesis and Style Transfer Using Histogram Losses**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.08893](https://arxiv.org/abs/1701.08893)\r\n\r\n**Arbitrary Style Transfer In Real-Time With Adaptive Instance Normalization**\r\n\r\n- intro: ICCV 2017. Cornell University\r\n- paper: [https://openreview.net/pdf?id=B1fUVMzKg](https://openreview.net/pdf?id=B1fUVMzKg)\r\n- github(Torch): [https://github.com/xunhuang1995/AdaIN-style](https://github.com/xunhuang1995/AdaIN-style)\r\n- github(TensorFlow): [https://github.com/elleryqueenhomels/arbitrary_style_transfer](https://github.com/elleryqueenhomels/arbitrary_style_transfer)\r\n\r\n**Picking an optimizer for Style Transfer**\r\n\r\n- blog: [https://medium.com/slavv/picking-an-optimizer-for-style-transfer-86e7b8cba84b#.cgv2oreaq](https://medium.com/slavv/picking-an-optimizer-for-style-transfer-86e7b8cba84b#.cgv2oreaq)\r\n- github: [https://github.com/slavivanov/Style-Tranfer](https://github.com/slavivanov/Style-Tranfer)\r\n\r\n**Multi-style Generative Network for Real-time Transfer**\r\n\r\n[https://arxiv.org/abs/1703.06953](https://arxiv.org/abs/1703.06953)\r\n\r\n**Deep Photo Style Transfer**\r\n\r\n- arxiv: [https://arxiv.org/abs/1703.07511](https://arxiv.org/abs/1703.07511)\r\n- github(Torch): [https://github.com/luanfujun/deep-photo-styletransfer](https://github.com/luanfujun/deep-photo-styletransfer)\r\n- github(Docker): [https://github.com/martinbenson/deep-photo-styletransfer](https://github.com/martinbenson/deep-photo-styletransfer)\r\n\r\n**Lightweight Neural Style on Pytorch**\r\n\r\n[https://github.com/lizeng614/SqueezeNet-Neural-Style-Pytorch](https://github.com/lizeng614/SqueezeNet-Neural-Style-Pytorch)\r\n\r\n**StyleBank: An Explicit Representation for Neural Image Style Transfer**\r\n\r\n- intro: CVPR 2017\r\n- arxiv: [https://arxiv.org/abs/1703.09210](https://arxiv.org/abs/1703.09210)\r\n\r\n**How to Make an Image More Memorable? A Deep Style Transfer Approach**\r\n\r\n- intro: ACM ICMR 2017\r\n- arxiv: [https://arxiv.org/abs/1704.01745](https://arxiv.org/abs/1704.01745)\r\n\r\n**Visual Attribute Transfer through Deep Image Analogy**\r\n\r\n- intro: SIGGRAPH 2017\r\n- keywords: Deep Image Analogy\r\n- arxiv: [https://arxiv.org/abs/1705.01088](https://arxiv.org/abs/1705.01088)\r\n- github: [https://github.com/msracver/Deep-Image-Analogy](https://github.com/msracver/Deep-Image-Analogy)\r\n\r\n**Characterizing and Improving Stability in Neural Style Transfer**\r\n\r\n[https://arxiv.org/abs/1705.02092](https://arxiv.org/abs/1705.02092)\r\n\r\n**Towards Metamerism via Foveated Style Transfer**\r\n\r\n[https://arxiv.org/abs/1705.10041](https://arxiv.org/abs/1705.10041)\r\n\r\n**Style Transfer for Sketches with Enhanced Residual U-net and Auxiliary Classifier GAN**\r\n\r\n[https://arxiv.org/abs/1706.03319](https://arxiv.org/abs/1706.03319)\r\n\r\n**Meta Networks for Neural Style Transfer**\r\n\r\n- arxiv: [https://arxiv.org/abs/1709.04111](https://arxiv.org/abs/1709.04111)\r\n- github: [https://github.com/FalongShen/styletransfer](https://github.com/FalongShen/styletransfer)\r\n\r\n**Neural Color Transfer between Images**\r\n\r\n- intro: Hong Kong University of Science and Technology & Microsoft Research\r\n- arxiv: [https://arxiv.org/abs/1710.00756](https://arxiv.org/abs/1710.00756)\r\n\r\n**Improved Style Transfer by Respecting Inter-layer Correlations**\r\n\r\n[https://arxiv.org/abs/1801.01933](https://arxiv.org/abs/1801.01933)\r\n\r\n**Face Destylization**\r\n\r\n[https://arxiv.org/abs/1802.01237](https://arxiv.org/abs/1802.01237)\r\n\r\n**Unsupervised Typography Transfer**\r\n\r\n[https://arxiv.org/abs/1802.02595](https://arxiv.org/abs/1802.02595)\r\n\r\n**Stereoscopic Neural Style Transfer**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1802.10591](https://arxiv.org/abs/1802.10591)\r\n\r\n**Arbitrary Style Transfer with Deep Feature Reshuffle**\r\n\r\n[https://arxiv.org/abs/1805.04103](https://arxiv.org/abs/1805.04103)\r\n\r\n**Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration**\r\n\r\n- intro: CVPR 2018. CUHK & SenseTime\r\n- arxiv: [https://arxiv.org/abs/1805.03857](https://arxiv.org/abs/1805.03857)\r\n\r\n**Beyond Textures: Learning from Multi-domain Artistic Images for Arbitrary Style Transfer**\r\n\r\n[https://arxiv.org/abs/1805.09987](https://arxiv.org/abs/1805.09987)\r\n\r\n**A Comprehensive Comparison between Neural Style Transfer and Universal Style Transfer**\r\n\r\n[https://arxiv.org/abs/1806.00868](https://arxiv.org/abs/1806.00868)\r\n\r\n**Unbiased Image Style Transfer**\r\n\r\n[https://arxiv.org/abs/1807.01424](https://arxiv.org/abs/1807.01424)\r\n\r\n**Uncorrelated Feature Encoding for Faster Image Style Transfer**\r\n\r\n[https://arxiv.org/abs/1807.01493](https://arxiv.org/abs/1807.01493)\r\n\r\n**Adjustable Real-time Style Transfer**\r\n\r\n- intro: University of Illinois at Urbana-Champaign && Google Brain\r\n- arxiv: [https://arxiv.org/abs/1811.08560](https://arxiv.org/abs/1811.08560)\r\n\r\n**Automated Deep Photo Style Transfer**\r\n\r\n- intro: University of Tubingen\r\n- arxiv: [https://arxiv.org/abs/1901.03915](https://arxiv.org/abs/1901.03915)\r\n\r\n**Attention-aware Multi-stroke Style Transfer**\r\n\r\n[https://arxiv.org/abs/1901.05127](https://arxiv.org/abs/1901.05127)\r\n\r\n**StyleNAS: An Empirical Study of Neural Architecture Search to Uncover Surprisingly Fast End-to-End Universal Style Transfer Networks**\r\n\r\n[https://arxiv.org/abs/1906.02470](https://arxiv.org/abs/1906.02470)\r\n\r\n**StyleNAS: An Empirical Study of Neural Architecture Search to Uncover Surprisingly Fast End-to-End Universal Style Transfer Networks**\r\n\r\n[https://arxiv.org/abs/1906.02470](https://arxiv.org/abs/1906.02470)\r\n\r\n# Neural Art On Audio\r\n\r\n**MSc AI Project on generative deep networks and neural style transfer for audio**\r\n\r\n- github: [https://github.com/Fr-d-rik/generative_audio](https://github.com/Fr-d-rik/generative_audio)\r\n- project report: [https://github.com/Fr-d-rik/generative_audio/blob/master/docs/project_report.pdf](https://github.com/Fr-d-rik/generative_audio/blob/master/docs/project_report.pdf)\r\n\r\n**Neural Song Style**\r\n\r\n- intro: Audio style transfer AI\r\n- github: [https://github.com/rupeshs/neuralsongstyle](https://github.com/rupeshs/neuralsongstyle)\r\n\r\n**Time Domain Neural Audio Style Transfer**\r\n\r\n- intro: NIPS 2017\r\n- arxiv: [https://arxiv.org/abs/1711.11160](https://arxiv.org/abs/1711.11160)\r\n- github: [https://github.com//pkmital/time-domain-neural-audio-style-transfer](https://github.com//pkmital/time-domain-neural-audio-style-transfer)\r\n\r\n**Learning Linear Transformations for Fast Arbitrary Style Transfer**\r\n\r\n[https://arxiv.org/abs/1808.04537](https://arxiv.org/abs/1808.04537)\r\n\r\n# Neural Art On Video\r\n\r\n**neural-style-video**\r\n\r\n- blog: [http://larseidnes.com/2015/12/18/painting-videos-with-neural-networks/](http://larseidnes.com/2015/12/18/painting-videos-with-neural-networks/)\r\n- github: [https://github.com/larspars/neural-style-video](https://github.com/larspars/neural-style-video)\r\n\r\n**Instructions for making a Neural-Style movie**\r\n\r\n- website: [https://gist.github.com/genekogan/d61c8010d470e1dbe15d](https://gist.github.com/genekogan/d61c8010d470e1dbe15d)\r\n- sample: [https://vimeo.com/139123754](https://vimeo.com/139123754)\r\n\r\n**Artistic style transfer for videos**\r\n\r\n- arxiv: [http://arxiv.org/abs/1604.08610](http://arxiv.org/abs/1604.08610)\r\n- github: [https://github.com/manuelruder/artistic-videos](https://github.com/manuelruder/artistic-videos)\r\n\r\n**Artistic style transfer for videos and spherical images**\r\n\r\n[https://arxiv.org/abs/1708.04538](https://arxiv.org/abs/1708.04538)\r\n\r\n**How Deep Learning Can Paint Videos in the Style of Art’s Great Masters**\r\n\r\n![](https://blogs.nvidia.com/wp-content/uploads/2016/05/artistic-video-1200x528.jpg)\r\n\r\n- blog: [https://blogs.nvidia.com/blog/2016/05/25/deep-learning-paints-videos/](https://blogs.nvidia.com/blog/2016/05/25/deep-learning-paints-videos/)\r\n\r\n**DeepMovie: Using Optical Flow and Deep Neural Networks to Stylize Movies**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.08153](http://arxiv.org/abs/1605.08153)\r\n\r\n**Coherent Online Video Style Transfer**\r\n\r\n[https://arxiv.org/abs/1703.09211](https://arxiv.org/abs/1703.09211)\r\n\r\n**Laplacian-Steered Neural Style Transfer**\r\n\r\n- intro: ACM Multimedia Conference (MM) 2017\r\n- arxiv: [https://arxiv.org/abs/1707.01253](https://arxiv.org/abs/1707.01253)\r\n\r\n**Real-Time Neural Style Transfer for Videos**\r\n\r\n- intro: Tsinghua University & Tencent AI Lab\r\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf)\r\n\r\n**Multi-Content GAN for Few-Shot Font Style Transfer**\r\n\r\n[https://arxiv.org/abs/1712.00516](https://arxiv.org/abs/1712.00516)\r\n\r\n**Deep Painterly Harmonization**\r\n\r\n- arxiv: [https://arxiv.org/abs/1804.03189](https://arxiv.org/abs/1804.03189)\r\n- github: [https://github.com/luanfujun/deep-painterly-harmonization](https://github.com/luanfujun/deep-painterly-harmonization)\r\n\r\n**ReCoNet: Real-time Coherent Video Style Transfer Network**\r\n\r\n- intro: The University of Hong Kong\r\n- arixv: [https://arxiv.org/abs/1807.01197](https://arxiv.org/abs/1807.01197)\r\n- supp: [https://www.dropbox.com/s/go6f7uopjjsala7/ReCoNet%20Supplementary%20Material.pdf?dl=0](https://www.dropbox.com/s/go6f7uopjjsala7/ReCoNet%20Supplementary%20Material.pdf?dl=0)\r\n\r\n**Fully-Featured Attribute Transfer**\r\n\r\n[https://arxiv.org/abs/1902.06258](https://arxiv.org/abs/1902.06258)\r\n\r\n# Neural Doodle\r\n\r\n**Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks**\r\n\r\n- paper: [http://nucl.ai/semantic-style-transfer.pdf](http://nucl.ai/semantic-style-transfer.pdf)\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/48zstj/my_wip_implementation_of_neural_image_analogies/](https://www.reddit.com/r/MachineLearning/comments/48zstj/my_wip_implementation_of_neural_image_analogies/)\r\n- github: [https://github.com/alexjc/neural-doodle](https://github.com/alexjc/neural-doodle)\r\n\r\n**Neural Doodle**\r\n\r\n![](https://raw.githubusercontent.com/alexjc/neural-doodle/master/docs/Workflow.gif)\r\n![](https://raw.githubusercontent.com/alexjc/neural-doodle/master/docs/Landscape_example.png)\r\n\r\n- github: [https://github.com/alexjc/neural-doodle](https://github.com/alexjc/neural-doodle)\r\n\r\n**Faster neural doodle**\r\n\r\n![](https://raw.githubusercontent.com/DmitryUlyanov/fast-neural-doodle/master/data/Renoir/grid.png)\r\n\r\n- github: [https://github.com/DmitryUlyanov/fast-neural-doodle](https://github.com/DmitryUlyanov/fast-neural-doodle)\r\n\r\n**Feed-forward neural doodle**\r\n\r\n![](https://raw.githubusercontent.com/DmitryUlyanov/online-neural-doodle/master/data/starry/grid.png)\r\n\r\n- blog: [http://dmitryulyanov.github.io/feed-forward-neural-doodle/](http://dmitryulyanov.github.io/feed-forward-neural-doodle/)\r\n- github: [https://github.com/DmitryUlyanov/online-neural-doodle](https://github.com/DmitryUlyanov/online-neural-doodle)\r\n- demo: [http://likemo.net/](http://likemo.net/)\r\n\r\n**neural image analogies: Generate image analogies using neural matching and blending**\r\n\r\n![](https://raw.githubusercontent.com/awentzonline/image-analogies/master/examples/images/sugarskull-analogy.jpg)\r\n\r\n- github: [https://github.com/awentzonline/image-analogies](https://github.com/awentzonline/image-analogies)\r\n\r\n**Neural doodle with Keras**\r\n\r\n[https://github.com/fchollet/keras/blob/master/examples/neural_doodle.py](https://github.com/fchollet/keras/blob/master/examples/neural_doodle.py)\r\n\r\n# Deep Dreams\r\n\r\n**deepdream**\r\n\r\n- github: [https://github.com/google/deepdream](https://github.com/google/deepdream)\r\n\r\n**cnn-vis: Use CNNs to generate images**\r\n\r\n- github: [https://github.com/jcjohnson/cnn-vis](https://github.com/jcjohnson/cnn-vis)\r\n\r\n**bat-country: A lightweight, extendible, easy to use Python package for deep dreaming and image generation with Caffe and CNNs**\r\n\r\n- github: [https://github.com/jrosebr1/bat-country](https://github.com/jrosebr1/bat-country)\r\n\r\n**DeepDreaming with TensorFlow**\r\n\r\n- ipn: [http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb](http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb)\r\n\r\n**deepdraw**\r\n\r\n- github: [https://github.com/auduno/deepdraw](https://github.com/auduno/deepdraw)\r\n\r\n**Understanding Deep Dreams**\r\n\r\n- blog: [http://www.alanzucconi.com/2015/07/06/live-your-deepdream-how-to-recreate-the-inceptionism-effect/](http://www.alanzucconi.com/2015/07/06/live-your-deepdream-how-to-recreate-the-inceptionism-effect/)\r\n\r\n**Generating Deep Dreams**\r\n\r\n- blog：[http://www.alanzucconi.com/2016/05/25/generating-deep-dreams/](http://www.alanzucconi.com/2016/05/25/generating-deep-dreams/)\r\n\r\n**Audio Deepdream: Optimizing Raw Audio With Convolutional Networks**\r\n\r\n- intro: Google Brain\r\n- paper: [https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/08/ardila-audio.pdf](https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/08/ardila-audio.pdf)\r\n- examples: [https://drive.google.com/drive/folders/0B7NUtSSG_AgqZWF1ajdhSjhEMlk](https://drive.google.com/drive/folders/0B7NUtSSG_AgqZWF1ajdhSjhEMlk)\r\n\r\n# Image Stylization\r\n\r\n**Automatic Portrait Segmentation for Image Stylization**\r\n\r\n![](http://xiaoyongshen.me/webpage_portrait/figures/teaser.png)\r\n\r\n- intro: The Chinese University of Hong Kong & Adobe Research\r\n- project page(data+code): [http://xiaoyongshen.me/webpage_portrait/index.html](http://xiaoyongshen.me/webpage_portrait/index.html)\r\n- paper: [http://www.cse.cuhk.edu.hk/leojia/papers/portrait_eg16.pdf](http://www.cse.cuhk.edu.hk/leojia/papers/portrait_eg16.pdf)\r\n- github(Tensorflow): [https://github.com/PetroWu/AutoPortraitMatting](https://github.com/PetroWu/AutoPortraitMatting)\r\n\r\n**Transfiguring Portraits**\r\n\r\n![](/assets/fun_with_dl/Transfiguring_Portraits.jpg)\r\n\r\n- paper: [http://homes.cs.washington.edu/~kemelmi/Transfiguring_Portraits_Kemelmacher_SIGGRAPH2016.pdf](http://homes.cs.washington.edu/~kemelmi/Transfiguring_Portraits_Kemelmacher_SIGGRAPH2016.pdf)\r\n\r\n**Stylize Aesthetic QR Code**\r\n\r\n- intro: Zhengzhou University & Zhejiang University\r\n- arxiv: [https://arxiv.org/abs/1803.01146](https://arxiv.org/abs/1803.01146)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","title":"Super-Resolution"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Super-Resolution\ndate: 2015-10-09\n---\n\n# Papers\n\n**Super-Resolution.Benckmark**\n\n- intro: Benchmark and resources for single super-resolution algorithms\n- github: [https://github.com/huangzehao/Super-Resolution.Benckmark](https://github.com/huangzehao/Super-Resolution.Benckmark)\n\n**Image Super-Resolution Using Deep Convolutional Networks**\n\n- intro: Microsoft Research\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html](http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html)\n- arxiv: [http://arxiv.org/abs/1501.00092](http://arxiv.org/abs/1501.00092)\n- training code: [http://mmlab.ie.cuhk.edu.hk/projects/SRCNN/SRCNN_train.zip](http://mmlab.ie.cuhk.edu.hk/projects/SRCNN/SRCNN_train.zip)\n- test code: [http://mmlab.ie.cuhk.edu.hk/projects/SRCNN/SRCNN_v1.zip](http://mmlab.ie.cuhk.edu.hk/projects/SRCNN/SRCNN_v1.zip)\n- github(Keras): [https://github.com/titu1994/Image-Super-Resolution](https://github.com/titu1994/Image-Super-Resolution)\n\n**Learning a Deep Convolutional Network for Image Super-Resolution**\n\n- Baidu-pan: [http://pan.baidu.com/s/1c0k0wRu](http://pan.baidu.com/s/1c0k0wRu)\n\n**Shepard Convolutional Neural Networks**\n\n- paper: [https://papers.nips.cc/paper/5774-shepard-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/5774-shepard-convolutional-neural-networks.pdf)\n- github: [https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/applications/Shepard_CNN](https://github.com/jimmy-ren/vcnn_double-bladed/tree/master/applications/Shepard_CNN)\n\n**Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution**\n\n- intro: NIPS 2015\n- paper: [https://papers.nips.cc/paper/5778-bidirectional-recurrent-convolutional-networks-for-multi-frame-super-resolution](https://papers.nips.cc/paper/5778-bidirectional-recurrent-convolutional-networks-for-multi-frame-super-resolution)\n\n**Deeply-Recursive Convolutional Network for Image Super-Resolution**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1511.04491](http://arxiv.org/abs/1511.04491)\n- paper: [http://cv.snu.ac.kr/publication/conf/2016/DRCN_CVPR2016.pdf](http://cv.snu.ac.kr/publication/conf/2016/DRCN_CVPR2016.pdf)\n\n**Accurate Image Super-Resolution Using Very Deep Convolutional Networks**\n\n- intro: CVPR 2016 Oral\n- project page: [http://cv.snu.ac.kr/research/VDSR/](http://cv.snu.ac.kr/research/VDSR/)\n- arxiv: [http://arxiv.org/abs/1511.04587](http://arxiv.org/abs/1511.04587)\n- code: [http://cv.snu.ac.kr/research/VDSR/VDSR_code.zip](http://cv.snu.ac.kr/research/VDSR/VDSR_code.zip)\n- github: [https://github.com/huangzehao/caffe-vdsr](https://github.com/huangzehao/caffe-vdsr)\n- github(Torch): [https://github.com/pby5/vdsr_torch](https://github.com/pby5/vdsr_torch)\n\n**Super-Resolution with Deep Convolutional Sufficient Statistics**\n\n- arxiv: [http://arxiv.org/abs/1511.05666](http://arxiv.org/abs/1511.05666)\n\n**Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network**\n\n- arxiv: [http://arxiv.org/abs/1607.01977](http://arxiv.org/abs/1607.01977)\n\n**Local- and Holistic- Structure Preserving Image Super Resolution via Deep Joint Component Learning**\n\n- arxiv: [http://arxiv.org/abs/1607.07220](http://arxiv.org/abs/1607.07220)\n\n**End-to-End Image Super-Resolution via Deep and Shallow Convolutional Networks**\n\n- arxiv: [http://arxiv.org/abs/1607.07680](http://arxiv.org/abs/1607.07680)\n\n**Accelerating the Super-Resolution Convolutional Neural Network**\n\n- intro: speed up of more than 40 times with even superior restoration quality, real-time performance on a generic CPU\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html](http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html)\n- arxiv: [http://arxiv.org/abs/1608.00367](http://arxiv.org/abs/1608.00367)\n\n**srez: Image super-resolution through deep learning**\n\n- github: [https://github.com/david-gpu/srez](https://github.com/david-gpu/srez)\n\n**Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network**\n\n- intro: CVPR 2017 Oral\n- arxiv: [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802)\n- github: [https://github.com/tensorlayer/SRGAN](https://github.com/tensorlayer/SRGAN)\n- github(Torch): [https://github.com/leehomyc/Photo-Realistic-Super-Resoluton](https://github.com/leehomyc/Photo-Realistic-Super-Resoluton)\n- github: [https://github.com/junhocho/SRGAN](https://github.com/junhocho/SRGAN)\n- github(Keras): [https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks](https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks)\n- github: [https://github.com/buriburisuri/SRGAN](https://github.com/buriburisuri/SRGAN)\n\n**Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1609.05158](http://arxiv.org/abs/1609.05158)\n- github: [https://github.com/Tetrachrome/subpixel](https://github.com/Tetrachrome/subpixel)\n\n**Is the deconvolution layer the same as a convolutional layer?**\n\n- intro: A note on Real­Time Single Image and Video Super­Resolution Using an Efficient Sub­Pixel Convolutional Neural Network.\n- arxiv: [http://arxiv.org/abs/1609.07009](http://arxiv.org/abs/1609.07009)\n\n**Amortised MAP Inference for Image Super-resolution**\n\n- arxiv: [https://arxiv.org/abs/1610.04490](https://arxiv.org/abs/1610.04490)\n\n**Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation**\n\n- arxiv: [https://arxiv.org/abs/1611.05250](https://arxiv.org/abs/1611.05250)\n\n**Super-Resolution on Satellite Imagery using Deep Learning**\n\n- part 1: [https://medium.com/the-downlinq/super-resolution-on-satellite-imagery-using-deep-learning-part-1-ec5c5cd3cd2#.4oxn9pafu](https://medium.com/the-downlinq/super-resolution-on-satellite-imagery-using-deep-learning-part-1-ec5c5cd3cd2#.4oxn9pafu)\n\n**Neural Enhance: Super Resolution for images using deep learning.**\n\n- github: [https://github.com/alexjc/neural-enhance](https://github.com/alexjc/neural-enhance)\n- docker: [https://github.com/alexjc/neural-enhance/blob/master/docker-cpu.df](https://github.com/alexjc/neural-enhance/blob/master/docker-cpu.df)\n\n**Texture Enhancement via High-Resolution Style Transfer for Single-Image Super-Resolution**\n\n- intro: Digital Media & Communications R&D Center, Samsung Electronics, Seoul, Korea\n- arxiv: [https://arxiv.org/abs/1612.00085](https://arxiv.org/abs/1612.00085)\n\n**EnhanceNet: Single Image Super-Resolution through Automated Texture Synthesis**\n\n- arxiv: [https://arxiv.org/abs/1612.07919](https://arxiv.org/abs/1612.07919)\n\n**Learning a Mixture of Deep Networks for Single Image Super-Resolution**\n\n- project page: [http://www.ifp.illinois.edu/~dingliu2/accv2016/](http://www.ifp.illinois.edu/~dingliu2/accv2016/)\n- arxiv: [https://arxiv.org/abs/1701.00823](https://arxiv.org/abs/1701.00823)\n- code: [http://www.ifp.illinois.edu/~dingliu2/accv2016/codes/python_accv2016.zip](http://www.ifp.illinois.edu/~dingliu2/accv2016/codes/python_accv2016.zip)\n\n**Dual Recovery Network with Online Compensation for Image Super-Resolution**\n\n- arxiv: [https://arxiv.org/abs/1701.05652](https://arxiv.org/abs/1701.05652)\n\n**Super-resolution Using Constrained Deep Texture Synthesis**\n\n- intro: Brown University & Georgia Institute of Technology\n- arxiv: [https://arxiv.org/abs/1701.07604](https://arxiv.org/abs/1701.07604)\n\n**Pixel Recursive Super Resolution**\n\n- arxiv: [https://arxiv.org/abs/1702.00783](https://arxiv.org/abs/1702.00783)\n- github(Tensorflow): [https://github.com/nilboy/pixel-recursive-super-resolution](https://github.com/nilboy/pixel-recursive-super-resolution)\n\n**GUN: Gradual Upsampling Network for single image super-resolution**\n\n- arxiv: [https://arxiv.org/abs/1703.04244](https://arxiv.org/abs/1703.04244)\n\n**Single Image Super-resolution with a Parameter Economic Residual-like Convolutional Neural Network**\n\n- intro: Extentions of mmm 2017 paper\n- arxiv: [https://arxiv.org/abs/1703.08173](https://arxiv.org/abs/1703.08173)\n\n**Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution**\n\n- intro: CVPR 2017\n- project page(code+dataset): [http://vllab1.ucmerced.edu/~wlai24/LapSRN/](http://vllab1.ucmerced.edu/~wlai24/LapSRN/)\n- arxiv: [https://arxiv.org/abs/1704.03915](https://arxiv.org/abs/1704.03915)\n- github(Matlab+MatConvNet): [https://github.com/phoenix104104/LapSRN](https://github.com/phoenix104104/LapSRN)\n\n**Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks**\n\n- project page: [http://vllab.ucmerced.edu/wlai24/LapSRN/](http://vllab.ucmerced.edu/wlai24/LapSRN/)\n- arxiv: [https://arxiv.org/abs/1710.01992](https://arxiv.org/abs/1710.01992)\n- github: [https://github.com/phoenix104104/LapSRN](https://github.com/phoenix104104/LapSRN)\n\n**Single Image Super-Resolution Using Multi-Scale Convolutional Neural Network**\n\n- intro: South China University of Technology\n- arxiv: [https://arxiv.org/abs/1705.05084](https://arxiv.org/abs/1705.05084)\n\n**Super-Resolution via Deep Learning**\n\n- intro: COMSATS Institute of IT (CIIT)\n- arxiv: [https://arxiv.org/abs/1706.09077](https://arxiv.org/abs/1706.09077)\n\n**High-Quality Face Image SR Using Conditional Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1707.00737](https://arxiv.org/abs/1707.00737)\n\n**Enhanced Deep Residual Networks for Single Image Super-Resolution**\n\n- intro: CVPR 2017 workshop. Best paper award of the NTIRE2017 workshop, and the winners of the NTIRE2017 Challenge on Single Image Super-Resolution\n- arxiv: [https://arxiv.org/abs/1707.02921](https://arxiv.org/abs/1707.02921)\n- paper: [http://cv.snu.ac.kr/publication/conf/2017/EDSR_fixed.pdf](http://cv.snu.ac.kr/publication/conf/2017/EDSR_fixed.pdf)\n- github: [https://github.com/LimBee/NTIRE2017](https://github.com/LimBee/NTIRE2017)\n\n**Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network**\n\n- arxiv: [https://arxiv.org/abs/1707.05425](https://arxiv.org/abs/1707.05425)\n- github(Tensorflow): [https://github.com/jiny2001/dcscn-super-resolution](https://github.com/jiny2001/dcscn-super-resolution)\n\n**Single Image Super-Resolution with Dilated Convolution based Multi-Scale Information Learning Inception Module**\n\n- intro: ICIP 2017\n- arxiv: [https://arxiv.org/abs/1707.07128](https://arxiv.org/abs/1707.07128)\n\n**Attention-Aware Face Hallucination via Deep Reinforcement Learning**\n\n[https://arxiv.org/abs/1708.03132](https://arxiv.org/abs/1708.03132)\n\n**CISRDCNN: Super-resolution of compressed images using deep convolutional neural networks**\n\n[https://arxiv.org/abs/1709.06229](https://arxiv.org/abs/1709.06229)\n\n**Deep Inception-Residual Laplacian Pyramid Networks for Accurate Single Image Super-Resolution**\n\n- intro: Chongqing University\n- arxiv: [https://arxiv.org/abs/1711.05431](https://arxiv.org/abs/1711.05431)\n\n**D-PCN: Parallel Convolutional Neural Networks for Image Recognition in Reverse Adversarial Style**\n\n{https://arxiv.org/abs/1711.04237}(https://arxiv.org/abs/1711.04237)\n\n**CT-SRCNN: Cascade Trained and Trimmed Deep Convolutional Neural Networks for Image Super Resolution**\n\n- intro: IEEE Winter Conf. on Applications of Computer Vision (WACV) 2018, Lake Tahoe, USA\n- arxiv: [https://arxiv.org/abs/1711.04048](https://arxiv.org/abs/1711.04048)\n\n**FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors**\n\n- intro: CVPR 2018 spotlight\n- arxiv: [https://arxiv.org/abs/1711.10703](https://arxiv.org/abs/1711.10703)\n- github: [https://github.com/tyshiwo/FSRNet](https://github.com/tyshiwo/FSRNet)\n\n**A Frequency Domain Neural Network for Fast Image Super-resolution**\n\n[https://arxiv.org/abs/1712.03037](https://arxiv.org/abs/1712.03037)\n\n**SRPGAN: Perceptual Generative Adversarial Network for Single Image Super Resolution**\n\n[https://arxiv.org/abs/1712.05927](https://arxiv.org/abs/1712.05927)\n\n**\"Zero-Shot\" Super-Resolution using Deep Internal Learning**\n\n- project page: [http://www.wisdom.weizmann.ac.il/~vision/zssr/](http://www.wisdom.weizmann.ac.il/~vision/zssr/)\n- arxiv: [https://arxiv.org/abs/1712.06087](https://arxiv.org/abs/1712.06087)\n- github: [https://github.com/jacobgil/pytorch-zssr](https://github.com/jacobgil/pytorch-zssr)\n\n**Super-Resolution with Deep Adaptive Image Resampling**\n\n[https://arxiv.org/abs/1712.06463](https://arxiv.org/abs/1712.06463)\n\n**SRPGAN: Perceptual Generative Adversarial Network for Single Image Super Resolution**\n\n- intro: Peking Univeristy\n- arxiv: [https://arxiv.org/abs/1712.05927](https://arxiv.org/abs/1712.05927)\n\n**SESR: Single Image Super Resolution with Recursive Squeeze and Excitation Networks**\n\n- intro: ICPR 2018\n- arxiv: [https://arxiv.org/abs/1801.10319](https://arxiv.org/abs/1801.10319)\n\n**Deep Image Super Resolution via Natural Image Priors**\n\n[https://arxiv.org/abs/1802.02721](https://arxiv.org/abs/1802.02721)\n\n**Residual Dense Network for Image Super-Resolution**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1802.08797](https://arxiv.org/abs/1802.08797)\n\n**Deep Back-Projection Networks For Super-Resolution**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.02735](https://arxiv.org/abs/1803.02735)\n\n**Fast, Accurate, and, Lightweight Super-Resolution with Cascading Residual Network**\n\n[https://arxiv.org/abs/1803.08664](https://arxiv.org/abs/1803.08664)\n\n**Fast and Accurate Single Image Super-Resolution via Information Distillation Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.09454](https://arxiv.org/abs/1803.09454)\n\n**Deep Residual Networks with a Fully Connected Recon-struction Layer for Single Image Super-Resolution**\n\n[https://arxiv.org/abs/1805.10143](https://arxiv.org/abs/1805.10143)\n\n**Adaptive Importance Learning for Improving Lightweight Image Super-resolution Network**\n\n[https://arxiv.org/abs/1806.01576](https://arxiv.org/abs/1806.01576)\n\n**Image Super-Resolution Using Very Deep Residual Channel Attention Networks**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.02758](https://arxiv.org/abs/1807.02758)\n\n**An Attention-Based Approach for Single Image Super Resolution**\n\n[https://arxiv.org/abs/1807.06779](https://arxiv.org/abs/1807.06779)\n\n**The Unreasonable Effectiveness of Texture Transfer for Single Image Super-resolution**\n\n[https://arxiv.org/abs/1808.00043](https://arxiv.org/abs/1808.00043)\n\n**Deep Learning for Single Image Super-Resolution: A Brief Review**\n\n[https://arxiv.org/abs/1808.03344](https://arxiv.org/abs/1808.03344)\n\n**Improving Super-Resolution Methods via Incremental Residual Learning**\n\n[https://arxiv.org/abs/1808.07110](https://arxiv.org/abs/1808.07110)\n\n**Deep Learning-based Image Super-Resolution Considering Quantitative and Perceptual Quality**\n\n- intro: Won the 2nd place for Region 2 in the PIRM Challenge on Perceptual Super Resolution at ECCV 2018\n- arxiv: [https://arxiv.org/abs/1809.04789](https://arxiv.org/abs/1809.04789)\n- github: [https://github.com/idearibosome/tf-perceptual-eusr](https://github.com/idearibosome/tf-perceptual-eusr)\n\n**Generative adversarial network-based image super-resolution using perceptual content losses**\n\n- intro: Won the 2nd place for Region 1 in the PIRM Challenge on Perceptual Super Resolution at ECCV 2018\n- arxiv: [https://arxiv.org/abs/1809.04783](https://arxiv.org/abs/1809.04783)\n\n**Channel-wise and Spatial Feature Modulation Network for Single Image Super-Resolution**\n\n[https://arxiv.org/abs/1809.11130](https://arxiv.org/abs/1809.11130)\n\n**Triple Attention Mixed Link Network for Single Image Super Resolution**\n\n[https://arxiv.org/abs/1810.03254](https://arxiv.org/abs/1810.03254)\n\n**Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search**\n\n[https://arxiv.org/abs/1901.07261](https://arxiv.org/abs/1901.07261)\n\n**Feedback Network for Image Super-Resolution**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.09814](https://arxiv.org/abs/1903.09814)\n- github(official, Pytorch): [https://github.com/Paper99/SRFBN_CVPR19](https://github.com/Paper99/SRFBN_CVPR19)\n\n## Video Super-resolution\n\n**Detail-revealing Deep Video Super-resolution**\n\n- arxiv: [https://arxiv.org/abs/1704.02738](https://arxiv.org/abs/1704.02738)\n- github: [https://github.com/jiangsutx/SPMC_VideoSR](https://github.com/jiangsutx/SPMC_VideoSR)\n\n**End-to-End Learning of Video Super-Resolution with Motion Compensation**\n\n- intro: GCPR 2017\n- arxiv: [https://arxiv.org/abs/1707.00471](https://arxiv.org/abs/1707.00471)\n\n**Frame-Recurrent Video Super-Resolution**\n\n[https://arxiv.org/abs/1801.04590](https://arxiv.org/abs/1801.04590)\n\n**Photorealistic Video Super Resolution**\n\n[https://arxiv.org/abs/1807.07930](https://arxiv.org/abs/1807.07930)\n\n**Recurrent Back-Projection Network for Video Super-Resolution**\n\n- intro: CVPR 2019\n- keywords: Vimeo90k\n- project page: [https://alterzero.github.io/projects/RBPN.html](https://alterzero.github.io/projects/RBPN.html)\n- arxiv: [https://arxiv.org/abs/1903.10128](https://arxiv.org/abs/1903.10128)\n- github: [https://github.com/alterzero/RBPN-PyTorch](https://github.com/alterzero/RBPN-PyTorch)\n\n**Fast Spatio-Temporal Residual Network for Video Super-Resolution**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.02870](https://arxiv.org/abs/1904.02870)\n\n# Projects\n\n**waifu2x**\n\n- intro: Image Super-Resolution for Anime-Style Art\n- github: [https://github.com/nagadomi/waifu2x](https://github.com/nagadomi/waifu2x)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","title":"Tracking"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Tracking\r\ndate: 2015-10-09\r\n---\r\n\r\n**Learning A Deep Compact Image Representation for Visual Tracking**\r\n\r\n- intro: NIPS 2013\r\n- intro: DLT\r\n- project page: [http://winsty.net/dlt.html](http://winsty.net/dlt.html)\r\n\r\n**Hierarchical Convolutional Features for Visual Tracking**\r\n\r\n- intro: ICCV 2015\r\n- project page: [https://sites.google.com/site/jbhuang0604/publications/cf2](https://sites.google.com/site/jbhuang0604/publications/cf2)\r\n- github: [https://github.com/jbhuang0604/CF2](https://github.com/jbhuang0604/CF2)\r\n\r\n**Robust Visual Tracking via Convolutional Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1501.04505](http://arxiv.org/abs/1501.04505)\r\n- paper: [http://kaihuazhang.net/CNT.pdf](http://kaihuazhang.net/CNT.pdf)\r\n- code: [http://kaihuazhang.net/CNT_matlab.rar](http://kaihuazhang.net/CNT_matlab.rar)\r\n\r\n**Transferring Rich Feature Hierarchies for Robust Visual Tracking**\r\n\r\n- intro: SO-DLT\r\n- arxiv: [http://arxiv.org/abs/1501.04587](http://arxiv.org/abs/1501.04587)\r\n- slides: [http://valse.mmcheng.net/ftp/20150325/RVT.pptx](http://valse.mmcheng.net/ftp/20150325/RVT.pptx)\r\n\r\n**Learning Multi-Domain Convolutional Neural Networks for Visual Tracking**\r\n\r\n- intro: The Winner of The VOT2015 Challenge\r\n- keywords: Multi-Domain Network (MDNet)\r\n- homepage: [http://cvlab.postech.ac.kr/research/mdnet/](http://cvlab.postech.ac.kr/research/mdnet/)\r\n- arxiv: [http://arxiv.org/abs/1510.07945](http://arxiv.org/abs/1510.07945)\r\n- github: [https://github.com/HyeonseobNam/MDNet](https://github.com/HyeonseobNam/MDNet)\r\n\r\n**RATM: Recurrent Attentive Tracking Model**\r\n\r\n- arxiv: [http://arxiv.org/abs/1510.08660](http://arxiv.org/abs/1510.08660)\r\n- github: [https://github.com/saebrahimi/RATM](https://github.com/saebrahimi/RATM)\r\n\r\n**Understanding and Diagnosing Visual Tracking Systems**\r\n\r\n![](http://winsty.net/diagnose/pipeline.png)\r\n\r\n- intro: ICCV 2015\r\n- project page: [http://winsty.net/tracker_diagnose.html](http://winsty.net/tracker_diagnose.html)\r\n- paper: [http://winsty.net/papers/diagnose.pdf](http://winsty.net/papers/diagnose.pdf)\r\n- code(Matlab): [http://120.52.72.43/winsty.net/c3pr90ntcsf0/diagnose/diagnose_code.zip](http://120.52.72.43/winsty.net/c3pr90ntcsf0/diagnose/diagnose_code.zip)\r\n\r\n**Recurrently Target-Attending Tracking**\r\n\r\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cui_Recurrently_Target-Attending_Tracking_CVPR_2016_paper.html](http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cui_Recurrently_Target-Attending_Tracking_CVPR_2016_paper.html)\r\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cui_Recurrently_Target-Attending_Tracking_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cui_Recurrently_Target-Attending_Tracking_CVPR_2016_paper.pdf)\r\n\r\n**Visual Tracking with Fully Convolutional Networks**\r\n\r\n- intro: ICCV 2015\r\n- paper: [http://202.118.75.4/lu/Paper/ICCV2015/iccv15_lijun.pdf](http://202.118.75.4/lu/Paper/ICCV2015/iccv15_lijun.pdf)\r\n- github: [https://github.com/scott89/FCNT](https://github.com/scott89/FCNT)\r\n\r\n**Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks**\r\n\r\n- intro: AAAI 2016\r\n- arxiv: [http://arxiv.org/abs/1602.00991](http://arxiv.org/abs/1602.00991)\r\n- github: [https://github.com/pondruska/DeepTracking](https://github.com/pondruska/DeepTracking)\r\n\r\n**Learning to Track at 100 FPS with Deep Regression Networks**\r\n\r\n![](http://davheld.github.io/GOTURN/pull7f-web_e2.png)\r\n\r\n- intro: ECCV 2015\r\n- intro: GOTURN: Generic Object Tracking Using Regression Networks\r\n- project page: [http://davheld.github.io/GOTURN/GOTURN.html](http://davheld.github.io/GOTURN/GOTURN.html)\r\n- arxiv: [http://arxiv.org/abs/1604.01802](http://arxiv.org/abs/1604.01802)\r\n- github: [https://github.com/davheld/GOTURN](https://github.com/davheld/GOTURN)\r\n\r\n**Learning by tracking: Siamese CNN for robust target association**\r\n\r\n- arxiv: [http://arxiv.org/abs/1604.07866](http://arxiv.org/abs/1604.07866)\r\n\r\n**Fully-Convolutional Siamese Networks for Object Tracking**\r\n\r\n![](http://www.robots.ox.ac.uk/~luca/stuff/siamesefc_conv-explicit.jpg)\r\n\r\n- intro: ECCV 2016\r\n- intro: State-of-the-art performance in arbitrary object tracking at 50-100 FPS with Fully Convolutional Siamese networks\r\n- project page: [http://www.robots.ox.ac.uk/~luca/siamese-fc.html](http://www.robots.ox.ac.uk/~luca/siamese-fc.html)\r\n- arxiv: [http://arxiv.org/abs/1606.09549](http://arxiv.org/abs/1606.09549)\r\n- github(official): [https://github.com/bertinetto/siamese-fc](https://github.com/bertinetto/siamese-fc)\r\n- github(official): [https://github.com/torrvision/siamfc-tf](https://github.com/torrvision/siamfc-tf)\r\n- valse-video: [http://www.iqiyi.com/w_19ruirwrel.html#vfrm=8-8-0-1](http://www.iqiyi.com/w_19ruirwrel.html#vfrm=8-8-0-1)\r\n\r\n**Hedged Deep Tracking**\r\n\r\n- project page(paper+code): [https://sites.google.com/site/yuankiqi/hdt](https://sites.google.com/site/yuankiqi/hdt)\r\n- paper: [https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnx5dWFua2lxaXxneDoxZjc2MmYwZGIzNjFhYTRl](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnx5dWFua2lxaXxneDoxZjc2MmYwZGIzNjFhYTRl)\r\n\r\n**Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking**\r\n\r\n![](http://guanghan.info/projects/ROLO/overview.jpeg)\r\n\r\n- intro: ROLO is short for Recurrent YOLO, aimed at simultaneous object detection and tracking\r\n- project page: [http://guanghan.info/projects/ROLO/](http://guanghan.info/projects/ROLO/)\r\n- arxiv: [http://arxiv.org/abs/1607.05781](http://arxiv.org/abs/1607.05781)\r\n- github: [https://github.com/Guanghan/ROLO](https://github.com/Guanghan/ROLO)\r\n\r\n**Visual Tracking via Shallow and Deep Collaborative Model**\r\n\r\n- arxiv: [http://arxiv.org/abs/1607.08040](http://arxiv.org/abs/1607.08040)\r\n\r\n**Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking**\r\n\r\n![](http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/method_fig.jpg)\r\n\r\n- intro: ECCV 2016\r\n- intro: OTB-2015 (+5.1% in mean OP), Temple-Color (+4.6% in mean OP), and VOT2015 (20% relative reduction in failure rate)\r\n- keywords: Continuous Convolution Operator Tracker (C-COT)\r\n- project page: [http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html](http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html)\r\n- arxiv: [http://arxiv.org/abs/1608.03773](http://arxiv.org/abs/1608.03773)\r\n- github(MATLAB): [https://github.com/martin-danelljan/Continuous-ConvOp](https://github.com/martin-danelljan/Continuous-ConvOp)\r\n\r\n**Unsupervised Learning from Continuous Video in a Scalable Predictive Recurrent Network**\r\n\r\n- keywords: Predictive Vision Model (PVM)\r\n- arxiv: [http://arxiv.org/abs/1607.06854](http://arxiv.org/abs/1607.06854)\r\n- github: [https://github.com/braincorp/PVM](https://github.com/braincorp/PVM)\r\n\r\n**Modeling and Propagating CNNs in a Tree Structure for Visual Tracking**\r\n\r\n- arxiv: [http://arxiv.org/abs/1608.07242](http://arxiv.org/abs/1608.07242)\r\n\r\n**Robust Scale Adaptive Kernel Correlation Filter Tracker With Hierarchical Convolutional Features**\r\n\r\n- paper: [http://ieeexplore.ieee.org/document/7496863/](http://ieeexplore.ieee.org/document/7496863/)\r\n\r\n**Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1609.09365](https://arxiv.org/abs/1609.09365)\r\n\r\n**OTB Results: visual tracker benchmark results**\r\n\r\n- github: [https://github.com/foolwood/benchmark_results](https://github.com/foolwood/benchmark_results)\r\n\r\n**Convolutional Regression for Visual Tracking**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.04215](https://arxiv.org/abs/1611.04215)\r\n\r\n**Semantic tracking: Single-target tracking with inter-supervised convolutional networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.06395](https://arxiv.org/abs/1611.06395)\r\n\r\n**SANet: Structure-Aware Network for Visual Tracking**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.06878](https://arxiv.org/abs/1611.06878)\r\n\r\n**ECO: Efficient Convolution Operators for Tracking**\r\n\r\n![](http://www.cvl.isy.liu.se/research/objrec/visualtracking/ecotrack/method_fig.jpg)\r\n\r\n- intro: CVPR 2017\r\n- project page: [http://www.cvl.isy.liu.se/research/objrec/visualtracking/ecotrack/index.html](http://www.cvl.isy.liu.se/research/objrec/visualtracking/ecotrack/index.html)\r\n- arxiv: [https://arxiv.org/abs/1611.09224](https://arxiv.org/abs/1611.09224)\r\n- github: [https://github.com/martin-danelljan/ECO](https://github.com/martin-danelljan/ECO)\r\n\r\n**Dual Deep Network for Visual Tracking**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.06053](https://arxiv.org/abs/1612.06053)\r\n\r\n**Deep Motion Features for Visual Tracking**\r\n\r\n- intro: ICPR 2016. Best paper award in the \"Computer Vision and Robot Vision\" track\r\n- arxiv: [https://arxiv.org/abs/1612.06615](https://arxiv.org/abs/1612.06615)\r\n\r\n**Globally Optimal Object Tracking with Fully Convolutional Networks**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.08274](https://arxiv.org/abs/1612.08274)\r\n\r\n**Robust and Real-time Deep Tracking Via Multi-Scale Domain Adaptation**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.00561](https://arxiv.org/abs/1701.00561)\r\n- bitbucket: [https://bitbucket.org/xinke_wang/msdat](https://bitbucket.org/xinke_wang/msdat)\r\n\r\n**Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term Dependencies**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.01909](https://arxiv.org/abs/1701.01909)\r\n\r\n**Large Margin Object Tracking with Circulant Feature Maps**\r\n\r\n- intro: CVPR 2017\r\n- intro: The experimental results demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms on the challenging benchmark sequences while runs at speed in excess of 80 frames per secon\r\n- arxiv: [https://arxiv.org/abs/1703.05020](https://arxiv.org/abs/1703.05020)\r\n- notes: [https://zhuanlan.zhihu.com/p/25761718](https://zhuanlan.zhihu.com/p/25761718)\r\n\r\n**DCFNet: Discriminant Correlation Filters Network for Visual Tracking**\r\n\r\n- arxiv: [https://arxiv.org/abs/1704.04057](https://arxiv.org/abs/1704.04057)\r\n- github: [https://github.com/foolwood/DCFNet](https://github.com/foolwood/DCFNet)\r\n\r\n**End-to-end representation learning for Correlation Filter based tracking**\r\n\r\n- intro: CVPR 2017. University of Oxford\r\n- intro: Training a Correlation Filter end-to-end allows lightweight networks of 2 layers (600 kB) to achieve state-of-the-art performance in tracking, at high-speed.\r\n- project page: [http://www.robots.ox.ac.uk/~luca/cfnet.html](http://www.robots.ox.ac.uk/~luca/cfnet.html)\r\n- arxiv: [https://arxiv.org/abs/1704.06036](https://arxiv.org/abs/1704.06036)\r\n- gtihub: [https://github.com/bertinetto/cfnet](https://github.com/bertinetto/cfnet)\r\n\r\n**Context-Aware Correlation Filter Tracking**\r\n\r\n- intro: CVPR 2017 Oral\r\n- project page: [https://ivul.kaust.edu.sa/Pages/pub-ca-cf-tracking.aspx](https://ivul.kaust.edu.sa/Pages/pub-ca-cf-tracking.aspx)\r\n- paper: [https://ivul.kaust.edu.sa/Documents/Publications/2017/Context-Aware%20Correlation%20Filter%20Tracking.pdf](https://ivul.kaust.edu.sa/Documents/Publications/2017/Context-Aware%20Correlation%20Filter%20Tracking.pdf)\r\n- github: [https://github.com/thias15/Context-Aware-CF-Tracking](https://github.com/thias15/Context-Aware-CF-Tracking)\r\n\r\n**Robust Multi-view Pedestrian Tracking Using Neural Networks**\r\n\r\n[https://arxiv.org/abs/1704.06370](https://arxiv.org/abs/1704.06370)\r\n\r\n**Re3 : Real-Time Recurrent Regression Networks for Object Tracking**\r\n\r\n- intro: University of Washington\r\n- arxiv: [https://arxiv.org/abs/1705.06368](https://arxiv.org/abs/1705.06368)\r\n- demo: [https://www.youtube.com/watch?v=PC0txGaYz2I](https://www.youtube.com/watch?v=PC0txGaYz2I)\r\n\r\n**Robust Tracking Using Region Proposal Networks**\r\n\r\n[https://arxiv.org/abs/1705.10447](https://arxiv.org/abs/1705.10447)\r\n\r\n**Hierarchical Attentive Recurrent Tracking**\r\n\r\n- intro: NIPS 2017. University of Oxford\r\n- arxiv: [https://arxiv.org/abs/1706.09262](https://arxiv.org/abs/1706.09262)\r\n- github: [https://github.com/akosiorek/hart](https://github.com/akosiorek/hart)\r\n- results: [https://youtu.be/Vvkjm0FRGSs](https://youtu.be/Vvkjm0FRGSs)\r\n\r\n**Siamese Learning Visual Tracking: A Survey**\r\n\r\n[https://arxiv.org/abs/1707.00569](https://arxiv.org/abs/1707.00569)\r\n\r\n**Robust Visual Tracking via Hierarchical Convolutional Features**\r\n\r\n- project page: [https://sites.google.com/site/chaoma99/hcft-tracking](https://sites.google.com/site/chaoma99/hcft-tracking)\r\n- arxiv: [https://arxiv.org/abs/1707.03816](https://arxiv.org/abs/1707.03816)\r\n- github: [https://github.com/chaoma99/HCFTstar](https://github.com/chaoma99/HCFTstar)\r\n\r\n**CREST: Convolutional Residual Learning for Visual Tracking**\r\n\r\n- intro: ICCV 2017\r\n- project page: [http://www.cs.cityu.edu.hk/~yibisong/iccv17/index.html](http://www.cs.cityu.edu.hk/~yibisong/iccv17/index.html)\r\n- arxiv: [https://arxiv.org/abs/1708.00225](https://arxiv.org/abs/1708.00225)\r\n- github: [https://github.com/ybsong00/CREST-Release](https://github.com/ybsong00/CREST-Release)\r\n\r\n**Learning Policies for Adaptive Tracking with Deep Feature Cascades**\r\n\r\n- intro: ICCV 2017 Spotlight\r\n- arxiv: [https://arxiv.org/abs/1708.02973](https://arxiv.org/abs/1708.02973)\r\n\r\n**Recurrent Filter Learning for Visual Tracking**\r\n\r\n- intro: ICCV 2017 Workshop on VOT\r\n- arxiv: [https://arxiv.org/abs/1708.03874](https://arxiv.org/abs/1708.03874)\r\n\r\n**Correlation Filters with Weighted Convolution Responses**\r\n\r\n- intro: ICCV 2017 workshop. 5th visual object tracking(VOT) tracker CFWCR\r\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/He_Correlation_Filters_With_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/He_Correlation_Filters_With_ICCV_2017_paper.pdf)\r\n- github: [https://github.com/he010103/CFWCR](https://github.com/he010103/CFWCR)\r\n\r\n**Semantic Texture for Robust Dense Tracking**\r\n\r\n[https://arxiv.org/abs/1708.08844](https://arxiv.org/abs/1708.08844)\r\n\r\n**Learning Multi-frame Visual Representation for Joint Detection and Tracking of Small Objects**\r\n\r\n**Differentiating Objects by Motion: Joint Detection and Tracking of Small Flying Objects**\r\n\r\n[https://arxiv.org/abs/1709.04666](https://arxiv.org/abs/1709.04666)\r\n\r\n**Tracking Persons-of-Interest via Unsupervised Representation Adaptation**\r\n\r\n- intro: Northwestern Polytechnical University & Virginia Tech & Hanyang University\r\n- keywords: Multi-face tracking\r\n- project page: [http://vllab1.ucmerced.edu/~szhang/FaceTracking/](http://vllab1.ucmerced.edu/~szhang/FaceTracking/)\r\n- arxiv: [https://arxiv.org/abs/1710.02139](https://arxiv.org/abs/1710.02139)\r\n\r\n**End-to-end Flow Correlation Tracking with Spatial-temporal Attention**\r\n\r\n[https://arxiv.org/abs/1711.01124](https://arxiv.org/abs/1711.01124)\r\n\r\n**UCT: Learning Unified Convolutional Networks for Real-time Visual Tracking**\r\n\r\n- intro: ICCV 2017 Workshops\r\n- arxiv: [https://arxiv.org/abs/1711.04661](https://arxiv.org/abs/1711.04661)\r\n\r\n**Pixel-wise object tracking**\r\n\r\n[https://arxiv.org/abs/1711.07377](https://arxiv.org/abs/1711.07377)\r\n\r\n**MAVOT: Memory-Augmented Video Object Tracking**\r\n\r\n[https://arxiv.org/abs/1711.09414](https://arxiv.org/abs/1711.09414)\r\n\r\n**Learning Hierarchical Features for Visual Object Tracking with Recursive Neural Networks**\r\n\r\n[https://arxiv.org/abs/1801.02021](https://arxiv.org/abs/1801.02021)\r\n\r\n**Parallel Tracking and Verifying**\r\n\r\n[https://arxiv.org/abs/1801.10496](https://arxiv.org/abs/1801.10496)\r\n\r\n**Saliency-Enhanced Robust Visual Tracking**\r\n\r\n[https://arxiv.org/abs/1802.02783](https://arxiv.org/abs/1802.02783)\r\n\r\n**A Twofold Siamese Network for Real-Time Object Tracking**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1802.08817](https://arxiv.org/abs/1802.08817)\r\n\r\n**Learning Dynamic Memory Networks for Object Tracking**\r\n\r\n[https://arxiv.org/abs/1803.07268](https://arxiv.org/abs/1803.07268)\r\n\r\n**Context-aware Deep Feature Compression for High-speed Visual Tracking**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1803.10537](https://arxiv.org/abs/1803.10537)\r\n\r\n**VITAL: VIsual Tracking via Adversarial Learning**\r\n\r\n- intro: CVPR 2018 Spotlight\r\n- arixv: [https://arxiv.org/abs/1804.04273](https://arxiv.org/abs/1804.04273)\r\n\r\n**Unveiling the Power of Deep Tracking**\r\n\r\n[https://arxiv.org/abs/1804.06833](https://arxiv.org/abs/1804.06833)\r\n\r\n**A Novel Low-cost FPGA-based Real-time Object Tracking System**\r\n\r\n- intro: ASICON 2017\r\n- arxiv: [https://arxiv.org/abs/1804.05535](https://arxiv.org/abs/1804.05535)\r\n\r\n**MV-YOLO: Motion Vector-aided Tracking by Semantic Object Detection**\r\n\r\n[https://arxiv.org/abs/1805.00107](https://arxiv.org/abs/1805.00107)\r\n\r\n**Information-Maximizing Sampling to Promote Tracking-by-Detection**\r\n\r\n[https://arxiv.org/abs/1806.02523](https://arxiv.org/abs/1806.02523)\r\n\r\n**Instance Segmentation and Tracking with Cosine Embeddings and Recurrent Hourglass Networks**\r\n\r\n- intro: MICCAI 2018\r\n- arxiv: [https://arxiv.org/abs/1806.02070](https://arxiv.org/abs/1806.02070)\r\n\r\n**Stochastic Channel Decorrelation Network and Its Application to Visual Tracking**\r\n\r\n[https://arxiv.org/abs/1807.01103](https://arxiv.org/abs/1807.01103)\r\n\r\n**Fast Dynamic Convolutional Neural Networks for Visual Tracking**\r\n\r\n[https://arxiv.org/abs/1807.03132](https://arxiv.org/abs/1807.03132)\r\n\r\n**DeepTAM: Deep Tracking and Mapping**\r\n\r\n[https://arxiv.org/abs/1808.01900](https://arxiv.org/abs/1808.01900)\r\n\r\n**Distractor-aware Siamese Networks for Visual Object Tracking**\r\n\r\n- intro: ECCV 2018\r\n- keywords: DaSiamRPN\r\n- arxiv: [https://arxiv.org/abs/1808.06048](https://arxiv.org/abs/1808.06048)\r\n- github: [https://github.com/foolwood/DaSiamRPN](https://github.com/foolwood/DaSiamRPN)\r\n\r\n**Multi-Branch Siamese Networks with Online Selection for Object Tracking**\r\n\r\n- intro: ISVC 2018 oral\r\n- arxiv: [https://arxiv.org/abs/1808.07349](https://arxiv.org/abs/1808.07349)\r\n\r\n**Real-Time MDNet**\r\n\r\n- intro: ECCV 2018\r\n- arxiv: [https://arxiv.org/abs/1808.08834](https://arxiv.org/abs/1808.08834)\r\n\r\n**Towards a Better Match in Siamese Network Based Visual Object Tracker**\r\n\r\n- intro: ECCV Visual Object Tracking Challenge Workshop VOT2018\r\n- arxiv: [https://arxiv.org/abs/1809.01368](https://arxiv.org/abs/1809.01368)\r\n\r\n**DensSiam: End-to-End Densely-Siamese Network with Self-Attention Model for Object Tracking**\r\n\r\n- intro: ISVC 2018\r\n- arxiv: [https://arxiv.org/abs/1809.02714](https://arxiv.org/abs/1809.02714)\r\n\r\n**Deformable Object Tracking with Gated Fusion**\r\n\r\n[https://arxiv.org/abs/1809.10417](https://arxiv.org/abs/1809.10417)\r\n\r\n**Deep Attentive Tracking via Reciprocative Learning**\r\n\r\n- intro: NIPS 2018\r\n- project page: [https://ybsong00.github.io/nips18_tracking/index](https://ybsong00.github.io/nips18_tracking/index)\r\n- arxiv: [https://arxiv.org/abs/1810.03851](https://arxiv.org/abs/1810.03851)\r\n- github: [https://github.com/shipubupt/NIPS2018](https://github.com/shipubupt/NIPS2018)\r\n\r\n**Online Visual Robot Tracking and Identification using Deep LSTM Networks**\r\n\r\n- intro: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, Canada, 2017. IROS RoboCup Best Paper Award\r\n- arxiv: [https://arxiv.org/abs/1810.04941](https://arxiv.org/abs/1810.04941)\r\n\r\n**Detect or Track: Towards Cost-Effective Video Object Detection/Tracking**\r\n\r\n- intro: AAAI 2019\r\n- arxiv: [https://arxiv.org/abs/1811.05340](https://arxiv.org/abs/1811.05340)\r\n\r\n**Deep Siamese Networks with Bayesian non-Parametrics for Video Object Tracking**\r\n\r\n[https://arxiv.org/abs/1811.07386](https://arxiv.org/abs/1811.07386)\r\n\r\n**Fast Online Object Tracking and Segmentation: A Unifying Approach**\r\n\r\n- intro: CVPR 2019\r\n- preject page: [http://www.robots.ox.ac.uk/~qwang/SiamMask/](http://www.robots.ox.ac.uk/~qwang/SiamMask/)\r\n- arxiv: [https://arxiv.org/abs/1812.05050](https://arxiv.org/abs/1812.05050)\r\n- github: [https://github.com/foolwood/SiamMask](https://github.com/foolwood/SiamMask)\r\n\r\n**Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking**\r\n\r\n- intro: Temple University\r\n- arxiv: [https://arxiv.org/abs/1812.06148](https://arxiv.org/abs/1812.06148)\r\n\r\n**Handcrafted and Deep Trackers: A Review of Recent Object Tracking Approaches**\r\n\r\n[https://arxiv.org/abs/1812.07368](https://arxiv.org/abs/1812.07368)\r\n\r\n**SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks**\r\n\r\n[https://arxiv.org/abs/1812.11703](https://arxiv.org/abs/1812.11703)\r\n\r\n**Deeper and Wider Siamese Networks for Real-Time Visual Tracking**\r\n\r\n[https://arxiv.org/abs/1901.01660](https://arxiv.org/abs/1901.01660)\r\n\r\n**SiamVGG: Visual Tracking using Deeper Siamese Networks**\r\n\r\n[https://arxiv.org/abs/1902.02804](https://arxiv.org/abs/1902.02804)\r\n\r\n**TrackNet: Simultaneous Object Detection and Tracking and Its Application in Traffic Video Analysis**\r\n\r\n[https://arxiv.org/abs/1902.01466](https://arxiv.org/abs/1902.01466)\r\n\r\n**Target-Aware Deep Tracking**\r\n\r\n- intro: CVPR 2019\r\n- intro: 1Harbin Institute of Technology &  Shanghai Jiao Tong University & Tencent AI Lab & University of California & Google Cloud AI\r\n- arxiv: [https://arxiv.org/abs/1904.01772](https://arxiv.org/abs/1904.01772)\r\n\r\n**Unsupervised Deep Tracking**\r\n\r\n- intro: CVPR 2019\r\n- intro: USTC & Tencent AI Lab & Shanghai Jiao Tong University\r\n- arxiv: [https://arxiv.org/abs/1904.01828](https://arxiv.org/abs/1904.01828)\r\n- github: [https://github.com/594422814/UDT](https://github.com/594422814/UDT)\r\n- github: [https://github.com/594422814/UDT_pytorch](https://github.com/594422814/UDT_pytorch)\r\n\r\n**Generic Multiview Visual Tracking**\r\n\r\n[https://arxiv.org/abs/1904.02553](https://arxiv.org/abs/1904.02553)\r\n\r\n**SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking**\r\n\r\n- intro: CVPR 2019\r\n- arxiv: [https://arxiv.org/abs/1904.04452](https://arxiv.org/abs/1904.04452)\r\n\r\n**A Strong Feature Representation for Siamese Network Tracker**\r\n\r\n[https://arxiv.org/abs/1907.07880](https://arxiv.org/abs/1907.07880)\r\n\r\n**Visual Tracking via Dynamic Memory Networks**\r\n\r\n- intro: TPAMI 2019\r\n- arxiv: [https://arxiv.org/abs/1907.07613](https://arxiv.org/abs/1907.07613)\r\n\r\n**Multi-Adapter RGBT Tracking**\r\n\r\n- arxiv: [https://arxiv.org/abs/1907.07485](https://arxiv.org/abs/1907.07485)\r\n- github: [https://github.com/Alexadlu/MANet](https://github.com/Alexadlu/MANet)\r\n\r\n**Teacher-Students Knowledge Distillation for Siamese Trackers**\r\n\r\n[https://arxiv.org/abs/1907.10586](https://arxiv.org/abs/1907.10586)\r\n\r\n**Tell Me What to Track**\r\n\r\n- intro: Boston University & Horizon Robotics & University of Chinese Academy of Sciences\r\n- arxiv: [https://arxiv.org/abs/1907.11751](https://arxiv.org/abs/1907.11751)\r\n\r\n**Learning to Track Any Object**\r\n\r\n- intro: ICCV 2019 Holistic Video Understanding workshop\r\n- arxiv: [https://arxiv.org/abs/1910.11844](https://arxiv.org/abs/1910.11844)\r\n\r\n**ROI Pooled Correlation Filters for Visual Tracking**\r\n\r\n- intro: CVPR 2019\r\n- arxiv: [https://arxiv.org/abs/1911.01668](https://arxiv.org/abs/1911.01668)\r\n\r\n**D3S -- A Discriminative Single Shot Segmentation Tracker**\r\n\r\n- intro: CVPR 2020\r\n- arxiv: [https://arxiv.org/abs/1911.08862](https://arxiv.org/abs/1911.08862)\r\n- github(PyTorch): [https://github.com/alanlukezic/d3s](https://github.com/alanlukezic/d3s)\r\n\r\n**Visual Tracking by TridentAlign and Context Embedding**\r\n\r\n- arxiv: [https://arxiv.org/abs/2007.06887](https://arxiv.org/abs/2007.06887)\r\n- github: [https://github.com/JanghoonChoi/TACT](https://github.com/JanghoonChoi/TACT)\r\n\r\n**Transformer Tracking**\r\n\r\n- intro: CVPR 2021\r\n- intro: Dalian University of Technology & Peng Cheng Laboratory & Remark AI\r\n- arxiv: [https://arxiv.org/abs/2103.15436](https://arxiv.org/abs/2103.15436)\r\n- github: [https://github.com/chenxin-dlut/TransT](https://github.com/chenxin-dlut/TransT)\r\n\r\n# Face Tracking\r\n\r\n**Mobile Face Tracking: A Survey and Benchmark**\r\n\r\n[https://arxiv.org/abs/1805.09749](https://arxiv.org/abs/1805.09749)\r\n\r\n# Multi-Object Tracking (MOT)\r\n\r\n**Simple Online and Realtime Tracking**\r\n\r\n- intro: ICIP 2016\r\n- arxiv: [https://arxiv.org/abs/1602.00763](https://arxiv.org/abs/1602.00763)\r\n- github: [https://github.com/abewley/sort](https://github.com/abewley/sort)\r\n\r\n**Simple Online and Realtime Tracking with a Deep Association Metric**\r\n\r\n- intro: ICIP 2017\r\n- arxiv: [https://arxiv.org/abs/1703.07402](https://arxiv.org/abs/1703.07402)\r\n- mot challenge: [https://motchallenge.net/tracker/DeepSORT_2](https://motchallenge.net/tracker/DeepSORT_2)\r\n- github(official, Python): [https://github.com/nwojke/deep_sort](https://github.com/nwojke/deep_sort)\r\n- github(C++): [https://github.com/oylz/ds](https://github.com/oylz/ds)\r\n\r\n**StrongSORT: Make DeepSORT Great Again**\r\n\r\n- intro: Beijing University of Posts and Telecommunications & Xidian University\r\n- arxiv: [https://arxiv.org/abs/2202.13514](https://arxiv.org/abs/2202.13514)\r\n\r\n**Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking**\r\n\r\n- intro: Carnegie Mellon University & The Chinese University of Hong Kong & Shanghai AI Laboratory\r\n- arxiv: [https://arxiv.org/abs/2203.14360](https://arxiv.org/abs/2203.14360)\r\n- github: [https://github.com/noahcao/OC_SORT](https://github.com/noahcao/OC_SORT)\r\n\r\n**BoT-SORT: Robust Associations Multi-Pedestrian Tracking**\r\n\r\n- intro: Tel-Aviv University\r\n- arxiv: [https://arxiv.org/abs/2206.14651](https://arxiv.org/abs/2206.14651)\r\n\r\n**Virtual Worlds as Proxy for Multi-Object Tracking Analysis**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.06457](http://arxiv.org/abs/1605.06457)\r\n- dataset(Virtual KITTI): [http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds](http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds)\r\n\r\n**Multi-Class Multi-Object Tracking using Changing Point Detection**\r\n\r\n- intro: changing point detection, entity transition, object detection from video, convolutional neural network\r\n- arxiv: [http://arxiv.org/abs/1608.08434](http://arxiv.org/abs/1608.08434)\r\n\r\n**POI: Multiple Object Tracking with High Performance Detection and Appearance Feature**\r\n\r\n- intro: ECCV workshop BMTT 2016. Sensetime\r\n- keywords: KDNT\r\n- arxiv: [https://arxiv.org/abs/1610.06136](https://arxiv.org/abs/1610.06136)\r\n\r\n**Multiple Object Tracking: A Literature Review**\r\n\r\n- intro: last revised 22 May 2017 (this version, v4)\r\n- arxiv: [https://arxiv.org/abs/1409.7618](https://arxiv.org/abs/1409.7618)\r\n\r\n**Deep Network Flow for Multi-Object Tracking**\r\n\r\n- intro: CVPR 2017\r\n- arxiv: [https://arxiv.org/abs/1706.08482](https://arxiv.org/abs/1706.08482)\r\n\r\n**Online Multi-Object Tracking Using CNN-based Single Object Tracker with Spatial-Temporal Attention Mechanism**\r\n\r\n[https://arxiv.org/abs/1708.02843](https://arxiv.org/abs/1708.02843)\r\n\r\n**Recurrent Autoregressive Networks for Online Multi-Object Tracking**\r\n\r\n[https://arxiv.org/abs/1711.02741](https://arxiv.org/abs/1711.02741)\r\n\r\n**SOT for MOT**\r\n\r\n- intro: Tsinghua University & Megvii Inc. (Face++)\r\n- arxiv: [https://arxiv.org/abs/1712.01059](https://arxiv.org/abs/1712.01059)\r\n\r\n**Multi-Target, Multi-Camera Tracking by Hierarchical Clustering: Recent Progress on DukeMTMC Project**\r\n\r\n[https://arxiv.org/abs/1712.09531](https://arxiv.org/abs/1712.09531)\r\n\r\n**Multiple Target Tracking by Learning Feature Representation and Distance Metric Jointly**\r\n\r\n[https://arxiv.org/abs/1802.03252](https://arxiv.org/abs/1802.03252)\r\n\r\n**Tracking Noisy Targets: A Review of Recent Object Tracking Approaches**\r\n\r\n[https://arxiv.org/abs/1802.03098](https://arxiv.org/abs/1802.03098)\r\n\r\n**Machine Learning Methods for Solving Assignment Problems in Multi-Target Tracking**\r\n\r\n- intro: University of Florida\r\n- arxiv: [https://arxiv.org/abs/1802.06897](https://arxiv.org/abs/1802.06897)\r\n\r\n**Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World**\r\n\r\n- intro: University of Modena and Reggio Emilia\r\n- arxiv: [https://arxiv.org/abs/1803.08319](https://arxiv.org/abs/1803.08319)\r\n\r\n**Features for Multi-Target Multi-Camera Tracking and Re-Identification**\r\n\r\n- intro: CVPR 2018 spotlight\r\n- intro: [https://arxiv.org/abs/1803.10859](https://arxiv.org/abs/1803.10859)\r\n\r\n**High Performance Visual Tracking with Siamese Region Proposal Network**\r\n\r\n- intro: CVPR 2018 spotlight\r\n- keywords: SiamRPN\r\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf)\r\n- slides: [https://drive.google.com/file/d/1OGIOUqANvYfZjRoQfpiDqhPQtOvPCpdq/view](https://drive.google.com/file/d/1OGIOUqANvYfZjRoQfpiDqhPQtOvPCpdq/view)\r\n\r\n**Trajectory Factory: Tracklet Cleaving and Re-connection by Deep Siamese Bi-GRU for Multiple Object Tracking**\r\n\r\n- intro: Peking University\r\n- arxiv: [https://arxiv.org/abs/1804.04555](https://arxiv.org/abs/1804.04555)\r\n\r\n**Automatic Adaptation of Person Association for Multiview Tracking in Group Activities**\r\n\r\n- intro: Carnegie Mellon University & Argo AI & Adobe Research\r\n- project page: [http://www.cs.cmu.edu/~ILIM/projects/IM/Association4Tracking/](http://www.cs.cmu.edu/~ILIM/projects/IM/Association4Tracking/)\r\n- arxiv: [https://arxiv.org/abs/1805.08717](https://arxiv.org/abs/1805.08717)\r\n\r\n**Improving Online Multiple Object tracking with Deep Metric Learning**\r\n\r\n[https://arxiv.org/abs/1806.07592](https://arxiv.org/abs/1806.07592)\r\n\r\n**Tracklet Association Tracker: An End-to-End Learning-based Association Approach for Multi-Object Tracking**\r\n\r\n- intro: Tsinghua Univeristy & Horizon Robotics\r\n- arxiv: [https://arxiv.org/abs/1808.01562](https://arxiv.org/abs/1808.01562)\r\n\r\n**Multiple Object Tracking in Urban Traffic Scenes with a Multiclass Object Detector**\r\n\r\n- intro: 13th International Symposium on Visual Computing (ISVC)\r\n- arxiv: [https://arxiv.org/abs/1809.02073](https://arxiv.org/abs/1809.02073)\r\n\r\n**Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers**\r\n\r\n[https://arxiv.org/abs/1809.03137](https://arxiv.org/abs/1809.03137)\r\n\r\n**Deep Affinity Network for Multiple Object Tracking**\r\n\r\n- intro: IEEE TPAMI 2018\r\n- arxiv: [https://arxiv.org/abs/1810.11780](https://arxiv.org/abs/1810.11780)\r\n- github: [https://github.com/shijieS/SST](https://github.com/shijieS/SST)\r\n\r\n**Exploit the Connectivity: Multi-Object Tracking with TrackletNet**\r\n\r\n[https://arxiv.org/abs/1811.07258](https://arxiv.org/abs/1811.07258)\r\n\r\n**Multi-Object Tracking with Multiple Cues and Switcher-Aware Classification**\r\n\r\n- intro: Sensetime Group Limited & Beihang University & The University of Sydney\r\n- arxiv: [https://arxiv.org/abs/1901.06129](https://arxiv.org/abs/1901.06129)\r\n\r\n**Online Multi-Object Tracking with Dual Matching Attention Networks**\r\n\r\n- intro: ECCV 2018\r\n- arxiv: [https://arxiv.org/abs/1902.00749](https://arxiv.org/abs/1902.00749)\r\n\r\n**Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic Model Refreshment**\r\n\r\n[https://arxiv.org/abs/1902.08231](https://arxiv.org/abs/1902.08231)\r\n\r\n**Tracking without bells and whistles**\r\n\r\n- intro: Technical University of Munich\r\n- keywords: Tracktor\r\n- arxiv: [https://arxiv.org/abs/1903.05625](https://arxiv.org/abs/1903.05625)\r\n- github: [https://github.com/phil-bergmann/tracking_wo_bnw](https://github.com/phil-bergmann/tracking_wo_bnw)\r\n\r\n**Spatial-Temporal Relation Networks for Multi-Object Tracking**\r\n\r\n- intro: Hong Kong University of Science and Technology & Tsinghua University & MSRA\r\n- arxiv: [https://arxiv.org/abs/1904.11489](https://arxiv.org/abs/1904.11489)\r\n\r\n**Fooling Detection Alone is Not Enough: First Adversarial Attack against Multiple Object Tracking**\r\n\r\n- intro: Baidu X-Lab & UC Irvine\r\n- arxiv: [https://arxiv.org/abs/1905.11026](https://arxiv.org/abs/1905.11026)\r\n\r\n**State-aware Re-identification Feature for Multi-target Multi-camera Tracking**\r\n\r\n- intro: CVPR-2019 TRMTMCT Workshop\r\n- intro: BUPT & Chinese Academy of Sciences & Horizon Robotics\r\n- arxiv: [https://arxiv.org/abs/1906.01357](https://arxiv.org/abs/1906.01357)\r\n\r\n**DeepMOT: A Differentiable Framework for Training Multiple Object Trackers**\r\n\r\n- intro: Inria\r\n- keywords: deep Hungarian network (DHN)\r\n- arxiv: [https://arxiv.org/abs/1906.06618](https://arxiv.org/abs/1906.06618)\r\n- gitlab: [https://gitlab.inria.fr/yixu/deepmot](https://gitlab.inria.fr/yixu/deepmot)\r\n\r\n**Graph Neural Based End-to-end Data Association Framework for Online Multiple-Object Tracking**\r\n\r\n- intro: Beihang University && Inception Institute of Artificial Intelligence\r\n- arxiv: [https://arxiv.org/abs/1907.05315](https://arxiv.org/abs/1907.05315)\r\n\r\n**End-to-End Learning Deep CRF models for Multi-Object Tracking**\r\n\r\n[https://arxiv.org/abs/1907.12176](https://arxiv.org/abs/1907.12176)\r\n\r\n**End-to-end Recurrent Multi-Object Tracking and Trajectory Prediction with Relational Reasoning**\r\n\r\n- intro: University of Oxford\r\n- arxiv: [https://arxiv.org/abs/1907.12887](https://arxiv.org/abs/1907.12887)\r\n\r\n**Robust Multi-Modality Multi-Object Tracking**\r\n\r\n- intro: ICCV 2019\r\n- keywords: LiDAR\r\n- arxiv: [https://arxiv.org/abs/1909.03850](https://arxiv.org/abs/1909.03850)\r\n- github: [https://github.com/ZwwWayne/mmMOT](https://github.com/ZwwWayne/mmMOT)\r\n\r\n**Learning Multi-Object Tracking and Segmentation from Automatic Annotations**\r\n\r\n[https://arxiv.org/abs/1912.02096](https://arxiv.org/abs/1912.02096)\r\n\r\n**Learning a Neural Solver for Multiple Object Tracking**\r\n\r\n- intro: Technical University of Munich\r\n- keywords: Message Passing Networks (MPNs)\r\n- arxiv: [https://arxiv.org/abs/1912.07515](https://arxiv.org/abs/1912.07515)\r\n- github: [https://github.com/dvl-tum/mot_neural_solver](https://github.com/dvl-tum/mot_neural_solver)\r\n\r\n**Multi-object Tracking via End-to-end Tracklet Searching and Ranking**\r\n\r\n- intro: Horizon Robotics Inc\r\n- arxiv: [https://arxiv.org/abs/2003.02795](https://arxiv.org/abs/2003.02795)\r\n\r\n**Refinements in Motion and Appearance for Online Multi-Object Tracking**\r\n\r\n[https://arxiv.org/abs/2003.07177](https://arxiv.org/abs/2003.07177)\r\n\r\n**A Unified Object Motion and Affinity Model for Online Multi-Object Tracking**\r\n\r\n- intro: CVPR 2020\r\n- arxiv: [https://arxiv.org/abs/2003.11291](https://arxiv.org/abs/2003.11291)\r\n- github: [https://github.com/yinjunbo/UMA-MOT](https://github.com/yinjunbo/UMA-MOT)\r\n\r\n**A Simple Baseline for Multi-Object Tracking**\r\n\r\n- intro: Microsoft Research Asia\r\n- arxiv: [https://arxiv.org/abs/2004.01888](https://arxiv.org/abs/2004.01888)\r\n- github: [https://github.com/ifzhang/FairMOT](https://github.com/ifzhang/FairMOT)\r\n\r\n**MOPT: Multi-Object Panoptic Tracking**\r\n\r\n- intro: University of Freiburg\r\n- arxiv: [https://arxiv.org/abs/2004.08189](https://arxiv.org/abs/2004.08189)\r\n\r\n**SQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking**\r\n\r\n- intro: Tsinghua University & Megvii Inc\r\n- arxiv: [https://arxiv.org/abs/2004.07472](https://arxiv.org/abs/2004.07472)\r\n\r\n**Multi-Object Tracking with Siamese Track-RCNN**\r\n\r\n- intro: Amazon Web Service (AWS) Rekognition\r\n- arixv: [https://arxiv.org/abs/2004.07786](https://arxiv.org/abs/2004.07786)\r\n\r\n**TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training Model**\r\n\r\n- intro: CVPR 2020 oral\r\n- arxiv: [https://arxiv.org/abs/2006.05683](https://arxiv.org/abs/2006.05683)\r\n- github: [https://github.com/BoPang1996/TubeTK](https://github.com/BoPang1996/TubeTK)\r\n\r\n**Quasi-Dense Similarity Learning for Multiple Object Tracking**\r\n\r\n- intro: CVPR 2021 oral\r\n- intro: Zhejiang University & Georgia Institute of Technology & ETH Zürich & Stanford University & UC Berkeley\r\n- project page: [https://www.vis.xyz/pub/qdtrack/](https://www.vis.xyz/pub/qdtrack/)\r\n- arxiv: [https://arxiv.org/abs/2006.06664](https://arxiv.org/abs/2006.06664)\r\n- github: [https://github.com/SysCV/qdtrack](https://github.com/SysCV/qdtrack)\r\n\r\n**imultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking**\r\n\r\n- intro: ECCV 2020\r\n- arxiv: [https://arxiv.org/abs/2008.08826](https://arxiv.org/abs/2008.08826)\r\n- github: [https://github.com/shijieS/DMMN](https://github.com/shijieS/DMMN)\r\n\r\n**MAT: Motion-Aware Multi-Object Tracking**\r\n\r\n[https://arxiv.org/abs/2009.04794](https://arxiv.org/abs/2009.04794)\r\n\r\n**SAMOT: Switcher-Aware Multi-Object Tracking and Still Another MOT Measure**\r\n\r\n[https://arxiv.org/abs/2009.10338](https://arxiv.org/abs/2009.10338)\r\n\r\n**GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking via Sinkhorn Normalization**\r\n\r\n- intro: Virginia Tech\r\n- arxiv: [https://arxiv.org/abs/2010.00067](https://arxiv.org/abs/2010.00067)\r\n\r\n**Rethinking the competition between detection and ReID in Multi-Object Tracking**\r\n\r\n- intro: University of Electronic Science and Technology of China(UESTC) & Chinese Academy of Sciences\r\n- arxiv: [https://arxiv.org/abs/2010.12138](https://arxiv.org/abs/2010.12138)\r\n\r\n**GMOT-40: A Benchmark for Generic Multiple Object Tracking**\r\n\r\n- intro: Temple University & Stony Brook University & Microsoft\r\n- arxiv: [https://arxiv.org/abs/2011.11858](https://arxiv.org/abs/2011.11858)\r\n\r\n**Multi-object Tracking with a Hierarchical Single-branch Network**\r\n\r\n[https://arxiv.org/abs/2101.01984](https://arxiv.org/abs/2101.01984)\r\n\r\n**Discriminative Appearance Modeling with Multi-track Pooling for Real-time Multi-object Tracking**\r\n\r\n- intro: Georgia Institute of Technology & Oregon State University\r\n- arxiv: [https://arxiv.org/abs/2101.12159](https://arxiv.org/abs/2101.12159)\r\n\r\n**Learning a Proposal Classifier for Multiple Object Tracking**\r\n\r\n- intro: CVPR 2021 poster\r\n- arxiv: [https://arxiv.org/abs/2103.07889](https://arxiv.org/abs/2103.07889)\r\n- github: [https://github.com/daip13/LPC_MOT](https://github.com/daip13/LPC_MOT)\r\n\r\n**Track to Detect and Segment: An Online Multi-Object Tracker**\r\n\r\n- intro: CVPR 2021\r\n- intro: SUNY Buffalo & TJU & Horizon Robotics\r\n- project page: [https://jialianwu.com/projects/TraDeS.html](https://jialianwu.com/projects/TraDeS.html)\r\n- arxiv: [https://arxiv.org/abs/2103.08808](https://arxiv.org/abs/2103.08808)\r\n\r\n**Learnable Graph Matching: Incorporating Graph Partitioning with Deep Feature Learning for Multiple Object Tracking**\r\n\r\n- intro: CVPR 2021\r\n- arxiv: [https://arxiv.org/abs/2103.16178](https://arxiv.org/abs/2103.16178)\r\n- github: [https://github.com/jiaweihe1996/GMTracker](https://github.com/jiaweihe1996/GMTracker)\r\n\r\n**Multiple Object Tracking with Correlation Learning**\r\n\r\n- intro: CVPR 2021\r\n- intro: Machine Intelligence Technology Lab, Alibaba Group\r\n- arxiv: [https://arxiv.org/abs/2104.03541](https://arxiv.org/abs/2104.03541)\r\n\r\n**ByteTrack: Multi-Object Tracking by Associating Every Detection Box**\r\n\r\n- intro: Huazhong University of Science and Technology & The University of Hong Kong & ByteDance\r\n- arxiv: [https://arxiv.org/abs/2110.06864](https://arxiv.org/abs/2110.06864)\r\n- github: [https://github.com/ifzhang/ByteTrack](https://github.com/ifzhang/ByteTrack)\r\n\r\n**SiamMOT: Siamese Multi-Object Tracking**\r\n\r\n- intro: Amazon Web Services (AWS)\r\n- arxiv: [https://arxiv.org/abs/2105.11595](https://arxiv.org/abs/2105.11595)\r\n- github: [https://github.com/amazon-research/siam-mot](https://github.com/amazon-research/siam-mot)\r\n\r\n**Synthetic Data Are as Good as the Real for Association Knowledge Learning in Multi-object Tracking**\r\n\r\n- arxiv: [https://arxiv.org/abs/2106.16100](https://arxiv.org/abs/2106.16100)\r\n- github: [https://github.com/liuyvchi/MOTX](https://github.com/liuyvchi/MOTX)\r\n\r\n**Track to Detect and Segment: An Online Multi-Object Tracker**\r\n\r\n- intro: CVPR 2021\r\n- intro: SUNY Buffalo & TJU & Horizon Robotics\r\n- project page: [https://jialianwu.com/projects/TraDeS.html](https://jialianwu.com/projects/TraDeS.html)\r\n- arxiv: [https://arxiv.org/abs/2103.08808](https://arxiv.org/abs/2103.08808)\r\n- github: [https://github.com/JialianW/TraDeS](https://github.com/JialianW/TraDeS)\r\n\r\n**Learning of Global Objective for Network Flow in Multi-Object Tracking**\r\n\r\n- intro: CVPR 2022\r\n- intro: Rochester Institute of Technology & Monash University\r\n- arxiv: [https://arxiv.org/abs/2203.16210](https://arxiv.org/abs/2203.16210)\r\n\r\n**MeMOT: Multi-Object Tracking with Memory**\r\n\r\n- intro: CVPR 2022 Oral\r\n- arxiv: [https://arxiv.org/abs/2203.16761](https://arxiv.org/abs/2203.16761)\r\n\r\n**TR-MOT: Multi-Object Tracking by Reference**\r\n\r\n- intro: University of Washington & Beihang University & SenseTime Research\r\n- arxiv: [https://arxiv.org/abs/2203.16621](https://arxiv.org/abs/2203.16621)\r\n\r\n**Towards Grand Unification of Object Tracking**\r\n\r\n- intro: ECCV 2022 Oral\r\n- intro: Dalian University of Technology & ByteDance & The University of Hong Kong\r\n- arxiv: [https://arxiv.org/abs/2207.07078](https://arxiv.org/abs/2207.07078)\r\n- github: [https://github.com/MasterBin-IIAU/Unicorn](https://github.com/MasterBin-IIAU/Unicorn)\r\n\r\n**Tracking Every Thing in the Wild**\r\n\r\n- intro: ECCV 2022\r\n- intro: Computer Vision Lab, ETH Zürich\r\n- project page: [https://www.vis.xyz/pub/tet/](https://www.vis.xyz/pub/tet/)\r\n- arxiv: [https://arxiv.org/abs/2207.12978](https://arxiv.org/abs/2207.12978)\r\n- github: [https://github.com/SysCV/tet](https://github.com/SysCV/tet)\r\n\r\n## Transformer\r\n\r\n**TransTrack: Multiple-Object Tracking with Transformer**\r\n\r\n- intro: The University of Hong Kong & ByteDance AI Lab & Tongji University & Carnegie Mellon University & Nanyang Technological University\r\n- arxiv: [https://arxiv.org/abs/2012.15460](https://arxiv.org/abs/2012.15460)\r\n- github: [https://github.com/PeizeSun/TransTrack](https://github.com/PeizeSun/TransTrack)\r\n\r\n**TrackFormer: Multi-Object Tracking with Transformers**\r\n\r\n- intro: Technical University of Munich & Facebook AI Research (FAIR)\r\n- arxiv: [https://arxiv.org/abs/2101.02702](https://arxiv.org/abs/2101.02702)\r\n\r\n**TransCenter: Transformers with Dense Queries for Multiple-Object Tracking**\r\n\r\n- intro: Inria & MIT & MIT-IBM Watson AI Lab\r\n- arxiv: [https://arxiv.org/abs/2103.15145](https://arxiv.org/abs/2103.15145)\r\n\r\n**Looking Beyond Two Frames: End-to-End Multi-Object Tracking UsingSpatial and Temporal Transformers**\r\n\r\n- intro: Monash University & The University of Adelaide & Australian Centre for Robotic Vision\r\n- arixiv: [https://arxiv.org/abs/2103.14829](https://arxiv.org/abs/2103.14829)\r\n\r\n**TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking**\r\n\r\n- intro: Microsoft & StonyBrook University\r\n- arxiv: [https://arxiv.org/abs/2104.00194](https://arxiv.org/abs/2104.00194)\r\n\r\n**MOTR: End-to-End Multiple-Object Tracking with TRansformer**\r\n\r\n- intro: MEGVII Technology\r\n- arxiv: [https://arxiv.org/abs/2105.03247](https://arxiv.org/abs/2105.03247)\r\n- github: [https://github.com/megvii-model/MOTR](https://github.com/megvii-model/MOTR)\r\n\r\n**Global Tracking Transformers**\r\n\r\n- intro: CVPR 2022\r\n- intro: The University of Texas at Austin & Apple\r\n- arxiv: [https://arxiv.org/abs/2203.13250](https://arxiv.org/abs/2203.13250)\r\n- github: [https://github.com/xingyizhou/GTR](https://github.com/xingyizhou/GTR)\r\n\r\n# Multiple People Tracking\r\n\r\n**Multi-Person Tracking by Multicut and Deep Matching**\r\n\r\n- intro: Max Planck Institute for Informatics\r\n- arxiv: [http://arxiv.org/abs/1608.05404](http://arxiv.org/abs/1608.05404)\r\n\r\n**Joint Flow: Temporal Flow Fields for Multi Person Tracking**\r\n\r\n- intro: University of Bonn\r\n- arxiv: [https://arxiv.org/abs/1805.04596](https://arxiv.org/abs/1805.04596)\r\n\r\n**Multiple People Tracking by Lifted Multicut and Person Re-identification**\r\n\r\n- intro: CVPR 2017\r\n- intro: Max Planck Institute for Informatics & Max Planck Institute for Intelligent Systems\r\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Multiple_People_Tracking_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Multiple_People_Tracking_CVPR_2017_paper.pdf)\r\n- code: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/people-detection-pose-estimation-and-tracking/multiple-people-tracking-with-lifted-multicut-and-person-re-identification/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/people-detection-pose-estimation-and-tracking/multiple-people-tracking-with-lifted-multicut-and-person-re-identification/)\r\n\r\n**Tracking by Prediction: A Deep Generative Model for Mutli-Person localisation and Tracking**\r\n\r\n- intro: WACV 2018\r\n- intro: Queensland University of Technology (QUT)\r\n- arxiv: [https://arxiv.org/abs/1803.03347](https://arxiv.org/abs/1803.03347)\r\n\r\n**Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification**\r\n\r\n- intro: ICME 2018\r\n- arxiv: [https://arxiv.org/abs/1809.04427](https://arxiv.org/abs/1809.04427)\r\n- github: [https://github.com/longcw/MOTDT](https://github.com/longcw/MOTDT)\r\n\r\n**Deep Person Re-identification for Probabilistic Data Association in Multiple Pedestrian Tracking**\r\n\r\n[https://arxiv.org/abs/1810.08565](https://arxiv.org/abs/1810.08565)\r\n\r\n**Multiple People Tracking Using Hierarchical Deep Tracklet Re-identification**\r\n\r\n[https://arxiv.org/abs/1811.04091](https://arxiv.org/abs/1811.04091)\r\n\r\n**Multi-person Articulated Tracking with Spatial and Temporal Embeddings**\r\n\r\n- intro: CVPR 2019\r\n- intro: SenseTime Research & The University of Sydney & SenseTime Computer Vision Research Group\r\n- arxiv: [https://arxiv.org/abs/1903.09214](https://arxiv.org/abs/1903.09214)\r\n\r\n**Instance-Aware Representation Learning and Association for Online Multi-Person Tracking**\r\n\r\n- intro: Pattern Recognition\r\n- intro: Sun Yat-sen University & Guangdong University of Foreign Studies & Carnegie Mellon University & University of California & Guilin University of Electronic Technology & WINNER Technology\r\n- arxiv: [https://arxiv.org/abs/1905.12409](https://arxiv.org/abs/1905.12409)\r\n\r\n**Online Multiple Pedestrian Tracking using Deep Temporal Appearance Matching Association**\r\n\r\n- intro: 2nd ranked tracker of the MOTChallenge on CVPR19 workshop\r\n- arxiv: [https://arxiv.org/abs/1907.00831](https://arxiv.org/abs/1907.00831)\r\n\r\n**Detecting Invisible People**\r\n\r\n- intro: Carnegie Mellon University & Argo AI\r\n- project page: [http://www.cs.cmu.edu/~tkhurana/invisible.htm](http://www.cs.cmu.edu/~tkhurana/invisible.htm)\r\n- arxiv: [https://arxiv.org/abs/2012.08419](https://arxiv.org/abs/2012.08419)\r\n\r\n# MOTS\r\n\r\n**MOTS: Multi-Object Tracking and Segmentation**\r\n\r\n- intro: CVPR 2019\r\n- intro: RWTH Aachen University\r\n- keywords: TrackR-CNN\r\n- project page: [https://www.vision.rwth-aachen.de/page/mots](https://www.vision.rwth-aachen.de/page/mots)\r\n- arxiv: [https://arxiv.org/abs/1902.03604](https://arxiv.org/abs/1902.03604)\r\n- github(official): [https://github.com/VisualComputingInstitute/TrackR-CNN](https://github.com/VisualComputingInstitute/TrackR-CNN)\r\n\r\n**Segment as Points for Efficient Online Multi-Object Tracking and Segmentation**\r\n\r\n- intro: ECCV 2020 oral\r\n- intro: PointTrack\r\n- arxiv: [https://arxiv.org/abs/2007.01550](https://arxiv.org/abs/2007.01550)\r\n- github: [https://github.com/detectRecog/PointTrack](https://github.com/detectRecog/PointTrack)\r\n\r\n**PointTrack++ for Effective Online Multi-Object Tracking and Segmentation**\r\n\r\n- intro: CVPR2020 MOTS Challenge Winner. PointTrack++ ranks first on KITTI MOTS\r\n- arxiv: [https://arxiv.org/abs/2007.01549](https://arxiv.org/abs/2007.01549)\r\n\r\n**Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation**\r\n\r\n- intro: NeurIPS 2021 Spotlight\r\n- intro: ETH Zürich & HKUST & Kuaishou Technology\r\n- keywords: Prototypical Cross-Attention Networks (PCAN)\r\n- project page: [https://www.vis.xyz/pub/pcan/](https://www.vis.xyz/pub/pcan/)\r\n- arxiv: [https://arxiv.org/abs/2106.11958](https://arxiv.org/abs/2106.11958)\r\n- github: [https://github.com/SysCV/pcan](https://github.com/SysCV/pcan)\r\n- youtube: [https://www.youtube.com/watch?v=hhAC2H0fmP8](https://www.youtube.com/watch?v=hhAC2H0fmP8)\r\n- bilibili: [https://www.bilibili.com/video/av593811548](https://www.bilibili.com/video/av593811548)\r\n- zhihu: [https://zhuanlan.zhihu.com/p/445457150](https://zhuanlan.zhihu.com/p/445457150)\r\n\r\n**Multi-Object Tracking and Segmentation with a Space-Time Memory Network**\r\n\r\n- intro: Polytechnique Montreal\r\n- project page: [http://www.mehdimiah.com/mentos+](http://www.mehdimiah.com/mentos+)\r\n- arxiv: [https://arxiv.org/abs/2110.11284](https://arxiv.org/abs/2110.11284)\r\n\r\n# Multi-target multi-camera tracking (MTMCT)\r\n\r\n**Traffic-Aware Multi-Camera Tracking of Vehicles Based on ReID and Camera Link Model**\r\n\r\n- intro: ACMMM 2020\r\n- arxiv: [https://arxiv.org/abs/2008.09785](https://arxiv.org/abs/2008.09785)\r\n\r\n# 3D MOT\r\n\r\n**A Baseline for 3D Multi-Object Tracking**\r\n\r\n- arxiv: [https://arxiv.org/abs/1907.03961](https://arxiv.org/abs/1907.03961)\r\n- github: [https://github.com/xinshuoweng/AB3DMOT](https://github.com/xinshuoweng/AB3DMOThttps://github.com/xinshuoweng/AB3DMOT)\r\n\r\n**Probabilistic 3D Multi-Object Tracking for Autonomous Driving**\r\n\r\n- intro: NeurIPS 2019\r\n- intro: 1st Place Award, NuScenes Tracking Challenge\r\n- intro: Stanford University $ Toyota Research Institute\r\n- arxiv: [https://arxiv.org/abs/2001.05673](https://arxiv.org/abs/2001.05673)\r\n- github: [https://github.com/eddyhkchiu/mahalanobis_3d_multi_object_tracking](https://github.com/eddyhkchiu/mahalanobis_3d_multi_object_tracking)\r\n\r\n**JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset**\r\n\r\n- intro: Stanford University\r\n- arxiv: [https://arxiv.org/abs/2002.08397](https://arxiv.org/abs/2002.08397)\r\n- github: [https://github.com/StanfordVL/JRMOT_ROS](https://github.com/StanfordVL/JRMOT_ROS)\r\n\r\n**Real-time 3D Deep Multi-Camera Tracking**\r\n\r\n- intro: Microsoft Cloud & AI\r\n- arxiv: [https://arxiv.org/abs/2003.11753](https://arxiv.org/abs/2003.11753)\r\n\r\n**P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds**\r\n\r\n- intro: CVPR 2020 oral\r\n- intro: Huazhong University of Science and Technology\r\n- arxiv: [https://arxiv.org/abs/2005.13888](https://arxiv.org/abs/2005.13888)\r\n- github: [https://github.com/HaozheQi/P2B](https://github.com/HaozheQi/P2B)\r\n\r\n**PnPNet: End-to-End Perception and Prediction with Tracking in the Loop**\r\n\r\n- intro: CVPR 2020\r\n- intro: Uber Advanced Technologies Group & University of Toronto\r\n- arxiv: [https://arxiv.org/abs/2005.14711](https://arxiv.org/abs/2005.14711)\r\n\r\n**GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning**\r\n\r\n- intro: CVPR 2020\r\n- intro: Carnegie Mellon University\r\n- arxiv: [https://arxiv.org/abs/2006.07327](https://arxiv.org/abs/2006.07327)\r\n- github(official, PyTorch): [https://github.com/xinshuoweng/GNN3DMOT](https://github.com/xinshuoweng/GNN3DMOT)\r\n\r\n**1st Place Solutions for Waymo Open Dataset Challenges -- 2D and 3D Tracking**\r\n\r\n- intro: Horizon Robotics Inc.\r\n- arxiv: [https://arxiv.org/abs/2006.15506](https://arxiv.org/abs/2006.15506)\r\n\r\n**Graph Neural Networks for 3D Multi-Object Tracking**\r\n\r\n- intro: ECCV 2020 workshop\r\n- intro: Robotics Institute, Carnegie Mellon University\r\n- project page: [http://www.xinshuoweng.com/projects/GNN3DMOT/](http://www.xinshuoweng.com/projects/GNN3DMOT/)\r\n- arxiv: [https://arxiv.org/abs/2008.09506](https://arxiv.org/abs/2008.09506)\r\n- github: [https://github.com/xinshuoweng/GNN3DMOT](https://github.com/xinshuoweng/GNN3DMOT)\r\n\r\n**Learnable Online Graph Representations for 3D Multi-Object Tracking**\r\n\r\n[https://arxiv.org/abs/2104.11747](https://arxiv.org/abs/2104.11747)\r\n\r\n**SimpleTrack: Understanding and Rethinking 3D Multi-object Tracking**\r\n\r\n- intro: UIUC & TuSimple\r\n- arxiv: [https://arxiv.org/abs/2111.09621](https://arxiv.org/abs/2111.09621)\r\n- github: [https://github.com/TuSimple/SimpleTrack](https://github.com/TuSimple/SimpleTrack)\r\n\r\n**Immortal Tracker: Tracklet Never Dies**\r\n\r\n- intro: University of Chinese Academy of Sciences & Tusimple & CASIA & UIUC\r\n- arxiv: [https://arxiv.org/abs/2111.13672](https://arxiv.org/abs/2111.13672)\r\n- github: [https://github.com/ImmortalTracker/ImmortalTracker](https://github.com/ImmortalTracker/ImmortalTracker)\r\n\r\n# Single Stage Joint Detection and Tracking\r\n\r\n**Bridging the Gap Between Detection and Tracking: A Unified Approach**\r\n\r\n- intro: ICCV 2019\r\n- paper: [https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf](https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf)\r\n\r\n**Towards Real-Time Multi-Object Tracking**\r\n\r\n- intro: Tsinghua University & Austrilian National University\r\n- arxiv: [https://arxiv.org/abs/1909.12605](https://arxiv.org/abs/1909.12605)\r\n- github: [https://github.com/Zhongdao/Towards-Realtime-MOT](https://github.com/Zhongdao/Towards-Realtime-MOT)\r\n\r\n**RetinaTrack: Online Single Stage Joint Detection and Tracking**\r\n\r\n- intro: CVPR 2020\r\n- intro: Google\r\n- arxiv: [https://arxiv.org/abs/2003.13870](https://arxiv.org/abs/2003.13870)\r\n\r\n**Tracking Objects as Points**\r\n\r\n- intro: UT Austin & Intel Labs\r\n- intro: Simultaneous object detection and tracking using center points.\r\n- keywords: CenterTrack\r\n- arxiv: [https://arxiv.org/abs/2004.01177](https://arxiv.org/abs/2004.01177)\r\n- github: [https://github.com/xingyizhou/CenterTrack](https://github.com/xingyizhou/CenterTrack)\r\n\r\n**Fully Convolutional Online Tracking**\r\n\r\n- intro: Nanjing University\r\n- arxiv: [https://arxiv.org/abs/2004.07109](https://arxiv.org/abs/2004.07109)\r\n- github(official, PyTorch): [https://github.com/MCG-NJU/FCOT](https://github.com/MCG-NJU/FCOT)\r\n\r\n**Accurate Anchor Free Tracking**\r\n\r\n- intro: Tongji University & UCLA\r\n- keywords: Anchor Free Siamese Network (AFSN)\r\n- arxiv: [https://arxiv.org/abs/2006.07560](https://arxiv.org/abs/2006.07560)\r\n\r\n**Ocean: Object-aware Anchor-free Tracking**\r\n\r\n- intro: ECCV 2020\r\n- intro: NLPR, CASIA & UCAS & Microsoft Research\r\n- arxiv: [https://arxiv.org/abs/2006.10721](https://arxiv.org/abs/2006.10721)\r\n- github: [https://github.com/researchmm/TracKit](https://github.com/researchmm/TracKit)\r\n\r\n**Joint Detection and Multi-Object Tracking with Graph Neural Networks**\r\n\r\n- intro: Carnegie Mellon University\r\n- arxiv: [https://arxiv.org/abs/2006.13164](https://arxiv.org/abs/2006.13164)\r\n\r\n## Joint Multiple-Object Detection and Tracking\r\n\r\n**Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking**\r\n\r\n- intro: ECCV 2020 spotlight\r\n- intro: Tencent Youtu Lab & Fudan University & Nara Institute of Science and Technology\r\n- keywords: Chained-Tracker (CTracker)\r\n- arxiv: [https://arxiv.org/abs/2007.14557](https://arxiv.org/abs/2007.14557)\r\n- github: [https://github.com/pjl1995/CTracker](https://github.com/pjl1995/CTracker)\r\n\r\n**SMOT: Single-Shot Multi Object Tracking**\r\n\r\n[https://arxiv.org/abs/2010.16031](https://arxiv.org/abs/2010.16031)\r\n\r\n**DEFT: Detection Embeddings for Tracking**\r\n\r\n- arxiv: [https://arxiv.org/abs/2102.02267](https://arxiv.org/abs/2102.02267)\r\n- github: [https://github.com/MedChaabane/DEFT](https://github.com/MedChaabane/DEFT)\r\n\r\n**Global Correlation Network: End-to-End Joint Multi-Object Detection and Tracking**\r\n\r\n- intro: ICCV 2021\r\n- intro: Intell Tsinghua University & Beihang University\r\n- arxiv: [https://arxiv.org/abs/2103.12511](https://arxiv.org/abs/2103.12511)\r\n\r\n# Tracking with Reinforcement Learning\r\n\r\n**Deep Reinforcement Learning for Visual Object Tracking in Videos**\r\n\r\n- intro: University of California at Santa Barbara & Samsung Research America\r\n- arxiv: [https://arxiv.org/abs/1701.08936](https://arxiv.org/abs/1701.08936)\r\n\r\n**Visual Tracking by Reinforced Decision Making**\r\n\r\n- arxiv: [https://arxiv.org/abs/1702.06291](https://arxiv.org/abs/1702.06291)\r\n\r\n**End-to-end Active Object Tracking via Reinforcement Learning**\r\n\r\n[https://arxiv.org/abs/1705.10561](https://arxiv.org/abs/1705.10561)\r\n\r\n**Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning**\r\n\r\n- project page: [https://sites.google.com/view/cvpr2017-adnet](https://sites.google.com/view/cvpr2017-adnet)\r\n- paper: [https://drive.google.com/file/d/0B34VXh5mZ22cZUs2Umc1cjlBMFU/view?usp=drive_web](https://drive.google.com/file/d/0B34VXh5mZ22cZUs2Umc1cjlBMFU/view?usp=drive_web)\r\n\r\n**Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning**\r\n\r\n[https://arxiv.org/abs/1707.04991](https://arxiv.org/abs/1707.04991)\r\n\r\n**Detect to Track and Track to Detect**\r\n\r\n- intro: ICCV 2017\r\n- project page: [https://www.robots.ox.ac.uk/~vgg/research/detect-track/](https://www.robots.ox.ac.uk/~vgg/research/detect-track/)\r\n- arxiv: [https://arxiv.org/abs/1710.03958](https://arxiv.org/abs/1710.03958)\r\n- github: [https://github.com/feichtenhofer/Detect-Track](https://github.com/feichtenhofer/Detect-Track)\r\n\r\n# Projects\r\n\r\n**MMTracking**\r\n\r\n- intro: OpenMMLab Video Perception Toolbox. It supports Single Object Tracking (SOT), Multiple Object Tracking (MOT), Video Object Detection (VID) with a unified framework.\r\n- github: [https://github.com/open-mmlab/mmtracking](https://github.com/open-mmlab/mmtracking)\r\n\r\n**Tensorflow_Object_Tracking_Video**\r\n\r\n- intro: Object Tracking in Tensorflow ( Localization Detection Classification ) developed to partecipate to ImageNET VID competition\r\n- github: [https://github.com/DrewNF/Tensorflow_Object_Tracking_Video](https://github.com/DrewNF/Tensorflow_Object_Tracking_Video)\r\n\r\n# Resources\r\n\r\n**Multi-Object-Tracking-Paper-List**\r\n\r\n- intro: Paper list and source code for multi-object-tracking\r\n- github: [https://github.com/SpyderXu/multi-object-tracking-paper-list](https://github.com/SpyderXu/multi-object-tracking-paper-list)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","title":"Transfer Learning"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Transfer Learning\ndate: 2015-10-09\n---\n\n# Papers\n\n**Discriminative Transfer Learning with Tree-based Priors**\n\n- intro: NIPS 2013\n- paper: [http://deeplearning.net/wp-content/uploads/2013/03/icml13_workshop.pdf](http://deeplearning.net/wp-content/uploads/2013/03/icml13_workshop.pdf)\n- paper: [http://www.cs.toronto.edu/~nitish/treebasedpriors.pdf](http://www.cs.toronto.edu/~nitish/treebasedpriors.pdf)\n\n**How transferable are features in deep neural networks?**\n\n- intro: NIPS 2014\n- arxiv: [http://arxiv.org/abs/1411.1792](http://arxiv.org/abs/1411.1792)\n- paper: [http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf](http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf)\n- github: [https://github.com/yosinski/convnet_transfer](https://github.com/yosinski/convnet_transfer)\n\n**Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks**\n\n- paper: [http://research.microsoft.com/pubs/214307/paper.pdf](http://research.microsoft.com/pubs/214307/paper.pdf)\n\n**Learning Transferable Features with Deep Adaptation Networks**\n\n- intro: ICML 2015\n- arxiv: [https://arxiv.org/abs/1502.02791](https://arxiv.org/abs/1502.02791)\n- gihtub: [https://github.com/caoyue10/icml-caffe](https://github.com/caoyue10/icml-caffe)\n\n**Transferring Knowledge from a RNN to a DNN**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1504.01483](https://arxiv.org/abs/1504.01483)\n\n**Simultaneous Deep Transfer Across Domains and Tasks**\n\n- intro: ICCV 2015\n- arxiv: [http://arxiv.org/abs/1510.02192](http://arxiv.org/abs/1510.02192)\n\n**Net2Net: Accelerating Learning via Knowledge Transfer**\n\n- arxiv: [http://arxiv.org/abs/1511.05641](http://arxiv.org/abs/1511.05641)\n- github: [https://github.com/soumith/net2net.torch](https://github.com/soumith/net2net.torch)\n- notes(by Hugo Larochelle): [https://www.evernote.com/shard/s189/sh/46414718-9663-440e-bbb7-65126b247b42/19688c438709251d8275d843b8158b03](https://www.evernote.com/shard/s189/sh/46414718-9663-440e-bbb7-65126b247b42/19688c438709251d8275d843b8158b03)\n\n**Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping**\n\n- arxiv: [http://arxiv.org/abs/1510.00098](http://arxiv.org/abs/1510.00098)\n\n**A theoretical framework for deep transfer learning**\n\n- key words: transfer learning, PAC learning, PAC-Bayesian, deep learning\n- homepage: [http://imaiai.oxfordjournals.org/content/early/2016/04/28/imaiai.iaw008](http://imaiai.oxfordjournals.org/content/early/2016/04/28/imaiai.iaw008)\n- paper: [http://imaiai.oxfordjournals.org/content/early/2016/04/28/imaiai.iaw008.full.pdf](http://imaiai.oxfordjournals.org/content/early/2016/04/28/imaiai.iaw008.full.pdf)\n\n**Transfer learning using neon**\n\n- blog: [http://www.nervanasys.com/transfer-learning-using-neon/](http://www.nervanasys.com/transfer-learning-using-neon/)\n\n**Hyperparameter Transfer Learning through Surrogate Alignment for Efficient Deep Neural Network Training**\n\n- arxiv: [http://arxiv.org/abs/1608.00218](http://arxiv.org/abs/1608.00218)\n\n**What makes ImageNet good for transfer learning?**\n\n- project page: [http://minyounghuh.com/papers/analysis/](http://minyounghuh.com/papers/analysis/)\n- arxiv: [http://arxiv.org/abs/1608.08614](http://arxiv.org/abs/1608.08614)\n\n**Fine-tuning a Keras model using Theano trained Neural Network & Introduction to Transfer Learning**\n\n- github: [https://www.analyticsvidhya.com/blog/2016/11/fine-tuning-a-keras-model-using-theano-trained-neural-network-introduction-to-transfer-learning/](https://www.analyticsvidhya.com/blog/2016/11/fine-tuning-a-keras-model-using-theano-trained-neural-network-introduction-to-transfer-learning/)\n\n**Multi-source Transfer Learning with Convolutional Neural Networks for Lung Pattern Analysis**\n\n- arxiv: [https://arxiv.org/abs/1612.02589](https://arxiv.org/abs/1612.02589)\n\n**Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning**\n\n- intro: CVPR 2017. The University of Hong Kong\n- arxiv: [https://arxiv.org/abs/1702.08690](https://arxiv.org/abs/1702.08690)\n\n**Optimal Transport for Deep Joint Transfer Learning**\n\n[https://arxiv.org/abs/1709.02995](https://arxiv.org/abs/1709.02995)\n\n**CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise**\n\n- intro: CVPR 2018. Microsoft AI and Research i& JD AI Research & Facebook\n- keywords: Food-101N\n- project page: [https://kuanghuei.github.io/CleanNetProject/](https://kuanghuei.github.io/CleanNetProject/)\n- arxiv: [https://arxiv.org/abs/1711.07131](https://arxiv.org/abs/1711.07131)\n- github(Tensorflow): [https://github.com/kuanghuei/clean-net](https://github.com/kuanghuei/clean-net)\n- blog: [https://www.microsoft.com/en-us/research/blog/using-transfer-learning-to-address-label-noise-for-large-scale-image-classification/](https://www.microsoft.com/en-us/research/blog/using-transfer-learning-to-address-label-noise-for-large-scale-image-classification/)\n\n**Transfer Learning with Binary Neural Networks**\n\n- intro: Machine Learning on the Phone and other Consumer Devices, NIPS2017 Workshop\n- arxiv: [https://arxiv.org/abs/1711.10761](https://arxiv.org/abs/1711.10761)\n\n**Gradual Tuning: a better way of Fine Tuning the parameters of a Deep Neural Network**\n\n- intro: Université Paris Descartes, Paris\n- arxiv: [https://arxiv.org/abs/1711.10177](https://arxiv.org/abs/1711.10177)\n\n**Born Again Neural Networks**\n\n- intro: University of Southern California & CMU & Amazon AI\n- paper: [http://metalearning.ml/papers/metalearn17_furlanello.pdf](http://metalearning.ml/papers/metalearn17_furlanello.pdf)\n\n**Taskonomy: Disentangling Task Transfer Learning**\n\n- intro: CVPR 2018 (Oral). CVPR 2018 Best paper award. Stanford University & UC Berkeley\n- project page: [http://taskonomy.stanford.edu/](http://taskonomy.stanford.edu/)\n- arxiv: [https://arxiv.org/abs/1804.08328](https://arxiv.org/abs/1804.08328)\n\n**Do Better ImageNet Models Transfer Better?**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1805.08974](https://arxiv.org/abs/1805.08974)\n\n**SOSELETO: A Unified Approach to Transfer Learning and Training with Noisy Labels**\n\n- keywords: SOSELETO (SOurce SELEction for Target Optimization)\n- arxiv: [https://arxiv.org/abs/1805.09622](https://arxiv.org/abs/1805.09622)\n\n**GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations**\n\n- intro: Carnegie Mellon University & New York University & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1806.05662](https://arxiv.org/abs/1806.05662)\n\n**Taskonomy: Disentangling Task Transfer Learning**\n\n- intro: CVPR 2018 oral\n- project page: [http://taskonomy.stanford.edu/](http://taskonomy.stanford.edu/)\n- arxiv: [https://arxiv.org/abs/1804.08328](https://arxiv.org/abs/1804.08328)\n- github: [https://github.com/StanfordVL/taskonomy/tree/master/taskbank](https://github.com/StanfordVL/taskonomy/tree/master/taskbank)\n\n# One Shot Learning\n\n**One-shot Learning with Memory-Augmented Neural Networks**\n\n- intro: Google DeepMind\n- arxiv: [https://arxiv.org/abs/1605.06065](https://arxiv.org/abs/1605.06065)\n- github(Tensorflow): [https://github.com/hmishra2250/NTM-One-Shot-TF](https://github.com/hmishra2250/NTM-One-Shot-TF)\n- note: [http://rylanschaeffer.github.io/content/research/one_shot_learning_with_memory_augmented_nn/main.html](http://rylanschaeffer.github.io/content/research/one_shot_learning_with_memory_augmented_nn/main.html)\n\n**Matching Networks for One Shot Learning**\n\n- intro: Google DeepMind\n- arxiv: [https://arxiv.org/abs/1606.04080](https://arxiv.org/abs/1606.04080)\n- notes: [https://blog.acolyer.org/2017/01/03/matching-networks-for-one-shot-learning/](https://blog.acolyer.org/2017/01/03/matching-networks-for-one-shot-learning/)\n\n**Learning feed-forward one-shot learners [NIPS 2016] [VALSE seminar]**\n\n- youtube: [https://www.youtube.com/watch?v=BnLN3uoXMRY](https://www.youtube.com/watch?v=BnLN3uoXMRY)\n- mirror: [https://pan.baidu.com/s/1mhAITmS](https://pan.baidu.com/s/1mhAITmS)\n\n**Generative Adversarial Residual Pairwise Networks for One Shot Learning**\n\n- intro: Indian Institute of Science\n- arxiv: [https://arxiv.org/abs/1703.08033](https://arxiv.org/abs/1703.08033)\n\n# Few-Shot Learning\n\n**Optimization as a Model for Few-Shot Learning**\n\n- intro: Twitter\n- paper: [https://openreview.net/pdf?id=rJY0-Kcll](https://openreview.net/pdf?id=rJY0-Kcll)\n- github: [https://github.com/twitter/meta-learning-lstm](https://github.com/twitter/meta-learning-lstm)\n\n**Learning to Compare: Relation Network for Few-Shot Learning**\n\n- intro: Queen Mary University of London & The University of Edinburgh\n- arxiv: [https://arxiv.org/abs/1711.06025](https://arxiv.org/abs/1711.06025)\n\n**Unleashing the Potential of CNNs for Interpretable Few-Shot Learning**\n\n- intro: Beihang University & Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/1711.08277](https://arxiv.org/abs/1711.08277)\n\n**Low-Shot Learning from Imaginary Data**\n\n- intro: Facebook AI Research (FAIR) & CMU & Cornell University\n- arxiv: [https://arxiv.org/abs/1801.05401](https://arxiv.org/abs/1801.05401)\n\n**Semantic Feature Augmentation in Few-shot Learning**\n\n- keywords: TriNet\n- arxiv: [https://arxiv.org/abs/1804.05298](https://arxiv.org/abs/1804.05298)\n- github: [https://github.com/tankche1/Semantic-Feature-Augmentation-in-Few-shot-Learning](https://github.com/tankche1/Semantic-Feature-Augmentation-in-Few-shot-Learning)\n\n**Transductive Propagation Network for Few-shot Learning**\n\n- intro: achieved the state-of-the-art results on miniImagenet\n- arxiv: [https://arxiv.org/abs/1805.10002](https://arxiv.org/abs/1805.10002)\n\n**TADAM: Task dependent adaptive metric for improved few-shot learning**\n\n- intro: Element AI\n- arxiv: [https://arxiv.org/abs/1805.10123](https://arxiv.org/abs/1805.10123)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","title":"Unsupervised Learning"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Unsupervised Learning\ndate: 2015-10-09\n---\n\nRestricted Boltzmann Machine (RBM)\n\nSparse Coding\n\n**Fast Convolutional Sparse Coding in the Dual Domain**\n\n[https://arxiv.org/abs/1709.09479](https://arxiv.org/abs/1709.09479)\n\nAuto-encoder\n\n# Papers\n\n**On Random Weights and Unsupervised Feature Learning**\n\n- intro: ICML 2011\n- paper: [http://www.robotics.stanford.edu/~ang/papers/icml11-RandomWeights.pdf](http://www.robotics.stanford.edu/~ang/papers/icml11-RandomWeights.pdf)\n\n**Unsupervised Learning of Spatiotemporally Coherent Metrics**\n\n- paper: [http://arxiv.org/abs/1412.6056](http://arxiv.org/abs/1412.6056)\n- code: [https://github.com/jhjin/flattened-cnn](https://github.com/jhjin/flattened-cnn)\n\n**Unsupervised Learning of Visual Representations using Videos**\n\n- intro: ICCV 2015\n- project page: [http://www.cs.cmu.edu/~xiaolonw/unsupervise.html](http://www.cs.cmu.edu/~xiaolonw/unsupervise.html)\n- arxiv: [http://arxiv.org/abs/1505.00687](http://arxiv.org/abs/1505.00687)\n- paper: [http://www.cs.toronto.edu/~nitish/depth_oral.pdf](http://www.cs.toronto.edu/~nitish/depth_oral.pdf)\n- github: [https://github.com/xiaolonw/caffe-video_triplet](https://github.com/xiaolonw/caffe-video_triplet)\n\n**Unsupervised Visual Representation Learning by Context Prediction**\n\n- intro: ICCV 2015\n- homepage: [http://graphics.cs.cmu.edu/projects/deepContext/](http://graphics.cs.cmu.edu/projects/deepContext/)\n- arxiv: [http://arxiv.org/abs/1505.05192](http://arxiv.org/abs/1505.05192)\n- github: [https://github.com/cdoersch/deepcontext](https://github.com/cdoersch/deepcontext)\n\n**Unsupervised Learning on Neural Network Outputs**\n\n- intro: \"use CNN trained on the ImageNet of 1000 classes to the ImageNet of over 20000 classes\"\n- arxiv: [http://arxiv.org/abs/1506.00990](http://arxiv.org/abs/1506.00990)\n- github: [https://github.com/yaolubrain/ULNNO](https://github.com/yaolubrain/ULNNO)\n\n**Unsupervised Domain Adaptation by Backpropagation**\n\n- intro: ICML 2015\n- project page: [http://sites.skoltech.ru/compvision/projects/grl/](http://sites.skoltech.ru/compvision/projects/grl/)\n- paper: [http://sites.skoltech.ru/compvision/projects/grl/files/paper.pdf](http://sites.skoltech.ru/compvision/projects/grl/files/paper.pdf)\n- github: [https://github.com/ddtm/caffe/tree/grl](https://github.com/ddtm/caffe/tree/grl)\n\n**Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles**\n\n- arxiv: [http://arxiv.org/abs/1603.09246](http://arxiv.org/abs/1603.09246)\n- notes: [http://www.inference.vc/notes-on-unsupervised-learning-of-visual-representations-by-solving-jigsaw-puzzles/](http://www.inference.vc/notes-on-unsupervised-learning-of-visual-representations-by-solving-jigsaw-puzzles/)\n\n**Tagger: Deep Unsupervised Perceptual Grouping**\n\n![](https://raw.githubusercontent.com/CuriousAI/tagger/master/figures/tagger_model.png)\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1606.06724](https://arxiv.org/abs/1606.06724)\n- github: [https://github.com/CuriousAI/tagger](https://github.com/CuriousAI/tagger)\n\n**Regularization for Unsupervised Deep Neural Nets**\n\n- arxiv: [http://arxiv.org/abs/1608.04426](http://arxiv.org/abs/1608.04426)\n\n**Sparse coding: A simple exploration**\n\n- blog: [https://blog.metaflow.fr/sparse-coding-a-simple-exploration-152a3c900a7c#.o7g2jk9zi](https://blog.metaflow.fr/sparse-coding-a-simple-exploration-152a3c900a7c#.o7g2jk9zi)\n- github: [https://github.com/metaflow-ai/blog/tree/master/sparse-coding](https://github.com/metaflow-ai/blog/tree/master/sparse-coding)\n\n**Navigating the unsupervised learning landscape**\n\n- blog: [https://culurciello.github.io//tech/2016/06/10/unsup.html](https://culurciello.github.io//tech/2016/06/10/unsup.html)\n\n**Unsupervised Learning using Adversarial Networks**\n\n- intro: Facebook AI Research\n- youtube: [https://www.youtube.com/watch?v=lalg1CuNB30](https://www.youtube.com/watch?v=lalg1CuNB30)\n\n**Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction**\n\n- intro: UC Berkeley\n- project page: [https://richzhang.github.io/splitbrainauto/](https://richzhang.github.io/splitbrainauto/)\n- arxiv: [https://arxiv.org/abs/1611.09842](https://arxiv.org/abs/1611.09842)\n- github: [https://github.com/richzhang/splitbrainauto](https://github.com/richzhang/splitbrainauto)\n\n**Learning Features by Watching Objects Move**\n\n- intro: CVPR 2017. Facebook AI Research & UC Berkeley\n- arxiv: [https://arxiv.org/abs/1612.06370](https://arxiv.org/abs/1612.06370)\n- github((Caffe+Torch): [https://github.com/pathak22/unsupervised-video](https://github.com/pathak22/unsupervised-video)\n\n**CNN features are also great at unsupervised classification**\n\n- intro: Arts et Métiers ParisTech\n- arxiv: [https://arxiv.org/abs/1707.01700](https://arxiv.org/abs/1707.01700)\n\n**Supervised Convolutional Sparse Coding**\n\n[https://arxiv.org/abs/1804.02678](https://arxiv.org/abs/1804.02678)\n\n**Momentum Contrast for Unsupervised Visual Representation Learning**\n\n- intro: CVPR 2020\n- intro: Facebook AI Research (FAIR)\n- intro: MoCo\n- arxiv: [https://arxiv.org/abs/1911.05722](https://arxiv.org/abs/1911.05722)\n- github(official, Pytorch): [https://github.com/facebookresearch/moco](https://github.com/facebookresearch/moco)\n\n**Improved Baselines with Momentum Contrastive Learning**\n\n- intro: Facebook AI Research (FAIR)\n- intro: MoCo v2\n- arxiv: [https://arxiv.org/abs/2003.04297](https://arxiv.org/abs/2003.04297)\n- github: [https://github.com/facebookresearch/moco](https://github.com/facebookresearch/moco)\n\n## Clustering\n\n**Deep clustering: Discriminative embeddings for segmentation and separation**\n\n- arxiv: [https://arxiv.org/abs/1508.04306](https://arxiv.org/abs/1508.04306)\n- github(Keras): [https://github.com/jcsilva/deep-clustering](https://github.com/jcsilva/deep-clustering)\n\n**Neural network-based clustering using pairwise constraints**\n\n- intro: ICLR 2016\n- arxiv: [https://arxiv.org/abs/1511.06321](https://arxiv.org/abs/1511.06321)\n\n**Unsupervised Deep Embedding for Clustering Analysis**\n\n- intro: ICML 2016. Deep Embedded Clustering (DEC)\n- arxiv: [https://arxiv.org/abs/1511.06335](https://arxiv.org/abs/1511.06335)\n- github: [https://github.com/piiswrong/dec](https://github.com/piiswrong/dec)\n\n**Joint Unsupervised Learning of Deep Representations and Image Clusters**\n\n- intro: CVPR 2016\n- arxiv: [https://arxiv.org/abs/1604.03628](https://arxiv.org/abs/1604.03628)\n- github(Torch): [https://github.com/jwyang/joint-unsupervised-learning](https://github.com/jwyang/joint-unsupervised-learning)\n\n**Single-Channel Multi-Speaker Separation using Deep Clustering**\n\n- arxiv: [http://arxiv.org/abs/1607.02173](http://arxiv.org/abs/1607.02173)\n\n**Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering**\n\n- arxiv: [https://arxiv.org/abs/1610.04794](https://arxiv.org/abs/1610.04794)\n\n**Deep Unsupervised Clustering with Gaussian Mixture Variational**\n\n- arxiv: [https://arxiv.org/abs/1611.02648](https://arxiv.org/abs/1611.02648)\n- github: [https://github.com/Nat-D/GMVAE](https://github.com/Nat-D/GMVAE)\n\n**Variational Deep Embedding: A Generative Approach to Clustering**\n\n- arxiv: [https://arxiv.org/abs/1611.05148](https://arxiv.org/abs/1611.05148)\n\n**A new look at clustering through the lens of deep convolutional neural networks**\n\n- intro: University of Central Florida & Purdue University\n- arxiv: [https://arxiv.org/abs/1706.05048](https://arxiv.org/abs/1706.05048)\n\n**Deep Subspace Clustering Networks**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1709.02508](https://arxiv.org/abs/1709.02508)\n\n**SpectralNet: Spectral Clustering using Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1801.01587](https://arxiv.org/abs/1801.01587)\n- github: [https://github.com//kstant0725/SpectralNet](https://github.com//kstant0725/SpectralNet)\n\n**Clustering with Deep Learning: Taxonomy and New Methods**\n\n- intro: Technical University of Munich\n- arxiv: [https://arxiv.org/abs/1801.07648](https://arxiv.org/abs/1801.07648)\n- github: [https://github.com/elieJalbout/Clustering-with-Deep-learning](https://github.com/elieJalbout/Clustering-with-Deep-learning)\n\n**Deep Continuous Clustering**\n\n- arxiv: [https://arxiv.org/abs/1803.01449](https://arxiv.org/abs/1803.01449)\n- github: [https://github.com/shahsohil/DCC](https://github.com/shahsohil/DCC)\n\n**Learning to Cluster**\n\n- openreview: [https://openreview.net/forum?id=HkWTqLsIz](https://openreview.net/forum?id=HkWTqLsIz)\n- github: [https://github.com/kutoga/learning2cluster](https://github.com/kutoga/learning2cluster)\n\n**Learning Neural Models for End-to-End Clustering**\n\n- intro: ANNPR\n- arxiv: [https://arxiv.org/abs/1807.04001](https://arxiv.org/abs/1807.04001)\n\n**Deep Clustering for Unsupervised Learning of Visual Features**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.05520](https://arxiv.org/abs/1807.05520)\n\n**Improving Image Clustering With Multiple Pretrained CNN Feature Extractors**\n\n- intro: Poster presentation at BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.07760](https://arxiv.org/abs/1807.07760)\n\n**Deep clustering: On the link between discriminative models and K-means**\n\n[https://arxiv.org/abs/1810.04246](https://arxiv.org/abs/1810.04246)\n\n**Deep Density-based Image Clustering**\n\n[https://arxiv.org/abs/1812.04287](https://arxiv.org/abs/1812.04287)\n\n**Deep Representation Learning Characterized by Inter-class Separation for Image Clustering**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1901.06474](https://arxiv.org/abs/1901.06474)\n\n**Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised Approach for Feature Embedding**\n\n- intro: BMVC 2020\n- arxiv: [https://arxiv.org/abs/2009.04091](https://arxiv.org/abs/2009.04091)\n\n**Contrastive Clustering**\n\n[https://arxiv.org/abs/2009.09687](https://arxiv.org/abs/2009.09687)\n\n**Deep Clustering by Semantic Contrastive Learning**\n\n- intro: Queen Mary University of London\n- arxiv: [https://arxiv.org/abs/2103.02662](https://arxiv.org/abs/2103.02662)\n\n# Auto-encoder\n\n**Auto-Encoding Variational Bayes**\n\n- arxiv: [http://arxiv.org/abs/1312.6114](http://arxiv.org/abs/1312.6114)\n\n**The Potential Energy of an Autoencoder**\n\n- intro: PAMI 2014\n- paper: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.698.4921&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.698.4921&rep=rep1&type=pdf)\n\n**Importance Weighted Autoencoders**\n\n- paper: [http://arxiv.org/abs/1509.00519](http://arxiv.org/abs/1509.00519)\n- github: [https://github.com/yburda/iwae](https://github.com/yburda/iwae)\n\n**Review of Auto-Encoders**\n\n- intro: Piotr Mirowski, Microsoft Bing London, 2014\n- slides: [https://piotrmirowski.files.wordpress.com/2014/03/piotrmirowski_2014_reviewautoencoders.pdf](https://piotrmirowski.files.wordpress.com/2014/03/piotrmirowski_2014_reviewautoencoders.pdf)\n- github: [https://github.com/piotrmirowski/Tutorial_AutoEncoders/](https://github.com/piotrmirowski/Tutorial_AutoEncoders/)\n\n**Stacked What-Where Auto-encoders**\n\n- arxiv: [http://arxiv.org/abs/1506.02351](http://arxiv.org/abs/1506.02351)\n\n**Ladder Variational Autoencoders**\n\n**How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks**\n\n- arxiv:[http://arxiv.org/abs/1602.02282](http://arxiv.org/abs/1602.02282)\n- github: [https://github.com/casperkaae/LVAE](https://github.com/casperkaae/LVAE)\n\n**Rank Ordered Autoencoders**\n\n- arxiv: [http://arxiv.org/abs/1605.01749](http://arxiv.org/abs/1605.01749)\n- github: [https://github.com/paulbertens/rank-ordered-autoencoder](https://github.com/paulbertens/rank-ordered-autoencoder)\n\n**Decoding Stacked Denoising Autoencoders**\n\n- arxiv: [http://arxiv.org/abs/1605.02832](http://arxiv.org/abs/1605.02832)\n\n**Keras autoencoders (convolutional/fcc)**\n\n- github: [https://github.com/nanopony/keras-convautoencoder](https://github.com/nanopony/keras-convautoencoder)\n\n**Building Autoencoders in Keras**\n\n- blog: [http://blog.keras.io/building-autoencoders-in-keras.html](http://blog.keras.io/building-autoencoders-in-keras.html)\n\n**Review of auto-encoders**\n\n- intro: Tutorial code for Auto-Encoders, implementing Marc'Aurelio Ranzato's Sparse Encoding Symmetric Machine and \ntesting it on the MNIST handwritten digits data.\n- paper: [https://github.com/piotrmirowski/Tutorial_AutoEncoders/blob/master/PiotrMirowski_2014_ReviewAutoEncoders.pdf](https://github.com/piotrmirowski/Tutorial_AutoEncoders/blob/master/PiotrMirowski_2014_ReviewAutoEncoders.pdf)\n- github: [https://github.com/piotrmirowski/Tutorial_AutoEncoders](https://github.com/piotrmirowski/Tutorial_AutoEncoders)\n\n**Autoencoders: Torch implementations of various types of autoencoders**\n\n- intro: AE / SparseAE / DeepAE / ConvAE / UpconvAE / DenoisingAE / VAE / AdvAE\n- github: [https://github.com/Kaixhin/Autoencoders](https://github.com/Kaixhin/Autoencoders)\n\n**Tutorial on Variational Autoencoders**\n\n- arxiv: [http://arxiv.org/abs/1606.05908](http://arxiv.org/abs/1606.05908)\n- github: [https://github.com/cdoersch/vae_tutorial](https://github.com/cdoersch/vae_tutorial)\n\n**Variational Autoencoders Explained**\n\n- blog: [http://kvfrans.com/variational-autoencoders-explained/](http://kvfrans.com/variational-autoencoders-explained/)\n- github: [https://github.com/kvfrans/variational-autoencoder](https://github.com/kvfrans/variational-autoencoder)\n\n**Introducing Variational Autoencoders (in Prose and Code)**\n\n- blog: [http://blog.fastforwardlabs.com/post/148842796218/introducing-variational-autoencoders-in-prose-and](http://blog.fastforwardlabs.com/post/148842796218/introducing-variational-autoencoders-in-prose-and)\n\n**Under the Hood of the Variational Autoencoder (in Prose and Code)**\n\n- blog: [http://blog.fastforwardlabs.com/post/149329060653/under-the-hood-of-the-variational-autoencoder-in](http://blog.fastforwardlabs.com/post/149329060653/under-the-hood-of-the-variational-autoencoder-in)\n\n**The Unreasonable Confusion of Variational Autoencoders**\n\n- blog: [https://jaan.io/unreasonable-confusion/](https://jaan.io/unreasonable-confusion/)\n\n**Variational Autoencoder for Deep Learning of Images, Labels and Captions**\n\n- intro: NIPS 2016. Duke University & Nokia Bell Labs\n- paper: [http://people.ee.duke.edu/~lcarin/Yunchen_nips_2016.pdf](http://people.ee.duke.edu/~lcarin/Yunchen_nips_2016.pdf)\n\n**Convolutional variational autoencoder with PyMC3 and Keras**\n\n[http://nbviewer.jupyter.org/github/taku-y/pymc3/blob/89b8634a2fd30ef96429953558bf360132b6153f/docs/source/notebooks/convolutional_vae_keras_advi.ipynb](http://nbviewer.jupyter.org/github/taku-y/pymc3/blob/89b8634a2fd30ef96429953558bf360132b6153f/docs/source/notebooks/convolutional_vae_keras_advi.ipynb)\n\n**Pixelvae: A Latent Variable Model For Natural Images**\n\n- paper: [http://openreview.net/pdf?id=BJKYvt5lg](http://openreview.net/pdf?id=BJKYvt5lg)\n\n**beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework**\n\n- paper: [http://openreview.net/pdf?id=Sy2fzU9gl](http://openreview.net/pdf?id=Sy2fzU9gl)\n- github: [https://github.com/crcrpar/chainer-VAE](https://github.com/crcrpar/chainer-VAE)\n\n**Variational Lossy Autoencoder**\n\n- arxiv: [https://arxiv.org/abs/1611.02731](https://arxiv.org/abs/1611.02731)\n\n**Convolutional Autoencoders**\n\n- blog: [https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/](https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/)\n\n**Convolutional Autoencoders in Tensorflow**\n\n- blog: [https://pgaleone.eu/neural-networks/deep-learning/2016/12/13/convolutional-autoencoders-in-tensorflow/](https://pgaleone.eu/neural-networks/deep-learning/2016/12/13/convolutional-autoencoders-in-tensorflow/)\n\n**A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe**\n\n- arxiv: [https://arxiv.org/abs/1701.04949](https://arxiv.org/abs/1701.04949)\n\n**Deep Matching Autoencoders**\n\n- intro: University of Edinburgh & RIKEN AIP\n- keywords: Deep Matching Autoencoders (DMAE)\n- arxiv: [https://arxiv.org/abs/1711.06047](https://arxiv.org/abs/1711.06047)\n\n**Understanding Autoencoders with Information Theoretic Concepts**\n\n- intro: University of Florida\n- arxiv: [https://arxiv.org/abs/1804.00057](https://arxiv.org/abs/1804.00057)\n\n**Hyperspherical Variational Auto-Encoders**\n\n- intro: University of Amsterdam\n- project page: [https://nicola-decao.github.io/s-vae/](https://nicola-decao.github.io/s-vae/)\n- arxiv: [https://arxiv.org/abs/1804.00891](https://arxiv.org/abs/1804.00891)\n- github: [https://github.com/nicola-decao/s-vae](https://github.com/nicola-decao/s-vae)\n\n**Spatial Frequency Loss for Learning Convolutional Autoencoders**\n\n[https://arxiv.org/abs/1806.02336](https://arxiv.org/abs/1806.02336)\n\n**DAQN: Deep Auto-encoder and Q-Network**\n\n[https://arxiv.org/abs/1806.00630](https://arxiv.org/abs/1806.00630)\n\n**Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1807.07543](https://arxiv.org/abs/1807.07543)\n- github: [https://github.com/brain-research/acai](https://github.com/brain-research/acai)\n\n# RBM (Restricted Boltzmann Machine)\n\n## Papers\n\n**Deep Boltzmann Machines**\n\n- author: Ruslan Salakhutdinov, Geoffrey Hinton\n- paper: [http://www.cs.toronto.edu/~hinton/absps/dbm.pdf](http://www.cs.toronto.edu/~hinton/absps/dbm.pdf)\n\n**On the Equivalence of Restricted Boltzmann Machines and Tensor Network States**\n\n- arxiv: [https://arxiv.org/abs/1701.04831](https://arxiv.org/abs/1701.04831)\n- github: [https://github.com/yzcj105/rbm2mps](https://github.com/yzcj105/rbm2mps)\n\n**Matrix Product Operator Restricted Boltzmann Machines**\n\n[https://arxiv.org/abs/1811.04608](https://arxiv.org/abs/1811.04608)\n\n## Blogs\n\n**A Tutorial on Restricted Boltzmann Machines**\n\n[http://xiangjiang.live/2016/02/12/a-tutorial-on-restricted-boltzmann-machines/](http://xiangjiang.live/2016/02/12/a-tutorial-on-restricted-boltzmann-machines/)\n\n**Dreaming of names with RBMs**\n\n- blog: [http://colinmorris.github.io/blog/dreaming-rbms](http://colinmorris.github.io/blog/dreaming-rbms)\n- github: [https://github.com/colinmorris/char-rbm](https://github.com/colinmorris/char-rbm)\n\n**on Cheap Learning: Partition Functions and RBMs**\n\n- blog: [https://charlesmartin14.wordpress.com/2016/09/10/on-cheap-learning-partition-functions-and-rbms/](https://charlesmartin14.wordpress.com/2016/09/10/on-cheap-learning-partition-functions-and-rbms/)\n\n**Improving RBMs with physical chemistry**\n\n- blog: [https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/](https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/)\n- github: [https://github.com/charlesmartin14/emf-rbm/blob/master/EMF_RBM_Test.ipynb](https://github.com/charlesmartin14/emf-rbm/blob/master/EMF_RBM_Test.ipynb)\n\n## Projects\n\n**Restricted Boltzmann Machine (Haskell)**\n\n- intro: \"This is an implementation of two machine learning algorithms, [Contrastive Divergence](http://rawgit.com/aeyakovenko/rbm/master/docs/hinton_rbm_guide.pdf) \nand [Back-propagation](http://rawgit.com/aeyakovenko/rbm/master/docs/rojas-backprop.pdf).\"\n- github: [https://github.com/aeyakovenko/rbm](https://github.com/aeyakovenko/rbm)\n\n**tensorflow-rbm: Tensorflow implementation of Restricted Boltzman Machine**\n\n- intro: Tensorflow implementation of Restricted Boltzman Machine for layerwise pretraining of deep autoencoders.\n- github: [https://github.com/meownoid/tensorfow-rbm](https://github.com/meownoid/tensorfow-rbm)\n\n## Videos\n\n**Modelling a text corpus using Deep Boltzmann Machines**\n\n- youtube: [https://www.youtube.com/watch?v=uju4RXEniA8](https://www.youtube.com/watch?v=uju4RXEniA8)\n\n**Foundations of Unsupervised Deep Learning**\n\n- intro: Ruslan Salakhutdinov [CMU]\n- youtube: [https://www.youtube.com/watch?v=rK6bchqeaN8](https://www.youtube.com/watch?v=rK6bchqeaN8)\n- mirror: [https://pan.baidu.com/s/1mi4nCow](https://pan.baidu.com/s/1mi4nCow)\n- sildes: [http://www.cs.cmu.edu/~rsalakhu/talk_unsup.pdf](http://www.cs.cmu.edu/~rsalakhu/talk_unsup.pdf)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","title":"Video Applications"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Video Applications\ndate: 2015-10-09\n---\n\n# Papers\n\n**You Lead, We Exceed: Labor-Free Video Concept Learningby Jointly Exploiting Web Videos and Images**\n\n- intro: CVPR 2016\n- intro: Lead–Exceed Neural Network (LENN), LSTM\n- paper: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/CVPR16_webly_final.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/CVPR16_webly_final.pdf)\n\n**Video Fill in the Blank with Merging LSTMs**\n\n- intro: for Large Scale Movie Description and Understanding Challenge (LSMDC) 2016, \"Movie fill-in-the-blank\" Challenge, UCF_CRCV\n- intro: Video-Fill-in-the-Blank (ViFitB)\n- arxiv: [https://arxiv.org/abs/1610.04062](https://arxiv.org/abs/1610.04062)\n\n**Video Pixel Networks**\n\n- intro: Google DeepMind\n- arxiv: [https://arxiv.org/abs/1610.00527](https://arxiv.org/abs/1610.00527)\n\n**Robust Video Synchronization using Unsupervised Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1610.05985](https://arxiv.org/abs/1610.05985)\n\n**Video Propagation Networks**\n\n- intro: CVPR 2017. Max Planck Institute for Intelligent Systems & Bernstein Center for Computational Neuroscience\n- project page: [https://varunjampani.github.io/vpn/](https://varunjampani.github.io/vpn/)\n- arxiv: [https://arxiv.org/abs/1612.05478](https://arxiv.org/abs/1612.05478)\n- github(Caffe): [https://github.com/varunjampani/video_prop_networks](https://github.com/varunjampani/video_prop_networks)\n\n**Video Frame Synthesis using Deep Voxel Flow**\n\n- project page: [https://liuziwei7.github.io/projects/VoxelFlow.html](https://liuziwei7.github.io/projects/VoxelFlow.html)\n- arxiv: [https://arxiv.org/abs/1702.02463](https://arxiv.org/abs/1702.02463)\n\n**Optimizing Deep CNN-Based Queries over Video Streams at Scale**\n\n- intro: Stanford InfoLab\n- keywords: NoScope. difference detectors, specialized models\n- arxiv: [https://arxiv.org/abs/1703.02529](https://arxiv.org/abs/1703.02529)\n- github: [https://github.com/stanford-futuredata/noscope](https://github.com/stanford-futuredata/noscope)\n- github: [https://github.com/stanford-futuredata/tensorflow-noscope](https://github.com/stanford-futuredata/tensorflow-noscope)\n\n**NoScope: 1000x Faster Deep Learning Queries over Video**\n\n[http://dawn.cs.stanford.edu/2017/06/22/noscope/](http://dawn.cs.stanford.edu/2017/06/22/noscope/)\n\n**Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos**\n\n- intro: CVPR 2017. Stanford University & University of Southern California\n- arxiv: [https://arxiv.org/abs/1703.02521](https://arxiv.org/abs/1703.02521)\n\n**ProcNets: Learning to Segment Procedures in Untrimmed and Unconstrained Videos**\n\n[https://arxiv.org/abs/1703.09788](https://arxiv.org/abs/1703.09788)\n\n**Unsupervised Learning Layers for Video Analysis**\n\n- intro: Baidu Research\n- intro: \"The experiments demonstrated the potential applications of UL layers and online learning algorithm to head orientation estimation and moving object localization\"\n- arxiv: [https://arxiv.org/abs/1705.08918](https://arxiv.org/abs/1705.08918)\n\n**Look, Listen and Learn**\n\n- intro: DeepMind\n- intro: \"Audio-Visual Correspondence\" learning\n- arxiv: [https://arxiv.org/abs/1705.08168](https://arxiv.org/abs/1705.08168)\n\n**Video Imagination from a Single Image with Transformation Generation**\n\n- intro: Peking University\n- arxiv: [https://arxiv.org/abs/1706.04124](https://arxiv.org/abs/1706.04124)\n- github: [https://github.com/gitpub327/VideoImagination](https://github.com/gitpub327/VideoImagination)\n\n**Learning to Learn from Noisy Web Videos**\n\n- intro: CVPR 2017. Stanford University & CMU & Simon Fraser University\n- arxiv: [https://arxiv.org/abs/1706.02884](https://arxiv.org/abs/1706.02884)\n\n**Convolutional Long Short-Term Memory Networks for Recognizing First Person Interactions**\n\n- intro: Accepted on the second International Workshop on Egocentric Perception, Interaction and Computing(EPIC) at International Conference on Computer Vision(ICCV-17)\n- arxiv: [https://arxiv.org/abs/1709.06495](https://arxiv.org/abs/1709.06495)\n\n**Learning Binary Residual Representations for Domain-specific Video Streaming**\n\n- intro: AAAI 2018\n- project page: [http://research.nvidia.com/publication/2018-02_Learning-Binary-Residual](http://research.nvidia.com/publication/2018-02_Learning-Binary-Residual)\n- arxiv: [https://arxiv.org/abs/1712.05087](https://arxiv.org/abs/1712.05087)\n\n**Video Representation Learning Using Discriminative Pooling**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.10628](https://arxiv.org/abs/1803.10628)\n\n**Rethinking the Faster R-CNN Architecture for Temporal Action Localization**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.07667](https://arxiv.org/abs/1804.07667)\n\n**Deep Keyframe Detection in Human Action Videos**\n\n- intro: two-stream ConvNet\n- arxiv: [https://arxiv.org/abs/1804.10021](https://arxiv.org/abs/1804.10021)\n\n**FFNet: Video Fast-Forwarding via Reinforcement Learning**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1805.02792](https://arxiv.org/abs/1805.02792)\n\n**Fast forwarding Egocentric Videos by Listening and Watching**\n\n[https://arxiv.org/abs/1806.04620](https://arxiv.org/abs/1806.04620)\n\n**Scanner: Efficient Video Analysis at Scale**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1805.07339](https://arxiv.org/abs/1805.07339)\n\n**Massively Parallel Video Networks**\n\n- intro: DeepMind & University of Oxford\n- arxiv: [https://arxiv.org/abs/1806.03863](https://arxiv.org/abs/1806.03863)\n\n**Object Level Visual Reasoning in Videos**\n\n- intro: LIRIS & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1806.06157](https://arxiv.org/abs/1806.06157)\n\n**Video Time: Properties, Encoders and Evaluation**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.06980](https://arxiv.org/abs/1807.06980)\n\n**Inserting Videos into Videos**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.06571](https://arxiv.org/abs/1903.06571)\n\n# Video Classification\n\n**Large-scale Video Classification with Convolutional Neural Networks**\n\n- intro: CVPR 2014\n- project page: [http://cs.stanford.edu/people/karpathy/deepvideo/](http://cs.stanford.edu/people/karpathy/deepvideo/)\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf)\n\n**Exploiting Image-trained CNN Architectures for Unconstrained Video Classification**\n\n- intro: Video-level event detection. extracting deep features for each frame, averaging frame-level deep features\n- arxiv: [http://arxiv.org/abs/1503.04144](http://arxiv.org/abs/1503.04144)\n\n**Beyond Short Snippets: Deep Networks for Video Classification**\n\n- intro: CNN + LSTM\n- arxiv: [http://arxiv.org/abs/1503.08909](http://arxiv.org/abs/1503.08909)\n- demo: [http://pan.baidu.com/s/1eQ9zLZk](http://pan.baidu.com/s/1eQ9zLZk)\n\n**Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification**\n\n- intro: ACM Multimedia, 2015\n- arxiv: [http://arxiv.org/abs/1504.01561](http://arxiv.org/abs/1504.01561)\n\n**Video Content Recognition with Deep Learning**\n\n- author: Zuxuan Wu, Fudan University\n- slides: [http://vision.ouc.edu.cn/valse/slides/20160420/Zuxuan%20Wu%20-%20Video%20Content%20Recognition%20with%20Deep%20Learning-Zuxuan%20Wu.pdf](http://vision.ouc.edu.cn/valse/slides/20160420/Zuxuan%20Wu%20-%20Video%20Content%20Recognition%20with%20Deep%20Learning-Zuxuan%20Wu.pdf)\n\n**Video Content Recognition with Deep Learning**\n\n- author: Yu-Gang Jiang, Lab for Big Video Data Analytics (BigVid), Fudan University\n- slides: [http://www.yugangjiang.info/slides/DeepVideoTalk-2015.pdf](http://www.yugangjiang.info/slides/DeepVideoTalk-2015.pdf)\n\n**Efficient Large Scale Video Classification**\n\n- intro: Google\n- arxiv: [http://arxiv.org/abs/1505.06250](http://arxiv.org/abs/1505.06250)\n\n**Fusing Multi-Stream Deep Networks for Video Classification**\n\n- arxiv: [http://arxiv.org/abs/1509.06086](http://arxiv.org/abs/1509.06086)\n\n**Learning End-to-end Video Classification with Rank-Pooling**\n\n- paper: [http://jmlr.org/proceedings/papers/v48/fernando16.html](http://jmlr.org/proceedings/papers/v48/fernando16.html)\n- paper: [http://jmlr.csail.mit.edu/proceedings/papers/v48/fernando16.pdf](http://jmlr.csail.mit.edu/proceedings/papers/v48/fernando16.pdf)\n- summary(by Hugo Larochelle): [http://www.shortscience.org/paper?bibtexKey=conf/icml/FernandoG16#hlarochelle](http://www.shortscience.org/paper?bibtexKey=conf/icml/FernandoG16#hlarochelle)\n\n**Deep Learning for Video Classification and Captioning**\n\n- arxiv: [http://arxiv.org/abs/1609.06782](http://arxiv.org/abs/1609.06782)\n\n**Fast Video Classification via Adaptive Cascading of Deep Models**\n\n- arxiv: [https://arxiv.org/abs/1611.06453](https://arxiv.org/abs/1611.06453)\n\n**Deep Feature Flow for Video Recognition**\n\n- intro: CVPR 2017\n- intro: It provides a simple, fast, accurate, and end-to-end framework for video recognition (e.g., object detection and semantic segmentation in videos)\n- arxiv: [https://arxiv.org/abs/1611.07715](https://arxiv.org/abs/1611.07715)\n- github(official, MXNet): [https://github.com/msracver/Deep-Feature-Flow](https://github.com/msracver/Deep-Feature-Flow)\n- youtube: [https://www.youtube.com/watch?v=J0rMHE6ehGw](https://www.youtube.com/watch?v=J0rMHE6ehGw)\n\n**Large-Scale YouTube-8M Video Understanding with Deep Neural Networks**\n\n[https://arxiv.org/abs/1706.04488](https://arxiv.org/abs/1706.04488)\n\n**Deep Learning Methods for Efficient Large Scale Video Labeling**\n\n- intro: Solution to the Kaggle's competition Google Cloud & YouTube-8M Video Understanding Challenge\n- arxiv: [https://arxiv.org/abs/1706.04572](https://arxiv.org/abs/1706.04572)\n- github: [https://github.com/mpekalski/Y8M](https://github.com/mpekalski/Y8M)\n\n**Learnable pooling with Context Gating for video classification**\n\n- intro: CVPR17 Youtube 8M workshop. Kaggle 1st place\n- arxiv: [https://arxiv.org/abs/1706.06905](https://arxiv.org/abs/1706.06905)\n- github: [https://github.com/antoine77340/LOUPE](https://github.com/antoine77340/LOUPE)\n\n**Aggregating Frame-level Features for Large-Scale Video Classification**\n\n- intro: Youtube-8M Challenge, 4th place\n- arxiv: [https://arxiv.org/abs/1707.00803](https://arxiv.org/abs/1707.00803)\n\n**Tensor-Train Recurrent Neural Networks for Video Classification**\n\n[https://arxiv.org/abs/1707.01786](https://arxiv.org/abs/1707.01786)\n\n**Hierarchical Deep Recurrent Architecture for Video Understanding**\n\n- intro: Classification Challenge Track paper in CVPR 2017 Workshop on YouTube-8M Large-Scale Video Understanding\n- arxiv: [https://arxiv.org/abs/1707.03296](https://arxiv.org/abs/1707.03296)\n\n**Large-scale Video Classification guided by Batch Normalized LSTM Translator**\n\n- intro: CVPR2017 Workshop on Youtube-8M Large-scale Video Understanding\n- arxiv: [https://arxiv.org/abs/1707.04045](https://arxiv.org/abs/1707.04045)\n\n**UTS submission to Google YouTube-8M Challenge 2017**\n\n- intro: CVPR'17 Workshop on YouTube-8M\n- arxiv: [https://arxiv.org/abs/1707.04143](https://arxiv.org/abs/1707.04143)\n- github: [https://github.com/ffmpbgrnn/yt8m](https://github.com/ffmpbgrnn/yt8m)\n\n**A spatiotemporal model with visual attention for video classification**\n\n[https://arxiv.org/abs/1707.02069](https://arxiv.org/abs/1707.02069)\n\n**Cultivating DNN Diversity for Large Scale Video Labelling**\n\n- intro: CVPR 2017 Youtube-8M Workshop\n- arxiv: [https://arxiv.org/abs/1707.04272](https://arxiv.org/abs/1707.04272)\n\n**Attention Transfer from Web Images for Video Recognition**\n\n- intro: ACM Multimedia, 2017\n- arxiv: [https://arxiv.org/abs/1708.00973](https://arxiv.org/abs/1708.00973)\n\n**Non-local Neural Networks**\n\n- intro: CVPR 2018. CMU & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1711.07971](https://arxiv.org/abs/1711.07971)\n- github(Caffe2): [https://github.com/facebookresearch/video-nonlocal-net](https://github.com/facebookresearch/video-nonlocal-net)\n\n**Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification**\n\n[https://arxiv.org/abs/1711.08200](https://arxiv.org/abs/1711.08200)\n\n**Appearance-and-Relation Networks for Video Classification**\n\n- arxiv: [https://arxiv.org/abs/1711.09125](https://arxiv.org/abs/1711.09125)\n- github: [https://github.com/wanglimin/ARTNet](https://github.com/wanglimin/ARTNet)\n\n**Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification**\n\n- intro: ECCV 2018. Google Research & University of California San Diego\n- arxiv: [https://arxiv.org/abs/1712.04851](https://arxiv.org/abs/1712.04851)\n\n**Long Activity Video Understanding using Functional Object-Oriented Network**\n\n[https://arxiv.org/abs/1807.00983](https://arxiv.org/abs/1807.00983)\n\n**Deep Architectures and Ensembles for Semantic Video Classification**\n\n[https://arxiv.org/abs/1807.01026](https://arxiv.org/abs/1807.01026)\n\n**Deep Discriminative Model for Video Classification**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.08259](https://arxiv.org/abs/1807.08259)\n\n**Deep Video Color Propagation**\n\n- intro: BMVC 2018\n- arxuv: [https://arxiv.org/abs/1808.03232](https://arxiv.org/abs/1808.03232)\n\n**Non-local NetVLAD Encoding for Video Classification**\n\n- intro: ECCV 2018 workshop on YouTube-8M Large-Scale Video Understanding\n- intro: Tencent AI Lab & Fudan University\n- arxiv: [https://arxiv.org/abs/1810.00207](https://arxiv.org/abs/1810.00207)\n\n**Learnable Pooling Methods for Video Classification**\n\n- intro: Youtube 8M ECCV18 Workshop\n- arxiv: [https://arxiv.org/abs/1810.00530](https://arxiv.org/abs/1810.00530)\n- github: [https://github.com/pomonam/LearnablePoolingMethods](https://github.com/pomonam/LearnablePoolingMethods)\n\n**NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification**\n\n- intro: ECCV 2018 workshop\n- arxiv: [https://arxiv.org/abs/1811.05014](https://arxiv.org/abs/1811.05014)\n- github: [https://github.com/linrongc/youtube-8m](https://github.com/linrongc/youtube-8m)\n\n**High Order Neural Networks for Video Classification**\n\n- intro: Fudan University, Carnegie Mellon University, Qiniu Inc., ByteDance AI Lab\n- arxiv: [https://arxiv.org/abs/1811.07519](https://arxiv.org/abs/1811.07519)\n\n**TSM: Temporal Shift Module for Efficient Video Understanding**\n\n- intro: ICCV 2019\n- intro: MIT & MIT-IBM Watson AI Lab\n- arxiv: [https://arxiv.org/abs/1811.08383](https://arxiv.org/abs/1811.08383)\n- github: [https://github.com/mit-han-lab/temporal-shift-module](https://github.com/mit-han-lab/temporal-shift-module)\n\n**SlowFast Networks for Video Recognition**\n\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/1812.03982](https://arxiv.org/abs/1812.03982)\n\n**Efficient Video Classification Using Fewer Frames**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1902.10640](https://arxiv.org/abs/1902.10640)\n\n**Video Classification with Channel-Separated Convolutional Networks**\n\n- intro: Facebook AI\n- arxiv: [https://arxiv.org/abs/1904.02811](https://arxiv.org/abs/1904.02811)\n\n**Two-Stream Video Classification with Cross-Modality Attention**\n\n[https://arxiv.org/abs/1908.00497](https://arxiv.org/abs/1908.00497)\n\n## Action Detection / Activity Recognition\n\n**3d convolutional neural networks for human action recognition**\n\n- paper: [http://www.cs.odu.edu/~sji/papers/pdf/Ji_ICML10.pdf](http://www.cs.odu.edu/~sji/papers/pdf/Ji_ICML10.pdf)\n\n**Sequential Deep Learning for Human Action Recognition**\n\n- paper: [http://liris.cnrs.fr/Documents/Liris-5228.pdf](http://liris.cnrs.fr/Documents/Liris-5228.pdf)\n\n**Two-stream convolutional networks for action recognition in videos**\n\n- arxiv: [http://arxiv.org/abs/1406.2199](http://arxiv.org/abs/1406.2199)\n\n**Finding action tubes**\n\n- intro: \"built action models from shape and motion cues. \nThey start from the image proposals and select the motion salient subset of them and\nextract saptio-temporal features to represent the video using the CNNs.\"\n- arxiv: [http://arxiv.org/abs/1411.6031](http://arxiv.org/abs/1411.6031)\n\n**Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition**\n\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf)\n\n**Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors**\n\n- intro: CVPR 2015. TDD\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf)\n- ext: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_105_ext.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_105_ext.pdf)\n- poster: [https://wanglimin.github.io/papers/WangQT_CVPR15_Poster.pdf](https://wanglimin.github.io/papers/WangQT_CVPR15_Poster.pdf)\n- github: [https://github.com/wanglimin/TDD](https://github.com/wanglimin/TDD)\n\n**Action Recognition by Hierarchical Mid-level Action Elements**\n\n- paper: [http://cvgl.stanford.edu/papers/tian2015.pdf](http://cvgl.stanford.edu/papers/tian2015.pdf)\n\n**Contextual Action Recognition with R CNN**\n\n- arxiv: [http://arxiv.org/abs/1505.01197](http://arxiv.org/abs/1505.01197)\n- github: [https://github.com/gkioxari/RstarCNN](https://github.com/gkioxari/RstarCNN)\n\n**Towards Good Practices for Very Deep Two-Stream ConvNets**\n\n- arxiv: [http://arxiv.org/abs/1507.02159](http://arxiv.org/abs/1507.02159)\n- github: [https://github.com/yjxiong/caffe](https://github.com/yjxiong/caffe)\n\n**Action Recognition using Visual Attention**\n\n- intro: LSTM / RNN\n- arxiv: [http://arxiv.org/abs/1511.04119](http://arxiv.org/abs/1511.04119)\n- project page: [http://shikharsharma.com/projects/action-recognition-attention/](http://shikharsharma.com/projects/action-recognition-attention/)\n- github(Python/Theano): [https://github.com/kracwarlock/action-recognition-visual-attention](https://github.com/kracwarlock/action-recognition-visual-attention)\n\n**End-to-end Learning of Action Detection from Frame Glimpses in Videos**\n\n- intro: CVPR 2016\n- project page: [http://ai.stanford.edu/~syyeung/frameglimpses.html](http://ai.stanford.edu/~syyeung/frameglimpses.html)\n- arxiv: [http://arxiv.org/abs/1511.06984](http://arxiv.org/abs/1511.06984)\n- paper: [http://vision.stanford.edu/pdf/yeung2016cvpr.pdf](http://vision.stanford.edu/pdf/yeung2016cvpr.pdf)\n\n**Multi-velocity neural networks for gesture recognition in videos**\n\n- arxiv: [http://arxiv.org/abs/1603.06829](http://arxiv.org/abs/1603.06829)\n\n**Active Learning for Online Recognition of Human Activities from Streaming Videos**\n\n- arxiv: [http://arxiv.org/abs/1604.02855](http://arxiv.org/abs/1604.02855)\n\n**Convolutional Two-Stream Network Fusion for Video Action Recognition**\n\n- arxiv: [http://arxiv.org/abs/1604.06573](http://arxiv.org/abs/1604.06573)\n- github: [https://github.com/feichtenhofer/twostreamfusion](https://github.com/feichtenhofer/twostreamfusion)\n\n**Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables**\n\n- arxiv: [http://arxiv.org/abs/1604.08880](http://arxiv.org/abs/1604.08880)\n\n**Unsupervised Semantic Action Discovery from Video Collections**\n\n- arxiv: [http://arxiv.org/abs/1605.03324](http://arxiv.org/abs/1605.03324)\n\n**Anticipating Visual Representations from Unlabeled Video**\n\n- paper: [http://web.mit.edu/vondrick/prediction.pdf](http://web.mit.edu/vondrick/prediction.pdf)\n\n**VideoLSTM Convolves, Attends and Flows for Action Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.01794](http://arxiv.org/abs/1607.01794)\n\n**Hierarchical Attention Network for Action Recognition in Videos (HAN)**\n\n- arxiv: [http://arxiv.org/abs/1607.06416](http://arxiv.org/abs/1607.06416)\n\n**Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.07043](http://arxiv.org/abs/1607.07043)\n\n**Connectionist Temporal Modeling for Weakly Supervised Action Labeling**\n\n- arxiv: [http://arxiv.org/abs/1607.08584](http://arxiv.org/abs/1607.08584)\n\n**CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016**\n\n- intro: won the 1st place in the untrimmed video classification task of ActivityNet Challenge 2016. TSN\n- arxiv: [http://arxiv.org/abs/1608.00797](http://arxiv.org/abs/1608.00797)\n- github: [https://github.com/yjxiong/anet2016-cuhk](https://github.com/yjxiong/anet2016-cuhk)\n\n**Actionness Estimation Using Hybrid FCNs**\n\n![](http://wanglimin.github.io/actionness_hfcn/actionness.png)\n\n- intro: CVPR 2016. H-FCN\n- project page: [http://wanglimin.github.io/actionness_hfcn/index.html](http://wanglimin.github.io/actionness_hfcn/index.html)\n- paper: [http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf](http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf)\n- github: [https://github.com/wanglimin/actionness-estimation/](https://github.com/wanglimin/actionness-estimation/)\n\n**Real-time Action Recognition with Enhanced Motion Vector CNNs**\n\n![](http://zbwglory.github.io/MV-CNN/framework.jpg)\n\n- intro: CVPR 2016\n- project page: [http://zbwglory.github.io/MV-CNN/index.html](http://zbwglory.github.io/MV-CNN/index.html)\n- paper: [http://wanglimin.github.io/papers/ZhangWWQW_CVPR16.pdf](http://wanglimin.github.io/papers/ZhangWWQW_CVPR16.pdf)\n- github: [https://github.com/zbwglory/MV-release](https://github.com/zbwglory/MV-release)\n\n**Temporal Segment Networks: Towards Good Practices for Deep Action Recognition**\n\n- intro: ECCV 2016. HMDB51: 69.4%, UCF101: 94.2%\n- arxiv: [http://arxiv.org/abs/1608.00859](http://arxiv.org/abs/1608.00859)\n- paper: [http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf](http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf)\n- github: [https://github.com/yjxiong/temporal-segment-networks](https://github.com/yjxiong/temporal-segment-networks)\n\n**Temporal Segment Networks for Action Recognition in Videos**\n\n- intro: An extension of submission [http://arxiv.org/abs/1608.00859](http://arxiv.org/abs/1608.00859)\n- arxiv: [https://arxiv.org/abs/1705.02953](https://arxiv.org/abs/1705.02953)\n\n**Hierarchical Attention Network for Action Recognition in Videos**\n\n- arxiv: [http://arxiv.org/abs/1607.06416](http://arxiv.org/abs/1607.06416)\n\n**DeepCAMP: Deep Convolutional Action & Attribute Mid-Level Patterns**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1608.03217](http://arxiv.org/abs/1608.03217)\n\n**Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition**\n\n- arxiv: [http://arxiv.org/abs/1608.04339](http://arxiv.org/abs/1608.04339)\n\n**Dynamic Image Networks for Action Recognition**\n\n- intro: CVPR 2016\n- arxiv: [http://users.cecs.anu.edu.au/~sgould/papers/cvpr16-dynamic_images.pdf](http://users.cecs.anu.edu.au/~sgould/papers/cvpr16-dynamic_images.pdf)\n- github: [https://github.com/hbilen/dynamic-image-nets](https://github.com/hbilen/dynamic-image-nets)\n\n**Human Action Recognition without Human**\n\n- arxiv: [http://arxiv.org/abs/1608.07876](http://arxiv.org/abs/1608.07876)\n\n**Temporal Convolutional Networks: A Unified Approach to Action Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1608.08242](http://arxiv.org/abs/1608.08242)\n- ECCV 2016 workshop: [http://bravenewmotion.github.io/](http://bravenewmotion.github.io/)\n\n**Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks**\n\n- intro: Bachelor Thesis Report at ETSETB TelecomBCN\n- project page: [https://imatge-upc.github.io/activitynet-2016-cvprw/](https://imatge-upc.github.io/activitynet-2016-cvprw/)\n- arxiv: [http://arxiv.org/abs/1608.08128](http://arxiv.org/abs/1608.08128)\n- github: [https://github.com/imatge-upc/activitynet-2016-cvprw](https://github.com/imatge-upc/activitynet-2016-cvprw)\n\n**Sequential Deep Trajectory Descriptor for Action Recognition with Three-stream CNN**\n\n- arxiv: [http://arxiv.org/abs/1609.03056](http://arxiv.org/abs/1609.03056)\n\n**Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at Extremely Low Resolutions**\n\n- arxiv: [https://arxiv.org/abs/1610.03898](https://arxiv.org/abs/1610.03898)\n\n**Spatiotemporal Residual Networks for Video Action Recognition**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1611.02155](https://arxiv.org/abs/1611.02155)\n\n**Action Recognition Based on Joint Trajectory Maps Using Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.02447](https://arxiv.org/abs/1611.02447)\n\n**Deep Recurrent Neural Network for Mobile Human Activity Recognition with High Throughput**\n\n- arxiv: [https://arxiv.org/abs/1611.03607](https://arxiv.org/abs/1611.03607)\n\n**Joint Network based Attention for Action Recognition**\n\n- arxiv: [https://arxiv.org/abs/1611.05215](https://arxiv.org/abs/1611.05215)\n\n**Temporal Convolutional Networks for Action Segmentation and Detection**\n\n- arxiv: [https://arxiv.org/abs/1611.05267](https://arxiv.org/abs/1611.05267)\n\n**AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos**\n\n- arxiv: [https://arxiv.org/abs/1611.08240](https://arxiv.org/abs/1611.08240)\n\n**ActionFlowNet: Learning Motion Representation for Action Recognition**\n\n- arxiv: [https://arxiv.org/abs/1612.03052](https://arxiv.org/abs/1612.03052)\n\n**Higher-order Pooling of CNN Features via Kernel Linearization for Action Recognition**\n\n- intro: Australian Center for Robotic Vision & Data61/CSIRO\n- arxiv: [https://arxiv.org/abs/1701.05432](https://arxiv.org/abs/1701.05432)\n\n**Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos**\n\n[https://arxiv.org/abs/1703.10664](https://arxiv.org/abs/1703.10664)\n\n**Temporal Action Detection with Structured Segment Networks**\n\n- project page: [http://yjxiong.me/others/ssn/](http://yjxiong.me/others/ssn/)\n- arxiv: [https://arxiv.org/abs/1704.06228](https://arxiv.org/abs/1704.06228)\n- github: [https://github.com/yjxiong/action-detection](https://github.com/yjxiong/action-detection)\n\n**Recurrent Residual Learning for Action Recognition**\n\n[https://arxiv.org/abs/1706.08807](https://arxiv.org/abs/1706.08807)\n\n**Hierarchical Multi-scale Attention Networks for Action Recognition**\n\n[https://arxiv.org/abs/1708.07590](https://arxiv.org/abs/1708.07590)\n\n**Two-stream Flow-guided Convolutional Attention Networks for Action Recognition**\n\n- intro: International Conference of Computer Vision Workshop (ICCVW), 2017\n- arxiv: [https://arxiv.org/abs/1708.09268](https://arxiv.org/abs/1708.09268)\n\n**Action Classification and Highlighting in Videos**\n\n[https://arxiv.org/abs/1708.09522](https://arxiv.org/abs/1708.09522)\n\n**Real-Time Action Detection in Video Surveillance using Sub-Action Descriptor with Multi-CNN**\n\n[https://arxiv.org/abs/1710.03383](https://arxiv.org/abs/1710.03383)\n\n**End-to-end Video-level Representation Learning for Action Recognition**\n\n- keywords: Deep networks with Temporal Pyramid Pooling (DTPP)\n- arxiv: [https://arxiv.org/abs/1711.04161](https://arxiv.org/abs/1711.04161)\n\n**Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low Resolution Action Recognition**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.03983](https://arxiv.org/abs/1801.03983)\n\n**DiscrimNet: Semi-Supervised Action Recognition from Videos using Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1801.07230](https://arxiv.org/abs/1801.07230)\n\n**A Fusion of Appearance based CNNs and Temporal evolution of Skeleton with LSTM for Daily Living Action Recognition**\n\n[https://arxiv.org/abs/1802.00421](https://arxiv.org/abs/1802.00421)\n\n**Real-Time End-to-End Action Detection with Two-Stream Networks**\n\n[https://arxiv.org/abs/1802.08362](https://arxiv.org/abs/1802.08362)\n\n**A Closer Look at Spatiotemporal Convolutions for Action Recognition**\n\n- intro: CVPR 2018. Facebook Research\n- intro: R(2+1)D and Mixed-Convolutions for Action Recognition.\n- project page: [https://dutran.github.io/R2Plus1D/](https://dutran.github.io/R2Plus1D/)\n- arxiv: [https://arxiv.org/abs/1711.11248](https://arxiv.org/abs/1711.11248)\n- github: [https://github.com/facebookresearch/R2Plus1D](https://github.com/facebookresearch/R2Plus1D)\n\n**VideoCapsuleNet: A Simplified Network for Action Detection**\n\n[https://arxiv.org/abs/1805.08162](https://arxiv.org/abs/1805.08162)\n\n**Where and When to Look? Spatio-temporal Attention for Action Recognition in Videos**\n\n[https://arxiv.org/abs/1810.04511](https://arxiv.org/abs/1810.04511)\n\n**Relational Long Short-Term Memory for Video Action Recognition**\n\n[https://arxiv.org/abs/1811.07059](https://arxiv.org/abs/1811.07059)\n\n**Temporal Recurrent Networks for Online Action Detection**\n\n[https://arxiv.org/abs/1811.073910](https://arxiv.org/abs/1811.07391)\n\n**Video Action Transformer Network**\n\n- intro: Carnegie Mellon University & DeepMind & University of Oxford\n- intro: Ranked first on the AVA (computer vision only) leaderboard of the ActivityNet Challenge 2018\n- project page: [https://rohitgirdhar.github.io/ActionTransformer/](https://rohitgirdhar.github.io/ActionTransformer/)\n- arxiv: [https://arxiv.org/abs/1812.02707](https://arxiv.org/abs/1812.02707)\n\n**D3D: Distilled 3D Networks for Video Action Recognition**\n\n- intro: Google & University of Michigan & Princeton University\n- arxiv: [https://arxiv.org/abs/1812.08249](https://arxiv.org/abs/1812.08249)\n\n**TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1905.13417](https://arxiv.org/abs/1905.13417)\n\n**Deformable Tube Network for Action Detection in Videos**\n\n[https://arxiv.org/abs/1907.01847](https://arxiv.org/abs/1907.01847)\n\n**You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization**\n\n[https://arxiv.org/abs/1911.06644](https://arxiv.org/abs/1911.06644)\n\n**TubeR: Tube-Transformer for Action Detection**\n\n- intro: University of Amsterdam & Amazon Web Service\n- arxiv: [https://arxiv.org/abs/2104.00969](https://arxiv.org/abs/2104.00969)\n\n**Revisiting Skeleton-based Action Recognition**\n\n[https://arxiv.org/abs/2104.13586](https://arxiv.org/abs/2104.13586)\n\n**End-to-end Temporal Action Detection with Transformer**\n\n- arxiv: [https://arxiv.org/abs/2106.10271](https://arxiv.org/abs/2106.10271)\n- github: [https://github.com/xlliu7/TadTR](https://github.com/xlliu7/TadTR)\n\n**OadTR: Online Action Detection with Transformers**\n\n- arxiv: [https://arxiv.org/abs/2106.11149](https://arxiv.org/abs/2106.11149)\n- github: [https://github.com/wangxiang1230/OadTR](https://github.com/wangxiang1230/OadTR)\n\n**VideoLightFormer: Lightweight Action Recognition using Transformers**\n\n[https://arxiv.org/abs/2107.00451](https://arxiv.org/abs/2107.00451)\n\n### Projects\n\n**A Torch Library for Action Recognition and Detection Using CNNs and LSTMs**\n\n- intro: CS231n student project report\n- paper: [http://cs231n.stanford.edu/reports2016/221_Report.pdf](http://cs231n.stanford.edu/reports2016/221_Report.pdf)\n- github: [https://github.com/garythung/torch-lrcn](https://github.com/garythung/torch-lrcn)\n\n**2016 ActivityNet action recognition challenge. CNN + LSTM approach. Multi-threaded loading.**\n\n- github: [https://github.com/jrbtaylor/ActivityNet](https://github.com/jrbtaylor/ActivityNet)\n\n**LSTM for Human Activity Recognition**\n\n- github: [https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/)\n- github(MXNet): [https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition](https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition)\n\n**Scanner: Efficient Video Analysis at Scale**\n\n- intro: Locate and recognize faces in a video, Detect shots in a film, Search videos by image\n- github: [https://github.com/scanner-research/scanner](https://github.com/scanner-research/scanner)\n\n**Charades Starter Code for Activity Classification and Localization**\n\n- intro: Activity Recognition Algorithms for the Charades Dataset\n- github: [https://github.com/gsig/charades-algorithms](https://github.com/gsig/charades-algorithms)\n\n**NonLocalNetwork and Sequeeze-Excitation Network**\n\n- intro: MXNet implementation of Non-Local and Squeeze-Excitation network\n- github: [https://github.com/WillSuen/NonLocalandSEnet](https://github.com/WillSuen/NonLocalandSEnet)\n\n# Event Recognition\n\n**TagBook: A Semantic Video Representation without Supervision for Event Detection**\n\n- arxiv: [http://arxiv.org/abs/1510.02899](http://arxiv.org/abs/1510.02899)\n\n**AENet: Learning Deep Audio Features for Video Analysis**\n\n- arxiv: [https://arxiv.org/abs/1701.00599](https://arxiv.org/abs/1701.00599)\n- github: [https://github.com/znaoya/aenet](https://github.com/znaoya/aenet)\n\n# Event Detection\n\n**DevNet: A Deep Event Network for Multimedia Event Detection and Evidence Recounting**\n\n- paper: [http://120.52.72.47/winsty.net/c3pr90ntcsf0/papers/devnet.pdf](http://120.52.72.47/winsty.net/c3pr90ntcsf0/papers/devnet.pdf)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf)\n\n**Detecting events and key actors in multi-person videos**\n\n![](https://tctechcrunch2011.files.wordpress.com/2016/06/basketball_actors.jpg)\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1511.02917](http://arxiv.org/abs/1511.02917)\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf)\n- paper: [http://vision.stanford.edu/pdf/johnson2016cvpr.pdf](http://vision.stanford.edu/pdf/johnson2016cvpr.pdf)\n- blog: [http://www.leiphone.com/news/201606/l1TKIRFLO3DUFNNu.html](http://www.leiphone.com/news/201606/l1TKIRFLO3DUFNNu.html)\n\n**Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection**\n\n- intro: INTERSPEECH 2016\n- arxiv: [https://arxiv.org/abs/1604.07160](https://arxiv.org/abs/1604.07160)\n\n**Efficient Action Detection in Untrimmed Videos via Multi-Task Learning**\n\n- arxiv: [https://arxiv.org/abs/1612.07403](https://arxiv.org/abs/1612.07403)\n\n**Joint Event Detection and Description in Continuous Video Streams**\n\n- intro: Joint Event Detection and Description Network (JEDDi-Net)\n- arxiv: [https://arxiv.org/abs/1802.10250](https://arxiv.org/abs/1802.10250)\n\n# Abnormality / Anomaly Detection\n\n**Fully Convolutional Neural Network for Fast Anomaly Detection in Crowded Scenes**\n\n- arxiv: [http://arxiv.org/abs/1609.00866](http://arxiv.org/abs/1609.00866)\n\n**Anomaly Detection in Video Using Predictive Convolutional Long Short-Term Memory Networks**\n\n- intro: Rochester Institute of Technology\n- arxiv: [https://arxiv.org/abs/1612.00390](https://arxiv.org/abs/1612.00390)\n\n**Abnormal Event Detection in Videos using Spatiotemporal Autoencoder**\n\n- arxiv: [https://arxiv.org/abs/1701.01546](https://arxiv.org/abs/1701.01546)\n- github: [https://github.com/yshean/abnormal-spatiotemporal-ae](https://github.com/yshean/abnormal-spatiotemporal-ae)\n\n**Abnormal Event Detection in Videos using Generative Adversarial Nets**\n\n- intro: Best Paper / Student Paper Award Finalist, IEEE International Conference on Image Processing (ICIP), 2017\n- arxiv: [https://arxiv.org/abs/1708.09644](https://arxiv.org/abs/1708.09644)\n\n**Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.09121](https://arxiv.org/abs/1709.09121)\n\n**An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos**\n\n- intro: Uncanny Vision Solutions\n- arxiv: [https://arxiv.org/abs/1801.03149](https://arxiv.org/abs/1801.03149)\n\n**STAN: Spatio-Temporal Adversarial Networks for Abnormal Event Detection**\n\n- intro: ICASSP 2018\n- arxiv: [https://arxiv.org/abs/1804.08381](https://arxiv.org/abs/1804.08381)\n\n**Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder**\n\n[https://arxiv.org/abs/1805.11223](https://arxiv.org/abs/1805.11223)\n\n**Attentioned Convolutional LSTM InpaintingNetwork for Anomaly Detection in Videos**\n\n[https://arxiv.org/abs/1811.10228](https://arxiv.org/abs/1811.10228)\n\n# Video Prediction\n\n**Deep multi-scale video prediction beyond mean square error**\n\n- intro: ICLR 2016\n- arxiv: [http://arxiv.org/abs/1511.05440](http://arxiv.org/abs/1511.05440)\n- github: [https://github.com/coupriec/VideoPredictionICLR2016](https://github.com/coupriec/VideoPredictionICLR2016)\n- github(TensorFlow): [https://github.com/dyelax/Adversarial_Video_Generation](https://github.com/dyelax/Adversarial_Video_Generation)\n- demo: [http://cs.nyu.edu/~mathieu/iclr2016.html](http://cs.nyu.edu/~mathieu/iclr2016.html)\n\n**Unsupervised Learning for Physical Interaction through Video Prediction**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1605.07157](https://arxiv.org/abs/1605.07157)\n- github: [https://github.com/tensorflow/models/tree/master/video_prediction](https://github.com/tensorflow/models/tree/master/video_prediction)\n\n**Generating Videos with Scene Dynamics**\n\n- intro: NIPS 2016\n- intro: The model learns to generate tiny videos using adversarial networks\n- project page: [http://web.mit.edu/vondrick/tinyvideo/](http://web.mit.edu/vondrick/tinyvideo/)\n- paper: [http://web.mit.edu/vondrick/tinyvideo/paper.pdf](http://web.mit.edu/vondrick/tinyvideo/paper.pdf)\n- github: [https://github.com/cvondrick/videogan](https://github.com/cvondrick/videogan)\n\n## PredNet\n\n**Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning**\n\n- project page: [https://coxlab.github.io/prednet/](https://coxlab.github.io/prednet/)\n- arxiv: [http://arxiv.org/abs/1605.08104](http://arxiv.org/abs/1605.08104)\n- github: [https://github.com/coxlab/prednet](https://github.com/coxlab/prednet)\n- github: [https://github.com/e-lab/torch-prednet](https://github.com/e-lab/torch-prednet)\n\n**Diversity encouraged learning of unsupervised LSTM ensemble for neural activity video prediction**\n\n- arxiv: [https://arxiv.org/abs/1611.04899](https://arxiv.org/abs/1611.04899)\n\n**Video Ladder Networks**\n\n- inro: NIPS 2016 workshop on ML for Spatiotemporal Forecasting\n- arxiv: [https://arxiv.org/abs/1612.01756](https://arxiv.org/abs/1612.01756)\n\n**Unsupervised Learning of Long-Term Motion Dynamics for Videos**\n\n- intro: Stanford University\n- arxiv: [https://arxiv.org/abs/1701.01821](https://arxiv.org/abs/1701.01821)\n\n**One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network**\n\n- intro: NCCV 2016\n- arxiv: [https://arxiv.org/abs/1702.04125](https://arxiv.org/abs/1702.04125)\n\n**Fully Context-Aware Video Prediction**\n\n- intro: ETH Zurich & NNAISENSE\n- keywords: unsupervised learning through video prediction, Parallel Multi-Dimensional LSTM\n- project page: [https://sites.google.com/view/contextvp](https://sites.google.com/view/contextvp)\n- arxiv: [https://arxiv.org/abs/1710.08518](https://arxiv.org/abs/1710.08518)\n\n**Novel Video Prediction for Large-scale Scene using Optical Flow**\n\n- intro: University of Victoria & Tongji University & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1805.12243](https://arxiv.org/abs/1805.12243)\n\n# Video Tagging\n\n**Automatic Image and Video Tagging**\n\n![](http://i2.wp.com/scottge.net/wp-content/uploads/2015/06/imagetagging.png?w=576)\n\n- blog: [http://scottge.net/2015/06/30/automatic-image-and-video-tagging/](http://scottge.net/2015/06/30/automatic-image-and-video-tagging/)\n\n**Tagging YouTube music videos with deep learning - Alexandre Passant**\n\n- keywords: Clarifai's deep learning API\n- blog: [http://apassant.net/2015/07/03/tagging-youtube-music-clarifai-deep-learning/](http://apassant.net/2015/07/03/tagging-youtube-music-clarifai-deep-learning/)\n\n# Shot Boundary Detection\n\n**Large-scale, Fast and Accurate Shot Boundary Detection through Spatio-temporal Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1705.03281](https://arxiv.org/abs/1705.03281)\n\n**Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks**\n\n- intro: obtains state-of-the-art results while running at an unprecedented speed of more than 120x real-time.\n- arxiv: [https://arxiv.org/abs/1705.08214](https://arxiv.org/abs/1705.08214)\n\n# Video Action Segmentation\n\n**TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation**\n\n- intro: University of Rochester\n- arxiv: [https://arxiv.org/abs/1705.07818](https://arxiv.org/abs/1705.07818)\n\n# Video2GIF\n\n**Video2GIF: Automatic Generation of Animated GIFs from Video (Robust Deep RankNet)**\n\n- intro: 3D CNN, ranking model, Huber loss, 100K GIFs/video sources dataset\n- arxiv: [http://arxiv.org/abs/1605.04850](http://arxiv.org/abs/1605.04850)\n- github(dataset): [https://github.com/gyglim/video2gif_dataset](https://github.com/gyglim/video2gif_dataset)\n- results: [http://video2gif.info/](http://video2gif.info/)\n- demo site: [http://people.ee.ethz.ch/~gyglim/work_public/autogif/](http://people.ee.ethz.ch/~gyglim/work_public/autogif/)\n- review: [http://motherboard.vice.com/read/these-fire-gifs-were-made-by-artificial-intelligence-yahoo](http://motherboard.vice.com/read/these-fire-gifs-were-made-by-artificial-intelligence-yahoo)\n\n**Creating Animated GIFs Automatically from Video**\n\n[https://yahooresearch.tumblr.com/post/148009705216/creating-animated-gifs-automatically-from-video](https://yahooresearch.tumblr.com/post/148009705216/creating-animated-gifs-automatically-from-video)\n\n**GIF2Video: Color Dequantization and Temporal Interpolation of GIF images**\n\n- intro: Stony Brook University & Megvii Research USA & UCLA\n- arxiv: [https://arxiv.org/abs/1901.02840](https://arxiv.org/abs/1901.02840)\n\n# Video2Speech\n\n**Vid2speech: Speech Reconstruction from Silent Video**\n\n- intro: ICASSP 2017\n- project page: [http://www.vision.huji.ac.il/vid2speech/](http://www.vision.huji.ac.il/vid2speech/)\n- arxiv: [https://arxiv.org/abs/1701.00495](https://arxiv.org/abs/1701.00495)\n- github(official): [https://github.com/arielephrat/vid2speech](https://github.com/arielephrat/vid2speech)\n\n# Video Captioning\n\n[http://handong1587.github.io/deep_learning/2015/10/09/image-video-captioning.html#video-captioning](http://handong1587.github.io/deep_learning/2015/10/09/image-video-captioning.html#video-captioning)\n\n# Video Summarization\n\nVideo summarization produces a short summary of a full-length video and ideally encapsulates its most informative parts, \nalleviates the problem of video browsing, editing and indexing.\n\n**Video Summarization with Long Short-term Memory**\n\n- arxiv: [http://arxiv.org/abs/1605.08110](http://arxiv.org/abs/1605.08110)\n\n**DeepVideo: Video Summarization using Temporal Sequence Modelling**\n\n- intro: CS231n student project report\n- paper: [http://cs231n.stanford.edu/reports2016/216_Report.pdf](http://cs231n.stanford.edu/reports2016/216_Report.pdf)\n\n**Semantic Video Trailers**\n\n- arxiv: [http://arxiv.org/abs/1609.01819](http://arxiv.org/abs/1609.01819)\n\n**Video Summarization using Deep Semantic Features**\n\n- inro: ACCV 2016\n- arxiv: [http://arxiv.org/abs/1609.08758](http://arxiv.org/abs/1609.08758)\n\n**CNN-Based Prediction of Frame-Level Shot Importance for Video Summarization**\n\n- intro: International Conference on new Trends in Computer Sciences (ICTCS), Amman-Jordan, 2017\n- arxiv: [https://arxiv.org/abs/1708.07023](https://arxiv.org/abs/1708.07023)\n\n**Video Summarization with Attention-Based Encoder-Decoder Networks**\n\n[https://arxiv.org/abs/1708.09545](https://arxiv.org/abs/1708.09545)\n\n**Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward**\n\n- intro: AAAI 2018. Chinese Academy of Sciences & Queen Mary University of London\n- project page: [https://kaiyangzhou.github.io/project_vsumm_reinforce/index.html](https://kaiyangzhou.github.io/project_vsumm_reinforce/index.html)\n- arxiv: [https://arxiv.org/abs/1801.00054](https://arxiv.org/abs/1801.00054)\n- github: [https://github.com//KaiyangZhou/vsumm-reinforce](https://github.com//KaiyangZhou/vsumm-reinforce)\n\n**Viewpoint-aware Video Summarization**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.02843](https://arxiv.org/abs/1804.02843)\n\n**DTR-GAN: Dilated Temporal Relational Adversarial Network for Video Summarization**\n\n[https://arxiv.org/abs/1804.11228](https://arxiv.org/abs/1804.11228)\n\n**Learning Video Summarization Using Unpaired Data**\n\n[https://arxiv.org/abs/1805.12174](https://arxiv.org/abs/1805.12174)\n\n**Video Summarization Using Fully Convolutional Sequence Networks**\n\n[https://arxiv.org/abs/1805.10538](https://arxiv.org/abs/1805.10538)\n\n**Video Summarisation by Classification with Deep Reinforcement Learning**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.03089](https://arxiv.org/abs/1807.03089)\n\n**Query-Conditioned Three-Player Adversarial Network for Video Summarization**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.06677](https://arxiv.org/abs/1807.06677)\n\n**Discriminative Feature Learning for Unsupervised Video Summarization**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.09791](https://arxiv.org/abs/1811.09791)\n\n**Rethinking the Evaluation of Video Summaries**\n\n- intro: CVPR 2019 poster\n- arxiv: [https://arxiv.org/abs/1903.11328](https://arxiv.org/abs/1903.11328)\n\n# Video Highlight Detection\n\n**Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders**\n\n- intro: ICCV 2015\n- intro: rely on an assumption that highlights of an event category are more frequently captured in short videos than non-highlights\n- arxiv: [http://arxiv.org/abs/1510.01442](http://arxiv.org/abs/1510.01442)\n\n**Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization**\n\n- keywords: wearable device\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf)\n- paper: [http://research.microsoft.com/apps/pubs/default.aspx?id=264919](http://research.microsoft.com/apps/pubs/default.aspx?id=264919)\n\n**Using Deep Learning to Find Basketball Highlights**\n\n![](https://cloud.githubusercontent.com/assets/10147637/7966603/228179fe-09f3-11e5-9ea7-31e76c8248fe.png)\n\n- blog: [http://public.hudl.com/bits/archives/2015/06/05/highlights/?utm_source=tuicool&utm_medium=referral](http://public.hudl.com/bits/archives/2015/06/05/highlights/?utm_source=tuicool&utm_medium=referral)\n\n**Real-Time Video Highlights for Yahoo Esports**\n\n- arxiv: [https://arxiv.org/abs/1611.08780](https://arxiv.org/abs/1611.08780)\n\n**A Deep Ranking Model for Spatio-Temporal Highlight Detection from a 360 Video**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1801.10312](https://arxiv.org/abs/1801.10312)\n\n**PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation**\n\n- intro: Nanyang Technological University & Google Research, Zurich\n- keywords: personalized highlight detection (PHD)\n- arxiv: [https://arxiv.org/abs/1804.06604](https://arxiv.org/abs/1804.06604)\n\n# Video Understanding\n\n**Scale Up Video Understandingwith Deep Learning**\n\n- intro: 2016, Tsinghua University\n- slides: [iiis.tsinghua.edu.cn/~jianli/courses/ATCS2016spring/talk_chuang.pptx](iiis.tsinghua.edu.cn/~jianli/courses/ATCS2016spring/talk_chuang.pptx)\n\n**Slicing Convolutional Neural Network for Crowd Video Understanding**\n\n![](http://www.ee.cuhk.edu.hk/~jshao/SCNN_files/fig_network5.jpg)\n\n- intro: CVPR 2016\n- intro: It aims at learning generic spatio-temporal features from crowd videos, especially for long-term temporal learning\n- project page: [http://www.ee.cuhk.edu.hk/~jshao/SCNN.html](http://www.ee.cuhk.edu.hk/~jshao/SCNN.html)\n- paper: [http://www.ee.cuhk.edu.hk/~jshao/papers_jshao/jshao_cvpr16_scnn.pdf](http://www.ee.cuhk.edu.hk/~jshao/papers_jshao/jshao_cvpr16_scnn.pdf)\n- github: [https://github.com/amandajshao/Slicing-CNN](https://github.com/amandajshao/Slicing-CNN)\n\n**Rethinking Spatiotemporal Feature Learning For Video Understanding**\n\n[https://arxiv.org/abs/1712.04851](https://arxiv.org/abs/1712.04851)\n\n**Hierarchical Video Understanding**\n\n[https://arxiv.org/abs/1809.03316](https://arxiv.org/abs/1809.03316)\n\n# Challenges\n\n**THUMOS Challenge 2014**\n\n- homepage: [http://crcv.ucf.edu/THUMOS14/home.html](http://crcv.ucf.edu/THUMOS14/home.html)\n- download: [http://crcv.ucf.edu/THUMOS14/download.html](http://crcv.ucf.edu/THUMOS14/download.html)\n\n**THUMOS Challenge 2015**\n\n- homepage: [http://www.thumos.info/](http://www.thumos.info/)\n- download: [http://www.thumos.info/download.html](http://www.thumos.info/download.html)\n\n**ActivityNet Challenge 2016**\n\n![](http://activity-net.org/challenges/2016/images/anet_cover.png)\n\n- homepage: [http://activity-net.org/challenges/2016/](http://activity-net.org/challenges/2016/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","title":"Visualizing and Interpreting Convolutional Neural Network"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Visualizing and Interpreting Convolutional Neural Network\r\ndate: 2015-10-09\r\n---\r\n\r\n# Papers\r\n\r\n**Deconvolutional Networks**\r\n\r\n- paper: [http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\r\n- video: [https://ipam.wistia.com/medias/zd0qnekkwc](https://ipam.wistia.com/medias/zd0qnekkwc)\r\n- presentation: [https://mathinstitutes.org/videos/videos/3295](https://mathinstitutes.org/videos/videos/3295)\r\n\r\n**Visualizing and Understanding Convolutional Network**\r\n\r\n- intro: ECCV 2014\r\n- arxiv: [http://arxiv.org/abs/1311.2901](http://arxiv.org/abs/1311.2901)\r\n- slides: [https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_dec5_DeepVis.pdf](https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_dec5_DeepVis.pdf)\r\n- slides: [http://videolectures.net/site/normal_dl/tag=921098/eccv2014_zeiler_convolutional_networks_01.pdf](http://videolectures.net/site/normal_dl/tag=921098/eccv2014_zeiler_convolutional_networks_01.pdf)\r\n- video: [http://videolectures.net/eccv2014_zeiler_convolutional_networks/](http://videolectures.net/eccv2014_zeiler_convolutional_networks/)\r\n- chs: [http://blog.csdn.net/kklots/article/details/17136059](http://blog.csdn.net/kklots/article/details/17136059)\r\n- github: [https://github.com/piergiaj/caffe-deconvnet](https://github.com/piergiaj/caffe-deconvnet)\r\n\r\n**Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps**\r\n\r\n- intro: ICLR 2014 workshop\r\n- arxiv: [http://arxiv.org/abs/1312.6034](http://arxiv.org/abs/1312.6034)\r\n- github: [https://github.com/yasunorikudo/vis-cnn](https://github.com/yasunorikudo/vis-cnn)\r\n\r\n**Understanding Deep Image Representations by Inverting Them**\r\n\r\n- arxiv: [http://arxiv.org/abs/1412.0035](http://arxiv.org/abs/1412.0035)\r\n- github: [https://github.com/aravindhm/deep-goggle](https://github.com/aravindhm/deep-goggle)\r\n\r\n**deepViz: Visualizing Convolutional Neural Networks for Image Classification**\r\n\r\n- paper: [http://vis.berkeley.edu/courses/cs294-10-fa13/wiki/images/f/fd/DeepVizPaper.pdf](http://vis.berkeley.edu/courses/cs294-10-fa13/wiki/images/f/fd/DeepVizPaper.pdf)\r\n- github: [https://github.com/bruckner/deepViz](https://github.com/bruckner/deepViz)\r\n\r\n**Inverting Convolutional Networks with Convolutional Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.02753](http://arxiv.org/abs/1506.02753)\r\n\r\n**Understanding Neural Networks Through Deep Visualization**\r\n\r\n- project page: [http://yosinski.com/deepvis](http://yosinski.com/deepvis)\r\n- arxiv: [http://arxiv.org/abs/1506.06579](http://arxiv.org/abs/1506.06579)\r\n- github: [https://github.com/yosinski/deep-visualization-toolbox](https://github.com/yosinski/deep-visualization-toolbox)\r\n\r\n**Visualizing Higher-Layer Features of a Deep Network**\r\n\r\n- paper: [http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/247](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/247)\r\n\r\n**Generative Modeling of Convolutional Neural Networks**\r\n\r\n- project page: [http://www.stat.ucla.edu/~yang.lu/Project/generativeCNN/main.html](http://www.stat.ucla.edu/~yang.lu/Project/generativeCNN/main.html)\r\n- arxiv: [http://arxiv.org/abs/1412.6296](http://arxiv.org/abs/1412.6296)\r\n- code: [http://www.stat.ucla.edu/~yang.lu/Project/generativeCNN/doc/caffe-generative.zip](http://www.stat.ucla.edu/~yang.lu/Project/generativeCNN/doc/caffe-generative.zip)\r\n\r\n**Understanding Intra-Class Knowledge Inside CNN**\r\n\r\n- arxiv: [http://arxiv.org/abs/1507.02379](http://arxiv.org/abs/1507.02379)\r\n\r\n**Learning FRAME Models Using CNN Filters for Knowledge Visualization**\r\n\r\n- project page: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html)\r\n- arxiv: [http://arxiv.org/abs/1509.08379](http://arxiv.org/abs/1509.08379)\r\n- code: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/code.zip](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/code.zip)\r\n\r\n**Convergent Learning: Do different neural networks learn the same representations?**\r\n\r\n- intro: ICLR 2016\r\n- arxiv: [http://arxiv.org/abs/1511.07543](http://arxiv.org/abs/1511.07543)\r\n- github: [https://github.com/yixuanli/convergent_learning](https://github.com/yixuanli/convergent_learning)\r\n- video: [http://videolectures.net/iclr2016_yosinski_convergent_learning/](http://videolectures.net/iclr2016_yosinski_convergent_learning/)\r\n\r\n**Visualizing and Understanding Deep Texture Representations**\r\n\r\n- homepage: [http://vis-www.cs.umass.edu/texture/](http://vis-www.cs.umass.edu/texture/)\r\n- arxiv: [http://arxiv.org/abs/1511.05197](http://arxiv.org/abs/1511.05197)\r\n- paper: [https://people.cs.umass.edu/~smaji/papers/texture-cvpr16.pdf](https://people.cs.umass.edu/~smaji/papers/texture-cvpr16.pdf)\r\n\r\n**Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.02017](http://arxiv.org/abs/1512.02017)\r\n\r\n**An Interactive Node-Link Visualization of Convolutional Neural Networks**\r\n\r\n- homepage: [http://scs.ryerson.ca/~aharley/vis/](http://scs.ryerson.ca/~aharley/vis/)\r\n- code: [http://scs.ryerson.ca/~aharley/vis/source.zip](http://scs.ryerson.ca/~aharley/vis/source.zip)\r\n- demo: [http://scs.ryerson.ca/~aharley/vis/conv/](http://scs.ryerson.ca/~aharley/vis/conv/)\r\n- review: [http://www.popsci.com/gaze-inside-mind-artificial-intelligence](http://www.popsci.com/gaze-inside-mind-artificial-intelligence)\r\n\r\n**Learning Deep Features for Discriminative Localization**\r\n\r\n![](https://camo.githubusercontent.com/fb9a2d0813e5d530f49fa074c378cf83959346f7/687474703a2f2f636e6e6c6f63616c697a6174696f6e2e637361696c2e6d69742e6564752f6672616d65776f726b2e6a7067)\r\n\r\n- project page: [http://cnnlocalization.csail.mit.edu/](http://cnnlocalization.csail.mit.edu/)\r\n- arxiv: [http://arxiv.org/abs/1512.04150](http://arxiv.org/abs/1512.04150)\r\n- github: [https://github.com/metalbubble/CAM](https://github.com/metalbubble/CAM)\r\n- blog: [http://jacobcv.blogspot.com/2016/08/class-activation-maps-in-keras.html](http://jacobcv.blogspot.com/2016/08/class-activation-maps-in-keras.html)\r\n- github: [https://github.com/jacobgil/keras-cam](https://github.com/jacobgil/keras-cam)\r\n\r\n**Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks**\r\n\r\n![](http://www.evolvingai.org/files/styles/content_width/public/fc8_layer_full.jpg?itok=q_X70Oj-)\r\n\r\n- intro: Visualization for Deep Learning workshop. ICML 2016\r\n- arxiv: [http://arxiv.org/abs/1602.03616](http://arxiv.org/abs/1602.03616)\r\n- homepage: [http://www.evolvingai.org/nguyen-yosinski-clune-2016-multifaceted-feature](http://www.evolvingai.org/nguyen-yosinski-clune-2016-multifaceted-feature)\r\n- github: [https://github.com/Evolving-AI-Lab/mfv](https://github.com/Evolving-AI-Lab/mfv)\r\n\r\n**A New Method to Visualize Deep Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.02518](http://arxiv.org/abs/1603.02518)\r\n\r\n**A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.07757](http://arxiv.org/abs/1606.07757)\r\n\r\n**VisualBackProp: visualizing CNNs for autonomous driving**\r\n\r\n**VisualBackProp: efficient visualization of CNNs**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.05418](https://arxiv.org/abs/1611.05418)\r\n- github: [https://github.com/mbojarski/VisualBackProp](https://github.com/mbojarski/VisualBackProp)\r\n\r\n**Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization**\r\n\r\n**Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization**\r\n\r\n![](https://camo.githubusercontent.com/450498bd998fd99d51b647d2b6c8631e94585522/687474703a2f2f692e696d6775722e636f6d2f4a614762645a352e706e67)\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)\r\n- github: [https://github.com/ramprs/grad-cam/](https://github.com/ramprs/grad-cam/)\r\n- github(Keras): [https://github.com/jacobgil/keras-grad-cam](https://github.com/jacobgil/keras-grad-cam)\r\n- github(TensorFlow): [https://github.com/Ankush96/grad-cam.tensorflow](https://github.com/Ankush96/grad-cam.tensorflow)\r\n\r\n**Grad-CAM: Why did you say that?**\r\n\r\n- intro: NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems\r\n- intro: extended abstract version of [arXiv:1610.02391](https://arxiv.org/abs/1610.02391)\r\n- arxiv: [https://arxiv.org/abs/1611.07450](https://arxiv.org/abs/1611.07450)\r\n\r\n**Visualizing Residual Networks**\r\n\r\n- intro: UC Berkeley CS 280 final project report\r\n- arxiv: [https://arxiv.org/abs/1701.02362](https://arxiv.org/abs/1701.02362)\r\n\r\n**Visualizing Deep Neural Network Decisions: Prediction Difference Analysis**\r\n\r\n- intro: University of Amsterdam & Canadian Institute of Advanced Research & Vrije Universiteit Brussel\r\n- intro: ICLR 2017\r\n- arxiv: [https://arxiv.org/abs/1702.04595](https://arxiv.org/abs/1702.04595)\r\n- github: [https://github.com/lmzintgraf/DeepVis-PredDiff](https://github.com/lmzintgraf/DeepVis-PredDiff)\r\n\r\n**ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models**\r\n\r\n- intro: Georgia Tech & Facebook\r\n- arxiv: [https://arxiv.org/abs/1704.01942](https://arxiv.org/abs/1704.01942)\r\n\r\n**Picasso: A Neural Network Visualizer**\r\n\r\n- arxiv: [https://arxiv.org/abs/1705.05627](https://arxiv.org/abs/1705.05627)\r\n- github: [https://github.com/merantix/picasso](https://github.com/merantix/picasso)\r\n- blog: [https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5](https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5)\r\n\r\n**CNN Fixations: An unraveling approach to visualize the discriminative image regions**\r\n\r\n- arxiv: [https://arxiv.org/abs/1708.06670](https://arxiv.org/abs/1708.06670)\r\n- github: [https://github.com/utsavgarg/cnn-fixations](https://github.com/utsavgarg/cnn-fixations)\r\n\r\n**A Forward-Backward Approach for Visualizing Information Flow in Deep Networks**\r\n\r\n- intro: NIPS 2017 Symposium on Interpretable Machine Learning. Iowa State University\r\n- arxiv: [https://arxiv.org/abs/1711.06221](https://arxiv.org/abs/1711.06221)\r\n\r\n**Using KL-divergence to focus Deep Visual Explanation**\r\n\r\n[https://arxiv.org/abs/1711.06431](https://arxiv.org/abs/1711.06431)\r\n\r\n**An Introduction to Deep Visual Explanation**\r\n\r\n- intro: NIPS 2017 - Workshop Interpreting, Explaining and Visualizing Deep Learning\r\n- arxiv: [https://arxiv.org/abs/1711.09482](https://arxiv.org/abs/1711.09482)\r\n\r\n**Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks**\r\n\r\n[https://arxiv.org/abs/1712.06302](https://arxiv.org/abs/1712.06302)\r\n\r\n**Visualizing the Loss Landscape of Neural Nets**\r\n\r\n- intro: University of Maryland & United States Naval Academy\r\n- arxiv: [https://arxiv.org/abs/1712.09913](https://arxiv.org/abs/1712.09913)\r\n\r\n**Visualizing Deep Similarity Networks**\r\n\r\n[https://arxiv.org/abs/1901.00536](https://arxiv.org/abs/1901.00536)\r\n\r\n## Interpreting Convolutional Neural Networks\r\n\r\n**Network Dissection: Quantifying Interpretability of Deep Visual Representations**\r\n\r\n![](http://netdissect.csail.mit.edu/image/dissect-arch.png)\r\n\r\n- intro: CVPR 2017 oral. MIT\r\n- project page: [http://netdissect.csail.mit.edu/](http://netdissect.csail.mit.edu/)\r\n- arxiv: [https://arxiv.org/abs/1704.05796](https://arxiv.org/abs/1704.05796)\r\n- github: [https://github.com/CSAILVision/NetDissect](https://github.com/CSAILVision/NetDissect)\r\n\r\n**Interpreting Deep Visual Representations via Network Dissection**\r\n\r\n[https://arxiv.org/abs/1711.05611](https://arxiv.org/abs/1711.05611)\r\n\r\n**Methods for Interpreting and Understanding Deep Neural Networks**\r\n\r\n- intro: Technische Universit¨at Berlin & Fraunhofer Heinrich Hertz Institute\r\n- arxiv: [https://arxiv.org/abs/1706.07979](https://arxiv.org/abs/1706.07979)\r\n\r\n**SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability**\r\n\r\n- intro: NIPS 2017. Google Brain & Uber AI Labs\r\n- arxiv: [https://arxiv.org/abs/1706.05806](https://arxiv.org/abs/1706.05806)\r\n- github: [https://github.com/google/svcca/](https://github.com/google/svcca/)\r\n- blog: [https://research.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html](https://research.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html)\r\n\r\n**Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples**\r\n\r\n- intro: Tsinghua University\r\n- arxiv: [https://arxiv.org/abs/1708.05493](https://arxiv.org/abs/1708.05493)\r\n\r\n**Interpretable Convolutional Neural Networks**\r\n\r\n[https://arxiv.org/abs/1710.00935](https://arxiv.org/abs/1710.00935)\r\n\r\n**Interpreting Convolutional Neural Networks Through Compression**\r\n\r\n- intro: NIPS 2017 Symposium on Interpretable Machine Learning\r\n- arxiv: [https://arxiv.org/abs/1711.02329](https://arxiv.org/abs/1711.02329)\r\n\r\n**Interpreting Deep Neural Networks**\r\n\r\n- blog: [http://www.shallowmind.co/jekyll/pixyll/2017/12/30/tree-regularization/](http://www.shallowmind.co/jekyll/pixyll/2017/12/30/tree-regularization/)\r\n\r\n**Interpreting CNNs via Decision Trees**\r\n\r\n[https://arxiv.org/abs/1802.00121](https://arxiv.org/abs/1802.00121)\r\n\r\n**Visual Interpretability for Deep Learning: a Survey**\r\n\r\n[https://arxiv.org/abs/1802.00614](https://arxiv.org/abs/1802.00614)\r\n\r\n**Interpreting Deep Classifier by Visual Distillation of Dark Knowledge**\r\n\r\n- intro: University of Edinburgh & Huawei Research America\r\n- arxiv: [https://arxiv.org/abs/1803.04042](https://arxiv.org/abs/1803.04042)\r\n\r\n**How convolutional neural network see the world - A survey of convolutional neural network visualization methods**\r\n\r\n- intro: Mathematical Foundations of Computing. George Mason University & Clarkson University\r\n- arxiv: [https://arxiv.org/abs/1804.11191](https://arxiv.org/abs/1804.11191)\r\n\r\n**Understanding Regularization to Visualize Convolutional Neural Networks**\r\n\r\n- intro: Konica Minolta Laboratory Europe & Technical University of Munich\r\n- arxiv: [https://arxiv.org/abs/1805.00071](https://arxiv.org/abs/1805.00071)\r\n\r\n**Deeper Interpretability of Deep Networks**\r\n\r\n- intro: University of Glasgow & University of Oxford & University of California\r\n- arxiv: [https://arxiv.org/abs/1811.07807](https://arxiv.org/abs/1811.07807)\r\n\r\n**Interpretable CNNs**\r\n\r\n[https://arxiv.org/abs/1901.02413](https://arxiv.org/abs/1901.02413)\r\n\r\n**Explaining AlphaGo: Interpreting Contextual Effects in Neural Networks**\r\n\r\n[https://arxiv.org/abs/1901.02184](https://arxiv.org/abs/1901.02184)\r\n\r\n**Interpretable BoW Networks for Adversarial Example Detection**\r\n\r\n[https://arxiv.org/abs/1901.02229](https://arxiv.org/abs/1901.02229)\r\n\r\n**Deep Features Analysis with Attention Networks**\r\n\r\n- intro: In AAAI-19 Workshop on Network Interpretability for Deep Learning\r\n- arxiv: [https://arxiv.org/abs/1901.10042](https://arxiv.org/abs/1901.10042)\r\n\r\n**Understanding Neural Networks via Feature Visualization: A survey**\r\n\r\n- intro: A book chapter in an Interpretable ML book ([http://www.interpretable-ml.org/book/](http://www.interpretable-ml.org/book/))\r\n- arxiv: [https://arxiv.org/abs/1904.08939](https://arxiv.org/abs/1904.08939)\r\n\r\n**Explaining Neural Networks via Perturbing Important Learned Features**\r\n\r\n[https://arxiv.org/abs/1911.11081](https://arxiv.org/abs/1911.11081)\r\n\r\n**Interpreting Adversarially Trained Convolutional Neural Networks**\r\n\r\n- intro: ICML 2019\r\n- arxiv: [https://arxiv.org/abs/1905.09797](https://arxiv.org/abs/1905.09797)\r\n- github: [https://github.com/PKUAI26/AT-CNN](https://github.com/PKUAI26/AT-CNN)\r\n\r\n# Projects\r\n\r\n**Interactive Deep Neural Net Hallucinations**\r\n\r\n- project page: [http://317070.github.io/Dream/](http://317070.github.io/Dream/)\r\n- github: [https://github.com/317070/Twitch-plays-LSD-neural-net](https://github.com/317070/Twitch-plays-LSD-neural-net)\r\n\r\n**torch-visbox**\r\n\r\n![](/assets/vis-cnn/torch-visbox-example.png)\r\n\r\n- github: [https://github.com/Aysegul/torch-visbox](https://github.com/Aysegul/torch-visbox)\r\n\r\n**draw_convnet: Python script for illustrating Convolutional Neural Network (ConvNet)**\r\n\r\n![](https://raw.githubusercontent.com/gwding/draw_convnet/master/convnet_fig.png)\r\n\r\n- github: [https://github.com/gwding/draw_convnet](https://github.com/gwding/draw_convnet)\r\n\r\n**Caffe prototxt visualization**\r\n\r\n![](/assets/vis-cnn/netscope.jpg)\r\n\r\n- intro: Recommended by Kaiming He\r\n- github: [https://github.com/ethereon/netscope](https://github.com/ethereon/netscope)\r\n- quickstart: [http://ethereon.github.io/netscope/quickstart.html](http://ethereon.github.io/netscope/quickstart.html)\r\n- demo: [http://ethereon.github.io/netscope/#/editor](http://ethereon.github.io/netscope/#/editor)\r\n\r\n**Keras Visualization Toolkit**\r\n\r\n- github: [https://github.com/raghakot/keras-vis](https://github.com/raghakot/keras-vis)\r\n- docs: [https://raghakot.github.io/keras-vis/](https://raghakot.github.io/keras-vis/)\r\n\r\n**mNeuron: A Matlab Plugin to Visualize Neurons from Deep Models**\r\n\r\n- project page: [http://vision03.csail.mit.edu/cnn_art/](http://vision03.csail.mit.edu/cnn_art/)\r\n- github: [https://github.com/donglaiw/mNeuron](https://github.com/donglaiw/mNeuron)\r\n\r\n**cnnvis-pytorch**\r\n\r\n- intro: visualization of CNN in PyTorch\r\n- github: [https://github.com/leelabcnbc/cnnvis-pytorch](https://github.com/leelabcnbc/cnnvis-pytorch)\r\n\r\n**VisualDL**\r\n\r\n- intro: A platform to visualize the deep learning process\r\n- homepage: [http://visualdl.paddlepaddle.org/](http://visualdl.paddlepaddle.org/)\r\n- github: [https://github.com/PaddlePaddle/VisualDL](https://github.com/PaddlePaddle/VisualDL)\r\n\r\n# Blogs\r\n\r\n\"Visualizing GoogLeNet Classes\"\r\n\r\n[http://auduno.com/post/125362849838/visualizing-googlenet-classes](http://auduno.com/post/125362849838/visualizing-googlenet-classes)\r\n\r\n**Visualizing CNN architectures side by side with mxnet**\r\n\r\n![](http://josephpcohen.com/w/wp-content/uploads/Screen-Shot-2016-01-14-at-11.25.15-AM-638x300.png)\r\n\r\n- blog: [http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/](http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/)\r\n\r\n**How convolutional neural networks see the world: An exploration of convnet filters with Keras**\r\n\r\n- blog: [http://blog.keras.io/how-convolutional-neural-networks-see-the-world.html](http://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\r\n- github: [https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py](https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py)\r\n\r\n**Visualizing Deep Learning with t-SNE (Tutorial and Video)**\r\n\r\n![](https://cdn-images-1.medium.com/max/800/1*LPWa2qRl7SwvNNrRgVEdoQ.png)\r\n\r\n- blog: [https://medium.com/@awjuliani/visualizing-deep-learning-with-t-sne-tutorial-and-video-e7c59ee4080c#.ubhijafw7](https://medium.com/@awjuliani/visualizing-deep-learning-with-t-sne-tutorial-and-video-e7c59ee4080c#.ubhijafw7)\r\n- github: [https://github.com/awjuliani/3D-TSNE](https://github.com/awjuliani/3D-TSNE)\r\n\r\n**Peeking inside Convnets**\r\n\r\n- blog: [https://auduno.github.io/2016/06/18/peeking-inside-convnets/](https://auduno.github.io/2016/06/18/peeking-inside-convnets/)\r\n\r\n**Visualizing Features from a Convolutional Neural Network**\r\n\r\n- blog: [http://kvfrans.com/visualizing-features-from-a-convolutional-neural-network/](http://kvfrans.com/visualizing-features-from-a-convolutional-neural-network/)\r\n- github: [https://github.com/kvfrans/feature-visualization](https://github.com/kvfrans/feature-visualization)\r\n\r\n**Visualizing Deep Neural Networks Classes and Features**\r\n\r\n[http://ankivil.com/visualizing-deep-neural-networks-classes-and-features/](http://ankivil.com/visualizing-deep-neural-networks-classes-and-features/)\r\n\r\n**Visualizing parts of Convolutional Neural Networks using Keras and Cats**\r\n\r\n- blog: [https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59#.bt6bb13dk](https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59#.bt6bb13dk)\r\n- github: [https://github.com/erikreppel/visualizing_cnns](https://github.com/erikreppel/visualizing_cnns)\r\n\r\n**Visualizing convolutional neural networks**\r\n\r\n- intro: How to build convolutional neural networks from scratch w/ Tensorflow\r\n- blog: [https://www.oreilly.com/ideas/visualizing-convolutional-neural-networks](https://www.oreilly.com/ideas/visualizing-convolutional-neural-networks)\r\n- github: [https://github.com//wagonhelm/Visualizing-Convnets/](https://github.com//wagonhelm/Visualizing-Convnets/)\r\n\r\n# Tools\r\n\r\n**Topological Visualisation of a Convolutional Neural Network**\r\n\r\n![](/assets/vis-cnn/topological_vis_cnn.jpg)\r\n\r\n[http://terencebroad.com/convnetvis/vis.html](http://terencebroad.com/convnetvis/vis.html)\r\n\r\n**Visualization of Places-CNN and ImageNet CNN**\r\n\r\n- homepage: [http://places.csail.mit.edu/visualizationCNN.html](http://places.csail.mit.edu/visualizationCNN.html)\r\n- DrawCNN: [http://people.csail.mit.edu/torralba/research/drawCNN/drawNet.html](http://people.csail.mit.edu/torralba/research/drawCNN/drawNet.html)\r\n\r\n**Visualization of a feed forward Neural Network using MNIST dataset**\r\n\r\n- homepage: [http://nn-mnist.sennabaum.com/](http://nn-mnist.sennabaum.com/)\r\n- github: [https://github.com/csenn/nn-visualizer](https://github.com/csenn/nn-visualizer)\r\n\r\n**CNNVis: Towards Better Analysis of Deep Convolutional Neural Networks.**\r\n\r\n[http://shixialiu.com/publications/cnnvis/demo/](http://shixialiu.com/publications/cnnvis/demo/)\r\n\r\n**Quiver: Interactive convnet features visualization for Keras**\r\n\r\n- homepage: [https://jakebian.github.io/quiver/](https://jakebian.github.io/quiver/)\r\n- github: [https://github.com/jakebian/quiver](https://github.com/jakebian/quiver)\r\n\r\n**Netron**\r\n\r\n- intro: Visualizer for deep learning and machine learning models\r\n- github: [https://github.com/lutzroeder/netron](https://github.com/lutzroeder/netron)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","title":"Visual Question Answering"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Visual Question Answering\ndate: 2015-10-09\n---\n\n**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks**\n\n- intro: Facebook AI Research\n- arxiv: [http://arxiv.org/abs/1502.05698v1](http://arxiv.org/abs/1502.05698v1)\n- github: [https://github.com/facebook/bAbI-tasks](https://github.com/facebook/bAbI-tasks)\n\n**VQA: Visual Question Answering**\n\n- intro: ICCV 2015\n- arxiv: [http://arxiv.org/abs/1505.00468](http://arxiv.org/abs/1505.00468)\n- homepage: [http://visualqa.org/](http://visualqa.org/)\n\n**Ask Your Neurons: A Neural-based Approach to Answering Questions about Images**\n\n- intro: ICCV 2015\n- arxiv: [http://arxiv.org/abs/1505.01121](http://arxiv.org/abs/1505.01121)\n- project: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)\n- video: [https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1](https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1)\n\n**Exploring Models and Data for Image Question Answering**\n\n![](https://camo.githubusercontent.com/ac498616bb6ea1db7aaabb1cf567d07e4bbef395/687474703a2f2f692e696d6775722e636f6d2f4a7669787832572e6a7067)\n\n- arxiv: [http://arxiv.org/abs/1505.02074](http://arxiv.org/abs/1505.02074)\n- gtihub(Tensorflow): [https://github.com/paarthneekhara/neural-vqa-tensorflow](https://github.com/paarthneekhara/neural-vqa-tensorflow)\n- github(Python+Keras): [https://github.com/ayushoriginal/NeuralNetwork-ImageQA](https://github.com/ayushoriginal/NeuralNetwork-ImageQA)\n\n**Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1505.05612](http://arxiv.org/abs/1505.05612)\n\n**Teaching Machines to Read and Comprehend**\n\n- intro: Google DeepMind\n- arxiv: [http://arxiv.org/abs/1506.03340](http://arxiv.org/abs/1506.03340)\n- github: [https://github.com/deepmind/rc-data](https://github.com/deepmind/rc-data)\n- github(Theano/Blocks): [https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend](https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend)\n- github(Tensorflow): [https://github.com/carpedm20/attentive-reader-tensorflow](https://github.com/carpedm20/attentive-reader-tensorflow)\n\n**Neural Module Networks**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1511.02799](http://arxiv.org/abs/1511.02799)\n- github: [https://github.com/jacobandreas/nmn2](https://github.com/jacobandreas/nmn2)\n\n**Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction**\n\n![](http://cvlab.postech.ac.kr/research/dppnet/images/figure2.png)\n\n- arxiv: [http://arxiv.org/abs/1511.05756](http://arxiv.org/abs/1511.05756)\n- github: [https://github.com/HyeonwooNoh/DPPnet](https://github.com/HyeonwooNoh/DPPnet)\n- project page: [http://cvlab.postech.ac.kr/research/dppnet/](http://cvlab.postech.ac.kr/research/dppnet/)\n\n**Neural Generative Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1512.01337](http://arxiv.org/abs/1512.01337)\n\n**Stacked Attention Networks for Image Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1511.02274](http://arxiv.org/abs/1511.02274)\n- github: [https://github.com/abhshkdz/neural-vqa-attention](https://github.com/abhshkdz/neural-vqa-attention)\n\n**Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1511.05234](http://arxiv.org/abs/1511.05234)\n\n**Simple Baseline for Visual Question Answering**\n\n- intro: Facebook AI Research. Bag-of-word\n- arxiv: [http://arxiv.org/abs/1512.02167](http://arxiv.org/abs/1512.02167)\n- github: [https://github.com/metalbubble/VQAbaseline](https://github.com/metalbubble/VQAbaseline)\n- demo: [http://visualqa.csail.mit.edu/](http://visualqa.csail.mit.edu/)\n\n**MovieQA: Understanding Stories in Movies through Question-Answering**\n\n- intro: CVPR 2016\n- project page: [http://movieqa.cs.toronto.edu/home/](http://movieqa.cs.toronto.edu/home/)\n- arxiv: [http://arxiv.org/abs/1512.02902](http://arxiv.org/abs/1512.02902)\n- gtihub: [https://github.com/makarandtapaswi/MovieQA_CVPR2016/](https://github.com/makarandtapaswi/MovieQA_CVPR2016/)\n\n**Deeper LSTM+ normalized CNN for Visual Question Answering**\n\n- intro: \"This current code can get 58.16 on Open-Ended and 63.09 on Multiple-Choice on test-standard split\"\n- github: [https://github.com/VT-vision-lab/VQA_LSTM_CNN](https://github.com/VT-vision-lab/VQA_LSTM_CNN)\n\n**A Neural Network for Factoid Question Answering over Paragraphs**\n\n- project page: [http://cs.umd.edu/~miyyer/qblearn/](http://cs.umd.edu/~miyyer/qblearn/)\n- paper: [https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf](https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf)\n- code+data: [https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz](https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz)\n\n**Learning to Compose Neural Networks for Question Answering**\n\n- intro: NAACL 2016 Best paper\n- arxiv: [http://arxiv.org/abs/1601.01705](http://arxiv.org/abs/1601.01705)\n\n**Generating Natural Questions About an Image**\n\n- arxiv: [http://arxiv.org/abs/1603.06059](http://arxiv.org/abs/1603.06059)\n\n**Question Answering on Freebase via Relation Extraction and Textual Evidence**\n\n- intro: ACL 2016\n- arxiv: [https://arxiv.org/abs/1603.00957](https://arxiv.org/abs/1603.00957)\n- github: [https://github.com/syxu828/QuestionAnsweringOverFB](https://github.com/syxu828/QuestionAnsweringOverFB)\n\n**Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus**\n\n- arxiv: [http://arxiv.org/abs/1603.06807](http://arxiv.org/abs/1603.06807)\n\n**Character-Level Question Answering with Attention**\n\n- arxiv: [http://arxiv.org/abs/1604.00727](http://arxiv.org/abs/1604.00727)\n- comment(by @Wenpeng_Yin): \"fancy model with minor improvement\"\n\n**A Focused Dynamic Attention Model for Visual Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1604.01485](http://arxiv.org/abs/1604.01485)\n\n**Visual Question Answering Literature Survey**\n\n- blog: [http://iamaaditya.github.io/research/literature/](http://iamaaditya.github.io/research/literature/)\n\n**The DIY Guide to Visual Question Answering**\n\n![](https://camo.githubusercontent.com/53c28e13bd645acbf49c9e71e82a36202d1981bc/687474703a2f2f7333322e706f7374696d672e6f72672f77636a6c7a7a7532742f53637265656e5f53686f745f323031365f30355f30385f61745f325f34325f30375f504d2e706e67)\n\n- github: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md)\n\n**Question Answering via Integer Programming over Semi-Structured Knowledge**\n\n- arxiv: [http://arxiv.org/abs/1604.06076](http://arxiv.org/abs/1604.06076)\n- github: [https://github.com/allenai/tableilp](https://github.com/allenai/tableilp)\n- youtube: [https://www.youtube.com/watch?v=7NS53icQRrs](https://www.youtube.com/watch?v=7NS53icQRrs)\n\n**Hierarchical Question-Image Co-Attention for Visual Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1606.00061](http://arxiv.org/abs/1606.00061)\n- github: [https://github.com/jiasenlu/HieCoAttenVQA](https://github.com/jiasenlu/HieCoAttenVQA)\n\n**Multimodal Residual Learning for Visual QA**\n\n- arxiv: [http://arxiv.org/abs/1606.01455](http://arxiv.org/abs/1606.01455)\n\n**Simple Question Answering by Attentive Convolutional Neural Network**\n\n- arxiv: [http://arxiv.org/abs/1606.03391](http://arxiv.org/abs/1606.03391)\n\n**Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?**\n\n![](https://computing.ece.vt.edu/~abhshkdz/vqa-hat/img/att_comparison_2row.jpg)\n\n- homepage: [https://computing.ece.vt.edu/~abhshkdz/vqa-hat/](https://computing.ece.vt.edu/~abhshkdz/vqa-hat/)\n- arxiv: [http://arxiv.org/abs/1606.03556](http://arxiv.org/abs/1606.03556)\n\n**Simple and Effective Question Answering with Recurrent Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1606.05029](http://arxiv.org/abs/1606.05029)\n\n**Analyzing the Behavior of Visual Question Answering Models**\n\n- arxiv: [http://arxiv.org/abs/1606.07356](http://arxiv.org/abs/1606.07356)\n\n**Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding**\n\n- arxiv: [https://arxiv.org/abs/1606.01847](https://arxiv.org/abs/1606.01847)\n- github: [https://github.com/akirafukui/vqa-mcb](https://github.com/akirafukui/vqa-mcb)\n\n**Deep Language Modeling for Question Answering using Keras**\n\n- blog: [http://benjaminbolte.com/blog/2016/keras-language-modeling.html](http://benjaminbolte.com/blog/2016/keras-language-modeling.html)\n- github: [https://github.com/codekansas/keras-language-modeling](https://github.com/codekansas/keras-language-modeling)\n\n**Interpreting Visual Question Answering Models**\n\n- arxiv: [http://arxiv.org/abs/1608.08974](http://arxiv.org/abs/1608.08974)\n\n**The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering**\n\n- intro: FSVQA\n- arxiv: [http://arxiv.org/abs/1609.06657](http://arxiv.org/abs/1609.06657)\n\n**Tutorial on Answering Questions about Images with Deep Learning**\n\n- intro: The tutorial was presented at '2nd Summer School on Integrating Vision and Language: Deep Learning' in Malta, 2016\n- arxiv: [https://arxiv.org/abs/1610.01076](https://arxiv.org/abs/1610.01076)\n\n**Hadamard Product for Low-rank Bilinear Pooling**\n\n- arxiv: [https://arxiv.org/abs/1610.04325](https://arxiv.org/abs/1610.04325)\n- github: [https://github.com/jnhwkim/MulLowBiVQA](https://github.com/jnhwkim/MulLowBiVQA)\n\n**Open-Ended Visual Question-Answering**\n\n![](https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/model.jpg)\n\n- intro: Bachelor thesis report graded with A with honours at ETSETB Telecom BCN school, Universitat Polit\\`ecnica de Catalunya (UPC). June 2016\n- project page: [http://imatge-upc.github.io/vqa-2016-cvprw/](http://imatge-upc.github.io/vqa-2016-cvprw/)\n- arxiv: [https://arxiv.org/abs/1610.02692](https://arxiv.org/abs/1610.02692)\n- slides: [http://www.slideshare.net/xavigiro/openended-visual-questionanswering](http://www.slideshare.net/xavigiro/openended-visual-questionanswering)\n- github: [https://github.com/imatge-upc/vqa-2016-cvprw](https://github.com/imatge-upc/vqa-2016-cvprw)\n\n**Deep Learning for Question Answering**\n\n- intro: UMD. Mohit Iyyer.\n- intro: Recurrent Neural Networks, Recursive Neural Network\n- slides: [http://cs.umd.edu/~miyyer/data/deepqa.pdf](http://cs.umd.edu/~miyyer/data/deepqa.pdf)\n\n**Dual Attention Networks for Multimodal Reasoning and Matching**\n\n- arxiv: [https://arxiv.org/abs/1611.00471](https://arxiv.org/abs/1611.00471)\n\n**Dynamic Coattention Networks For Question Answering**\n\n- arxiv: [https://arxiv.org/abs/1611.01604](https://arxiv.org/abs/1611.01604)\n\n**State of the art deep learning model for question answering**\n\n- blog: [http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/](http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/)\n\n**Zero-Shot Visual Question Answering**\n\n- arxiv: [https://arxiv.org/abs/1611.05546](https://arxiv.org/abs/1611.05546)\n\n**Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation**\n\n- intro: University of Rochester & Microsoft & University College London\n- arxiv: [https://arxiv.org/abs/1701.08251](https://arxiv.org/abs/1701.08251)\n\n**Question Answering through Transfer Learning from Large Fine-grained Supervision Data**\n\n- intro: Seoul National University & University of Washington\n- arxiv: [https://arxiv.org/abs/1702.02171](https://arxiv.org/abs/1702.02171)\n\n**Question Answering from Unstructured Text by Retrieval and Comprehension**\n\n- arxiv: [https://arxiv.org/abs/1703.08885](https://arxiv.org/abs/1703.08885)\n- notes: [https://theneuralperspective.com/2017/04/26/question-answering-from-unstructured-text-by-retrieval-and-comprehension/](https://theneuralperspective.com/2017/04/26/question-answering-from-unstructured-text-by-retrieval-and-comprehension/)\n\n**Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering**\n\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/1704.03162](https://arxiv.org/abs/1704.03162)\n\n**Learning to Reason: End-to-End Module Networks for Visual Question Answering**\n\n- intro: UC Berkeley, Boston University\n- arxiv: [https://arxiv.org/abs/1704.05526](https://arxiv.org/abs/1704.05526)\n\n**TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering**\n\n- intro: CVPR 2017.Seoul National University & Yahoo Research\n- arxiv: [https://arxiv.org/abs/1704.04497](https://arxiv.org/abs/1704.04497)\n- github: [https://github.com/YunseokJANG/tgif-qa](https://github.com/YunseokJANG/tgif-qa)\n\n**Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks**\n\n- intro: ACL 2017 (short)\n- project page: [https://rajarshd.github.io/TextKBQA/](https://rajarshd.github.io/TextKBQA/)\n- arxiv: [https://arxiv.org/abs/1704.08384](https://arxiv.org/abs/1704.08384)\n- github: [https://github.com/rajarshd/TextKBQA](https://github.com/rajarshd/TextKBQA)\n\n**Learning Convolutional Text Representations for Visual Question Answering**\n\n- arxiv: [https://arxiv.org/abs/1705.06824](https://arxiv.org/abs/1705.06824)\n- github: [https://github.com/divelab/vqa-text](https://github.com/divelab/vqa-text)\n\n**Compact Tensor Pooling for Visual Question Answering**\n\n[https://arxiv.org/abs/1706.06706](https://arxiv.org/abs/1706.06706)\n\n**Long-Term Memory Networks for Question Answering**\n\n- intro: SUNY Buffalo & LinkedIn & LinkedIn\n- arxiv: [https://arxiv.org/abs/1707.01961](https://arxiv.org/abs/1707.01961)\n\n**Bottom-Up and Top-Down Attention for Image Captioning and VQA**\n\n**Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering**\n\n- intro: Winner of the Visual Question Answering Challenge at CVPR 2017\n- project page: [http://www.panderson.me/up-down-attention/](http://www.panderson.me/up-down-attention/)\n- arxiv: [https://arxiv.org/abs/1707.07998](https://arxiv.org/abs/1707.07998)\n- paper: [http://www.panderson.me/images/1707.07998-up-down.pdf](http://www.panderson.me/images/1707.07998-up-down.pdf)\n- github: [https://github.com//peteanderson80/bottom-up-attention](https://github.com//peteanderson80/bottom-up-attention)\n\n**Structured Attentions for Visual Question Answering**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.02071](https://arxiv.org/abs/1708.02071)\n- github: [https://github.com/zhuchen03/vqa-sva](https://github.com/zhuchen03/vqa-sva)\n\n**Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge**\n\n- intro: Winner of the 2017 Visual Question Answering (VQA) Challenge at CVPR\n- intro: The University of Adelaide & Australian National University & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1708.02711](https://arxiv.org/abs/1708.02711)\n\n**MemexQA: Visual Memex Question Answering**\n\n- intro: Carnegie Mellon University, Customer Service AI, Yahoo\n- project page: [https://memexqa.cs.cmu.edu/](https://memexqa.cs.cmu.edu/)\n- arxiv: [https://arxiv.org/abs/1708.01336](https://arxiv.org/abs/1708.01336)\n\n**Automatic Question-Answering Using A Deep Similarity Neural Network**\n\n- intro: New York University & AT&T Research Labs\n- arxiv: [https://arxiv.org/abs/1708.01713](https://arxiv.org/abs/1708.01713)\n\n**Question Dependent Recurrent Entity Network for Question Answering**\n\n- intro: University of Pisa\n- arxiv: [https://arxiv.org/abs/1707.07922](https://arxiv.org/abs/1707.07922)\n- github: [https://github.com/andreamad8/QDREN](https://github.com/andreamad8/QDREN)\n\n**Visual Question Generation as Dual Task of Visual Question Answering**\n\n[https://arxiv.org/abs/1709.07192](https://arxiv.org/abs/1709.07192)\n\n**A Read-Write Memory Network for Movie Story Understanding**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.09345](https://arxiv.org/abs/1709.09345)\n\n**iVQA: Inverse Visual Question Answering**\n\n[https://arxiv.org/abs/1710.03370](https://arxiv.org/abs/1710.03370)\n\n**DCN+: Mixed Objective and Deep Residual Coattention for Question Answering**\n\n- intro: Salesforce Research\n- arxiv: [https://arxiv.org/abs/1711.00106](https://arxiv.org/abs/1711.00106)\n- github: [https://github.com/mjacar/tensorflow-dcn-plus](https://github.com/mjacar/tensorflow-dcn-plus)\n\n**High-Order Attention Models for Visual Question Answering**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1711.04323](https://arxiv.org/abs/1711.04323)\n\n**Asking the Difficult Questions: Goal-Oriented Visual Question Generation via Intermediate Rewards**\n\n- intro: The University of Adelaide & University of Technology Sydney & Nanjing University of Science and Technology\n- arxiv: [https://arxiv.org/abs/1711.07614](https://arxiv.org/abs/1711.07614)\n\n**Visual Question Answering as a Meta Learning Task**\n\n[https://arxiv.org/abs/1711.08105](https://arxiv.org/abs/1711.08105)\n\n**Embodied Question Answering**\n\n- intro: Georgia Institute of Technology & Facebook AI Research\n- project page: [https://embodiedqa.org/](https://embodiedqa.org/)\n- arxiv: [https://arxiv.org/abs/1711.11543](https://arxiv.org/abs/1711.11543)\n- github: [https://github.com/facebookresearch/EmbodiedQA](https://github.com/facebookresearch/EmbodiedQA)\n\n**Learning by Asking Questions**\n\n[https://arxiv.org/abs/1712.01238](https://arxiv.org/abs/1712.01238)\n\n**Interpretable Counting for Visual Question Answering**\n\n[https://arxiv.org/abs/1712.08697](https://arxiv.org/abs/1712.08697)\n\n**Structured Triplet Learning with POS-tag Guided Attention for Visual Question Answering**\n\n- arxiv: [https://arxiv.org/abs/1801.07853](https://arxiv.org/abs/1801.07853)\n- github: [https://github.com/wangzheallen/STL-VQA](https://github.com/wangzheallen/STL-VQA)\n\n**DVQA: Understanding Data Visualizations via Question Answering**\n\n- intro: RIT & Adobe Research\n- arxiv: [https://arxiv.org/abs/1801.08163](https://arxiv.org/abs/1801.08163)\n\n**Object-based reasoning in VQA**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.09718](https://arxiv.org/abs/1801.09718)\n\n**Dual Recurrent Attention Units for Visual Question Answering**\n\n[https://arxiv.org/abs/1802.00209](https://arxiv.org/abs/1802.00209)\n\n**Differential Attention for Visual Question Answering**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.00298](https://arxiv.org/abs/1804.00298)\n\n**Question Type Guided Attention in Visual Question Answering**\n\n[https://arxiv.org/abs/1804.02088](https://arxiv.org/abs/1804.02088)\n\n**Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1804.09412](https://arxiv.org/abs/1804.09412)\n\n**Reciprocal Attention Fusion for Visual Question Answering**\n\n[https://arxiv.org/abs/1805.04247](https://arxiv.org/abs/1805.04247)\n\n**Learning to Count Objects in Natural Images for Visual Question Answering**\n\n- intro: ICLR 2018\n- arxiv: [https://arxiv.org/abs/1802.05766](https://arxiv.org/abs/1802.05766)\n\n**Bilinear Attention Networks**\n\n- intro: CVPR 2018. Seoul National University\n- arxiv: [https://arxiv.org/abs/1805.07932](https://arxiv.org/abs/1805.07932)\n- slides: [https://bi.snu.ac.kr/~jhkim/slides/bilinear%20attention%20networks_8min.pdf](https://bi.snu.ac.kr/~jhkim/slides/bilinear%20attention%20networks_8min.pdf)\n- github(official, PyTorch): [https://github.com/jnhwkim/ban-vqa](https://github.com/jnhwkim/ban-vqa)\n\n**Reproducibility Report for \"Learning To Count Objects In Natural Images For Visual Question Answering\"**\n\n[https://arxiv.org/abs/1805.08174](https://arxiv.org/abs/1805.08174)\n\n**Cross-Dataset Adaptation for Visual Question Answering**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1806.03726](https://arxiv.org/abs/1806.03726)\n\n**Learning Answer Embeddings for Visual Question Answering**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1806.03724](https://arxiv.org/abs/1806.03724)\n\n**Learning Visual Question Answering by Bootstrapping Hard Attention**\n\n- intro: ECCV 2018\n- axrxiv: [https://arxiv.org/abs/1808.00300](https://arxiv.org/abs/1808.00300)\n\n**Question-Guided Hybrid Convolution for Visual Question Answering**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.02632](https://arxiv.org/abs/1808.02632)\n\n**Interpretable Visual Question Answering by Reasoning on Dependency Trees**\n\n[https://arxiv.org/abs/1809.01810](https://arxiv.org/abs/1809.01810)\n\n**The Visual QA Devil in the Details: The Impact of Early Fusion and Batch Norm on CLEVR**\n\n- intro: ECCV 2018 Workshop on Shortcomings in Vision and Language\n- arxiv: [https://arxiv.org/abs/1809.04482](https://arxiv.org/abs/1809.04482)\n\n**Knowing Where to Look? Analysis on Attention of Visual Question Answering System**\n\n- intro: ECCV SiVL Workshop paper\n- arxiv: [https://arxiv.org/abs/1810.03821](https://arxiv.org/abs/1810.03821)\n\n**VQA with no questions-answers training**\n\n[https://arxiv.org/abs/1811.08481](https://arxiv.org/abs/1811.08481)\n\n**Visual Commonsense R-CNN**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2002.12204](https://arxiv.org/abs/2002.12204)\n\n# Video Question Answering\n\n**DeepStory: Video Story QA by Deep Embedded Memory Networks**\n\n- intro: IJCAI 2017. Seoul National University\n- arxiv: [https://arxiv.org/abs/1707.00836](https://arxiv.org/abs/1707.00836)\n\n**Video Question Answering via Attribute-Augmented Attention Network Learning**\n\n- intro: SIGIR 2017\n- arxiv: [https://arxiv.org/abs/1707.06355](https://arxiv.org/abs/1707.06355)\n\n**Leveraging Video Descriptions to Learn Video Question Answering**\n\n- intro: AAAI 2017\n- arxiv: [https://arxiv.org/abs/1611.04021](https://arxiv.org/abs/1611.04021)\n\n**A Joint Sequence Fusion Model for Video Question Answering and Retrieval**\n\n- intro: ECCV 2018\n- arixv: [https://arxiv.org/abs/1808.02559](https://arxiv.org/abs/1808.02559)\n\n# Projects\n\n**VQA Demo: Visual Question Answering Demo on pretrained model**\n\n- github: [https://github.com/iamaaditya/VQA_Demo](https://github.com/iamaaditya/VQA_Demo)\n- ref: [http://iamaaditya.github.io/research/](http://iamaaditya.github.io/research/)\n\n**deep-qa: Implementation of the Convolution Neural Network for factoid QA on the answer sentence selection task**\n\n- github: [https://github.com/aseveryn/deep-qa](https://github.com/aseveryn/deep-qa)\n\n**YodaQA: A Question Answering system built on top of the Apache UIMA framework**\n\n- homepage: [http://ailao.eu/yodaqa/](http://ailao.eu/yodaqa/)\n- github: [https://github.com/brmson/yodaqa](https://github.com/brmson/yodaqa)\n\n**insuranceQA-cnn-lstm: tensorflow and theano cnn code for insurance QA(question Answer matching)**\n\n- github: [https://github.com/white127/insuranceQA-cnn-lstm](https://github.com/white127/insuranceQA-cnn-lstm)\n\n**Tensorflow Implementation of Deeper LSTM+ normalized CNN for Visual Question Answering**\n\n![](https://cloud.githubusercontent.com/assets/19935904/16358326/e6812310-3add-11e6-914f-c61c19d6ab5a.png)\n\n- github: [https://github.com/JamesChuanggg/VQA-tensorflow](https://github.com/JamesChuanggg/VQA-tensorflow)\n\n**Visual Question Answering with Keras**\n\n![](https://camo.githubusercontent.com/f52d44199710c8f3939fb182a339d1d6a0b09a3f/687474703a2f2f692e696d6775722e636f6d2f327a4a30396d512e706e67)\n\n- project page: [https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/](https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/)\n- github: [https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering](https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering)\n\n**Deep Learning Models for Question Answering with Keras**\n\n- blog: [http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html](http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html)\n\n**GuessWhat?! Visual object discovery through multi-modal dialogue**\n\n- intro: University of Montreal & Univ. Lille & Google DeepMind & Twitter\n- arxiv: [https://arxiv.org/abs/1611.08481](https://arxiv.org/abs/1611.08481)\n\n**Deep QA: Using deep learning to answer Aristo's science questions**\n\n- github: [https://github.com/allenai/deep_qa](https://github.com/allenai/deep_qa)\n\n**Visual Question Answering in Pytorch**\n\n[https://github.com/Cadene/vqa.pytorch](https://github.com/Cadene/vqa.pytorch)\n\n# Dataset\n\n**Visual7W: Grounded Question Answering in Images**\n\n![](http://web.stanford.edu/~yukez/images/img/visual7w_examples.png)\n\n- homepage: [http://web.stanford.edu/~yukez/visual7w/](http://web.stanford.edu/~yukez/visual7w/)\n- github: [https://github.com/yukezhu/visual7w-toolkit](https://github.com/yukezhu/visual7w-toolkit)\n- github: [https://github.com/yukezhu/visual7w-qa-models](https://github.com/yukezhu/visual7w-qa-models)\n\n# Resources\n\n**Awesome Visual Question Answering**\n\n- github: [https://github.com/JamesChuanggg/awesome-vqa](https://github.com/JamesChuanggg/awesome-vqa)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","title":"Keep Up With New Trends"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Keep Up With New Trends\r\ndate: 2017-12-18\r\n---\r\n\r\n**ComputerVisionFoundation Videos**\r\n\r\n[https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists](https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/playlists)\r\n\r\n# ECCV 2018\r\n\r\n**ECCV 2018 papers**\r\n\r\n[http://openaccess.thecvf.com/ECCV2018.py](http://openaccess.thecvf.com/ECCV2018.py)\r\n\r\n# ICML 2018\r\n\r\n**DeepMind papers at ICML 2018**\r\n\r\n**Facebook Research at ICML 2018**\r\n\r\n[https://research.fb.com/facebook-research-at-icml-2018/](https://research.fb.com/facebook-research-at-icml-2018/)\r\n\r\n**ICML 2018 Notes**\r\n\r\n- day1: [https://gmarti.gitlab.io/ml/2018/07/10/icml18-tutorials.html](https://gmarti.gitlab.io/ml/2018/07/10/icml18-tutorials.html)\r\n- day2: [https://gmarti.gitlab.io/ml/2018/07/11/icml18-day-2.html](https://gmarti.gitlab.io/ml/2018/07/11/icml18-day-2.html)\r\n- day3: [https://gmarti.gitlab.io/ml/2018/07/12/icml18-day-3.html](https://gmarti.gitlab.io/ml/2018/07/12/icml18-day-3.html)\r\n- day4: [https://gmarti.gitlab.io/ml/2018/07/13/icml18-day-4.html](https://gmarti.gitlab.io/ml/2018/07/13/icml18-day-4.html)\r\n\r\n**ICML 2018 Notes**\r\n\r\n- notes: [https://david-abel.github.io/blog/posts/misc/icml_2018.pdf](https://david-abel.github.io/blog/posts/misc/icml_2018.pdf)\r\n- github: [https://david-abel.github.io/](https://david-abel.github.io/)\r\n\r\n# IJCAI 2018\r\n\r\n**Proceedings of IJCAI 2018**\r\n\r\n[https://www.ijcai.org/proceedings/2018/](https://www.ijcai.org/proceedings/2018/)\r\n\r\n# CVPR 2018\r\n\r\n**CVPR 2018 open access**\r\n\r\n[http://openaccess.thecvf.com/CVPR2018.py](http://openaccess.thecvf.com/CVPR2018.py)\r\n\r\n**CVPR18: Tutorials**\r\n\r\n- youtube: [https://www.youtube.com/playlist?list=PL_bDvITUYucD54Ym5XKGqTv9xNsrOX0aS](https://www.youtube.com/playlist?list=PL_bDvITUYucD54Ym5XKGqTv9xNsrOX0aS)\r\n- bilibili: [https://www.bilibili.com/video/av27038992/](https://www.bilibili.com/video/av27038992/)\r\n\r\n# VALSE 2018\r\n\r\n[http://ice.dlut.edu.cn/valse2018/programs.html](http://ice.dlut.edu.cn/valse2018/programs.html)\r\n\r\n# NIPS 2017\r\n\r\n**NIPS 2017 Spotlights**\r\n\r\n- youtube: [https://www.youtube.com/playlist?list=PLbVjlVq6hjK89WtlGHdC_PNwcawrzht5S](https://www.youtube.com/playlist?list=PLbVjlVq6hjK89WtlGHdC_PNwcawrzht5S)\r\n\r\n**NIPS 2017 — notes and thoughs**\r\n\r\n[https://olgalitech.wordpress.com/2017/12/12/nips-2017-notes-and-thoughs/](https://olgalitech.wordpress.com/2017/12/12/nips-2017-notes-and-thoughs/)\r\n\r\n**NIPS 2017 Notes**\r\n\r\n- notes: [https://cs.brown.edu/~dabel/blog/posts/misc/nips_2017.pdf](https://cs.brown.edu/~dabel/blog/posts/misc/nips_2017.pdf)\r\n- blog: [https://cs.brown.edu/~dabel/blog.html](https://cs.brown.edu/~dabel/blog.html)\r\n\r\n**NIPS 2017**\r\n\r\n- intro: A list of resources for all invited talks, tutorials, workshops and presentations at NIPS 2017\r\n- github: [https://github.com//hindupuravinash/nips2017](https://github.com//hindupuravinash/nips2017)\r\n\r\n**Global NIPS 2017 Paper Implementation Challenge**\r\n\r\n- intro: 8th December 2017 - 31st January 2018 (Application closed)\r\n- homepage: [https://nurture.ai/nips-challenge](https://nurture.ai/nips-challenge)\r\n\r\n# ICCV 2017\r\n\r\n**ICCV 2017 open access**\r\n\r\n[http://openaccess.thecvf.com/ICCV2017.py](http://openaccess.thecvf.com/ICCV2017.py)\r\n\r\n**ICCV 2017 Workshops, Venice Italy**\r\n\r\n[http://openaccess.thecvf.com/ICCV2017_workshops/menu.py](http://openaccess.thecvf.com/ICCV2017_workshops/menu.py)\r\n\r\n**ICCV17 Tutorials**\r\n\r\n[https://www.youtube.com/playlist?list=PL_bDvITUYucBGj2Hmv1e7CP9U82kHWVOT](https://www.youtube.com/playlist?list=PL_bDvITUYucBGj2Hmv1e7CP9U82kHWVOT)\r\n\r\n**Facebook at ICCV 2017**\r\n\r\n[https://research.fb.com/facebook-at-iccv-2017/](https://research.fb.com/facebook-at-iccv-2017/)\r\n\r\n**ICCV 2017 Tutorial on GANs**\r\n\r\n- homepage: [https://sites.google.com/view/iccv-2017-gans/schedule](https://sites.google.com/view/iccv-2017-gans/schedule)\r\n- youtube: {https://www.youtube.com/playlist?list=PL_bDvITUYucDEzjMTgh1cgtTIODZe3prZ}(https://www.youtube.com/playlist?list=PL_bDvITUYucDEzjMTgh1cgtTIODZe3prZ)\r\n\r\n# ILSVRC 2017\r\n\r\n**Overview of ILSVRC 2017**\r\n\r\n[http://image-net.org/challenges/talks_2017/ILSVRC2017_overview.pdf](http://image-net.org/challenges/talks_2017/ILSVRC2017_overview.pdf)\r\n\r\n**ImageNet: Where are we going? And where have we been?**\r\n\r\n- intro: by Fei-Fei Li, Jia Deng\r\n- slides: [http://image-net.org/challenges/talks_2017/imagenet_ilsvrc2017_v1.0.pdf](http://image-net.org/challenges/talks_2017/imagenet_ilsvrc2017_v1.0.pdf)\r\n\r\n# Deep Learning and Reinforcement Learning Summer School 2017\r\n\r\n- homepage: [https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/](https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/)\r\n- slides: [https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/slides/](https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/slides/)\r\n- mirror: [https://pan.baidu.com/s/1eSvijvW#list/path=%2F](https://pan.baidu.com/s/1eSvijvW#list/path=%2F)\r\n\r\n# ICLR 2017\r\n\r\n**ICLR 2017 Videos**\r\n\r\n[https://www.facebook.com/pg/iclr.cc/videos/](https://www.facebook.com/pg/iclr.cc/videos/)\r\n\r\n# CVPR 2017\r\n\r\n**CVPR 2017 open access**\r\n\r\n[http://openaccess.thecvf.com/CVPR2017.py](http://openaccess.thecvf.com/CVPR2017.py)\r\n\r\n**CVPR 2017 Workshops, Honolulu Hawaii**\r\n\r\n[http://openaccess.thecvf.com/CVPR2017_workshops/menu.py](http://openaccess.thecvf.com/CVPR2017_workshops/menu.py)\r\n\r\n## CVPR 2017 Tutorial\r\n\r\n**CVPR'17 Tutorial: Deep Learning for Objects and Scenes**\r\n\r\n[http://deeplearning.csail.mit.edu/](http://deeplearning.csail.mit.edu/)\r\n\r\n**Lecture 1: Learning Deep Representations for Visual Recognition**\r\n\r\n- intro: by Kaiming He\r\n- slides: [http://deeplearning.csail.mit.edu/cvpr2017_tutorial_kaiminghe.pdf](http://deeplearning.csail.mit.edu/cvpr2017_tutorial_kaiminghe.pdf)\r\n- youtube: [https://www.youtube.com/watch?v=jHv37mKAhV4](https://www.youtube.com/watch?v=jHv37mKAhV4)\r\n\r\n**Lecture 2: Deep Learning for Instance-level Object Understanding**\r\n\r\n- intro: by Ross Girshick\r\n- slides: [http://deeplearning.csail.mit.edu/instance_ross.pdf](http://deeplearning.csail.mit.edu/instance_ross.pdf)\r\n- youtube: [https://www.youtube.com/watch?v=jHv37mKAhV4&feature=youtu.be&t=2349](https://www.youtube.com/watch?v=jHv37mKAhV4&feature=youtu.be&t=2349)\r\n\r\n# NIPS 2016\r\n\r\n**NIPS 2016 Schedule**\r\n\r\n[https://nips.cc/Conferences/2016/Schedule](https://nips.cc/Conferences/2016/Schedule)\r\n\r\n**DeepMind Papers @ NIPS (Part 1)**\r\n\r\n[https://deepmind.com/blog/deepmind-papers-nips-part-1/](https://deepmind.com/blog/deepmind-papers-nips-part-1/)\r\n\r\n**DeepMind Papers @ NIPS (Part 2)**\r\n\r\n[https://deepmind.com/blog/deepmind-papers-nips-part-2/](https://deepmind.com/blog/deepmind-papers-nips-part-2/)\r\n\r\n**DeepMind Papers @ NIPS (Part 3)**\r\n\r\n[https://deepmind.com/blog/deepmind-papers-nips-part-3/](https://deepmind.com/blog/deepmind-papers-nips-part-3/)\r\n\r\n**NIPS 2016 Review, Days 0 & 1**\r\n\r\n[https://gab41.lab41.org/nips-2016-review-day-1-6e504bcf1451#.ldaft47ea](https://gab41.lab41.org/nips-2016-review-day-1-6e504bcf1451#.ldaft47ea)\r\n\r\n**NIPS 2016 Review, Day 2**\r\n\r\n[https://gab41.lab41.org/nips-2016-review-day-2-daff1088135e#.o9r8li43x](https://gab41.lab41.org/nips-2016-review-day-2-daff1088135e#.o9r8li43x)\r\n\r\n**NIPS 2016 — Day 1 Highlights**\r\n\r\n[https://blog.insightdatascience.com/nips-2016-day-1-6ae1207cab82#.c248ycixg](https://blog.insightdatascience.com/nips-2016-day-1-6ae1207cab82#.c248ycixg)\r\n\r\n**NIPS 2016 — Day 2 Highlights: Platform wars, RL and RNNs**\r\n\r\n[https://blog.insightdatascience.com/nips-2016-day-2-highlights-platform-wars-rl-and-rnns-9dca43bc1448#.zgtu1rtr0](https://blog.insightdatascience.com/nips-2016-day-2-highlights-platform-wars-rl-and-rnns-9dca43bc1448#.zgtu1rtr0)\r\n\r\n**50 things I learned at NIPS 2016**\r\n\r\n[https://blog.ought.com/nips-2016-875bb8fadb8c#.f1a1161hq](https://blog.ought.com/nips-2016-875bb8fadb8c#.f1a1161hq)\r\n\r\n**NIPS 2016 Highlights**\r\n\r\n- slides: [http://www.slideshare.net/SebastianRuder/nips-2016-highlights-sebastian-ruder](http://www.slideshare.net/SebastianRuder/nips-2016-highlights-sebastian-ruder)\r\n- mirror: [https://pan.baidu.com/s/1kUKnCJ9](https://pan.baidu.com/s/1kUKnCJ9)\r\n\r\n**Brad Neuberg’s NIPS 2016 Notes**\r\n\r\n- blog: [https://paper.dropbox.com/doc/Brad-Neubergs-NIPS-2016-Notes-XUFRdpNYyBhau0gWcybRo](https://paper.dropbox.com/doc/Brad-Neubergs-NIPS-2016-Notes-XUFRdpNYyBhau0gWcybRo)\r\n\r\n**All Code Implementations for NIPS 2016 papers**\r\n\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/](https://www.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/)\r\n\r\n# Heuritech Deep Learning Meetup\r\n\r\n**Heuritech Deep Learning Meetup #7: more than 100 attendees for convolutionnal neural networks**\r\n\r\n- blog: [https://blog.heuritech.com/2016/11/03/heuritech-deep-learning-meetup-7-more-than-100-attendees-for-convolutionnal-neural-networks/](https://blog.heuritech.com/2016/11/03/heuritech-deep-learning-meetup-7-more-than-100-attendees-for-convolutionnal-neural-networks/)\r\n\r\n# ECCV 2016\r\n\r\n**ECCV Brings Together the Brightest Minds in Computer Vision**\r\n\r\n[https://research.facebook.com/blog/eccv-brings-together-the-brightest-minds-in-computer-vision/](https://research.facebook.com/blog/eccv-brings-together-the-brightest-minds-in-computer-vision/)\r\n\r\n**ECCV in a theatrical setting**\r\n\r\n- blog: [http://zoyathinks.blogspot.jp/2016/10/eccv-in-theatrical-setting.html](http://zoyathinks.blogspot.jp/2016/10/eccv-in-theatrical-setting.html)\r\n\r\n# 2nd ImageNet + COCO Joint Workshop\r\n\r\n**2nd ImageNet and COCO Visual Recognition Challenges Joint Workshop**\r\n\r\n[http://image-net.org/challenges/ilsvrc+coco2016](http://image-net.org/challenges/ilsvrc+coco2016)\r\n\r\n# DLSS 2016\r\n\r\n**Montréal Deep Learning Summer School 2016**\r\n\r\n- video lectures: [http://videolectures.net/deeplearning2016_montreal/](http://videolectures.net/deeplearning2016_montreal/)\r\n- material: [https://github.com/mila-udem/summerschool2016](https://github.com/mila-udem/summerschool2016)\r\n- slides: [https://sites.google.com/site/deeplearningsummerschool2016/speakers](https://sites.google.com/site/deeplearningsummerschool2016/speakers)\r\n- mirror: [http://pan.baidu.com/s/1kUWrWI7](http://pan.baidu.com/s/1kUWrWI7)\r\n\r\n**Highlights from the Deep Learning Summer School (Part 1)**\r\n\r\n[https://vkrakovna.wordpress.com/2016/08/25/highlights-from-the-deep-learning-summer-school-part-1/](https://vkrakovna.wordpress.com/2016/08/25/highlights-from-the-deep-learning-summer-school-part-1/)\r\n\r\n**What I learned from Deep Learning Summer School 2016**\r\n\r\n[https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi](https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi)\r\n\r\n# ICML 2016\r\n\r\n**10 Papers from ICML and CVPR**\r\n\r\n[https://leotam.github.io/general/2016/07/12/ICMLcVPR.html](https://leotam.github.io/general/2016/07/12/ICMLcVPR.html)\r\n\r\n**ICML 2016 was awesome**\r\n\r\n- blog: [http://hunch.net/?p=4710099](http://hunch.net/?p=4710099)\r\n\r\n**Highlights from ICML 2016**\r\n\r\n[http://www.lunametrics.com/blog/2016/07/05/highlights-icml-2016/](http://www.lunametrics.com/blog/2016/07/05/highlights-icml-2016/)\r\n\r\n**ICML 2016 tutorials**\r\n\r\n[http://icml.cc/2016/?page_id=97](http://icml.cc/2016/?page_id=97)\r\n\r\n**Deep Learning, Tools and Methods workshop**\r\n\r\n- intro: 3 hour tutorials on Torch, Tensorflow and Talks by Yoshua Bengio, NVIDIA, AMD\r\n- homepage: [https://portal.klewel.com/watch/webcast/deep-learning-tools-and-methods-workshop/](https://portal.klewel.com/watch/webcast/deep-learning-tools-and-methods-workshop/)\r\n- slides: [http://www.idiap.ch/workshop/dltm/](http://www.idiap.ch/workshop/dltm/)\r\n- Torch tutorials: [https://github.com/szagoruyko/idiap-tutorials](https://github.com/szagoruyko/idiap-tutorials)\r\n\r\n**ICML 2016 Conference and Workshops**\r\n\r\n- intro: talks, orals, tutorials\r\n- homepage: [http://techtalks.tv/icml/2016/](http://techtalks.tv/icml/2016/)\r\n\r\n# ICLR 2016\r\n\r\n**Deep Learning Trends @ ICLR 2016**\r\n\r\n[http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html](http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html)\r\n\r\n**WACV 2016: IEEE Winter Conference on Applications of Computer Vision**\r\n\r\n- homepage: [http://www.wacv16.org/](http://www.wacv16.org/)\r\n- youtube: [https://www.youtube.com/channel/UCdV5ooxkvhbpmv0_3MzIo9g/videos](https://www.youtube.com/channel/UCdV5ooxkvhbpmv0_3MzIo9g/videos)\r\n\r\n**ICLR 2016 Takeaways: Adversarial Models & Optimization**\r\n\r\n[https://indico.io/blog/iclr-2016-takeaways/](https://indico.io/blog/iclr-2016-takeaways/)\r\n\r\n**tensor talk - Latest AI Code: conference-iclr-2016**\r\n\r\n[https://tensortalk.com/?cat=conference-iclr-2016](https://tensortalk.com/?cat=conference-iclr-2016)\r\n\r\n# CVPR 2016\r\n\r\n**CVPR 2016**\r\n\r\n- homepage: [http://cvpr2016.thecvf.com/program/main_conference](http://cvpr2016.thecvf.com/program/main_conference)\r\n- Object Recognition and Detection: [http://cvpr2016.thecvf.com/program/main_conference#O1-2A](http://cvpr2016.thecvf.com/program/main_conference#O1-2A)\r\n- Object Detection 1: [http://cvpr2016.thecvf.com/program/main_conference#S1-2A](http://cvpr2016.thecvf.com/program/main_conference#S1-2A)\r\n- Object Detection 2: [http://cvpr2016.thecvf.com/program/main_conference#S2-2A](http://cvpr2016.thecvf.com/program/main_conference#S2-2A)\r\n\r\n**Workshop @ CVPR16: Deep Vision Workshop**\r\n\r\n- youtube: [https://www.youtube.com/playlist?list=PL_bDvITUYucC8uLRtWw8fdvVr3DdwzAeH](https://www.youtube.com/playlist?list=PL_bDvITUYucC8uLRtWw8fdvVr3DdwzAeH)\r\n\r\n**Five Things I Learned at CVPR 2016**\r\n\r\n- day 1: [https://gab41.lab41.org/all-your-questions-answered-cvpr-day-1-40f488103076#.ejrgol28h](https://gab41.lab41.org/all-your-questions-answered-cvpr-day-1-40f488103076#.ejrgol28h)\r\n- day 2: [https://gab41.lab41.org/the-sounds-of-cvpr-day-2-f33a3625cbf3#.nifea1blu](https://gab41.lab41.org/the-sounds-of-cvpr-day-2-f33a3625cbf3#.nifea1blu)\r\n- day 3: [https://gab41.lab41.org/animated-gifs-and-video-clips-cvpr-day-3-96fdcfc36e2c#.x9wd86lym](https://gab41.lab41.org/animated-gifs-and-video-clips-cvpr-day-3-96fdcfc36e2c#.x9wd86lym)\r\n- day 4: [https://gab41.lab41.org/caption-this-cvpr-day-4-8fe94d7aeb71#.rhzd3zg5j](https://gab41.lab41.org/caption-this-cvpr-day-4-8fe94d7aeb71#.rhzd3zg5j)\r\n- day 5: [https://gab41.lab41.org/five-things-i-learned-at-cvpr-2016-5e857c017f7b#.umag6vs3v](https://gab41.lab41.org/five-things-i-learned-at-cvpr-2016-5e857c017f7b#.umag6vs3v)\r\n\r\n# VALSE 2016\r\n\r\n**VALSE 2016**\r\n\r\n[http://mclab.eic.hust.edu.cn/valse2016/program.html](http://mclab.eic.hust.edu.cn/valse2016/program.html)\r\n\r\n**Science: Table of Contents: Artificial Intelligence**\r\n\r\n[http://science.sciencemag.org/content/349/6245.toc](http://science.sciencemag.org/content/349/6245.toc)\r\n\r\n**Deep Learning and the Future of AI**\r\n\r\n- author: by Prof. Yann LeCun (Director of AI Research at Facebook & Professor at NYU)\r\n- homapage: [http://indico.cern.ch/event/510372/](http://indico.cern.ch/event/510372/)\r\n- slides: [http://indico.cern.ch/event/510372/attachments/1245509/1840815/lecun-20160324-cern.pdf](http://indico.cern.ch/event/510372/attachments/1245509/1840815/lecun-20160324-cern.pdf)\r\n\r\n# ICML 2015\r\n\r\n**Video Recordings of the ICML’15 Deep Learning Workshop**\r\n\r\n- homepage: [http://dpkingma.com/?page_id=483](http://dpkingma.com/?page_id=483)\r\n- youtube: [https://www.youtube.com/playlist?list=PLdH9u0f1XKW8cUM3vIVjnpBfk_FKzviCu](https://www.youtube.com/playlist?list=PLdH9u0f1XKW8cUM3vIVjnpBfk_FKzviCu)\r\n\r\n# ICCV 2015\r\n\r\n**International Conference on Computer Vision (ICCV) 2015, Santiago**\r\n\r\n[http://videolectures.net/iccv2015_santiago/](http://videolectures.net/iccv2015_santiago/)\r\n\r\n**ICCV 2015 Tutorial on Tools for Efficient Object Detection**\r\n\r\n[http://mp7.watson.ibm.com/ICCV2015/ObjectDetectionICCV2015.html](http://mp7.watson.ibm.com/ICCV2015/ObjectDetectionICCV2015.html)\r\n\r\n**ICCV 2015 Tutorials**\r\n\r\n[http://pamitc.org/iccv15/tutorials.php](http://pamitc.org/iccv15/tutorials.php)\r\n\r\n**ICCV 2015 Tutorial on Tools for Efficient Object Detection**\r\n\r\n[http://mp7.watson.ibm.com/ICCV2015/ObjectDetectionICCV2015.html](http://mp7.watson.ibm.com/ICCV2015/ObjectDetectionICCV2015.html)\r\n\r\n# ImageNet + COCO Joint Workshop\r\n\r\n**ImageNet and MS COCO Visual Recognition Challenges Joint Workshop**\r\n\r\n[http://image-net.org/challenges/ilsvrc+mscoco2015](http://image-net.org/challenges/ilsvrc+mscoco2015)\r\n\r\n**OpenAI: Some thoughts, mostly questions**\r\n\r\n[https://medium.com/@kleinsound/openai-some-thoughts-mostly-questions-30fb63d53ef0#.32u1yt6oy](https://medium.com/@kleinsound/openai-some-thoughts-mostly-questions-30fb63d53ef0#.32u1yt6oy)\r\n\r\n**OpenAI — quick thoughts**\r\n\r\n[http://wp.goertzel.org/openai-quick-thoughts/](http://wp.goertzel.org/openai-quick-thoughts/)\r\n\r\n# NIPS 2015\r\n\r\n**NIPS 2015 workshop on non-convex optimization**\r\n\r\n[http://www.offconvex.org/2016/01/25/non-convex-workshop/](http://www.offconvex.org/2016/01/25/non-convex-workshop/)\r\n\r\n**10 Deep Learning Trends at NIPS 2015**\r\n\r\n[http://codinginparadise.org/ebooks/html/blog/ten_deep_learning_trends_at_nips_2015.html](http://codinginparadise.org/ebooks/html/blog/ten_deep_learning_trends_at_nips_2015.html)\r\n\r\n**NIPS 2015 – Deep RL Workshop**\r\n\r\n[https://gridworld.wordpress.com/2015/12/13/nips-2015-deep-rl-workshop/](https://gridworld.wordpress.com/2015/12/13/nips-2015-deep-rl-workshop/)\r\n\r\n**My takeaways from NIPS 2015**\r\n\r\n- blog: [http://www.danvk.org/2015/12/12/nips-2015.html](http://www.danvk.org/2015/12/12/nips-2015.html)\r\n\r\n**On the spirit of NIPS 2015 and OpenAI**\r\n\r\n- blog: [https://blogs.princeton.edu/imabandit/2015/12/13/on-the-spirit-of-nips-2015-and-openai/](https://blogs.princeton.edu/imabandit/2015/12/13/on-the-spirit-of-nips-2015-and-openai/)\r\n\r\n**NIPS 2015**\r\n\r\n- Part 1: [https://memming.wordpress.com/2015/12/07/nips-2015-part-1/](https://memming.wordpress.com/2015/12/07/nips-2015-part-1/)\r\n- Part 2: [https://memming.wordpress.com/2015/12/09/nips-2015-part-2/](https://memming.wordpress.com/2015/12/09/nips-2015-part-2/)\r\n\r\n**Deep Learning - NIPS’2015 Tutorial (By Geoff Hinton, Yoshua Bengio & Yann LeCun)**\r\n\r\n- slides: [http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf](http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf)\r\n\r\n**NIPS 2015 Posner Lecture – Zoubin Ghahramani: Probabilistic Machine Learning**\r\n\r\n[https://gridworld.wordpress.com/2015/12/08/nips-2015-posner-lecture-zoubin-ghahramani/](https://gridworld.wordpress.com/2015/12/08/nips-2015-posner-lecture-zoubin-ghahramani/)\r\n\r\n**NIPS 2015 Deep Learning Tutorial Notes**\r\n\r\n[http://jatwood.org/blog/nips-deep-learning-tutorial.html](http://jatwood.org/blog/nips-deep-learning-tutorial.html)\r\n\r\n# DLSS 2015\r\n\r\n**26 Things I Learned in the Deep Learning Summer School**\r\n\r\n[http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/](http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/) <br />\r\n[http://www.csdn.net/article/2015-09-16/2825716](http://www.csdn.net/article/2015-09-16/2825716)\r\n\r\n**Deep Learning Summer School 2015**\r\n\r\n- homepage: [https://sites.google.com/site/deeplearningsummerschool/schedule](https://sites.google.com/site/deeplearningsummerschool/schedule)\r\n- slides: [http://docs.huihoo.com/deep-learning/deeplearningsummerschool/2015/](http://docs.huihoo.com/deep-learning/deeplearningsummerschool/2015/)\r\n- github: [https://github.com/mila-udem/summerschool2015](https://github.com/mila-udem/summerschool2015)\r\n\r\n# ICLR 2015\r\n\r\n**Conference Schedule**\r\n\r\n[http://www.iclr.cc/doku.php?id=iclr2015:main&utm_content=buffer0b339&utm_campaign=buffer#conference_schedule](http://www.iclr.cc/doku.php?id=iclr2015:main&utm_content=buffer0b339&utm_campaign=buffer#conference_schedule)\r\n\r\n# CVPR 2014\r\n\r\n**TUTORIAL ON DEEP LEARNING FOR VISION**\r\n\r\n[https://sites.google.com/site/deeplearningcvpr2014/](https://sites.google.com/site/deeplearningcvpr2014/)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","title":"3D"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: 3D\ndate: 2021-07-28\n---\n\n# Papers\n\n**Expressive Body Capture: 3D Hands, Face, and Body from a Single Image**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.05866](https://arxiv.org/abs/1904.05866)\n- project page: [https://smpl-x.is.tue.mpg.de/](https://smpl-x.is.tue.mpg.de/)\n- github: [https://github.com/vchoutas/smplify-x](https://github.com/vchoutas/smplify-x)\n\n**Collaborative Regression of Expressive Bodies using Moderation**\n\n- intro: PIXIE\n- project page: [https://pixie.is.tue.mpg.de/](https://pixie.is.tue.mpg.de/)\n- arxiv: [https://arxiv.org/abs/2105.05301](https://arxiv.org/abs/2105.05301)\n- github: [https://github.com/YadiraF/PIXIE](https://github.com/YadiraF/PIXIE)\n\n**Hand Image Understanding via Deep Multi-Task Learning**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2107.11646](https://arxiv.org/abs/2107.11646)\n\n**VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the Wild**\n\n[https://arxiv.org/abs/2108.02452](https://arxiv.org/abs/2108.02452)\n\n**EventHPE: Event-based 3D Human Pose and Shape Estimation**\n\n- intro: ICCV 2021\n- intro: University of Alberta & Shandong University & Celepixel Technology & University of Guelph & Nanyang Technological University\n- arxiv: [https://arxiv.org/abs/2108.06819](https://arxiv.org/abs/2108.06819)\n\n# Monocular 3D Object Detection\n\n**Monocular 3D Object Detection and Box Fitting Trained End-to-End Using Intersection-over-Union Loss**\n\n- keywords: SS3D\n- arxiv: [https://arxiv.org/abs/1906.08070](https://arxiv.org/abs/1906.08070)\n- video: [https://www.youtube.com/playlist?list=PL4jJwJr7UjMb4bzLwUGHdVmhfNS2Ads_x](https://www.youtube.com/playlist?list=PL4jJwJr7UjMb4bzLwUGHdVmhfNS2Ads_x)\n\n**M3D-RPN: Monocular 3D Region Proposal Network for Object Detection**\n\n- intro: ICCV 2019 oral\n- project page: [http://cvlab.cse.msu.edu/project-m3d-rpn.html](http://cvlab.cse.msu.edu/project-m3d-rpn.html)\n- arxiv: [https://arxiv.org/abs/1907.06038](https://arxiv.org/abs/1907.06038)\n- github: [https://github.com/garrickbrazil/M3D-RPN](https://github.com/garrickbrazil/M3D-RPN)\n\n**Learning Depth-Guided Convolutions for Monocular 3D Object Detection**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/1912.04799](https://arxiv.org/abs/1912.04799)\n- github: [https://github.com/dingmyu/D4LCN](https://github.com/dingmyu/D4LCN)\n\n**RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2001.03343](https://arxiv.org/abs/2001.03343)\n\n**SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation**\n\n- intro: CVPR 2020\n- intro: ZongMu Tech & TU/e\n- arxiv: [https://arxiv.org/abs/2002.10111](https://arxiv.org/abs/2002.10111)\n- github(official): [https://github.com/lzccccc/SMOKE](https://github.com/lzccccc/SMOKE)\n\n**Center3D: Center-based Monocular 3D Object Detection with Joint Depth Understanding**\n\n- keywords: one-stage anchor-free\n- arxiv: [https://arxiv.org/abs/2005.13423](https://arxiv.org/abs/2005.13423)\n\n**Monocular Differentiable Rendering for Self-Supervised 3D Object Detection**\n\n- intro: ECCV 2020\n- intro: Preferred Networks, Inc & Toyota Research Institute\n- arxiv: [https://arxiv.org/abs/2009.14524](https://arxiv.org/abs/2009.14524)\n\n**M3DSSD: Monocular 3D Single Stage Object Detector**\n\n- intro: CVPR 2021\n- intro: Zhejiang University & Mohamed bin Zayed University of Artificial Intelligence & Inception Institute of Artificial Intelligence\n- arxiv: [https://arxiv.org/abs/2103.13164](https://arxiv.org/abs/2103.13164)\n\n**Delving into Localization Errors for Monocular 3D Object Detection**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.16237](https://arxiv.org/abs/2103.16237)\n- github: [https://github.com/xinzhuma/monodle](https://github.com/xinzhuma/monodle)\n\n**Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection**\n\n- github: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.16470](https://arxiv.org/abs/2103.16470)\n- github: [https://github.com/fudan-zvg/DDMP](https://github.com/fudan-zvg/DDMP)\n\n**GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.17202](https://arxiv.org/abs/2103.17202)\n\n**Objects are Different: Flexible Monocular 3D Object Detection**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.02323](https://arxiv.org/abs/2104.02323)\n- github: [https://github.com/zhangyp15/MonoFlex](https://github.com/zhangyp15/MonoFlex)\n\n**Geometry-based Distance Decomposition for Monocular 3D Object Detection**\n\n[https://arxiv.org/abs/2104.03775](https://arxiv.org/abs/2104.03775)\n\n**Geometry-aware data augmentation for monocular 3D object detection**\n\n[https://arxiv.org/abs/2104.05858](https://arxiv.org/abs/2104.05858)\n\n**OCM3D: Object-Centric Monocular 3D Object Detection**\n\n[https://arxiv.org/abs/2104.06041](https://arxiv.org/abs/2104.06041)\n\n**Exploring 2D Data Augmentation for 3D Monocular Object Detection**\n\n[https://arxiv.org/abs/2104.10786](https://arxiv.org/abs/2104.10786)\n\n**Progressive Coordinate Transforms for Monocular 3D Object Detection**\n\n- intro: Fudan University & Amazon Inc.\n- arxiv: [https://arxiv.org/abs/2108.05793](https://arxiv.org/abs/2108.05793)\n- github: [https://github.com/amazon-research/progressive-coordinate-transforms](https://github.com/amazon-research/progressive-coordinate-transforms)\n\n**AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection**\n\n- intro: ICCV 2021\n- intro: Baidu Research\n- arxiv: [https://arxiv.org/abs/2108.11127](https://arxiv.org/abs/2108.11127)\n- github: [https://github.com/zongdai/AutoShape](https://github.com/zongdai/AutoShape)\n\n**Categorical Depth Distribution Network for Monocular 3D Object Detection**\n\n- intro: CVPR 2021 oral\n- intro: University of Toronto Robotics Institute\n- project page: [https://trailab.github.io/CaDDN/](https://trailab.github.io/CaDDN/)\n- arxiv: [https://arxiv.org/abs/2103.01100](https://arxiv.org/abs/2103.01100)\n- github: [https://github.com/TRAILab/CaDDN](https://github.com/TRAILab/CaDDN)\n\n**The Devil is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2112.14023](https://arxiv.org/abs/2112.14023)\n\n**SGM3D: Stereo Guided Monocular 3D Object Detection**\n\n- intro: Fudan University & Baidu Inc.\n- arxiv: [https://arxiv.org/abs/2112.01914](https://arxiv.org/abs/2112.01914)\n- github: [https://github.com/zhouzheyuan/sgm3d](https://github.com/zhouzheyuan/sgm3d)\n\n**MonoDistill: Learning Spatial Features for Monocular 3D Object Detection**\n\n- intro: ICLR 2022\n- intro: Dalian University of Technology & The University of Sydney\n- arxiv: [https://arxiv.org/abs/2201.10830](https://arxiv.org/abs/2201.10830)\n- github: [https://github.com/monster-ghost/MonoDistill](https://github.com/monster-ghost/MonoDistill)\n\n**Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.02112](https://arxiv.org/abs/2203.02112)\n- github:[https://github.com/revisitq/Pseudo-Stereo-3D](https://github.com/revisitq/Pseudo-Stereo-3D)\n\n**MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection**\n\n- intro: CVPR 2022\n- intro: The Hong Kong University of Science and Technology & DJI\n- arxiv: [https://arxiv.org/abs/2203.08563](https://arxiv.org/abs/2203.08563)\n\n**MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer**\n\n- intro: CVPR 2022\n- intro: National Taiwan University & Mobile Drive Technology\n- arxiv: [https://arxiv.org/abs/2203.10981](https://arxiv.org/abs/2203.10981)\n- github: [https://github.com/kuanchihhuang/MonoDTR](https://github.com/kuanchihhuang/MonoDTR)\n\n**MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection**\n\n- intro: Shanghai AI Laboratory & Peking University & The Chinese University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2203.13310](https://arxiv.org/abs/2203.13310)\n\n**Homography Loss for Monocular 3D Object Detection**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.00754](https://arxiv.org/abs/2204.00754)\n\n**Towards Model Generalization for Monocular 3D Object Detection**\n\n- intro: Harbin Institute of Technology & University of Science and Technology of China & SenseTime Research\n- arxiv: [https://arxiv.org/abs/2205.11664](https://arxiv.org/abs/2205.11664)\n\n**Delving into the Pre-training Paradigm of Monocular 3D Object Detection**\n\n- intro: Tsinghua University & Huazhong University of Science and Technology\n- arxiv: [https://arxiv.org/abs/2206.03657](https://arxiv.org/abs/2206.03657)\n\n**MonoGround: Detecting Monocular 3D Objects from the Ground**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2206.07372](https://arxiv.org/abs/2206.07372)\n- github: [https://github.com/cfzd/MonoGround](https://github.com/cfzd/MonoGround)\n\n**Densely Constrained Depth Estimator for Monocular 3D Object Detection**\n\n- intro: ECCV 2022\n- intro: CASIA & UCAS & HKISI CAS\n- arxiv: [https://arxiv.org/abs/2207.10047](https://arxiv.org/abs/2207.10047)\n- github:[https://github.com/BraveGroup/DCD](https://github.com/BraveGroup/DCD)\n\n**Consistency of Implicit and Explicit Features Matters for Monocular 3D Object Detection**\n\n- intro: DiDi\n- arxiv: [https://arxiv.org/abs/2207.07933](https://arxiv.org/abs/2207.07933)\n\n**DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection**\n\n- intro: ECCV 2022\n- intro: Zhejiang University & Fabu Inc.\n- arxiv: [https://arxiv.org/abs/2207.08531](https://arxiv.org/abs/2207.08531)\n- github: [https://github.com/SPengLiang/DID-M3D](https://github.com/SPengLiang/DID-M3D)\n\n**DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection**\n\n- intro: ECCV 2022\n- intro: Michigan State University & Meta AI & Ford Motor Company\n- arxiv: [https://arxiv.org/abs/2207.10758](https://arxiv.org/abs/2207.10758)\n- github: [https://github.com/abhi1kumar/DEVIANT](https://github.com/abhi1kumar/DEVIANT)\n\n**Monocular 3D Object Detection with Depth from Motion**\n\n- intro: ECCV 2022 Oral\n- intro: The Chinese University of Hong Kong & Shanghai AI Laboratory\n- arxiv: [https://arxiv.org/abs/2207.12988](https://arxiv.org/abs/2207.12988)\n- github: [https://github.com/Tai-Wang/Depth-from-Motion](https://github.com/Tai-Wang/Depth-from-Motion)\n\n**MV-FCOS3D++: Multi-View Camera-Only 4D Object Detection with Pretrained Monocular Backbones**\n\n- intro: The Chinese University of Hong Kong & Hong Kong University of Science and Technology & The Chinese University of Hong Kong & 4Nanyang Technological University\n- arxiv: [https://arxiv.org/abs/2207.12716](https://arxiv.org/abs/2207.12716)\n- github: [https://github.com/Tai-Wang/Depth-from-Motion](https://github.com/Tai-Wang/Depth-from-Motion)\n\n# Multi-Modal 3D Object Detection\n\n**AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection**\n\n- intro: IJCAI 2022\n- intro: University of Science and Technology & Harbin Institute of Technology & SenseTime Research & The Chinese University of Hong Kong & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2201.06493](https://arxiv.org/abs/2201.06493)\n\n**AutoAlignV2: Deformable Feature Aggregation for Dynamic Multi-Modal 3D Object Detection**\n\n- intro: ECCV 2022\n- intro: University of Science and Technology of China & Harbin Institute of Technology & SenseTime Research\n- arxiv: [https://arxiv.org/abs/2207.10316](https://arxiv.org/abs/2207.10316)\n- github: [https://github.com/zehuichen123/AutoAlignV2](https://github.com/zehuichen123/AutoAlignV2)\n\n# Monocular 3D Detection and Tracking\n\n**Time3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving**\n\n- intro: CVPR 2022\n- intro: PP-CEM & Rising Auto\n- arxiv: [https://arxiv.org/abs/2205.14882](https://arxiv.org/abs/2205.14882)\n\n**Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking**\n\n- intro: Waymo LLC & Johns Hopkins University & Cornell University\n- arxiv: [https://arxiv.org/abs/2206.03666](https://arxiv.org/abs/2206.03666)\n\n# Multi-Camera 3D Object Detection\n\n**PETR: Position Embedding Transformation for Multi-View 3D Object Detection**\n\n- intro: MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2203.05625](https://arxiv.org/abs/2203.05625)\n\n**PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images**\n\n- intro: MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2206.01256](https://arxiv.org/abs/2206.01256)\n\n# Multi-Camera Multiple 3D Object Tracking\n\n**Multi-Camera Multiple 3D Object Tracking on the Move for Autonomous Vehicles**\n\n- intro: CVPR Workshop 2022\n- arxiv: [https://arxiv.org/abs/2204.09151](https://arxiv.org/abs/2204.09151)\n\n**SRCN3D: Sparse R-CNN 3D Surround-View Camera Object Detection and Tracking for Autonomous Driving**\n\n- intro: Tsinghua University\n- arxiv: [https://arxiv.org/abs/2206.14451](https://arxiv.org/abs/2206.14451)\n- github: [https://github.com/synsin0/SRCN3D](https://github.com/synsin0/SRCN3D)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","title":"BEV"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: BEV\ndate: 2022-06-27\n---\n\n# Papers\n\n# Multi-Camera 3D Object Detection\n\n**Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D**\n\n- intro: ECCV 2020\n- intro: NVIDIA, Vector Institute, University of Toronto\n- project page: [https://nv-tlabs.github.io/lift-splat-shoot/](https://nv-tlabs.github.io/lift-splat-shoot/)\n- arxiv: [https://arxiv.org/abs/2008.05711](https://arxiv.org/abs/2008.05711)\n- github: [https://github.com/nv-tlabs/lift-splat-shoot](https://github.com/nv-tlabs/lift-splat-shoot)\n\n**BEVDet: High-Performance Multi-Camera 3D Object Detection in Bird-Eye-View**\n\n- intro: PhiGent Robotics\n- arxiv: [https://arxiv.org/abs/2112.11790](https://arxiv.org/abs/2112.11790)\n\n**BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection**\n\n- intro: PhiGent Robotics\n- arxiv: [https://arxiv.org/abs/2203.17054](https://arxiv.org/abs/2203.17054)\n\n**BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving**\n\n- intro: Tsinghua University & PhiGent Robotics\n- arxiv: [https://arxiv.org/abs/2205.09743](https://arxiv.org/abs/2205.09743)\n- github: [https://github.com/zhangyp15/BEVerse](https://github.com/zhangyp15/BEVerse)\n\n**BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers**\n\n- intro: Nanjing University & Shanghai AI Laboratory & The University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2203.17270](https://arxiv.org/abs/2203.17270)\n- github: [https://github.com/zhiqi-li/BEVFormer](https://github.com/zhiqi-li/BEVFormer)\n\n**HFT: Lifting Perspective Representations via Hybrid Feature Transformation**\n\n- intro: Institute of Automation, Chinese Academy of Sciences & PhiGent Robotics\n- arxiv: [https://arxiv.org/abs/2204.05068](https://arxiv.org/abs/2204.05068)\n- github: [https://github.com/JiayuZou2020/HFT](https://github.com/JiayuZou2020/HFT)\n\n**M^2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation**\n\n- project page: [https://xieenze.github.io/projects/m2bev/](https://xieenze.github.io/projects/m2bev/)\n- arxiv: [https://arxiv.org/abs/2204.05088](https://arxiv.org/abs/2204.05088)\n\n**BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation**\n\n- project page: [https://bevfusion.mit.edu/](https://bevfusion.mit.edu/)\n- arxiv: [https://arxiv.org/abs/2205.13542](https://arxiv.org/abs/2205.13542)\n- github: [https://github.com/mit-han-lab/bevfusion](https://github.com/mit-han-lab/bevfusion)\n\n**BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework**\n\n- intro: Peking University & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2205.13790](https://arxiv.org/abs/2205.13790)\n- github: [https://github.com/ADLab-AutoDrive/BEVFusion](https://github.com/ADLab-AutoDrive/BEVFusion)\n\n**A Simple Baseline for BEV Perception Without LiDAR**\n\n- intro: Carnegie Mellon University & Toyota Research Institute\n- project page: [http://www.cs.cmu.edu/~aharley/bev/](http://www.cs.cmu.edu/~aharley/bev/)\n- arxiv: [https://arxiv.org/abs/2206.07959](https://arxiv.org/abs/2206.07959)\n\n**BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object Detection**\n\n- intro: Megvii Inc. (Face++) & Huazhong University of Science and Technology & Xi’an Jiaotong University\n- arxiv: [https://arxiv.org/abs/2206.10092](https://arxiv.org/abs/2206.10092)\n\n**PolarFormer: Multi-camera 3D Object Detection with Polar Transformers**\n\n- intro: 1Fudan University & CASIA & Alibaba DAMO Academy & University of Surrey\n- arxiv: [https://arxiv.org/abs/2206.15398](https://arxiv.org/abs/2206.15398)\n- github: [https://github.com/fudan-zvg/PolarFormer](https://github.com/fudan-zvg/PolarFormer)\n\n**ORA3D: Overlap Region Aware Multi-view 3D Object Detection**\n\n- intro: Korea University & KAIST & Hyundai Motor Company R&D Division\n- arxiv: [https://arxiv.org/abs/2207.00865](https://arxiv.org/abs/2207.00865)\n\n# HD Map Construction\n\n**HDMapNet: An Online HD Map Construction and Evaluation Framework**\n\n- intro: ICRA 2022\n- intro: Tsinghua University & MIT & Li Auto\n- project page: [https://tsinghua-mars-lab.github.io/HDMapNet/](https://tsinghua-mars-lab.github.io/HDMapNet/)\n- arxiv: [https://arxiv.org/abs/2107.06307](https://arxiv.org/abs/2107.06307)\n- github: [https://github.com/Tsinghua-MARS-Lab/HDMapNet](https://github.com/Tsinghua-MARS-Lab/HDMapNet)\n\n**VectorMapNet: End-to-end Vectorized HD Map Learning**\n\n- intro: Tsinghua University & MIT & Li Auto\n- arxiv: [https://arxiv.org/abs/2206.08920](https://arxiv.org/abs/2206.08920)\n\n**UniFormer: Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View**\n\n- intro: Zhejiang University & DJI & Shanghai AI Lab\n- arxiv: [https://arxiv.org/abs/2207.08536](https://arxiv.org/abs/2207.08536)\n\n# Semantic Segmentation\n\n**LaRa: Latents and Rays for Multi-Camera Bird's-Eye-View Semantic Segmentation**\n\n- intro: Valeo.ai & Inria\n- arxiv: [https://arxiv.org/abs/2206.13294](https://arxiv.org/abs/2206.13294)\n\n**CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers**\n\n- intro: University of California, Los Angeles & University of Texas at Austin & University of California\n- arxiv: [https://arxiv.org/abs/2207.02202](https://arxiv.org/abs/2207.02202)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","title":"Papers, Blogs and Websites"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: Papers, Blogs and Websites\ndate: 2015-09-27\n---\n\n# Blogs\n\n**The Death of Greatness**\n\n[http://kongming.net/novel/writings/blanning/death_of_greatness.php](http://kongming.net/novel/writings/blanning/death_of_greatness.php)\n\n# Websites\n\n**Bodies left on Everest**\n\n- website: [http://imgur.com/gallery/4UJj0](http://imgur.com/gallery/4UJj0)\n\n**GeoS Demo — An End to End Geometry Problem Solver**\n\n![](/assets/leisure/blogs-and-websites/GeoS_Demo.jpg)\n\n[http://geometry.allenai.org/demo/](http://geometry.allenai.org/demo/)\n\n**Intellectual work by tech giants: Bill Gates, Steve Jobs, Google/Baidu founders ...**\n\n- including Bill Gates's paper, Steve Jobs's patent, and Lei Jun's asm code. Funny stuffs.\n\n[https://nfil.es/U9QDwH/](https://nfil.es/U9QDwH/)\n\n**Mapping Militants Organizations**\n\n[http://web.stanford.edu/group/mappingmilitants/cgi-bin/](http://web.stanford.edu/group/mappingmilitants/cgi-bin/)\n\n**Stefan Bleekrode: Cityscapes**\n\n![](/assets/leisure/blogs-and-websites/Stefan_Bleekrode_Cityscapes.jpg)\n\n[http://stefanbleekrode.exto.org/kunstwerken/111013_Cityscapes.html#.VnQiEVUrLIW](http://stefanbleekrode.exto.org/kunstwerken/111013_Cityscapes.html#.VnQiEVUrLIW)\n\n**CreativeApplications.Net**\n\n[http://www.creativeapplications.net/](http://www.creativeapplications.net/)\n\n**Battkes: Public Interface**\n\n![](/assets/leisure/blogs-and-websites/battles_nodegoat.jpg)\n\n[http://battles.nodegoat.net/viewer.p/23/385/scenario/1/geo/fullscreen](http://battles.nodegoat.net/viewer.p/23/385/scenario/1/geo/fullscreen)\n\n**Panama Papers: The Power Players**\n\n![](/assets/leisure/blogs-and-websites/panama_papers.jpg)\n\n[https://projects.icij.org/panama-papers/power-players/index.html](https://projects.icij.org/panama-papers/power-players/index.html)\n\n# APPs\n\n**Pikazo: Remix your photos into incredible artwork**\n\n[http://www.pikazoapp.com/](http://www.pikazoapp.com/)\n\n**Music Understanding Demo**\n\n[http://demo.niland.io/](http://demo.niland.io/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","title":"Games"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: game\ntitle: Games\ndate: 2015-12-13\n---\n\n**Super Mario In HTML5**\n\n[http://html5-mario.memolas.es/mario.php](http://html5-mario.memolas.es/mario.php)\n\n**Mario Games**\n\n[http://www.pcmariogames.com/super-mario-crossover.php](http://www.pcmariogames.com/super-mario-crossover.php)\n\n**JamStik+**\n\n![](https://cdn.shopify.com/s/files/1/0348/1157/t/60/assets/isolatedIMG.png?11984439211291042956)\n\n[http://jamstik.com/](http://jamstik.com/)\n\n**The Hardest Computer Game of All Time**\n\n![](http://www.slate.com/content/dam/slate/articles/technology/bitwise/2014/01/140124_BIT_RO-01-TitleScreen.png.CROP.promo-mediumlarge.png)\n\n- blog: [http://www.slate.com/articles/technology/bitwise/2014/01/robot_odyssey_the_hardest_computer_game_of_all_time.html](http://www.slate.com/articles/technology/bitwise/2014/01/robot_odyssey_the_hardest_computer_game_of_all_time.html)\n- blog(zh): [http://blog.jobbole.com/59347/](http://blog.jobbole.com/59347/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","title":"All About Enya"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: All About Enya\ndate: 2015-12-21\n---\n\n![](/assets/leisure/enya_a_day_withou_rain_cover.jpg)\n\n<embed src=\"http://www.xiami.com/widget/0_1769247445/singlePlayer.swf\" type=\"application/x-shockwave-flash\" width=\"257\" height=\"33\" wmode=\"transparent\"></embed>\n\n# Dark Sky Island\n\n![](/assets/leisure/Enya/Enya-Dark-Sky-Island-426x426.jpg)\n\n**CD review: Enya, ‘Dark Sky Island’**\n\n[http://www.star-telegram.com/entertainment/arts-culture/article45220161.html](http://www.star-telegram.com/entertainment/arts-culture/article45220161.html)\n\n**Enya’s ‘Dark Sky Island’: Review Revue**\n\n“It’s amazing how timeless Enya’s sound is: Above a whimsical mix of Celtic and New Age-revival, \nher ethereal vocals float through the songs like clouds through the skies.”\n\n[http://www.idolator.com/7615659/enya-dark-sky-island-review](http://www.idolator.com/7615659/enya-dark-sky-island-review)\n\n# Website\n\n[enya.com](enya.com)\n\n[http://enya.sk/](http://enya.sk/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","title":"Coldplay"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: Coldplay\ndate: 2015-12-21\n---\n\n**Life In Technicolor**\n\n**Viva La Vida**"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","title":"Green Day"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: Green Day\ndate: 2015-12-21\n---\n\n![](/assets/leisure/greenday/green_day___viva_la_gloria_by_viictimofauthority.jpg)\n![](/assets/leisure/greenday/viva_la_gloria_by_fjordluv-d37ypgw.png)\n\n**Viva La Gloria!**\n\n![](/assets/leisure/greenday/viva_la_gloria_by_paco608.jpg)\n\n**Jesus Of Suburbia**\n\n![](/assets/leisure/greenday/the_jesus_of_suburbia_by_vampgeepy.jpg)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","title":"Muse! Muse!"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: Muse! Muse!\ndate: 2015-12-21\n---\n\n![Dominic Howard, Chris Wolstenholme and Matthew Bellamy](/assets/leisure/muse/Muse.JPG)\n\n![Matthew](/assets/leisure/muse/Matthew.jpg)\n\n[http://muse.mu/](http://muse.mu/)\n\n[http://muse.mu/music-video/discog/album.htm](http://muse.mu/music-video/discog/album.htm)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","title":"Coldplay"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: Coldplay\ndate: 2015-12-21\n---\n\n**Sacrament Of Wilderness**\n\n**The Escapist**\n\n**Dark Chest of Wonders**"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","title":"Oasis"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: Oasis\ndate: 2015-12-21\n---\n\n![](/assets/leisure/Oasis/Oasis-Knebworth-Park.jpg)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","title":"Welcome To The Black Parade"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: Welcome To The Black Parade\ndate: 2015-12-21\n---\n\n![](/assets/leisure/MCR/My_Chemical_Romance2.jpg)\n\n**Welcome To The Black Parade**\n\n![](/assets/leisure/MCR/welcome_to_the_black_parade__by_dragon_flies-d3k8ii3.jpg)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","title":"Paintings By J.M."},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: leisure\ntitle: Paintings By J.M.\ndate: 2016-03-08\n---\n\nPaintings from one of my best girl friends~\n\n![](/assets/leisure/PaintingsByJM/44467935.jpg)\n\n![](/assets/leisure/PaintingsByJM/1154354131.jpg)\n\n![](/assets/leisure/PaintingsByJM/1515663317.jpg)\n\n![](/assets/leisure/PaintingsByJM/1547326978.jpg)\n\n![](/assets/leisure/PaintingsByJM/2139909562.jpg)\n\n![](/assets/leisure/PaintingsByJM/IMG_0193.JPG)\n\n![](/assets/leisure/PaintingsByJM/IMG_0201.JPG)\n\n![](/assets/leisure/PaintingsByJM/IMG_0202.JPG)\n\n![](/assets/leisure/PaintingsByJM/IMG_0206.JPG)\n\n![](/assets/leisure/PaintingsByJM/IMG_0664.JPG)\n\n![](/assets/leisure/PaintingsByJM/IMG_1057.JPG)\n\n![](/assets/leisure/PaintingsByJM/IMG_1550.JPG)\n\n![](/assets/leisure/PaintingsByJM/IMG_1724.JPG)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","title":"Bayesian Methods"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Bayesian Methods\ndate: 2015-08-27\n---\n\n# Bayesian Learning\n\n**Auto-Encoding Variational Bayes**\n\n- paper: [http://arxiv.org/abs/1312.6114](http://arxiv.org/abs/1312.6114)\n- github: [https://github.com/stitchfix/fauxtograph](https://github.com/stitchfix/fauxtograph)\n- github: [https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py](https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py)\n\n**Practical Bayesian Optimization of Machine Learning Algorithms**\n\n- paper: [http://arxiv.org/abs/1206.2944v2](http://arxiv.org/abs/1206.2944v2)\n- github: [https://github.com/JasperSnoek/spearmint](https://github.com/JasperSnoek/spearmint)\n- github: [https://github.com/HIPS/Spearmint](https://github.com/HIPS/Spearmint)\n\n**Bayesian Face Revisited: A Joint Formulation**\n\n- intro: ECCV 2012\n- paper: [http://research.microsoft.com/en-us/um/people/jiansun/papers/ECCV12_BayesianFace.pdf](http://research.microsoft.com/en-us/um/people/jiansun/papers/ECCV12_BayesianFace.pdf)\n- github: [https://github.com/MaoXu/Joint_Bayesian/](https://github.com/MaoXu/Joint_Bayesian/)\n- github: [https://github.com/cyh24/Joint-Bayesian](https://github.com/cyh24/Joint-Bayesian)\n\n**UAI 2015 Amsterdam Tutorial: Optimal Algorithms for Learning Bayesian Network Structures**\n\n- video: [https://www.youtube.com/watch?v=__JD3ptBR-w&hd=1](https://www.youtube.com/watch?v=__JD3ptBR-w&hd=1)\n- video: [http://pan.baidu.com/s/1ntEf4lr](http://pan.baidu.com/s/1ntEf4lr)\n- slides: [http://auai.org/uai2015/proceedings/slides/UAI2015_LearningBN_PartI.pdf](http://auai.org/uai2015/proceedings/slides/UAI2015_LearningBN_PartI.pdf)\n- slides: [http://auai.org/uai2015/proceedings/slides/UAI2015_LearningBN_PartII.pdf](http://auai.org/uai2015/proceedings/slides/UAI2015_LearningBN_PartII.pdf)\n\n**Bayesian Learning**\n\n[https://github.com/ReactiveCJ/BayesianLearning](https://github.com/ReactiveCJ/BayesianLearning)\n\n**Bayesian Methods of Parameter Estimation**\n\n- paper: [http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV0809/eshky.pdf](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV0809/eshky.pdf)\n\n**A Tutorial on Bayesian Belief Networks**\n\n- intro: 2001\n- paper: [http://dspace.dsto.defence.gov.au/dspace/bitstream/1947/3537/1/DSTO-TN-0403.pdf](http://dspace.dsto.defence.gov.au/dspace/bitstream/1947/3537/1/DSTO-TN-0403.pdf)\n- paper: [http://pan.baidu.com/s/1eQNtUxW](http://pan.baidu.com/s/1eQNtUxW)\n\n**A Tutorial on Learning With Bayesian Networks**\n\n- page: [http://research.microsoft.com/apps/pubs/default.aspx?id=69588](http://research.microsoft.com/apps/pubs/default.aspx?id=69588)\n- paper: [http://research.microsoft.com/pubs/69588/tr-95-06.pdf](http://research.microsoft.com/pubs/69588/tr-95-06.pdf)\n\n**An Introduction to Bayesian Networks: Concepts and Learning from Data**\n\n- slides: [http://nlp.postech.ac.kr/Course/CS704/LectureNotes/BayesianNetworks.pdf](http://nlp.postech.ac.kr/Course/CS704/LectureNotes/BayesianNetworks.pdf)\n\n**Taking the Human Out of the Loop: A Review of Bayesian Optimization**\n\n- paper: [http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352306](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352306)\n\n**Bayesian Optimization for Machine Learning : A Practical Guidebook**\n\n- intro: SigOpt\n- arxiv: [https://arxiv.org/abs/1612.04858](https://arxiv.org/abs/1612.04858)\n- homepage: [https://sigopt.com/](https://sigopt.com/)\n- github: [https://github.com/sigopt](https://github.com/sigopt)\n\n# Probabilistic Programming\n\n**Probabilistic Programming in Python**\n\n- slides: [http://www.slideshare.net/PeadarCoyle/probabilistic-programming-in-python?qid=5982ead9-5606-4ef8-9faa-5d1782f34702&v=qf1](http://www.slideshare.net/PeadarCoyle/probabilistic-programming-in-python?qid=5982ead9-5606-4ef8-9faa-5d1782f34702&v=qf1)\n- slides: [http://pan.baidu.com/s/1gdP0O8N](http://pan.baidu.com/s/1gdP0O8N)\n\n**Reading List for MIT Probabilistic Computing Project**\n\n[http://probcomp.csail.mit.edu/readings/](http://probcomp.csail.mit.edu/readings/)\n\n**Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation**\n\n- paper: [http://web.stanford.edu/~ngoodman/papers/lightweight-mcmc-aistats2011.pdf](http://web.stanford.edu/~ngoodman/papers/lightweight-mcmc-aistats2011.pdf)\n- talk: [http://videolectures.net/aistats2011_wingate_lightweight/](http://videolectures.net/aistats2011_wingate_lightweight/)\n- github: (\"probabilistic-js\"): [https://github.com/dritchie/probabilistic-js](https://github.com/dritchie/probabilistic-js)\n\n**Introduction to Probabilistic Programming**\n\n![](http://austinrochford.com/resources/talks/img/probabilistic_programming.png)\n\n- blog: [http://austinrochford.com/resources/talks/dataphilly-jul2016.slides.html](http://austinrochford.com/resources/talks/dataphilly-jul2016.slides.html)\n\n**webppl: Probabilistic programming for the web**\n\n- intro: webppl is a small but feature-rich probabilistic programming language embedded in Javascript.\n- homepage: [http://webppl.org/](http://webppl.org/)\n- github: [https://github.com/probmods/webppl](https://github.com/probmods/webppl)\n\n**Probabilistic Data Analysis with Probabilistic Programming**\n\n- intro: MIT\n- arxiv: [http://arxiv.org/abs/1608.05347](http://arxiv.org/abs/1608.05347)\n\n**The Design and Implementation of Probabilistic Programming Languages**\n\n- book: [http://dippl.org/](http://dippl.org/)\n\n**Model-Based Machine Learning and Probabilistic Programming in RStan**\n\n- blog: [https://blog.dominodatalab.com/video-model-based-machine-learning-webcast/](https://blog.dominodatalab.com/video-model-based-machine-learning-webcast/)\n\n**Deep Probabilistic Programming**\n\n- intro: Columbia University & Adobe Research & Google Research & Google Brain\n- arxiv: [https://arxiv.org/abs/1701.03757](https://arxiv.org/abs/1701.03757)\n- homepage: [http://edwardlib.org/](http://edwardlib.org/)\n- zoo: [http://edwardlib.org/zoo](http://edwardlib.org/zoo)\n- code: [http://edwardlib.org/iclr2017](http://edwardlib.org/iclr2017)\n\n**The Algorithms Behind Probabilistic Programming**\n\n[http://blog.fastforwardlabs.com/2017/01/30/the-algorithms-behind-probabilistic-programming.html](http://blog.fastforwardlabs.com/2017/01/30/the-algorithms-behind-probabilistic-programming.html)\n\n**Edward: A library for probabilistic modeling, inference, and criticism.**\n\n- intro: A library for probabilistic modeling, inference, and criticism. Deep generative models, variational inference. Runs on TensorFlow.\n- homepage: [http://edwardlib.org/](http://edwardlib.org/)\n- github: [https://github.com/blei-lab/edward](https://github.com/blei-lab/edward)\n\n# Books\n\n**Probabilistic Programming & Bayesian Methods for Hackers**\n\n- homepage: [http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n- github: [https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n\n# Blogs\n\n**Let’s Learn: Probabilistic Programming**\n\n- part 1: [https://medium.com/@Algomancer/lets-learn-about-probabilistic-programming-part-1-7988d6501e53#.dzjdiexu5](https://medium.com/@Algomancer/lets-learn-about-probabilistic-programming-part-1-7988d6501e53#.dzjdiexu5)\n- part 2: [https://medium.com/@Algomancer/let-s-learn-probabilistic-programming-part-2-c208f79f1dc1#.6hq2zzb7l](https://medium.com/@Algomancer/let-s-learn-probabilistic-programming-part-2-c208f79f1dc1#.6hq2zzb7l)\n\n**Bayesian Optimization of Machine Learning Models**\n\n[http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html](http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html)\n\n**Probabilistic Programming - Part 1**\n\n[http://javaagile.blogspot.kr/2016/09/probabilistic-programming-part-1.html](http://javaagile.blogspot.kr/2016/09/probabilistic-programming-part-1.html)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","title":"Clustering Algorithms Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Clustering Algorithms Resources\ndate: 2015-08-27\n---\n\n# K-means\n\n**Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup**\n\n- paper: [http://jmlr.org/proceedings/papers/v37/ding15.html](http://jmlr.org/proceedings/papers/v37/ding15.html)\n- github: [https://github.com/src-d/kmcuda](https://github.com/src-d/kmcuda)\n- code: [http://research.csc.ncsu.edu/nc-caps/yykmeans.tar.bz2](http://research.csc.ncsu.edu/nc-caps/yykmeans.tar.bz2)\n\n**Semi-supervised K-means++**\n\n- arxiv: [http://arxiv.org/abs/1602.00360](http://arxiv.org/abs/1602.00360)\n\n**k-Means Clustering Is Matrix Factorization**\n\n- arxiv: [http://arxiv.org/abs/1512.07548](http://arxiv.org/abs/1512.07548)\n- note: [http://blog.csdn.net/cyh_24/article/details/50408884](http://blog.csdn.net/cyh_24/article/details/50408884)\n\n**An efficient K-means algorithm for Massive Data**\n\n- arxiv: [http://arxiv.org/abs/1605.02989](http://arxiv.org/abs/1605.02989)\n\n**Boost K-Means**\n\n- arxiv: [https://arxiv.org/abs/1610.02483](https://arxiv.org/abs/1610.02483)\n\n**Compressive K-means**\n\n- arxiv: [https://arxiv.org/abs/1610.08738](https://arxiv.org/abs/1610.08738)\n\n**Convergence rate of stochastic k-means**\n\n- arxiv: [https://arxiv.org/abs/1610.04900](https://arxiv.org/abs/1610.04900)\n- arxiv: [https://arxiv.org/abs/1611.05132](https://arxiv.org/abs/1611.05132)\n\n**Fast and Provably Good Seedings for k-Means using k-MC^2 and AFK-MC^2**\n\n- github: [https://github.com/obachem/kmc2](https://github.com/obachem/kmc2)\n- blog: [https://www.infoq.com/news/2016/12/AFK-MC2-boosts-kMeans-seeding](https://www.infoq.com/news/2016/12/AFK-MC2-boosts-kMeans-seeding)\n\n**An efficient K -means clustering algorithm for massive data**\n\n- keywords: Clustering, massive data, parallelization, unsupervised learning, K-means, K-means++, Mini-batch\n- arxiv: [https://arxiv.org/abs/1801.02949](https://arxiv.org/abs/1801.02949)\n\n# Stream Clustering\n\n# Neural Network-based Clustering\n\n# Spectral Clustering\n\n**On Spectral Clustering: Analysis and an algorithm**\n\n- intro: NIPS 2001. Andrew Ng\n- paper: [https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf](https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf)\n- paper: [http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf](http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf)\n\n# Hierarchical Clustering\n\n# Online Clustering\n\n# Papers\n\n**On Clustering Validation Techniques (2001)**\n\n- intro: \"This paper introduces the fundamental concepts of clustering while \nit surveys the widely known clustering algorithms in a comparative way\"\n- paper: [http://web.itu.edu.tr/sgunduz/courses/verimaden/paper/validity_survey.pdf](http://web.itu.edu.tr/sgunduz/courses/verimaden/paper/validity_survey.pdf)\n\n**Stream Clustering**\n\n- slides: [www.cse.msu.edu/~cse902/S14/ppt/Stream%20Clustering.pptx](www.cse.msu.edu/~cse902/S14/ppt/Stream%20Clustering.pptx)\n\n**Neural network-based clustering using pairwise constraints**\n\n![](https://raw.githubusercontent.com/yenchanghsu/NNclustering/gh-pages/paper_img/concept.png)\n\n- arxiv: [http://arxiv.org/abs/1511.06321](http://arxiv.org/abs/1511.06321)\n- homepage: [http://yenchanghsu.github.io/NNclustering/](http://yenchanghsu.github.io/NNclustering/)\n- github: [https://github.com/yenchanghsu/NNclustering](https://github.com/yenchanghsu/NNclustering)\n\n**PAC-Bayesian Online Clustering**\n\n- arxiv: [http://arxiv.org/abs/1602.00522](http://arxiv.org/abs/1602.00522)\n\n**Compressive Spectral Clustering**\n\n- arxiv: [http://arxiv.org/abs/1602.02018](http://arxiv.org/abs/1602.02018)\n\n**Interactive Bayesian Hierarchical Clustering**\n\n- arxiv: [http://arxiv.org/abs/1602.03258](http://arxiv.org/abs/1602.03258)\n\n**Practical Introduction to Clustering Data**\n\n- arxiv: [http://arxiv.org/abs/1602.05124](http://arxiv.org/abs/1602.05124)\n\n**Rényi divergence minimization based co-regularized multiview clustering**\n\n- paper: [http://rd.springer.com.sci-hub.io/article/10.1007/s10994-016-5543-2?wt_mc=internal.event.1.SEM.ArticleAuthorOnlineFirst](http://rd.springer.com.sci-hub.io/article/10.1007/s10994-016-5543-2?wt_mc=internal.event.1.SEM.ArticleAuthorOnlineFirst)\n\n**Consistent Algorithms for Clustering Time Series**\n\n- paper: [http://jmlr.org/papers/v17/khaleghi16a.html](http://jmlr.org/papers/v17/khaleghi16a.html)\n\n**Hybridization of Expectation-Maximization and K-Means Algorithms for Better Clustering Performance**\n\n- arxiv: [http://arxiv.org/abs/1603.07879](http://arxiv.org/abs/1603.07879)\n\n**mst_clustering: Clustering via Euclidean Minimum Spanning Trees**\n\n- paper: [http://joss.theoj.org/papers/10.21105/joss.00012](http://joss.theoj.org/papers/10.21105/joss.00012)\n- paper: [https://github.com/openjournals/joss-papers/blob/master/joss.00012/10.21105.joss.00012.pdf](https://github.com/openjournals/joss-papers/blob/master/joss.00012/10.21105.joss.00012.pdf)\n- github: [https://github.com/jakevdp/mst_clustering](https://github.com/jakevdp/mst_clustering)\n\n**k2-means for fast and accurate large scale clustering**\n\n- arxiv: [http://arxiv.org/abs/1605.09299](http://arxiv.org/abs/1605.09299)\n\n**Context Aware Nonnegative Matrix Factorization Clustering**\n\n- arxiv: [http://arxiv.org/abs/1609.04628](http://arxiv.org/abs/1609.04628)\n\n**Clustering by fast search and find of density peaks**\n\n[http://science.sciencemag.org/content/344/6191/1492](http://science.sciencemag.org/content/344/6191/1492)\n- slides: [http://conference.mipt.ru/img/conference/material-design-2014/talks/Laio-talk.pdf](http://conference.mipt.ru/img/conference/material-design-2014/talks/Laio-talk.pdf)\n- github: [https://github.com/thomasp85/densityClust](https://github.com/thomasp85/densityClust)\n- github: [https://github.com/GuipengLi/Dcluster](https://github.com/GuipengLi/Dcluster)\n- blog: [http://eric-yuan.me/clustering-fast-search-find-density-peaks/](http://eric-yuan.me/clustering-fast-search-find-density-peaks/)\n\n**Comment on \"Clustering by fast search and find of density peaks\"**\n\n[https://arxiv.org/abs/1501.04267](https://arxiv.org/abs/1501.04267)\n\n# Datasets\n\n**Clustering datasets**\n\n[https://cs.joensuu.fi/sipu/datasets/](https://cs.joensuu.fi/sipu/datasets/)\n\n# Books\n\n**Introduction to Clustering and Unsupervised Learning | PACKT Books**\n\n- intro: 《Machine Learning with R - Second Edition》by Brett Lantz\n- book: [https://www.packtpub.com/books/content/introduction-clustering-and-unsupervised-learning](https://www.packtpub.com/books/content/introduction-clustering-and-unsupervised-learning)\n\n# Blogs\n\n**Finding the K in K-means by Parametric Bootstrap**\n\n- blog: [http://www.win-vector.com/blog/2016/02/finding-the-k-in-k-means-by-parametric-bootstrap/](http://www.win-vector.com/blog/2016/02/finding-the-k-in-k-means-by-parametric-bootstrap/)\n\n**Random walk vectors for clustering**\n\n- part I – similarity between objects: [http://int8.io/random-walk-vectors-for-clustering-part-i-similarity-between-objects/](http://int8.io/random-walk-vectors-for-clustering-part-i-similarity-between-objects/)\n- part II – perspective switch: [http://int8.io/random-walk-vectors-for-clustering-part-ii-perspective-switch/](http://int8.io/random-walk-vectors-for-clustering-part-ii-perspective-switch/)\n- part III: [http://int8.io/random-walk-vectors-for-clustering-iii/](http://int8.io/random-walk-vectors-for-clustering-iii/)\n- final: [http://int8.io/random-walk-vectors-for-clustering-final/](http://int8.io/random-walk-vectors-for-clustering-final/)\n\n**A comparison between PCA and hierarchical clustering**\n\n[http://www.kdnuggets.com/2016/02/qlucore-comparison-pca-hierarchical-clustering.html](http://www.kdnuggets.com/2016/02/qlucore-comparison-pca-hierarchical-clustering.html)\n\n**Visualization of Centroid Movements for K-Means Clustering**\n\n[http://web.cecs.pdx.edu/~lane7/](http://web.cecs.pdx.edu/~lane7/)\n\n**K-Means Clustering on Handwritten Digits**\n\n[http://johnloeber.com/docs/kmeans.html](http://johnloeber.com/docs/kmeans.html)\n\n**Improved Seeding For Clustering With K-Means++ (★★★★★)**\n\n[https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/](https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/)\n\n**Spectral Clustering – How Math is Redefining Decision Making**\n\n[http://www.datasciencecentral.com/profiles/blogs/spectral-clustering-how-math-is-redefining-decision-making?overrideMobileRedirect=1](http://www.datasciencecentral.com/profiles/blogs/spectral-clustering-how-math-is-redefining-decision-making?overrideMobileRedirect=1)\n\n**Visual comparison of machine learning algorithms: Clustering**\n\n[http://haifengl.github.io/smile/index.html#clustering](http://haifengl.github.io/smile/index.html#clustering)\n\n**Clustering Algorithms: From Start To State Of The Art**\n\n[https://www.toptal.com/machine-learning/clustering-algorithms](https://www.toptal.com/machine-learning/clustering-algorithms)\n\n**Hierarchical clustering, using it to invest**\n\n- blog: [http://quantdare.com/2016/06/hierarchical-clustering/](http://quantdare.com/2016/06/hierarchical-clustering/)\n\n**Spectral Clustering: A quick overview**\n\n[https://charlesmartin14.wordpress.com/2012/10/09/spectral-clustering/](https://charlesmartin14.wordpress.com/2012/10/09/spectral-clustering/)\n\n**Why K-Means is not always a good idea**\n\n[https://datasciencemadesimpler.wordpress.com/2016/03/05/why-k-means-is-not-always-a-good-idea/](https://datasciencemadesimpler.wordpress.com/2016/03/05/why-k-means-is-not-always-a-good-idea/)\n\n**High Quality, High Performance Clustering with HDBSCAN | SciPy 2016**\n\n- youtube: [https://www.youtube.com/watch?v=AgPQ76RIi6A&list=PLYx7XA2nY5Gf37zYZMw6OqGFRPjB1jCy6&index=10](https://www.youtube.com/watch?v=AgPQ76RIi6A&list=PLYx7XA2nY5Gf37zYZMw6OqGFRPjB1jCy6&index=10)\n\n# Projects\n\n**MusicMappr: Find patterns in your favorite songs and remix them on the fly!**\n\n- intro: MusicMappr finds chunks of songs that are similar, and clusters them accordingly. \nYou can visualize these clusters and play them back at will.\nThis is for music lovers who are curious about the structures inherent to their favorite songs.\n- github: [https://github.com/fatsmcgee/MusicMappr](https://github.com/fatsmcgee/MusicMappr)\n\n**TfKmeans: A implementation of k-means clustering in TensorFlow**\n\n- github: [http://rasbt.github.io/mlxtend/user_guide/tf_cluster/TfKmeans/](http://rasbt.github.io/mlxtend/user_guide/tf_cluster/TfKmeans/)\n\n**CUDA K-Means Clustering: A CUDA implementation of the k-means clustering algorithm**\n\n- homepage: [http://serban.org/software/kmeans/](http://serban.org/software/kmeans/)\n- github: [https://github.com/serban/kmeans](https://github.com/serban/kmeans)\n\n**kmeans_cuda: CUDA implementation of k-means**\n\n- github: [https://github.com/phvu/kmeans_cuda](https://github.com/phvu/kmeans_cuda)\n\n**K-means in TensorFlow**\n\n- blog: [http://nxn.se/post/145634722580/k-means-in-tensorflow](http://nxn.se/post/145634722580/k-means-in-tensorflow)\n- gist: [https://gist.github.com/vals/a01a37b14c4918df7937b30d43327837](https://gist.github.com/vals/a01a37b14c4918df7937b30d43327837)\n\n**VAE-Clustering**\n\n- intro: Unsupervised clustering with (Gaussian mixture) VAEs\n- github: [https://github.com/RuiShu/vae-clustering](https://github.com/RuiShu/vae-clustering)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","title":"Competitions"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Competitions\ndate: 2015-08-27\n---\n\n**10 Steps to Success in Kaggle Data Science Competitions**\n\n[http://www.kdnuggets.com/2015/03/10-steps-success-kaggle-data-science-competitions.html](http://www.kdnuggets.com/2015/03/10-steps-success-kaggle-data-science-competitions.html)\n\n**Classifying plankton with deep neural networks**\n\n- blog: [http://benanne.github.io/2015/03/17/plankton.html](http://benanne.github.io/2015/03/17/plankton.html)\n- github: [https://github.com/benanne/kaggle-ndsb](https://github.com/benanne/kaggle-ndsb)\n\n**Keras Deep Learning Tutorial for Kaggle 2nd Annual Data Science Bowl**\n\n- github: [https://github.com/jocicmarko/kaggle-dsb2-keras](https://github.com/jocicmarko/kaggle-dsb2-keras)\n\n**Kaggle_CrowdFlower: 1st Place Solution for Search Results Relevance Competition on Kaggle**\n\n![](https://raw.githubusercontent.com/ChenglongChen/Kaggle_CrowdFlower/master/Doc/FlowChart.jpg)\n\n- details: [https://www.kaggle.com/c/crowdflower-search-relevance](https://www.kaggle.com/c/crowdflower-search-relevance)\n- github: [https://github.com/ChenglongChen/Kaggle_CrowdFlower](https://github.com/ChenglongChen/Kaggle_CrowdFlower)\n\n**Machine learning best practices we've learned from hundreds of competitions (Kaggle: Ben Hamner)**\n\n- youtube: [https://www.youtube.com/watch?v=9Zag7uhjdYo&hd=1](https://www.youtube.com/watch?v=9Zag7uhjdYo&hd=1)\n- baidu-pan: [http://pan.baidu.com/s/1pJLrICN](http://pan.baidu.com/s/1pJLrICN)\n\n**kaggle-lmgpip: Top15 Solution for Kaggle-Competition \"Liberty Mutual Group: Property Inspection Prediction\"**\n\n- github: [https://github.com/Far0n/kaggle-lmgpip](https://github.com/Far0n/kaggle-lmgpip)\n\n**Kaggle_HomeDepot: Turing Test's Solution for Home Depot Product Search Relevance Competition on Kaggle**\n\n- github: [https://github.com/ChenglongChen/Kaggle_HomeDepot](https://github.com/ChenglongChen/Kaggle_HomeDepot)\n\n**How to get into the top 15 of a Kaggle competition using Python**\n\n- blog: [https://www.dataquest.io/blog/kaggle-tutorial/](https://www.dataquest.io/blog/kaggle-tutorial/)\n\n**Data Science: A Kaggle Walkthrough**\n\n- part1: [http://brettromero.com/wordpress/data-science-a-kaggle-walkthrough-introduction/](http://brettromero.com/wordpress/data-science-a-kaggle-walkthrough-introduction/)\n- part2: [http://brettromero.com/wordpress/data-science-a-kaggle-walkthrough-understanding-the-data/](http://brettromero.com/wordpress/data-science-a-kaggle-walkthrough-understanding-the-data/)\n- part3: [http://brettromero.com/wordpress/data-science-kaggle-walkthrough-cleaning-data/](http://brettromero.com/wordpress/data-science-kaggle-walkthrough-cleaning-data/)\n- part4: [http://brettromero.com/wordpress/data-science-kaggle-walkthrough-data-transformation-feature-extraction/](http://brettromero.com/wordpress/data-science-kaggle-walkthrough-data-transformation-feature-extraction/)\n- part5: [http://brettromero.com/wordpress/data-science-kaggle-walkthrough-adding-new-data/](http://brettromero.com/wordpress/data-science-kaggle-walkthrough-adding-new-data/)\n- part6: [http://brettromero.com/wordpress/data-science-kaggle-walkthrough-creating-model/](http://brettromero.com/wordpress/data-science-kaggle-walkthrough-creating-model/) \n\n**My Standard Work for every new competition**\n\n[https://www.kaggle.com/forums/f/15/kaggle-forum/t/19959/my-standard-work-for-every-new-competition/113888](https://www.kaggle.com/forums/f/15/kaggle-forum/t/19959/my-standard-work-for-every-new-competition/113888)\n\n**Step by step Kaggle competition tutorial**\n\n[https://datanice.wordpress.com/2016/04/10/step-by-step-kaggle-competition-tutorial/](https://datanice.wordpress.com/2016/04/10/step-by-step-kaggle-competition-tutorial/)\n\n**Winton Kaggle Competition**\n\n[http://blog.nycdatascience.com/students-work/winton-kaggle-competition/](http://blog.nycdatascience.com/students-work/winton-kaggle-competition/)\n\n**Kaggle Master, data scientist, & author: An interview with Luca Massaron**\n\n- blog: [http://blog.kaggle.com/2016/07/14/kaggle-master-data-scientist-author-an-interview-with-luca-massaron/](http://blog.kaggle.com/2016/07/14/kaggle-master-data-scientist-author-an-interview-with-luca-massaron/)\n\n**Recognizing and Localizing Endangered Right Whales with Extremely Deep Neural Networks (2nd place solution to the Kaggle Right Whale challenge)**\n\n![](http://felixlaumon.github.io/assets/kaggle-right-whale/aligner_localization_approach.png)\n\n- blog: [http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html](http://felixlaumon.github.io/2015/01/08/kaggle-right-whale.html)\n- github: [https://github.com/felixlaumon/kaggle-right-whale](https://github.com/felixlaumon/kaggle-right-whale)\n\n**Kaggle Ensembling Guide**\n\n- blog: [http://mlwave.com/kaggle-ensembling-guide/](http://mlwave.com/kaggle-ensembling-guide/)\n- mirror: [http://pan.baidu.com/s/1jGGgvEQ](http://pan.baidu.com/s/1jGGgvEQ)\n- github: [https://github.com/MLWave/Kaggle-Ensemble-Guide](https://github.com/MLWave/Kaggle-Ensemble-Guide)\n\n**Approaching (Almost) Any Machine Learning Problem | Abhishek Thakur**\n\n![](http://blog.kaggle.com/wp-content/uploads/2016/07/abhishek_2.png)\n\n- blog: [http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)\n\n**Facebook V: Predicting Check Ins: I won the Kaggle competition!**\n\n- blog: [https://ttvand.github.io/Winning-approach-of-the-Facebook-V-Kaggle-competition/](https://ttvand.github.io/Winning-approach-of-the-Facebook-V-Kaggle-competition/)\n- github: [https://github.com/ttvand/Facebook-V](https://github.com/ttvand/Facebook-V)\n\n**Entity Embeddings of Categorical Variables**\n\n- intro: 3rd place solution in Kaggle competition\n- arxiv: [http://arxiv.org/abs/1604.06737](http://arxiv.org/abs/1604.06737)\n- github: [https://github.com/entron/entity-embedding-rossmann](https://github.com/entron/entity-embedding-rossmann)\n\n**Kaggle First Steps With Julia (Chars74k): First Place using Convolutional Neural Networks**\n\n![](http://ankivil.com/wp-content/uploads/2016/09/Kaggle_FirstStepsJulia_Cover-1024x576.png)\n\n[http://ankivil.com/kaggle-first-steps-with-julia-chars74k-first-place-using-convolutional-neural-networks/](http://ankivil.com/kaggle-first-steps-with-julia-chars74k-first-place-using-convolutional-neural-networks/)\n\n**Winning Kaggle Competitions**\n\n- slides: [http://www.slideshare.net/HJvanVeen/kaggle-presentation](http://www.slideshare.net/HJvanVeen/kaggle-presentation)\n- mirror: [https://pan.baidu.com/s/1bpgee55](https://pan.baidu.com/s/1bpgee55)\n\n**How should a beginner get started on Kaggle?**\n\n- quora: [https://www.quora.com/How-should-a-beginner-get-started-on-Kaggle](https://www.quora.com/How-should-a-beginner-get-started-on-Kaggle)\n\n**Most popular kaggle competition solutions**\n\n[http://dataaspirant.com/2016/10/06/most-popular-kaggle-competition-solutions/](http://dataaspirant.com/2016/10/06/most-popular-kaggle-competition-solutions/)\n\n**Code for Kaggle Competitions**\n\n- github: [https://github.com/tdeboissiere/Kaggle](https://github.com/tdeboissiere/Kaggle)\n\n**Kaggle competition Painter by Numbers: Winning solution for the Kaggle competition Painter by Numbers**\n\n![](https://raw.githubusercontent.com/inejc/painters/master/misc/front.jpg)\n\n- github: [https://github.com/inejc/painters](https://github.com/inejc/painters)\n\n**Using Keras+TensorFlow to solve NCFM-Leadboard Top 5%**\n\n- github: [https://github.com/pengpaiSH/Kaggle_NCFM](https://github.com/pengpaiSH/Kaggle_NCFM)\n\n**2nd Place Solution of the Kaggle Competition - Santander Product Recommendation**\n\n- github: [https://github.com/ttvand/Santander-Product-Recommendation](https://github.com/ttvand/Santander-Product-Recommendation)\n\n**Deep Learning Tutorial for Kaggle Ultrasound Nerve Segmentation competition, using Keras**\n\n- intro: U-Net\n- github: [https://github.com/jocicmarko/ultrasound-nerve-segmentation](https://github.com/jocicmarko/ultrasound-nerve-segmentation)\n\n**Nexar’s Deep Learning Challenge: the winners reveal their secrets**\n\n[https://blog.getnexar.com/nexars-deep-learning-challenge-the-winners-reveal-their-secrets-e80c24147f2d#.rdu2pdv7h](https://blog.getnexar.com/nexars-deep-learning-challenge-the-winners-reveal-their-secrets-e80c24147f2d#.rdu2pdv7h)\n\n**Nature Conservancy Fish Classification**\n\n- github: [https://github.com/rdcolema/nc-fish-classification](https://github.com/rdcolema/nc-fish-classification)\n\n**Leaf Classification Competition: 1st Place Winner's Interview, Ivan Sosnovik**\n\n[http://blog.kaggle.com/2017/03/24/leaf-classification-competition-1st-place-winners-interview-ivan-sosnovik/](http://blog.kaggle.com/2017/03/24/leaf-classification-competition-1st-place-winners-interview-ivan-sosnovik/)\n\n**猫狗大战: Dogs vs. Cats**\n\n[https://github.com/ypwhs/dogs_vs_cats](https://github.com/ypwhs/dogs_vs_cats)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","title":"Dimensionality Reduction Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Dimensionality Reduction Resources\ndate: 2015-08-27\n---\n\n# PCA\n\n**A Tutorial on Principal Component Analysis**\n\n- arxiv: [http://arxiv.org/abs/1404.1100](http://arxiv.org/abs/1404.1100)\n\n**Compressive PCA on Graphs**\n\n- arxiv: [http://arxiv.org/abs/1602.02070](http://arxiv.org/abs/1602.02070)\n\n**Robust Principal Component Analysis on Graphs**\n\n![](https://lts2.epfl.ch/blog/nauman/wp-content/uploads/sites/12/2015/01/Slide11.jpg)\n\n- arxiv: [http://arxiv.org/abs/1504.06151](http://arxiv.org/abs/1504.06151)\n- project: [https://lts2.epfl.ch/blog/nauman/recent-projects/](https://lts2.epfl.ch/blog/nauman/recent-projects/)\n- code: [http://pan.baidu.com/s/1gepgIUv](http://pan.baidu.com/s/1gepgIUv)\n\n**Fast Randomized PCA/SVD**\n\n- blog: [https://research.facebook.com/blog/294071574113354/fast-randomized-svd/](https://research.facebook.com/blog/294071574113354/fast-randomized-svd/)\n- baidu-pan: [http://pan.baidu.com/s/1nt8I8qt](http://pan.baidu.com/s/1nt8I8qt)\n- github: [https://github.com/facebook/fbpca](https://github.com/facebook/fbpca)\n\n**Improper applications of Principal Component Analysis on multimodal data**\n\n- blog: [http://nxn.se/post/117471851320/improper-applications-of-principal-component](http://nxn.se/post/117471851320/improper-applications-of-principal-component)\n\n**Principal Component Analysis**\n\n- blog: [http://setosa.io/ev/principal-component-analysis/](http://setosa.io/ev/principal-component-analysis/)\n- homepage(\"Explained Visually\"): [http://setosa.io/ev/](http://setosa.io/ev/)\n\n**Fast Algorithms for Robust PCA via Gradient Descent**\n\n- arxiv: [http://arxiv.org/abs/1605.07784](http://arxiv.org/abs/1605.07784)\n- code: [https://people.orie.cornell.edu/yudong.chen/rpca_gd/RPCA_GD.zip](https://people.orie.cornell.edu/yudong.chen/rpca_gd/RPCA_GD.zip)\n\n**Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis**\n\n- arxiv: [https://arxiv.org/abs/1609.04789](https://arxiv.org/abs/1609.04789)\n\n**A Fast Factorization-based Approach to Robust PCA**\n\n- arxiv: [https://arxiv.org/abs/1609.08677](https://arxiv.org/abs/1609.08677)\n\n**PCA Tutorial**\n\n- blog: [http://www.danvatterott.com/blog/2016/11/06/pca-tutorial/](http://www.danvatterott.com/blog/2016/11/06/pca-tutorial/)\n\n# SVD\n\n**Feature Reduction using SVD**\n\n- blog: [http://blog.applied.ai/feature-reduction-using-svd/](http://blog.applied.ai/feature-reduction-using-svd/)\n- ipn: [http://nbviewer.jupyter.org/urls/bitbucket.org/appliedai/appliedai_svd/raw/02901de3aaa01be7b4536f484401113ddd5da8a9/01_FeatureReduction.ipynb](http://nbviewer.jupyter.org/urls/bitbucket.org/appliedai/appliedai_svd/raw/02901de3aaa01be7b4536f484401113ddd5da8a9/01_FeatureReduction.ipynb)\n\n**data-projector: Visualizing High-Dimensional Data in the Browser with SVD, t-SNE and Three.js**\n\n- github: [https://github.com/datacratic/data-projector](https://github.com/datacratic/data-projector)\n- blog: [http://datacratic.com/site/blog/visualizing-high-dimensional-data-browser-svd-t-sne-and-threejs](http://datacratic.com/site/blog/visualizing-high-dimensional-data-browser-svd-t-sne-and-threejs)\n- demo: [http://opensource.datacratic.com/data-projector/](http://opensource.datacratic.com/data-projector/)\n\n**RedSVD**\n\n- code: [https://code.google.com/p/redsvd/](https://code.google.com/p/redsvd/)\n- github: [https://github.com/handong1587/redsvd](https://github.com/handong1587/redsvd)\n- github: [https://github.com/ntessore/redsvd-h](https://github.com/ntessore/redsvd-h)\n\n**OpenCV 3.1.0: cv::SVD Class Reference**\n\n[http://docs.opencv.org/3.1.0/df/df7/classcv_1_1SVD.html#gsc.tab=0](http://docs.opencv.org/3.1.0/df/df7/classcv_1_1SVD.html#gsc.tab=0)\n\n**Singular Value Decomposition Part 1: Perspectives on Linear Algebra**\n\n- blog: [https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/](https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/)\n\n**Even Faster SVD Decomposition Yet Without Agonizing Pain**\n\n- arxiv: [http://arxiv.org/abs/1607.03463](http://arxiv.org/abs/1607.03463)\n\n# ICA\n\n**Independent component analysis**\n\n- blog: [http://efavdb.com/independent-component-analysis/](http://efavdb.com/independent-component-analysis/)\n- ipython notebook: [http://nbviewer.jupyter.org/github/EFavDB/ICA/blob/master/pyaudio.ipynb](http://nbviewer.jupyter.org/github/EFavDB/ICA/blob/master/pyaudio.ipynb)\n\n# Projects\n\n**Decomposition module for Torch7: Component Analysis using Torch7 (PCA, Whitened PCA, LDA, LPP, NPP, FastICA)**\n\n- github: [https://github.com/iassael/torch-decomposition](https://github.com/iassael/torch-decomposition)\n\n# Resources\n\n**Linear Dimensionality Reduction**\n\n- instructor: Percy Liang. CS294-10\n- slides: [http://vis.lbl.gov/~romano/mlgroup/papers/linear-dim-red.pdf](http://vis.lbl.gov/~romano/mlgroup/papers/linear-dim-red.pdf)\n\n# Readings and Questions\n\n**What is better PCA or SVD | Reddit**\n\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/4dkxm3/what_is_better_pca_or_svd/](https://www.reddit.com/r/MachineLearning/comments/4dkxm3/what_is_better_pca_or_svd/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","title":"Fun With Machine Learning"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Fun With Machine Learning\ndate: 2015-08-27\n---\n\n**Image Analogies**\n\n![](https://raw.githubusercontent.com/awentzonline/image-analogies/master/images/sugarskull-analogy.jpg)\n\n- project page: [http://www.mrl.nyu.edu/projects/image-analogies/index.html](http://www.mrl.nyu.edu/projects/image-analogies/index.html)\n- github: [https://github.com/awentzonline/image-analogies](https://github.com/awentzonline/image-analogies)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","title":"Graphical Models Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Graphical Models Resources\ndate: 2015-08-27\n---\n\n# Courses\n\n**Probabilistic Graphical Models**\n\n- intro: Master a new way of reasoning and learning in complex domains\n- instructor: Daphne Koller, Professor\n- coursera: [https://www.coursera.org/learn/probabilistic-graphical-models](https://www.coursera.org/learn/probabilistic-graphical-models)\n\n# Tutorials\n\n**Scalable learning of graphical models: A KDD'16 Tutorial**\n\n- homepage: [http://www.francois-petitjean.com/Research/KDD-2016-Tutorial/](http://www.francois-petitjean.com/Research/KDD-2016-Tutorial/)\n- mirror: [https://pan.baidu.com/s/1kUPCeLT](https://pan.baidu.com/s/1kUPCeLT)\n\n# Resources\n\n**A Brief Introduction to Graphical Models and Bayesian Networks**\n\n[http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html](http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html)\n\n**An Introduction To Graphical Models(by Michael I. Jordan. 1997)**\n\n- paper: [http://www.cis.upenn.edu/~mkearns/papers/barbados/jordan-tut.pdf](http://www.cis.upenn.edu/~mkearns/papers/barbados/jordan-tut.pdf)\n\n**Human-level concept learning through probabilistic program induction**\n\n- intro:  Science 2015\n- paper: [http://cdn1.almosthuman.cn/wp-content/uploads/2015/12/Human-level-concept-learning-through-probabilistic-program-induction.pdf](http://cdn1.almosthuman.cn/wp-content/uploads/2015/12/Human-level-concept-learning-through-probabilistic-program-induction.pdf)\n- article: [http://www.sciencemag.org/content/350/6266/1332.full](http://www.sciencemag.org/content/350/6266/1332.full)\n- github: [https://github.com/brendenlake/BPL](https://github.com/brendenlake/BPL)\n\n**Graphical Models Lectures 2015**\n\n- homepage: [http://www.stats.ox.ac.uk/~lienart/gml.html](http://www.stats.ox.ac.uk/~lienart/gml.html)\n\n**2 Graphical Models in a Nutshell**\n\n[http://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pdf](http://ai.stanford.edu/~koller/Papers/Koller+al:SRL07.pdf)\n\n**An ICCV 2015 Tutorial on Inference and Learning in Discrete Graphical Models: Theory and Practice**\n\n- homepage: [http://cvlab-dresden.de/research/optimization-and-learning/gm-tutorial-iccv2015/](http://cvlab-dresden.de/research/optimization-and-learning/gm-tutorial-iccv2015/)\n- slides(\"inference\"): [http://cvlab-dresden.de/wp-content/uploads/2016/01/inference.pdf](http://cvlab-dresden.de/wp-content/uploads/2016/01/inference.pdf)\n- slides(\"learning\"): [http://cvlab-dresden.de/wp-content/uploads/2015/12/learning.pdf](http://cvlab-dresden.de/wp-content/uploads/2015/12/learning.pdf)\n\n# Tools/Libraries\n\n**pomegranate: Fast, flexible and easy to use probabilistic modelling in Python**\n\n- github: [https://github.com/jmschrei/pomegranate](https://github.com/jmschrei/pomegranate)\n- docs: [http://pomegranate.readthedocs.org/en/latest/](http://pomegranate.readthedocs.org/en/latest/)\n\n**merlin: An extensible C++ library for probabilistic inference in graphical models**\n\n- github：[https://github.com/radum2275/merlin](https://github.com/radum2275/merlin)\n\n**pgmpy: Python Library for Probabilistic Graphical Models**\n\n- homepage: [http://pgmpy.org/](http://pgmpy.org/)\n- github: [https://github.com/pgmpy/pgmpy](https://github.com/pgmpy/pgmpy)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","title":"Machine Learning Courses"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Machine Learning Courses\ndate: 2015-08-27\n---\n\n# Courses\n\n**Courses on machine learning**\n\n[http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/MLPAGES/mlcourses.htm](http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/MLPAGES/mlcourses.htm)\n\n**CSC2535 -- Spring 2013 Advanced Machine Learning**\n\n- instructor: by Hinton, University of Toronto\n- homepage: [http://www.cs.toronto.edu/~hinton/csc2535/](http://www.cs.toronto.edu/~hinton/csc2535/)\n\n**Stanford CME 323: Distributed Algorithms and Optimization**\n\n[http://stanford.edu/~rezab/dao/](http://stanford.edu/~rezab/dao/)\n\n**University at Buffalo CSE574: Machine Learning and Probabilistic Graphical Models Course**\n\n[http://www.cedar.buffalo.edu/~srihari/CSE574/](http://www.cedar.buffalo.edu/~srihari/CSE574/)\n\n**Stanford CS229: Machine Learning Autumn 2015**\n\n- instructor: Andrew Ng\n- homepage: [http://cs229.stanford.edu/](http://cs229.stanford.edu/)\n- project page: [http://cs229.stanford.edu/projects2015.html](http://cs229.stanford.edu/projects2015.html)\n\n**Stanford / Winter 2014-2015 CS229T/STATS231: Statistical Learning Theory**\n\n- instructor: Percy Liang\n- homepage: [http://web.stanford.edu/class/cs229t/](http://web.stanford.edu/class/cs229t/)\n- lecture notes: [http://web.stanford.edu/class/cs229t/notes.pdf](http://web.stanford.edu/class/cs229t/notes.pdf)\n\n**CMU Fall 2015 10-715: Advanced Introduction to Machine Learning**\n\n- instructor: Alex Smola, Barnabas Poczos\n- homepage: [http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/](http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/)\n- video: [http://pan.baidu.com/s/1qWvcsUS](http://pan.baidu.com/s/1qWvcsUS)\n\n**2015 Machine Learning Summer School: Convex Optimization Short Course**\n\n- instructor: S. Boyd and S. Diamond\n- Lecture slides and IPython notebooks: [https://stanford.edu/~boyd/papers/cvx_short_course.html](https://stanford.edu/~boyd/papers/cvx_short_course.html)\n\n**STA 4273H (Winter 2015): Large Scale Machine Learning**\n\n[http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/](http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/)\n\n**University of Oxford: Machine Learning: 2014-2015**\n\n- homepage: [https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n- course materials: [https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n- lectures: [http://pan.baidu.com/s/1bndbxJh#path=%252FDeep%2520Learning%2520Lectures](http://pan.baidu.com/s/1bndbxJh#path=%252FDeep%2520Learning%2520Lectures)\n- github: [https://github.com/oxford-cs-ml-2015/](https://github.com/oxford-cs-ml-2015/)\n\n**Computer Science 294: Practical Machine Learning (Fall 2009)**\n\n- instructor: Michael Jordan\n- homepage: [https://www.cs.berkeley.edu/~jordan/courses/294-fall09/](https://www.cs.berkeley.edu/~jordan/courses/294-fall09/)\n\n**Statistics, Probability and Machine Learning Short Course**\n\n- homepage: [http://www-staff.it.uts.edu.au/~ydxu/statistics.htm](http://www-staff.it.uts.edu.au/~ydxu/statistics.htm)\n- youku: [http://i.youku.com/u/UMzIzNDgxNTg5Ng](http://i.youku.com/u/UMzIzNDgxNTg5Ng)\n- youbube: [https://www.youtube.com/playlist?list=PLFze15KrfxbF0n1zTNoFIaDpxnSyfgNgc](https://www.youtube.com/playlist?list=PLFze15KrfxbF0n1zTNoFIaDpxnSyfgNgc)\n\n**Statistical Learning**\n\n[https://lagunita.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about](https://lagunita.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about)\n\n**Machine learning courses online**\n\n[http://fastml.com/machine-learning-courses-online/](http://fastml.com/machine-learning-courses-online/)\n\n**Build Intelligent Applications: Master machine learning fundamentals in five hands-on courses (Coursera)**\n\n[https://www.coursera.org/specializations/machine-learning](https://www.coursera.org/specializations/machine-learning)\n\n**Machine Learning**\n\n[http://www.cs.ubc.ca/~nando/540-2013/lectures.html](http://www.cs.ubc.ca/~nando/540-2013/lectures.html)\n\n**Princeton Computer Science 598D: Overcoming Intractability in Machine Learning**\n\n[http://www.cs.princeton.edu/courses/archive/spring15/cos598D/](http://www.cs.princeton.edu/courses/archive/spring15/cos598D/)\n\n**Princeton Computer Science 511: Theoretical Machine Learning**\n\n- instructor: Rob Schapire\n- homepage: [http://www.cs.princeton.edu/courses/archive/spring14/cos511/schedule.html](http://www.cs.princeton.edu/courses/archive/spring14/cos511/schedule.html)\n\n**MACHINE LEARNING FOR MUSICIANS AND ARTISTS**\n\n[https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info](https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info)\n\n**CMSC 726: Machine Learning**\n\n- homepage: [http://www.cbcb.umd.edu/~hcorrada/PML/index.html](http://www.cbcb.umd.edu/~hcorrada/PML/index.html)\n\n**MIT: 9.520: Statistical Learning Theory and Applications, Fall 2015**\n\n[http://www.mit.edu/~9.520/fall15/](http://www.mit.edu/~9.520/fall15/)\n\n**CMU: Machine Learning: 10-701/15-781, Spring 2011**\n\n- instructor: Tom Mitchell\n- homepage: [http://www.cs.cmu.edu/~tom/10701_sp11/](http://www.cs.cmu.edu/~tom/10701_sp11/)\n- lectures: [http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml)\n\n**NLA 2015 course material**\n\n- ipn: [http://nbviewer.jupyter.org/github/Bihaqo/nla2015/blob/master/table_of_contents.ipynb](http://nbviewer.jupyter.org/github/Bihaqo/nla2015/blob/master/table_of_contents.ipynb)\n\n**CS 189/289A: Introduction to Machine Learning(with videos)**\n\n![](http://www.cs.berkeley.edu/~jrs/189/qdaaniso3d.png)\n\n- homepage: [http://www.cs.berkeley.edu/~jrs/189/](http://www.cs.berkeley.edu/~jrs/189/)\n\n**An Introduction to Statistical Machine Learning Spring 2014 (for ACM Class)**\n\n[http://bcmi.sjtu.edu.cn/log/courses/ml_2014_spring_acm.html](http://bcmi.sjtu.edu.cn/log/courses/ml_2014_spring_acm.html)\n\n**CS 159: Advanced Topics in Machine Learning (Spring 2016)**\n\n- intro:  Online Learning, Multi-Armed Bandits, Active Learning, Human-in-the-Loop Learning, Reinforcement Learning\n- instructor: Yisong Yue\n- homepage: [http://www.yisongyue.com/courses/cs159/](http://www.yisongyue.com/courses/cs159/)\n\n**Advanced Statistical Computing (Vanderbilt University)**\n\n- intro: Course covers numerical optimization, Markov Chain Monte Carlo (MCMC), Metropolis-Hastings, Gibbs sampling, \nestimation-maximization (EM) algorithms, data augmentation algorithms \nwith applications for model fitting and techniques for dealing with missing data\n- homepage: [http://stronginference.com/Bios8366/](http://stronginference.com/Bios8366/)\n- lecture: [http://stronginference.com/Bios8366/lectures.html](http://stronginference.com/Bios8366/lectures.html)\n- github: [https://github.com/fonnesbeck/Bios8366](https://github.com/fonnesbeck/Bios8366)\n\n**Stanford CS229: Machine Learning Spring 2016**\n\n- instructor: John Duchi\n- homepage: [http://cs229.stanford.edu/](http://cs229.stanford.edu/)\n- materials: [http://cs229.stanford.edu/materials.html](http://cs229.stanford.edu/materials.html)\n\n**Machine Learning:  2015-2016**\n\n- homepage: [https://www.cs.ox.ac.uk/teaching/courses/2015-2016/ml/](https://www.cs.ox.ac.uk/teaching/courses/2015-2016/ml/)\n- materials: [http://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-HT2016/index.html](http://www.cs.ox.ac.uk/people/varun.kanade/teaching/ML-HT2016/index.html)\n\n**CS273a: Introduction to Machine Learning**\n\n- homepage: [http://sli.ics.uci.edu/Classes/2015W-273a](http://sli.ics.uci.edu/Classes/2015W-273a)\n- youtube: [https://www.youtube.com/playlist?list=PLaXDtXvwY-oDvedS3f4HW0b4KxqpJ_imw](https://www.youtube.com/playlist?list=PLaXDtXvwY-oDvedS3f4HW0b4KxqpJ_imw)\n- course notes: [http://sli.ics.uci.edu/Classes-CS178-Notes/Classes-CS178-Notes](http://sli.ics.uci.edu/Classes-CS178-Notes/Classes-CS178-Notes)\n\n**Machine Learning CS-433**\n\n- homepage: [http://mlo.epfl.ch/page-136795.html](http://mlo.epfl.ch/page-136795.html)\n- github: [https://github.com/epfml/ML_course](https://github.com/epfml/ML_course)\n\n**Machine Learning Introduction: A machine learning course using Python, Jupyter Notebooks, and OpenML**\n\n[https://github.com/joaquinvanschoren/ML-course](https://github.com/joaquinvanschoren/ML-course)\n\n## Machine Learning on Distributed System\n\n**Distributed Machine Learning with Apache Spark**\n\n- edx: [https://prod-edx-mktg-edit.edx.org/course/distributed-machine-learning-apache-uc-berkeleyx-cs120x](https://prod-edx-mktg-edit.edx.org/course/distributed-machine-learning-apache-uc-berkeleyx-cs120x)\n\n## PhD-level Courses (with video lectures)\n\n**Phd-level courses**\n\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses/](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses/)\n\n**Advanced Introduction to Machine Learning**\n\n- homepage: [http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/index.html](http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/index.html)\n- video: [https://www.youtube.com/playlist?list=PL4DwY1suLMkcu-wytRDbvBNmx57CdQ2pJ&jct=q4qVgISGxJql7TlE6eSLKa8Wwci8SA](https://www.youtube.com/playlist?list=PL4DwY1suLMkcu-wytRDbvBNmx57CdQ2pJ&jct=q4qVgISGxJql7TlE6eSLKa8Wwci8SA)\n\n**STA 4273H (Winter 2015): Large Scale Machine Learning**\n\n[http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/](http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/)\n\n**Statistical Learning Theory and Applications (MIT)**\n\n- homepage: [http://www.mit.edu/~9.520/fall15/index.html](http://www.mit.edu/~9.520/fall15/index.html)\n- video: [https://www.youtube.com/playlist?list=PLyGKBDfnk-iDj3FBd0Avr_dLbrU8VG73O](https://www.youtube.com/playlist?list=PLyGKBDfnk-iDj3FBd0Avr_dLbrU8VG73O)\n\n**(REGML 2016) Regularization Methods for Machine Learning**\n\n- homepage: [http://lcsl.mit.edu/courses/regml/regml2016/](http://lcsl.mit.edu/courses/regml/regml2016/)\n- video: [https://www.youtube.com/playlist?list=PLbF0BXX_6CPJ20Gf_KbLFnPWjFTvvRwCO](https://www.youtube.com/playlist?list=PLbF0BXX_6CPJ20Gf_KbLFnPWjFTvvRwCO)\n\n**Convex Optimization: Spring 2015**\n\n- homepage: [http://www.stat.cmu.edu/~ryantibs/convexopt-S15/](http://www.stat.cmu.edu/~ryantibs/convexopt-S15/)\n- video: [https://www.youtube.com/playlist?list=PLjbUi5mgii6BZBhJ9nW7eydgycyCOYeZ6](https://www.youtube.com/playlist?list=PLjbUi5mgii6BZBhJ9nW7eydgycyCOYeZ6)\n\n**CMU: Probabilistic Graphical Models (10-708, Spring 2014)**\n\n- instructor: Eric Xing\n- homepage: [http://www.cs.cmu.edu/~epxing/Class/10708/](http://www.cs.cmu.edu/~epxing/Class/10708/)\n- lecture: [http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html](http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html)\n\n**Advanced Optimization and Randomized Methods**\n\n- instructor: A. Smola, S. Sra\n- homepage: [http://www.cs.cmu.edu/~suvrit/teach/index.html](http://www.cs.cmu.edu/~suvrit/teach/index.html)\n\n**Machine Learning for Robotics and Computer Vision**\n\n- homepage: [http://vision.in.tum.de/teaching/ws2013/ml_ws13](http://vision.in.tum.de/teaching/ws2013/ml_ws13)\n- video: [https://www.youtube.com/watch?v=QZmZFeZxEKI&list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl](https://www.youtube.com/watch?v=QZmZFeZxEKI&list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl)\n\n**Statistical Machine Learning**\n\n- homepage: [http://www.stat.cmu.edu/~larry/=sml/](http://www.stat.cmu.edu/~larry/=sml/)\n- video: [https://www.youtube.com/playlist?list=PLTB9VQq8WiaCBK2XrtYn5t9uuPdsNm7YE](https://www.youtube.com/playlist?list=PLTB9VQq8WiaCBK2XrtYn5t9uuPdsNm7YE)\n- mirror: [http://pan.baidu.com/s/1eSuJ1Nc](http://pan.baidu.com/s/1eSuJ1Nc)\n\n## PhD-level Courses (without video lectures)\n\n**Probabilistic Graphical Models (10-708, Spring 2016)**\n\n[http://www.cs.cmu.edu/~epxing/Class/10708-16/lecture.html](http://www.cs.cmu.edu/~epxing/Class/10708-16/lecture.html)\n\n# Resources\n\n**Learn Machine learning online – List of machine learning courses available online**\n\n- blog: [http://bafflednerd.com/learn-machine-learning-online/](http://bafflednerd.com/learn-machine-learning-online/)\n\n**awesomeMLmath**\n\n- intro: Curated list to learn the math basics for machine learning. Note that this is a biased list from a Deep Learning research.\n- github: [https://github.com/EderSantana/awesomeMLmath](https://github.com/EderSantana/awesomeMLmath)\n\n**MOOCs for Machine Learning**\n\n[https://medium.com/@amarbudhiraja/moocs-for-machine-learning-5a2f2c6cdcfe#.1m2v38e0y](https://medium.com/@amarbudhiraja/moocs-for-machine-learning-5a2f2c6cdcfe#.1m2v38e0y)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","title":"Neural Network"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Neural Network\ndate: 2015-08-27\n---\n\n# Tutorials\n\n**Neural Network Tutorial**\n\n- intro: In this tutorial series we develop the back-propagation algorithm, \nexplore how it functions, and build a back propagation neural network library in C#\n- youtube: [https://www.youtube.com/playlist?list=PL29C61214F2146796](https://www.youtube.com/playlist?list=PL29C61214F2146796)\n\n**A history of Bayesian neural networks**\n\n- intro: by Zoubin Ghahramani, University of Cambridge. NIPS 2016 Bayesian Deep Learning\n- slides: [http://bayesiandeeplearning.org/slides/nips16bayesdeep.pdf](http://bayesiandeeplearning.org/slides/nips16bayesdeep.pdf)\n\n# Papers\n\n**A Fast C++ Implementation of Neural Network Backpropagation Training Algorithm: Application to Bayesian Optimal Image Demosaicking**\n\n- project page: [http://www.ipol.im/pub/art/2015/137/](http://www.ipol.im/pub/art/2015/137/)\n- paper: [http://www.ipol.im/pub/art/2015/137/article_lr.pdf](http://www.ipol.im/pub/art/2015/137/article_lr.pdf)\n- code: [http://www.ipol.im/pub/art/2015/137/NN.tar.gz](http://www.ipol.im/pub/art/2015/137/NN.tar.gz)\n\n**CrAIg: Using Neural Networks to learn Mario**\n\n- blog: [https://medium.com/@savas/craig-using-neural-networks-to-learn-mario-a76036b639ad](https://medium.com/@savas/craig-using-neural-networks-to-learn-mario-a76036b639ad)\n- github: [https://github.com/joenot443/crAIg](https://github.com/joenot443/crAIg)\n- paper: [http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)\n\n**Calculus and Backpropagation**\n\n- paper: [https://github.com/mtomassoli/papers/blob/master/backprop.pdf](https://github.com/mtomassoli/papers/blob/master/backprop.pdf)\n\n**A Tutorial about Random Neural Networks in Supervised Learning**\n\n- inro: Neural Network World\n- arxiv: [https://arxiv.org/abs/1609.04846](https://arxiv.org/abs/1609.04846)\n\n# Blogs\n\n**Artificial Neural Networks (ANN) Introduction**\n\n- blog: [https://annalyzin.wordpress.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/](https://annalyzin.wordpress.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/)\n\n**Getting It Done - What I learnt from finishing the Neural Network Algorithm**\n\n- blog: [http://nicksparallellaideas.blogspot.com.au/2015/12/getting-it-done-what-i-learnt-from.html](http://nicksparallellaideas.blogspot.com.au/2015/12/getting-it-done-what-i-learnt-from.html)\n- blog: [http://pan.baidu.com/s/1sk6VPBf](http://pan.baidu.com/s/1sk6VPBf)\n\n**Learning How To Code Neural Networks**\n\n- blog: [https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e#.nc2bx42ct](https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e#.nc2bx42ct)\n\n**Watch Tiny Neural Nets Learn**\n\n[http://swanintelligence.com/watch-tiny-neural-nets-learn.html](http://swanintelligence.com/watch-tiny-neural-nets-learn.html)\n\n**Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained**\n\n[https://medium.com/a-year-of-artificial-intelligence/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.9igtj84gm](https://medium.com/a-year-of-artificial-intelligence/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d#.9igtj84gm)\n\n**The Essence of Artificial Neural Networks**\n\n![](https://cdn-images-1.medium.com/max/800/1*eBMwpBBboAXgqsawwOKkPw.png)\n\n[https://medium.com/@ivanliljeqvist/the-essence-of-artificial-neural-networks-5de300c995d6#.yk0kydmf8](https://medium.com/@ivanliljeqvist/the-essence-of-artificial-neural-networks-5de300c995d6#.yk0kydmf8)\n\n**Neural Networks with PHP**\n\n- youtube: [https://www.youtube.com/watch?v=5bFxDsoNFzU](https://www.youtube.com/watch?v=5bFxDsoNFzU)\n- mirror: [http://pan.baidu.com/s/1boc933p](http://pan.baidu.com/s/1boc933p)\n\n**Neural Networs in MySQL**\n\n- blog: [http://blog.itdxer.com/2016/07/01/neural-networs-in-mysql.html](http://blog.itdxer.com/2016/07/01/neural-networs-in-mysql.html)\n- github: [https://github.com/itdxer/Neural-Network-in-MySQL](https://github.com/itdxer/Neural-Network-in-MySQL)\n\n**Neural Networks in JavaScript**\n\n- blog: [http://www.antoniodeluca.info/blog/10-08-2016/neural-networks-in-javascript.html](http://www.antoniodeluca.info/blog/10-08-2016/neural-networks-in-javascript.html)\n\n**Multiplication with simple neural nets**\n\n- blog: [http://nbviewer.jupyter.org/gist/fperez/c7b1cb4810f9d0935e893f34c41f0c62](http://nbviewer.jupyter.org/gist/fperez/c7b1cb4810f9d0935e893f34c41f0c62)\n\n**Training Neural Networks with Theano**\n\n[http://blog.asidatascience.com/training-neural-networks-with-theano/](http://blog.asidatascience.com/training-neural-networks-with-theano/)\n\n**An Introduction to Machine Learning in Julia**\n\n[http://juliacomputing.com/blog/2016/09/28/knn-char-recognition.html](http://juliacomputing.com/blog/2016/09/28/knn-char-recognition.html)\n\n**A Quick Introduction to Neural Networks**\n\n[http://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html](http://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html)\n\n**Starting with Neural Networks in Swift 3.0**\n\n- blog: [https://medium.com/@michael.m/starting-with-neural-networks-in-swift-3-0-d260c1f0bf74#.rghtp2ip3](https://medium.com/@michael.m/starting-with-neural-networks-in-swift-3-0-d260c1f0bf74#.rghtp2ip3)\n- bitbucket: [https://bitbucket.org/mmick66/neural1](https://bitbucket.org/mmick66/neural1)\n\n**Visual and Interactive Guide to the Basics of Neural Networks**\n\n[http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)\n\n## Backpropagation\n\n**A Step by Step Backpropagation Example**\n\n- blog: [http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/](http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n- github: [https://github.com/mattm/simple-neural-network](https://github.com/mattm/simple-neural-network)\n\n**Can you give a visual explanation for the back propagation algorithm for neural networks?**\n\n- github: [https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md)\n\n**一文弄懂神经网络中的反向传播法——BackPropagation**\n\n[http://www.cnblogs.com/charlotte77/p/5629865.html](http://www.cnblogs.com/charlotte77/p/5629865.html)\n\n**Visualize Algorithms based on the Backpropagation**\n\n- blog: [http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent](http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent)\n\n**Backpropagation — How Neural Networks Learn Complex Behaviors**\n\n- blog: [https://medium.com/autonomous-agents/backpropagation-how-neural-networks-learn-complex-behaviors-9572ac161670#.ipu4jr2ka](https://medium.com/autonomous-agents/backpropagation-how-neural-networks-learn-complex-behaviors-9572ac161670#.ipu4jr2ka)\n\n**Derivation of Backpropagation**\n\n[https://www.cs.swarthmore.edu/~meeden/cs81/s10/BackPropDeriv.pdf](https://www.cs.swarthmore.edu/~meeden/cs81/s10/BackPropDeriv.pdf)\n\n**Backpropagation**\n\n[https://en.wikipedia.org/wiki/Backpropagation#Derivation](https://en.wikipedia.org/wiki/Backpropagation#Derivation)\n\n**A Derivation of Backpropagation in Matrix Form**\n\n- blog: [http://sudeepraja.github.io/Neural/](http://sudeepraja.github.io/Neural/)\n\n**Backpropagation — How Neural Networks Learn Complex Behaviors**\n\n- blog: [https://medium.com/autonomous-agents/backpropagation-how-neural-networks-learn-complex-behaviors-9572ac161670#.dmdy8glec](https://medium.com/autonomous-agents/backpropagation-how-neural-networks-learn-complex-behaviors-9572ac161670#.dmdy8glec)\n\n**How to Implement the Backpropagation Algorithm From Scratch In Python**\n\n- blog: [http://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/](http://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)\n\n**Notes on Backpropagation**\n\n- intro: This document derives backpropagation for some common error functions and describes some other tricks.\n- paper: [https://www.ics.uci.edu/~pjsadows/notes.pdf](https://www.ics.uci.edu/~pjsadows/notes.pdf)\n\n**Yes you should understand backprop**\n\n- intro: Andrej Karpathy\n- blog: [https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.hbbhgjzi9](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.hbbhgjzi9)\n\n**Back-propagation, an introduction**\n\n- blog: [http://www.offconvex.org/2016/12/20/backprop/](http://www.offconvex.org/2016/12/20/backprop/)\n\n# Project\n\n**Neural Networks demo in Javascript**\n\n- demo: [http://cs.stanford.edu/people/karpathy/svmjs/demo/demonn.html](http://cs.stanford.edu/people/karpathy/svmjs/demo/demonn.html)\n\n**Interactive visualization of artificial neural networks**\n\n- homepage: [http://experiments.mostafa.io/public/ffbpann/](http://experiments.mostafa.io/public/ffbpann/)\n- github: [https://github.com/drdrsh/interactive-bpann](https://github.com/drdrsh/interactive-bpann)\n\n**NNX: Neural networks for Excel**\n\n![](https://nnx-addin.org/img/NNX-hero-movie.gif)\n\n- intro: \"Multilayer perceptrons done the Excel way\"\n- homepage: [https://nnx-addin.org/](https://nnx-addin.org/)\n- github: [https://github.com/ikhramts/nnx](https://github.com/ikhramts/nnx)\n\n**GNU Gneural Network**\n\n- homepage: [http://www.gnu.org/software/gneuralnetwork/](http://www.gnu.org/software/gneuralnetwork/)\n\n**Generating procedural plants with neural networks**\n\n![](http://www.iotapersei.com/img/grow2.gif)\n\n- blog: [http://www.iotapersei.com/_neural_networks_article.html](http://www.iotapersei.com/_neural_networks_article.html)\n\n**NeuroFlow: A lightweight, scala based library for Artificial Neural Networks**\n\n- github: [https://github.com/zenecture/neuroflow](https://github.com/zenecture/neuroflow)\n\n**DN2A - Digital Neural Network Architecture**\n\n![](http://www.dn2a.org/images/concept-two-light.png)\n\n- homepage: [http://www.dn2a.org/](http://www.dn2a.org/)\n- github: [https://github.com/dn2a/dn2a-javascript](https://github.com/dn2a/dn2a-javascript)\n\n**visual-neural-net**\n\n- intro: Interactive visualization of artificial neural networks that leverages deeplearning4j, spring, and angular2\n- github(server): [https://github.com/JavaFXpert/visual-neural-net-server](https://github.com/JavaFXpert/visual-neural-net-server)\n- github(client):[https://github.com/JavaFXpert/ng2-spring-websocket-client](https://github.com/JavaFXpert/ng2-spring-websocket-client)\n\n**NN++: A small and easy to use neural net implementation for C++**\n\n- intro: It includes the neural net implementation and a Matrix class for basic linear algebra operations\n- github: [https://github.com/stagadish/NNplusplus](https://github.com/stagadish/NNplusplus)\n\n**Neural networks module for Redis**\n\n- github: [https://github.com/antirez/neural-redis](https://github.com/antirez/neural-redis)\n\n**Introduction of neural-redis**\n\n- part 1: [https://medium.com/the-quarter-espresso/introduction-of-neural-redis-part-1-fa13c1faeef1#.v4sqluonf](https://medium.com/the-quarter-espresso/introduction-of-neural-redis-part-1-fa13c1faeef1#.v4sqluonf)\n- part 2: [https://medium.com/the-quarter-espresso/introduction-of-neural-redis-part-2-6c22b42f412c#.gt452jx8c](https://medium.com/the-quarter-espresso/introduction-of-neural-redis-part-2-6c22b42f412c#.gt452jx8c)\n- part 3: [https://medium.com/the-quarter-espresso/introduction-of-neural-redis-part-3-9d59012120a#.wo9gblc1u](https://medium.com/the-quarter-espresso/introduction-of-neural-redis-part-3-9d59012120a#.wo9gblc1u)\n\n**Neural Nets in native Haskell**\n\n- github: [https://github.com/brunjlar/neural](https://github.com/brunjlar/neural)\n\n**Command Line Neural Network**\n\n- github: [https://github.com/hugorut/neural-cli](https://github.com/hugorut/neural-cli)\n\n**Chai: An open source neural network library**\n\n- github: [https://github.com/SullyChen/Chai](https://github.com/SullyChen/Chai)\n\n**Neural networks in JavaScript**\n\n- homepage: [http://brainjs.com/](http://brainjs.com/)\n- github: [https://github.com/harthur-org/brain.js](https://github.com/harthur-org/brain.js)\n\n**Neural Network in Rust**\n\n- homepage: [http://juggernaut.rs/](http://juggernaut.rs/)\n- github: [https://github.com/afshinm/juggernaut](https://github.com/afshinm/juggernaut)\n\n# Videos\n\n**Neural Networks with PHP**\n\n- youtube: [https://www.youtube.com/watch?v=5bFxDsoNFzU](https://www.youtube.com/watch?v=5bFxDsoNFzU)\n- video: [http://pan.baidu.com/s/1boc933p](http://pan.baidu.com/s/1boc933p)\n\n# Books\n\n**Neural Network Programming with Java**\n\n[https://www.packtpub.com/mapt/book/All%20Books/9781785880902](https://www.packtpub.com/mapt/book/All%20Books/9781785880902)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","title":"Natural Language Processing"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Natural Language Processing\ndate: 2015-08-27\n---\n\n# Courses\n\n**Introduction to Natural Language Processing**\n\n- coursera: [https://www.coursera.org/learn/natural-language-processing](https://www.coursera.org/learn/natural-language-processing)\n\n# Projects\n\n**Automatic text summarizer: Module for automatic summarization of text documents and HTML pages**\n\n- homepage: [https://pypi.python.org/pypi/sumy](https://pypi.python.org/pypi/sumy)\n- github: [https://github.com/miso-belica/sumy](https://github.com/miso-belica/sumy)\n\n**textacy: higher-level NLP built on spaCy**\n\n- github: [https://github.com/chartbeat-labs/textacy](https://github.com/chartbeat-labs/textacy)\n- docs: [http://textacy.readthedocs.io/en/latest/](http://textacy.readthedocs.io/en/latest/)\n\n**Word2Vec in C++ 11**\n\n- github: [https://github.com/jdeng/word2vec](https://github.com/jdeng/word2vec)\n\n## Stanford NLP\n\n**Stanford NLP**\n\n- homepage: [http://nlp.stanford.edu/software/](http://nlp.stanford.edu/software/)\n\n**Stanford CoreNLP – a suite of core NLP tools**\n\n- homepage: [http://stanfordnlp.github.io/CoreNLP/index.html](http://stanfordnlp.github.io/CoreNLP/index.html)\n\n**Head First Stanford NLP**\n\n- (1): [http://hujiaweibujidao.github.io/blog/2016/03/30/Stanford-NLP/](http://hujiaweibujidao.github.io/blog/2016/03/30/Stanford-NLP/)\n- (2): [http://hujiaweibujidao.github.io/blog/2016/03/31/Head-First-Standford-NLP-2/](http://hujiaweibujidao.github.io/blog/2016/03/31/Head-First-Standford-NLP-2/)\n- (3): [http://hujiaweibujidao.github.io/blog/2016/03/31/Head-First-Standford-NLP-3/](http://hujiaweibujidao.github.io/blog/2016/03/31/Head-First-Standford-NLP-3/)\n- (4): [http://hujiaweibujidao.github.io/blog/2016/04/01/Head-First-Standford-NLP-4/](http://hujiaweibujidao.github.io/blog/2016/04/01/Head-First-Standford-NLP-4/)\n\n**Thinc: Practical Machine Learning for NLP in Python**\n\n- intro: spaCy's Machine Learning library for NLP in Python\n- github: [https://github.com/explosion/thinc](https://github.com/explosion/thinc)\n\n# Blogs\n\n**Natural Language Understanding by Matching Parse Trees**\n\n- blog: [http://blog.ayoungprogrammer.com/2016/07/natural-language-understanding-by.html/](http://blog.ayoungprogrammer.com/2016/07/natural-language-understanding-by.html/)\n\n**A Tutorial about Programming for Natural Language Processing**\n\n- github: [https://github.com/neubig/nlptutorial](https://github.com/neubig/nlptutorial)\n\n# Resources\n\n**awesome-nlp: A curated list of resources dedicated to Natural Language Processing (NLP)**\n\n- github: [https://github.com/keonkim/awesome-nlp](https://github.com/keonkim/awesome-nlp)\n\n**A curated list of beginner resources in Natural Language Processing**\n\n- github: [https://github.com/gutfeeling/beginner_nlp](https://github.com/gutfeeling/beginner_nlp)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","title":"Random Field"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Random Field\ndate: 2015-08-27\n---\n\n# Markov Random Field\n\n# Conditional Random Fields (CRF)\n\n**conditional random fields**\n\n- intro: \"This page contains material on, or relating to, conditional random fields.\"\n- blog: [http://www.inference.phy.cam.ac.uk/hmw26/crf/](http://www.inference.phy.cam.ac.uk/hmw26/crf/)\n\n**An Introduction to Conditional Random Fields**\n\n- author: Charles Sutton and Andrew McCallum\n- intro: 90 pages\n- arxiv: [http://arxiv.org/abs/1011.4088](http://arxiv.org/abs/1011.4088\n- paper: [http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf](http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)\n\n**Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials**\n\n- intro: NIPS 2011\n- arxiv: [https://arxiv.org/abs/1210.5644](https://arxiv.org/abs/1210.5644)\n\n**Introduction to Conditional Random Fields**\n\n[http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/](http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/)\n\n**Log-linear Models and Conditional Random Fields**\n\n- video: [http://videolectures.net/cikm08_elkan_llmacrf/](http://videolectures.net/cikm08_elkan_llmacrf/)\n- slides: [http://pan.baidu.com/s/1sjE0WfN](http://pan.baidu.com/s/1sjE0WfN)\n\n**Conditional Random Fields: A Beginner’s Survey**\n\n- blog: [https://onionesquereality.wordpress.com/2011/08/20/conditional-random-fields-a-beginners-survey/](https://onionesquereality.wordpress.com/2011/08/20/conditional-random-fields-a-beginners-survey/)\n\n**CRF++代码分析**\n\n- blog: [http://www.hankcs.com/ml/crf-code-analysis.html](http://www.hankcs.com/ml/crf-code-analysis.html)\n\n**Learning Arbitrary Potentials in CRFs with Gradient Descent**\n\n- intro: Chalmers Univ. of Technology & Oxford Univ & Australian National Univ.\n- arxiv: [https://arxiv.org/abs/1701.06805](https://arxiv.org/abs/1701.06805)\n\n# Gibbs Random Field\n\n# Gaussian Random Field\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","title":"Random Forests"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Random Forests\ndate: 2015-08-27\n---\n\n# Papers\n\n**Mondrian Forests: Efficient Online Random Forests**\n\n- arxiv: [http://arxiv.org/abs/1406.2673](http://arxiv.org/abs/1406.2673)\n- github: [https://github.com/balajiln/mondrianforest](https://github.com/balajiln/mondrianforest)\n- video: [http://videolectures.net/sahd2014_teh_mondrian_forests/](http://videolectures.net/sahd2014_teh_mondrian_forests/)\n\n**Food-101 – Mining Discriminative Components with Random Forests**\n\n![](http://www.vision.ee.ethz.ch/datasets/food-101/static/img/rf-comp-mining.png)\n\n- homepage: [http://www.vision.ee.ethz.ch/datasets/food-101/](http://www.vision.ee.ethz.ch/datasets/food-101/)\n\n**Bernoulli Random Forests: Closing the Gap between Theoretical Consistency and Empirical Soundness (IJCAI 2016)**\n\n- paper: [http://www.ijcai.org/Proceedings/16/Papers/309.pdf](http://www.ijcai.org/Proceedings/16/Papers/309.pdf)\n\n**TensorForest: Scalable Random Forests on TensorFlow**\n\n- intro: Google Inc. NIPS 2016\n- paper: [https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE)\n- github: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tensor_forest](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tensor_forest)\n\n**ggRandomForests: Exploring Random Forest Survival**\n\n- intro: ggRandomForests: Visually Exploring Random Forests\n- arxiv: [https://arxiv.org/abs/1612.08974](https://arxiv.org/abs/1612.08974)\n- package(R): [https://cran.r-project.org/web/packages/ggRandomForests/index.html](https://cran.r-project.org/web/packages/ggRandomForests/index.html)\n- github: [https://github.com/ehrlinger/ggRandomForests](https://github.com/ehrlinger/ggRandomForests)\n\n# Resources\n\n**AWESOME-RANDOM-FOREST - a curated list of resources regarding random forest**\n\n- blog: [http://jiwonkim.org/awesome-random-forest/](http://jiwonkim.org/awesome-random-forest/)\n- github: [https://github.com/kjw0612/awesome-random-forest](https://github.com/kjw0612/awesome-random-forest)\n\n**Random Forests - What, Why, And How**\n\n- slides: [https://nyhackr.blob.core.windows.net/presentations/Random-Forests-What-Why-and-How_Andy_Liaw.pdf](https://nyhackr.blob.core.windows.net/presentations/Random-Forests-What-Why-and-How_Andy_Liaw.pdf)\n\n# Blogs\n\n**The Unreasonable Effectiveness of Random Forests**\n\n- blog: [https://medium.com/rants-on-machine-learning/the-unreasonable-effectiveness-of-random-forests-f33c3ce28883](https://medium.com/rants-on-machine-learning/the-unreasonable-effectiveness-of-random-forests-f33c3ce28883)\n- mirror: [http://pan.baidu.com/s/1gd6dsMR](http://pan.baidu.com/s/1gd6dsMR)\n\n**Random forest interpretation with scikit-learn**\n\n- blog: [http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/)\n- github: [https://github.com/andosa/treeinterpreter](https://github.com/andosa/treeinterpreter)\n\n**Machine learning - Random forests (by Nando de Freitas)**\n\n- youtube: [https://www.youtube.com/watch?v=3kYujfDgmNk&hd=1](https://www.youtube.com/watch?v=3kYujfDgmNk&hd=1)\n- lectures: [http://www.cs.ubc.ca/~nando/540-2013/lectures.html](http://www.cs.ubc.ca/~nando/540-2013/lectures.html)\n\n**A tour of random forests**\n\n[https://codewords.recurse.com/issues/seven/a-tour-of-random-forests](https://codewords.recurse.com/issues/seven/a-tour-of-random-forests)\n\n# Projects\n\n**Random Forest demo in Javascript**\n\n- demo: [http://cs.stanford.edu/people/karpathy/svmjs/demo/demoforest.html](http://cs.stanford.edu/people/karpathy/svmjs/demo/demoforest.html)\n- github: [https://github.com/karpathy/forestjs](https://github.com/karpathy/forestjs)\n\n**ParallelForest: Random Forest Classification with Parallel Computing**\n\n- intro: R package implementing random forest classification using parallel computing, built with Fortran and OpenMP in the backend.\n- project page: [https://cran.r-project.org/web/packages/ParallelForest/vignettes/ParallelForest-intro.html](https://cran.r-project.org/web/packages/ParallelForest/vignettes/ParallelForest-intro.html)\n- github: [https://github.com/bert9bert/ParallelForest](https://github.com/bert9bert/ParallelForest)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","title":"Regression"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Regression\ndate: 2015-08-27\n---\n\n# Papers\n\n**Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression**\n\n- arxiv: [http://arxiv.org/abs/1602.05419](http://arxiv.org/abs/1602.05419)\n\n**Structured Sparse Regression via Greedy Hard-Thresholding**\n\n- arxiv: [http://arxiv.org/abs/1602.06042](http://arxiv.org/abs/1602.06042)\n\n**Evaluating Regularization and Optimization Algorithms for Logistic Regression on Spam Classification**\n\n- arxiv: [http://rnowling.github.io/data/science/2016/09/04/comparing-lr-regularization-and-optimizers.html](http://rnowling.github.io/data/science/2016/09/04/comparing-lr-regularization-and-optimizers.html)\n\n# Blogs\n\n**10 types of regressions. Which one to use?**\n\n- blog: [http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use](http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use)\n\n**Linear Regression for Machine Learning**\n\n[http://machinelearningmastery.com/linear-regression-for-machine-learning/](http://machinelearningmastery.com/linear-regression-for-machine-learning/)\n\n**Regression, Logistic Regression and Maximum Entropy**\n\n[http://ataspinar.com/2016/03/28/regression-logistic-regression-and-maximum-entropy/](http://ataspinar.com/2016/03/28/regression-logistic-regression-and-maximum-entropy/)\n\n**Principal Components Regression**\n\n- \"Pt.1: The Standard Method\": [http://www.win-vector.com/blog/2016/05/pcr_part1_xonly/](http://www.win-vector.com/blog/2016/05/pcr_part1_xonly/)\n\n**Sequence prediction using recurrent neural networks(LSTM) with TensorFlow: LSTM regression using TensorFlow**\n\n- blog: [http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html](http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html)\n- github: [https://github.com/mouradmourafiq/tensorflow-lstm-regression](https://github.com/mouradmourafiq/tensorflow-lstm-regression)\n\n**From simple Regression to Multiple Regression with Decision Trees**\n\n[https://blog.cambridgecoding.com/2016/01/10/from-simple-regression-to-multiple-regression-with-decision-trees/](https://blog.cambridgecoding.com/2016/01/10/from-simple-regression-to-multiple-regression-with-decision-trees/)\n\n**Linear Regression**\n\n![](http://efavdb.com/wp-content/uploads/2016/05/line-4.jpg)\n\n[http://efavdb.com/linear-regression/](http://efavdb.com/linear-regression/)\n\n**What is Softmax regression and how is it related to Logistic regression?**\n\n-github: [https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/softmax_regression.md)\n\n**A Neat Trick to Increase Robustness of Regression Models**\n\n- blog: [https://blog.clevertap.com/a-neat-trick-to-increase-robustness-of-regression-models/](https://blog.clevertap.com/a-neat-trick-to-increase-robustness-of-regression-models/)\n\n**Visualising Residuals**\n\n![](https://svbtleusercontent.com/9jiwlvibugesq.png)\n\n- blog: [https://drsimonj.svbtle.com/visualising-residuals](https://drsimonj.svbtle.com/visualising-residuals)\n\n**Applications Of Isotonic Regression**\n\n[http://jxieeducation.com/2016-09-18/Applications-of-Isotonic-Regression/](http://jxieeducation.com/2016-09-18/Applications-of-Isotonic-Regression/)\n\n**Bayesian Linear Regression**\n\n- github: [https://github.com/liviu-/notebooks/blob/master/bayesian_linear_regression.ipynb](https://github.com/liviu-/notebooks/blob/master/bayesian_linear_regression.ipynb)\n\n**45 questions to test a Data Scientist on Regression (Skill test – Regression Solution)**\n\n[https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/](https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/)\n\n**A comprehensive beginners guide for Linear, Ridge and Lasso Regression**\n\n[https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/](https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/)\n\n## Logistic regression\n\n**Guide to an in-depth understanding of logistic regression**\n\n- blog: [http://www.dataschool.io/guide-to-logistic-regression/](http://www.dataschool.io/guide-to-logistic-regression/)\n\n**Demystifying Logistic Regression**\n\n[http://tech.magnetic.com/2016/04/demystifying-logistic-regression.html](http://tech.magnetic.com/2016/04/demystifying-logistic-regression.html)\n\n**The Sigmoid Function in Logistic Regression**\n\n[http://karlrosaen.com/ml/notebooks/logistic-regression-why-sigmoid/](http://karlrosaen.com/ml/notebooks/logistic-regression-why-sigmoid/)\n\n**A Brief Primer on Linear Regression**\n\n- part I: [https://blog.clevertap.com/a-brief-primer-on-linear-regression-part-i/](https://blog.clevertap.com/a-brief-primer-on-linear-regression-part-i/)\n- part II: [https://blog.clevertap.com/a-brief-primer-on-linear-regression-part-ii/](https://blog.clevertap.com/a-brief-primer-on-linear-regression-part-ii/) \n- part III: [https://blog.clevertap.com/a-brief-primer-on-linear-regression-part-iii/](https://blog.clevertap.com/a-brief-primer-on-linear-regression-part-iii/)\n\n**A Primer on Logistic Regression**\n\n- blog: [https://blog.clevertap.com/a-primer-on-logistic-regression-part-i/](https://blog.clevertap.com/a-primer-on-logistic-regression-part-i/)\n\n**Logistic Regression in Tensorflow with SMOTE**\n\n- blog: [http://aqibsaeed.github.io/2016-08-10-logistic-regression-tf/](http://aqibsaeed.github.io/2016-08-10-logistic-regression-tf/)\n\n**Introduction to Logistic Regression**\n\n[https://blog.bigml.com/2016/09/22/introduction-to-logistic-regression/](https://blog.bigml.com/2016/09/22/introduction-to-logistic-regression/)\n\n**Logistic regression**\n\n- blog: [http://eli.thegreenplace.net/2016/logistic-regression/](http://eli.thegreenplace.net/2016/logistic-regression/)\n- github: [https://github.com/eliben/deep-learning-samples/tree/master/logistic-regression](https://github.com/eliben/deep-learning-samples/tree/master/logistic-regression)\n\n# Ridge Regression\n\n**Lecture notes on ridge regression**\n\n- arxiv: [https://arxiv.org/abs/1509.09169](https://arxiv.org/abs/1509.09169)\n\n**Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging**\n\n- intro: UC Berkeley & Rensselaer Polytechnic Institute\n- arxiv: [https://arxiv.org/abs/1702.04837](https://arxiv.org/abs/1702.04837)\n- github: [https://github.com/wangshusen/SketchedRidgeRegression](https://github.com/wangshusen/SketchedRidgeRegression)\n\n# Support Vector Regressor (SVR)\n\n**A Tutorial on Support Vector Regression**\n\n- intro: Alex J. Smola. 2003\n- paper: [https://alex.smola.org/papers/2003/SmoSch03b.pdf](https://alex.smola.org/papers/2003/SmoSch03b.pdf)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","title":"Support Vector Machine"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Support Vector Machine\ndate: 2015-08-27\n---\n\n# Papers\n\n**Sequential Minimal Optimization Algorithm for Support Vector Machines**\n\n- blog: [http://stiglerdiet.com/blog/2015/Nov/21/sequential-minimal-optimization-algorithm-for-support-vector-machines/](http://stiglerdiet.com/blog/2015/Nov/21/sequential-minimal-optimization-algorithm-for-support-vector-machines/)\n- github: [https://github.com/tdhopper/smo-svm](https://github.com/tdhopper/smo-svm)\n- paper: [https://github.com/tdhopper/smo-svm/blob/master/OR706%20Support%20Vector%20Machines.pdf](https://github.com/tdhopper/smo-svm/blob/master/OR706%20Support%20Vector%20Machines.pdf)\n\n**Minimal Support Vector Machine**\n\n[https://arxiv.org/abs/1804.02370](https://arxiv.org/abs/1804.02370)\n\n# Blogs\n\n**Everything You Wanted to Know about the Kernel Trick**\n\n[http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)\n\n**Kernels and SVMs**\n\n[http://blog.rokkincat.com/support-vector-machines/](http://blog.rokkincat.com/support-vector-machines/)\n\n**Support Vector Machines: A Guide for Beginners**\n\n[https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners](https://www.quantstart.com/articles/Support-Vector-Machines-A-Guide-for-Beginners)\n\n**Understanding Support Vector Machines**\n\n- blog: [https://blog.knoldus.com/2016/08/19/understanding-support-vector-machines/](https://blog.knoldus.com/2016/08/19/understanding-support-vector-machines/)\n\n**Multi-class SVM Loss**\n\n[http://www.pyimagesearch.com/2016/09/05/multi-class-svm-loss/](http://www.pyimagesearch.com/2016/09/05/multi-class-svm-loss/)\n\n**SVM versus a monkey. Make your bets.**\n\n[http://quantdare.com/2016/09/svm-versus-a-monkey/](http://quantdare.com/2016/09/svm-versus-a-monkey/)\n\n**Support Vector Machines: A Concise Technical Overview**\n\n[http://www.kdnuggets.com/2016/09/support-vector-machines-concise-technical-overview.html](http://www.kdnuggets.com/2016/09/support-vector-machines-concise-technical-overview.html)\n\n**Why use SVM?**\n\n[http://blog.yhat.com/posts/why-support-vector-machine.html](http://blog.yhat.com/posts/why-support-vector-machine.html)\n\n# Resources\n\n**Collection Of SVM Libraries By Language**\n\n[http://www.datasciencecentral.com/profiles/blogs/collection-of-svm-libraries-by-language](http://www.datasciencecentral.com/profiles/blogs/collection-of-svm-libraries-by-language)\n\n**A Simple Introduction to Support Vector Machines (Lecture for CSE 802)**\n\n- slides: [http://www.cise.ufl.edu/class/cis4930sp11dtm/notes/intro_svm_new.pdf](http://www.cise.ufl.edu/class/cis4930sp11dtm/notes/intro_svm_new.pdf)\n\n**16. Learning: Support Vector Machines**\n\n- youtube: [https://www.youtube.com/watch?v=_PwhiWxHK8o](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n\n# Projects\n\n**ThunderSVM: A Fast SVM Library on GPUs and CPUs**\n\n[https://github.com//zeyiwen/thundersvm](https://github.com//zeyiwen/thundersvm)\n\n**Support Vector Machine in Javascript**\n\n- demo: [http://cs.stanford.edu/people/karpathy/svmjs/demo/](http://cs.stanford.edu/people/karpathy/svmjs/demo/)\n- github: [https://github.com/karpathy/svmjs](https://github.com/karpathy/svmjs)\n\n**LIBIRWLS: A parallel IRWLS procedure to train full and semiparametric SVMs.**\n\n- homepage: [https://robedm.github.io/LIBIRWLS/](https://robedm.github.io/LIBIRWLS/)\n- github: [https://github.com/RobeDM/LIBIRWLS](https://github.com/RobeDM/LIBIRWLS)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","title":"Topic Model"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Topic Model\ndate: 2015-08-27\n---\n\n**Care and Feeding of Topic Models: Problems, Diagnostics, and Improvements**\n\n- book: [http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf](http://www.cs.colorado.edu/~jbg/docs/2014_book_chapter_care_and_feeding.pdf)\n\n**Introduction to Probabilistic Topic Models**\n\n- paper: [https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf](https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf)\n\n**Topic Modeling Bibliography**\n\n- blog: [http://qpleple.com/bib/](http://qpleple.com/bib/)\n\n**LDAvis: Interactive Visualization of Topic Models**\n\n- homepage: [https://cran.r-project.org/web/packages/LDAvis/](https://cran.r-project.org/web/packages/LDAvis/)\n- paper: [http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf](http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf)\n- github: [https://github.com/cpsievert/LDAvis](https://github.com/cpsievert/LDAvis)\n- tutorial: [http://cpsievert.github.io/LDAvis/reviews/reviews.html](http://cpsievert.github.io/LDAvis/reviews/reviews.html)\n\n**Deep Belief Nets for Topic Modeling**\n\n- arxiv: [http://arxiv.org/abs/1501.04325](http://arxiv.org/abs/1501.04325)\n\n**Visualizing Topic Models**\n\n- youtube: [https://www.youtube.com/watch?v=tGxW2BzC_DU&index=4&list=PLykRMO7ZuHwP5cWnbEmP_mUIVgzd5DZgH](https://www.youtube.com/watch?v=tGxW2BzC_DU&index=4&list=PLykRMO7ZuHwP5cWnbEmP_mUIVgzd5DZgH)\n\n**Spectral Learning for Supervised Topic Models**\n\n- arxiv: [http://arxiv.org/abs/1602.06025](http://arxiv.org/abs/1602.06025)\n\n**Beginners Guide to Topic Modeling in Python**\n\n- blog: [https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","title":"Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: mathematics\ntitle: Resources\ndate: 2016-02-24\n---\n\n**Packings of Regular Pentagons in the Plane**\n\n- intro: every packing of congruent regular pentagons in the Euclidean plane has density at most (5−5√)/3, which is about 0.92\n- arxiv: [http://arxiv.org/abs/1602.07220](http://arxiv.org/abs/1602.07220)\n\n**A Primer on Bézier Curves: A free, online book for when you really need to know how to do Bézier things.**\n\n[http://pomax.github.io/bezierinfo/](http://pomax.github.io/bezierinfo/)\n\n**Matrix Differentiation ( and some other stuff )**\n\n[http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n\n**MathBox: Tools for Thought: Graphical Algebra and Fourier Analysis**\n\n[https://acko.net/files/gltalks/toolsforthought/#0](https://acko.net/files/gltalks/toolsforthought/#0)\n\n**The Matrix Cookbook**\n\n[https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)\n\n**Central Limit Theorem**\n\n- intro: This is an attempt to visually explain the core concepts of the Central Limit Theorem\n- blog: [http://mfviz.com/central-limit/](http://mfviz.com/central-limit/)\n\n**Mathematics at Google**\n\n- intro: Javier Tordable\n- slides: [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/38331.pdf](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/38331.pdf)\n\n**An Interactive Guide To The Fourier Transform**\n\n- blog: [https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/](https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/)\n\n**Image Fourier Transform**\n\n- github: [https://github.com/turbomaze/JS-Fourier-Image-Analysis](https://github.com/turbomaze/JS-Fourier-Image-Analysis)\n- demo: [http://turbomaze.github.io/JS-Fourier-Image-Analysis/](http://turbomaze.github.io/JS-Fourier-Image-Analysis/)\n\n# Courses\n\n**Stanford EE261 - The Fourier Transform and its Applications**\n\n- homepage: [https://see.stanford.edu/Course/EE261](https://see.stanford.edu/Course/EE261)\n\n# Books\n\n**An Infinitely Large Napkin (Draft)**\n\n- intro: MIT\n- homepage: [http://www.mit.edu/~evanchen/napkin.html](http://www.mit.edu/~evanchen/napkin.html)\n- mirror: [https://pan.baidu.com/s/1i5oa53B](https://pan.baidu.com/s/1i5oa53B)\n\n# Projects\n\n**openai-gemm: Open single and half precision gemm implementations**\n\n- github: [https://github.com/openai/openai-gemm](https://github.com/openai/openai-gemm)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","title":"Programming Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Programming Resources\ndate: 2015-07-01\n---\n\n# Courses\n\n**Courses with Video Lectures**\n\n[http://cmlakhan.github.io/courses/videos.html](http://cmlakhan.github.io/courses/videos.html)\n\n**CMU 15-814: Types and Programming Languages**\n\n[http://www.cs.cmu.edu/~rwh/courses/typesys/](http://www.cs.cmu.edu/~rwh/courses/typesys/)\n\n**Introduction to DevOps**\n\n- intro: Learn how to make your application lifecycle faster and more predictable for both developers and the operations team.\n- homepage: [https://www.edx.org/course/introduction-devops-microsoft-dev212x](https://www.edx.org/course/introduction-devops-microsoft-dev212x)\n\n**Top Machine Learning MOOCs and Online Lectures: A Comprehensive Survey**\n\n- blog: [http://www.kdnuggets.com/2016/07/top-machine-learning-moocs-online-lectures.html](http://www.kdnuggets.com/2016/07/top-machine-learning-moocs-online-lectures.html)\n\n# Assembly\n\n **A fundamental introduction to x86 assembly programming**\n\n ![](https://www.nayuki.io/res/a-fundamental-introduction-to-x86-assembly-programming/cpu-model.png)\n\n- blog: [https://www.nayuki.io/page/a-fundamental-introduction-to-x86-assembly-programming](https://www.nayuki.io/page/a-fundamental-introduction-to-x86-assembly-programming)\n\n# C/C++\n\n**Templatized C++ Command Line Parser Library**\n\n- intro: TCLAP is a small, flexible library \nthat provides a simple interface for defining and accessing command line arguments\n- homepage: [http://tclap.sourceforge.net/](http://tclap.sourceforge.net/)\n- manual: [http://tclap.sourceforge.net/manual.html](http://tclap.sourceforge.net/manual.html)\n\n**C/C++ tip: How to get the process resident set size (physical memory use)**\n\n- blog: [http://nadeausoftware.com/articles/2012/07/c_c_tip_how_get_process_resident_set_size_physical_memory_use](http://nadeausoftware.com/articles/2012/07/c_c_tip_how_get_process_resident_set_size_physical_memory_use)\n\n**Adcanced Linux Programming**\n\n- book: [http://advancedlinuxprogramming.com/](http://advancedlinuxprogramming.com/)\n\n**The Ultimate Question of Programming, Refactoring, and Every**\n\n- intro: The book covers 42 recommendations about programming in C/C++. \nIt contains real examples with errors and the author gives explanations of how these bugs could be avoided. — Alex Astva\n- gitbook: [https://www.gitbook.com/book/alexastva/the-ultimate-question-of-programming-refactoring-/details](https://www.gitbook.com/book/alexastva/the-ultimate-question-of-programming-refactoring-/details)\n- pdf: [https://www.gitbook.com/download/pdf/book/alexastva/the-ultimate-question-of-programming-refactoring-](https://www.gitbook.com/download/pdf/book/alexastva/the-ultimate-question-of-programming-refactoring-)\n\n**openFrameworks: an open source C++ toolkit for creative coding.**\n\n- homepage: [http://openframeworks.cc/](http://openframeworks.cc/)\n\n**Awesome C++: A curated list of awesome C/C++ frameworks, libraries, resources, and shiny things**\n\n- homepage: [https://cpp.libhunt.com/](https://cpp.libhunt.com/)\n\n**shpp: Call c++ functions from a shell with any arguments of any types parsed automatically**\n\n![](https://cloud.githubusercontent.com/assets/17955551/17453191/c089d6c2-5b6f-11e6-9c1c-d5e094b270da.gif)\n\n- github: [https://github.com/GrossoMoreira/shpp](https://github.com/GrossoMoreira/shpp)\n\n**AwesomePerfCpp:  curated list of awesome C/C++ performance optimization resources: talks, articles, books, libraries, tools, sites, blogs**\n\n- homepage: [https://fenbf.github.io/AwesomePerfCpp/](https://fenbf.github.io/AwesomePerfCpp/)\n- github: [https://github.com/fenbf/AwesomePerfCpp](https://github.com/fenbf/AwesomePerfCpp)\n\n# CUDA\n\n**Implementing Run-length encoding in CUDA**\n\n- blog: [https://erkaman.github.io/posts/cuda_rle.html](https://erkaman.github.io/posts/cuda_rle.html)\n\n**Mixed-Precision Programming with CUDA 8**\n\n- blog: [https://devblogs.nvidia.com/parallelforall/mixed-precision-programming-cuda-8/](https://devblogs.nvidia.com/parallelforall/mixed-precision-programming-cuda-8/)\n\n# Docker\n\n**Docker for Beginners: A comprehensive tutorial on getting started with Docker!**\n\n- tutorial: [http://prakhar.me/docker-curriculum/](http://prakhar.me/docker-curriculum/)\n- github: [https://github.com/prakhar1989/docker-curriculum](https://github.com/prakhar1989/docker-curriculum)\n\n**Introduction to Docker (for Data Scientists)**\n\n- github: [https://github.com/jseabold/pydata-chi-docker](https://github.com/jseabold/pydata-chi-docker)\n\n# Git\n\n**Learn Git in 30 Minutes**\n\n- blog: [http://tutorialzine.com/2016/06/learn-git-in-30-minutes/](http://tutorialzine.com/2016/06/learn-git-in-30-minutes/)\n\n**Git diff tips and tricks**\n\n[https://blog.twobucks.co/git-diff-tips-and-tricks/](https://blog.twobucks.co/git-diff-tips-and-tricks/)\n\n**Git-Repo The ultimate utility for managing services**\n\n[http://i.got.nothing.to/code/on/git-repo:_the_utility_for_services/](http://i.got.nothing.to/code/on/git-repo:_the_utility_for_services/)\n\n**GitHub vs. Bitbucket vs. GitLab vs. Coding**\n\n![](https://cdn-images-1.medium.com/max/800/1*7HpNPipvf7O_92JrJYOmcA.png)\n\n[https://medium.com/flow-ci/github-vs-bitbucket-vs-gitlab-vs-coding-7cf2b43888a1#.9fk5ubn5n](https://medium.com/flow-ci/github-vs-bitbucket-vs-gitlab-vs-coding-7cf2b43888a1#.9fk5ubn5n)\n\n**Git as a NoSql database**\n\n- blog: [https://www.kenneth-truyers.net/2016/10/13/git-nosql-database/](https://www.kenneth-truyers.net/2016/10/13/git-nosql-database/)\n\n# Go\n\n**Go by Example**\n\n - blog: [https://gobyexample.com/](https://gobyexample.com/)\n\n# Java\n\n**Java Decompiler**\n\n[http://jd.benow.ca/](http://jd.benow.ca/)\n\n# JavaScript\n\n**JavaScript on Board: With Maps, Dynamic Arrays and Objects out of the box. Just plug in and start creating in seconds**\n\n[http://www.espruino.com/](http://www.espruino.com/)\n\n**Introduction to Functional Programming in JavaScript (Part 1)**\n\n[https://asep.co/introduction-to-functional-programming-in-javascript-part-1/?utm_source=hn&utm_medium=direct-share&utm_campaign=fp-in-javascript](https://asep.co/introduction-to-functional-programming-in-javascript-part-1/?utm_source=hn&utm_medium=direct-share&utm_campaign=fp-in-javascript)\n\n# Lisp\n\n**mal - Make a Lisp**\n\n![](https://raw.githubusercontent.com/kanaka/mal/master/process/stepA_mal.png)\n\n- intro: Mal is a learning tool, implemented in 56 languages\n- github: [https://github.com/kanaka/mal](https://github.com/kanaka/mal)\n- guide: [https://github.com/kanaka/mal/blob/master/process/guide.md](https://github.com/kanaka/mal/blob/master/process/guide.md)\n\n**Build Your Own Lisp: Learn C and build your own programming language in 1000 lines of code!**\n\n[http://www.buildyourownlisp.com/](http://www.buildyourownlisp.com/)\n\n# Math\n\n**Benchmarking matrix multiplication implementations**\n\n- intro: Naive / Transposed / sdot / SSE sdot / SSE+tiling sdot / OpenBLAS / uBLAS / Eigen\n- github: [https://github.com/attractivechaos/matmul](https://github.com/attractivechaos/matmul)\n\n**ISAAC: Input-aware BLAS for super high-performance Linear Algebra on CUDA and OpenCL**\n\n- github: [https://github.com/ptillet/isaac](https://github.com/ptillet/isaac)\n\n# Python\n\n**Python Computing for Data Science: An Undergraduate/Graduate Seminar Course at UC Berkeley (AY 250)**\n\n- github: [https://github.com/profjsb/python-seminar](https://github.com/profjsb/python-seminar)\n\n**Awesome Python: A curated list of awesome Python frameworks, libraries, software and resources**\n\n- github: [https://github.com/vinta/awesome-python](https://github.com/vinta/awesome-python)\n\n**Python 资源大全中文版**\n\n- intro: Python资源大全中文版，内容包括：Web框架、网络爬虫、网络内容提取、模板引擎、数据库、数据可视化、\n图片处理、文本处理、自然语言处理、机器学习、日志、代码分析等\n- github: [https://github.com/jobbole/awesome-python-cn](https://github.com/jobbole/awesome-python-cn)\n\n**live programming mode of Python Tutor**\n\n- intro: This is the live programming mode of Python Tutor, \nwhich continually runs and visualizes your code as you type\n- website: [http://pythontutor.com/live.html#mode=edit](http://pythontutor.com/live.html#mode=edit)\n\n**A collection of useful scripts, tutorials, and other Python-related things**\n\n- github: [https://github.com/rasbt/python_reference](https://github.com/rasbt/python_reference)\n\n**CME 193: Introduction to Scientific Python**\n\n- course page: [http://web.stanford.edu/~arbenson/cme193.html](http://web.stanford.edu/~arbenson/cme193.html)\n\n**Interesting Python Tutorials**\n\n- blog: [https://pythontips.com/2016/08/19/interesting-python-tutorials/](https://pythontips.com/2016/08/19/interesting-python-tutorials/)\n\n**PyInstaller**\n\n- intro: PyInstaller bundles a Python application and all its dependencies into a single package. \nThe user can run the packaged app without installing a Python interpreter or any modules.\n- website: [http://www.pyinstaller.org/](http://www.pyinstaller.org/)\n- docs: [http://pyinstaller.readthedocs.io/en/stable/](http://pyinstaller.readthedocs.io/en/stable/)\n- github: [https://github.com/pyinstaller/pyinstaller](https://github.com/pyinstaller/pyinstaller)\n\n**From Python to Numpy**\n\n- book: [http://www.labri.fr/perso/nrougier/from-python-to-numpy/](http://www.labri.fr/perso/nrougier/from-python-to-numpy/)\n- github: [https://github.com/rougier/from-python-to-numpy](https://github.com/rougier/from-python-to-numpy)\n\n**Numpy exercises.**\n\n- github: [https://github.com/Kyubyong/numpy_exercises](https://github.com/Kyubyong/numpy_exercises)\n\n**Python 2 vs Python 3: Practical Considerations**\n\n- blog: [https://www.digitalocean.com/community/tutorials/python-2-vs-python-3-practical-considerations-2](https://www.digitalocean.com/community/tutorials/python-2-vs-python-3-practical-considerations-2)\n\n**problem-solving-with-algorithms-and-data-structure-using-python 中文版**\n\n- github: [https://github.com/facert/python-data-structure-cn](https://github.com/facert/python-data-structure-cn)\n\n# Regulex\n\n**Regulex: JavaScript Regular Expression Visualizer**\n\n[https://jex.im/regulex/#!embed=false&flags=&re=%5E(a%7Cb)*%3F%24](https://jex.im/regulex/#!embed=false&flags=&re=%5E(a%7Cb)*%3F%24)\n\n**Demystifying The Regular Expression That Checks If A Number Is Prime**\n\n[https://iluxonchik.github.io/regular-expression-check-if-number-is-prime/](https://iluxonchik.github.io/regular-expression-check-if-number-is-prime/)\n\n## Visualization of Regulex\n\n[http://refiddle.com/](http://refiddle.com/)\n\n[https://regexper.com/](https://regexper.com/)\n\n[https://www.debuggex.com/](https://www.debuggex.com/)\n\n# Rust\n\n**RustPrimer**\n\n- github: [https://github.com/rustcc/RustPrimer](https://github.com/rustcc/RustPrimer)\n\n**intellij-rust: Rust plugin for IntelliJ IDEA**\n\n[https://intellij-rust.github.io/](https://intellij-rust.github.io/)\n\n\n# Shell\n\n**ShellCheck - A shell script static analysis tool**\n\n- intro: ShellCheck is a GPLv3 tool that gives warnings and suggestions for bash/sh shell script\n- github: [https://github.com/koalaman/shellcheck](https://github.com/koalaman/shellcheck)\n\n# Vim\n\n**Things About Vim I Wish I Knew Earlier**\n\n- blog: [http://blog.petrzemek.net/2016/04/06/things-about-vim-i-wish-i-knew-earlier/](http://blog.petrzemek.net/2016/04/06/things-about-vim-i-wish-i-knew-earlier/)\n\n**PacVim**\n\n![](https://raw.githubusercontent.com/jmoon018/PacVim/master/gifs/all.gif)\n\n- intro: PacVim is a game that teaches you vim commands. \nYou must move pacman (the green cursor) to highlight each word on the gameboard while avoiding the ghosts (in red).\n- github: [https://github.com/jmoon018/PacVim](https://github.com/jmoon018/PacVim)\n\n**Turning vim into an IDE through vim plugins**\n\n[https://www.safaribooksonline.com/blog/2014/11/23/way-vim-ide/](https://www.safaribooksonline.com/blog/2014/11/23/way-vim-ide/)\n\n## Powerful VIM config on Github\n\n**spf13-vim: The Ultimate Vim Distribution**\n\n![(http://i.imgur.com/kZWj1.png)]\n\n- homepage: [http://vim.spf13.com/](http://vim.spf13.com/)\n- github: [https://github.com/spf13/spf13-vim](https://github.com/spf13/spf13-vim)\n\n**dot-vimrc: Maple's vim config files**\n\n- github: [https://github.com/humiaozuzu/dot-vimrc](https://github.com/humiaozuzu/dot-vimrc)\n\n**vimrc: The Ultimate vimrc**\n\n- github: [https://github.com/amix/vimrc](https://github.com/amix/vimrc)\n\n# Reverse Engineering\n\n**Reverse Engineering: Cracking Sublime Text 3**\n\n- blog: [http://blog.fernandodominguez.me/cracking-sublime-text-3/](http://blog.fernandodominguez.me/cracking-sublime-text-3/)\n\n# Tmux\n\n**A Tmux crash course: tips and tweaks.**\n\n- blog: [http://tangosource.com/blog/a-tmux-crash-course-tips-and-tweaks/](http://tangosource.com/blog/a-tmux-crash-course-tips-and-tweaks/)\n\n# Coding\n\n**Awesome Creative Coding**\n\n- github: [https://github.com/terkelg/awesome-creative-coding](https://github.com/terkelg/awesome-creative-coding)\n\n**Top 10 coding challenges Websites**\n\n- blog: [http://infocreeds.com/top-10-coding-challenges-websites/](http://infocreeds.com/top-10-coding-challenges-websites/)\n\n**Hackerrank - A great collection of questions to solidify your programming skills**\n\n[https://www.hackerrank.com/domains](https://www.hackerrank.com/domains)\n\n**Python / C++ 11 Solutions of All 418 LeetCode Questions**\n\n- github: [https://github.com/kamyu104/LeetCode](https://github.com/kamyu104/LeetCode)\n\n**OpenGrok - a wicked fast source browser**\n\n- homepage: [http://opengrok.github.io/OpenGrok/](http://opengrok.github.io/OpenGrok/)\n- github: [https://github.com/OpenGrok/OpenGrok](https://github.com/OpenGrok/OpenGrok)\n\n## Code of Honor Series\n\n**Tough times on the road to Starcraft**\n\n- blog: [http://www.codeofhonor.com/blog/tough-times-on-the-road-to-starcraft](http://www.codeofhonor.com/blog/tough-times-on-the-road-to-starcraft)\n\n**StarCraft: Orcs in space go down in flames**\n\n- blog: [http://www.codeofhonor.com/blog/starcraft-orcs-in-space-go-down-in-flames](http://www.codeofhonor.com/blog/starcraft-orcs-in-space-go-down-in-flames)\n\n**The StarCraft path-finding hack**\n\n- blog: [](http://www.codeofhonor.com/blog/the-starcraft-path-finding-hack)\n\n**The making of Warcraft**\n\n- part 1: [http://www.codeofhonor.com/blog/the-making-of-warcraft-part-1](http://www.codeofhonor.com/blog/the-making-of-warcraft-part-1)\n- part 2: [http://www.codeofhonor.com/blog/the-making-of-warcraft-part-2](http://www.codeofhonor.com/blog/the-making-of-warcraft-part-2)\n- part 3: [http://www.codeofhonor.com/blog/the-making-of-warcraft-part-3](http://www.codeofhonor.com/blog/the-making-of-warcraft-part-3)\n\n## Music and Coding\n\n![](/assets/programming_study/blogs/Lifestyle.jpg)\n\n**Music and Coding Part 1: Why listen while you work?**\n\n[https://blog.idrsolutions.com/2014/07/music-coding-part-1-listen-work/](https://blog.idrsolutions.com/2014/07/music-coding-part-1-listen-work/)\n\n**Music and Coding Part 2: What to listen to?**\n\n[https://blog.idrsolutions.com/2014/08/music-and-coding-part-2-listen/](https://blog.idrsolutions.com/2014/08/music-and-coding-part-2-listen/)\n\n**Music and Coding Part 3: How to listen to music**\n\n[https://blog.idrsolutions.com/2014/08/music-and-coding-part-3-listen-music/](https://blog.idrsolutions.com/2014/08/music-and-coding-part-3-listen-music/)\n\n**Programming Languages Used for Music**\n\n- blog: [http://www.nosuch.com/plum/cgi/showlist.cgi?sort=name&concise=yes](http://www.nosuch.com/plum/cgi/showlist.cgi?sort=name&concise=yes)\n\n# Debugging\n\n**Debugging with GDB: a real life example**\n\n- blog: [http://blog.0x972.info/?d=2015/09/09/09/19/14-debugging-with-gdb-a-real-life-example](http://blog.0x972.info/?d=2015/09/09/09/19/14-debugging-with-gdb-a-real-life-example)\n\n**Tips for Productive Debugging with GDB**\n\n![](https://metricpanda.com/assets/gdb-dashboard-a3babff3749376737a532bf1eea9b170ef8a8ea20fbda85645f41554e5dc01e7.png)\n\n- blog: [https://metricpanda.com/tips-for-productive-debugging-with-gdb](https://metricpanda.com/tips-for-productive-debugging-with-gdb)\n\n**A browser-based frontend/gui for GDB**\n\n![](https://raw.githubusercontent.com/cs01/gdbgui/master/screenshots/gdbgui.png)\n\n- intro: A modern, browser-based frontend to gdb (gnu debugger). \nAdd breakpoints, view stack traces, and more in C, C++, Go, and Rust! \nSimply run gdbgui from the terminal and a new tab will open in your browser.\n- github: [https://github.com/cs01/gdbgui](https://github.com/cs01/gdbgui)\n\n**GDB基础 - 张银奎**\n\n- slides: [http://001001.org/gedu/06_LINUX_Raymond_GDB.pdf](http://001001.org/gedu/06_LINUX_Raymond_GDB.pdf)\n\n# Programming\n\n**Why is Object-Oriented Programming Useful? (With a Role Playing Game Example)**\n[http://inventwithpython.com/blog/2014/12/02/why-is-object-oriented-programming-useful-with-an-role-playing-game-example](http://inventwithpython.com/blog/2014/12/02/why-is-object-oriented-programming-useful-with-an-role-playing-game-example)\n\n**Scraping NBA Play-by-Play Data with Scrapy & MongoDB**\n\n[http://blog.nycdatascience.com/students-work/scraping-nba-play-by-play-data-with-scrapy-mongodb/](http://blog.nycdatascience.com/students-work/scraping-nba-play-by-play-data-with-scrapy-mongodb/)\n\n**15 Sites for Programming Exercises**\n\n- blog: [http://programmingzen.com/15-sites-for-programming-exercises/](http://programmingzen.com/15-sites-for-programming-exercises/)\n\n**Eliminate null-checks using arrays**\n\n- blog: [http://firstclassthoughts.co.uk/Articles/Readability/NullCheckEliminationUsingArrays.html](http://firstclassthoughts.co.uk/Articles/Readability/NullCheckEliminationUsingArrays.html)\n\n## Game AI Programming\n\n**The AI Programmer's Bookshelf: A list of useful books for game AI programming.**\n\n- intro: Jeff Orkin, MIT Media Lab\n- homepage: [http://alumni.media.mit.edu/~jorkin/aibooks.html](http://alumni.media.mit.edu/~jorkin/aibooks.html)\n\n**GENERAL EXAMS READING LIST**\n\n[http://alumni.media.mit.edu/~jorkin//generals/general_exams.html](http://alumni.media.mit.edu/~jorkin//generals/general_exams.html)\n\n**UAlbertaBot - StarCraft AI Competition Bot**\n\n[https://github.com/davechurchill/ualbertabot](https://github.com/davechurchill/ualbertabot)\n\n# Software Development\n\n**Professional Software Development: For Students**\n\n- book: [http://mixmastamyk.bitbucket.org/pro_soft_dev/](http://mixmastamyk.bitbucket.org/pro_soft_dev/)\n\n**Software Engineering at Google**\n\n- intro: catalog and describe Google's key software engineering practices.\n- arxiv: [https://arxiv.org/abs/1702.01715](https://arxiv.org/abs/1702.01715)\n\n# Codes\n\n**The Archive of Interesting Code**\n\n- intro: 95 entries\n- author: by Keith Schwarz, Stanford CS166.\n- blog: [http://www.keithschwarz.com/interesting/](http://www.keithschwarz.com/interesting/)\n\n**Dijkstra Cartography**\n\n![](https://github.com/ibaaj/dijkstra-cartography/blob/master/results/SD/paris.png)\n\n- intro: Using Dijkstra's algorithm (\"finding the shortest paths between nodes in a graph\") to draw maps\n- github: [https://github.com/ibaaj/dijkstra-cartography](https://github.com/ibaaj/dijkstra-cartography)\n\n# Interesting Projects\n\n## Game\n\n**(Update: this repo has been taken down by Blizzard) StarCraft: HTML5 version for StarCraft game**\n\n![](/assets/programming_study/Starcraft_Demo.jpg)\n\n![](/assets/programming_study/Starcraft_Demo.gif)\n\n- project: [https://github.com/gloomyson/StarCraft](https://github.com/gloomyson/StarCraft)\n- news: [https://torrentfreak.com/blizzard-nukes-popular-html5-version-of-starcraft-game-151010/](https://torrentfreak.com/blizzard-nukes-popular-html5-version-of-starcraft-game-151010/)\n- news: [http://thenextweb.com/apps/2015/09/07/starcraft-now-works-in-the-browser-and-its-amazing/](http://thenextweb.com/apps/2015/09/07/starcraft-now-works-in-the-browser-and-its-amazing/)\n\n**The Brood War API**\n\n- project: [https://github.com/bwapi/bwapi](https://github.com/bwapi/bwapi)\n\n**CodeCombat: Multiplayer programming game for learning how to code**\n\n![](/assets/programming_study/CodeCombat_demo.png)\n\n![](/assets/programming_study/CodeCombat_example1.jpg)\n\n![](/assets/programming_study/CodeCombat_example2.jpg)\n\n- github: [https://github.com/codecombat/codecombat](https://github.com/codecombat/codecombat)\n- homepage: [http://codecombat.com/](http://codecombat.com/)\n- wiki: [https://github.com/codecombat/codecombat/wiki/Archmage-Home](https://github.com/codecombat/codecombat/wiki/Archmage-Home)\n\n**BrowserQuest: a HTML5/JavaScript multiplayer game experiment**\n\n![](/assets/programming_study/BrowserQuest_demo.jpg)\n\n- github: [https://github.com/mozilla/BrowserQuest](https://github.com/mozilla/BrowserQuest)\n- homepage: [http://browserquest.mozilla.org/](http://browserquest.mozilla.org/)\n\n**2048-AI**\n\n![](/assets/programming_study/2048.png)\n\n- demo: [http://ovolve.github.io/2048-AI/](http://ovolve.github.io/2048-AI/)\n- github: [https://github.com/ovolve/2048-AI](https://github.com/ovolve/2048-AI)\n- stackoverflow: [http://stackoverflow.com/questions/22342854/what-is-the-optimal-algorithm-for-the-game-2048](http://stackoverflow.com/questions/22342854/what-is-the-optimal-algorithm-for-the-game-2048)\n- zh-blog: [http://blogread.cn/it/article/6827?f=wb](http://blogread.cn/it/article/6827?f=wb)\n\n**The Berkeley Overmind Project**\n\n- intro: \"The Overmind is an agent for playing StarCraft®: Brood War®, using [BWAPI](https://code.google.com/p/bwapi/).\"\n- homepage: [http://overmind.cs.berkeley.edu/](http://overmind.cs.berkeley.edu/)\n\n**Spaceship.codes - A Game for Programmers**\n\n!()[https://fotino.me/content/images/2016/04/thrust-1.png]\n\n- intro: A programming game in which the user writes JavaScript code to \ncontrol a spaceship in order to complete some objective.\n[https://fotino.me/intro-to-space-ai/](https://fotino.me/intro-to-space-ai/)\n[https://github.com/rfotino/space-ai](https://github.com/rfotino/space-ai)\n\n**SnapSudoku: Extract and solve Sudoku from an image**\n\n![](https://camo.githubusercontent.com/b48905b57f007218604c23fdc244f9d876f67af4/68747470733a2f2f6c68332e676f6f676c6575736572636f6e74656e742e636f6d2f2d6854504e346d53444e69592f5677793855675463784e492f41414141414141414731632f6536376745395453414b517263642d4144486d41674f74754d4451506879437267434c63422f733530302f41667465722b50726570726f63657373696e672e706e67)\n\n- github: [https://github.com/prajwalkr/SnapSudoku](https://github.com/prajwalkr/SnapSudoku)\n\n**TerraLegion**\n\n![](https://camo.githubusercontent.com/9c5c7bfd83a5448b6e1a2c0f55b194b9694ed9a6/687474703a2f2f692e696d6775722e636f6d2f6d36516342676a2e706e67)\n\n- github: [https://github.com/jmrapp1/TerraLegion](https://github.com/jmrapp1/TerraLegion)\n\n## Editing\n\n**Latex to HTML5**\n\n- project: [https://github.com/smarr/latex-to-html5](https://github.com/smarr/latex-to-html5)\n\n**LaTeX handwritten symbol recognition: Detexify Backend Server implemented in Haskell**\n\n- demo: [http://detexify.kirelabs.org/classify.html](http://detexify.kirelabs.org/classify.html)\n- github: [https://github.com/kirel/detexify-hs-backend](https://github.com/kirel/detexify-hs-backend)\n\n**Enhancements for The official editor for Code in the Dark**\n\n![](https://cloud.githubusercontent.com/assets/688415/11440971/aadfae8e-9507-11e5-8aa0-0ecc87ca84b6.gif)\n\n- github: [https://github.com/codeinthedark/editor/pull/1)\n\n**activate-power-mode atom package**\n\n![](https://i.github-camo.com/b1d03b9b7a9d7dc9a32d1eab307b5378f8c59a7b/68747470733a2f2f636c6f75642e67697468756275736572636f6e74656e742e636f6d2f6173736574732f3638383431352f31313631353536352f31306631363435362d396336352d313165352d386166342d3236356630316663383361302e676966)\n\n- package: [https://atom.io/packages/activate-power-mode](https://atom.io/packages/activate-power-mode)\n\n**ActivatePowerMode: plugin for Xcode**\n\n![](https://raw.githubusercontent.com/poboke/ActivatePowerMode/master/Screenshots/about.gif)\n\n- github: [https://github.com/poboke/ActivatePowerMode](https://github.com/poboke/ActivatePowerMode)\n\n**Texter**\n\n- intro: Texter is a little javascript experiment that lets you explore your creativity by drawing with words. \n- demo: [http://tholman.com/texter/](http://tholman.com/texter/)\n- github: [https://github.com/tholman/texter](https://github.com/tholman/texter)\n\n**PDFMiner: a tool for extracting information from PDF documents**\n\n- github: [https://github.com/euske/pdfminer/](https://github.com/euske/pdfminer/)\n\n**A business card in LaTeX.**\n\n- github: [https://github.com/opieters/business-card](https://github.com/opieters/business-card)\n\n## Painting\n\n**polygen: genetic algorithm for approximating an image with polygons (Golang)**\n\n![](https://raw.githubusercontent.com/armhold/polygen/master/images/starry.jpg)\n![](https://raw.githubusercontent.com/armhold/polygen/master/images/starry-50-polygons.png)\n\n- github: [https://github.com/armhold/polygen](https://github.com/armhold/polygen)\n\n**PyGraphArt: Some Python code to make GIFs from graph algos**\n\n![](https://raw.githubusercontent.com/dnlcrl/PyGraphArt/master/doc/bfs.gif)\n\n- github: [https://github.com/dnlcrl/PyGraphArt](https://github.com/dnlcrl/PyGraphArt)\n\n**WaveFunctionCollapse: Bitmap & tilemap generation from a single example with the help of ideas from quantum mechanics.**\n\n![](https://camo.githubusercontent.com/3ea3b800ee9de783abea49612a40beeda97879de/687474703a2f2f692e696d6775722e636f6d2f67317947764c372e706e67)\n\n- github: [https://github.com/mxgmn/WaveFunctionCollapse](https://github.com/mxgmn/WaveFunctionCollapse)\n\n## Utilities\n\n**Python Gems: A collection of python scripts that solve interesting problems**\n\n- github: [https://github.com/RealHacker/python-gems](https://github.com/RealHacker/python-gems)\n\n**nude.js: Nudity detection with JavaScript and HTMLCanvas**\n\n- website: [http://www.patrick-wied.at/static/nudejs/](http://www.patrick-wied.at/static/nudejs/)\n- github: [https://github.com/pa7/nude.js](https://github.com/pa7/nude.js)\n\n**Hastily - A downloader of subtitles(Haskell)**\n\n[https://hackage.haskell.org/package/hastily](https://hackage.haskell.org/package/hastily)\n\n**nba-cli: Get NBA Data From The Command Line**\n\n![](https://camo.githubusercontent.com/782c2fcbc5de96d54ac8a2a866503c96f16a0d80/68747470733a2f2f692e696d6775722e636f6d2f364f43595239532e706e67)\n\n- github: [https://github.com/jaebradley/nba-cli](https://github.com/jaebradley/nba-cli)\n\n**Octotree: Code tree for GitHub and GitLab**\n\n![](https://raw.githubusercontent.com/buunguyen/octotree/master/docs/chrome-github.png)\n\n- github: [https://github.com/buunguyen/octotree](https://github.com/buunguyen/octotree)\n\n**coursera-dl-all**\n\n![](https://camo.githubusercontent.com/7bc2e3e4ba0fbc9f04d46fd136049cd682e27d82/687474703a2f2f7777772e696d6775722e636f6d2f485464303238422e706e67)\n\n- github: [https://github.com/Chillee/coursera-dl-all](https://github.com/Chillee/coursera-dl-all)\n\n**gifify: Convert any video file to an optimized animated GIF.**\n\n![](https://raw.githubusercontent.com/vvo/gifify/master/screencast.gif)\n\n- intro: Convert any video file to an optimized animated GIF. Either in its full length or only a part of it.\n- github: [https://github.com/vvo/gifify](https://github.com/vvo/gifify)\n\n**pdf-diff: A PDF comparison utility in Python**\n\n![](https://raw.githubusercontent.com/JoshData/pdf-diff/master/example.png)\n\n[https://github.com/JoshData/pdf-diff](https://github.com/JoshData/pdf-diff)\n\n**Web Image Downloader Tools**\n\n- intro: Python utilities for automated download of images from various web sources\n- github: [https://github.com/kencoken/imsearch-tools](https://github.com/kencoken/imsearch-tools)\n\n## Multi-media\n\n**cmus — C* Music Player**\n\n![](https://cmus.github.io/cmus-2.4.3-osx.png)\n\n- website: [https://cmus.github.io/]\n- github: [https://github.com/cmus/cmus](https://github.com/cmus/cmus)\n\n**a2mp3 - anything2mp3**\n\n- github: [https://github.com/apfeltee/a2mp3](https://github.com/apfeltee/a2mp3)\n\n**eyeD3**\n\n- intro: a Python tool for working with audio files, specifically mp3 files containing ID3 metadata (i.e. song info).\n- homepage: [http://eyed3.nicfit.net/](http://eyed3.nicfit.net/)\n- installation: [http://eyed3.nicfit.net/installation.html](http://eyed3.nicfit.net/installation.html)\n\n## Algorithm\n\n**Lean: a new open source theorem prover**\n\n- intro: Microsoft Research\n- website: [http://leanprover.github.io/](http://leanprover.github.io/)\n\n## Technology\n\n**RF-Capture: Capturing the Human Figure Through a Wall**\n\n![](http://rfcapture.csail.mit.edu/images/intro-figures.jpg)\n\n- project: [http://rfcapture.csail.mit.edu/](http://rfcapture.csail.mit.edu/)\n- paper: [http://rfcapture.csail.mit.edu/rfcapture-paper.pdf](http://rfcapture.csail.mit.edu/rfcapture-paper.pdf)\n\n**Stealing Keys from PCs using a Radio: Cheap Electromagnetic Attacks on Windowed Exponentiation**\n\n- homepage: [https://www.tau.ac.il/~tromer/radioexp/](https://www.tau.ac.il/~tromer/radioexp/)\n\n**A Windows 95 Simulator on Browser**\n\n[https://win95.ajf.me/win95.html](https://win95.ajf.me/win95.html)\n\n**eviltransform: Transform coordinate between earth(WGS-84) and mars in china(GCJ-02)**\n\n- github: [https://github.com/googollee/eviltransform](https://github.com/googollee/eviltransform)\n- blog(\"Why You Can't Trust GPS in China\"): [http://www.travelandleisure.com/articles/digital-maps-skewed-china](http://www.travelandleisure.com/articles/digital-maps-skewed-china)\n\n**Apollo-11: Original Apollo 11 Guidance Computer (AGC) source code for the command and lunar modules**\n\n- github: [https://github.com/chrislgarry/Apollo-11](https://github.com/chrislgarry/Apollo-11)\n\n## Others\n\n**Module 7: Introduction to D3.js**\n\n- github: [https://github.com/INFO-474/m7-d3-intro](https://github.com/INFO-474/m7-d3-intro)\n\n# Resources\n\n**24 Data Science, R, Python, Excel, and Machine Learning Cheat Sheets**\n\n- blog: [http://www.datasciencecentral.com/profiles/blogs/20-data-science-r-python-excel-and-machine-learning-cheat-sheets](http://www.datasciencecentral.com/profiles/blogs/20-data-science-r-python-excel-and-machine-learning-cheat-sheets)\n\n**Reading for graduate students**\n\n[http://matt.might.net/articles/books-papers-materials-for-graduate-students/](http://matt.might.net/articles/books-papers-materials-for-graduate-students/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","title":"Embedding Python In C/C++"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Embedding Python In C/C++\ndate: 2015-10-10\n---\n\n# Preparatory Work\n\nCopy all necessary Python files to your project directory. It would be convenient that your program could still work even if other people's computer doesn't install a Python toolkit.\n\nWhat We need:\n\n(1) \"C:\\\\Python27\\\\include\". This directory will be added to VS-C++\n\n(2) \"C:\\\\Python27\\\\lib\". This directory will be added to\n\n(3) Copy all files in \"C:\\\\Python27\\\\DLLS\" and \"C:\\\\Python27\\\\Lib\" to one directory, e.g: \"Python27\". This is gonna be a huge directory..but you can remove many packages that you don't actually need.\n\nSo finally my project structure just as below:\n\n# Coding Work\n\nNow that we created a huge Python27 directory containing all packages, we can directly start our coding work by:\n\n{% highlight python %}\nfrom time import time\ntime.print()\n{% endhighlight %}\n\nHowever if you try to import those 3rd-party packages, such as numpy/cv2, your program will crash without any warning. Well, you probably already installed those packages into \"C:\\\\Python27\\\\Lib\\\\site-packages\"(now what we got is a Python27 directory, so it is \"Python27\\\\site-packages\" in your project directory). Before import any package, you should add one more line ahead:\n\n{% highlight python %}\nimport cv2\n{% endhighlight %}\n\nNow you can import cv2 and manipulate images successfully!\n\n# Hard Wording Remainded\n\nIt would be a frustrated if you've written a lot of codes but the program can not run - it just crash, without any warning. Debugging would be a nightmare.\n\n# Reference\n\n(1) \"在 C++ 程序中嵌入 Python 脚本\": [http://www.yangyuan.info/post.php?id=1071](http://www.yangyuan.info/post.php?id=1071)\n\n(2) \"Embedding Python in C/C++: Part I\": [http://www.codeproject.com/Articles/11805/Embedding-Python-in-C-C-Part-I](http://www.codeproject.com/Articles/11805/Embedding-Python-in-C-C-Part-I)\n\n(3) \"Embedding Python in C/C++: Part II\": [http://www.codeproject.com/Articles/11843/Embedding-Python-in-C-C-Part-II](http://www.codeproject.com/Articles/11843/Embedding-Python-in-C-C-Part-II)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","title":"Horrible Wired Errors Come From Simple Stupid Mistake"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Horrible Wired Errors Come From Simple Stupid Mistake\ndate: 2015-10-16\n---\n\nSeveral days ago I was transplanting some codes from Linux to Windows x86 platform.\n\nThe VS 2010 project worked fine on Windows server; But when I tried to re-run the program on my PC, many wired errors occurred: run-time errors, memory leaks, and many unexplainable exceptions.\n\nAfter many aimless, exhausted searching on Google, I started to think about giving up. I opened up the module/call stack windows of Visual Studio, compared those with Windows server's version. And I found something strange: My program was unexpectedly calling msvcp120.dll! I suddenly realized that I should check the entire calling stack of the program.\n\nI launched one utility software: Dependency Walker (depends.exe), loaded my program, and finally, everything was clear: the program depended on one 3rd-party .dll file which has been put into target directory. But in fact it was calling the other same name .dll file on my PC's system directory: \"C:/Windows\". All this shit-staff - I spent several days to debug and google, wasted - resulted from my one stupid operation, dated back almost ten months ago. I was self-studying some tutorials about Deep Learning, CNN staff, meanwhile I built some open-source programs to generate some 3rd-party .dll files. I put one .dll to \"C:/Windows\" - this careless mistake finally led to all this nasty, unintended consequences.\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","title":"Gflags Build Problems on Windows X86 and Visual Studio 2015"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Gflags Build Problems on Windows X86 and Visual Studio 2015\ndate: 2015-10-23\n---\n\n# Gflags Build Problems on Windows X86 and Visual Studio 2015\n\nError:\n\ngflags.lib(gflags.obj) : error LNK2001: unresolved external symbol __imp__PathMatchSpecA@8\n\nResolve:\n\nAdd \"shlwapi.lib\" to \"Project - Property - Linker - Input - Additional Dependencies\".\n\nReference:\n\n[https://groups.google.com/forum/#!topic/google-gflags/cM4DuGOS_GI](https://groups.google.com/forum/#!topic/google-gflags/cM4DuGOS_GI)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","title":"Glog Build Problems on Windows X86 and Visual Studio 2015"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Glog Build Problems on Windows X86 and Visual Studio 2015\ndate: 2015-10-23\n---\n\nI git clone glog from [https://github.com/google/glog](https://github.com/google/glog).\n\nSystem info: Windows X86, Visual Studio 2015.\n\nTwo errors:\n\n(1)\n\n```\nglog-0.3.4\\src\\windows\\port.h(116): error C2375: 'snprintf': redefinition; different linkage\nc:\\program files\\windows kits\\10\\include\\10.0.10150.0\\ucrt\\stdio.h(1932): note: see declaration of 'snprintf'\n```\n\nResolve:\n\nAdd \"HAVE_SNPRINTF\" to \"C/C++ - Preprocessor - Preprocessor definitions\".\n\n(2)\n\n```\nglog-0.3.4\\src\\windows\\glog\\logging.h(1268): error C2280: 'std::basic_ios<char,std::char_traits<char>>::basic_ios(const std::basic_ios<char,std::char_traits<char>> &)': attempting to reference a deleted function  <br />\nc:\\program files\\microsoft visual studio 14.0\\vc\\include\\ios(189): note: see declaration of 'std::basic_ios<char,std::char_traits<char>>::basic_ios'\n```\n\nResolve:\n\nFollow this modification:\n\n[https://github.com/google/glog/commit/856ff81a8268a5c22d026a65d4c12a2e1136f73f](https://github.com/google/glog/commit/856ff81a8268a5c22d026a65d4c12a2e1136f73f)\n\n(3)\n\n```\ncommon.obj : error LNK2001: unresolved external symbol \"__declspec(dllimport) void __cdecl google::InstallFailureSignalHandler(void)\" (__imp_?InstallFailureSignalHandler@google@@YAXXZ)\n```\n\nResolve:\n\nThis function appears in \"glog-0.3.4\\src\\signalhandler.cc\". But can just comment out target line:\n\n```\n::google::InstallFailureSignalHandler();\n```\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","title":"Commands To Suppress Some Building Errors With Visual Studio"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Commands To Suppress Some Building Errors With Visual Studio\ndate: 2015-10-24\n---\n\nHere are some commands you would probably frequently use when you're building Linux codes with VS2013~VS2015. Go to \"C/C++ - Project - Properties - Additional Options\", add following commands(each command separated by one blank):\n\n(1) **/D _CRT_SECURE_NO_WARNINGS**:  to suppress warnings:\n\nerror C4996: 'strcpy': This function or variable may be unsafe. Consider using strcpy_s instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See online help for details.\n\n(2) **/D _SCL_SECURE_NO_WARNINGS**:\n\nc:\\program files\\microsoft visual studio 14.0\\vc\\include\\xutility(2230): error C4996: 'std::_Copy_impl': Function call with parameters that may be unsafe - this call relies on the caller to check that the passed values are correct. To disable this warning, use -D_SCL_SECURE_NO_WARNINGS. See documentation on how to use Visual C++ 'Checked Iterators'\n\n(3) **/D _CRT_NONSTDC_NO_DEPRECATE**: to suppress warnings:\n\nerror C4996: 'close': The POSIX name for this item is deprecated. Instead, use the ISO C and C++ conformant name: _close. See online help for details.\n\n(4) **/D _SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS**\n\nc:\\program files (x86)\\microsoft visual studio 14.0\\vc\\include\\hash_map(17): \nerror C2338: <hash_map> is deprecated and will be REMOVED. Please use <unordered_map>. \nYou can define _SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS to acknowledge that you have received this warning."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","title":"Install Jekyll To Fix Some Local Github-pages Defects"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ntitle: Install Jekyll To Fix Some Local Github-pages Defects\ndate: 2015-11-21 00:02:26\ncategory: \"web_dev\"\n---\n\n# Install jekyll\n\nI follow the blog: [http://blog.csdn.net/itmyhome1990/article/details/41982625](http://blog.csdn.net/itmyhome1990/article/details/41982625) \nto install Ruby, Devkit, and Jekyll.\n\n1. Download Ruby and DevKit: [http://rubyinstaller.org/downloads/](http://rubyinstaller.org/downloads/)\n\n2. Check \"Add Ruby executables to your PATH\" when installing Ruby. You can execute:\n\n<pre class=\"terminal\"><code>ruby -v</code></pre>\n\nto detect if Ruby successfully installed.\n\n3. Install DevKit. After that, cd to RubyDevKit directory:\n\n```\nC:\\> cd RubyDevKit\nC:\\RubyDevKit> ruby dk.rb init\nC:\\RubyDevKit> ruby dk.rb install\n```\n\n# Intall github-pages\n\nWhen I try to install github-pages by \"gem install github-pages\", an error(FetchError) is encountered: \n\n![](/assets/web_dev/fetch_error.PNG)\n\nSeems like it is because the Ruby website is blocked. So I follow the instructions by @fighterleslie in \n[http://segmentfault.com/q/1010000003891086](http://segmentfault.com/q/1010000003891086), create a .gemrc \nfile into \"C:\\Users\\MyName\", and problem solved:\n\n```\n:sources:\n- https://ruby.taobao.org\n:update_sources: true\n```\n\nYou may still encounter this FetchError:\n\n![](/assets/web_dev/fetch_error_2.PNG)\n\n![](/assets/web_dev/fetch_error_3.png)\n\nI found a workaround here (Really a \"workaround\". It opens a possible security vulnerability):\nadd the line to your .gemrc file:\n\n```\n:ssl_verify_mode: 0\n```\n\nSo now my .gemrc file is like:\n\n```\n:ssl_verify_mode: 0\n:sources:\n- https://ruby.taobao.org\n:update_sources: true\n```\n\nAnyway, now jekyll is successfully installed on my system.\n\n# Try jekyll build!\n\nFollow [http://rockhong.github.io/github-pages-fails-to-update.html](http://rockhong.github.io/github-pages-fails-to-update.html)\nto detect my github-pages defects.\n\nOK, try the instruction below:\n\n<pre class=\"terminal\">\n<code>$ jekyll build --safe</code>\n</pre>\n\nThen I get:\n\n![](/assets/web_dev/jekyll_build_reuslts.png)\n\nFollow the error information, do some minor changes, and finally my github-pages can successfully be shown.\n\nDon't miss the chance to use jekyll to preview your site! Running the line:\n\n```\njekyll serve –watch\n```\n\nand then open website `http://localhost:4000/` in browser.\nIf want to open a specific page, use `http://localhost:4000/web_dev/2015/11/21/install-jekyll.html`.\n\n# Something else to note..\n\nThe above-mentioned instructions just work fine for me on my laptop(Windows 8.1, X64). \nBut some other errors may happen, like Cygwin and Windows git can't play nicely \ntogether(on my work PC, Windows 7, X32, with Cygwin installed).\nOne particulr error message is like:\n\n![](/assets/web_dev/gem_install_github-pages_cygwin_error.jpg)\n\nWhen I try to execute a \"gem install github-pages\".\n\nI found two posts helpful:\n\n[http://stackoverflow.com/questions/19259272/error-installing-gem-couldnt-reserve-space-for-cygwins-heap-win32-error-487](http://stackoverflow.com/questions/19259272/error-installing-gem-couldnt-reserve-space-for-cygwins-heap-win32-error-487)\n\n[http://blog.arithm.com/2014/02/14/couldnt-reserve-space-for-cygwins-heap-win32-error-0/](http://blog.arithm.com/2014/02/14/couldnt-reserve-space-for-cygwins-heap-win32-error-0/)\n\nIn a nutshell, I download rebase-1.18-1.exe from [http://www.tishler.net/jason/software/rebase/](http://www.tishler.net/jason/software/rebase/),\nrun it. \nGo to RubyDevkit directory, run devkitvars.bat. \nFire up a git bash (or Windows Prompt, must run as administrator), execute instruction below:\n\n<pre class=\"terminal\">\n<code>$ rebase.exe -b 0x50000000 msys-1.0.dll</code>\n</pre>\n\nRetry `gem install github-pages`. Now *voila*, everything works nicely!\n\n# Refs\n\n[1] [http://blog.csdn.net/itmyhome1990/article/details/41982625](http://blog.csdn.net/itmyhome1990/article/details/41982625)\n\n[2] [http://segmentfault.com/q/1010000003891086](http://segmentfault.com/q/1010000003891086)\n\n[3] [http://railsapps.github.io/openssl-certificate-verify-failed.html](http://railsapps.github.io/openssl-certificate-verify-failed.html)\n\n[4] [http://rockhong.github.io/github-pages-fails-to-update.html](http://rockhong.github.io/github-pages-fails-to-update.html)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","title":"Enable Large Addresses On VS2015"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Enable Large Addresses On VS2015\ndate: 2015-12-14\n---\n\nA Win32 process running under Win64 can use up to 4 GB if IMAGE_FILE_LARGE_ADDRESS_AWARE is set, \nmake sure Visual Studio Project Properties have this enabled:\n\n**Configuration Properties->Linker->System:**\n\n\"Enable Large Addresses\"\n\nset to:\n\n**Support Addresses Larger Than 2 Gigabytes (/LARGEADDRESSAWARE)**\n\n# Reference\n\n[https://social.msdn.microsoft.com/Forums/en-US/bd656b83-b715-4e08-af4d-c6372979fa9a/increase-memory-for-visual-studio-2010-c?forum=Vsexpressvc](https://social.msdn.microsoft.com/Forums/en-US/bd656b83-b715-4e08-af4d-c6372979fa9a/increase-memory-for-visual-studio-2010-c?forum=Vsexpressvc)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","title":"Fix min/max Error In VS2015"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Fix min/max Error In VS2015\ndate: 2016-02-17\n---\n\nVS2010 projects using min/max functions can be built successfully, \nwhile in VS2015 it will keep printing build error:\n\nerror C3861: 'min': identifier not found\n\nI finally find out that std::max() requires the `<algorithm>` header."},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","title":"Working on Visual Studio"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: Working on Visual Studio\ndate: 2016-04-03\n---\n# Visual Studio\n\n**Debug C++ code on Linux from Visual Studio**\n\n[https://blogs.msdn.microsoft.com/vcblog/2015/04/29/debug-c-code-on-linux-from-visual-studio/](https://blogs.msdn.microsoft.com/vcblog/2015/04/29/debug-c-code-on-linux-from-visual-studio/)\n\n**Announcing the VS GDB Debugger extension**\n\n[https://blogs.msdn.microsoft.com/vcblog/2015/11/18/announcing-the-vs-gdb-debugger-extension/](https://blogs.msdn.microsoft.com/vcblog/2015/11/18/announcing-the-vs-gdb-debugger-extension/)\n\n**Visual C++ for Linux Development**\n\n![](https://msdnshared.blob.core.windows.net/media/2016/03/Projects.png)\n\n- blog: [https://blogs.msdn.microsoft.com/vcblog/2016/03/30/visual-c-for-linux-development/](https://blogs.msdn.microsoft.com/vcblog/2016/03/30/visual-c-for-linux-development/)\n\n**Visual C++ for Linux 1.0.5 Updates**\n\n[https://blogs.msdn.microsoft.com/vcblog/2016/09/13/visual-c-for-linux-1-0-5-updates/](https://blogs.msdn.microsoft.com/vcblog/2016/09/13/visual-c-for-linux-1-0-5-updates/)\n\n**Visual C++ for Linux and Raspberry Pi Development**\n\n- blog: [http://www.hanselman.com/blog/VisualCForLinuxAndRaspberryPiDevelopment.aspx](http://www.hanselman.com/blog/VisualCForLinuxAndRaspberryPiDevelopment.aspx)\n\n# Visual Studio Code\n\n**C/C++ for Visual Studio Code**\n\n- intro: Complete C/C++ language support including code-editing and debugging.\n- blog: [https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools](https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools)\n\n**C/C++ extension for Visual Studio Code**\n\n![](https://msdnshared.blob.core.windows.net/media/2016/03/C_Cpp_icons2.png)\n\n[https://blogs.msdn.microsoft.com/vcblog/2016/03/31/cc-extension-for-visual-studio-code/](https://blogs.msdn.microsoft.com/vcblog/2016/03/31/cc-extension-for-visual-studio-code/)\n\n**September 2016 (version 1.6)**\n\n[https://code.visualstudio.com/updates/v1_6](https://code.visualstudio.com/updates/v1_6)\n\n# NuGet\n\n**NuGet: package manager for the Microsoft development platform**\n\n[http://www.nuget.org/](http://www.nuget.org/)\n\n**Is there a way to download packages from nuget.org then do an offline installation into Visual Studio?**\n\n- stackoverflow: [http://stackoverflow.com/questions/8120289/is-there-a-way-to-download-packages-from-nuget-org-then-do-an-offline-installati](http://stackoverflow.com/questions/8120289/is-there-a-way-to-download-packages-from-nuget-org-then-do-an-offline-installati)\n\n**How to install a Nuget Package .nupkg file locally?**\n\n- stackoverflow: [http://stackoverflow.com/questions/10240029/how-to-install-a-nuget-package-nupkg-file-locally](http://stackoverflow.com/questions/10240029/how-to-install-a-nuget-package-nupkg-file-locally)\n\n# Download Links\n\n**Microsoft Visual C++ 2008 Redistributable Package (x86)**\n\n[https://www.microsoft.com/en-us/download/details.aspx?id=29](https://www.microsoft.com/en-us/download/details.aspx?id=29)\n\n**Visual C++ Redistributable for Visual Studio 2012 Update 4 (MSVC11)**\n\n[https://www.microsoft.com/en-us/download/details.aspx?id=30679](https://www.microsoft.com/en-us/download/details.aspx?id=30679)\n\n**Visual C++ Redistributable Packages for Visual Studio 2013 (MSVC12)**\n\n[https://www.microsoft.com/en-us/download/details.aspx?id=40784](https://www.microsoft.com/en-us/download/details.aspx?id=40784)\n\n# News\n\n**Why Microsoft needed to make Windows run Linux software**\n\n![It's bash, it's Windows, it's not a virtual machine.](http://cdn.arstechnica.net/wp-content/uploads/2016/04/img_0018-640x426.jpg)\n\n- blog: [http://arstechnica.com/information-technology/2016/04/why-microsoft-needed-to-make-windows-run-linux-software/](http://arstechnica.com/information-technology/2016/04/why-microsoft-needed-to-make-windows-run-linux-software/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","title":"Android Development Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: android_dev\ntitle: Android Development Resources\ndate: 2016-05-23\n---\n\n# Eclipse\n\n## DDMS\n\n**How to enable native heap tracking in DDMS**\n\n![](http://bricolsoftconsulting.com/wp-content/uploads/2012/05/ddms_native.png)\n\n- blog: [http://bricolsoftconsulting.com/how-to-enable-native-heap-tracking-in-ddms/](http://bricolsoftconsulting.com/how-to-enable-native-heap-tracking-in-ddms/)\n\n**Tips for Optimizing Android* Application Memory Usage**\n\n![](https://software.intel.com/sites/default/files/managed/15/01/tips-for-optimizing-android-app-memory-fig2-ddms-heap-updates-track-allocation.png)\n\n- blog: [https://software.intel.com/en-us/android/articles/tips-for-optimizing-android-application-memory-usage](https://software.intel.com/en-us/android/articles/tips-for-optimizing-android-application-memory-usage)\n\n**使用DDMS中的内存监测工具Heap来优化内存**\n\n![](http://images.cnitblog.com/blog/651487/201502/021532244064511.gif)\n\n- blog: [http://www.cnblogs.com/tianzhijiexian/p/4267919.html](http://www.cnblogs.com/tianzhijiexian/p/4267919.html)\n\n## Memory Analyzer Tool (MAT)\n\n**Memory Analyzer 1.5.0 Release**\n\n![](http://www.eclipse.org/mat/home/mat_thumb.png)\n\n- homepage: [http://www.eclipse.org/mat/](http://www.eclipse.org/mat/)\n- download page: [http://www.eclipse.org/mat/downloads.php](http://www.eclipse.org/mat/downloads.php)\n\n**Eclipse Memory Analyzer (MAT) - Tutorial**\n\n- blog: [http://www.vogella.com/tutorials/EclipseMemoryAnalyzer/article.html](http://www.vogella.com/tutorials/EclipseMemoryAnalyzer/article.html)\n\n**10 Tips for using the Eclipse Memory Analyzer**\n\n[http://eclipsesource.com/blogs/2013/01/21/10-tips-for-using-the-eclipse-memory-analyzer/](http://eclipsesource.com/blogs/2013/01/21/10-tips-for-using-the-eclipse-memory-analyzer/)\n\n**[Android] 内存泄漏调试经验分享 (二)**\n\n- intro: 内存监测工具 DDMS --> Heap, 内存分析工具 MAT(Memory Analyzer Tool)\n- blog: [http://rayleeya.iteye.com/blog/755657](http://rayleeya.iteye.com/blog/755657)\n\n**Hunting Your Leaks: Memory Management in Android (Part 2 of 2)**\n\n- blog: [http://www.raizlabs.com/dev/2014/04/hunting-your-leaks-memory-management-in-android-part-2-of-2/](http://www.raizlabs.com/dev/2014/04/hunting-your-leaks-memory-management-in-android-part-2-of-2/)\n\n# Valgrind\n\n**Valgrind**\n\n- intro: Valgrind is an instrumentation framework for building dynamic analysis tools. \nThere are Valgrind tools that can automatically detect many memory management and threading bugs, \nand profile your programs in detail.\n- homepage: [http://valgrind.org/](http://valgrind.org/)\n\n**The compiler under Windows Valgrind for Android**\n\n- intro: Windows 7, Cygwin, Valgrind 3.9.0\n- blog: [http://www.programering.com/a/MjM3UzMwATE.html](http://www.programering.com/a/MjM3UzMwATE.html)\n- csdn: [http://blog.csdn.net/foruok/article/details/20701991](http://blog.csdn.net/foruok/article/details/20701991)\n\n**Building and running valgrind on Android**\n\n- blog: [https://blog.frals.se/2014/07/02/building-and-running-valgrind-on-android/](https://blog.frals.se/2014/07/02/building-and-running-valgrind-on-android/)\n- gist: [https://gist.github.com/frals/7775c413a52763d80de3](https://gist.github.com/frals/7775c413a52763d80de3)\n\n**android valgrind build**\n\n- blog: [http://none53.hatenablog.com/entry/20150325/1427242876](http://none53.hatenablog.com/entry/20150325/1427242876)\n\n**valgrind: failed to start tool 'memcheck' for platform 'arm-linux': Permission denied**\n\n- blog: [http://none53.hatenablog.com/entry/20150325/1427249228](http://none53.hatenablog.com/entry/20150325/1427249228)\n- my notes: this blog really helped me..\n\n# LeakCanary\n\n**LeakCanary: A memory leak detection library for Android and Java**\n\n![](https://raw.githubusercontent.com/square/leakcanary/master/assets/screenshot.png)\n\n- github: [https://github.com/square/leakcanary](https://github.com/square/leakcanary)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","title":"Notes On Valgrind and Others"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: android_dev\ntitle: Notes On Valgrind and Others\ndate: 2016-05-30\n---\n\n# Valgrind\n\nValgrind is a suite of tools for Profiling and debugging, I intend to use Memcheck to check my APP's memory leak issue.\nCheck out Vlagrind homepage at: [http://valgrind.org/](http://valgrind.org/), the version I use is: valgrind-3.11.0.\n\nI follow the blog to configure Valgrind:\n\n[https://blog.frals.se/2014/07/02/building-and-running-valgrind-on-android/](https://blog.frals.se/2014/07/02/building-and-running-valgrind-on-android/)\n\nThe author also provides some shell files on his gist: [https://gist.github.com/frals/7775c413a52763d80de3](https://gist.github.com/frals/7775c413a52763d80de3). \nThey are useful, but since I work on Windows 7 x86, I add some modifications. [https://github.com/handong1587/run_valgrind](https://github.com/handong1587/run_valgrind)\n\n(How to access gist in China? Add \n\n`192.30.252.141 gist.github.com` \n\nto \n\n`C:/Windows/System32/drivers/etc/hosts`\n\n)\n\nOne thing to note is that on Windows sometimes there will be some '\\r', '\\r\\n' problems, \nso we'd better use a dos2unix tool to convert the text format after every time we edit shell files on Windows.\nI use a tool from: [http://dos2unix.sourceforge.net/](http://dos2unix.sourceforge.net/)\n\nTo successfully build, we need to modify the kernel version in configure \n(my VM's kernel verison is 2.0.1, which is not valid by default):\n\nChange:\n\n```\ncase \"${kernel}\" in\n0.*|1.*|2.0.*|2.1.*|2.2.*|2.3.*|2.4.*|2.5.*)\n```\n\nto:\n\n```\ncase \"${kernel}\" in\n0.*|1.*|2.1.*|2.2.*|2.3.*|2.4.*|2.5.*)\n```\n\nNormally we should wait for about half an hour for bulding. All looks okay by far. But when I try to test it, things become nasty.\n\nWhen I build a 32-bit Valgrind, every time I try to start memcheck, it will output an error log:\n\n```\n$ $SDKROOT/adb.exe shell \"/data/local/Inst/bin/valgrind am start -a android.intent.action.MAIN -n com.example.MyAPP/.MyAPPMain\"\nvalgrind: failed to start tool 'memcheck' for platform 'arm64-linux': No such file or directory\n```\n\nWired thing is that there is a memcheck-arm-linux in /data/local/Inst/lib/valgrind/, \nand obviously Valgrind should call it since they are of 32-bit. But Valgrind always try to call a 64-bit memcheck. \nUse `strace` command can show the calling procedure easily:\n\n```\n$ $SDKROOT/adb.exe shell \"strace /data/local/Inst/bin/valgrind am start -a android.intent.action.MAIN -n com.example.MyAPP/.MyAPPMain\"\n```\n\nOutputs:\n\n```\nexecve(\"/data/local/Inst/lib/valgrind/memcheck-arm64-linux\", [\"/data/local/Inst/bin/valgrind\", \"am\", \"start\", \"-a\", \"android.intent.action.MAIN\", \"-n\", \"com.example.SGallery/.SGalleryMa\"...], [/* 17 vars */]) = -1 ENOENT (No such file or directory)\n```\n\nSo I was stucked on this.\nAnd it is more frustrating that if I change to 64-bit configure, many Valgrind tools are failed to build. Still not memcheck-arm64-linux.\n\n# DDMS\n\nDDMS (Dalvik Debug Monitor Server) is a debugging tool used in the Android platform, often downloaded as part of the Android SDK.\nYou can launch it from: \"Eclipse > Window > Open Perspective > Other... > DDMS\" or \ndirectly from: \"PathToAndroidSDK\\android-sdk-windows\\tools\\ddms.bat\".\n\nNeed to modify the ddms.cfg (`c:/Users/username/.android/ddms.cfg`) file to enable native heap tracking in DDMS:\n\nAdd\n\n```\nnative=true\n```\n\nin ddms.cfg.\n\nBasicly I use DDMS to check if my Android app has memory leaks issues.\nAfter DDMS detect your app is running, you can see that there will be updatings in \"Heap > data object > Total Size\"\neverytime you interact with your app. That value indicates how much memory your app occupies while running.\nIf app does not free all allocted Java data objects, then this value will keep increasing after every GC.\nSo app will explode if excess some memory limitations and the process will be killed.\n\n![](https://software.intel.com/sites/default/files/managed/15/01/tips-for-optimizing-android-app-memory-fig2-ddms-heap-updates-track-allocation.png)\n\n# MAT\n\nMAT (Memory Analyzer Tool) is a Java heap analyzer that can help to find memory leaks. \nYou can download it from: [http://www.eclipse.org/mat/](http://www.eclipse.org/mat/).\n\nUsually we use DDMS to dump a HPROF file, then use MAT to get reports.\n\n# Refs\n\n1. [http://www.programering.com/a/MjM3UzMwATE.html](http://www.programering.com/a/MjM3UzMwATE.html)\n2. [https://blog.frals.se/2014/07/02/building-and-running-valgrind-on-android/](https://blog.frals.se/2014/07/02/building-and-running-valgrind-on-android/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","title":"Web Development Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ntitle: Web Development Resources\ndate: 2016-06-21\ncategory: \"web_dev\"\n---\n\n**How to Install Node.js® and NPM on Windows**\n\n- blog: [http://blog.teamtreehouse.com/install-node-js-npm-windows](http://blog.teamtreehouse.com/install-node-js-npm-windows)\n\n# How to install a gem correctly: use RVM\n\n**Why do I get a “permission denied” error while installing a gem?**\n\n- stackoverflow: [http://stackoverflow.com/questions/17550903/why-do-i-get-a-permission-denied-error-while-installing-a-gem](http://stackoverflow.com/questions/17550903/why-do-i-get-a-permission-denied-error-while-installing-a-gem)\n\n**Installing RVM**\n\n[https://rvm.io/rvm/install](https://rvm.io/rvm/install)\n\n**Integrating RVM with gnome-terminal**\n\n[https://rvm.io/integration/gnome-terminal](https://rvm.io/integration/gnome-terminal)\n\n# Install bower\n\n**/usr/bin/env: node: No such file or directory**\n\n```\nlinjinbin@ubuntu:~/sw/node-v0.1.100$ cd /usr/local/lib/node_modules/bower/bin/\nlinjinbin@ubuntu:/usr/local/lib/node_modules/bower/bin$ ./bower -v\n/usr/bin/env: node: No such file or directory\nlinjinbin@ubuntu:/usr/local/lib/node_modules/bower/bin$ sudo ln -s /usr/bin/nodejs /usr/bin/node\n```\n\n# Toc of Markdown\n\n**为Jekyll博客添加目录与ScrollSpy效果**\n\n- blog: [http://t.hengwei.me/post/%E4%B8%BAjekyll%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0%E7%9B%AE%E5%BD%95%E4%B8%8Escrollspy%E6%95%88%E6%9E%9C](http://t.hengwei.me/post/%E4%B8%BAjekyll%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0%E7%9B%AE%E5%BD%95%E4%B8%8Escrollspy%E6%95%88%E6%9E%9C)\n\n**How to add Table of Contents to Jekyll**\n\n[http://blog.webjeda.com/jekyll-toc/](http://blog.webjeda.com/jekyll-toc/)\n\n**Tocify**\n\n![](/assets/web_dev/tocify.png)\n\n- blog: [http://gregfranko.com/jquery.tocify.js/](http://gregfranko.com/jquery.tocify.js/)\n- blog: [http://www.helloweba.com/demo/2016/tocify/](http://www.helloweba.com/demo/2016/tocify/)\n- github: [https://github.com/gfranko/jquery.tocify.js](https://github.com/gfranko/jquery.tocify.js)\n\n# Javascript\n\n**6 Best JavaScript Frameworks to Learn In 2016**\n\n- keywords: AngularJS / React / Ember / Adonis / Vue.js / Backbone.js\n- blog: [http://www.discoversdk.com/blog/6-best-javascript-frameworks-to-learn-in-2016/#/wow](http://www.discoversdk.com/blog/6-best-javascript-frameworks-to-learn-in-2016/#/wow)\n\n**A Brief Overview of Popular JS Frameworks: React, Angular, Bootstrap, and Polymer**\n\n[https://dzone.com/articles/react-angular-bootstrap-and-polymer-the-basics?utm_source=Top%205&utm_medium=email&utm_campaign=top5%202016-09-02](https://dzone.com/articles/react-angular-bootstrap-and-polymer-the-basics?utm_source=Top%205&utm_medium=email&utm_campaign=top5%202016-09-02)\n\n**How it feels to learn JavaScript in 2016**\n\n[https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.7ux04vedl](https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.7ux04vedl)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","title":"Install Therubyracer Failure"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ntitle: Install Therubyracer Failure\ndate: 2016-07-03\ncategory: \"web_dev\"\n---\n\nI try to install therubyracer via gem on Windows 10, but keep getting an error associated with -rdynamic flag, \nwhich results in failure to build the native extensions:\n\n![](/assets/web_dev/gem_install_therubyracer.png)\n\nYou can find -rdynamic flag in extconf.rb and Makefile:\n\n`C:\\Ruby23-x64\\lib\\ruby\\gems\\2.3.0\\gems\\therubyracer-0.12.2\\ext\\v8\\extconf.rb`\n\n```\n$CPPFLAGS += \" -rdynamic\" unless $CPPFLAGS.split.include? \"-rdynamic\"\n$CPPFLAGS += \" -fPIC\" unless $CPPFLAGS.split.include? \"-rdynamic\" or RUBY_PLATFORM =~ /darwin/\n```\n\n`C:\\Ruby23-x64\\lib\\ruby\\gems\\2.3.0\\gems\\therubyracer-0.12.2\\ext\\v8\\Makefile`\n\n```\nCPPFLAGS =  -DFD_SETSIZE=2048 -D_WIN32_WINNT=0x0501 -D__MINGW_USE_VC2005_COMPAT $(DEFS) $(cppflags) -Wall -g -rdynamic\n```\n\nSomebody figures it out by changing gcc compiler to version 4.2 (mine is 4.9.3).\nFor some reason the newer gcc version don't just ignore the -rdynamic flag, \nwhich is only present for compiling on Linux and is not actually compatible with Windows and OS X. \n\n[https://stackoverflow.com/questions/35741536/trouble-installing-therubyracer-gem-due-to-compiler-issue-on-mac](https://stackoverflow.com/questions/35741536/trouble-installing-therubyracer-gem-due-to-compiler-issue-on-mac)\n\nMore detailed explanation is that:\n[-rdynamic passes the flag -export-dynamic to ELF linker, on targets that support it](https://gcc.gnu.org/onlinedocs/gcc/Link-Options.html)\n\nExecutable formats in OS X and Windows are not ELF, \nthus the option -rdynamic is not supported building for these operating systems.\n\n[http://stackoverflow.com/questions/29534519/why-gcc-doesnt-recognize-rdynamic-option](http://stackoverflow.com/questions/29534519/why-gcc-doesnt-recognize-rdynamic-option)\n\nOne solution is using an older gcc compiler, like gcc-4.2.\n\n[http://preshing.com/20141108/how-to-install-the-latest-gcc-on-windows/](http://preshing.com/20141108/how-to-install-the-latest-gcc-on-windows/)\n\n```\nwget http://ftpmirror.gnu.org/gcc/gcc-4.2.4/gcc-4.2.4.tar.gz\n```\n\n```\nwget http://ftpmirror.gnu.org/gcc/gcc-4.2.4/gcc-g++-4.2.4.tar.bz2\n```"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","title":"PHP Hello World"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ntitle: PHP Hello World\ndate: 2016-07-04\ncategory: \"web_dev\"\n---\n\nDownload PHP package on [http://windows.php.net/download/](http://windows.php.net/download/).\n\nInstall PhpStorm.\n\nCreate an \"PHP Empty Project\", create php file *hello_world.php* with following code:\n\n```\n<?php\necho \"hello world!\";\nphpinfo();\n?>\n```\n\nConfigure PHP setting in: File > Settings > Languages & Frameworks > PHP, \nsuch as PHP package dirctory and interpretor:\n\n![](/assets/web_dev/php_hello_world.PNG)\n\nRight click hello_world.php > Open in Browser > Default, you can get an output on browser like:\n\n![](/assets/web_dev/php_hello_world_2.PNG)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","title":"Add Lunr Search Plugin For Blog"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ntitle: Add Lunr Search Plugin For Blog\ndate: 2016-07-31\ncategory: \"web_dev\"\n---\n\nI decided to add a full-text search plugin to my blog:\n\n[https://github.com/slashdotdash/jekyll-lunr-js-search](https://github.com/slashdotdash/jekyll-lunr-js-search) .\n\nAlthough it should be an easy work, there are still some rules I think are somewhat crucial to follow (for me..).\n\nFirst rule: DO NOT try to do this on Windows.\n\nOn windows (and OS X), you can not even manage to gem install therubyracer, which is essential component required by jekyll-lunr-js-search. \nSee my previous post: \n\n[http://handong1587.github.io/web_dev/2016/07/03/install-therubyracer.html](http://handong1587.github.io/web_dev/2016/07/03/install-therubyracer.html)\n\nKeep yourself aware that you don't include jQuery twice. It can really cause all sorts of issues.\n\nThis post explains in a more detail: \n\n**Double referencing jQuery deletes all assigned plugins.**\n\n[https://bugs.jquery.com/ticket/10066](https://bugs.jquery.com/ticket/10066)\n\nIt kept me receiving one wired error like:\n\n```\nTypeError: $(...).lunrSearch is not a function\n```\n\nand took me a long time to find out why this happened.\n\nFor a newbie like me who *know nothing at all* about front-end web development, \nall the work become trial and error, and google plus stackoverflow. So great now it can work.\n\nThanks to *My Chemical Romance* for helping me through those tough debugging nights!"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","title":"C++ Programming Solutions"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: C++ Programming Solutions\ndate: 2016-09-07\n---\n\n# Reference a nonstatic MFC class member in a static thread function\n\nDeclare a thread function:\n\n```\nstatic DWORD WINAPI ThreadFunc(LPVOID lpParameter);\n```\n\nPass a `this` pointer to thread function:\n\n```\nHANDLE hThread = CreateThread(NULL, 0, ThreadFunc, this, 0, NULL);\n```\n\nIn the thread function definition:\n\n```\nDWORD WINAPI CMFCDemoDlg::ThreadFunc(LPVOID lpParameter)\n{\n    //convert lpParameter to class pointer type\n    CMFCDemoDlg* pMfcDemo = (CMFCDemoDlg*)lpParameter;\n\n    // Now you can reference the CMFCDemoDlg class members\n    ......\n}\n```\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","title":"PyInstsaller and Others"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: programming_study\ntitle: PyInstsaller and Others\ndate: 2016-12-24\n---\n\n# Quick introduction\n\nI recently need to convert one Python program into binary mode program. \nThat is, you don't want to expose any of your source code, data files, \nonly one binary executable file will be provided.\n\n[PyInstaller](http://www.pyinstaller.org/) is a fairly good choice to use, \nand can work on many platforms like Linux, Windows, etc.\n\nYou can check out its official git repository at \n[https://github.com/pyinstaller/pyinstaller](https://github.com/pyinstaller/pyinstaller).\n\nIt is recommended that first try out its officially, stable release -- \nbut when something weird come just around, you can turn to the github dev branch for help -- actually that is what I did.\n\n# hidden-import\n\nThere are 2 basic ways to process Python scripts. I chose to use pyinstaller.py directly,\nalthough you can use *spec* file if you want.\n\nWhen building Python scripts, you probably will get some build errors telling you that some Python packages cannot be imported.\nLike:\n\n```\nImportError: The 'packaging' package is required\n```\n\n```\nImportError: No module named core_cy\n```\n\nI might explain it in the future, but to put it simply, some Python packages need to be \"hidden-import\" to get around this issue.\nSo now we can setup a fundamental build script to help our work:\n\n```\n/path/to/git/pyinstaller/pyinstaller.py \\\n    --onefile \\\n    --hidden-import=skimage.io \\\n    --hidden-import=skimage.transform \\\n    --hidden-import=skimage.filter.rank.core_cy \\\n    --hidden-import=packaging \\\n    --hidden-import=packaging.version \\\n    --hidden-import=packaging.specifiers \\\n    --hidden-import=packaging.requirements \\\n    --hidden-import=scipy.linalg \\\n    --hidden-import=scipy.linalg.cython_blas \\\n    --hidden-import=scipy.linalg.cython_lapack \\\n    --hidden-import=scipy.ndimage \\\n    --hidden-import=skimage._shared.interpolation \\\n    --hidden-import=google.protobuf.internal \\\n    --hidden-import=google.protobuf.internal.enum_type_wrapper \\\n    --hidden-import=google.protobuf.descriptor \\\n    target_program.py\n```\n\n# What is wrong with MKL\n\nOne weird error I met was the Intel MKL FATAL ERROR:\n\n```\nIntel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.\n```\n\nSince I use anaconda, I find MKL has already been installed on the anaconda install location \nand can find these 2 files easily, but this error still pop out.\nIf I remember correctly, the solution is even more weird:\nsimply update numpy to a latest version:\n\n```\nconda update numpy\n```\n\nor:\n\n```\nconda install linux-64_numpy-1.11.2-py27_0.tar.bz2\n```\n\nI don't know what happened exactly but looks like it been fixed. Hmm...\n\n# --add-data and _MEIPASS\n\nPyInstaller can also bundle data files to your programs. When bundled app runs, \nit will load these data files, in a different location.\nHere is a helper function to locate your data files:\n\n```\ndef resource_path(relative):\n    bundle_dir = os.environ.get(\"_MEIPASS2\", os.path.abspath(\".\"))\n    if getattr(sys, 'frozen', False):\n        # we are running in a bundle\n        bundle_dir = sys._MEIPASS\n    else:\n        # we are running in a normal Python environment\n        bundle_dir = os.path.dirname(os.path.abspath(__file__))\n\n    return os.path.join(bundle_dir, relative)\n```\n\nYou can put your data file in your local directory, \nbut need to specify the data file name in Python script in a right way:\n\n```\ntarget_file = resource_path('target_data_file1')\n```\n\nIn build script, you need to configure the data files or folders:\n\n   ```\n    --add-data=\"target_data_file1:.\" \\\n    --add-data=\"target_data_file2:.\" \\\n    --add-data=\"folder1/sub_folder1/target_data_file3:folder1/sub_folder1/target_data_file3\" \\\n   ```\n\n# Missing libs\n\n   ```\n    --add-binary=\"libgfortran.so.1:lib\" \\\n   ```\n\nThe build error told me one \\*so file is required. So just add it.\n\n# Config PYTHONPATH\n\nSome of your Python scripts might depends on some relative path, \nso you will need to put this dependencies into the build script:\n\n```\n--paths=\"../dependency_folder\" \\\n```\n\n# Continue tackling weird stuffs\n\nUtil now it sounds like an easy task.\nBut what happened next consumed me about 2 days -- I wish I could have known how to avoid it :-(\n\nMy Python project includes A Caffe module which run a simple image classification process.\nOne basic function is [Caffe](https://github.com/BVLC/caffe) calling skimage.io to load image:\n\n[https://github.com/BVLC/caffe/blob/master/python/caffe/io.py](https://github.com/BVLC/caffe/blob/master/python/caffe/io.py)\n\n```\ndef load_image(filename, color=True):\n    img = skimage.img_as_float(skimage.io.imread(filename, as_grey=not color)).astype(np.float32)\n    if img.ndim == 2:\n        img = img[:, :, np.newaxis]\n        if color:\n            img = np.tile(img, (1, 1, 3))\n    elif img.shape[2] == 4:\n        img = img[:, :, :3]\n    return img\n```\n\nI wonder if PyInstaller currently has a good support for Python package skimage.\nBut from what I know by now, it doesn't.\n\nRun from Python source code files, it works fine. But when I packed all things into one single binary file,\nit can not load image at all. And after debugging and googleing for a long time -- \nI always thought maybe I did something wrong -- I get rid of this. PyInstaller hates skimage! \nSo at last I use cv2 instead. And it works smoothly.\n\n```\ndef cv2_load_image(filename, color=True):\n    img = cv2.imread(filename).astype(np.float32) / 255\n    if img.ndim == 3:\n        img[:,:,:] = img[:,:,2::-1]\n\n    if img.ndim == 2:\n        img = img[:, :, np.newaxis]\n        if color:\n            img = np.tile(img, (1, 1, 3))\n    elif img.shape[2] == 4:\n        img = img[:, :, :3]\n    return img\n```\n\nFor all above details, please do check out PyInstaller Documentation: \n[https://media.readthedocs.org/pdf/pyinstaller/latest/pyinstaller.pdf](https://media.readthedocs.org/pdf/pyinstaller/latest/pyinstaller.pdf)\n\n# Looks like we make it!\n\n```\n/path/to/git/pyinstaller/pyinstaller.py \\\n    --onefile \\\n    --hidden-import=skimage.io \\\n    --hidden-import=skimage.transform \\\n    --hidden-import=skimage.filter.rank.core_cy \\\n    --hidden-import=packaging \\\n    --hidden-import=packaging.version \\\n    --hidden-import=packaging.specifiers \\\n    --hidden-import=packaging.requirements \\\n    --hidden-import=scipy.linalg \\\n    --hidden-import=scipy.linalg.cython_blas \\\n    --hidden-import=scipy.linalg.cython_lapack \\\n    --hidden-import=scipy.ndimage \\\n    --hidden-import=skimage._shared.interpolation \\\n    --hidden-import=google.protobuf.internal \\\n    --hidden-import=google.protobuf.internal.enum_type_wrapper \\\n    --hidden-import=google.protobuf.descriptor \\\n    --add-binary=\"libgfortran.so.1:lib\" \\\n    --add-data=\"target_data_file1:.\" \\\n    --add-data=\"target_data_file2:.\" \\\n    --add-data=\"folder1/sub_folder1/target_data_file3:folder1/sub_folder1/target_data_file3\" \\\n    --paths=\"../dependency_folder\" \\\n    target_program.py\n```\n\n# Misc\n\nI just find a simple method to read/write binary file via Python: \nusing cPickle to dump data to file in binary format.\n\n```\nimport cPickle\n\na = ('img_path1', 1111, 222.222, 333, 444, 555, 6666)\nb = ('img_path2', 777, 88.8888, 9999, 1010, 1111, 1212)\nc = []\nc.append(a)\nc.append(b)\n\nwith open('wb_txt', 'wb') as f:\n    cPickle.dump(c, f, cPickle.HIGHEST_PROTOCOL)\n\nwith open('wb_txt', 'rb') as f:\n    data = cPickle.load(f)\n    print data\n```\n\nHopefully this note can guide someone new to PyInstaller like me to walk out of sloughy.\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","title":"Book Reading List"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: reading_and_thoughts\ntitle: Book Reading List\ndate: 2015-12-04\n---\n\n*围城*\n\n*百年孤独*\n\n*卡尔维诺文集*\n\n![](/assets/reading_and_thoughts/book-reading-list/Calvino_collected_works_5.jpg)\n\n*不能承受的生命之轻*\n\n![](/assets/reading_and_thoughts/book-reading-list/LE_INSOUTENABLE_LEGERETE_DE_LETRE.jpg)\n\n*德国反犹史*\n\n*博尔赫斯小说集*\n\n![](/assets/reading_and_thoughts/book-reading-list/Borges_stories_collection.jpg)\n\n*万历十五年*\n\n*白夜行*\n\n*Gödel,Escher,Bach: An Eternal Golden Braid*\n\n*哥德尔、艾舍尔、巴赫：集异璧之大成*\n\n![](/assets/reading_and_thoughts/book-reading-list/geb.png)\n\n- webpage: [http://geb.stenius.org/](http://geb.stenius.org/)\n- MIT course: [http://ocw5.mit.edu/courses/special-programs/sp-258-goedel-escher-bach-spring-2007/](http://ocw5.mit.edu/courses/special-programs/sp-258-goedel-escher-bach-spring-2007/)\n- Stanford course: [http://ssp11si.stanford.edu/](http://ssp11si.stanford.edu/)\n\n*Steve Jobs*\n\n*万有引力之虹*\n\n## 2015-11-22\n\n- 看完了<*癸酉本石头记*>.\n\n## 2016-01-03\n\n- *解忧杂货铺*\n\n## 2016-01-20\n\n- *聂卫平围棋人生*"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","title":"Funny Papers"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: reading_and_thoughts\ntitle: Funny Papers\ndate: 2015-12-04\n---\n\n**On the reception and detection of pseudo-profound bullshit**\n\n- summary: \"A new scientific study has found that those who are receptive to pseudo-profound, \nintellectual-sounding 'bullshit' are less intelligent, less reflective, and more likely to be believe in \nconspiracy theories, the paranormal and alternative medicine.\"\n- keywords: 'wise-sounding quotes', 'pseudo-profound bullshit'\n- paper: [http://journal.sjdm.org/15/15923a/jdm15923a.pdf](http://journal.sjdm.org/15/15923a/jdm15923a.pdf)\n- review: [http://www.independent.co.uk/news/science/scientists-find-a-link-between-low-intelligence-and-acceptance-of-pseudo-profound-bulls-a6757731.html](http://www.independent.co.uk/news/science/scientists-find-a-link-between-low-intelligence-and-acceptance-of-pseudo-profound-bulls-a6757731.html)\n- review: [http://www.forbes.com/sites/emilywillingham/2015/11/30/why-do-some-people-find-deepak-chopra-quotes-deep-and-not-dung/](http://www.forbes.com/sites/emilywillingham/2015/11/30/why-do-some-people-find-deepak-chopra-quotes-deep-and-not-dung/)\n\n**Sex, drugs, and violence**\n\n- arxiv: [http://arxiv.org/abs/1608.03448](http://arxiv.org/abs/1608.03448)\n\n**The Shortest Papers Ever Published**\n\n[https://paperpile.com/blog/shortest-papers/](https://paperpile.com/blog/shortest-papers/)\n\n**The Kardashian Kernel**\n\n[http://oneweirdkerneltrick.com/sigbovik_2012.pdf](http://oneweirdkerneltrick.com/sigbovik_2012.pdf)\n\n**Visually Identifying Rank**\n\n[http://oneweirdkerneltrick.com/rank.pdf](http://oneweirdkerneltrick.com/rank.pdf)\n\n**Stopping GAN Violence: Generative Unadversarial Networks**\n\n- arxiv: [https://arxiv.org/abs/1703.02528](https://arxiv.org/abs/1703.02528)\n- github: [https://github.com/albanie/SIGBOVIK17-GUNs](https://github.com/albanie/SIGBOVIK17-GUNs)\n\n**Deep Spreadsheets with ExcelNet**\n\n- project page: [http://www.deepexcel.net/](http://www.deepexcel.net/)\n- paper: [http://www.deepexcel.net/paper.pdf](http://www.deepexcel.net/paper.pdf)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","title":"Reading Materials"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: reading_and_thoughts\ntitle: Reading Materials\ndate: 2016-01-18\n---\n\n**A PhD Is Not Enough! - A Guide to Survival in Science**\n\n[http://www.sceomasincukraljevo.blog.com/files/2011/04/di-0356.pdf](http://www.sceomasincukraljevo.blog.com/files/2011/04/di-0356.pdf)\n\n**A Survival Guide to a PhD**\n\n![](http://karpathy.github.io/assets/phd/phds.jpg)\n\n- blog: [http://karpathy.github.io/2016/09/07/phd/](http://karpathy.github.io/2016/09/07/phd/)\n\n**Hi, I'm a digital junkie, and I suffer from infomania**\n\n- blog: [http://www.latimes.com/business/technology/la-fi-thedownload-infomania-20160119-story.html](http://www.latimes.com/business/technology/la-fi-thedownload-infomania-20160119-story.html)\n\n**Of Course China, Like All Great Powers, Will Ignore an International Legal Verdict**\n\n- quote: “the strong do as they will; the weak suffer as they must”\n- news article: [http://thediplomat.com/2016/07/of-course-china-like-all-great-powers-will-ignore-an-international-legal-verdict/](http://thediplomat.com/2016/07/of-course-china-like-all-great-powers-will-ignore-an-international-legal-verdict/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","title":"Essay Writting"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: study\ntitle: Essay Writting\ndate: 2016-01-11\n---\n\n# Courses\n\n**Edx: English Grammar and Essay Writing**\n\n[https://www.edx.org/course/english-grammar-essay-writing-uc-berkeleyx-colwri2-2x](https://www.edx.org/course/english-grammar-essay-writing-uc-berkeleyx-colwri2-2x)\n\n# Book\n\n**Style Lessons in Clarity and Grace (11th Edition)**\n\n[https://pan.baidu.com/s/1i5xlAfV](https://pan.baidu.com/s/1i5xlAfV)\n\n**Style: Toward Clarity and Grace**\n\n[http://www.unalmed.edu.co/~poboyca/documentos/Doc.%20Seminario%20I/style.pdf](http://www.unalmed.edu.co/~poboyca/documentos/Doc.%20Seminario%20I/style.pdf)\n\n# Resources\n\n**Begin LaTeX in minutes**\n\n- intro: Brief Intro to LaTeX for beginners that helps you use LaTeX with ease. Comments and Contributions are welcomed\n- github: [https://github.com/VoLuong/Begin-Latex-in-minutes](https://github.com/VoLuong/Begin-Latex-in-minutes)\n\n**The Most Common Habits from more than 200 English Papers written by Graduate Chinese Engineering Students**\n\n- paper: [https://pan.baidu.com/s/1hrIwQOW](https://pan.baidu.com/s/1hrIwQOW)\n\n**Essay Writting Resource Guide**\n\n- intro: A comprehensive guide for students, teachers and writers\n- blog: [http://www.supersummary.com/essay-writing-guide/](http://www.supersummary.com/essay-writing-guide/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","title":"Job Hunting"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: study\ntitle: Job Hunting\ndate: 2016-06-02\n---\n\n# Deep Learning\n\n**What are the toughest neural networks and deep learning interview questions?**\n\n[https://www.quora.com/What-are-the-toughest-neural-networks-and-deep-learning-interview-questions](https://www.quora.com/What-are-the-toughest-neural-networks-and-deep-learning-interview-questions)\n\n**How to Get a Job In Deep Learning**\n\n- blog: [http://blog.deepgram.com/how-to-get-a-job-in-deep-learning/](http://blog.deepgram.com/how-to-get-a-job-in-deep-learning/)\n\n**Alternatives to a Degree to Prove Yourself in Deep Learning**\n\n[http://www.fast.ai/2017/04/06/alternatives/](http://www.fast.ai/2017/04/06/alternatives/)\n\n**40 Questions to test a data scientist on Deep Learning [Solution: SkillPower – Deep Learning, DataFest 2017]**\n\n[https://www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-deep-learning/](https://www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-deep-learning/)\n\n# Machine Learning\n\n**What are the best interview questions to evaluate a machine learning researcher?**\n\n- quora: [https://www.quora.com/What-are-the-best-interview-questions-to-evaluate-a-machine-learning-researcher](https://www.quora.com/What-are-the-best-interview-questions-to-evaluate-a-machine-learning-researcher)\n\n**Machine Learning Engineer interview questions**\n\n- blog: [http://resources.workable.com/machine-learning-engineer-interview-questions](http://resources.workable.com/machine-learning-engineer-interview-questions)\n\n**7 key skills required for Machine Learning jobs**\n\n- blog: [http://bigdata-madesimple.com/7-key-skills-required-for-machine-learning-jobs/](http://bigdata-madesimple.com/7-key-skills-required-for-machine-learning-jobs/)\n\n**Collection of Machine Learning Interview Questions**\n\n- blog: [http://analyticscosm.com/machine-learning-interview-questions-for-data-scientist-interview/](http://analyticscosm.com/machine-learning-interview-questions-for-data-scientist-interview/)\n\n**To be a data scientist in a tech company (Google, Microsoft, Facebook, etc.), how well do I need to know machine learning algorithms?**\n\n- quora: [https://www.quora.com/To-be-a-data-scientist-in-a-tech-company-Google-Microsoft-Facebook-etc-how-well-do-I-need-to-know-machine-learning-algorithms/](https://www.quora.com/To-be-a-data-scientist-in-a-tech-company-Google-Microsoft-Facebook-etc-how-well-do-I-need-to-know-machine-learning-algorithms/)\n\n**40 Interview Questions asked at Startups in Machine Learning / Data Science**\n\n[https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/](https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)\n\n**5 EBooks to Read Before Getting into A Machine Learning Career**\n\n[http://www.kdnuggets.com/2016/10/5-free-ebooks-machine-learning-career.html](http://www.kdnuggets.com/2016/10/5-free-ebooks-machine-learning-career.html)\n\n**45 Questions to test a data scientist on basics of Deep Learning (along with solution)**\n\n- github: [https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/](https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/)\n\n# Data Science\n\n**21 Must-Know Data Science Interview Questions and Answers**\n\n- part 1: [http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html](http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html)\n- part 2: [http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers-part2.html](http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers-part2.html)\n\n# Algorithms and Data Structure\n\n**C & Data Structure Interview Questions**\n\n[https://intellipaat.com/interview-question/c-data-structure-interview-questions/](https://intellipaat.com/interview-question/c-data-structure-interview-questions/)\n\n**Technical Interview Megarepo: Study materials for SE/CS technical interviews**\n\n- intro: Algorithms, Data Structures, Mathematics, Operating Systems, System Design, Interview Tips\n- github: [https://github.com/jdsutton/Technical-Interview-Megarepo](https://github.com/jdsutton/Technical-Interview-Megarepo)\n\n**350+ Data structures programming interview questions**\n\n[https://techiedelight.quora.com/350%2B-Data-structures-programming-interview-questions?srid=hOqIb](https://techiedelight.quora.com/350%2B-Data-structures-programming-interview-questions?srid=hOqIb)\n\n**500 Data structures and algorithms interview questions and their solutions in C++**\n\n[https://techiedelight.quora.com/500-Data-structures-and-algorithms-interview-questions-and-their-solutions-in-C%2B%2B](https://techiedelight.quora.com/500-Data-structures-and-algorithms-interview-questions-and-their-solutions-in-C%2B%2B)\n\n**Algorithms and Data Structures: Implementation of Algorithms and Data Structures, Interview Questions and Answers**\n\n- intro: This is the collection of algorithms, data structures and Interview Questions with solutions\n- github: [https://github.com/sherxon/AlgoDS](https://github.com/sherxon/AlgoDS)\n\n# Coding\n\n**Coding interview questions with solutions (C++)**\n\n- homepage: [http://itgeekworkhard.com/coding-questions/](http://itgeekworkhard.com/coding-questions/)\n- github: [https://github.com/checkcheckzz/coding-questions](https://github.com/checkcheckzz/coding-questions)\n\n**Everything you need to kick ass on your coding interview**\n\n- github: [https://github.com/andreis/interview](https://github.com/andreis/interview)\n\n**Interviews - Everything you need to know to get the job**\n\n[https://github.com/kdn251/Interviews](https://github.com/kdn251/Interviews)\n\n# Linux\n\n**Top Linux Interview Questions – Most Asked**\n\n[https://intellipaat.com/interview-question/linux-interview-questions/](https://intellipaat.com/interview-question/linux-interview-questions/)\n\n# Python\n\n**Top Answers to Python Interview Questions**\n\n- blog: [https://intellipaat.com/interview-question/python-interview-questions/](https://intellipaat.com/interview-question/python-interview-questions/)\n\n# Resumes\n\n**A very cool resume**\n\n![](/assets/programming_study/strml.net.png)\n\n[http://strml.net/](http://strml.net/)\n\n**This résumé for Elon Musk proves you never, ever need to use more than one page**\n\n[http://www.businessinsider.com/elons-musk-resume-all-on-one-page-2016-4?utm_source=tuicool&utm_medium=referral](http://www.businessinsider.com/elons-musk-resume-all-on-one-page-2016-4?utm_source=tuicool&utm_medium=referral)\n\n**An elegant \\LaTeX\\ résumé template**\n\n![](https://camo.githubusercontent.com/49acefc685c74053315818a494b8ca2e15745386/687474703a2f2f37786f6a72782e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f646f63732f726573756d652e706e67)\n\n- github: [https://github.com/billryan/resume](https://github.com/billryan/resume)\n- github(中文): [https://github.com/billryan/resume/tree/zh_CN](https://github.com/billryan/resume/tree/zh_CN)\n\n**How to Make Your Data Science Resume Stand Out**\n\n[http://datasciencedegree.wisconsin.edu/blog/data-science-resume/](http://datasciencedegree.wisconsin.edu/blog/data-science-resume/)\n\n# Resources\n\n**Awesome Interviews: A curated awesome list of lists of interview questions**\n\n- github: [https://github.com/MaximAbramchuck/awesome-interview-questions](https://github.com/MaximAbramchuck/awesome-interview-questions)\n\n**Stories & Tips: 50+ Interviews With Facebook, Twitter, Amazon & Others**\n\n- blog: [http://blog.robertelder.org/50-interviews-with-facebook-twitter-amazon-others/](http://blog.robertelder.org/50-interviews-with-facebook-twitter-amazon-others/)\n\n**Google Interview University**\n\n- github: [https://github.com/jwasham/google-interview-university](https://github.com/jwasham/google-interview-university)\n\n**Guide to Technical Interviews**\n\n- github: [https://github.com/Developer-Y/technical-interviews](https://github.com/Developer-Y/technical-interviews)\n\n**Interview questions**\n\n- github: [https://github.com/mission-peace/interview](https://github.com/mission-peace/interview)\n- wiki: [https://github.com/mission-peace/interview/wiki](https://github.com/mission-peace/interview/wiki)\n\n**How to Interview Engineers**\n\n[http://blog.triplebyte.com/how-to-interview-engineers](http://blog.triplebyte.com/how-to-interview-engineers)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","title":"Courses"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: Study\r\ntitle: Courses\r\ndate: 2017-11-28\r\n---\r\n\r\n**CS 007: PERSONAL FINANCE FOR ENGINEERS**\r\n\r\n- intro: Stanford University 2017-8\r\n- homepage: [https://cs007.blog/](https://cs007.blog/)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","title":"Study Resources"},"frontmatter":{"draft":false},"rawBody":"---\r\nlayout: post\r\ncategory: Study\r\ntitle: Study Resources\r\ndate: 2018-04-18\r\n---\r\n\r\n**draw.io**\r\n\r\n- intro: an app to create diagrams. You can use it online, download it or add it to Android and iOS for free\r\n- homepage: [https://www.draw.io/](https://www.draw.io/)\r\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","title":"Linux Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: linux_study\ntitle: Linux Resources\ndate: 2015-07-24\n---\n\n# Libraries\n\n**Porting Windows Dynamic Link Libraries to Linux**\n\n- intro: This repository contains a library that allows native Linux programs to load and call functions from a Windows DLL.\n- github: [https://github.com/taviso/loadlibrary](https://github.com/taviso/loadlibrary)\n\n# Tools\n\n**Proxyee-down**\n\n- intro: 一款支持Win/Mac/Linux的百度云文件下载工具\n- github: [https://github.com/monkeyWie/proxyee-down](https://github.com/monkeyWie/proxyee-down)\n- download: [http://pan.lanzou.com/b203390/](http://pan.lanzou.com/b203390/)\n\n## You-Get\n\n**You-Get: Dumb downloader that scrapes the web**\n\n![](/assets/linux_study/you-get.jpg)\n\n- homepage: [https://you-get.org/](https://you-get.org/)\n- github: [https://github.com/soimort/you-get](https://github.com/soimort/you-get)\n\n## Icdiff\n\n**Icdiff: improved colored diff**\n\n![](https://www.jefftk.com/icdiff-css-demo-tall-2x.png)\n\n- homepage: [https://www.jefftk.com/icdiff](https://www.jefftk.com/icdiff)\n- github: [https://github.com/jeffkaufman/icdiff](https://github.com/jeffkaufman/icdiff)\n\n## NcFTP\n\n**NcFTP**\n\n- intro: NcFTP Client (also known as just NcFTP) is a set of FREE application programs implementing the File Transfer Protocol (FTP).\n- homepage: [http://www.ncftp.com/ncftp/](http://www.ncftp.com/ncftp/)\n- my gist: [https://gist.github.com/handong1587/1f367ae3dfdb91f0ec4490a75459cb7c](https://gist.github.com/handong1587/1f367ae3dfdb91f0ec4490a75459cb7c)\n\n**Linux / Unix ncftp: Upload Directory Tree To Remote FTP Server Recursively**\n\n- blog: [http://www.cyberciti.biz/tips/linux-upload-the-files-and-directory-tree-to-remote-ftp-server.html](http://www.cyberciti.biz/tips/linux-upload-the-files-and-directory-tree-to-remote-ftp-server.html)\n\n## PowerShell\n\n*PowerShell for every system!***\n\n- homepage: [https://msdn.microsoft.com/en-us/powershell](https://msdn.microsoft.com/en-us/powershell)\n- github: [https://github.com/PowerShell/PowerShell](https://github.com/PowerShell/PowerShell)\n- docs: [https://github.com/PowerShell/PowerShell/tree/master/docs/learning-powershell](https://github.com/PowerShell/PowerShell/tree/master/docs/learning-powershell)\n\n**PowerShell is open sourced and is available on Linux**\n\n- blog: [https://azure.microsoft.com/en-us/blog/powershell-is-open-sourced-and-is-available-on-linux/](https://azure.microsoft.com/en-us/blog/powershell-is-open-sourced-and-is-available-on-linux/)\n\n# Blogs\n\n**Linux debugging tools I love**\n\n- keywords: strace, dstat, tcpdump + wireshark, perf, opensnoop\n- blog: [http://jvns.ca/blog/2016/07/03/debugging-tools-i-love/](http://jvns.ca/blog/2016/07/03/debugging-tools-i-love/)\n\n**gdb Debugging Full Example (Tutorial): ncurses**\n\n- blog: [http://www.brendangregg.com/blog/2016-08-09/gdb-example-ncurses.html](http://www.brendangregg.com/blog/2016-08-09/gdb-example-ncurses.html)\n\n**Command-line tools can be 235x faster than your Hadoop cluster**\n\n- blog: [http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html](http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html)\n\n# Resources\n\n**Awesome Linux Software: A curated list of awesome applications, softwares, tools and other materials for Linux distros**\n\n![](https://cloud.githubusercontent.com/assets/6733770/17458846/b8cca9ae-5bf7-11e6-85ba-e6f2461e82de.jpg)\n\n- github: [https://github.com/VoLuong/Awesome-Linux-Software](https://github.com/VoLuong/Awesome-Linux-Software)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","title":"Useful Linux Commands"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: linux_study\ntitle: Useful Linux Commands\ndate: 2015-07-25\n---\n\n# Pack/Unpack/Compress/Uncompress\n\n|file type    |  Pack/Compress                      | Unpack/Uncompress           |\n|:-----------:|:-----------------------------------:|:---------------------------:|\n|.tar         |  tar cvf FileName.tar DirName       |  tar xvf FileName.tar       |\n|.gz          |  gzip FileName                      |  gunzip FileName.gz         |\n|.gz          |  gzip FileName                      |  gzip -d FileName.gz        |\n|.tar.gz, .tgz|  tar -zcvf FileName.tar.gz DirName  |  tar zxvf FileName.tar.gz   |\n|.bz2         |  bzip2 -z FileName                  |  bzip2 -d FileName.bz2      |\n|.bz2         |                                     |  bunzip2 FileName.bz2       |\n|.tar.bz2     |  tar jcvf FileName.tar.bz2 DirName  |  tar jxvf FileName.tar.bz2  |\n|.bz          |                                     |  bzip2 -d FileName.bz       |\n|.bz          |                                     |  bunzip2 FileName.bz        |\n|.tar.bz      |                                     |  tar jxvf FileName.tar.bz   |\n|.Z           |  compress FileName                  |  uncompress FileName.Z      |\n|.tar.Z       |  tar Zcvf FileName.tar.Z DirName    |  tar Zxvf FileName.tar.Z    |\n|.zip         |  zip FileName.zip DirName           |  unzip FileName.zip         |\n|.zip         |  zip -r target.zip dir1 dir2 dir3   |  unzip FileName.zip -d targetFolder |\n|.rar         |  rar a FileName.rar DirName         |  rar x FileName.rar         |\n|.lha         |  lha -a FileName.lha FileName       |  lha -e FileName.lha        |\n|.rpm         |                                     |  rpm2cpio FileName.rpm \\| cpio -div |\n|.deb         |                                     |  ar x example.deb           |\n|.deb         |                                     |  ar p FileName.deb data.tar.gz \\| tar zx |\n|.deb         |                                     |  dpkg -x somepackage.deb ~/temp/ |\n|.xz          |                                     |  xz -d myfiles.tar.xz       |\n|.xz          |                                     |  tar -xf myfiles.tar        |\n|.xz          |                                     |  tar -Jxf myfiles.tar.xz    |\n|.7z          |  7za a myfiles.7z myfiles/          |  7za x myfiles.7z           |\n\nFor compress/uncompress those files:\n\n.tar .tgz .tar.gz .tar.Z .tar.bz .tar.bz2 .zip .cpio .rpm .deb .slp .arj .rar .ace .lha .lzh .lzx .lzs .arc .sda .sfx .lnx .zoo .cab .kar .cpt .pit .sit .sea\n\n```\ncompress:   sEx a FileName.* FileName\nuncompress: sEx x FileName.*\n```\n\n**unzip a tar gz archive to a specific destination**\n\n```\ncd /root/Desktop/folder\ntar xf /root/Documents/file.tar.gz\n```\n\nor:\n\n```\ntar xf file.tar.gz -C /root/Desktop/folder\n```\n\n## Wiew a detailed table of contents for an archive\n\n|file type    |  view contents cmd                  |\n|:-----------:|:-----------------------------------:|\n|.tar.gz      |  tar -tvf my-data.tar.gz            |\n\n# Print info\n\n| task                                              | command                 |\n| :-----------------------------                    | :---------------------: |\n| Print system info                                 | cat /proc/version       |\n| Print kernel version                              | uname -a                |\n| Print distribution information                    | lsb_release -a          |\n| Print software info                               | whereis SOFEWARE        |\n|                                                   | which SOFEWARE          |\n|                                                   | locate SOFEWARE         |\n| Print CPU info                                    | cat /proc/cpuinfo       |\n|                                                   | dmesg \\| grep -i xeon   |\n| Print memory info                                 | cat /proc/meminfo       |\n|                                                   | free -m                 |\n| Print pid info                                    | ps aux \\| grep 'target_pid' |\n| Print graphics card version                       | nvcc --version          |\n| Print graphics card GPU info                      | lspci  \\| grep -i vga   |\n| Print graphics card GPU running info              | nvidia-smi              |\n| Print graphics card GPU info dynamically          | watch -n0.1 nvidia-smi  |\n| Print disk free space                             | df -h                   |\n|                                                   | df -hl                  |\n| Print current folder size                         | du -sh DIRNAME          |\n| Print target folder volume                        | du -sh                  |\n| Print target folder volume (in MB)                | du -sm                  |\n| Prints one entry per line of output (bare format) | ls -1a                  |\n\nCuDNN Version Check:\n\n```\ncat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n```\n\nPrint lines 20 to 40:\n\n```\nsed -n '20,40p' file_name\n```\n\nor:\n\n```\nsed -n '20,40p;41q' file_name\n```\n\nor:\n\n```\nawk 'FNR>=20 && FNR<=40' file_name\n```\n\nTo print range with other specific line (5 - 8 & 10):\n\n```\n$ sed -n -e 5,8p -e 10p file\nLine 5\nLine 6\nLine 7\nLine 8\nLine 10\n```\n\n# Download\n\nDownload file:\n\n```\nwget \"http://domain.com/directory/4?action=AttachFile&do=view&target=file.tgz\"\n```\n\nDownload file to specific directory:\n\n```\nwget -O /home/omio/Desktop/ \"http://thecanadiantestbox.x10.mx/CC.zip\"\n```\n\nDownload all files from a folder on a website or FTP:\n\n```\nwget -r --no-parent --reject \"index.html*\" http://vision.cs.utexas.edu/voc/\n```\n\n# Transfer Files\n\nRemote transfer files, remote -> local:\n\n```\nscp account@111.111.111.111:/path/to/remote/file /path/to/local/\n```\n\nRemote transfer files, local -> remote:\n\n```\nscp /path/to/local/file account@111.111.111.111:/path/to/remote/\n```\n\nPrint directory structure:\n\n```\nls -l -R\n```\n\nPrint folder in current directory:\n\n```\nls -lF |grep /\n```\n\n```\nls -l |grep '^d'\n```\n\n\nPrint history command:\n\n```\nhistory\n```\n\n```\nhistory | less\n```\n\n# Rename\n\nPerl-based rename commands (-n: test commands; -v: print renamed files):\n\n```\nrename -n 's/\\.htm$/\\.html/' \\*.htm\n```\n\n```\nrename -v 's/\\.htm$/\\.html/' \\*.htm\n```\n\n**1. Replace first letter of all files' name with 'q':**\n\n```\nfor i in `ls`; do mv -f $i `echo $i | sed 's/^./q/'`; done\n```\n\n**same with a bash script:**\n\n```\nfor file in `ls`\ndo\n  newfile =`echo $i | sed 's/^./q/'`\n　mv $file $newfile\ndone\n```\n\n**2. Replace first 5 letters with 'abcde'**\n\n```\nfor i in `ls`; do mv -f $i `echo $i | sed 's/^...../abcde/'`;\n```\n\n**3. Replace last 5 letters with 'abcde'**\n\n```\nfor i in `ls`; do mv -f $i `echo $i | sed 's/.....$/abcde/'`;\n```\n\n**4. Add 'abcde' to the front**\n\n```\nfor i in `ls`; do mv -f $i `echo \"abcde\"$i`; done\n```\n\n**5. Convert all lower case to upper case**\n\n```\nfor i in `ls`; do mv -f $i `echo $i | tr a-z A-Z`; done\n```\n\n# Count\n\nCount lines in a document\n\n```\nwc -l /dir/file.txt\n```\n\n```\ncat /dir/file.txt | wc -l\n```\n\nFilter and count only lines with pattern, or with -v to invert match\n\n```\ngrep -w \"pattern\" -c file\n```\n\n```\ngrep -w \"pattern\" -c -v file\n```\n\nCount files in the current directory:\n\n```\nls -l | grep “^-” | wc -l\n```\n\nexample: counting all js files in directory \"/home/account/\" (recursively):\n\n```\nls -lR /home/account | grep js | wc -l\n```\n\n```\nls -l \"/home/account\" | grep \"js\" | wc -l\n```\n\nCount files in the current directory, recursively:\n\n```\nls -lR | grep “^-” | wc -l\n```\n\nCount folders in the current directory, recursively:\n\n```\nls -lR | grep “^d” | wc -l\n```\n\nCount words in a file:\n\n```\ngrep -o objStr  filename|wc -l\ngrep -o ‘objStr1\\|objStr2'  filename|wc -l\n```\n\nCount words in a file under Vim:\n\n```\n:%s/objStr//gn\n```\n\n# Search\n\n**Search for a file by its file name**\n\nThe command below will search for the query in the current directory and any subdirectories.\nUsing -iname instead of -name ignores the case of your query. The -name command is case-sensitive.\n\n```\nfind -iname \"filename\"\n```\n\n**Finding all files containing a text string**\n\n```\ngrep -rnw '/path/to/somewhere/' -e \"pattern\"\n\ngrep --include=\\*.{c,h} -rnw '/path/to/somewhere/' -e \"pattern\"\n\ngrep --exclude=*.o -rnw '/path/to/somewhere/' -e \"pattern\"\n\ngrep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/somewhere/' -e \"pattern\"\n```\n\n1. -r or -R is recursive,\n2. -n is line number, and\n3. -w stands match the whole word.\n4. -l (lower-case L) can be added to just give the file name of matching files.\n5. Along with these, --exclude or --include parameter could be used for efficient searching.\n\n**Finding all files containing a text string on Linux**\n\n- stackoverflow: [http://stackoverflow.com/questions/16956810/finding-all-files-containing-a-text-string-on-linux](http://stackoverflow.com/questions/16956810/finding-all-files-containing-a-text-string-on-linux)\n\nCount occurrences of a char(e.g, 'aaa') in plain text file\n\n```\nfgrep -o 'aaa' <file> | wc -l\n```\n\n**references**\n\n**How to Find a File in Linux**\n\n[http://www.wikihow.com/Find-a-File-in-Linux](http://www.wikihow.com/Find-a-File-in-Linux)\n\n**How to find files in Linux using 'find'**\n\n[http://www.codecoffee.com/tipsforlinux/articles/21.html](http://www.codecoffee.com/tipsforlinux/articles/21.html)\n\n**35 Practical Examples of Linux Find Command**\n\n[http://www.tecmint.com/35-practical-examples-of-linux-find-command/](http://www.tecmint.com/35-practical-examples-of-linux-find-command/)\n\n**How to use grep to search for strings in files on the shell**\n\n[https://www.howtoforge.com/tutorial/linux-grep-command/](https://www.howtoforge.com/tutorial/linux-grep-command/)\n\n**Find All Files of a Particular Size**\n\n```\nfind /home/ -type f -size +512k -exec ls -lh {} \\;\n```\n\nAs units you can use:\n```\n    b – for 512-byte blocks (this is the default if no suffix is used)\n    c – for bytes\n    w – for two-byte words\n    k – for Kilobytes (units of 1024 bytes)\n    M – for Megabytes (units of 1048576 bytes)\n    G – for Gigabytes (units of 1073741824 bytes)\n```\n\nref: [http://www.ducea.com/2008/02/12/linux-tips-find-all-files-of-a-particular-size/](http://www.ducea.com/2008/02/12/linux-tips-find-all-files-of-a-particular-size/)\n\n# GDB\n\n**GDB reference card**\n\n[https://web.stanford.edu/class/cs107/gdb_refcard.pdf](https://web.stanford.edu/class/cs107/gdb_refcard.pdf)\n\n**GDB Tutorial: A Walkthrough with Examples**\n\n- slides: [https://www.cs.umd.edu/~srhuang/teaching/cmsc212/gdb-tutorial-handout.pdf](https://www.cs.umd.edu/~srhuang/teaching/cmsc212/gdb-tutorial-handout.pdf)\n\n**GNU GDB Debugger Command Cheat Sheet**\n\n[http://www.yolinux.com/TUTORIALS/GDB-Commands.html](http://www.yolinux.com/TUTORIALS/GDB-Commands.html)\n\n**Cheat Sheet**:\n\n| shortcut   | command       | explanation      |\n|:----------:|:-------------:|:----------------:|\n| q          | quit          |                  |\n| h          | help          |                  |\n| h command  | help command  | print command meaning|\n| b          | break         | example: b 5     |\n| disable codenum|           |                  |\n| enable codenum |           ||\n| condition codenum xxx|     | set break condition|\n| c          | continue      |                  |\n| n          | next          |                  |\n| s          | step          |                  |\n| w          |               | print code in current execution point|\n| j codenum  |               | jump to line j   |\n| l          |               | list nearby code |\n| p          |               | print var value  |\n| a          |               | print current func/var value|\n| Enter      |               | repeat last command|\n\n```\ngdb --arg python demo.py 2>&1 | tee demo.log\n```\n\n# pdb\n\n```\nimport pdb\npdb.set_trace()\n```\n\n**exit pdb and allow program to continue**\n\n1. To remove the breakpoint (if inserted it manually):\n\n    ```\n    (Pdb) break\n    Num Type         Disp Enb   Where\n    1   breakpoint   keep yes   at /path/to/test.py:5\n    (Pdb) clear 1\n    Deleted breakpoint 1\n    (Pdb) continue\n    ```\n\n2. if you're using pdb.set_trace(), you can try this (although if you're using pdb in more fancy ways, this may break things...)\n\n    ```\n    (Pdb) pdb.set_trace = lambda: None  # This replaces the set_trace() function!\n    (Pdb) continue\n    # No more breaks!\n    ```\n\n- ref: [http://stackoverflow.com/questions/17820618/how-to-exit-pdb-and-allow-program-to-continue](http://stackoverflow.com/questions/17820618/how-to-exit-pdb-and-allow-program-to-continue)\n\n# Ctags\n\n| command         | explanation                                       |\n|:---------------:|:-------------------------------------------------:|\n| ctags –R *      | Generate tags files in source code root directory |\n| vim -t func/var | find func/var definition                          |\n| :ts             | give a list if func/var has multiple definitions  |\n| Ctrl+]          | jump to definition                                |\n| Ctrl+T          | jump back                                         |\n\n# screen\n\n| task                        | command                       |\n|:---------------------------:|:-----------------------------:|\n| Run program in screen mode  | screen python demo.py --gpu 1 |\n| Detach screen               | Ctrl + a, c                   |\n| Detach screen               | Ctrl + a, d                   |\n| Re-connect screen           | screen -r pid                 |\n| Display all screens         | screen -ls                    |\n| Delete screen               | kill pid                      |\n| Naming a screen             | screen -S sessionName         |\n\n# nohup\n\n```\nnohup command-with-options &\n```\n\n```\nnohup xxx.sh 1 > log.txt 2>&1 &\n```\n\n**nohup - get the process ID to kill a nohup process**\n\n```\nps -ef | grep \"command name\"\n```\n\nor:\n\n```\nnohup command-with-options & \necho $! > save_pid.txt\nkill -9 `cat save_pid.txt`\n```\n\n# cscope\n\n| task                              | command                         |\n|:---------------------------------:|:-------------------------------:|\n| Generate Cscope database          | find . -name \"*.c\" -o -name \"*.cc\" -o -name \"*.cpp\" -o -name \"*.cu\" -o -name \"*.h\" -o -name \"*.hpp\" -o -name \"*.py\" -o -name \"*.proto\" > cscope.files |\n| Build a Cscope reference database | cscope -q -R -b -i cscope.files |\n| Start the Cscope browser          | cscope -d                       |\n| Exit a Cscope browser             | Ctrl + d                        |\n\n**Cheat Sheet**\n\n```\n-b  Build the cross-reference only.\n-C  Ignore letter case when searching.\n-c  Use only ASCII characters in the cross-ref file (don’t compress).\n-d  Do not update the cross-reference.\n-e  Suppress the -e command prompt between files.\n-F  symfile Read symbol reference lines from symfile.\n-f  reffile Use reffile as cross-ref file name instead of cscope.out.\n-h  This help screen.\n-I  incdir Look in incdir for any #include files.\n-i  namefile Browse through files listed in namefile, instead of cscope.files\n-k  Kernel Mode – don’t use /usr/include for #include files.\n-L  Do a single search with line-oriented output.\n-l  Line-oriented interface.\n-num  pattern Go to input field num (counting from 0) and find pattern.\n-P  path Prepend path to relative file names in pre-built cross-ref file.\n-p  n Display the last n file path components.\n-q  Build an inverted index for quick symbol searching.\n-R  Recurse directories for files.\n-s  dir Look in dir for additional source files.\n-T  Use only the first eight characters to match against C symbols.\n-U  Check file time stamps.\n-u  Unconditionally build the cross-reference file.\n-v  Be more verbose in line mode.\n-V  Print the version number.\n```\n\n# Vim\n\n**Shifting blocks visually**\n\n[http://vim.wikia.com/wiki/Shifting_blocks_visually](http://vim.wikia.com/wiki/Shifting_blocks_visually)\n\n| mode        | task                      | command   |\n|:-----------:|:-------------------------:|:---------:|\n| normal mode | indent the current line   | type \\>\\> |\n| normal mode | unindent the current line | type \\<\\< |\n| insert mode | indent the current line   | Ctrl-T    |\n| insert mode | unindent the current line | Ctrl-D    |\n\nFor all commands, pressing **.** repeats the operation.\n\nFor example, typing **5>>..** shifts five lines to the right, and then repeats\nthe operation twice so that the five lines are shifted three times.\n\nInsert current file name:\n\n```\n:r! echo %\n```\n\n**Insert characters at specific lines head**\n\n```\n:80,90s/^/#/\n```\n\n**Switch windows**\n\n```\ngt            go to next tab\ngT            go to previous tab\n{i}gt         go to tab in position i\n```\n\n**Fold code block under spf-13 Vim**\n\n```\nza\n```\n\n```\nvimdiff file1 file2\n```\n\nOr After vimed file1:\n\n```\n:vert diffsplit file2\n```\n\n# Matlab\n\nComment multi-lines in Matlab: Ctrl+R, Ctrl+T\n\nLaunch Matlab:\n\n```\ncd /usr/local/bin/\nsudo ln -s /usr/local/MATLAB/R2012a/bin/matlab Matlab\ngedit ~/.bashrc\nalias matlab=\"/usr/local/MATLAB/R2012a/bin/matlab\"\n```\n\nStart MATLAB Without Desktop:\n\n```\nmatlab -nojvm -nodisplay -nosplash\n```\n\nMatlab + nohup:\n\nrunGenerareSSProposals.sh:\n\n```\n#!/bin/sh\ncd /path/to/detection-proposals\nmatlab -nojvm -nodisplay -nosplash -r \"startup; callRunCOCO; exit\"\n```\n\nrunNohup.sh:\n\n```\ntime=`date +%Y%m%d_%H%M%S`\ncd /path/to/detection-proposals\nnohup ./runGenerareSSProposals.sh > runGenerareSSProposals_${time}.log 2>&1 &\necho $! > save_runGenerareSSProposals_val_pid.txt\n```\n\n# Hadoop\n\n**Delete a directory from Hadoop cluster**\n\n```\nhadoop fs -rm -r -f /user/the/path/to/your/dir\n```\n\n# Others\n\n**Hotkeys to speed up Linux CLI navigation:**\n\n| hotkey    |                                                                                                              |\n| :------:  | :----------------------------------------------------------------------------------------------------------: |\n| Ctrl + a  | go to the start of the command line                                                                          |\n| Ctrl + e  | go to the end of the command line                                                                            |\n| Ctrl + k  | delete from cursor to the end of the command line                                                            |\n| Ctrl + u  | delete from cursor to the start of the command line                                                          |\n| Ctrl + w  | delete from cursor to start of word (i.e. delete backwards one word)                                         |\n| Ctrl + y  | paste word or text that was cut using one of the deletion shortcuts (such as the one above) after the cursor |\n| Ctrl + xx | move between start of command line and current cursor position (and back again)                              |\n| Alt + b   | move backward one word (or go to start of word the cursor is currently on)                                   |\n| Alt + f   | move forward one word (or go to end of word the cursor is currently on)                                      |\n| Alt + d   | delete to end of word starting at cursor (whole word if cursor is at the beginning of word)                  |\n| Alt + c   | capitalize to end of word starting at cursor (whole word if cursor is at the beginning of word)              |\n| Alt + u   | make uppercase from cursor to end of word                                                                    |\n| Alt + l   | make lowercase from cursor to end of word                                                                    |\n| Alt + t   | swap current word with previous                                                                              |\n| Ctrl + f  | move forward one character                                                                                   |\n| Ctrl + b  | move backward one character                                                                                  |\n| Ctrl + d  | delete character under the cursor                                                                            |\n| Ctrl + h  | delete character before the cursor                                                                           |\n| Ctrl + t  | swap character under cursor with the previous one                                                            |\n\nLaunch terminal in Ubuntu: Ctrl+Alt+T\n\nCreate symbol link:\n\n```\nln -s EXISTING_FILE SYMLINK_FILE\nln -s /path/to/file /path/to/symlink\n```\n\nOpen image:\n\n```\neog /path/to/image/im.jpg\n```\n\n```\ndisplay /path/to/image/im.jpg\n```\n\nConvert text files with DOS or MAC line breaks to Unix line breaks:\n\n```\nsudo dos2unix /path/to/file\n```\n\nor:\n\n```\nsed -i 's/\\r//' /path/to/file\n```\n\n```\nsudo sed -i -e 's/\\r$//' /path/to/file\n```\n\nCreate new file list:\n\n```\nsed 's?^?'`pwd`'/detection_images/?; s?$?.jpg?' trainval.txt > voc.2007trainval.list\n```\n\n**Merge two files consistently line by line**\n\n```\npaste -d \" \" file1.txt file2.txt\n```\n\n[http://stackoverflow.com/questions/16394176/how-to-merge-two-files-consistently-line-by-line](http://stackoverflow.com/questions/16394176/how-to-merge-two-files-consistently-line-by-line)\n\n**shuffle file lines**\n\n```\nshuf file.list > file_shuffled.list\n```\n\n**Combine multiple files into one file**\n\n```\ncat file1 file2 file3 .... >> merged_file\n```\n\n**Show all hidden characters**:\n\n```\ncat -A filename\n```\n\n**Get recursive full-path listing**\n\n```\nfind /path/to/folder\n```\n\nwant files only (omit directories, devices, etc):\n\n```\nfind /path/to/folder -type f\n```\n\n**Remove specific file types**\n\n```\nrm `find -type f /path/to/dir/ | grep \"filetype\"`\n```\n\n```\nrm `find -type f /path/to/dir/ | grep -E \"filetype1 | filetype2\"`\n```\n\n**Zip multiple files/folers to one named zip file**\n\n```\nzip -r target.zip file1 file2 folder1 folder2\n```\n\n**Remove Windows format line breaks**\n\n```\nsed -i 's/^M$//g'\n```\n\nNote:\n\n^M = Ctrl+v,Ctrl+m\n\n**Replace tab characters with spaces**\n\n```\nsed -i 's/^I//g'\n```\n\nNote:\n\n^I = Ctrl+v,Ctrl+I\n\n**Remove duplicate text lines**\n\n```\nsort {file-name} | uniq -u\n```\n\n**Remove duplicate text lines and only keep one line**\n```\nperl -lne '$seen{$_}++ and next or print;' data.txt > output.txt\n```\n\n**Exit a shell if some commands do not execute correctly**\n\n```\n./do_something.sh || exit 1\n```\n\n**Split string on Shell**\n\n```\nstr=\"/aaaa/bbbbb/cccc\"\necho $str | tr \"/\" \"\\n\"\n```\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","title":"Linux Git Commands"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: linux_study\ntitle: Linux Git Commands\ndate: 2015-08-02\n---\n\n- Push local modification to server\n\n```\ngit add deep learning/paper/reinforcement\ngit commit -m \"xxxxxx\"\ngit push -u origin master\n```\n\n- Solution to ERROR: \"fatal: The remote end hung up unexpectedly\"\n\n```\ngit config http.postBuffer 524288000\ngit config --global http.postBuffer 157286400\n```\n\n- Undo last two commits which not pushed yet (NOTE: this will also delete relevant local files)\n\n```\ngit reset --hard HEAD~2\n```\n\n- Undo commit, also roll-back codes to previous commit\n\n```\ngit reset --hard commit_id\n```\n\n- Undo commit, but won't undo local codes modification (Can re-commit local changes by `git commit`):\n\n```\ngit reset HEAD~\n```\n\nor:\n\n\n```\ngit reset HEAD to-undo-file\n```\n\nor:\n\n```\ngit reset commit_id\n```\n\n- Only view how many non-pushed commits\n\n```\ngit status\n```\n\n- Only view comments/descriptions of non-pushed commits\n\n```\ngit cherry -v\n```\n\n- View detailed informations of non-pushed commits\n\n```\ngit log master ^origin/master\n```\n\n- Find id of last commit\n\n```\ngit log\n```\n\n- Clone a particular version of commit-id\n\nAfter git clone the newest repo:\n\n```\ngit checkout [commit-id]\n```\n\n- Convert that repo to my forked repo (stay tuned..)\n\n- Clone a specific Git branch\n\n```\ngit clone -b\n```\n\nExample:\n\n``` \ngit clone -b my-branch https://git@github.com/username/myproject.git\n```\n\nClone a Fast R-CNN COCO branch:\n\n``` \ngit clone --recursive -b coco https://github.com/rbgirshick/fast-rcnn.git\n```\n\n- Save git username/password on Git for Windows\n\nCreate .git-credentials to save git username/password:\n\n```\nhttps://username:password@github.com \n```\n\n```\ngit config --global credential.helper store\n```\n\nRe-run git bash.\n\n- One way to address SSL certificate problem\n\nSSL certificate problem:\n\n```\n$ git clone https://github.com/BVLC/caffe.git\nCloning into 'caffe'...\nfatal: unable to access 'https://github.com/BVLC/caffe.git/': SSL certificate problem: certificate is not yet valid\n```\n\nThe easiest way is to disable the SSL CERT verification (This will prevent CURL to verity the HTTPS certification):\n\n```\ngit config --global http.sslVerify false\n```\n\nFor one repository only:\n\n```\ngit config http.sslVerify falseSoluton\n```\n\n([http://stackoverflow.com/questions/3777075/ssl-certificate-rejected-trying-to-access-github-over-https-behind-firewall](http://stackoverflow.com/questions/3777075/ssl-certificate-rejected-trying-to-access-github-over-https-behind-firewall))\n\n**Remove all local branches which are remotely deleted**\n\n```\ngit fetch -p\n```\n\n- Failed to connect to github.com port 443: Timed out\n\n**GitHub - failed to connect to github 443 windows/ Failed to connect to gitHub - No Error**\n\n- stackoverflow: [http://stackoverflow.com/questions/18356502/github-failed-to-connect-to-github-443-windows-failed-to-connect-to-github](http://stackoverflow.com/questions/18356502/github-failed-to-connect-to-github-443-windows-failed-to-connect-to-github)\n\n**Keeping a brach of a forked repo up-to-date**\n\n```\ngit clone https://github.com/handong1587/caffe_cuhk\ncd caffe_cuhk\ngit checkout master-with-comment\ngit remote add upstream https://github.com/BVLC/caffe\ngit fetch upstream\ngit pull upstream master\n```\n\nThen do some conflicts merging jobs...\n\nFinally:\n\n```\ngit push\n```\n\n-ref: [https://gist.github.com/CristinaSolana/1885435](https://gist.github.com/CristinaSolana/1885435)\n\n**Find and restore a deleted file from commit history**\n\n```\ngit checkout $commit~1 filename\n```\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","title":"Linux SVN Commands"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: linux_study\ntitle: Linux SVN Commands\ndate: 2015-08-03\n---\n\n1. Create a repository:\n\n```\nsvnadmin create /svn/foo/mydirname\n```\n\n2. Want to version control /home/user/mydirname:\n\n```\ncd /home/user/mydirname\n```\n\n3. This only creates the \".svn\" folder for version control:\n\n```\nsvn co file:///svn/foo/mydirname .\n```\n\n4. Tell svn you want to version control all files in this directory:\n\n```\nsvn add ./*\n```\n\n5. Check the files in:\n\n```\nsvn ci\n```\n\n6. Check file in with comment:\n\n```\nsvn ci -m \"your_comment\"\n```\n\n7. Checkout project\n\n```\ncd /home/user/projectx\nsvn checkout file:///svnrepo/projectx .\n```\n\n8. Show only the last 4 log entries (need to \"svn update\" first in working copy directory)\n\n```\nsvn update\nsvn log --limit 4\nsvn log -l 4\n```\n\n9. Add all files:\n\n```\nsvn add --force path/to/dir\nsvn add --force .\n```\n\n10. Checkout specified revision (1234 = some resivion number)\n\n```\nsvn checkout svn://somepath@1234 working-directory\nsvn checkout -r 1234 url://repository/path\nsvn checkout -r 10 https://101.101.101.101/svn/SomeRepo\n```\n\n11. Ignore a directory:\n\n```\nsvn propset svn:ignore dirname .\n```\n\n12. Ignore a directory if already checkin using file.txt:\n\n```\nsvn rm --keep-local dirname\nsvn propset svn:ignore -F file.txt .\n```\n\n13. Specify a file that contains a list of file name patterns to ignore. \nAlso, use the -R (or --recursive) flag to specify that the command should be applied recursively:\n\n.svnignore (like .gitignore):\n\n```\nbin gen proguard .classpath .project local.properties Thumbs.db *.apk *.ap_ *.class *.dex\nsvn propset svn:ignore -R -F .svnignore .\n```\n\n# Reference\n\n**Getting svn to ignore files and directories**\n\n[http://superchlorine.com/2013/08/getting-svn-to-ignore-files-and-directories/](http://superchlorine.com/2013/08/getting-svn-to-ignore-files-and-directories/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","title":"Create Multiple Forks of a GitHub Repo"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: linux_study\ntitle: Create Multiple Forks of a GitHub Repo\ndate: 2015-12-18\n---\n\n# Step-1: Clone the original repo to your local machine\n\n```\ngit clone https://github.com/handong1587/caffe.git caffe-yolo\n```\n\n![](/assets/linux_study/create_multi_fork_1.jpg)\n\n```\ncd caffe-yolo\n```\n\n# Step-2: Create a new empty repo in your GitHub account\n\n![](/assets/linux_study/create_multi_fork_2.jpg)\n\n# Step-3: Manually create the necessary remote links\n\n```\ngit remote -v\n```\n\n![](/assets/linux_study/create_multi_fork_3.jpg)\n\n# Step-4: Rename origin to upstream and add our new empty repo as the origin\n\n```\ngit remote rename origin upstream\ngit remote add origin https://github.com/handong1587/caffe-yolo.git\ngit remote -v\n```\n\n![](/assets/linux_study/create_multi_fork_4.jpg)\n\n# Step-5: Push from your local repo to your new remote one\n\n```\ngit push -u origin master\n```\n\n![](/assets/linux_study/create_multi_fork_5.jpg)\n\nDone.\n\n# Reference\n\n(In step-4 the author use a SSH method to \"git remote add\" while I can only use HTTPS method to finally succeed)\n\n**Create multiple forks of a GitHub repo**\n\n[https://adrianshort.org/create-multiple-forks-of-a-github-repo/](https://adrianshort.org/create-multiple-forks-of-a-github-repo/)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","title":"Setup vsftpd on Ubuntu 14.10"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: linux_study\ntitle: Setup vsftpd on Ubuntu 14.10\ndate: 2016-07-27\n---\n\n# Setup vsftpd\n\nInstall vsftpd:\n\n```\nsudo apt-get install vsftpd\n```\n\nCheck if vsftpd installed successfully:\n\n```\nsudo service vsftpd status\n```\n\nAdd `/home/uftp` as user home directory:\n\n```\nsudo mkdir /data/jinbin.lin/uftp\n```\n\nAdd user `uftp` and set password:\n\n```\nsudo useradd -d /data/jinbin.lin/uftp -s /bin/bash uftp\n```\n\nSet user password (need to enter password twice):\n\n```\nsudo passwd uftp\n```\n\nEdit vsftpd configuration file:\n\n```\n/etc/vsftpd.conf\n```\n\nAdd following commands at the end of `vsftpd.conf`:\n\n```\nuserlist_deny=NO\nuserlist_enable=YES\nuserlist_file=/etc/allowed_users\n```\n\nModify following configurations:\n\n```\nlocal_enable=YES\nwrite_enable=YES\n```\n\nEdit `/etc/allowed_users`，add username: uftp\n\nCheck file `/etc/ftpusers`, delete `uftp` (if file contains this username). \nThis file recording usernames which are forbidden to access FTP server.\n\nRestart vsftpd:\n\n```\nsudo service vsftpd restart\n```\n\n# Close FTP server\n\n```\nsudo service vsftpd stop\n```\n\n# Visit FTP server \n\n(By default, the anonymous user is disabled)\n\n```\nftp://user:password@hostname/\n```\n\n# Forbid user access top level directory\n\nCreate file `vsftpd.chroot_list` but don't add anything:\n\n```\nsudo touch /etc/vsftpd.chroot_list\n```\n\nModify configurations as following:\n\n```\nchroot_local_user=YES\nchroot_list_enable=NO\nchroot_list_file=/etc/vsftpd.chroot_list\n```\n\nIf want to have write permission to user home directory (otherwise you would meet this error when login: \n\"500 OOPS: vsftpd: refusing to run with writable root inside chroot ()\"):\n\n```\nallow_writeable_chroot=YES\n```\n\nRestart vsftpd:\n\n```\nsudo service vsftpd restart\n```\n\n# Does not allow the user to change the specified chroot_list_file root\n\n```\nchroot_local_user=NO\nchroot_list_enable=YES\nchroot_list_file=/etc/vsftpd.chroot_list\n```\n\n# Allows only specified users to change chroot_list_file root\n\n```\nchroot_local_user=YES\nchroot_list_enable=YES\nchroot_list_file=/etc/vsftpd.chroot_list\n```\n\n# Frequently used command\n\n`mkdir`\n\n`dir` or `ls`\n\n`put`\n\n`get`\n\n# Refs\n\n**How to Install and Configure vsftpd on Ubuntu 14.04 LTS**\n\n[http://www.liquidweb.com/kb/how-to-install-and-configure-vsftpd-on-ubuntu-14-04-lts/](http://www.liquidweb.com/kb/how-to-install-and-configure-vsftpd-on-ubuntu-14-04-lts/)\n\n**vsftpd 配置:chroot_local_user与chroot_list_enable详解**\n\n[http://blog.csdn.net/bluishglc/article/details/42398811](http://blog.csdn.net/bluishglc/article/details/42398811)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","title":"vsftpd Commands"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: linux_study\ntitle: vsftpd Commands\ndate: 2016-07-28\n---\n\nFTP命令是Internet用户使用最频繁的命令之一，不论是在DOS还是UNIX操作系统下使用FTP，都会遇到大量的FTP内部命令。\n熟悉并灵活应用FTP的内部命令，可以大大方便使用者，并收到事半功倍之效。 \nFTP的命令行格式为： ftp -v -d -i -n -g [主机名] ，其中 -v 显示远程服务器的所有响应信息； \n-n 限制ftp的自动登录，即不使用； \n.n etrc文件； \n-d 使用调试方式； \n-g 取消全局文件名。\n\nftp使用的内部命令如下(中括号表示可选项):\n\n1.![cmd[args]]：在本地机中执行交互shell，exit回到ftp环境，如： !ls*.zip.\n\n2.$ macro-ame[args]：执行宏定义macro-name.\n\n3.account[password]：提供登录远程系统成功后访问系统资源所需的补充口令。\n\n4.append local-file[remote-file]：将本地文件追加到远程系统主机，若未指定远程系统文件名，则使用本地文件名。\n\n5.ascii：使用ascii类型传输方式。\n\n6.bell：每个命令执行完毕后计算机响铃一次。\n\n7.bin：使用二进制文件传输方式。\n\n8.bye：退出ftp会话过程。\n\n9.case：在使用mget时，将远程主机文件名中的大写转为小写字母。\n\n10.`cd remote-dir`：进入远程主机目录。\n\n11.cdup：进入远程主机目录的父目录。\n\n12.`chmod mode file-name`：将远程主机文件file-name的存取方式设置为mode，如：`chmod 777 a.out`。\n\n13.close：中断与远程服务器的ftp会话(与open对应)。\n\n14.cr：使用asscii方式传输文件时，将回车换行转换为回行。\n\n15.`delete remote-file`：删除远程主机文件。\n\n16.debug [debug-value]：设置调试方式，显示发送至远程主机的每条命令，如：deb up 3，若设为0，表示取消debug。\n\n17.`dir [remote-dir] [local-file]`：显示远程主机目录，并将结果存入本地文件local-file。\n\n18.disconnection：同close。\n\n19.form format：将文件传输方式设置为format，缺省为file方式。\n\n20.`get remote-file [local-file]`：将远程主机的文件remote-file传至本地硬盘的local-file。\n\n21.glob：设置mdelete，mget，mput的文件名扩展，缺省时不扩展文件名，同命令行的-g参数。\n\n22.hash：每传输1024字节，显示一个hash符号(#)。\n\n23.help [cmd]：显示ftp内部命令cmd的帮助信息，如：help get。\n\n24.idle [seconds]：将远程服务器的休眠计时器设为[seconds]秒。\n\n25.image：设置二进制传输方式(同binary)。\n\n26.lcd [dir]：将本地工作目录切换至dir。\n\n27.`ls [remote-dir] [local-file]`：显示远程目录remote-dir，并存入本地文件local-file。\n\n28.macdef macro-name：定义一个宏，遇到macdef下的空行时，宏定义结束。\n\n29.`mdelete [remote-file]`：删除远程主机文件。\n\n30.`mdir remote-files local-file`：与dir类似，但可指定多个远程文件，如：mdir *.o.*.zipoutfile。\n\n31.`mget remote-files`：传输多个远程文件。\n\n32.`mkdir dir-name`：在远程主机中建一目录。\n\n33.`mls remote-file local-file`：同nlist，但可指定多个文件名。\n\n34.mode [modename]：将文件传输方式设置为modename，缺省为stream方式。\n\n35.modtime file-name：显示远程主机文件的最后修改时间。\n\n36.mput local-file：将多个文件传输至远程主机。\n\n37.newer file-name： 如果远程机中file-name的修改时间比本地硬盘同名文件的时间更近，则重传该文件。\n\n38.nlist [remote-dir] [local-file]：显示远程主机目录的文件清单，并存入本地硬盘的local-file。\n\n39.nmap [inpattern outpattern]：设置文件名映射机制，使得文件传输时，文件中的某些字符相互转换，如：nmap $1.$2.$3[$1，$2].[$2，$3]，则传输文件a1.a2.a3时，文件名变为a1，a2。该命令特别适用于远程主机为非UNIX机的情况。\n\n40.ntrans [inchars[outchars]]：设置文件名字符的翻译机制，如ntrans 1R，则文件名LLL将变为RRR。\n\n41.open host [port]：建立指定ftp服务器连接，可指定连接端口。\n\n42.passive：进入被动传输方式。\n\n43.prompt：设置多个文件传输时的交互提示。\n\n44.proxy ftp-cmd：在次要控制连接中，执行一条ftp命令， 该命令允许连接两个ftp服务器，以在两个服务器间传输文件。第一条ftp命令必须为open，以首先建立两个服务器间的连接。\n\n45.`put local-file [remote-file]`：将本地文件local-file传送至远程主机。\n\n46.pwd：显示远程主机的当前工作目录。\n\n47.quit：同bye，退出ftp会话。\n\n48.quote arg1，arg2...：将参数逐字发至远程ftp服务器，如：quote syst.\n\n49.`recv remote-file [local-file]`：同get。\n\n50.`reget remote-file [local-file]`：类似于get，但若local-file存在，则从上次传输中断处续传。\n\n51.rhelp [cmd-name]：请求获得远程主机的帮助。\n\n52.rstatus [file-name]：若未指定文件名，则显示远程主机的状态，否则显示文件状态。\n\n53.`rename [from] [to]`：更改远程主机文件名。\n\n54.reset：清除回答队列。\n\n55.restart marker：从指定的标志marker处，重新开始get或put，如：restart 130。\n\n56.`rmdir dir-name`：删除远程主机目录。\n\n57.runique：设置文件名唯一性存储，若文件存在，则在原文件后加后缀 ..1，.2等。\n\n58.send local-file[remote-file]：同put。\n\n59.sendport：设置PORT命令的使用。\n\n60.site arg1，arg2...：将参数作为SITE命令逐字发送至远程ftp主机。\n\n61.`size file-name`：显示远程主机文件大小，如：site idle 7200。\n\n62.`status`：显示当前ftp状态。\n\n63.struct [struct-name]：将文件传输结构设置为struct-name，缺省时使用stream结构。\n\n64.sunique：将远程主机文件名存储设置为唯一(与runique对应)。\n\n65.system：显示远程主机的操作系统类型。\n\n66.tenex：将文件传输类型设置为TENEX机的所需的类型。\n\n67.tick：设置传输时的字节计数器。\n\n68.trace：设置包跟踪。\n\n69.type [type-name]：设置文件传输类型为type-name，缺省为ascii，如：type binary，设置二进制传输方式。\n\n70.umask [newmask]：将远程服务器的缺省umask设置为newmask，如：umask 3。\n\n71.`user user-name [password] [account]`：向远程主机表明自己的身份，需要口令时，必须输入口令，如：user anonymous my@email。\n\n72.verbose：同命令行的-v参数，即设置详尽报告方式，ftp服务器的所有 响应都将显示给用户，缺省为on.\n\n73.?[cmd]：同help.\n\n# Ref\n\n[http://www.jb51.net/os/RedHat/1133.html](http://www.jb51.net/os/RedHat/1133.html)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","title":"Mac Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: working_on_mac\ntitle: Mac Resources\ndate: 2015-07-25\n---\n\n# Useful Commands\n\n**Check MD5 Hash on Mac**\n\n```\nopenssl md5 big_huge_file.iso\n```\n\n# Tools\n\n**Mounty for NTFS**\n\n- intro: A tiny tool to re-mount write-protected NTFS volumes under Mac OS X 10.9+ in read-write mode.\n- homepage: [http://enjoygineering.com/mounty/](http://enjoygineering.com/mounty/)\n\n**Texture: A word processor for structured content.**\n\n- intro: As open as LaTeX and as simple as a classic word processor.\n- homepage: [http://substance.io/texture/](http://substance.io/texture/)\n- github: [https://github.com/substance/texture](https://github.com/substance/texture)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","title":"Windows Dev Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: working_on_windows\ntitle: Windows Dev Resources\ndate: 2015-10-27\n---\n\n**How to Install gcc/g++ 4.7.2 on Cygwin**\n\n- blog: [http://matpack.de/cygwin/](http://matpack.de/cygwin/)\n\n**How to install a newer version of GCC**\n\n- blog: [http://cygwin.wikia.com/wiki/How_to_install_a_newer_version_of_GCC](http://cygwin.wikia.com/wiki/How_to_install_a_newer_version_of_GCC)\n\n# Perforce / P4V\n\n**How to delete a perforce pending changelist**\n\n11 clicks in P4V, through an arbitrary sequence of menu items.\n\n1. Right click on the changelist\n2. Delete shelved files\n3. 'Yes'\n4. Right click on the changelist\n5. Remove all jobs\n6. Right click on the changelist\n7. Revert files\n8. 'Revert'\n9. Right click on the changelist\n10. Delete pending changelist\n11. 'Yes'\n\n- stackoverflow: [http://stackoverflow.com/questions/12296080/how-to-delete-a-perforce-client-with-pending-changelist](http://stackoverflow.com/questions/12296080/how-to-delete-a-perforce-client-with-pending-changelist)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","title":"Windows Commands and Utilities"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: working_on_windows\ntitle: Windows Commands and Utilities\ndate: 2015-10-27\n---\n\n# Commands\n\n**Create symbolic link in Windows 7**\n\nUse admin privileges to run Command Prompt.\n\nExample:\n\n```\nD:\\coding\\Python_VS2015\\densebox\\fast-rcnn\\data>mklink /D VOCdevkit2007 D:\\data\\public_dataset\\VOCdevkit\n```\n\nCommand Parameters:\n\n{% highlight bash %}\nMKLINK [[/D] | [/H] | [/J]] Link Target\n\n/D      Creates a directory symbolic link.  Default is a file\nsymbolic link.\n/H      Creates a hard link instead of a symbolic link.\n/J      Creates a Directory Junction.\nLink    specifies the new symbolic link name.\nTarget  specifies the path (relative or absolute) that the new link refers to.\n{% endhighlight %}\n\nRef:\n\n**DIR: Display a list of files and subfolders**\n\n[http://ss64.com/nt/dir.html](http://ss64.com/nt/dir.html)\n\n**List all files/folders into a txt**\n\n```\ndir /s /b /a *.jpg > list.txt\n```\n\n**List only folders into a txt**\n\n```\ndir /s /b /ad *.jpg > list.txt\n```\n\n**List only files into a txt**\n\n```\ndir /s /b /a:-d *.mp4 *.mkv > list.txt\n```\n\n**Arrays, linked lists and other data structures in cmd.exe (batch) script**\n\n[http://stackoverflow.com/questions/10166386/arrays-linked-lists-and-other-data-structures-in-cmd-exe-batch-script/10167990#10167990](http://stackoverflow.com/questions/10166386/arrays-linked-lists-and-other-data-structures-in-cmd-exe-batch-script/10167990#10167990)\n\n**Loop through each line in a text file using a Windows batch file**\n\n```\nfor /F \"tokens=*\" %%A in (myfile.txt) do [process] %%A\n```\n\n- stackoverflow: [http://stackoverflow.com/questions/155932/how-do-you-loop-through-each-line-in-a-text-file-using-a-windows-batch-file](http://stackoverflow.com/questions/155932/how-do-you-loop-through-each-line-in-a-text-file-using-a-windows-batch-file)\n\n## Download\n\n```\nbitsadmin  /transfer job_name       /download  /priority priority   URL  local\\path\\file\n```\n\nExample:\n\n```\nbitsadmin  /transfer mydownloadjob  /download  /priority normal   http://example.com/filename.zip  C:\\Users\\username\\Downloads\\filename.zip\n```\n\n# Utilities\n\n**XX-Net: a web proxy tool**\n\n- github: [https://github.com/XX-net/XX-Net](https://github.com/XX-net/XX-Net)\n\n**A Utility to Unassociate File Types in Windows 7 and Vista**\n\n[http://www.winhelponline.com/blog/unassociate-file-types-windows-7-vista/](http://www.winhelponline.com/blog/unassociate-file-types-windows-7-vista/)\n\n**Video DownloadHelper**\n\n- chrome-plugin: [https://chrome.google.com/webstore/detail/video-downloadhelper/lmjnegcaeklhafolokijcfjliaokphfk](https://chrome.google.com/webstore/detail/video-downloadhelper/lmjnegcaeklhafolokijcfjliaokphfk)\n- filefox-plugin: [https://addons.mozilla.org/zh-cn/firefox/addon/video-downloadhelper/](https://addons.mozilla.org/zh-cn/firefox/addon/video-downloadhelper/)\n- homepage: [http://www.downloadhelper.net/welcome.php?version=5.4.2](http://www.downloadhelper.net/welcome.php?version=5.4.2)\n\n**BulkFileChanger**\n\n- intro: \"BulkFileChanger is a small utility that allows you to create files list from multiple folders, \nand then make some action on them - Modify their created/modified/accessed time, \nchange their file attribute (Read Only, Hidden, System), \nrun an executable with these files as parameter, and copy/cut paste into Explorer.\"\n- website: [http://www.nirsoft.net/utils/bulk_file_changer.html](http://www.nirsoft.net/utils/bulk_file_changer.html)\n\n**AVS Video ReMaker**\n\n- intro: Edit AVI, VOB, MP4, DVD, Blu-ray, TS, MKV, \nHD-videos fast and without reconversion.\nBurn Blu-ray or DVD discs with menus.\n- website: [http://www.avs4you.com/AVS-Video-ReMaker.aspx](http://www.avs4you.com/AVS-Video-ReMaker.aspx)\n\n**AVS Audio Editor**\n\n- intro: Cut, join, trim, mix, delete parts, split audio files.\nApply various effects and filters. Record audio from various inputs. Save files to all key audio formats.\n- website: [http://www.avs4you.com/AVS-Audio-Editor.aspx](http://www.avs4you.com/AVS-Audio-Editor.aspx)\n\n**Dos2Unix / Unix2Dos - Text file format converters**\n\n![](http://dos2unix.sourceforge.net/dos2unix2mac.png)\n\n- homepage: [http://waterlan.home.xs4all.nl/dos2unix.html](http://waterlan.home.xs4all.nl/dos2unix.html)\n- homepage: [http://dos2unix.sourceforge.net/](http://dos2unix.sourceforge.net/)\n- download(dos2unix): [http://waterlan.home.xs4all.nl/dos2unix.html#DOS2UNIX](http://waterlan.home.xs4all.nl/dos2unix.html#DOS2UNIX)\n\n**Git for Windows 国内下载站**\n\n- github: [https://github.com/waylau/git-for-win](https://github.com/waylau/git-for-win)\n\n**Zotero: a free, easy-to-use tool to help you collect, organize, cite, and share your research sources**\n\n![](https://www.zotero.org/static/images/index/features/capture-research-data.1473223289.png)\n![](https://www.zotero.org/static/images/index/features/store-anything.1473223289.png)\n\n[https://www.zotero.org/](https://www.zotero.org/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","title":"FFmpeg Collection of Utility Methods"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: working_on_windows\ntitle: FFmpeg Collection of Utility Methods\ndate: 2016-06-05\n---\n\n- homepage: [http://ffmpeg.org/](http://ffmpeg.org/)\n- github: [https://github.com/FFmpeg/FFmpeg](https://github.com/FFmpeg/FFmpeg)\n\n# Split audio from video\n\nffmpeg -i video.mp4 output_audio.wav\n\n[http://superuser.com/questions/609740/extracting-wav-from-mp4-while-preserving-the-highest-possible-quality](http://superuser.com/questions/609740/extracting-wav-from-mp4-while-preserving-the-highest-possible-quality)\n\n# Merge audio to a video\n\nffmpeg -i video.mp4 -i audio.wav -c:v copy -c:a aac -strict experimental output.mp4\n\n[http://superuser.com/questions/277642/how-to-merge-audio-and-video-file-in-ffmpeg](http://superuser.com/questions/277642/how-to-merge-audio-and-video-file-in-ffmpeg)\n\n# Compress MP4\n\nffmpeg -i video.mp4 -vcodec h264 -b:v 1000k -acodec mp2 output.mp4\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","title":"Computer Vision Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_vision\ntitle: Computer Vision Resources\ndate: 2015-09-12\n---\n\n# Courses\n\n**Mobile Computer Vision (Spring 2015)**\n\n![](http://web.stanford.edu/class/cs231m/assets/img/course-splash.png)\n\n- homepage: [http://web.stanford.edu/class/cs231m/](http://web.stanford.edu/class/cs231m/)\n- syllabus: [http://web.stanford.edu/class/cs231m/syllabus.html](http://web.stanford.edu/class/cs231m/syllabus.html)\n- projects: [http://web.stanford.edu/class/cs231m/projects.html](http://web.stanford.edu/class/cs231m/projects.html)\n- resources: [http://web.stanford.edu/class/cs231m/resources.html](http://web.stanford.edu/class/cs231m/resources.html)\n\n**CSCI1950-G Computational Photography**\n\n![](http://cs.brown.edu/courses/csci1950-g/images/montage_large.jpg)\n\n[http://cs.brown.edu/courses/csci1950-g/](http://cs.brown.edu/courses/csci1950-g/)\n\n**MIT CSAIL: 6.819/6.869: Advances in Computer Vision (Fall 2015)**\n\n![](http://6.869.csail.mit.edu/fa15/images/teaser.jpg)\n\n- homepage: [http://6.869.csail.mit.edu/fa15/index.html](http://6.869.csail.mit.edu/fa15/index.html)\n\n**EECS 432 Advanced Computer Vision**\n\n- course website: [http://www.ece.northwestern.edu/~yingwu/teaching/EECS432/index.html](http://www.ece.northwestern.edu/~yingwu/teaching/EECS432/index.html)c\n- handouts: [http://www.ece.northwestern.edu/~yingwu/teaching/EECS432/EECS432_hand.html](http://www.ece.northwestern.edu/~yingwu/teaching/EECS432/EECS432_hand.html)\n\n**EECS 286 Advanced Topics in Computer Vision**\n\n- homepage: [http://faculty.ucmerced.edu/mhyang/course/eecs286/index.htm](http://faculty.ucmerced.edu/mhyang/course/eecs286/index.htm)\n- syllabus: [http://faculty.ucmerced.edu/mhyang/course/eecs286/syllabus.htm](http://faculty.ucmerced.edu/mhyang/course/eecs286/syllabus.htm)\n- lectures: [http://faculty.ucmerced.edu/mhyang/course/eecs286/lecture.htm](http://faculty.ucmerced.edu/mhyang/course/eecs286/lecture.htm)\n- lecture(\"How to get your CVPR paper rejected?\"): [http://faculty.ucmerced.edu/mhyang/course/eecs286/lectures/introduction.pptx](http://faculty.ucmerced.edu/mhyang/course/eecs286/lectures/introduction.pptx)\n\n**CS280: Computer Vision (University of California Berkeley)**\n\n- homepage: [http://www-inst.eecs.berkeley.edu/~cs280/sp15/index.html](http://www-inst.eecs.berkeley.edu/~cs280/sp15/index.html)\n- lectures: [http://docs.huihoo.com/computer-vision/berkeley/cs280-computer-vision/](http://docs.huihoo.com/computer-vision/berkeley/cs280-computer-vision/)\n\n**CSCI2951-T Data-driven Computer Vision (Spring 2016)**\n\n![](http://cs.brown.edu/courses/csci2951-t/images/detection_teaser.png)\n\n- instructor: Genevieve Patterson\n- homepage: [http://cs.brown.edu/courses/csci2951-t/](http://cs.brown.edu/courses/csci2951-t/)\n\n# Images Denoising\n\n**Fast Burst Images Denoising**\n\n![](http://personal.ie.cuhk.edu.hk/~lz013/projects/burstdenoising/intro.png)\n\n- intro: SIGGRAPH Asia 2014. CUHK, Microsoft Research\n- project: [http://personal.ie.cuhk.edu.hk/~lz013/projects/BurstDenoising.html](http://personal.ie.cuhk.edu.hk/~lz013/projects/BurstDenoising.html)\n- paper: [http://personal.ie.cuhk.edu.hk/~lz013/papers/burstdenoising.pdf](http://personal.ie.cuhk.edu.hk/~lz013/papers/burstdenoising.pdf)\n\n**Robust non-linear regression analysis: A greedy approach employing kernels and application to image denoising**\n\n- intro: KGARD\n- arxiv: [http://arxiv.org/abs/1601.00595](http://arxiv.org/abs/1601.00595)\n- code(Matlab): [http://bouboulis.mysch.gr/kernels.html](http://bouboulis.mysch.gr/kernels.html)\n\n**Blind Image Denoising via Dependent Dirichlet Process Tree**\n\n- arxiv: [http://arxiv.org/abs/1601.03117](http://arxiv.org/abs/1601.03117)\n\n**Image denoising via group sparsity residual constraint**\n\n- arxiv: [http://arxiv.org/abs/1609.03302](http://arxiv.org/abs/1609.03302)\n\n# Image Blur / Deblur\n\n**Motion Blurred Images Generation**\n\n- homepage: [http://home.deib.polimi.it/boracchi/Projects/PSFGeneration.html](http://home.deib.polimi.it/boracchi/Projects/PSFGeneration.html)\n- code(Matlab): [http://home.deib.polimi.it/boracchi/Projects/PSF_generation/PSF_generation.zip](http://home.deib.polimi.it/boracchi/Projects/PSF_generation/PSF_generation.zip)\n\n**Blind Image Deblurring Using Dark Channel Prior**\n\n**Good Regions to Deblur**\n\n- project page: [https://eng.ucmerced.edu/people/zhu/GoodRegion.html](https://eng.ucmerced.edu/people/zhu/GoodRegion.html)\n- paper: [https://eng.ucmerced.edu/people/zhu/ECCV12.pdf](https://eng.ucmerced.edu/people/zhu/ECCV12.pdf)\n- code(Matlab): [https://eng.ucmerced.edu/people/zhu/ECCV12_code.zip](https://eng.ucmerced.edu/people/zhu/ECCV12_code.zip)\n\n# Painting\n\n**Real-Time Gradient-Domain Painting**\n\n- intro: SIGGRAPH 2009\n- homepage: [http://graphics.cs.cmu.edu/projects/gradient-paint/](http://graphics.cs.cmu.edu/projects/gradient-paint/)\n- paper: [http://graphics.cs.cmu.edu/projects/gradient-paint/grad.light.r2226.pdf](http://graphics.cs.cmu.edu/projects/gradient-paint/grad.light.r2226.pdf)\n\n**Sketch2Photo: Internet Image Montage**\n\n![](http://cg.cs.tsinghua.edu.cn/montage/figures/teaser.jpg)\n\n- intro: ACM SIGGRAPH ASIA 2009\n- project page: [http://cg.cs.tsinghua.edu.cn/montage/main.htm](http://cg.cs.tsinghua.edu.cn/montage/main.htm)\n- paper: [http://cg.cs.tsinghua.edu.cn/montage/files/montage.pdf](http://cg.cs.tsinghua.edu.cn/montage/files/montage.pdf)\n\n**Combining Sketch and Tone for Pencil Drawing Production**\n\n- intro: NPAR 2012 Best Paper Award\n- homepage: [http://www.cse.cuhk.edu.hk/~leojia/projects/pencilsketch/pencil_drawing.htm](http://www.cse.cuhk.edu.hk/~leojia/projects/pencilsketch/pencil_drawing.htm)\n- paper: [http://www.cse.cuhk.edu.hk/~leojia/projects/pencilsketch/npar12_pencil.pdf](http://www.cse.cuhk.edu.hk/~leojia/projects/pencilsketch/npar12_pencil.pdf)\n- github: [https://github.com/fumin/pencil](https://github.com/fumin/pencil)\n\n# Image Retrieval\n\n**Multi-modal image retrieval with random walk on multi-layer graphs**\n\n- arxiv: [http://arxiv.org/abs/1607.03406](http://arxiv.org/abs/1607.03406)\n\n**Content-based image retrieval tutorial**\n\n- intro: KNN, SVM, MatLab GUI\n- arxiv: [http://arxiv.org/abs/1608.03811](http://arxiv.org/abs/1608.03811)\n- github: [https://github.com/kirk86/ImageRetrieval](https://github.com/kirk86/ImageRetrieval)\n\n# Image Summary\n\n**Summarizing Visual Data Using Bidirectional Similarity**\n\n- homepage: [http://denis.simakov.info/weizmann/summarization_talk_20101116/summarization.html](http://denis.simakov.info/weizmann/summarization_talk_20101116/summarization.html)\n- paper: [http://www.wisdom.weizmann.ac.il/~vision/VisualSummary/bidirectional_similarity_CVPR2008.pdf](http://www.wisdom.weizmann.ac.il/~vision/VisualSummary/bidirectional_similarity_CVPR2008.pdf)\n\n# Image Retargeting / Editing\n\n**PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing**\n\n![](http://gfx.cs.princeton.edu/pubs/Barnes_2009_PAR/patchmatch_title.png)\n\n- homepage(paper+code): [http://gfx.cs.princeton.edu/pubs/Barnes_2009_PAR/](http://gfx.cs.princeton.edu/pubs/Barnes_2009_PAR/)\n- paper: [http://gfx.cs.princeton.edu/pubs/Barnes_2009_PAR/patchmatch.pdf](http://gfx.cs.princeton.edu/pubs/Barnes_2009_PAR/patchmatch.pdf)\n- code: [http://gfx.cs.princeton.edu/pubs/Barnes_2009_PAR/patchmatch-2.1.zip](http://gfx.cs.princeton.edu/pubs/Barnes_2009_PAR/patchmatch-2.1.zip)\n\n**The Generalized PatchMatch Correspondence Algorithm**\n\n![](http://gfx.cs.princeton.edu/pubs/Barnes_2010_TGP/gpm_teaser.png)\n\n- homapage(paper+code): [http://gfx.cs.princeton.edu/pubs/Barnes_2010_TGP/index.php](http://gfx.cs.princeton.edu/pubs/Barnes_2010_TGP/index.php)\n- paper: [http://gfx.cs.princeton.edu/pubs/Barnes_2010_TGP/generalized_pm.pdf](http://gfx.cs.princeton.edu/pubs/Barnes_2010_TGP/generalized_pm.pdf)\n- code: [http://www.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/patchmatch-2.0.zip](http://www.cs.princeton.edu/gfx/pubs/Barnes_2009_PAR/patchmatch-2.0.zip)\n\n**Seamless Image Editing**\n\n![](http://www.cmlab.csie.ntu.edu.tw/~dreamway/seamless/img/teaser.jpg)\n\n- homepage: [http://www.cmlab.csie.ntu.edu.tw/~dreamway/seamless/](http://www.cmlab.csie.ntu.edu.tw/~dreamway/seamless/)\n\n# Image Inpaiting\n\n**Patch-based Texture Synthesis for Image Inpainting**\n\n- arxiv: [http://arxiv.org/abs/1605.01576](http://arxiv.org/abs/1605.01576)\n\n# Image Dithering\n\n**Image Dithering: Eleven Algorithms and Source Code**\n\n- blog: [http://www.tannerhelland.com/4660/dithering-eleven-algorithms-source-code/](http://www.tannerhelland.com/4660/dithering-eleven-algorithms-source-code/)\n\n# Image Enhancement\n\n**LIME: A Method for Low-light IMage Enhancement**\n\n![](http://photo.weibo.com/2578103464/wbphotos/large/mid/3971098712490115/pid/99aabca8jw1f3ibhb6o8ej20ck09cmzl)\n\n- arxiv: [http://arxiv.org/abs/1605.05034](http://arxiv.org/abs/1605.05034)\n- github: [http://cs.tju.edu.cn/orgs/vision/~xguo/code/LIME.zip](http://cs.tju.edu.cn/orgs/vision/~xguo/code/LIME.zip)\n- author homepage: [http://cs.tju.edu.cn/orgs/vision/~xguo/homepage.htm](http://cs.tju.edu.cn/orgs/vision/~xguo/homepage.htm)\n\n**SelPh: Progressive Learning and Support of Manual Photo Color Enhancement**\n\n![](http://koyama.xyz/project/SelPh/teaser1.gif)\n\n- homepage: [http://koyama.xyz/project/SelPh/](http://koyama.xyz/project/SelPh/)\n- paper: [http://koyama.xyz/project/SelPh/chi2016_paper.pdf](http://koyama.xyz/project/SelPh/chi2016_paper.pdf)\n- bitbucket: [https://bitbucket.org/yukikoyama/selph/](https://bitbucket.org/yukikoyama/selph/)\n\n# Image Resizing\n\n- blog: [http://parellagram.com/posts/carving](http://parellagram.com/posts/carving)\n- github: [https://github.com/aaparella/carve](https://github.com/aaparella/carve)\n\n# Image Cloning\n\n**Coordinates for Instant Image Cloning**\n\n![](http://www.cs.huji.ac.il/~danix/mvclone/teaser.jpg)\n\n- intro: SIGGRAPH 2009\n- homepage: [http://www.cs.huji.ac.il/~danix/mvclone/](http://www.cs.huji.ac.il/~danix/mvclone/)\n- paper: [http://www.cs.huji.ac.il/~danix/mvclone/files/mvc-final-opt.pdf](http://www.cs.huji.ac.il/~danix/mvclone/files/mvc-final-opt.pdf)\n\n# Image Compositing\n\n**Interactive Digital Photomontage**\n\n- intro: SIGGRAPH 2004\n- homepage: [http://grail.cs.washington.edu/projects/photomontage/](http://grail.cs.washington.edu/projects/photomontage/)\n- code: [http://grail.cs.washington.edu/projects/photomontage/release/](http://grail.cs.washington.edu/projects/photomontage/release/)\n- paper: [http://grail.cs.washington.edu/projects/photomontage/photomontage.pdf](http://grail.cs.washington.edu/projects/photomontage/photomontage.pdf)\n- paper: [http://www.researchgate.net/publication/2941744_Interactive_Digital_Photomontage](http://www.researchgate.net/publication/2941744_Interactive_Digital_Photomontage)\n\n**Panorama Stitching**\n\n**CS510 Visual Computing, Project 2: Panorama Stitching**\n\n[http://web.cecs.pdx.edu/~kstew2/cs510vision/stitcher/](http://web.cecs.pdx.edu/~kstew2/cs510vision/stitcher/)\n\n# Image Stylization\n\n**stylize: Regressor based image stylization**\n\n![](https://raw.githubusercontent.com/Newmu/stylize/master/resources/iggy.gif)\n\n- github: [https://github.com/Newmu/stylize](https://github.com/Newmu/stylize)\n\n**Procedurally Generating Stylized Farmland Scenes**\n\n![](http://graphics.cs.williams.edu/courses/cs371/f16/gallery/4-midterm/terrain/TopImage.jpg)\n\n[http://graphics.cs.williams.edu/courses/cs371/f16/gallery/4-midterm/terrain/report.md.html](http://graphics.cs.williams.edu/courses/cs371/f16/gallery/4-midterm/terrain/report.md.html)\n\n# Image Haze Removal\n\n**Single Image Haze Removal**\n\n- project page: [http://research.microsoft.com/en-us/um/people/kahe/cvpr09/](http://research.microsoft.com/en-us/um/people/kahe/cvpr09/)\n\n**DehazeNet: An End-to-End System for Single Image Haze Removal**\n\n- arxiv: [http://arxiv.org/abs/1601.07661](http://arxiv.org/abs/1601.07661)\n\n# Image Blending\n\nLinear Blending, Poisson Blending, Multiband Blending, Feather Blending, Alpha Blending, Laplacian Blending\n\n**Image Blending**\n\n- course-info: 15-463: Computational Photography. Alexei Efros, CMU, Spring 2010\n- lecture: [http://graphics.cs.cmu.edu/courses/15-463/2010_spring/Lectures/blending.pdf](http://graphics.cs.cmu.edu/courses/15-463/2010_spring/Lectures/blending.pdf)\n\n**CS 195-G: Image Blending**\n\n- homepage: [https://cs.brown.edu/courses/csci1950-g/results/proj2/edwallac/](https://cs.brown.edu/courses/csci1950-g/results/proj2/edwallac/)\n\n**Panoramic Image Mosaic**\n\n- homepage: [http://pages.cs.wisc.edu/~csverma/CS766_09/ImageMosaic/imagemosaic.html](http://pages.cs.wisc.edu/~csverma/CS766_09/ImageMosaic/imagemosaic.html)\n\n## Linear Blending\n\n**Adding (blending) two images using OpenCV**\n\n[http://docs.opencv.org/master/d5/dc4/tutorial_adding_images.html#gsc.tab=0](http://docs.opencv.org/master/d5/dc4/tutorial_adding_images.html#gsc.tab=0)\n\n## Poisson Blending\n\n**Poisson Image Editing**\n\n- intro: SIGGRAPH 2003\n- paper: [http://cs.brown.edu/courses/csci1290/asgn/proj2/resources/PoissonImageEditing.pdf](http://cs.brown.edu/courses/csci1290/asgn/proj2/resources/PoissonImageEditing.pdf)\n- paper: [https://www.cs.jhu.edu/~misha/Fall07/Papers/Perez03.pdf](https://www.cs.jhu.edu/~misha/Fall07/Papers/Perez03.pdf)\n- slides: [https://graphics.ethz.ch/teaching/former/seminar/handouts/Weyrich_PoissonImageEditing.pdf](https://graphics.ethz.ch/teaching/former/seminar/handouts/Weyrich_PoissonImageEditing.pdf)\n- code(Matlab+C#): [https://code.google.com/p/imageblending/](https://code.google.com/p/imageblending/)\n- github: [https://github.com/fbessho/PyPoi](https://github.com/fbessho/PyPoi)\n- github(C++): [https://github.com/cheind/poisson-image-editing](https://github.com/cheind/poisson-image-editing)\n\n**Poisson Blending**\n\n- blog: [http://eric-yuan.me/poisson-blending/](http://eric-yuan.me/poisson-blending/)\n\n**Poisson Blending II**\n\n- blog: [http://eric-yuan.me/poisson-blending-2/](http://eric-yuan.me/poisson-blending-2/)\n- code: [http://codepad.org/ANqtikKR](http://codepad.org/ANqtikKR)\n\n**Solving the Discrete Poisson Equation using Jacobi, SOR, Conjugate Gradients, and the FFT**\n\n- intro: CS267: Lectures 15 and 16, Mar 5 and 7 1996\n- lecture: [http://www.cs.berkeley.edu/~demmel/cs267/lecture24/lecture24.html](http://www.cs.berkeley.edu/~demmel/cs267/lecture24/lecture24.html)\n\n**Gradient Domain Fusion Using Poisson Blending**\n\n![](http://120.52.72.72/cs.brown.edu/c3pr90ntcsf0/courses/cs129/results/proj2/taox/images/result_09.jpg)\n\n[http://cs.brown.edu/courses/cs129/results/proj2/taox/](http://cs.brown.edu/courses/cs129/results/proj2/taox/)\n\n# Image Stitching\n\n**Natural and Seamless Image Composition with Color Control**\n\n[http://www3.ntu.edu.sg/home/asjfcai/tip04594.pdf](http://www3.ntu.edu.sg/home/asjfcai/tip04594.pdf)\n\n**Object-aware Gradient-Domain Image Compositing**\n\n[http://www.cg.cs.tu-bs.de/media/publications/Eisemann11OAG.pdf](http://www.cg.cs.tu-bs.de/media/publications/Eisemann11OAG.pdf)\n\n**Improving Image Matting using Comprehensive Sampling Sets**\n\n[http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shahrian_Improving_Image_Matting_2013_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shahrian_Improving_Image_Matting_2013_CVPR_paper.pdf)\n\n**Multi-scale Image Harmonization**\n\n![](http://gvi.seas.harvard.edu/sites/all/files/paper-rep-images/harmonization_teaser_3.png)\n\n- homepage: [http://gvi.seas.harvard.edu/paper/multiscale-image-harmonization](http://gvi.seas.harvard.edu/paper/multiscale-image-harmonization)\n- paper: [http://gvi.seas.harvard.edu/sites/all/files/Harmonization_SIGGRAPH10.pdf](http://gvi.seas.harvard.edu/sites/all/files/Harmonization_SIGGRAPH10.pdf)\n- slides: [http://gvi.seas.harvard.edu/sites/all/files/Harmonization_SIGGRAPH10.pptx](http://gvi.seas.harvard.edu/sites/all/files/Harmonization_SIGGRAPH10.pptx)\n\n**Drag-and-Drop Pasting**\n\n[http://research.microsoft.com/pubs/69331/dragdroppasting_siggraph06.pdf](http://research.microsoft.com/pubs/69331/dragdroppasting_siggraph06.pdf)\n\n**Cross Dissolve Without Cross Fade: Preserving Contrast, Color and Salience in Image Compositing**\n\n[https://www.cl.cam.ac.uk/research/rainbow/projects/compositing/EG06-Cross-Dissolve-Without-Cross-Fade.pdf](https://www.cl.cam.ac.uk/research/rainbow/projects/compositing/EG06-Cross-Dissolve-Without-Cross-Fade.pdf)\n\n**Snap Image Composition**\n\n[http://www.cs.huji.ac.il/~peleg/papers/SnapComposition.pdf](http://www.cs.huji.ac.il/~peleg/papers/SnapComposition.pdf)\n\n**Stitching Stabilizer: Two-frame-stitching Video Stabilization for Embedded Systems**\n\n- arxiv: [http://arxiv.org/abs/1603.06678](http://arxiv.org/abs/1603.06678)\n\n**Stitching and Matting**\n\n- lectures: [http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2015/bil721/lectures/w06-stitching-matting.pdf](http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2015/bil721/lectures/w06-stitching-matting.pdf)\n\n**Image Stitching**\n\n- lectures: [https://courses.engr.illinois.edu/cs498dwh/fa2010/lectures/Lecture%2017%20-%20Photo%20Stitching.pdf](https://courses.engr.illinois.edu/cs498dwh/fa2010/lectures/Lecture%2017%20-%20Photo%20Stitching.pdf)\n\n**Graphics isn't all about 3-D**\n\n- lectures: [http://www.cs.cmu.edu/afs/cs/academic/class/15462-s09/www/lec/25/lec25.pdf](http://www.cs.cmu.edu/afs/cs/academic/class/15462-s09/www/lec/25/lec25.pdf)\n\n- slides: [http://upcommons.upc.edu/bitstream/handle/2099.1/22861/PujolAlba-TFG-FinalReport.pdf;jsessionid=7AE46AE2A5420F75760F246381D429BC?sequence=5](http://upcommons.upc.edu/bitstream/handle/2099.1/22861/PujolAlba-TFG-FinalReport.pdf;jsessionid=7AE46AE2A5420F75760F246381D429BC?sequence=5)\n\n**Assignment: Image stitching with RANSAC**\n\n- assignments: [https://people.cs.umass.edu/~elm/Teaching/Docs/assign_RANSAC.pdf](https://people.cs.umass.edu/~elm/Teaching/Docs/assign_RANSAC.pdf)\n\n**OpenCV panorama stitching**\n\n- blog: [http://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/](http://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/)\n\n**Real-time panorama and image stitching with OpenCV**\n\n- blog: [http://www.pyimagesearch.com/2016/01/25/real-time-panorama-and-image-stitching-with-opencv/](http://www.pyimagesearch.com/2016/01/25/real-time-panorama-and-image-stitching-with-opencv/)\n\n# Image Super-Resolution\n\n**Super-Resolution From a Single Image**\n\n- project: [http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html](http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html)\n- paper: [http://www.wisdom.weizmann.ac.il/~vision/single_image_SR/files/single_image_SR.pdf](http://www.wisdom.weizmann.ac.il/~vision/single_image_SR/files/single_image_SR.pdf)\n\n**Aperture-scanning Fourier ptychography for 3D refocusing and super-resolution macroscopic imaging**\n\n- paper: [http://www.its.caltech.edu/~roarke/research/FPM/FPM_Aperture_Scanning.pdf](http://www.its.caltech.edu/~roarke/research/FPM/FPM_Aperture_Scanning.pdf)\n- slides: [http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754138](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754138)\n\n**Single Image Super-Resolution from Transformed Self-Exemplars**\n\n- homepage: [https://sites.google.com/site/jbhuang0604/publications/struct_sr](https://sites.google.com/site/jbhuang0604/publications/struct_sr)\n- github: [https://github.com/jbhuang0604/SelfExSR](https://github.com/jbhuang0604/SelfExSR)\n\n# Photo Collage\n\n**AutoCollage**\n\n- intro: SIGGRAPH 2006\n- homepage: [http://research.microsoft.com/en-us/projects/i3l/autocollage.aspx](http://research.microsoft.com/en-us/projects/i3l/autocollage.aspx)\n- paper: [http://research.microsoft.com/pubs/67894/autocollage_rotheretal_siggraph2006.pdf](http://research.microsoft.com/pubs/67894/autocollage_rotheretal_siggraph2006.pdf)\n- slides: [http://research.microsoft.com/en-us/UM/cambridge/projects/VisionImageVideoEditing/autocollage/TalkSiggraph2006Compressed.zip](http://research.microsoft.com/en-us/UM/cambridge/projects/VisionImageVideoEditing/autocollage/TalkSiggraph2006Compressed.zip)\n- demo: [http://research.microsoft.com/en-us/um/cambridge/projects/autocollage/](http://research.microsoft.com/en-us/um/cambridge/projects/autocollage/)\n\n**Picture Collage**\n\n- intro: 2006\n- paper: [http://research.microsoft.com/en-us/um/people/jiansun/papers/PictureCollage_CVPR2006.pdf](http://research.microsoft.com/en-us/um/people/jiansun/papers/PictureCollage_CVPR2006.pdf)\n- paper: [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.5727](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.5727)\n\n**Picture Collage**\n\n- intro: 2009\n- paper: [http://mmlab.ie.cuhk.edu.hk/archive/2009/07_Picture.pdf](http://mmlab.ie.cuhk.edu.hk/archive/2009/07_Picture.pdf)\n\n**Efficient Optimization of Photo Collage**\n\n- paper: [http://research.microsoft.com/pubs/80783/Collage_techreport.pdf](http://research.microsoft.com/pubs/80783/Collage_techreport.pdf)\n\n# Video Collage\n\n**Stained-Glass Visualization for Highly Condensed Video Summaries (ICME 2004)**\n\n- intro: ICME 2004\n- paper: [https://www.fxpal.com/publications/stained-glass-visualization-for-highly-condensed-video-summaries.pdf](https://www.fxpal.com/publications/stained-glass-visualization-for-highly-condensed-video-summaries.pdf)\n\n**Video collage: A novel presentation of video sequence**\n\n- intro: ICME 2007\n- paper: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.3728&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.3728&rep=rep1&type=pdf)\n\n**Stained Glass Photo Collages**\n\n[http://uist.acm.org/archive/adjunct/2004/pdf/posters/p7-girgensohn.pdf](http://uist.acm.org/archive/adjunct/2004/pdf/posters/p7-girgensohn.pdf)\n\n**Visual Storylines: Semantic Visualization of Movie Sequence**\n\n- paper: [http://cg.cs.tsinghua.edu.cn/papers/C&G2012_videostoryline.pdf](http://cg.cs.tsinghua.edu.cn/papers/C&G2012_videostoryline.pdf)\n- paper: [http://cg.cs.tsinghua.edu.cn/people/~taochen/papers/VisualStorylines.pdf](http://cg.cs.tsinghua.edu.cn/people/~taochen/papers/VisualStorylines.pdf)\n\n**Video collage: presenting a video sequence using a single image**\n\n[http://iris.usc.edu/people/yangbo/papers/vcj08.pdf](http://iris.usc.edu/people/yangbo/papers/vcj08.pdf)\n\n**Efficient Optimization of Photo Collage**\n\n[http://research.microsoft.com/en-us/people/yichenw/collage_techreport.pdf](http://research.microsoft.com/en-us/people/yichenw/collage_techreport.pdf)\n\n**Puzzle-like Collage (2010)**\n\n[http://webee.technion.ac.il/~ayellet/Ps/10-PuzzleCollage.pdf](http://webee.technion.ac.il/~ayellet/Ps/10-PuzzleCollage.pdf)\n\n**Browsing Large Image Datasets through Voronoi Diagrams**\n\n[http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=576998825C3E40A32826A00B64089DF6?doi=10.1.1.230.5997&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=576998825C3E40A32826A00B64089DF6?doi=10.1.1.230.5997&rep=rep1&type=pdf)\n\n**Content-aware Photo Collage Using Circle Packing**\n\n- intro: TVCG 2014. NJU\n- homepage: [http://cs.nju.edu.cn/ywguo/PhotoCollage/Index.html](http://cs.nju.edu.cn/ywguo/PhotoCollage/Index.html)\n- paper: [http://cs.nju.edu.cn/ywguo/webs/paperdownload/Content-aware%20Photo%20Collage%20Using%20Circle%20Packing.pdf](http://cs.nju.edu.cn/ywguo/webs/paperdownload/Content-aware%20Photo%20Collage%20Using%20Circle%20Packing.pdf)\n- demo: [http://cs.nju.edu.cn/ywguo/PhotoCollage/dload.html](http://cs.nju.edu.cn/ywguo/PhotoCollage/dload.html)\n\n**Automatic Generation of Social Media Snippets for Mobile Browsing**\n\n- intro: Microsoft Research. ACM Multimedia 2013\n- homepage: [http://research.microsoft.com/apps/pubs/default.aspx?id=204877](http://research.microsoft.com/apps/pubs/default.aspx?id=204877)\n- paper: [http://research.microsoft.com/pubs/204877/mm035-yin.pdf](http://research.microsoft.com/pubs/204877/mm035-yin.pdf)\n\n# Video Tapestry\n\n**Digital Tapestry**\n\n- intro: MSR. CVPR 2005\n- homepage: [http://research.microsoft.com/apps/pubs/default.aspx?id=67404](http://research.microsoft.com/apps/pubs/default.aspx?id=67404)\n- paper: [http://pub.ist.ac.at/~vnk/papers/tapestry_cvpr05.pdf](http://pub.ist.ac.at/~vnk/papers/tapestry_cvpr05.pdf)\n\n**Video Tapestries with Continuous Temporal Zoom**\n\n![](http://gfx.cs.princeton.edu/gfx/pubs/Barnes_2010_VTW/teaser.png)\n\n- intro: Princeton. SIGGRAPH 2010\n- homepage: [http://gfx.cs.princeton.edu/gfx/pubs/Barnes_2010_VTW/index.php](http://gfx.cs.princeton.edu/gfx/pubs/Barnes_2010_VTW/index.php)\n- paper: [http://www.connellybarnes.com/work/publications/2010_tapestry_electronic.pdf](http://www.connellybarnes.com/work/publications/2010_tapestry_electronic.pdf)\n\n# Video Creativity\n\n**6 Seconds of Sound and Vision: Creativity in Micro-Videos**\n\n- intro:  CVPR 2014\n- homepage: [http://www.di.unito.it/~schifane/dataset/vine-dataset-cvpr14/](http://www.di.unito.it/~schifane/dataset/vine-dataset-cvpr14/)\n- arxiv: [http://arxiv.org/abs/1411.4080](http://arxiv.org/abs/1411.4080)\n\n# Video Highlights\n\n**Ranking Domain-specific Highlights by Analyzing Edited Videos**\n\n![](http://aliensunmin.github.io/project/at-a-glance/highlight_teaser.png)\n\n- intro: ECCV 2014\n- homepage: [http://aliensunmin.github.io/project/at-a-glance/](http://aliensunmin.github.io/project/at-a-glance/)\n- paper: [http://grail.cs.washington.edu/wp-content/uploads/2015/08/sun2014rdh.pdf](http://grail.cs.washington.edu/wp-content/uploads/2015/08/sun2014rdh.pdf)\n- paper: [https://drive.google.com/file/d/0ByJgUdTb1N2CM3Y5VU1BRjlmR3c/edit](https://drive.google.com/file/d/0ByJgUdTb1N2CM3Y5VU1BRjlmR3c/edit)\n- tech: [https://drive.google.com/file/d/0ByJgUdTb1N2CM1ktb1N4RVV3Mzg/view](https://drive.google.com/file/d/0ByJgUdTb1N2CM1ktb1N4RVV3Mzg/view)\n- github: [https://github.com/aliensunmin/DomainSpecificHighlight](https://github.com/aliensunmin/DomainSpecificHighlight)\n\n**Salient Montages from Unconstrained Videos**\n\n![](http://aliensunmin.github.io/project/at-a-glance/montage_teaser.png)\n\n- homepage: [http://aliensunmin.github.io/project/at-a-glance/](http://aliensunmin.github.io/project/at-a-glance/)\n- paper: [http://grail.cs.washington.edu/wp-content/uploads/2015/08/sun2014smf.pdf](http://grail.cs.washington.edu/wp-content/uploads/2015/08/sun2014smf.pdf)\n- paper: [https://drive.google.com/file/d/0ByJgUdTb1N2CbzNYTjdxX0ZiRmc/edit](https://drive.google.com/file/d/0ByJgUdTb1N2CbzNYTjdxX0ZiRmc/edit)\n- github: [https://github.com/aliensunmin/salientMontages](https://github.com/aliensunmin/salientMontages)\n\n# Video Summarization\n\n**Creating Summaries from User Videos**\n\n![](http://www.vision.ee.ethz.ch/~hegrabne/visualInterestingness/vsum.png)\n\n- intro: ECCV 2014\n- project page: [https://people.ee.ethz.ch/~gyglim/vsum/index.php](https://people.ee.ethz.ch/~gyglim/vsum/index.php)\n- paper: [https://people.ee.ethz.ch/~gyglim/vsum/GygliECCV14_vsum.pdf](https://people.ee.ethz.ch/~gyglim/vsum/GygliECCV14_vsum.pdf)\n- paper: [http://www.vision.ee.ethz.ch/~hegrabne/papers/Gygli2014CreatingSummariesfrom.pdf](http://www.vision.ee.ethz.ch/~hegrabne/papers/Gygli2014CreatingSummariesfrom.pdf)\n- code: [https://people.ee.ethz.ch/~gyglim/vsum/index.php#sf_code](https://people.ee.ethz.ch/~gyglim/vsum/index.php#sf_code) \n\n**Joint Summarization of Large-scale Collections of Web Images and Videos for Storyline Reconstruction**\n\n- intro: CVPR 2014\n- paper: [http://www.cs.cmu.edu/~gunhee/publish/cvpr14_videostory.pdf](http://www.cs.cmu.edu/~gunhee/publish/cvpr14_videostory.pdf)\n\n**Video Summarization by Learning Submodular Mixtures of Objectives**\n\n![](http://www.vision.ee.ethz.ch/~hegrabne/visualInterestingness/vsum2.jpg)\n\n- intro: CVPR 2015\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gygli_Video_Summarization_by_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gygli_Video_Summarization_by_2015_CVPR_paper.pdf)\n\n**TVSum: Summarizing Web Videos Using Titles**\n\n![](https://qph.is.quoracdn.net/main-qimg-0c0bb88876258e99272200655e2dc2ea?convert_to_webp=true)\n\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf)\n\n**Summarizing While Recording: Context-Based Highlight Detection for Egocentric Videos**\n\n- keywords: structured SVM (SSVM)\n- paper: [http://www.umiacs.umd.edu/~morariu/publications/LinEgocentricICCVW15.pdf](http://www.umiacs.umd.edu/~morariu/publications/LinEgocentricICCVW15.pdf)\n\n**Title Generation for User Generated Videos**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1608.07068](http://arxiv.org/abs/1608.07068)\n\n# Activity Recognition\n\n**Latent Hierarchical Model for Activity Recognition**\n\n- paper: [http://arxiv.org/abs/1503.01820](http://arxiv.org/abs/1503.01820)\n- github: [https://github.com/louxi11/activity_recognition](https://github.com/louxi11/activity_recognition)\n- author page: [https://staff.fnwi.uva.nl/n.hu/](https://staff.fnwi.uva.nl/n.hu/)\n\n# Virtual Reality (VR)\n\n**Surround360 System: Facebook's open source hardware and software for capturing stereoscopic 3D 360 video for VR**\n\n![](https://s2.wp.com/wp-content/themes/vip/fbspherical/images/static/surround-360-inside.png)\n\n- homepage: [https://facebook360.fb.com/facebook-surround-360/](https://facebook360.fb.com/facebook-surround-360/)\n- code: [https://code.facebook.com/posts/265413023819735/surround-360-is-now-open-source/](https://code.facebook.com/posts/265413023819735/surround-360-is-now-open-source/)\n- github: [https://github.com/facebook/Surround360](https://github.com/facebook/Surround360)\n\n**Virtual Reality**\n\n- intro: Steven M. LaValle. Cambridge University Press 2016\n- book: [http://vr.cs.uiuc.edu/](http://vr.cs.uiuc.edu/)\n\n# SLAM\n\n**Why SLAM Matters, The Future of Real-Time SLAM, and Deep Learning vs SLAM**\n\n- blog: [http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html?m=1](http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html?m=1)\n\n**一起做RGB-D SLAM**\n\n- blog: [http://www.cnblogs.com/gaoxiang12/tag/%E4%B8%80%E8%B5%B7%E5%81%9ARGB-D%20SLAM/](http://www.cnblogs.com/gaoxiang12/tag/%E4%B8%80%E8%B5%B7%E5%81%9ARGB-D%20SLAM/)\n- github: [https://github.com/gaoxiang12/rgbd-slam-tutorial-gx](https://github.com/gaoxiang12/rgbd-slam-tutorial-gx)\n\n**PySceneDetect: a command-line application and a Python library for automatically detecting scene changes in video files**\n\n- homepage: [http://pyscenedetect.readthedocs.org/en/latest/](http://pyscenedetect.readthedocs.org/en/latest/)\n\n**The Future of Real-Time SLAM and Deep Learning vs SLAM**\n\n- blog: [http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html](http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html)\n\n**Awesome SLAM**\n\n- github: [https://github.com/kanster/awesome-slam](https://github.com/kanster/awesome-slam)\n\n**ORB-SLAM2: Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities**\n\n- github: [https://github.com/raulmur/ORB_SLAM2](https://github.com/raulmur/ORB_SLAM2)\n\n**Cartographer**\n\n- intro: Cartographer is a system that provides real-time simultaneous localization \nand mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations.\n- github: [https://github.com/googlecartographer/cartographer](https://github.com/googlecartographer/cartographer)\n\n**Introducing Cartographer**\n\n- blog: [https://opensource.googleblog.com/2016/10/introducing-cartographer.html](https://opensource.googleblog.com/2016/10/introducing-cartographer.html)\n\n**Real-Time Loop Closure in 2D LIDAR SLAM**\n\n- intro: ICRA 2016. Google. Cartographer\n- paper: [http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45466.pdf](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45466.pdf)\n\n# Optical Flow\n\n**A Database and Evaluation Methodology for Optical Flow**\n\n- intro: IJCV 2011\n- project page: [http://vision.middlebury.edu/flow/](http://vision.middlebury.edu/flow/)\n- paper: [http://vision.middlebury.edu/flow/floweval-ijcv2011.pdf](http://vision.middlebury.edu/flow/floweval-ijcv2011.pdf)\n\n**SimpleFlow: A Non-iterative, Sublinear Optical Flow Algorithm**\n\n- intro: Eurographics 2012\n- project page: [http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/](http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/)\n- paper: [http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/Tao-SAN-2012-05.pdf](http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/Tao-SAN-2012-05.pdf)\n- code: [graphics.berkeley.edu/papers/Tao-SAN-2012-05/SimpleFlow_Source.zip](graphics.berkeley.edu/papers/Tao-SAN-2012-05/SimpleFlow_Source.zip)\n\n# OCR\n\n**Ocular: a state-of-the-art historical OCR system**\n\n- github: [https://github.com/tberg12/ocular](https://github.com/tberg12/ocular)\n\n**SESHAT: Handwritten math expression parser**\n\n- intro: Seshat is an open-source system for recognizing handwritten mathematical expressions. \nGiven a sample represented as a sequence of strokes, the parser is able to convert it to LaTeX or other formats like InkML or MathML.\n- github: [https://github.com/falvaro/seshat](https://github.com/falvaro/seshat)\n\n**Awesome OCR: Links to awesome OCR projects**\n\n- github: [https://github.com/kba/awesome-ocr](https://github.com/kba/awesome-ocr)\n\n**【OCR/机器学习/搜索引擎】基于 Tesseract的图文识别搜**\n\n- github: [https://github.com/daijiale/OCR_FontsSearchEngine](https://github.com/daijiale/OCR_FontsSearchEngine)\n\n**The Simple + Practical Path to Machine Learning Capability: A Common Benchmark Task**\n\n![](https://indico.io/blog/wp-content/uploads/2016/09/TF0.5a-1-sm-768x520.png)\n\n- blog: [https://indico.io/blog/simple-practical-path-to-machine-learning-capability-part2/](https://indico.io/blog/simple-practical-path-to-machine-learning-capability-part2/)\n\n**Optical Character Recognition (OCR)**\n\n- blog: [http://aosabook.org/en/500L/pages/optical-character-recognition-ocr.html](http://aosabook.org/en/500L/pages/optical-character-recognition-ocr.html)\n\n**Sharingan: Newspaper text and context extractor**\n\n- intro: Tool to extract news articles from newspaper and give the context about the news\n- blog: [http://www.vipul.xyz/2017/03/sharingan-newspaper-text-and-context.html](http://www.vipul.xyz/2017/03/sharingan-newspaper-text-and-context.html)\n- github: [https://github.com/vipul-sharma20/sharingan](https://github.com/vipul-sharma20/sharingan)\n\n# Codec\n\n**JPEG 101 - How does JPEG work?**\n\n- blog: [http://arjunsreedharan.org/post/146070390717/jpeg-101-how-does-jpeg-work](http://arjunsreedharan.org/post/146070390717/jpeg-101-how-does-jpeg-work)\n\n# Face Alignment\n\n**Supervised Descent Method and its Applications to Face Alignment**\n\n- intro: CVPR 2013\n- project page: [http://patrikhuber.github.io/superviseddescent/](http://patrikhuber.github.io/superviseddescent/)\n- paper: [http://101.96.8.164/www.ri.cmu.edu/pub_files/2013/5/main.pdf](http://101.96.8.164/www.ri.cmu.edu/pub_files/2013/5/main.pdf)\n- github: [https://github.com/patrikhuber/superviseddescent](https://github.com/patrikhuber/superviseddescent)\n\n**Face Alignment at 3000 FPS via Regressing Local Binary Features**\n\n- intro: CVPR 2014. MSRA\n- paper: [http://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/yichenw-cvpr14_facealignment.pdf](http://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/yichenw-cvpr14_facealignment.pdf)\n- github: [https://github.com/yulequan/face-alignment-in-3000fps](https://github.com/yulequan/face-alignment-in-3000fps)\n\n**Joint Cascade Face Detection and Alignment**\n\n- intro: ECCV 2014\n- paper: [http://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/ECCV14_JointCascade.pdf](http://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/ECCV14_JointCascade.pdf)\n- github: [https://github.com/FaceDetect/jointCascade_py](https://github.com/FaceDetect/jointCascade_py)\n\n# Papers\n\n**RGB-W: When Vision Meets Wireless**\n\n![](http://vision.stanford.edu/pubimg/rgbw15_1.png)\n\n- paper: [http://vision.stanford.edu/pdf/RGBW_ICCV15.pdf](http://vision.stanford.edu/pdf/RGBW_ICCV15.pdf)\n\n**A Computational Approach for Obstruction-Free Photography**\n\n- paper: [https://people.csail.mit.edu/mrub/papers/ObstructionFreePhotograpy_SIGGRAPH2015.pdf](https://people.csail.mit.edu/mrub/papers/ObstructionFreePhotograpy_SIGGRAPH2015.pdf)\n\n**My Text in Your Handwriting**\n\n![](http://visual.cs.ucl.ac.uk/pubs/handwriting/img/results.jpg)\n\n- homepage: [http://visual.cs.ucl.ac.uk/pubs/handwriting/](http://visual.cs.ucl.ac.uk/pubs/handwriting/)\n- paper: [http://visual.cs.ucl.ac.uk/pubs/handwriting/handwriting_visual_main.pdf](http://visual.cs.ucl.ac.uk/pubs/handwriting/handwriting_visual_main.pdf)\n\n**Seeing the Arrow of Time**\n\n- intro: CVPR 2013\n- intro: \"is it possible to tell whether a video is running forwards or backwards?\"\n- project page: [http://people.csail.mit.edu/yichangshih/toa_web/](http://people.csail.mit.edu/yichangshih/toa_web/)\n- project page: [http://www.robots.ox.ac.uk/~vgg/research/arrow/](http://www.robots.ox.ac.uk/~vgg/research/arrow/)\n- paper: [http://www.robots.ox.ac.uk/~vgg/publications/2014/Pickup14/pickup14.pdf](http://www.robots.ox.ac.uk/~vgg/publications/2014/Pickup14/pickup14.pdf)\n- paper: [http://people.csail.mit.edu/yichangshih/toa_web/ArrowCVPR131101.pdf](http://people.csail.mit.edu/yichangshih/toa_web/ArrowCVPR131101.pdf)\n\n**Time-lapse Mining from Internet Photos**\n\n![](http://grail.cs.washington.edu/projects/timelapse/teaser2.jpg)\n\n- intro: SIGGRAPH 2015\n- project page: [http://grail.cs.washington.edu/projects/timelapse/](http://grail.cs.washington.edu/projects/timelapse/)\n- paper: [http://grail.cs.washington.edu/projects/timelapse/TimelapseMiningSIGGRAPH15.pdf](http://grail.cs.washington.edu/projects/timelapse/TimelapseMiningSIGGRAPH15.pdf)\n- wired: [https://www.wired.com/2015/05/crowdsourced-timelapse/](https://www.wired.com/2015/05/crowdsourced-timelapse/)\n\n**3D Time-lapse Reconstruction from Internet Photos**\n\n- intro: ICCV 2015 (oral)\n- project page: [http://grail.cs.washington.edu/projects/timelapse3d/](http://grail.cs.washington.edu/projects/timelapse3d/)\n- paper: [http://grail.cs.washington.edu/projects/timelapse3d/3DTimelapseReconstructionICCV15.pdf](http://grail.cs.washington.edu/projects/timelapse3d/3DTimelapseReconstructionICCV15.pdf)\n\n**The Fast Bilateral Solver**\n\n- intro: ECCV 2016 Best Honorable Mention Award\n- arxiv: [https://arxiv.org/abs/1511.03296](https://arxiv.org/abs/1511.03296)\n- github: [https://github.com/poolio/bilateral_solver](https://github.com/poolio/bilateral_solver)\n\n**Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects**\n\n![](http://grail.cs.washington.edu/projects/size/images/teaser.jpg)\n\n- arxiv: [http://arxiv.org/abs/1602.00753](http://arxiv.org/abs/1602.00753)\n- project page: [http://grail.cs.washington.edu/projects/size/](http://grail.cs.washington.edu/projects/size/)\n\n**Atoms of recognition in human and computer vision**\n\n![](http://www.wisdom.weizmann.ac.il/~dannyh/Mircs/cover.jpg)\n\n- homepage: [http://www.wisdom.weizmann.ac.il/~dannyh/Mircs/mircs.html](http://www.wisdom.weizmann.ac.il/~dannyh/Mircs/mircs.html)\n- paper: [https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20150929153916/Live-Texturing-of-Augmented-Reality-Characters-from-Colored-Drawings-Paper.pdf](https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20150929153916/Live-Texturing-of-Augmented-Reality-Characters-from-Colored-Drawings-Paper.pdf)\n\n**Live Texturing of Augmented Reality Characters from Colored Drawings**\n\n![](https://www.disneyresearch.com/wp-content/uploads/Live-Texturing-of-Augmented-Reality-Characters-from-Colored-Drawings-Image-1024x576.png)\n\n- homepage: [https://www.disneyresearch.com/publication/live-texturing-of-augmented-reality-characters/](https://www.disneyresearch.com/publication/live-texturing-of-augmented-reality-characters/)\n\n**Colorization for Image Compression**\n\n- arxiv: [http://arxiv.org/abs/1606.06314](http://arxiv.org/abs/1606.06314)\n\n**Face2Face: Real-time Face Capture and Reenactment of RGB Videos**\n\n![](http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/teaser.jpg)\n\n- project page: [http://www.graphics.stanford.edu/~niessner/thies2016face.html](http://www.graphics.stanford.edu/~niessner/thies2016face.html)\n- paper: [http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf](http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf)\n\n# Applications\n\n**Target acquired: Finding targets in drone and quadcopter video streams using Python and OpenCV**\n[http://www.pyimagesearch.com/2015/05/04/target-acquired-finding-targets-in-drone-and-quadcopter-video-streams-using-python-and-opencv/](http://www.pyimagesearch.com/2015/05/04/target-acquired-finding-targets-in-drone-and-quadcopter-video-streams-using-python-and-opencv/)\n\n**FaceDirector: Continuous Control of Facial Performance in Video**\n\n![](https://www.disneyresearch.com/wp-content/uploads/FaceDirector-Continuous-Control-of-Facial-Performance-in-Video-Image.png)\n\n- homepage: [http://www.disneyresearch.com/publication/facedirector/](http://www.disneyresearch.com/publication/facedirector/)\n- paper: [http://disneyresearch.s3-us-west-1.amazonaws.com/wp-content/uploads/20151210174750/FaceDirector-Continuous-Control-of-Facial-Performance-in-Video-Paper.pdf](http://disneyresearch.s3-us-west-1.amazonaws.com/wp-content/uploads/20151210174750/FaceDirector-Continuous-Control-of-Facial-Performance-in-Video-Paper.pdf)\n\n**Real-time Expression Transfer for Facial Reenactment**\n\n![](http://graphics.stanford.edu/~niessner/papers/2015/10face/teaser.jpg)\n\n- homepage: [http://graphics.stanford.edu/~niessner/thies2015realtime.html](http://graphics.stanford.edu/~niessner/thies2015realtime.html)\n- paper: [http://graphics.stanford.edu/~niessner/papers/2015/10face/thies2015realtime.pdf](http://graphics.stanford.edu/~niessner/papers/2015/10face/thies2015realtime.pdf)\n\n**Photo Stylistic Brush: Robust Style Transfer via Superpixel-Based Bipartite Graph**\n\n- arxiv: [http://arxiv.org/abs/1606.03871](http://arxiv.org/abs/1606.03871)\n\n**GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence**\n\n![](https://i1.wp.com/jwbian.net/wp-content/uploads/2017/03/dog_ours.jpg?resize=768%2C512)\n\n- intro: CVPR 2017\n- project page: [http://jwbian.net/gms](http://jwbian.net/gms)\n- github: [https://github.com/JiawangBian/GMS-Feature-Matcher](https://github.com/JiawangBian/GMS-Feature-Matcher)\n\n# Projects\n\n**OpenBR: Open Source Biometrics, Face Recognition, Age Estimation, Gender Estimation**\n\n![](http://openbiometrics.org/diagram.png)\n\n- homepage: [http://openbiometrics.org/](http://openbiometrics.org/)\n- github: [https://github.com/biometrics/openbr](https://github.com/biometrics/openbr)\n- docs: [http://openbiometrics.org/docs/index.html](http://openbiometrics.org/docs/index.html)\n\n**SmartMirror**\n\n<p align=\"center\">\n  <img src=\"/docs/SmartMirror_DisplayMenu_Preview.gif\"/>       <img src=\"/docs/SmartMirror_Widget_Preview.gif\"/>\n</p>\n\n- github: [https://github.com/Shinao/SmartMirror](https://github.com/Shinao/SmartMirror)\n\n**Home Surveilance with Facial Recognition**\n\n![](https://raw.githubusercontent.com/BrandonJoffe/home_surveillance/prototype/system/debugging/dashboard.png)\n\n- github: [https://github.com/BrandonJoffe/home_surveillance](https://github.com/BrandonJoffe/home_surveillance)\n\n**Image unshredding using a TSP solver**\n\n![](https://camo.githubusercontent.com/fbf30f6bce6931eee114366a5dc3372f259451ff/68747470733a2f2f726f62696e686f7573746f6e2e6769746875622e696f2f696d6167652d756e736872656464696e672f696d616765732f6c656173742d737175617265732f626c75652d686f75722d70617269732e706e67)\n\n- github: [https://github.com/robinhouston/image-unshredding](https://github.com/robinhouston/image-unshredding)\n\n# Resources\n\n**Awesome Computer Vision**\n\n- github: [https://github.com/jbhuang0604/awesome-computer-vision](https://github.com/jbhuang0604/awesome-computer-vision)\n\n**Resources: Visual Recognition and Search**\n\n- intro: \"Non-exhaustive list of state-of-the-art implementations related to visual recognition and search\"\n- blog: [http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html](http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html)\n\n# Libraries\n\n**BoofCV: an open source Java library for real-time computer vision and robotics applications**\n\n[http://boofcv.org/index.php?title=Main_Page](http://boofcv.org/index.php?title=Main_Page)\n\n**tracking.js: A modern approach for Computer Vision on the web**\n\n- homepage: [https://trackingjs.com/](https://trackingjs.com/)\n- github: [https://github.com/eduardolundgren/tracking.js/](https://github.com/eduardolundgren/tracking.js/)\n\n**FastCV Computer Vision SDK**\n\n- homepage: [https://developer.qualcomm.com/software/fastcv-sdk](https://developer.qualcomm.com/software/fastcv-sdk)\n\n**Video++, a C++14 high performance video and image processing library**\n\n- github: [https://github.com/matt-42/vpp](https://github.com/matt-42/vpp)\n- doc: [http://documentup.com/matt-42/vpp](http://documentup.com/matt-42/vpp)\n\n**VLFeat -- Vision Lab Features Library**\n\n- intro: Algorithms include Fisher Vector, VLAD, SIFT, MSER, k-means, hierarchical k-means, \nagglomerative information bottleneck, SLIC superpixels, quick shift superpixels, large scale SVM training, and many others\n- homapage: [http://www.vlfeat.org/](http://www.vlfeat.org/)\n- github: [https://github.com/vlfeat/vlfeat](https://github.com/vlfeat/vlfeat)\n\n# Datasets\n\n**CVonline: Image Databases**\n\n[http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm](http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm)\n\n**Yet Another Computer Vision Index To Datasets (YACVID)**\n\n[http://riemenschneider.hayko.at/vision/dataset/](http://riemenschneider.hayko.at/vision/dataset/)\n\n# Blogs\n\n**From feature descriptors to deep learning: 20 years of computer vision**\n\n- blog: [http://www.computervisionblog.com/2015/01/from-feature-descriptors-to-deep.html](http://www.computervisionblog.com/2015/01/from-feature-descriptors-to-deep.html)\n\n**Unsupervised Computer Vision: The State of the Art | Stitch Fix Technology – Multithreaded**\n\n- blog: [http://multithreaded.stitchfix.com/blog/2016/02/04/computer-vision-state-of-the-art](http://multithreaded.stitchfix.com/blog/2016/02/04/computer-vision-state-of-the-art)\n- slides: [http://pan.baidu.com/s/1c0Sxzvq](http://pan.baidu.com/s/1c0Sxzvq)\n\n**Exploring Computer Vision**\n\n- Part I: Convolutional Neural Networks: [https://indico.io/blog/exploring-computer-vision-convolutional-neural-nets/](https://indico.io/blog/exploring-computer-vision-convolutional-neural-nets/)\n- Part II: Transfer Learning: [https://indico.io/blog/exploring-computer-vision-transfer-learning/](https://indico.io/blog/exploring-computer-vision-transfer-learning/)\n\n**Image Processing with Numpy**\n\n- blog: [http://www.degeneratestate.org/posts/2016/Oct/23/image-processing-with-numpy/](http://www.degeneratestate.org/posts/2016/Oct/23/image-processing-with-numpy/)\n\n# Conferences\n\n**SIGGRAPH 2016 papers on the web**\n\n[http://kesen.realtimerendering.com/sig2016.html](http://kesen.realtimerendering.com/sig2016.html)\n\n# Resources\n\n**The Ultimate List of 300+ Computer Vision Resources**\n\n- blog: [https://hackerlists.com/computer-vision-resources/](https://hackerlists.com/computer-vision-resources/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","title":"Distribued System Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: distribued_system\ntitle: Distribued System Resources\ndate: 2015-12-12\n---\n\n# Courses\n\n**15-440/640, Spring 2014: Distributed Systems**\n\n- homepage: [http://www.cs.cmu.edu/~dga/15-440/S14/](http://www.cs.cmu.edu/~dga/15-440/S14/)\n\n**15-712 Advanced and Distributed Operating Systems, Spring 2012**\n\n- homepage: [http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15712-s12/www/](http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15712-s12/www/)\n\n**CSE 552 Fall 2013 Lecture Topics**\n\n[http://courses.cs.washington.edu/courses/cse552/13au/calendar/lecturelist.html](http://courses.cs.washington.edu/courses/cse552/13au/calendar/lecturelist.html)\n\n**An introduction to distributed systems**\n\n- github: [https://github.com/aphyr/distsys-class](https://github.com/aphyr/distsys-class)\n\n**Principles of Distributed Computing (lecture collection)**\n\n[http://dcg.ethz.ch/lectures/podc_allstars/](http://dcg.ethz.ch/lectures/podc_allstars/)\n\n**CME 323: Distributed Algorithms and Optimization**\n\n[http://stanford.edu/~rezab/dao/](http://stanford.edu/~rezab/dao/)\n\n# Tutorials\n\n**Distributed systems: for fun and profit**\n\n- book: [http://book.mixu.net/distsys/intro.html](http://book.mixu.net/distsys/intro.html)\n\n**Fallacies of Distributed Computing Explained**\n\n[http://www.rgoarchitects.com/Files/fallacies.pdf](http://www.rgoarchitects.com/Files/fallacies.pdf)\n\n**A Note on Distributed Computing (1994)**\n\n- paper: [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628)\n\n**Distributed systems theory for the distributed systems engineer**\n\n[http://the-paper-trail.org/blog/distributed-systems-theory-for-the-distributed-systems-engineer/](http://the-paper-trail.org/blog/distributed-systems-theory-for-the-distributed-systems-engineer/)\n\n# Papers\n\n**linalg: Matrix Computations in Apache Spark**\n\n- arxiv: [http://arxiv.org/abs/1509.02256](http://arxiv.org/abs/1509.02256)\n\n# Blogs\n\n**Spark vs. Hadoop MapReduce**\n\n[https://www.xplenty.com/blog/2014/11/apache-spark-vs-hadoop-mapreduce/](https://www.xplenty.com/blog/2014/11/apache-spark-vs-hadoop-mapreduce/)\n\n**Spark Tutorial**\n\n[https://www.supergloo.com/spark-tutorial/](https://www.supergloo.com/spark-tutorial/)\n\n# Reading and Questions\n\n**学习分布式系统需要怎样的知识？**\n\n- zhihu: [https://www.zhihu.com/question/23645117](https://www.zhihu.com/question/23645117)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","title":"Writting CS Papers"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: computer_science\ntitle: Writting CS Papers\ndate: 2015-11-30\n---\n\n# Blog\n\n**Ten Tips for Writing CS Papers**\n\n- Part 1: [http://www.nowozin.net/sebastian/blog/ten-tips-for-writing-cs-papers-part-1.html](http://www.nowozin.net/sebastian/blog/ten-tips-for-writing-cs-papers-part-1.html)\n- Part 2: [http://www.nowozin.net/sebastian/blog/ten-tips-for-writing-cs-papers-part-2.html](http://www.nowozin.net/sebastian/blog/ten-tips-for-writing-cs-papers-part-2.html)\n\n**Tips on publishing in NIPS, ICML or any top tier conferences for ML**\n\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/3x3urc/tips_on_publishing_in_nips_icml_or_any_top_tier/](https://www.reddit.com/r/MachineLearning/comments/3x3urc/tips_on_publishing_in_nips_icml_or_any_top_tier/)\n\n**[ICML 2016] [META] What makes a good paper, and submission in Deep Learning**\n\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/44a72t/icml_2016_meta_what_makes_a_good_paper_and](https://www.reddit.com/r/MachineLearning/comments/44a72t/icml_2016_meta_what_makes_a_good_paper_and)\n\n# Books\n\n**How to Write a Thesis**\n\n![](https://mitpress.mit.edu/sites/default/files/9780262527132_0_0.jpg)\n\n- book: [https://mitpress.mit.edu/index.php?q=books/how-write-thesis](https://mitpress.mit.edu/index.php?q=books/how-write-thesis)\n- review: [https://www.insidehighered.com/views/2015/03/18/review-umberto-eco-how-write-thesis](https://www.insidehighered.com/views/2015/03/18/review-umberto-eco-how-write-thesis)"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","title":"Deep Learning Applications"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Deep Learning Applications\ndate: 2015-10-09\n---\n\n# Applications\n\n**DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations**\n\n- arxiv: [http://arxiv.org/abs/1510.02927](http://arxiv.org/abs/1510.02927)\n\n**Some like it hot - visual guidance for preference prediction**\n\n- arxiv: [http://arxiv.org/abs/1510.07867](http://arxiv.org/abs/1510.07867)\n- demo: [http://howhot.io/](http://howhot.io/)\n\n**Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey**\n\n- arxiv: [http://arxiv.org/abs/1512.03131](http://arxiv.org/abs/1512.03131)\n\n**Deep Relative Attributes**\n\n- intro: ACCV 2016\n- arxiv: [http://arxiv.org/abs/1512.04103](http://arxiv.org/abs/1512.04103)\n- github: [https://github.com/yassersouri/ghiaseddin](https://github.com/yassersouri/ghiaseddin)\n\n**Deep-Spying: Spying using Smartwatch and Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1512.05616](http://arxiv.org/abs/1512.05616)\n- github: [https://github.com/tonybeltramelli/Deep-Spying](https://github.com/tonybeltramelli/Deep-Spying)\n\n**Camera identification with deep convolutional networks**\n\n- key word: copyright infringement cases, ownership attribution\n- arxiv: [http://arxiv.org/abs/1603.01068](http://arxiv.org/abs/1603.01068)\n\n**An Analysis of Deep Neural Network Models for Practical Applications**\n\n- arxiv: [http://arxiv.org/abs/1605.07678](http://arxiv.org/abs/1605.07678)\n\n**8 Inspirational Applications of Deep Learning**\n\n- intro: Colorization of Black and White Images, Adding Sounds To Silent Movies, Automatic Machine Translation\nObject Classification in Photographs, Automatic Handwriting Generation, Character Text Generation, \nImage Caption Generation, Automatic Game Playing\n- blog: [http://machinelearningmastery.com/inspirational-applications-deep-learning/](http://machinelearningmastery.com/inspirational-applications-deep-learning/)\n\n**16 Open Source Deep Learning Models Running as Microservices**\n\n- intro: Places 365 Classifier, Deep Face Recognition, Real Estate Classifier, Colorful Image Colorization, \nIllustration Tagger, InceptionNet, Parsey McParseface, ArtsyNetworks\n- blog: [http://blog.algorithmia.com/2016/07/open-source-deep-learning-algorithm-roundup/](http://blog.algorithmia.com/2016/07/open-source-deep-learning-algorithm-roundup/)\n\n**Deep Cascaded Bi-Network for Face Hallucination**\n\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/CBN.html](http://mmlab.ie.cuhk.edu.hk/projects/CBN.html)\n- arxiv: [http://arxiv.org/abs/1607.05046](http://arxiv.org/abs/1607.05046)\n\n**DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation**\n\n![](http://sites.skoltech.ru/compvision/projects/deepwarp/images/pipeline.svg)\n\n- project page: [http://yaroslav.ganin.net/static/deepwarp/](http://yaroslav.ganin.net/static/deepwarp/)\n- arxiv: [http://arxiv.org/abs/1607.07215](http://arxiv.org/abs/1607.07215)\n\n**Autoencoding Blade Runner**\n\n- blog: [https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.9kckqg7cq](https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.9kckqg7cq)\n- github: [https://github.com/terrybroad/Learned-Sim-Autoencoder-For-Video-Frames](https://github.com/terrybroad/Learned-Sim-Autoencoder-For-Video-Frames)\n\n**A guy trained a machine to \"watch\" Blade Runner. Then things got seriously sci-fi.**\n\n[http://www.vox.com/2016/6/1/11787262/blade-runner-neural-network-encoding](http://www.vox.com/2016/6/1/11787262/blade-runner-neural-network-encoding)\n\n**Deep Convolution Networks for Compression Artifacts Reduction**\n\n![](http://mmlab.ie.cuhk.edu.hk/projects/ARCNN/img/fig1.png)\n\n- intro: ICCV 2015\n- project page(code): [http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html](http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html)\n- arxiv: [http://arxiv.org/abs/1608.02778](http://arxiv.org/abs/1608.02778)\n\n**Deep GDashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks**\n\n- intro: Deep Genomic Dashboard (Deep GDashboard)\n- arxiv: [http://arxiv.org/abs/1608.03644](http://arxiv.org/abs/1608.03644)\n\n**Instagram photos reveal predictive markers of depression**\n\n- arxiv: [http://arxiv.org/abs/1608.03282](http://arxiv.org/abs/1608.03282)\n\n**How an Algorithm Learned to Identify Depressed Individuals by Studying Their Instagram Photos**\n\n- review: [https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/](https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/)\n\n**IM2CAD**\n\n- arxiv: [http://arxiv.org/abs/1608.05137](http://arxiv.org/abs/1608.05137)\n\n**Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks**\n\n- paper: [https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/melicher](https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/melicher)\n- github: [https://github.com/cupslab/neural_network_cracking](https://github.com/cupslab/neural_network_cracking)\n\n**Defeating Image Obfuscation with Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1609.00408](http://arxiv.org/abs/1609.00408)\n\n**Detecting Music BPM using Neural Networks**\n\n![](https://nlml.github.io/images/convnet_diagram.png)\n\n- keywords: BPM (Beats Per Minutes)\n- blog: [https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/](https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/)\n- github: [https://github.com/nlml/bpm](https://github.com/nlml/bpm)\n\n**Generative Visual Manipulation on the Natural Image Manifold**\n\n![](https://raw.githubusercontent.com/junyanz/iGAN/master/pics/demo_teaser.jpg)\n\n- intro: ECCV 2016\n- project page: [https://people.eecs.berkeley.edu/~junyanz/projects/gvm/](https://people.eecs.berkeley.edu/~junyanz/projects/gvm/)\n- arxiv: [http://arxiv.org/abs/1609.03552](http://arxiv.org/abs/1609.03552)\n- github: [https://github.com/junyanz/iGAN](https://github.com/junyanz/iGAN)\n\n**Deep Impression: Audiovisual Deep Residual Networks for Multimodal Apparent Personality Trait Recognition**\n\n- arxiv: [http://arxiv.org/abs/1609.05119](http://arxiv.org/abs/1609.05119)\n\n**Deep Gold: Using Convolution Networks to Find Minerals**\n\n- blog: [https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df#.lgh95ub4a](https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df#.lgh95ub4a)\n- github: [https://github.com/scottvallance/DeepGold](https://github.com/scottvallance/DeepGold)\n\n**Predicting First Impressions with Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1610.08119](https://arxiv.org/abs/1610.08119)\n\n**Judging a Book By its Cover**\n\n- arxiv: [https://arxiv.org/abs/1610.09204](https://arxiv.org/abs/1610.09204)\n- review: [https://www.technologyreview.com/s/602807/deep-neural-network-learns-to-judge-books-by-their-covers/](https://www.technologyreview.com/s/602807/deep-neural-network-learns-to-judge-books-by-their-covers/)\n\n**Image Credibility Analysis with Effective Domain Transferred Deep Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.05328](https://arxiv.org/abs/1611.05328)\n\n**A novel image tag completion method based on convolutional neural network**\n\n- arxiv: [https://www.arxiv.org/abs/1703.00586](https://www.arxiv.org/abs/1703.00586)\n\n**Image operator learning coupled with CNN classification and its application to staff line removal**\n\n- intro: ICDAR 2017\n- arxiv: [https://arxiv.org/abs/1709.06476](https://arxiv.org/abs/1709.06476)\n\n**Joint Image Filtering with Deep Convolutional Networks**\n\n- intro: University of California, Merced & Virginia Tech & University of Illinois\n- project page: [http://vllab1.ucmerced.edu/~yli62/DJF_residual/](http://vllab1.ucmerced.edu/~yli62/DJF_residual/)\n- arxiv: [https://arxiv.org/abs/1710.04200](https://arxiv.org/abs/1710.04200)\n- github: [https://github.com/Yijunmaverick/DeepJointFilter](https://github.com/Yijunmaverick/DeepJointFilter)\n\n**DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1704.02470](https://arxiv.org/abs/1704.02470)\n- github: [https://github.com/aiff22/DPED](https://github.com/aiff22/DPED)\n\n**Neural Scene De-rendering**\n\n- intro: CVPR 2017\n- project page: [http://nsd.csail.mit.edu/](http://nsd.csail.mit.edu/)\n- paper: [http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf](http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf)\n- gihtub: [https://github.com/jiajunwu/nsd](https://github.com/jiajunwu/nsd)\n\n**Image2GIF: Generating Cinemagraphs using Recurrent Deep Q-Networks**\n\n- intro: WACV 2018\n- project page: [http://bvision11.cs.unc.edu/bigpen/yipin/WACV2018/](http://bvision11.cs.unc.edu/bigpen/yipin/WACV2018/)\n- arxiv: [https://arxiv.org/abs/1801.09042](https://arxiv.org/abs/1801.09042)\n\n**Deep Neural Networks In Fully Connected CRF For Image Labeling With Social Network Metadata**\n\n[https://arxiv.org/abs/1801.09108](https://arxiv.org/abs/1801.09108)\n\n**Single Image Reflection Removal Using Deep Encoder-Decoder Network**\n\n[https://arxiv.org/abs/1802.00094](https://arxiv.org/abs/1802.00094)\n\n**CRRN: Multi-Scale Guided Concurrent Reflection Removal Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1805.11802](https://arxiv.org/abs/1805.11802)\n\n**Learning Deep Convolutional Networks for Demosaicing**\n\n[https://arxiv.org/abs/1802.03769](https://arxiv.org/abs/1802.03769)\n\n**Fully convolutional watermark removal attack**\n\n- github: [https://github.com/marcbelmont/cnn-watermark-removal](https://github.com/marcbelmont/cnn-watermark-removal)\n\n**ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes**\n\n- arxiv: [https://arxiv.org/abs/1803.10562](https://arxiv.org/abs/1803.10562)\n- github: [https://github.com/Prinsphield/ELEGANT](https://github.com/Prinsphield/ELEGANT)\n\n**Learning to See in the Dark**\n\n- intro: CVPR 2018\n- project page: [http://web.engr.illinois.edu/~cchen156/SID.html](http://web.engr.illinois.edu/~cchen156/SID.html)\n- arxiv: [https://arxiv.org/abs/1805.01934](https://arxiv.org/abs/1805.01934)\n- github: [https://github.com/cchen156/Learning-to-See-in-the-Dark](https://github.com/cchen156/Learning-to-See-in-the-Dark)\n- video: [https://www.youtube.com/watch?v=qWKUFK7MWvg&feature=youtu.be](https://www.youtube.com/watch?v=qWKUFK7MWvg&feature=youtu.be)\n- video: [https://www.bilibili.com/video/av23195280/](https://www.bilibili.com/video/av23195280/)\n\n**Generative Smoke Removal**\n\n[https://arxiv.org/abs/1902.00311](https://arxiv.org/abs/1902.00311)\n\n**Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data**\n\n[https://arxiv.org/abs/1903.10683](https://arxiv.org/abs/1903.10683)\n\n**Blind Visual Motif Removal from a Single Image**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.02756](https://arxiv.org/abs/1904.02756)\n\n**Neural Camera Simulators**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.05237](https://arxiv.org/abs/2104.05237)\n\n**Lighting the Darkness in the Deep Learning Era**\n\n- arxiv: [https://arxiv.org/abs/2104.10729](https://arxiv.org/abs/2104.10729)\n- github: [https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open](https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open)\n\n# Boundary / Edge / Contour Detection\n\n**Holistically-Nested Edge Detection**\n\n![](https://camo.githubusercontent.com/da32e7e3275c2a9693dd2a6925b03a1151e2b098/687474703a2f2f70616765732e756373642e6564752f7e7a74752f6865642e6a7067)\n\n- intro: ICCV 2015, Marr Prize\n- paper: [http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf)\n- arxiv: [http://arxiv.org/abs/1504.06375](http://arxiv.org/abs/1504.06375)\n- github: [https://github.com/s9xie/hed](https://github.com/s9xie/hed)\n- github: [https://github.com/moabitcoin/holy-edge](https://github.com/moabitcoin/holy-edge)\n\n**Unsupervised Learning of Edges**\n\n- intro: CVPR 2016. Facebook AI Research\n- arxiv: [http://arxiv.org/abs/1511.04166](http://arxiv.org/abs/1511.04166)\n- zn-blog: [http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html](http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html)\n\n**Pushing the Boundaries of Boundary Detection using Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1511.07386](http://arxiv.org/abs/1511.07386)\n\n**Convolutional Oriented Boundaries**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1608.02755](http://arxiv.org/abs/1608.02755)\n\n**Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks**\n\n- project page: [http://www.vision.ee.ethz.ch/~cvlsegmentation/](http://www.vision.ee.ethz.ch/~cvlsegmentation/)\n- arxiv: [https://arxiv.org/abs/1701.04658](https://arxiv.org/abs/1701.04658)\n- github: [https://github.com/kmaninis/COB](https://github.com/kmaninis/COB)\n\n**Richer Convolutional Features for Edge Detection**\n\n- intro: CVPR 2017\n- keywords: richer convolutional features (RCF)\n- arxiv: [https://arxiv.org/abs/1612.02103](https://arxiv.org/abs/1612.02103)\n- github: [https://github.com/yun-liu/rcf](https://github.com/yun-liu/rcf)\n\n**Contour Detection from Deep Patch-level Boundary Prediction**\n\n[https://arxiv.org/abs/1705.03159](https://arxiv.org/abs/1705.03159)\n\n**CASENet: Deep Category-Aware Semantic Edge Detection**\n\n- intro: CVPR 2017. CMU & Mitsubishi Electric Research Laboratories (MERL)\n- arxiv: [https://arxiv.org/abs/1705.09759](https://arxiv.org/abs/1705.09759)\n- code: [http://www.merl.com/research/license#CASENet](http://www.merl.com/research/license#CASENet)\n- video: [https://www.youtube.com/watch?v=BNE1hAP6Qho](https://www.youtube.com/watch?v=BNE1hAP6Qho)\n\n**Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1801.00524](https://arxiv.org/abs/1801.00524)\n\n**Deep Crisp Boundaries: From Boundaries to Higher-level Tasks**\n\n[https://arxiv.org/abs/1801.02439](https://arxiv.org/abs/1801.02439)\n\n**DOOBNet: Deep Object Occlusion Boundary Detection from an Image**\n\n[https://arxiv.org/abs/1806.03772](https://arxiv.org/abs/1806.03772)\n\n**Dynamic Feature Fusion for Semantic Edge Detection**\n\n[https://arxiv.org/abs/1902.09104](https://arxiv.org/abs/1902.09104)\n\n**EDTER: Edge Detection with Transformer**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.08566](https://arxiv.org/abs/2203.08566)\n- github: [https://github.com/MengyangPu/EDTER](https://github.com/MengyangPu/EDTER)\n\n# Image Processing\n\n**Fast Image Processing with Fully-Convolutional Networks**\n\n- intro: ICCV 2017. Qifeng Chen (陈启峰)\n- project page: [http://www.cqf.io/ImageProcessing/](http://www.cqf.io/ImageProcessing/)\n- arxiv: [https://arxiv.org/abs/1709.00643](https://arxiv.org/abs/1709.00643)\n- supp: [https://youtu.be/eQyfHgLx8Dc](https://youtu.be/eQyfHgLx8Dc)\n- github: [https://github.com/CQFIO/FastImageProcessing](https://github.com/CQFIO/FastImageProcessing)\n\n**DeepISP: Learning End-to-End Image Processing Pipeline**\n\n[https://arxiv.org/abs/1801.06724](https://arxiv.org/abs/1801.06724)\n\n**Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.04323](https://arxiv.org/abs/1811.04323)\n\n# Image-Text\n\n**Learning Two-Branch Neural Networks for Image-Text Matching Tasks**\n\n[https://arxiv.org/abs/1704.03470](https://arxiv.org/abs/1704.03470)\n\n**Dual-Path Convolutional Image-Text Embedding**\n\n- arxiv: [https://arxiv.org/abs/1711.05535](https://arxiv.org/abs/1711.05535)\n- github: [https://github.com//layumi/Image-Text-Embedding](https://github.com//layumi/Image-Text-Embedding)\n\n**Conditional Image-Text Embedding Networks**\n\n[https://arxiv.org/abs/1711.08389](https://arxiv.org/abs/1711.08389)\n\n**AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1711.10485](https://arxiv.org/abs/1711.10485)\n\n**Stacked Cross Attention for Image-Text Matching**\n\n[https://arxiv.org/abs/1803.08024](https://arxiv.org/abs/1803.08024)\n\n# Age Estimation\n\n**Deeply-Learned Feature for Age Estimation**\n\n- paper: [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7045931&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7045931](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7045931&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7045931)\n\n**Age and Gender Classification using Convolutional Neural Networks**\n\n- paper: [http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf](http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf)\n- project page: [http://www.openu.ac.il/home/hassner/projects/cnn_agegender/](http://www.openu.ac.il/home/hassner/projects/cnn_agegender/)\n- github: [https://github.com/GilLevi/AgeGenderDeepLearning](https://github.com/GilLevi/AgeGenderDeepLearning)\n\n**Group-Aware Deep Feature Learning For Facial Age Estimation**\n\n- paper: [http://www.sciencedirect.com/science/article/pii/S0031320316303417](http://www.sciencedirect.com/science/article/pii/S0031320316303417)\n\n**Local Deep Neural Networks for Age and Gender Classification**\n\n[https://arxiv.org/abs/1703.08497](https://arxiv.org/abs/1703.08497)\n\n**Understanding and Comparing Deep Neural Networks for Age and Gender Classification**\n\n[https://arxiv.org/abs/1708.07689](https://arxiv.org/abs/1708.07689)\n\n**Age Group and Gender Estimation in the Wild with Deep RoR Architecture**\n\n- intro: IEEE ACCESS\n- arxiv: [https://arxiv.org/abs/1710.02985](https://arxiv.org/abs/1710.02985)\n\n**Age and gender estimation based on Convolutional Neural Network and TensorFlow**\n\n[https://github.com/BoyuanJiang/Age-Gender-Estimate-TF](https://github.com/BoyuanJiang/Age-Gender-Estimate-TF)\n\n**Deep Regression Forests for Age Estimation**\n\n- intro: Shanghai University & Johns Hopkins University & Nankai University\n- arxiv: [https://arxiv.org/abs/1712.07195](https://arxiv.org/abs/1712.07195)\n\n# Face Aging\n\n**Recurrent Face Aging**\n\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf)\n\n**Face Aging With Conditional Generative Adversarial Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.01983](https://arxiv.org/abs/1702.01983)\n\n**Learning Face Age Progression: A Pyramid Architecture of GANs**\n\n[https://arxiv.org/abs/1711.10352](https://arxiv.org/abs/1711.10352)\n\n**Face Aging with Contextual Generative Adversarial Nets**\n\n- intro: ACM Multimedia 2017\n- arxiv: [https://arxiv.org/abs/1802.00237](https://arxiv.org/abs/1802.00237)\n\n**Recursive Chaining of Reversible Image-to-image Translators For Face Aging**\n\n[https://arxiv.org/abs/1802.05023](https://arxiv.org/abs/1802.05023)\n\n# Emotion Recognition / Expression Recognition\n\n**Real-time emotion recognition for gaming using deep convolutional network features**\n\n- paper: [http://arxiv.org/abs/1408.3750v1](http://arxiv.org/abs/1408.3750v1)\n- code: [https://github.com/Zebreu/ConvolutionalEmotion](https://github.com/Zebreu/ConvolutionalEmotion)\n\n**Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns**\n\n- project page: [http://www.openu.ac.il/home/hassner/projects/cnn_emotions/](http://www.openu.ac.il/home/hassner/projects/cnn_emotions/)\n- paper: [http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf](http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf)\n- github: [https://gist.github.com/GilLevi/54aee1b8b0397721aa4b](https://gist.github.com/GilLevi/54aee1b8b0397721aa4b)\n- blog: [https://gilscvblog.com/2017/01/31/emotion-recognition-in-the-wild-via-convolutional-neural-networks-and-mapped-binary-patterns/](https://gilscvblog.com/2017/01/31/emotion-recognition-in-the-wild-via-convolutional-neural-networks-and-mapped-binary-patterns/)\n\n**DeXpression: Deep Convolutional Neural Network for Expression Recognition**\n\n- paper: [http://arxiv.org/abs/1509.05371](http://arxiv.org/abs/1509.05371)\n\n**DEX: Deep EXpectation of apparent age from a single image**\n\n![](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/img/pipeline.png)\n\n- intro: ICCV 2015\n- paper: [https://www.vision.ee.ethz.ch/en/publications/papers/proceedings/eth_biwi_01229.pdf](https://www.vision.ee.ethz.ch/en/publications/papers/proceedings/eth_biwi_01229.pdf)\n- homepage: [https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/)\n\n**EmotioNet: EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild**\n\n- intro: CVPR 2016\n- paper: [http://cbcsl.ece.ohio-state.edu/cvpr16.pdf](http://cbcsl.ece.ohio-state.edu/cvpr16.pdf)\n- database: [http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html](http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html)\n\n**How Deep Neural Networks Can Improve Emotion Recognition on Video Data**\n\n- intro: ICIP 2016\n- arxiv: [http://arxiv.org/abs/1602.07377](http://arxiv.org/abs/1602.07377)\n\n**Peak-Piloted Deep Network for Facial Expression Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.06997](http://arxiv.org/abs/1607.06997)\n\n**Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution**\n\n- arxiv: [http://arxiv.org/abs/1608.01041](http://arxiv.org/abs/1608.01041)\n\n**A Recursive Framework for Expression Recognition: From Web Images to Deep Models to Game Dataset**\n\n- arxiv: [http://arxiv.org/abs/1608.01647](http://arxiv.org/abs/1608.01647)\n\n**FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition**\n\n- arxiv: [http://arxiv.org/abs/1609.06591](http://arxiv.org/abs/1609.06591)\n\n**EmotionNet Challenge**\n\n- homrepage: [http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html](http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html)\n- dataset: [http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html](http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html)\n\n**Baseline CNN structure analysis for facial expression recognition**\n\n- intro: RO-MAN2016 Conference\n- arxiv: [https://arxiv.org/abs/1611.04251](https://arxiv.org/abs/1611.04251)\n\n**Facial Expression Recognition using Convolutional Neural Networks: State of the Art**\n\n- arxiv: [https://arxiv.org/abs/1612.02903](https://arxiv.org/abs/1612.02903)\n\n**DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Network**\n\n- arxiv: [https://arxiv.org/abs/1702.04280](https://arxiv.org/abs/1702.04280)\n- api: [https://www.sighthound.com/products/cloud](https://www.sighthound.com/products/cloud)\n\n**Deep generative-contrastive networks for facial expression recognition**\n\n[https://arxiv.org/abs/1703.07140](https://arxiv.org/abs/1703.07140)\n\n**Convolutional Neural Networks for Facial Expression Recognition**\n\n[https://arxiv.org/abs/1704.06756](https://arxiv.org/abs/1704.06756)\n\n**End-to-End Multimodal Emotion Recognition using Deep Neural Networks**\n\n- intro: Imperial College London\n- arxiv: [https://arxiv.org/abs/1704.08619](https://arxiv.org/abs/1704.08619)\n\n**Spatial-Temporal Recurrent Neural Network for Emotion Recognition**\n\n[https://arxiv.org/abs/1705.04515](https://arxiv.org/abs/1705.04515)\n\n**Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units**\n\n[https://arxiv.org/abs/1706.01509](https://arxiv.org/abs/1706.01509)\n\n**Temporal Multimodal Fusion for Video Emotion Classification in the Wild**\n\n[https://arxiv.org/abs/1709.07200](https://arxiv.org/abs/1709.07200)\n\n**Island Loss for Learning Discriminative Features in Facial Expression Recognition**\n\n[https://arxiv.org/abs/1710.03144](https://arxiv.org/abs/1710.03144)\n\n**Real-time Convolutional Neural Networks for Emotion and Gender Classification**\n\n[https://arxiv.org/abs/1710.07557](https://arxiv.org/abs/1710.07557)\n\n# Attribution Prediction\n\n**PANDA: Pose Aligned Networks for Deep Attribute Modeling**\n\n- intro: Facebook. CVPR 2014\n- arxiv: [http://arxiv.org/abs/1311.5591](http://arxiv.org/abs/1311.5591)\n- github: [https://github.com/facebook/pose-aligned-deep-networks](https://github.com/facebook/pose-aligned-deep-networks)\n\n**Predicting psychological attributions from face photographs with a deep neural network**\n\n- arxiv: [http://arxiv.org/abs/1512.01289](http://arxiv.org/abs/1512.01289)\n\n**Learning Human Identity from Motion Patterns**\n\n- arxiv: [http://arxiv.org/abs/1511.03908](http://arxiv.org/abs/1511.03908)\n\n# Place Recognition\n\n**NetVLAD: CNN architecture for weakly supervised place recognition**\n\n![](http://www.di.ens.fr/willow/research/netvlad/images/teaser.png)\n\n- intro: CVPR 2016\n- intro: Google Street View Time Machine, soft-assignment, Weakly supervised triplet ranking loss\n- homepage: [http://www.di.ens.fr/willow/research/netvlad/](http://www.di.ens.fr/willow/research/netvlad/)\n- arxiv: [http://arxiv.org/abs/1511.07247](http://arxiv.org/abs/1511.07247)\n\n**PlaNet - Photo Geolocation with Convolutional Neural Networks**\n\n![](https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=590&cx=0&cy=0&cw=928&ch=614)\n\n- arxiv: [http://arxiv.org/abs/1602.05314](http://arxiv.org/abs/1602.05314)\n- review(\"Google Unveils Neural Network with “Superhuman” Ability to Determine the Location of Almost Any Image\"): [https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/](https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/)\n- github(\"City-Recognition: CS231n Project for Winter 2016\"): [https://github.com/dmakian/LittlePlaNet](https://github.com/dmakian/LittlePlaNet)\n- github: [https://github.com/wulfebw/LittlePlaNet-Models](https://github.com/wulfebw/LittlePlaNet-Models)\n\n**Visual place recognition using landmark distribution descriptors**\n\n- arxiv: [http://arxiv.org/abs/1608.04274](http://arxiv.org/abs/1608.04274)\n\n**Low-effort place recognition with WiFi fingerprints using deep learning**\n\n- arxiv: [https://arxiv.org/abs/1611.02049](https://arxiv.org/abs/1611.02049)\n- github: [https://github.com/aqibsaeed/Place-Recognition-using-Autoencoders-and-NN](https://github.com/aqibsaeed/Place-Recognition-using-Autoencoders-and-NN)\n- github(Keras): [https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning](https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning)\n\n**Deep Learning Features at Scale for Visual Place Recognition**\n\n- intro: ICRA 2017\n- arxiv: [https://arxiv.org/abs/1701.05105](https://arxiv.org/abs/1701.05105)\n\n**Place recognition: An Overview of Vision Perspective**\n\n[https://arxiv.org/abs/1707.03470](https://arxiv.org/abs/1707.03470)\n\n## Camera Relocalization\n\n**PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization**\n\n- paper: [http://arxiv.org/abs/1505.07427](http://arxiv.org/abs/1505.07427)\n- project page: [http://mi.eng.cam.ac.uk/projects/relocalisation/#results](http://mi.eng.cam.ac.uk/projects/relocalisation/#results)\n- github: [https://github.com/alexgkendall/caffe-posenet](https://github.com/alexgkendall/caffe-posenet)\n- github(TensorFlow): [https://github.com/kentsommer/tensorflow-posenet](https://github.com/kentsommer/tensorflow-posenet)\n\n**Modelling Uncertainty in Deep Learning for Camera Relocalization**\n\n- paper: [http://arxiv.org/abs/1509.05909](http://arxiv.org/abs/1509.05909)\n\n**Random Forests versus Neural Networks - What's Best for Camera Relocalization?**\n\n- arxiv: [http://arxiv.org/abs/1609.05797](http://arxiv.org/abs/1609.05797)\n\n**Deep Convolutional Neural Network for 6-DOF Image Localization**\n\n- arxiv: [https://arxiv.org/abs/1611.02776](https://arxiv.org/abs/1611.02776)\n\n**DSAC - Differentiable RANSAC for Camera Localization**\n\n- arxiv: [https://arxiv.org/abs/1611.05705](https://arxiv.org/abs/1611.05705)\n\n**Image-based Localization with Spatial LSTMs**\n\n- arxiv: [https://arxiv.org/abs/1611.07890](https://arxiv.org/abs/1611.07890)\n\n**VidLoc: 6-DoF Video-Clip Relocalization**\n\n- arxiv: [https://arxiv.org/abs/1702.06521](https://arxiv.org/abs/1702.06521)\n\n**Towards CNN Map Compression for camera relocalisation**\n\n- arxiv: [https://www.arxiv.org/abs/1703.00845](https://www.arxiv.org/abs/1703.00845)\n\n**Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network**\n\n- intro: Aalto University & Indian Institute of Technology\n- arxiv: [https://arxiv.org/abs/1707.09733](https://arxiv.org/abs/1707.09733)\n\n**MapNet: Geometry-Aware Learning of Maps for Camera Localization**\n\n- intro: Georgia Institute of Technology & NVIDIA\n- arxiv: [https://arxiv.org/abs/1712.03342](https://arxiv.org/abs/1712.03342)\n\n**Image-to-GPS Verification Through A Bottom-Up Pattern Matching Network**\n\n[https://arxiv.org/abs/1811.07288](https://arxiv.org/abs/1811.07288)\n\n# Activity Recognition\n\n**Implementing a CNN for Human Activity Recognition in Tensorflow**\n\n- blog: [http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/](http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/)\n- github: [https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN](https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN)\n\n**Concurrent Activity Recognition with Multimodal CNN-LSTM Structure**\n\n- arxiv: [https://arxiv.org/abs/1702.01638](https://arxiv.org/abs/1702.01638)\n\n**CERN: Confidence-Energy Recurrent Network for Group Activity Recognition**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.03058](https://arxiv.org/abs/1704.03058)\n\n**Deploying Tensorflow model on Andorid device for Human Activity Recognition**\n\n- blog: [http://aqibsaeed.github.io/2017-05-02-deploying-tensorflow-model-andorid-device-human-activity-recognition/](http://aqibsaeed.github.io/2017-05-02-deploying-tensorflow-model-andorid-device-human-activity-recognition/)\n- github: [https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/tree/master/ActivityRecognition](https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/tree/master/ActivityRecognition)\n\n# Music Classification / Sound Classification\n\n**Explaining Deep Convolutional Neural Networks on Music Classification**\n\n- arxiv: [http://arxiv.org/abs/1607.02444](http://arxiv.org/abs/1607.02444)\n- blog: [https://keunwoochoi.wordpress.com/2015/12/09/ismir-2015-lbd-auralisation-of-deep-convolutional-neural-networks-listening-to-learned-features-auralization/](https://keunwoochoi.wordpress.com/2015/12/09/ismir-2015-lbd-auralisation-of-deep-convolutional-neural-networks-listening-to-learned-features-auralization/)\n- blog: [https://keunwoochoi.wordpress.com/2016/03/23/what-cnns-see-when-cnns-see-spectrograms/](https://keunwoochoi.wordpress.com/2016/03/23/what-cnns-see-when-cnns-see-spectrograms/)\n- github: [https://github.com/keunwoochoi/Auralisation](https://github.com/keunwoochoi/Auralisation)\n- audio samples: [https://soundcloud.com/kchoi-research](https://soundcloud.com/kchoi-research)\n\n**Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification**\n\n- project page: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html)\n- arxiv: [http://arxiv.org/abs/1608.04363](http://arxiv.org/abs/1608.04363)\n\n**Convolutional Recurrent Neural Networks for Music Classification**\n\n![](https://keunwoochoi.files.wordpress.com/2016/09/screen-shot-2016-09-14-at-20-38-27.png?w=1200)\n\n- arxiv: [http://arxiv.org/abs/1609.04243](http://arxiv.org/abs/1609.04243)\n- blog: [https://keunwoochoi.wordpress.com/2016/09/15/paper-is-out-convolutional-recurrent-neural-networks-for-music-classification/](https://keunwoochoi.wordpress.com/2016/09/15/paper-is-out-convolutional-recurrent-neural-networks-for-music-classification/)\n- github: [https://github.com/keunwoochoi/music-auto_tagging-keras](https://github.com/keunwoochoi/music-auto_tagging-keras)\n\n**CNN Architectures for Large-Scale Audio Classification**\n\n- intro: Google\n- arxiv: [https://arxiv.org/abs/1609.09430](https://arxiv.org/abs/1609.09430)\n- demo: [https://www.youtube.com/watch?v=oAAo_r7ZT8U&feature=youtu.be](https://www.youtube.com/watch?v=oAAo_r7ZT8U&feature=youtu.be)\n\n**SoundNet: Learning Sound Representations from Unlabeled Video**\n\n- intro: MIT. NIPS 2016\n- project page: [http://projects.csail.mit.edu/soundnet/](http://projects.csail.mit.edu/soundnet/)\n- arxiv: [https://arxiv.org/abs/1610.09001](https://arxiv.org/abs/1610.09001)\n- paper: [http://web.mit.edu/vondrick/soundnet.pdf](http://web.mit.edu/vondrick/soundnet.pdf)\n- github: [https://github.com/cvondrick/soundnet](https://github.com/cvondrick/soundnet)\n- github: [https://github.com/eborboihuc/SoundNet-tensorflow](https://github.com/eborboihuc/SoundNet-tensorflow)\n- youtube: [https://www.youtube.com/watch?v=yJCjVvIY4dU](https://www.youtube.com/watch?v=yJCjVvIY4dU)\n\n**Deep Learning 'ahem' detector**\n\n- github: [https://github.com/worldofpiggy/deeplearning-ahem-detector](https://github.com/worldofpiggy/deeplearning-ahem-detector)\n- slides: [https://docs.google.com/presentation/d/1QXQEOiAMj0uF2_Gafr2bn-kMniUJAIM1PLTFm1mUops/edit#slide=id.g35f391192_00](https://docs.google.com/presentation/d/1QXQEOiAMj0uF2_Gafr2bn-kMniUJAIM1PLTFm1mUops/edit#slide=id.g35f391192_00)\n- mirror: [https://pan.baidu.com/s/1c2KGlwO](https://pan.baidu.com/s/1c2KGlwO)\n\n**GenreFromAudio: Finding the genre of a song with Deep Learning**\n\n- intro: A pipeline to build a dataset from your own music library and use it to fill the missing genres\n- github: [https://github.com/despoisj/DeepAudioClassification](https://github.com/despoisj/DeepAudioClassification)\n\n**TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition**\n\n- arxiv: [https://arxiv.org/abs/1703.10667](https://arxiv.org/abs/1703.10667)\n- github: [https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN](https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN)\n\n**On the Robustness of Deep Convolutional Neural Networks for Music Classification**\n\n- intro: Queen Mary University of London & New York University\n- arxiv: [https://arxiv.org/abs/1706.02361](https://arxiv.org/abs/1706.02361)\n\n# NSFW Detection / Classification\n\n**Nipple Detection using Convolutional Neural Network**\n\n- reddit: [https://www.reddit.com/over18?dest=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F33n77s%2Fandroid_app_nipple_detection_using_convolutional%2F](https://www.reddit.com/over18?dest=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F33n77s%2Fandroid_app_nipple_detection_using_convolutional%2F)\n\n**Applying deep learning to classify pornographic images and videos**\n\n- arxiv: [http://arxiv.org/abs/1511.08899](http://arxiv.org/abs/1511.08899)\n\n**MODERATE, FILTER, OR CURATE ADULT CONTENT WITH CLARIFAI’S NSFW MODEL**\n\n- blog: [http://blog.clarifai.com/moderate-filter-or-curate-adult-content-with-clarifais-nsfw-model/#.VzVhM-yECZY](http://blog.clarifai.com/moderate-filter-or-curate-adult-content-with-clarifais-nsfw-model/#.VzVhM-yECZY)\n\n**WHAT CONVOLUTIONAL NEURAL NETWORKS LOOK AT WHEN THEY SEE NUDITY**\n\n- blog: [http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity#.VzVh_-yECZY](http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity#.VzVh_-yECZY)\n\n**Open Sourcing a Deep Learning Solution for Detecting NSFW Images**\n\n- intro: Yahoo\n- blog: [https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for](https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for)\n- github: [https://github.com/yahoo/open_nsfw](https://github.com/yahoo/open_nsfw)\n\n**Miles Deep - AI Porn Video Editor**\n\n- intro: Deep Learning Porn Video Classifier/Editor with Caffe\n- github: [https://github.com/ryanjay0/miles-deep](https://github.com/ryanjay0/miles-deep)\n\n# Image Reconstruction / Inpainting\n\n**Context Encoders: Feature Learning by Inpainting**\n\n![](http://www.cs.berkeley.edu/~pathak/context_encoder/resources/result_fig.jpg)\n\n- intro: CVPR 2016\n- intro: Unsupervised Feature Learning by Image Inpainting using GANs\n- project page: [http://www.cs.berkeley.edu/~pathak/context_encoder/](http://www.cs.berkeley.edu/~pathak/context_encoder/)\n- arxiv: [https://arxiv.org/abs/1604.07379](https://arxiv.org/abs/1604.07379)\n- github(official): [https://github.com/pathak22/context-encoder](https://github.com/pathak22/context-encoder)\n- github: [https://github.com/BoyuanJiang/context_encoder_pytorch](https://github.com/BoyuanJiang/context_encoder_pytorch)\n\n**Semantic Image Inpainting with Perceptual and Contextual Losses**\n\n**Semantic Image Inpainting with Deep Generative Models**\n\n- keywords: Deep Convolutional Generative Adversarial Network (DCGAN)\n- arxiv: [http://arxiv.org/abs/1607.07539](http://arxiv.org/abs/1607.07539)\n- github: [https://github.com/bamos/dcgan-completion.tensorflow](https://github.com/bamos/dcgan-completion.tensorflow)\n\n**High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis**\n\n- intro: University of Southern California & Adobe Research\n- arxiv: [https://arxiv.org/abs/1611.09969](https://arxiv.org/abs/1611.09969)\n\n**Face Image Reconstruction from Deep Templates**\n\n[https://www.arxiv.org/abs/1703.00832](https://www.arxiv.org/abs/1703.00832)\n\n**Deep Learning-Guided Image Reconstruction from Incomplete Data**\n\n[https://arxiv.org/abs/1709.00584](https://arxiv.org/abs/1709.00584)\n\n**Image Inpainting using Multi-Scale Feature Image Translation**\n\n[https://arxiv.org/abs/1711.08590](https://arxiv.org/abs/1711.08590)\n\n**Image Inpainting for High-Resolution Textures using CNN Texture Synthesis**\n\n[https://arxiv.org/abs/1712.03111](https://arxiv.org/abs/1712.03111)\n\n**Context-Aware Semantic Inpainting**\n\n[https://arxiv.org/abs/1712.07778](https://arxiv.org/abs/1712.07778)\n\n**Deep Blind Image Inpainting**\n\n[https://arxiv.org/abs/1712.09078](https://arxiv.org/abs/1712.09078)\n\n**Deep Stacked Networks with Residual Polishing for Image Inpainting**\n\n[https://arxiv.org/abs/1801.00289](https://arxiv.org/abs/1801.00289)\n\n**Light-weight pixel context encoders for image inpainting**\n\n[https://arxiv.org/abs/1801.05585](https://arxiv.org/abs/1801.05585)\n\n**Deep Structured Energy-Based Image Inpainting**\n\n[https://arxiv.org/abs/1801.07939](https://arxiv.org/abs/1801.07939)\n\n**Shift-Net: Image Inpainting via Deep Feature Rearrangement**\n\n[https://arxiv.org/abs/1801.09392](https://arxiv.org/abs/1801.09392)\n\n**Cascade context encoder for improved inpainting**\n\n[https://arxiv.org/abs/1803.04033](https://arxiv.org/abs/1803.04033)\n\n**SPG-Net: Segmentation Prediction and Guidance Network for Image Inpainting**\n\n- intro: University of Southern California & Baidu Research\n- arxiv: [https://arxiv.org/abs/1805.03356](https://arxiv.org/abs/1805.03356)\n\n**Free-Form Image Inpainting with Gated Convolution**\n\n[https://arxiv.org/abs/1806.03589](https://arxiv.org/abs/1806.03589)\n\n**Keras implementation of Image OutPainting**\n\n- intro: Stanford CS230 project\n- paper: [https://cs230.stanford.edu/projects_spring_2018/posters/8265861.pdf](https://cs230.stanford.edu/projects_spring_2018/posters/8265861.pdf)\n- github: [https://github.com/bendangnuksung/Image-OutPainting](https://github.com/bendangnuksung/Image-OutPainting)\n\n**Image Inpainting via Generative Multi-column Convolutional Neural Networks**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.08771](https://arxiv.org/abs/1810.08771)\n\n**Deep Inception Generative Network for Cognitive Image Inpainting**\n\n[https://arxiv.org/abs/1812.01458](https://arxiv.org/abs/1812.01458)\n\n**Foreground-aware Image Inpainting**\n\n- intro: University of Rochester & University of Illinois at Urbana-Champaign & Adobe Research\n- arxiv: [https://arxiv.org/abs/1901.05945](https://arxiv.org/abs/1901.05945)\n\n# Image Restoration\n\n**Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections**\n\n- intro: NIPS 2016\n- arxiv: [http://arxiv.org/abs/1603.09056](http://arxiv.org/abs/1603.09056)\n\n**Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections**\n\n- arxiv: [http://arxiv.org/abs/1606.08921](http://arxiv.org/abs/1606.08921)\n\n**Image Completion with Deep Learning in TensorFlow**\n\n- blog: [http://bamos.github.io/2016/08/09/deep-completion/](http://bamos.github.io/2016/08/09/deep-completion/)\n\n**Deeply Aggregated Alternating Minimization for Image Restoration**\n\n- arxiv: [https://arxiv.org/abs/1612.06508](https://arxiv.org/abs/1612.06508)\n\n**A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction**\n\n- intro: Seoul National University\n- arxiv: [https://arxiv.org/abs/1701.06190](https://arxiv.org/abs/1701.06190)\n\n**MemNet: A Persistent Memory Network for Image Restoration**\n\n- intro: ICCV 2017 (Spotlight presentation)\n- arxiv: [https://arxiv.org/abs/1708.02209](https://arxiv.org/abs/1708.02209)\n- github: [https://github.com/tyshiwo/MemNet](https://github.com/tyshiwo/MemNet)\n\n**Deep Mean-Shift Priors for Image Restoration**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1709.03749](https://arxiv.org/abs/1709.03749)\n\n**xUnit: Learning a Spatial Activation Function for Efficient Image Restoration**\n\n- arxiv: [https://arxiv.org/abs/1711.06445](https://arxiv.org/abs/1711.06445)\n- github: [https://github.com/kligvasser/xUnit](https://github.com/kligvasser/xUnit)\n\n**Deep Image Prior**\n\n- intro: Skolkovo Institute of Science and Technology & University of Oxford\n- project page: [https://dmitryulyanov.github.io/deep_image_prior](https://dmitryulyanov.github.io/deep_image_prior)\n- arxiv: [https://arxiv.org/abs/1711.10925](https://arxiv.org/abs/1711.10925)\n- paper: [https://sites.skoltech.ru/app/data/uploads/sites/25/2017/11/deep_image_prior.pdf](https://sites.skoltech.ru/app/data/uploads/sites/25/2017/11/deep_image_prior.pdf)\n- github: [https://github.com//DmitryUlyanov/deep-image-prior](https://github.com//DmitryUlyanov/deep-image-prior)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/7gls3j/r_deep_image_prior_deep_superresolution/](https://www.reddit.com/r/MachineLearning/comments/7gls3j/r_deep_image_prior_deep_superresolution/)\n\n**MemNet: A Persistent Memory Network for Image Restoration**\n\n- intro: ICCV 2017 spotlight\n- paper: [http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf](http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf)\n- github: [https://github.com//tyshiwo/MemNet](https://github.com//tyshiwo/MemNet)\n\n**Denoising Prior Driven Deep Neural Network for Image Restoration**\n\n[https://arxiv.org/abs/1801.06756](https://arxiv.org/abs/1801.06756)\n\n**Globally and Locally Consistent Image Completion**\n\n- intro: SIGGRAPH 2017\n- project page: [http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/](http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/)\n- paper: [http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf](http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf)\n- github(official): [https://github.com/satoshiiizuka/siggraph2017_inpainting](https://github.com/satoshiiizuka/siggraph2017_inpainting)\n- github: [https://github.com/akmtn/pytorch-siggraph2017-inpainting](https://github.com/akmtn/pytorch-siggraph2017-inpainting)\n\n**Multi-level Wavelet-CNN for Image Restoration**\n\n- intro: CVPR 2018 NTIRE Workshop\n- arxiv: [https://arxiv.org/abs/1805.07071](https://arxiv.org/abs/1805.07071)\n\n**Non-Local Recurrent Network for Image Restoration**\n\n- intro: University of Illinois at Urbana-Champaign & The Chinese University of Hong Kong\n- arxiv: [https://arxiv.org/abs/1806.02919](https://arxiv.org/abs/1806.02919)\n\n**Residual Non-local Attention Networks for Image Restoration**\n\n- intro: ICLR 2019\n- arxiv: [https://arxiv.org/abs/1903.10082](https://arxiv.org/abs/1903.10082)\n\n## Face Completion\n\n**Generative Face Completion**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.05838](https://arxiv.org/abs/1704.05838)\n\n**High Resolution Face Completion with Multiple Controllable Attributes via Fully End-to-End Progressive Generative Adversarial Networks**\n\n- intro: North Carolina State University\n- arxiv: [https://arxiv.org/abs/1801.07632](https://arxiv.org/abs/1801.07632)\n\n# Image Denoising\n\n**Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising**\n\n- arxiv: [http://arxiv.org/abs/1608.03981](http://arxiv.org/abs/1608.03981)\n- github: [https://github.com/cszn/DnCNN](https://github.com/cszn/DnCNN)\n\n**Medical image denoising using convolutional denoising autoencoders**\n\n- arxiv: [http://arxiv.org/abs/1608.04667](http://arxiv.org/abs/1608.04667)\n\n**Rectifier Neural Network with a Dual-Pathway Architecture for Image Denoising**\n\n- arxiv: [http://arxiv.org/abs/1609.03024](http://arxiv.org/abs/1609.03024)\n\n**Non-Local Color Image Denoising with Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.06757](https://arxiv.org/abs/1611.06757)\n\n**Joint Visual Denoising and Classification using Deep Learning**\n\n- intro: ICIP 2016\n- arxiv: [https://arxiv.org/abs/1612.01075](https://arxiv.org/abs/1612.01075)\n- github: [https://github.com/ganggit/jointmodel](https://github.com/ganggit/jointmodel)\n\n**Deep Convolutional Denoising of Low-Light Images**\n\n- arxiv: [https://arxiv.org/abs/1701.01687](https://arxiv.org/abs/1701.01687)\n\n**Deep Class Aware Denoising**\n\n- arxiv: [https://arxiv.org/abs/1701.01698](https://arxiv.org/abs/1701.01698)\n\n**End-to-End Learning for Structured Prediction Energy Networks**\n\n- intro: University of Massachusetts & CMU\n- arxiv: [https://arxiv.org/abs/1703.05667](https://arxiv.org/abs/1703.05667)\n\n**Block-Matching Convolutional Neural Network for Image Denoising**\n\n[https://arxiv.org/abs/1704.00524](https://arxiv.org/abs/1704.00524)\n\n**When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach**\n\n[https://arxiv.org/abs/1706.04284](https://arxiv.org/abs/1706.04284)\n\n**Wide Inference Network for Image Denoising**\n\n[https://arxiv.org/abs/1707.05414](hmttps://arxiv.org/abs/1707.05414)\n\n**Learning Pixel-Distribution Prior with Wider Convolution for Image Denoising**\n\n- arxiv: [https://arxiv.org/abs/1707.09135](https://arxiv.org/abs/1707.09135)\n- github(MatConvNet): [https://github.com/cswin/WIN](https://github.com/cswin/WIN)\n\n**Image Denoising via CNNs: An Adversarial Approach**\n\n- intro: Indian Institute of Science\n- arxiv: [https://arxiv.org/abs/1708.00159](https://arxiv.org/abs/1708.00159)\n\n**An ELU Network with Total Variation for Image Denoising**\n\n- intro: 24th International Conference on Neural Information Processing (2017)\n- arxiv: [https://arxiv.org/abs/1708.04317](https://arxiv.org/abs/1708.04317)\n\n**Dilated Residual Network for Image Denoising**\n\n[https://arxiv.org/abs/1708.05473](https://arxiv.org/abs/1708.05473)\n\n**FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising**\n\n- arxiv: [https://arxiv.org/abs/1710.04026](https://arxiv.org/abs/1710.04026)\n- github(MatConvNet): [https://github.com/cszn/FFDNet](https://github.com/cszn/FFDNet)\n\n**Universal Denoising Networks : A Novel CNN-based Network Architecture for Image Denoising**\n\n[https://arxiv.org/abs/1711.07807](https://arxiv.org/abs/1711.07807)\n\n**Burst Denoising with Kernel Prediction Networks**\n\n- project page: [http://people.eecs.berkeley.edu/~bmild/kpn/](http://people.eecs.berkeley.edu/~bmild/kpn/)\n- arxiv: [https://arxiv.org/abs/1712.02327](https://arxiv.org/abs/1712.02327)\n\n**Chaining Identity Mapping Modules for Image Denoising**\n\n[https://arxiv.org/abs/1712.02933](https://arxiv.org/abs/1712.02933)\n\n**Deep Burst Denoising**\n\n[https://arxiv.org/abs/1712.05790](https://arxiv.org/abs/1712.05790)\n\n**Fast, Trainable, Multiscale Denoising**\n\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/1802.06130](https://arxiv.org/abs/1802.06130)\n\n**Training Deep Learning based Denoisers without Ground Truth Data**\n\n[https://arxiv.org/abs/1803.01314](https://arxiv.org/abs/1803.01314)\n\n**Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising**\n\n[https://arxiv.org/abs/1806.05229](https://arxiv.org/abs/1806.05229)\n\n**Class-Aware Fully-Convolutional Gaussian and Poisson Denoising**\n\n[https://arxiv.org/abs/1808.06562](https://arxiv.org/abs/1808.06562)\n\n**Connecting Image Denoising and High-Level Vision Tasks via Deep Learning**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1809.01826](https://arxiv.org/abs/1809.01826)\n- github: [https://github.com/Ding-Liu/DeepDenoising](https://github.com/Ding-Liu/DeepDenoising)\n\n**DN-ResNet: Efficient Deep Residual Network for Image Denoising**\n\n[https://arxiv.org/abs/1810.06766](https://arxiv.org/abs/1810.06766)\n\n**Deep Learning for Image Denoising: A Survey**\n\n[https://arxiv.org/abs/1810.05052](https://arxiv.org/abs/1810.05052)\n\n# Image Dehazing / Image Haze Removal\n\n**DehazeNet: An End-to-End System for Single Image Haze Removal**\n\n- arxiv: [http://arxiv.org/abs/1601.07661](http://arxiv.org/abs/1601.07661)\n\n**An All-in-One Network for Dehazing and Beyond**\n\n- intro: All-in-One Dehazing Network (AOD-Net)\n- arxiv: [https://arxiv.org/abs/1707.06543](https://arxiv.org/abs/1707.06543)\n\n**Joint Transmission Map Estimation and Dehazing using Deep Networks**\n\n[https://arxiv.org/abs/1708.00581](https://arxiv.org/abs/1708.00581)\n\n**End-to-End United Video Dehazing and Detection**\n\n[https://arxiv.org/abs/1709.03919](https://arxiv.org/abs/1709.03919)\n\n**Image Dehazing using Bilinear Composition Loss Function**\n\n[https://arxiv.org/abs/1710.00279](https://arxiv.org/abs/1710.00279)\n\n**Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond**\n\n[https://arxiv.org/abs/1711.06787](https://arxiv.org/abs/1711.06787)\n\n**CANDY: Conditional Adversarial Networks based Fully End-to-End System for Single Image Haze Removal**\n\n[https://arxiv.org/abs/1801.02892](https://arxiv.org/abs/1801.02892)\n\n**C2MSNet: A Novel approach for single image haze removal**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.08406](https://arxiv.org/abs/1801.08406)\n\n**A Cascaded Convolutional Neural Network for Single Image Dehazing**\n\n- intro: IEEE ACCESS\n- arxiv: [https://arxiv.org/abs/1803.07955](https://arxiv.org/abs/1803.07955)\n\n**Densely Connected Pyramid Dehazing Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.08396](https://arxiv.org/abs/1803.08396)\n- github: [https://github.com/hezhangsprinter/DCPDN](https://github.com/hezhangsprinter/DCPDN)\n\n**Gated Fusion Network for Single Image Dehazing**\n\n- project page: [https://sites.google.com/site/renwenqi888/research/dehazing/gfn](https://sites.google.com/site/renwenqi888/research/dehazing/gfn)\n- arxiv: [https://arxiv.org/abs/1804.00213](https://arxiv.org/abs/1804.00213)\n\n**Semantic Single-Image Dehazing**\n\n[https://arxiv.org/abs/1804.05624](https://arxiv.org/abs/1804.05624)\n\n**Perceptually Optimized Generative Adversarial Network for Single Image Dehazing**\n\n[https://arxiv.org/abs/1805.01084](https://arxiv.org/abs/1805.01084)\n\n**PAD-Net: A Perception-Aided Single Image Dehazing Network**\n\n- arxiv: [https://arxiv.org/abs/1805.03146](https://arxiv.org/abs/1805.03146)\n- github: [https://github.com/guanlongzhao/single-image-dehazing](https://github.com/guanlongzhao/single-image-dehazing)\n\n**The Effectiveness of Instance Normalization: a Strong Baseline for Single Image Dehazing**\n\n[https://arxiv.org/abs/1805.03305](https://arxiv.org/abs/1805.03305)\n\n**Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing**\n\n- intro: CVPRW: NTIRE 2018\n- arxiv: [https://arxiv.org/abs/1805.05308](https://arxiv.org/abs/1805.05308)\n\n**Deep learning for dehazing: Comparison and analysis**\n\n- intro: CVCS 2018\n- arxiv: [https://arxiv.org/abs/1806.10923](https://arxiv.org/abs/1806.10923)\n\n**Generic Model-Agnostic Convolutional Neural Network for Single Image Dehazing**\n\n[https://arxiv.org/abs/1810.02862](https://arxiv.org/abs/1810.02862)\n\n# Image Rain Removal / De-raining\n\n**Clearing the Skies: A deep network architecture for single-image rain removal**\n\n- intro: DerainNet\n- project page: [http://smartdsp.xmu.edu.cn/derainNet.html](http://smartdsp.xmu.edu.cn/derainNet.html)\n- arxiv: [http://arxiv.org/abs/1609.02087](http://arxiv.org/abs/1609.02087)\n- code(Matlab): [http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/derainNet/code.zip](http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/derainNet/code.zip)\n\n**Joint Rain Detection and Removal via Iterative Region Dependent Multi-Task Learning**\n\n- arxiv: [http://arxiv.org/abs/1609.07769](http://arxiv.org/abs/1609.07769)\n\n**Image De-raining Using a Conditional Generative Adversarial Network**\n\n- arxiv: [https://arxiv.org/abs/1701.05957](https://arxiv.org/abs/1701.05957)\n\n**Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1712.06830](https://arxiv.org/abs/1712.06830)\n\n**Deep joint rain and haze removal from single images**\n\n[https://arxiv.org/abs/1801.06769](https://arxiv.org/abs/1801.06769)\n\n**Density-aware Single Image De-raining using a Multi-stream Dense Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.08396](https://arxiv.org/abs/1803.08396)\n- github: [https://github.com/hezhangsprinter/DID-MDN](https://github.com/hezhangsprinter/DID-MDN)\n\n**Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework**\n\n[https://arxiv.org/abs/1803.10433](https://arxiv.org/abs/1803.10433)\n\n**Fast Single Image Rain Removal via a Deep Decomposition-Composition Network**\n\n[https://arxiv.org/abs/1804.02688](https://arxiv.org/abs/1804.02688)\n\n**Residual-Guide Feature Fusion Network for Single Image Deraining**\n\n[https://arxiv.org/abs/1804.07493](https://arxiv.org/abs/1804.07493)\n\n**Lightweight Pyramid Networks for Image Deraining**\n\n[https://arxiv.org/abs/1805.06173](https://arxiv.org/abs/1805.06173)\n\n**Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.05698](https://arxiv.org/abs/1807.05698)\n- code: [https://xialipku.github.io/RESCAN/](https://xialipku.github.io/RESCAN/)\n\n**Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining**\n\n- intro: ACM Multimedia 2018\n- arxiv: [https://arxiv.org/abs/1808.01491](https://arxiv.org/abs/1808.01491)\n\n**Gated Context Aggregation Network for Image Dehazing and Deraining**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1811.08747](https://arxiv.org/abs/1811.08747)\n\n**A Deep Tree-Structured Fusion Model for Single Image Deraining**\n\n[https://arxiv.org/abs/1811.08632](https://arxiv.org/abs/1811.08632)\n\n**A^2Net: Adjacent Aggregation Networks for Image Raindrop Removal**\n\n[https://arxiv.org/abs/1811.09780](https://arxiv.org/abs/1811.09780)\n\n**Single Image Deraining: A Comprehensive Benchmark Analysis**\n\n- arxiv: [https://arxiv.org/abs/1903.08558](https://arxiv.org/abs/1903.08558)\n- github: [https://github.com/lsy17096535/Single-Image-Deraining](https://github.com/lsy17096535/Single-Image-Deraining)\n\n**Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset**\n\n- intro: CVPR 2019\n- project pge: [https://stevewongv.github.io/derain-project.html](https://stevewongv.github.io/derain-project.html)\n- arxiv: [https://arxiv.org/abs/1904.01538](https://arxiv.org/abs/1904.01538)\n\n# Fence Removal\n\n**My camera can see through fences: A deep learning approach for image de-fencing**\n\n- intro: ACPR 2015\n- arxiv: [https://arxiv.org/abs/1805.07442](https://arxiv.org/abs/1805.07442)\n\n**Deep learning based fence segmentation and removal from an image using a video sequence**\n\n- intro: ECCV Workshop on Video Segmentation, 2016\n- arxiv: [http://arxiv.org/abs/1609.07727](http://arxiv.org/abs/1609.07727)\n\n**Accurate and efficient video de-fencing using convolutional neural networks and temporal information**\n\n[https://arxiv.org/abs/1806.10781](https://arxiv.org/abs/1806.10781)\n\n# Snow Removal\n\n**DesnowNet: Context-Aware Deep Network for Snow Removal**\n\n[https://arxiv.org/abs/1708.04512](https://arxiv.org/abs/1708.04512)\n\n# Blur Detection and Removal\n\n**Learning to Deblur**\n\n- arxiv: [http://arxiv.org/abs/1406.7444](http://arxiv.org/abs/1406.7444)\n\n**Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal**\n\n- arxiv: [http://arxiv.org/abs/1503.00593](http://arxiv.org/abs/1503.00593)\n\n**End-to-End Learning for Image Burst Deblurring**\n\n- arxiv: [http://arxiv.org/abs/1607.04433](http://arxiv.org/abs/1607.04433)\n\n**Deep Video Deblurring**\n\n- intro: CVPR 2017 spotlight paper\n- project page(code+dataset): [http://www.cs.ubc.ca/labs/imager/tr/2017/DeepVideoDeblurring/](http://www.cs.ubc.ca/labs/imager/tr/2017/DeepVideoDeblurring/)\n- arxiv: [https://arxiv.org/abs/1611.08387](https://arxiv.org/abs/1611.08387)\n[https://github.com/shuochsu/DeepVideoDeblurring](https://github.com/shuochsu/DeepVideoDeblurring)\n\n**Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring**\n\n- arxiv: [https://arxiv.org/abs/1612.02177](https://arxiv.org/abs/1612.02177)\n- github(official. Torch)): [https://github.com/SeungjunNah/DeepDeblur_release](https://github.com/SeungjunNah/DeepDeblur_release)\n\n**From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur**\n\n- arxiv: [https://arxiv.org/abs/1612.02583](https://arxiv.org/abs/1612.02583)\n\n**Motion Deblurring in the Wild**\n\n- arxiv: [https://arxiv.org/abs/1701.01486](https://arxiv.org/abs/1701.01486)\n\n**Deep Face Deblurring**\n\n[https://arxiv.org/abs/1704.08772](https://arxiv.org/abs/1704.08772)\n\n**Learning Blind Motion Deblurring**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.04208](https://arxiv.org/abs/1708.04208)\n\n**Deep Generative Filter for Motion Deblurring**\n\n[https://arxiv.org/abs/1709.03481](https://arxiv.org/abs/1709.03481)\n\n**DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks**\n\n- intro: Ukrainian Catholic University & CTU in Prague\n- arxiv: [https://arxiv.org/abs/1711.07064](https://arxiv.org/abs/1711.07064)\n- github: [https://github.com/KupynOrest/DeblurGAN](https://github.com/KupynOrest/DeblurGAN)\n\n**DeepDeblur: Fast one-step blurry face images restoration**\n\n[https://arxiv.org/abs/1711.09515](https://arxiv.org/abs/1711.09515)\n\n**Reblur2Deblur: Deblurring Videos via Self-Supervised Learning**\n\n- arxiv: [https://arxiv.org/abs/1801.05117](https://arxiv.org/abs/1801.05117)\n- supplementary: [https://drive.google.com/file/d/17Itta-z89lpWUdvUjpafKzJRSLoxHF5c/view](https://drive.google.com/file/d/17Itta-z89lpWUdvUjpafKzJRSLoxHF5c/view)\n\n**Scale-recurrent Network for Deep Image Deblurring**\n\n- intro: CUHK & Tecent & Megvii Inc.\n- arxiv: [https://arxiv.org/abs/1802.01770](https://arxiv.org/abs/1802.01770)\n\n**Deep Semantic Face Deblurring**\n\n- intro: CVPR 2018. Beijing Institute of Technology & University of California, Merced & Nvidia Research\n- project page: [https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur](https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur)\n- arxiv: [https://arxiv.org/abs/1803.03345](https://arxiv.org/abs/1803.03345)\n\n**Motion deblurring of faces**\n\n[https://arxiv.org/abs/1803.03330](https://arxiv.org/abs/1803.03330)\n\n**Learning a Discriminative Prior for Blind Image Deblurring**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.03363](https://arxiv.org/abs/1803.03363)\n\n**Adversarial Spatio-Temporal Learning for Video Deblurring**\n\n[https://arxiv.org/abs/1804.00533](https://arxiv.org/abs/1804.00533)\n\n**Learning to Deblur Images with Exemplars**\n\n- intro: PAMI 2018\n- arxiv: [https://arxiv.org/abs/1805.05503](https://arxiv.org/abs/1805.05503)\n\n**Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networks for Non-Uniform Single Image Deblurring**\n\n[https://arxiv.org/abs/1903.10157](https://arxiv.org/abs/1903.10157)\n\n# Image Compression\n\n**An image compression and encryption scheme based on deep learning**\n\n- arxiv: [http://arxiv.org/abs/1608.05001](http://arxiv.org/abs/1608.05001)\n\n**Full Resolution Image Compression with Recurrent Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1608.05148](http://arxiv.org/abs/1608.05148)\n- github: [https://github.com/tensorflow/models/tree/master/compression](https://github.com/tensorflow/models/tree/master/compression)\n\n**Image Compression with Neural Networks**\n\n- blog: [https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html](https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html)\n\n**Lossy Image Compression With Compressive Autoencoders**\n\n- paper: [http://openreview.net/pdf?id=rJiNwv9gg](http://openreview.net/pdf?id=rJiNwv9gg)\n- review: [http://qz.com/835569/twitter-is-getting-close-to-making-all-your-pictures-just-a-little-bit-smaller/](http://qz.com/835569/twitter-is-getting-close-to-making-all-your-pictures-just-a-little-bit-smaller/)\n\n**End-to-end Optimized Image Compression**\n\n- arxiv: [https://arxiv.org/abs/1611.01704](https://arxiv.org/abs/1611.01704)\n- notes: [https://blog.acolyer.org/2017/05/08/end-to-end-optimized-image-compression/](https://blog.acolyer.org/2017/05/08/end-to-end-optimized-image-compression/)\n\n**CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression**\n\n- arxiv: [https://arxiv.org/abs/1611.07233](https://arxiv.org/abs/1611.07233)\n\n**Semantic Perceptual Image Compression using Deep Convolution Networks**\n\n- intro: Accepted to Data Compression Conference\n- intro: Semantic JPEG image compression using deep convolutional neural network (CNN)\n- arxiv: [https://arxiv.org/abs/1612.08712](https://arxiv.org/abs/1612.08712)\n- github: [https://github.com/iamaaditya/image-compression-cnn](https://github.com/iamaaditya/image-compression-cnn)\n\n**Generative Compression**\n\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1703.01467](https://arxiv.org/abs/1703.01467)\n\n**Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks**\n\n[https://arxiv.org/abs/1703.10114](https://arxiv.org/abs/1703.10114)\n\n**Learning Convolutional Networks for Content-weighted Image Compression**\n\n[https://arxiv.org/abs/1703.10553](https://arxiv.org/abs/1703.10553)\n\n**Real-Time Adaptive Image Compression**\n\n- intro: ICML 2017\n- keywords: GAN\n- project page: [http://www.wave.one/icml2017](http://www.wave.one/icml2017)\n- arxiv: [https://arxiv.org/abs/1705.05823](https://arxiv.org/abs/1705.05823)\n\n**Learning to Inpaint for Image Compression**\n\n[https://arxiv.org/abs/1709.08855](https://arxiv.org/abs/1709.08855)\n\n**Efficient Trimmed Convolutional Arithmetic Encoding for Lossless Image Compression**\n\n[https://arxiv.org/abs/1801.04662](https://arxiv.org/abs/1801.04662)\n\n**Conditional Probability Models for Deep Image Compression**\n\n[https://arxiv.org/abs/1801.04260](https://arxiv.org/abs/1801.04260)\n\n**Multiple Description Convolutional Neural Networks for Image Compression**\n\n[https://arxiv.org/abs/1801.06611](https://arxiv.org/abs/1801.06611)\n\n**Near-lossless L-infinity constrained Multi-rate Image Decompression via Deep Neural Network**\n\n[https://arxiv.org/abs/1801.07987](https://arxiv.org/abs/1801.07987)\n\n**DeepSIC: Deep Semantic Image Compression**\n\n[https://arxiv.org/abs/1801.09468](https://arxiv.org/abs/1801.09468)\n\n**Spatially adaptive image compression using a tiled deep network**\n\n- intro: ICIP 2017\n- arxiv: [https://arxiv.org/abs/1802.02629](https://arxiv.org/abs/1802.02629)\n\n**Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1803.05787](https://arxiv.org/abs/1803.05787)\n\n**DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression Framework**\n\n- intro: DAC 2018\n- arxiv: [https://arxiv.org/abs/1803.05788](https://arxiv.org/abs/1803.05788)\n\n**The Effects of JPEG and JPEG2000 Compression on Attacks using Adversarial Examples**\n\n[https://arxiv.org/abs/1803.10418](https://arxiv.org/abs/1803.10418)\n\n**Generative Adversarial Networks for Extreme Learned Image Compression**\n\n- intro: ETH Zurich\n- homepage: [https://data.vision.ee.ethz.ch/aeirikur/extremecompression/](https://data.vision.ee.ethz.ch/aeirikur/extremecompression/)\n- arxiv: [https://arxiv.org/abs/1804.02958](https://arxiv.org/abs/1804.02958)\n\n**Deformation Aware Image Compression**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.04593](https://arxiv.org/abs/1804.04593)\n\n**Neural Multi-scale Image Compression**\n\n[https://arxiv.org/abs/1805.06386](https://arxiv.org/abs/1805.06386)\n\n**Deep Image Compression via End-to-End Learning**\n\n[https://arxiv.org/abs/1806.01496](https://arxiv.org/abs/1806.01496)\n\n# Image Quality Assessment\n\n**Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment**\n\n- arxiv: [https://arxiv.org/abs/1612.01697](https://arxiv.org/abs/1612.01697)\n\n# Image Blending\n\n**GP-GAN: Towards Realistic High-Resolution Image Blending**\n\n- project page: [https://wuhuikai.github.io/GP-GAN-Project/](https://wuhuikai.github.io/GP-GAN-Project/)\n- arxiv: [https://arxiv.org/abs/1703.07195](https://arxiv.org/abs/1703.07195)\n- github(Official, Chainer): [https://github.com/wuhuikai/GP-GAN](https://github.com/wuhuikai/GP-GAN)\n\n# Image Enhancement\n\n**Deep Bilateral Learning for Real-Time Image Enhancement**\n\n- intro: MIT & Google Research\n- arxiv: [https://arxiv.org/abs/1707.02880](https://arxiv.org/abs/1707.02880)\n\n**Aesthetic-Driven Image Enhancement by Adversarial Learning**\n\n- intro: CUHK\n- arxiv: [https://arxiv.org/abs/1707.05251](https://arxiv.org/abs/1707.05251)\n\n**Learned Perceptual Image Enhancement**\n\n[https://arxiv.org/abs/1712.02864](https://arxiv.org/abs/1712.02864)\n\n**Deep Underwater Image Enhancement**\n\n[https://arxiv.org/abs/1807.03528](https://arxiv.org/abs/1807.03528)\n\n# Abnormality Detection / Anomaly Detection\n\n**Toward a Taxonomy and Computational Models of Abnormalities in Images**\n\n- arxiv: [http://arxiv.org/abs/1512.01325](http://arxiv.org/abs/1512.01325)\n\n**GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training**\n\n[https://arxiv.org/abs/1805.06725](https://arxiv.org/abs/1805.06725)\n\n# Depth Prediction / Depth Estimation\n\n**Deep Convolutional Neural Fields for Depth Estimation from a Single Image**\n\n- intro: CVPR 2015\n- arxiv: [https://arxiv.org/abs/1411.6387](https://arxiv.org/abs/1411.6387)\n\n**Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields**\n\n- intro: IEEE T. Pattern Analysis and Machine Intelligence\n- arxiv: [https://arxiv.org/abs/1502.07411](https://arxiv.org/abs/1502.07411)\n- bitbucket: [https://bitbucket.org/fayao/dcnf-fcsp](https://bitbucket.org/fayao/dcnf-fcsp)\n\n**Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1603.04992](https://arxiv.org/abs/1603.049921)\n- github: [https://github.com/Ravi-Garg/Unsupervised_Depth_Estimation](https://github.com/Ravi-Garg/Unsupervised_Depth_Estimation)\n\n**Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions**\n\n- intro: NIPS 2016\n- project pag: [http://ttic.uchicago.edu/~ayanc/mdepth/](http://ttic.uchicago.edu/~ayanc/mdepth/)\n- arxiv: [http://arxiv.org/abs/1605.07081](http://arxiv.org/abs/1605.07081)\n- github: [https://github.com/ayanc/mdepth/](https://github.com/ayanc/mdepth/)\n\n**Deeper Depth Prediction with Fully Convolutional Residual Networks**\n\n- arxiv: [https://arxiv.org/abs/1606.00373](https://arxiv.org/abs/1606.00373)\n- github: [https://github.com/iro-cp/FCRN-DepthPrediction](https://github.com/iro-cp/FCRN-DepthPrediction)\n\n**Single image depth estimation by dilated deep residual convolutional neural network and soft-weight-sum inference**\n\n[https://arxiv.org/abs/1705.00534](https://arxiv.org/abs/1705.00534)\n\n**Monocular Depth Estimation with Hierarchical Fusion of Dilated CNNs and Soft-Weighted-Sum Inference**\n\n- intro: Northwestern Polytechnical University\n- arxiv: [https://arxiv.org/abs/1708.02287](https://arxiv.org/abs/1708.02287)\n\n**Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image**\n\n- arxiv: [https://arxiv.org/abs/1709.07492](https://arxiv.org/abs/1709.07492)\n- video: [https://www.youtube.com/watch?v=vNIIT_M7x7Y](https://www.youtube.com/watch?v=vNIIT_M7x7Y)\n- github: [https://github.com/fangchangma/sparse-to-dense](https://github.com/fangchangma/sparse-to-dense)\n\n**Size-to-depth: A New Perspective for Single Image Depth Estimation**\n\n[https://arxiv.org/abs/1801.04461](https://arxiv.org/abs/1801.04461)\n\n**Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.00150](https://arxiv.org/abs/1808.00150)\n\n**Rethinking Monocular Depth Estimation with Adversarial Training**\n\n[https://arxiv.org/abs/1808.07528](https://arxiv.org/abs/1808.07528)\n\n**CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth**\n\n- intro: CVPR 2019\n- project page: [http://webdiis.unizar.es/~jmfacil/camconvs/](http://webdiis.unizar.es/~jmfacil/camconvs/)\n- arxiv: [https://arxiv.org/abs/1904.02028](https://arxiv.org/abs/1904.02028)\n\n# Texture Synthesis\n\n**Texture Synthesis Using Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1505.07376](http://arxiv.org/abs/1505.07376)\n\n**Texture Networks: Feed-forward Synthesis of Textures and Stylized Images**\n\n- intro: IMCL 2016\n- arxiv: [http://arxiv.org/abs/1603.03417](http://arxiv.org/abs/1603.03417)\n- github: [https://github.com/DmitryUlyanov/texture_nets](https://github.com/DmitryUlyanov/texture_nets)\n- notes: [https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/](https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/)\n\n**Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.04382](http://arxiv.org/abs/1604.04382)\n- github(Torch): [https://github.com/chuanli11/MGANs](https://github.com/chuanli11/MGANs)\n\n**Texture Synthesis with Spatial Generative Adversarial Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.08207](https://arxiv.org/abs/1611.08207)\n\n**Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis**\n\n- intro: Skolkovo Institute of Science and Technology & Yandex & University of Oxford\n- arxiv: [https://arxiv.org/abs/1701.02096](https://arxiv.org/abs/1701.02096)\n\n**Deep TEN: Texture Encoding Network**\n\n- intro: CVPR 2017\n- project page: [http://zhanghang1989.github.io/DeepEncoding/](http://zhanghang1989.github.io/DeepEncoding/)\n- arxiv: [https://arxiv.org/abs/1612.02844](https://arxiv.org/abs/1612.02844)\n- github: [https://github.com/zhanghang1989/Deep-Encoding](https://github.com/zhanghang1989/Deep-Encoding)\n- notes: [https://zhuanlan.zhihu.com/p/25013378](https://zhuanlan.zhihu.com/p/25013378)\n\n**Diversified Texture Synthesis with Feed-forward Networks**\n\n- intro: CVPR 2017. University of California & Adobe Research\n- arxiv: [https://arxiv.org/abs/1703.01664](https://arxiv.org/abs/1703.01664)\n- github: [https://github.com/Yijunmaverick/MultiTextureSynthesis](https://github.com/Yijunmaverick/MultiTextureSynthesis)\n\n# Image Cropping\n\n**Deep Cropping via Attention Box Prediction and Aesthetics Assessment**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1710.08014](https://arxiv.org/abs/1710.08014)\n\n**A2-RL: Aesthetics Aware Reinforcement Learning for Automatic Image Cropping**\n\n- intro: CVPR 2018\n- project page: [http://debangli.info/A2RL/](http://debangli.info/A2RL/)\n- arxiv: [https://arxiv.org/abs/1709.04595](https://arxiv.org/abs/1709.04595)\n- github(official): [https://github.com/wuhuikai/TF-A2RL](https://github.com/wuhuikai/TF-A2RL)\n- demo: [http://wuhuikai.me/TF-A2RL/](http://wuhuikai.me/TF-A2RL/)\n\n**Automatic Image Cropping for Visual Aesthetic Enhancement Using Deep Neural Networks and Cascaded Regression**\n\n- intro: IEEE Transactions on Multimedia, 2017\n- arxiv: [https://arxiv.org/abs/1712.09048](https://arxiv.org/abs/1712.09048)\n\n**Grid Anchor based Image Cropping: A New Benchmark and An Efficient Model**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1909.08989](https://arxiv.org/abs/1909.08989)\n- github: [https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch](https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch)\n\n**Image Cropping with Composition and Saliency Aware Aesthetic Score Map**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.10492](https://arxiv.org/abs/1911.10492)\n\n# Image Synthesis\n\n**Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis**\n\n- arxiv: [http://arxiv.org/abs/1601.04589](http://arxiv.org/abs/1601.04589)\n\n**Generative Adversarial Text to Image Synthesis**\n\n![](https://camo.githubusercontent.com/1925e23b5b6e19efa60f45daa3787f1f4a098ef3/687474703a2f2f692e696d6775722e636f6d2f644e6c32486b5a2e6a7067)\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1605.05396](http://arxiv.org/abs/1605.05396)\n- github(Tensorflow): [https://github.com/paarthneekhara/text-to-image](https://github.com/paarthneekhara/text-to-image)\n\n**StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks**\n\n- intro: Rutgers University & Lehigh University & The Chinese University of Hong Kong & University of North Carolina at Charlotte\n- arxiv: [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242)\n- github: [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)\n- github: [https://github.com/brangerbriz/docker-StackGAN](https://github.com/brangerbriz/docker-StackGAN)\n\n**Semantic Image Synthesis via Adversarial Learning**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.06873](https://arxiv.org/abs/1707.06873)\n- github(PyTorch): [https://github.com//woozzu/dong_iccv_2017](https://github.com//woozzu/dong_iccv_2017)\n\n**An Introduction to Image Synthesis with Generative Adversarial Nets**\n\n- intro: University of Illinois at Chicago & Toutiao AI Lab\n- arxiv: [https://arxiv.org/abs/1803.04469](https://arxiv.org/abs/1803.04469)\n\n**Text Guided Person Image Synthesis**\n\n- intr: CVPR 2019\n- intro: Zhejiang University & Nanjing University\n- arxiv: [https://arxiv.org/abs/1904.05118](https://arxiv.org/abs/1904.05118)\n\n# Image Tagging\n\n**Fast Zero-Shot Image Tagging**\n\n![](http://crcv.ucf.edu/projects/fastzeroshot/overview.png)\n\n- project: [http://crcv.ucf.edu/projects/fastzeroshot/](http://crcv.ucf.edu/projects/fastzeroshot/)\n\n**Flexible Image Tagging with Fast0Tag**\n\n![](https://cdn-images-1.medium.com/max/800/1*SsIf1Bhe-G4HmN6DPDogmQ.png)\n\n- blog: [https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7](https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7)\n\n**Sampled Image Tagging and Retrieval Methods on User Generated Content**\n\n- arxiv: [https://arxiv.org/abs/1611.06962](https://arxiv.org/abs/1611.06962)\n- github: [https://github.com/lab41/attalos](https://github.com/lab41/attalos)\n\n**Kill Two Birds with One Stone: Weakly-Supervised Neural Network for Image Annotation and Tag Refinement**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1711.06998](https://arxiv.org/abs/1711.06998)\n\n**Deep Multiple Instance Learning for Zero-shot Image Tagging**\n\n[https://arxiv.org/abs/1803.06051](https://arxiv.org/abs/1803.06051)\n\n# Image Matching\n\n**Learning Fine-grained Image Similarity with Deep Ranking**\n\n- intro: CVPR 2014\n- intro: Triplet Sampling\n- arxiv: [http://arxiv.org/abs/1404.4661](http://arxiv.org/abs/1404.4661)\n\n**Learning to compare image patches via convolutional neural networks**\n\n- intro: CVPR 2015. siamese network\n- project page: [http://imagine.enpc.fr/~zagoruys/deepcompare.html](http://imagine.enpc.fr/~zagoruys/deepcompare.html)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf)\n- github: [https://github.com/szagoruyko/cvpr15deepcompare](https://github.com/szagoruyko/cvpr15deepcompare)\n\n**MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching**\n\n- intro: CVPR 2015. siamese network\n- paper: [http://www.cs.unc.edu/~xufeng/cs/papers/cvpr15-matchnet.pdf](http://www.cs.unc.edu/~xufeng/cs/papers/cvpr15-matchnet.pdf)\n- extended abstract: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_114_ext.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_114_ext.pdf)\n- github: [https://github.com/hanxf/matchnet](https://github.com/hanxf/matchnet)\n\n**Fashion Style in 128 Floats**\n\n![](http://hi.cs.waseda.ac.jp/~esimo/images/stylenet/fashionfeat.png)\n\n- intro: CVPR 2016. StyleNet\n- project page: [http://hi.cs.waseda.ac.jp/~esimo/en/research/stylenet/](http://hi.cs.waseda.ac.jp/~esimo/en/research/stylenet/)\n- paper: [http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf](http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf)\n- github: [https://github.com/bobbens/cvpr2016_stylenet](https://github.com/bobbens/cvpr2016_stylenet)\n\n**Fully-Trainable Deep Matching**\n\n- intro: BMVC 2016\n- project page: [http://lear.inrialpes.fr/src/deepmatching/](http://lear.inrialpes.fr/src/deepmatching/)\n- arxiv: [http://arxiv.org/abs/1609.03532](http://arxiv.org/abs/1609.03532)\n\n**Local Similarity-Aware Deep Feature Embedding**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1610.08904](https://arxiv.org/abs/1610.08904)\n\n**Convolutional neural network architecture for geometric matching**\n\n- intro: CVPR 2017. Inria\n- project page: [http://www.di.ens.fr/willow/research/cnngeometric/](http://www.di.ens.fr/willow/research/cnngeometric/)\n- arxiv: [https://arxiv.org/abs/1703.05593](https://arxiv.org/abs/1703.05593)\n- github: [https://github.com/ignacio-rocco/cnngeometric_matconvnet](https://github.com/ignacio-rocco/cnngeometric_matconvnet)\n\n**Multi-Image Semantic Matching by Mining Consistent Features**\n\n[https://arxiv.org/abs/1711.07641](https://arxiv.org/abs/1711.07641)\n\n# Image Editing\n\n**Neural Photo Editing with Introspective Adversarial Networks**\n\n![](https://camo.githubusercontent.com/c66848752d9fa05c3194ae36d48b869ec9d21743/687474703a2f2f692e696d6775722e636f6d2f773155323045492e706e67)\n\n- intro: Heriot-Watt University\n- arxiv: [http://arxiv.org/abs/1609.07093](http://arxiv.org/abs/1609.07093)\n- github: [https://github.com/ajbrock/Neural-Photo-Editor](https://github.com/ajbrock/Neural-Photo-Editor)\n\n**Deep Feature Interpolation for Image Content Changes**\n\n- intro: CVPR 2017. Cornell University & Washington University\n- arxiv: [https://arxiv.org/abs/1611.05507](https://arxiv.org/abs/1611.05507)\n- github(official): [https://github.com/paulu/deepfeatinterp](https://github.com/paulu/deepfeatinterp)\n- github: [https://github.com/slang03/dfi-tensorflow](https://github.com/slang03/dfi-tensorflow)\n\n**Invertible Conditional GANs for image editing**\n\n![](https://raw.githubusercontent.com/Guim3/IcGAN/master/images/model_overview.png)\n\n- intro: NIPS 2016 Workshop on Adversarial Training\n- arxiv: [https://arxiv.org/abs/1611.06355](https://arxiv.org/abs/1611.06355)\n- github: [https://github.com/Guim3/IcGAN](https://github.com/Guim3/IcGAN)\n\n**Semantic Facial Expression Editing using Autoencoded Flow**\n\n- intro: University of Illinois at Urbana-Champaign & The Chinese University of Hong Kong & Google\n- arxiv: [https://arxiv.org/abs/1611.09961](https://arxiv.org/abs/1611.09961)\n\n**Language-Based Image Editing with Recurrent Attentive Models**\n\n[https://arxiv.org/abs/1711.06288](https://arxiv.org/abs/1711.06288)\n\n## Face Swap & Face Editing\n\n**Fast Face-swap Using Convolutional Neural Networks**\n\n- intro: Ghent University & Twitter\n- arxiv: [https://arxiv.org/abs/1611.09577](https://arxiv.org/abs/1611.09577)\n\n**Neural Face Editing with Intrinsic Image Disentangling**\n\n- intro: CVPR 2017 oral\n- project page: [http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html](http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html)\n- arxiv: [https://arxiv.org/abs/1704.04131](https://arxiv.org/abs/1704.04131)\n\n**Arbitrary Facial Attribute Editing: Only Change What You Want**\n\n- arxiv: [https://arxiv.org/abs/1711.10678](https://arxiv.org/abs/1711.10678)\n- github: [https://github.com/LynnHo/AttGAN-Tensorflow](https://github.com/LynnHo/AttGAN-Tensorflow)\n\n**RSGAN: Face Swapping and Editing using Face and Hair Representation in Latent Spaces**\n\n[https://arxiv.org/abs/1804.03447](https://arxiv.org/abs/1804.03447)\n\n**FaceShop: Deep Sketch-based Face Image Editing**\n\n[https://arxiv.org/abs/1804.08972](https://arxiv.org/abs/1804.08972)\n\n# Stereo\n\n**End-to-End Learning of Geometry and Context for Deep Stereo Regression**\n\n[https://arxiv.org/abs/1703.04309](https://arxiv.org/abs/1703.04309)\n\n**Unsupervised Adaptation for Deep Stereo**\n\n- intro: ICCV 2017\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf)\n- paper: [http://vision.disi.unibo.it/~mpoggi/papers/iccv2017_adaptation.pdf](http://vision.disi.unibo.it/~mpoggi/papers/iccv2017_adaptation.pdf)\n- github: [https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo](https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo)\n\n**Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching**\n\n[https://arxiv.org/abs/1708.09204](https://arxiv.org/abs/1708.09204)\n\n**StereoConvNet: Stereo convolutional neural network for depth map prediction from stereo images**\n\n- github: [https://github.com/LouisFoucard/StereoConvNet](https://github.com/LouisFoucard/StereoConvNet)\n\n**EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching**\n\n[https://arxiv.org/abs/1803.05196](https://arxiv.org/abs/1803.05196)\n\n**Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains**\n\n- intro: CVPR 2018. SenseTime Research & Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/1803.06641](https://arxiv.org/abs/1803.06641)\n\n**Pyramid Stereo Matching Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.08669](https://arxiv.org/abs/1803.08669)\n- github: [https://github.com/JiaRenChang/PSMNet](https://github.com/JiaRenChang/PSMNet)\n\n**Cascaded multi-scale and multi-dimension convolutional neural network for stereo matching**\n\n[https://arxiv.org/abs/1803.09437](https://arxiv.org/abs/1803.09437)\n\n**Left-Right Comparative Recurrent Model for Stereo Matching**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.00796](https://arxiv.org/abs/1804.00796)\n\n**Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching**\n\n[https://arxiv.org/abs/1806.01677](https://arxiv.org/abs/1806.01677)\n\n**Open-World Stereo Video Matching with Deep RNN**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.03959](https://arxiv.org/abs/1808.03959)\n\n**Real-time self-adaptive deep stereo**\n\n[https://arxiv.org/abs/1810.05424](https://arxiv.org/abs/1810.05424)\n[https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo](https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo)\n\n**Group-wise Correlation Stereo Network**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.04025](https://arxiv.org/abs/1903.04025)\n- github(official): [https://github.com/xy-guo/GwcNet](https://github.com/xy-guo/GwcNet)\n\n**Self-calibrating Deep Photometric Stereo Networks**\n\n- intro: CVPR 2019 oral\n- intro: The University of Hong Kong & University of Oxford & Peking University & Peng Cheng Laboratory & Osaka University\n- keywords: Learning Based Uncalibrated Photometric Stereo for Non-Lambertian Surface\n- project page: [http://gychen.org/SDPS-Net/](http://gychen.org/SDPS-Net/)\n- arxiv: [https://arxiv.org/abs/1903.07366](https://arxiv.org/abs/1903.07366)\n- github(official, PyTorch): [https://github.com/guanyingc/SDPS-Net](https://github.com/guanyingc/SDPS-Net)\n\n**Learning to Adapt for Stereo**\n\n- intro: CVPR 2019\n- intro: University of Bologna & University of Oxford & Australian National University & FiveAI\n- arxiv: [https://arxiv.org/abs/1904.02957](https://arxiv.org/abs/1904.02957)\n- github: [https://github.com/CVLAB-Unibo/Learning2AdaptForStereo](https://github.com/CVLAB-Unibo/Learning2AdaptForStereo)\n\n**StereoDRNet: Dilated Residual Stereo Net**\n\n- intro: CVPR 2019\n- intro: University of North Carolina at Chapel Hill & Facebook Reality Labs\n- arxiv: [https://arxiv.org/abs/1904.02251](https://arxiv.org/abs/1904.02251)\n\n**GA-Net: Guided Aggregation Net for End-to-end Stereo Matching**\n\n- intro: CVPR 2019 oral\n- intro: University of Oxford & Baidu Research\n- arxiv: [https://arxiv.org/abs/1904.06587](https://arxiv.org/abs/1904.06587)\n\n**Multi-Scale Geometric Consistency Guided Multi-View Stereo**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.08103](https://arxiv.org/abs/1904.08103)\n\n**Guided Stereo Matching**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1905.10107](https://arxiv.org/abs/1905.10107)\n\n**OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.06257](https://arxiv.org/abs/1908.06257)\n\n**DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.05845](https://arxiv.org/abs/1909.05845)\n\n**Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers**\n\n- intro: Johns Hopkins University\n- keywords: STereo TRansformer (STTR)\n- arxiv: [https://arxiv.org/abs/2011.02910](https://arxiv.org/abs/2011.02910)\n- github: [https://github.com/mli0603/stereo-transformer](https://github.com/mli0603/stereo-transformer)\n\n**EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection**\n\n- intro: Tianjin University\n- arxiv: [https://arxiv.org/abs/2111.14055](https://arxiv.org/abs/2111.14055)\n\n# 3D\n\n**Learning Spatiotemporal Features with 3D Convolutional Networks**\n\n**C3D: Generic Features for Video Analysis**\n\n- project page: [http://vlg.cs.dartmouth.edu/c3d/](http://vlg.cs.dartmouth.edu/c3d/)\n- arxiv: [http://arxiv.org/abs/1412.0767](http://arxiv.org/abs/1412.0767)\n- slides: [http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w07-conv3d.pdf](http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w07-conv3d.pdf)\n- github: [https://github.com/facebook/C3D](https://github.com/facebook/C3D)\n\n**C3D Model for Keras trained over Sports 1M**\n\n- project page: [https://imatge.upc.edu/web/resources/c3d-model-keras-trained-over-sports-1m](https://imatge.upc.edu/web/resources/c3d-model-keras-trained-over-sports-1m)\n\n**Sports 1M C3D Network to Keras**\n\n- notebook: [http://nbviewer.jupyter.org/gist/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2/Sports1M%20C3D%20Network%20to%20Keras.ipynb](http://nbviewer.jupyter.org/gist/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2/Sports1M%20C3D%20Network%20to%20Keras.ipynb)\n\n**Deep End2End Voxel2Voxel Prediction**\n\n- arxiv: [http://arxiv.org/abs/1511.06681](http://arxiv.org/abs/1511.06681)\n\n**Aligning 3D Models to RGB-D Images of Cluttered Scenes**\n\n- paper: [http://www.cs.berkeley.edu/~rbg/papers/cvpr15/align2rgbd.pdf](http://www.cs.berkeley.edu/~rbg/papers/cvpr15/align2rgbd.pdf)\n\n**Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images**\n\n![](http://dss.cs.princeton.edu/teaser.jpg)\n\n- homepage: [http://dss.cs.princeton.edu/](http://dss.cs.princeton.edu/)\n- arxiv: [http://arxiv.org/abs/1511.02300](http://arxiv.org/abs/1511.02300)\n\n**Multi-view 3D Models from Single Images with a Convolutional Network**\n\n- arxiv: [http://arxiv.org/abs/1511.06702](http://arxiv.org/abs/1511.06702)\n\n**RotationNet: Learning Object Classification Using Unsupervised Viewpoint Estimation**\n\n- arxiv: [http://arxiv.org/abs/1603.06208](http://arxiv.org/abs/1603.06208)\n\n**DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding**\n\n![](http://deepcontext.cs.princeton.edu/teaser.png)\n\n- paper: [http://deepcontext.cs.princeton.edu/paper.pdf](http://deepcontext.cs.princeton.edu/paper.pdf)\n- project page: [http://deepcontext.cs.princeton.edu/](http://deepcontext.cs.princeton.edu/)\n\n**Volumetric and Multi-View CNNs for Object Classification on 3D Data**\n\n![](http://graphics.stanford.edu/projects/3dcnn/teaser.jpg)\n\n- homepage: [http://graphics.stanford.edu/projects/3dcnn/](http://graphics.stanford.edu/projects/3dcnn/)\n- arxiv: [https://arxiv.org/abs/1604.03265](https://arxiv.org/abs/1604.03265)\n- github: [https://github.com/charlesq34/3dcnn.torch](https://github.com/charlesq34/3dcnn.torch)\n\n**Deep3D: Automatic 2D-to-3D Video Conversion with CNNs**\n\n![](https://raw.githubusercontent.com/piiswrong/deep3d/master/img/teaser.png)\n\n- project page: [http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html](http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html)\n- paper: [http://homes.cs.washington.edu/~jxie/pdf/deep3d.pdf](http://homes.cs.washington.edu/~jxie/pdf/deep3d.pdf)\n- github: [https://github.com/piiswrong/deep3d](https://github.com/piiswrong/deep3d)\n\n**Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.03650](http://arxiv.org/abs/1604.03650)\n\n**3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction**\n\n![](https://raw.githubusercontent.com/chrischoy/3D-R2N2/master/imgs/overview.png)\n\n- arxiv: [http://arxiv.org/abs/1604.00449](http://arxiv.org/abs/1604.00449)\n- github: [https://github.com/chrischoy/3D-R2N2](https://github.com/chrischoy/3D-R2N2)\n\n**Body Meshes as Points**\n\n- intro: CVPR 2021\n- intro: National University of Singapore & ByteDance AI Lab & Yitu Technology\n- arxiv: [https://arxiv.org/abs/2105.02467](https://arxiv.org/abs/2105.02467)\n- github: [https://github.com/jfzhang95/BMP](https://github.com/jfzhang95/BMP)\n\n# Deep Learning for Makeup\n\n**Makeup like a superstar: Deep Localized Makeup Transfer Network**\n\n- intro: IJCAI 2016\n- arxiv: [http://arxiv.org/abs/1604.07102](http://arxiv.org/abs/1604.07102)\n\n**Makeup-Go: Blind Reversion of Portrait Edit**\n\n- intro: The Chinese University of Hong Kong & Tencent Youtu Lab\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper.pdf)\n- paper: [http://open.youtu.qq.com/content/file/iccv17_makeupgo.pdf](http://open.youtu.qq.com/content/file/iccv17_makeupgo.pdf)\n\n# Music Tagging\n\n**Automatic tagging using deep convolutional neural networks**\n\n- arxiv: [https://arxiv.org/abs/1606.00298](https://arxiv.org/abs/1606.00298)\n- github: [https://github.com/keunwoochoi/music-auto_tagging-keras](https://github.com/keunwoochoi/music-auto_tagging-keras)\n\n**Music tagging and feature extraction with MusicTaggerCRNN**\n\n[https://keras.io/applications/#music-tagging-and-feature-extraction-with-musictaggercrnn](https://keras.io/applications/#music-tagging-and-feature-extraction-with-musictaggercrnn)\n\n# Action Recognition\n\n**Single Image Action Recognition by Predicting Space-Time Saliency**\n\n[https://arxiv.org/abs/1705.04641](https://arxiv.org/abs/1705.04641)\n\n**Attentional Pooling for Action Recognition**\n\n- intro: NIPS 2017\n- project page: [https://rohitgirdhar.github.io/AttentionalPoolingAction/](https://rohitgirdhar.github.io/AttentionalPoolingAction/)\n- arxiv: [https://arxiv.org/abs/1711.01467](https://arxiv.org/abs/1711.01467)\n- github: [https://github.com/rohitgirdhar/AttentionalPoolingAction/](https://github.com/rohitgirdhar/AttentionalPoolingAction/)\n\n**Memory Attention Networks for Skeleton-based Action Recognition**\n\n- intro: IJCAI 2018\n- keywords: Temporal Attention Recalibration Module (TARM) and a Spatio-Temporal Convolution Module (STCM)\n- arixv: [https://arxiv.org/abs/1804.08254](https://arxiv.org/abs/1804.08254)\n- github: [https://github.com/memory-attention-networks](https://github.com/memory-attention-networks)\n\n**Deep Analysis of CNN-based Spatio-temporal Representations for Action Recognition**\n\n- intro: MIT-IBM Watson AI Lab & MIT\n- arxiv: [https://arxiv.org/abs/2010.11757](https://arxiv.org/abs/2010.11757)\n- github: [https://github.com/IBM/action-recognition-pytorch](https://github.com/IBM/action-recognition-pytorch)\n\n**Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition**\n\n- intro: ECCV 2020\n- paper: [https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690528.pdf](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690528.pdf)\n- github: [https://github.com/kchengiva/DecoupleGCN-DropGraph](https://github.com/kchengiva/DecoupleGCN-DropGraph)\n\n**Temporal-Relational CrossTransformers for Few-Shot Action Recognition**\n\n- intro: University of Bristol\n- arxiv: [https://arxiv.org/abs/2101.06184](https://arxiv.org/abs/2101.06184)\n- github: [https://github.com/tobyperrett/trx](https://github.com/tobyperrett/trx)\n\n# CTR Prediction\n\n**Deep CTR Prediction in Display Advertising**\n\n- intro: ACM Multimedia Conference 2016\n- arxiv: [https://arxiv.org/abs/1609.06018](https://arxiv.org/abs/1609.06018)\n\n**DeepFM: A Factorization-Machine based Neural Network for CTR Prediction**\n\n- intro: Harbin Institute of Technology & Huawei\n- arxiv: [https://arxiv.org/abs/1703.04247](https://arxiv.org/abs/1703.04247)\n\n**Deep Interest Network for Click-Through Rate Prediction**\n\n- intro: Alibaba Inc.\n- arxiv: [https://arxiv.org/abs/1706.06978](https://arxiv.org/abs/1706.06978)\n\n**Image Matters: Jointly Train Advertising CTR Model with Image Representation of Ad and User Behavior**\n\n- intro: Alibaba Inc.\n- arxiv: [https://arxiv.org/abs/1711.06505](https://arxiv.org/abs/1711.06505)\n\n# Cryptography\n\n**Learning to Protect Communications with Adversarial Neural Cryptography**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1610.06918](https://arxiv.org/abs/1610.06918)\n- github(Theano): [https://github.com/nlml/adversarial-neural-crypt](https://github.com/nlml/adversarial-neural-crypt)\n- github(TensorFlow): [https://github.com/ankeshanand/neural-cryptography-tensorflow](https://github.com/ankeshanand/neural-cryptography-tensorflow)\n\n**Adversarial Neural Cryptography in Theano**\n\n- blog: [https://nlml.github.io/neural-networks/adversarial-neural-cryptography/](https://nlml.github.io/neural-networks/adversarial-neural-cryptography/)\n\n**Embedding Watermarks into Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1701.04082](https://arxiv.org/abs/1701.04082)\n- github: [https://github.com/yu4u/dnn-watermark](https://github.com/yu4u/dnn-watermark)\n\n**Digital Watermarking for Deep Neural Networks**\n\n- intro: International Journal of Multimedia Information Retrieval\n- arxiv: [https://arxiv.org/abs/1802.02601](https://arxiv.org/abs/1802.02601)\n\n# Cyber Security\n\n**Collection of Deep Learning Cyber Security Research Papers**\n\n- blog: [https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042#.fcus2cu9m](https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042#.fcus2cu9m)\n\n# Lip Reading\n\n**LipNet: Sentence-level Lipreading**\n\n**LipNet: End-to-End Sentence-level Lipreading**\n\n- arxiv: [https://arxiv.org/abs/1611.01599](https://arxiv.org/abs/1611.01599)\n- paper: [http://openreview.net/pdf?id=BkjLkSqxg](http://openreview.net/pdf?id=BkjLkSqxg)\n- github: [https://github.com/bshillingford/LipNet](https://github.com/bshillingford/LipNet)\n\n**Lip Reading Sentences in the Wild**\n\n- intro: University of Oxford & Google DeepMind\n- arxiv: [https://arxiv.org/abs/1611.05358](https://arxiv.org/abs/1611.05358)\n- youtube: [https://www.youtube.com/watch?v=5aogzAUPilE](https://www.youtube.com/watch?v=5aogzAUPilE)\n\n**Combining Residual Networks with LSTMs for Lipreading**\n\n- arxiv: [https://arxiv.org/abs/1703.04105](https://arxiv.org/abs/1703.04105)\n\n**End-to-End Multi-View Lipreading**\n\n- intro: BMVC 2017\n- arxiv: [https://arxiv.org/abs/1709.00443](https://arxiv.org/abs/1709.00443)\n\n**LCANet: End-to-End Lipreading with Cascaded Attention-CTC**\n\n- intro: FG 2018\n- arxiv: [https://arxiv.org/abs/1803.04988](https://arxiv.org/abs/1803.04988)\n\n# Event Recognition\n\n**Better Exploiting OS-CNNs for Better Event Recognition in Images**\n\n- arxiv: [http://arxiv.org/abs/1510.03979](http://arxiv.org/abs/1510.03979)\n\n**Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images**\n\n- arxiv: [http://arxiv.org/abs/1609.00162](http://arxiv.org/abs/1609.00162)\n\n**IOD-CNN: Integrating Object Detection Networks for Event Recognition**\n\n[https://arxiv.org/abs/1703.07431](https://arxiv.org/abs/1703.07431)\n\n# Trajectory Prediction\n\n**Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving**\n\n- intro: Machine Learning for Autonomous Driving @ NeurIPS 2020\n- intro: Carnegie Mellon University & Bosch Research Pittsburgh\n- arxiv: [https://arxiv.org/abs/2011.14910](https://arxiv.org/abs/2011.14910)\n\n# Human-Object Interaction\n\n**Learning Human-Object Interactions by Graph Parsing Neural Networks**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.07962](https://arxiv.org/abs/1808.07962)\n- github: [https://github.com/SiyuanQi/gpnn](https://github.com/SiyuanQi/gpnn)\n\n**Interact as You Intend: Intention-Driven Human-Object Interaction Detection**\n\n[https://arxiv.org/abs/1808.09796](https://arxiv.org/abs/1808.09796)\n\n**iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection**\n\n- intro: BMVC 2018\n- project page: [https://gaochen315.github.io/iCAN/](https://gaochen315.github.io/iCAN/)\n- arxiv: [https://arxiv.org/abs/1808.10437](https://arxiv.org/abs/1808.10437)\n- github: [https://github.com/vt-vl-lab/iCAN](https://github.com/vt-vl-lab/iCAN)\n\n**Pose-aware Multi-level Feature Network for Human Object Interaction Detection**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.08453](https://arxiv.org/abs/1909.08453)\n\n**End-to-End Human Object Interaction Detection with HOI Transformer**\n\n- intro: CVPR 2021\n- intro: MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2103.04503](https://arxiv.org/abs/2103.04503)\n- github: [https://github.com/bbepoch/HoiTransformer](https://github.com/bbepoch/HoiTransformer)\n\n**Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction**\n\n- intro: ICCV 2021\n- intro: OPPO Research Institute & Xmotors & University of California\n- arxiv: [https://arxiv.org/abs/2110.03278](https://arxiv.org/abs/2110.03278)\n\n**Hand-Object Contact Prediction via Motion-Based Pseudo-Labeling and Guided Progressive Label Correction**\n\n- intro: BMVC 2021\n- arxiv: [https://arxiv.org/abs/2110.10174](https://arxiv.org/abs/2110.10174)\n- github: [https://github.com/takumayagi/hand_object_contact_prediction](https://github.com/takumayagi/hand_object_contact_prediction)\n\n# Deep Learning in Finance\n\n**Deep Learning in Finance**\n\n- arxiv: [http://arxiv.org/abs/1602.06561](http://arxiv.org/abs/1602.06561)\n\n**A Survey of Deep Learning Techniques Applied to Trading**\n\n- blog: [http://gregharris.info/a-survey-of-deep-learning-techniques-applied-to-trading/](http://gregharris.info/a-survey-of-deep-learning-techniques-applied-to-trading/)\n\n**Deep Learning and Long-Term Investing**\n\n- part 1: [http://www.euclidean.com/deep-learning-long-term-investing-1](http://www.euclidean.com/deep-learning-long-term-investing-1)\n- part 2: [http://www.euclidean.com/deep-learning-investing-part-2-preprocessing-data](http://www.euclidean.com/deep-learning-investing-part-2-preprocessing-data)\n\n**Deep Learning in Trading**\n\n- youtube: [https://www.youtube.com/watch?v=FoQKCeDuPiY](https://www.youtube.com/watch?v=FoQKCeDuPiY)\n- mirror: [https://pan.baidu.com/s/1sltRra9](https://pan.baidu.com/s/1sltRra9)\n\n**Research to Products: Machine & Human Intelligence in Finance**\n\n- intro: Peter Sarlin, Hanken School of Economics - Deep Learning in Finance Summit 2016 #reworkfin\n- youtube: [https://www.youtube.com/watch?v=Fd7Cc-KOVXg](https://www.youtube.com/watch?v=Fd7Cc-KOVXg)\n- mirror: [https://pan.baidu.com/s/1kVpZKur#list/path=%2F](https://pan.baidu.com/s/1kVpZKur#list/path=%2F)\n\n**eep Neural Networks for Real-time Market Predictions**\n\n- youtube: [https://www.youtube.com/watch?v=Kzz2-wAEK7A](https://www.youtube.com/watch?v=Kzz2-wAEK7A)\n\n**Deep Learning the Stock Market**\n\n- blog: [https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.z752rf43u](https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.z752rf43u)\n- github: [https://github.com/talolard/MarketVectors](https://github.com/talolard/MarketVectors)\n\n**rl_portfolio**\n\n- intro: This Repository uses Reinforcement Learning and Supervised learning to Optimize portfolio allocation.\n- github: [https://github.com/deependersingla/deep_portfolio](https://github.com/deependersingla/deep_portfolio)\n\n**Neural networks for algorithmic trading. Multivariate time series**\n\n- blog: [https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57](https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57)\n- github: [https://github.com/Rachnog/Deep-Trading/tree/master/multivariate](https://github.com/Rachnog/Deep-Trading/tree/master/multivariate)\n\n**Deep-Trading: Algorithmic trading with deep learning experiments**\n\n[https://github.com/Rachnog/Deep-Trading](https://github.com/Rachnog/Deep-Trading)\n\n**Neural networks for algorithmic trading. Multimodal and multitask deep learning**\n\n- blog: [https://becominghuman.ai/neural-networks-for-algorithmic-trading-multimodal-and-multitask-deep-learning-5498e0098caf](https://becominghuman.ai/neural-networks-for-algorithmic-trading-multimodal-and-multitask-deep-learning-5498e0098caf)\n- github: [https://github.com/Rachnog/Deep-Trading/tree/master/multimodal](https://github.com/Rachnog/Deep-Trading/tree/master/multimodal)\n\n**Deep Learning with Python in Finance - Singapore Python User Group**\n\n- youtube: [https://www.youtube.com/watch?v=xvm-M-R2fZY](https://www.youtube.com/watch?v=xvm-M-R2fZY)\n\n**A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem**\n\n- intro: Xi’an Jiaotong-Liverpool University\n- keywords: PGPortfolio: Policy Gradient Portfolio\n- arxiv: [https://arxiv.org/abs/1706.10059](https://arxiv.org/abs/1706.10059)\n- github: [https://github.com//ZhengyaoJiang/PGPortfolio](https://github.com//ZhengyaoJiang/PGPortfolio)\n\n**Stock Prediction: a method based on extraction of news features and recurrent neural networks**\n\n- intro: Peking University. The 22nd China Conference on Information Retrieval\n- arxiv: [https://arxiv.org/abs/1707.07585](https://arxiv.org/abs/1707.07585)\n\n**Multidimensional LSTM Networks to Predict Bitcoin Price**\n\n- blog: [http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price](http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price)\n- github: [https://github.com/jaungiers/Multidimensional-LSTM-BitCoin-Time-Series](https://github.com/jaungiers/Multidimensional-LSTM-BitCoin-Time-Series)\n\n**Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals**\n\n- intro: Euclidean Technologies & Amazon AI\n- arxiv: [https://arxiv.org/abs/1711.04837](https://arxiv.org/abs/1711.04837)\n\n**Findings from our Research on Applying Deep Learning to Long-Term Investing**\n\n[http://www.euclidean.com/paper-on-deep-learning-long-term-investing](http://www.euclidean.com/paper-on-deep-learning-long-term-investing)\n\n**Predicting Cryptocurrency Prices With Deep Learning**\n\n- intro: This post brings together cryptos and deep learning in a desperate attempt for Reddit popularity\n- blog: [https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/](https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/)\n\n**Deep Trading Agent**\n\n- intro: Deep Reinforcement Learning based Trading Agent for Bitcoin\n- arxiv: [https://github.com/samre12/deep-trading-agent](https://github.com/samre12/deep-trading-agent)\n\n**Financial Trading as a Game: A Deep Reinforcement Learning Approach**\n\n- intro: National Chiao Tung University\n- arxiv: [https://arxiv.org/abs/1807.02787](https://arxiv.org/abs/1807.02787)\n\n# Deep Learning in Speech\n\n**Deep Speech 2: End-to-End Speech Recognition in English and Mandarin**\n\n- intro: Baidu Research, ICML 2016\n- arxiv: [https://arxiv.org/abs/1512.02595](https://arxiv.org/abs/1512.02595)\n- github(Neon): [https://github.com/NervanaSystems/deepspeech](https://github.com/NervanaSystems/deepspeech)\n\n**End-to-end speech recognition with neon**\n\n- blog: [https://www.nervanasys.com/end-end-speech-recognition-neon/](https://www.nervanasys.com/end-end-speech-recognition-neon/)\n\n## WaveNet\n\n**WaveNet: A Generative Model for Raw Audio**\n\n- homepage: [https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)\n- paper: [https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view](https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view)\n- mirror: [https://pan.baidu.com/s/1gfmGWaJ](https://pan.baidu.com/s/1gfmGWaJ)\n- github: [https://github.com/usernaamee/keras-wavenet](https://github.com/usernaamee/keras-wavenet)\n- github: [https://github.com/ibab/tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet)\n- github: [https://github.com/monthly-hack/chainer-wavenet](https://github.com/monthly-hack/chainer-wavenet)\n- github: [https://github.com/huyouare/WaveNet-Theano](https://github.com/huyouare/WaveNet-Theano)\n- github(Keras): [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet)\n- github: [https://github.com/ritheshkumar95/WaveNet](https://github.com/ritheshkumar95/WaveNet)\n\n**A TensorFlow implementation of DeepMind's WaveNet paper for text generation.**\n\n- github: [https://github.com/Zeta36/tensorflow-tex-wavenet](https://github.com/Zeta36/tensorflow-tex-wavenet)\n\n**Fast Wavenet Generation Algorithm**\n\n- intro: An efficient Wavenet generation implementation\n- arxiv: [https://arxiv.org/abs/1611.09482](https://arxiv.org/abs/1611.09482)\n- github [https://github.com/tomlepaine/fast-wavenet](https://github.com/tomlepaine/fast-wavenet)\n\n**Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition based on DeepMind's WaveNet and tensorflow**\n\n- github: [https://github.com/buriburisuri/speech-to-text-wavenet](https://github.com/buriburisuri/speech-to-text-wavenet)\n\n**Wav2Letter: an End-to-End ConvNet-based Speech Recognition System**\n\n- arxiv: [http://arxiv.org/abs/1609.03193](http://arxiv.org/abs/1609.03193)\n\n**TristouNet: Triplet Loss for Speaker Turn Embedding**\n\n- arxiv: [https://arxiv.org/abs/1609.04301](https://arxiv.org/abs/1609.04301)\n- github: [https://github.com/hbredin/TristouNet](https://github.com/hbredin/TristouNet)\n\n**Speech Recognion and Deep Learning**\n\n- intro: Baidu Research Silicon Valley AI Lab\n- slides: [http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf](http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf)\n- mirror: [https://pan.baidu.com/s/1qYrPkPQ](https://pan.baidu.com/s/1qYrPkPQ)\n- github: [https://github.com/baidu-research/ba-dls-deepspeech](https://github.com/baidu-research/ba-dls-deepspeech)\n\n**Robust end-to-end deep audiovisual speech recognition**\n\n- intro: CMU\n- github: [https://arxiv.org/abs/1611.06986](https://arxiv.org/abs/1611.06986)\n\n**An Experimental Comparison of Deep Neural Networks for End-to-end Speech Recognition**\n\n- arxiv: [https://arxiv.org/abs/1611.07174](https://arxiv.org/abs/1611.07174)\n\n**Recurrent Deep Stacking Networks for Speech Recognition**\n\n- intro: The Ohio State University\n- arxiv: [https://arxiv.org/abs/1612.04675](https://arxiv.org/abs/1612.04675)\n\n**Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks**\n\n- intro: Universite de Montreal & CIFAR\n- arxiv: [https://arxiv.org/abs/1701.02720](https://arxiv.org/abs/1701.02720)\n\n# Deep Learning for Sound / Music\n\n## Sound\n\n**Suggesting Sounds for Images from Video Collections**\n\n- intro: ETH Zurich & 2Disney Research\n- paper: [https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20161014182443/Suggesting-Sounds-for-Images-from-Video-Collections-Paper.pdf](https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20161014182443/Suggesting-Sounds-for-Images-from-Video-Collections-Paper.pdf)\n\n**Disney AI System Associates Images with Sounds**\n\n- blog: [https://news.developer.nvidia.com/disneys-ai-system-associates-images-with-sounds/](https://news.developer.nvidia.com/disneys-ai-system-associates-images-with-sounds/)\n\n**Convolutional Recurrent Neural Networks for Bird Audio Detection**\n\n[https://arxiv.org/abs/1703.02317](https://arxiv.org/abs/1703.02317)\n\n**Visual to Sound: Generating Natural Sound for Videos in the Wild**\n\n- project page: [http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html](http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html)\n- arxiv: [https://arxiv.org/abs/1712.01393](https://arxiv.org/abs/1712.01393)\n\n## Music\n\n**Learning Features of Music from Scratch**\n\n- intro: University of Washington. MusicNet\n- project page: [http://homes.cs.washington.edu/~thickstn/musicnet.html](http://homes.cs.washington.edu/~thickstn/musicnet.html)\n- arxiv: [https://arxiv.org/abs/1611.09827](https://arxiv.org/abs/1611.09827)\n- demo: [http://homes.cs.washington.edu/~thickstn/demos.html](http://homes.cs.washington.edu/~thickstn/demos.html)\n\n**DeepBach: a Steerable Model for Bach chorales generation**\n\n- project page: [http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-generation/](http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-generation/)\n- arxiv: [https://arxiv.org/abs/1612.01010](https://arxiv.org/abs/1612.01010)\n- github: [https://github.com/SonyCSL-Paris/DeepBach](https://github.com/SonyCSL-Paris/DeepBach)\n- youtube: [https://www.youtube.com/watch?v=QiBM7-5hA6o](https://www.youtube.com/watch?v=QiBM7-5hA6o)\n\n**Deep Learning for Music**\n\n- blog: [https://amundtveit.com/2016/11/22/deep-learning-for-music/](https://amundtveit.com/2016/11/22/deep-learning-for-music/)\n\n**First International Workshop on Deep Learning and Music**\n\n[https://arxiv.org/html/1706.08675](https://arxiv.org/html/1706.08675)\n\n# Deep Learning in Medicine and Biology\n\n**Low Data Drug Discovery with One-shot Learning**\n\n- intro: MIT & Stanford University\n- arxiv: [https://arxiv.org/abs/1611.03199](https://arxiv.org/abs/1611.03199)\n- homepage: [http://deepchem.io/](http://deepchem.io/)\n- github: [https://github.com/deepchem/deepchem](https://github.com/deepchem/deepchem)\n\n**Democratizing Drug Discovery with DeepChem**\n\n- youtube: [https://www.youtube.com/watch?v=sntikyFI8s8](https://www.youtube.com/watch?v=sntikyFI8s8)\n\n**Introduction to Deep Learning in Medicine and Biology**\n\n- blog: [http://a12d.com/deep-learning-biomedicine](http://a12d.com/deep-learning-biomedicine)\n\n**Deep Learning for Alzheimer Diagnostics and Decision Support**\n\n[https://amundtveit.com/2016/11/18/deep-learning-for-alzheimer-diagnostics-and-decision-support/](https://amundtveit.com/2016/11/18/deep-learning-for-alzheimer-diagnostics-and-decision-support/)\n\n**DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning**\n\n- intro: University of Florida\n- arxiv: [https://arxiv.org/abs/1612.03211](https://arxiv.org/abs/1612.03211)\n\n**Towards biologically plausible deep learning**\n\n- intro: Yoshua\tBengio, NIPS’2016 Workshops\n- slides: [http://www.iro.umontreal.ca/~bengioy/talks/Brains+Bits-NIPS2016Workshop.pptx.pdf](http://www.iro.umontreal.ca/~bengioy/talks/Brains+Bits-NIPS2016Workshop.pptx.pdf)\n\n**Deep Learning and Its Applications to Machine Health Monitoring: A Survey**\n\n- arxiv: [https://arxiv.org/abs/1612.07640](https://arxiv.org/abs/1612.07640)\n\n**Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1701.01329](https://arxiv.org/abs/1701.01329)\n\n**Deep Learning Applications in Medical Imaging**\n\n- blog: [http://techemergence.com/deep-learning-medical-applications/](http://techemergence.com/deep-learning-medical-applications/)\n\n**Dermatologist-level classification of skin cancer with deep neural networks**\n\n- intro: Stanford University. Nature 2017\n- paper: [http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature21056.pdf](http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature21056.pdf)\n\n**Deep Learning for Health Informatics**\n\n- intro: Imperial College London\n- paper: [http://ieeexplore.ieee.org/abstract/document/7801947/](http://ieeexplore.ieee.org/abstract/document/7801947/)\n\n# Deep Learning for Fashion\n\n**Convolutional Neural Networks for Fashion Classification and Object Detection**\n\n- intro: CS231N project\n- paper: [http://cs231n.stanford.edu/reports/BLAO_KJAG_CS231N_FinalPaperFashionClassification.pdf](http://cs231n.stanford.edu/reports/BLAO_KJAG_CS231N_FinalPaperFashionClassification.pdf)\n\n**DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations**\n\n- intro: CVPR 2016\n- project page: [http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html](http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf)\n\n**Deep Learning for Fast and Accurate Fashion Item Detection**\n\n- keywords:  MultiBox and Fast R-CNN, Kuznech-Fashion-156 and Kuznech-Fashion-205 fashion item detection datasets\n- paper: [https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf](https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf)\n\n**Deep Learning at GILT**\n\n- keywords: automated tagging, automatic dress faceting\n- blog: [http://tech.gilt.com/machine/learning,/deep/learning/2016/12/22/deep-learning-at-gilt](http://tech.gilt.com/machine/learning,/deep/learning/2016/12/22/deep-learning-at-gilt)\n\n**Working with Fashion Models**\n\n- blog: [https://making.lyst.com/2017/02/21/working-with-fashion-models/](https://making.lyst.com/2017/02/21/working-with-fashion-models/)\n- youtube: [https://www.youtube.com/watch?v=emr2qaCQOQs](https://www.youtube.com/watch?v=emr2qaCQOQs)\n\n**Fashion Forward: Forecasting Visual Style in Fashion**\n\n- intro: Karlsruhe Institute of Technology & The University of Texas at Austin\n- arxiv: [https://arxiv.org/abs/1705.06394](https://arxiv.org/abs/1705.06394)\n\n**StreetStyle: Exploring world-wide clothing styles from millions of photos**\n\n- homepage: [http://streetstyle.cs.cornell.edu/](http://streetstyle.cs.cornell.edu/)\n- arxiv: [https://arxiv.org/abs/1706.01869](https://arxiv.org/abs/1706.01869)\n- demo: [http://streetstyle.cs.cornell.edu/trends.html](http://streetstyle.cs.cornell.edu/trends.html)\n\n**Fashioning with Networks: Neural Style Transfer to Design Clothes**\n\n- intro: ML4Fashion 2017\n- arxiv: [https://arxiv.org/abs/1707.09899](https://arxiv.org/abs/1707.09899)\n\n**Deep Learning Our Way Through Fashion Week**\n\n[https://inside.edited.com/deep-learning-our-way-through-fashion-week-ea55bf50bab8](https://inside.edited.com/deep-learning-our-way-through-fashion-week-ea55bf50bab8)\n\n**Be Your Own Prada: Fashion Synthesis with Structural Coherence**\n\n- intro: ICCV 2017\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Be_Your_Own_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Be_Your_Own_ICCV_2017_paper.pdf)\n- github: [https://github.com/zhusz/ICCV17-fashionGAN](https://github.com/zhusz/ICCV17-fashionGAN)\n\n# Others\n\n**Selfai: Predicting Facial Beauty in Selfies**\n\n**Selfai: A Method for Understanding Beauty in Selfies**\n\n- blog: [http://www.erogol.com/selfai-predicting-facial-beauty-selfies/](http://www.erogol.com/selfai-predicting-facial-beauty-selfies/)\n- github: [https://github.com/erogol/beauty.torch](https://github.com/erogol/beauty.torch)\n\n**Deep Learning Enables You to Hide Screen when Your Boss is Approaching**\n\n- blog: [http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/](http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/)\n- github: [https://github.com/Hironsan/BossSensor](https://github.com/Hironsan/BossSensor)\n\n# Blogs\n\n**40 Ways Deep Learning is Eating the World**\n\n[https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d#.rxq8xvbfz](https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d#.rxq8xvbfz)\n\n**Applications**\n\n[http://www.deeplearningpatterns.com/doku.php/applications](http://www.deeplearningpatterns.com/doku.php/applications)\n\n**Systematic Approach To Applications Of Deep Learning**\n\n[https://gettocode.com/2016/11/25/systematic-approach-to-applications-of-deep-learning/](https://gettocode.com/2016/11/25/systematic-approach-to-applications-of-deep-learning/)\n\n# Resources\n\n**Deep Learning Gallery - a curated collection of deep learning projects**\n\n[http://deeplearninggallery.com/](http://deeplearninggallery.com/)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","title":"Training Deep Neural Networks"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Training Deep Neural Networks\ndate: 2015-10-09\n---\n\n# Tutorials\n\n**Popular Training Approaches of DNNs — A Quick Overview**\n\n[https://medium.com/@asjad/popular-training-approaches-of-dnns-a-quick-overview-26ee37ad7e96#.pqyo039bb](https://medium.com/@asjad/popular-training-approaches-of-dnns-a-quick-overview-26ee37ad7e96#.pqyo039bb)\n\n**Optimisation and training techniques for deep learning**\n\n[https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/](https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/)\n\n# Papers\n\n**SNIPER: Efficient Multi-Scale Training**\n\n[https://arxiv.org/abs/1805.09300](https://arxiv.org/abs/1805.09300)\n\n**RePr: Improved Training of Convolutional Filters**\n\n[https://arxiv.org/abs/1811.07275](https://arxiv.org/abs/1811.07275)\n\n# Activation functions\n\n## ReLU\n\n**Rectified linear units improve restricted boltzmann machines**\n\n- intro: ReLU\n- paper: [http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf)\n\n**Expressiveness of Rectifier Networks**\n\n- intro: ICML 2016\n- intro: This paper studies the expressiveness of ReLU Networks\n- arxiv: [https://arxiv.org/abs/1511.05678](https://arxiv.org/abs/1511.05678)\n\n**How can a deep neural network with ReLU activations in its hidden layers approximate any function?**\n\n- quora: [https://www.quora.com/How-can-a-deep-neural-network-with-ReLU-activations-in-its-hidden-layers-approximate-any-function](https://www.quora.com/How-can-a-deep-neural-network-with-ReLU-activations-in-its-hidden-layers-approximate-any-function)\n\n**Understanding Deep Neural Networks with Rectified Linear Units**\n\n- intro: Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/1611.01491](https://arxiv.org/abs/1611.01491)\n\n**Learning ReLUs via Gradient Descent**\n\n[https://arxiv.org/abs/1705.04591](https://arxiv.org/abs/1705.04591)\n\n**Training Better CNNs Requires to Rethink ReLU**\n\n[https://arxiv.org/abs/1709.06247](https://arxiv.org/abs/1709.06247)\n\n**Deep Learning using Rectified Linear Units (ReLU)**\n\n- intro: Adamson University\n- arxiv: [https://arxiv.org/abs/1803.08375](https://arxiv.org/abs/1803.08375)\n- github: [https://github.com/AFAgarap/relu-classifier](https://github.com/AFAgarap/relu-classifier)\n\n**Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks**\n\n- intro: University of California, Los Angeles\n- arxiv: [https://arxiv.org/abs/1811.08888](https://arxiv.org/abs/1811.08888)\n\n## LReLU\n\n**Rectifier Nonlinearities Improve Neural Network Acoustic Models**\n\n- intro: leaky-ReLU, aka LReLU\n- paper: [http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf](http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)\n\n**Deep Sparse Rectifier Neural Networks**\n\n- paper: [http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf](http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)\n\n## PReLU\n\n**Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification**\n\n- keywords: PReLU, Caffe \"msra\" weights initilization\n- arxiv: [http://arxiv.org/abs/1502.01852](http://arxiv.org/abs/1502.01852)\n\n**Empirical Evaluation of Rectified Activations in Convolutional Network**\n\n- intro: ReLU / LReLU / PReLU / RReLU\n- arxiv: [http://arxiv.org/abs/1505.00853](http://arxiv.org/abs/1505.00853)\n\n## SReLU\n\n**Deep Learning with S-shaped Rectified Linear Activation Units**\n\n- intro:  SReLU\n- arxiv: [http://arxiv.org/abs/1512.07030](http://arxiv.org/abs/1512.07030)\n\n**Parametric Activation Pools greatly increase performance and consistency in ConvNets**\n\n- blog: [http://blog.claymcleod.io/2016/02/06/Parametric-Activation-Pools-greatly-increase-performance-and-consistency-in-ConvNets/](http://blog.claymcleod.io/2016/02/06/Parametric-Activation-Pools-greatly-increase-performance-and-consistency-in-ConvNets/)\n\n**From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification**\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1602.02068](http://arxiv.org/abs/1602.02068)\n- github: [https://github.com/gokceneraslan/SparseMax.torch](https://github.com/gokceneraslan/SparseMax.torch)\n- github: [https://github.com/Unbabel/sparsemax](https://github.com/Unbabel/sparsemax)\n\n**Revise Saturated Activation Functions**\n\n- arxiv: [http://arxiv.org/abs/1602.05980](http://arxiv.org/abs/1602.05980)\n\n**Noisy Activation Functions**\n\n- arxiv: [http://arxiv.org/abs/1603.00391](http://arxiv.org/abs/1603.00391)\n\n## MBA\n\n**Multi-Bias Non-linear Activation in Deep Neural Networks**\n\n- intro: MBA\n- arxiv: [https://arxiv.org/abs/1604.00676](https://arxiv.org/abs/1604.00676)\n\n**Learning activation functions from data using cubic spline interpolation**\n\n- arxiv: [http://arxiv.org/abs/1605.05509](http://arxiv.org/abs/1605.05509)\n- bitbucket: [https://bitbucket.org/ispamm/spline-nn](https://bitbucket.org/ispamm/spline-nn)\n\n**What is the role of the activation function in a neural network?**\n\n- quora: [https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network](https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network)\n\n## Concatenated ReLU (CRelu)\n\n**Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units**\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1603.05201](http://arxiv.org/abs/1603.05201)\n\n**Implement CReLU (Concatenated ReLU)**\n\n- github: [https://github.com/pfnet/chainer/pull/1142](https://github.com/pfnet/chainer/pull/1142)\n\n## GELU\n\n**Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units**\n\n- arxiv: [https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)\n\n**Formulating The ReLU**\n\n- blog: [http://www.jefkine.com/general/2016/08/24/formulating-the-relu/](http://www.jefkine.com/general/2016/08/24/formulating-the-relu/)\n\n**Activation Ensembles for Deep Neural Networks**\n\n[https://arxiv.org/abs/1702.07790](https://arxiv.org/abs/1702.07790)\n\n## SELU\n\n**Self-Normalizing Neural Networks**\n\n- intro: SELU\n- arxiv: [https://arxiv.org/abs/1706.02515](https://arxiv.org/abs/1706.02515)\n- github: [https://github.com/bioinf-jku/SNNs](https://github.com/bioinf-jku/SNNs)\n- notes: [https://github.com/kevinzakka/research-paper-notes/blob/master/snn.md](https://github.com/kevinzakka/research-paper-notes/blob/master/snn.md)\n- github(Chainer): [https://github.com/musyoku/self-normalizing-networks](https://github.com/musyoku/self-normalizing-networks)\n\n**SELUs (scaled exponential linear units) - Visualized and Histogramed Comparisons among ReLU and Leaky ReLU**\n\n[https://github.com/shaohua0116/Activation-Visualization-Histogram](https://github.com/shaohua0116/Activation-Visualization-Histogram)\n\n**Difference Between Softmax Function and Sigmoid Function**\n\n[http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/](http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/)\n\n**Flexible Rectified Linear Units for Improving Convolutional Neural Networks**\n\n- keywords: flexible rectified linear unit (FReLU)\n- arxiv: [https://arxiv.org/abs/1706.08098](https://arxiv.org/abs/1706.08098)\n\n**Be Careful What You Backpropagate: A Case For Linear Output Activations & Gradient Boosting**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1707.04199](https://arxiv.org/abs/1707.04199)\n\n## EraseReLU\n\n**EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks**\n\n[https://arxiv.org/abs/1709.07634](https://arxiv.org/abs/1709.07634)\n\n## Swish\n\n**Swish: a Self-Gated Activation Function**\n\n**Searching for Activation Functions**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/77gcrv/d_swish_is_not_performing_very_well/](https://www.reddit.com/r/MachineLearning/comments/77gcrv/d_swish_is_not_performing_very_well/)\n\n**Deep Learning with Data Dependent Implicit Activation Function**\n\n[https://arxiv.org/abs/1802.00168](https://arxiv.org/abs/1802.00168)\n\n## Series on Initialization of Weights for DNN\n\n**Initialization Of Feedfoward Networks**\n\n- blog: [http://www.jefkine.com/deep/2016/07/27/initialization-of-feedfoward-networks/](http://www.jefkine.com/deep/2016/07/27/initialization-of-feedfoward-networks/)\n\n**Initialization Of Deep Feedfoward Networks**\n\n- blog: [http://www.jefkine.com/deep/2016/08/01/initialization-of-deep-feedfoward-networks/](http://www.jefkine.com/deep/2016/08/01/initialization-of-deep-feedfoward-networks/)\n\n**Initialization Of Deep Networks Case of Rectifiers**\n\n- blog: [http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/](http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)\n\n# Weights Initialization\n\n**An Explanation of Xavier Initialization**\n\n- blog: [http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)\n\n**Random Walk Initialization for Training Very Deep Feedforward Networks**\n\n- arxiv: [http://arxiv.org/abs/1412.6558](http://arxiv.org/abs/1412.6558)\n\n**Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?**\n\n- arxiv: [http://arxiv.org/abs/1504.08291](http://arxiv.org/abs/1504.08291)\n\n**All you need is a good init**\n\n- intro: ICLR 2016\n- intro: Layer-sequential unit-variance (LSUV) initialization\n- arxiv: [http://arxiv.org/abs/1511.06422](http://arxiv.org/abs/1511.06422)\n- github(Caffe): [https://github.com/ducha-aiki/LSUVinit](https://github.com/ducha-aiki/LSUVinit)\n- github(Torch): [https://github.com/yobibyte/torch-lsuv](https://github.com/yobibyte/torch-lsuv)\n- github: [https://github.com/yobibyte/yobiblog/blob/master/posts/all-you-need-is-a-good-init.md](https://github.com/yobibyte/yobiblog/blob/master/posts/all-you-need-is-a-good-init.md)\n- github(Keras): [https://github.com/ducha-aiki/LSUV-keras](https://github.com/ducha-aiki/LSUV-keras)\n- review: [http://www.erogol.com/need-good-init/](http://www.erogol.com/need-good-init/)\n\n**All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation**\n\n- intro: CVPR 2017. HIKVision\n- arxiv: [https://arxiv.org/abs/1703.01827](https://arxiv.org/abs/1703.01827)\n\n**Data-dependent Initializations of Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1511.06856](http://arxiv.org/abs/1511.06856)\n- github: [https://github.com/philkr/magic_init](https://github.com/philkr/magic_init)\n\n**What are good initial weights in a neural network?**\n\n- stackexchange: [http://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network](http://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network)\n\n**RandomOut: Using a convolutional gradient norm to win The Filter Lottery**\n\n- arxiv: [http://arxiv.org/abs/1602.05931](http://arxiv.org/abs/1602.05931)\n\n**Categorical Reparameterization with Gumbel-Softmax**\n\n- intro: Google Brain & University of Cambridge & Stanford University\n- arxiv: [https://arxiv.org/abs/1611.01144](https://arxiv.org/abs/1611.01144)\n- github: [https://github.com/ericjang/gumbel-softmax](https://github.com/ericjang/gumbel-softmax)\n\n**On weight initialization in deep neural networks**\n\n- arxiv: [https://arxiv.org/abs/1704.08863](https://arxiv.org/abs/1704.08863)\n- github: [https://github.com/sidkk86/weight_initialization](https://github.com/sidkk86/weight_initialization)\n\n**Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks**\n\n- intro: ICML 2018. Google Brain\n- arxiv: [https://arxiv.org/abs/1806.05393](https://arxiv.org/abs/1806.05393)\n\n## Batch Normalization\n\n**Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n\n- intro: ImageNet top-5 error: 4.82%\n- keywords: internal covariate shift problem\n- arxiv: [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167)\n- blog: [https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/](https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/)\n- notes: [http://blog.csdn.net/happynear/article/details/44238541](http://blog.csdn.net/happynear/article/details/44238541)\n\n**Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1602.07868](http://arxiv.org/abs/1602.07868)\n- github(Lasagne): [https://github.com/TimSalimans/weight_norm](https://github.com/TimSalimans/weight_norm)\n- github: [https://github.com/openai/weightnorm](https://github.com/openai/weightnorm)\n- notes: [http://www.erogol.com/my-notes-weight-normalization/](http://www.erogol.com/my-notes-weight-normalization/)\n\n**Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks**\n\n- arxiv: [http://arxiv.org/abs/1603.01431](http://arxiv.org/abs/1603.01431)\n\n**Revisiting Batch Normalization For Practical Domain Adaptation**\n\n- intro: Peking University & TuSimple & SenseTime\n- intro: Pattern Recognition\n- keywords: Adaptive Batch Normalization (AdaBN)\n- arxiv: [https://arxiv.org/abs/1603.04779](https://arxiv.org/abs/1603.04779)\n\n**Implementing Batch Normalization in Tensorflow**\n\n- blog: [http://r2rt.com/implementing-batch-normalization-in-tensorflow.html](http://r2rt.com/implementing-batch-normalization-in-tensorflow.html)\n\n**Deriving the Gradient for the Backward Pass of Batch Normalization**\n\n- blog: [https://kevinzakka.github.io/2016/09/14/batch_normalization/](https://kevinzakka.github.io/2016/09/14/batch_normalization/)\n\n**Exploring Normalization in Deep Residual Networks with Concatenated Rectified Linear Units**\n\n- intro: Oculus VR & Facebook & NEC Labs America\n- paper: [https://research.fb.com/publications/exploring-normalization-in-deep-residual-networks-with-concatenated-rectified-linear-units/](https://research.fb.com/publications/exploring-normalization-in-deep-residual-networks-with-concatenated-rectified-linear-units/)\n\n**Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models**\n\n- intro: Sergey Ioffe, Google\n- arxiv: [https://arxiv.org/abs/1702.03275](https://arxiv.org/abs/1702.03275)\n\n**Comparison of Batch Normalization and Weight Normalization Algorithms for the Large-scale Image Classification**\n\n[https://arxiv.org/abs/1709.08145](https://arxiv.org/abs/1709.08145)\n\n**In-Place Activated BatchNorm for Memory-Optimized Training of DNNs**\n\n- intro: Mapillary Research\n- arxiv: [https://arxiv.org/abs/1712.02616](https://arxiv.org/abs/1712.02616)\n- github: [https://github.com/mapillary/inplace_abn](https://github.com/mapillary/inplace_abn)\n\n**Batch Kalman Normalization: Towards Training Deep Neural Networks with Micro-Batches**\n\n[https://arxiv.org/abs/1802.03133](https://arxiv.org/abs/1802.03133)\n\n**Decorrelated Batch Normalization**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.08450](https://arxiv.org/abs/1804.08450)\n- github: [https://github.com/umich-vl/DecorrelatedBN](https://github.com/umich-vl/DecorrelatedBN)\n\n**Understanding Batch Normalization**\n\n[https://arxiv.org/abs/1806.02375](https://arxiv.org/abs/1806.02375)\n\n**Implementing Synchronized Multi-GPU Batch Normalization**\n\n[http://hangzh.com/PyTorch-Encoding/notes/syncbn.html](http://hangzh.com/PyTorch-Encoding/notes/syncbn.html)\n\n**Restructuring Batch Normalization to Accelerate CNN Training**\n\n[https://arxiv.org/abs/1807.01702](https://arxiv.org/abs/1807.01702)\n\n**Intro to optimization in deep learning: Busting the myth about batch normalization**\n\n- blog: [https://blog.paperspace.com/busting-the-myths-about-batch-normalization/](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)\n\n**Understanding Regularization in Batch Normalization**\n\n[https://arxiv.org/abs/1809.00846](https://arxiv.org/abs/1809.00846)\n\n**How Does Batch Normalization Help Optimization?**\n\n- intro: NeurIPS 2018. MIT\n- arxiv: [https://arxiv.org/abs/1805.11604](https://arxiv.org/abs/1805.11604)\n- video: [https://www.youtube.com/watch?v=ZOabsYbmBRM](https://www.youtube.com/watch?v=ZOabsYbmBRM)\n\n**Cross-Iteration Batch Normalization**\n\n[https://arxiv.org/abs/2002.05712](https://arxiv.org/abs/2002.05712)\n\n**Extended Batch Normalization**\n\n- intro: Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/2003.05569](https://arxiv.org/abs/2003.05569)\n\n**Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization**\n\n- intro: ICLR 2020 Poster\n- keywords: Moving Average Batch Normalization\n- openreview: [https://openreview.net/forum?id=SkgGjRVKDS](https://openreview.net/forum?id=SkgGjRVKDS)\n- github(official, Pytorch): [https://github.com/megvii-model/MABN](https://github.com/megvii-model/MABN)\n\n**Rethinking “Batch” in BatchNorm**\n\n- intro: Facebook AI Research\n- arxiv: [https://arxiv.org/abs/2105.07576](https://arxiv.org/abs/2105.07576)\n\n**Delving into the Estimation Shift of Batch Normalization in a Network**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.10778](https://arxiv.org/abs/2203.10778)\n- gtihub: [https://github.com/huangleiBuaa/XBNBlock](https://github.com/huangleiBuaa/XBNBlock)\n\n### Backward pass of BN\n\n**Understanding the backward pass through Batch Normalization Layer**\n\n[https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n\n**Deriving the Gradient for the Backward Pass of Batch Normalization**\n\n[https://kevinzakka.github.io/2016/09/14/batch_normalization/](https://kevinzakka.github.io/2016/09/14/batch_normalization/)\n\n**What does the gradient flowing through batch normalization looks like ?**\n\n[http://cthorey.github.io./backpropagation/](http://cthorey.github.io./backpropagation/)\n\n## Layer Normalization\n\n**Layer Normalization**\n\n- arxiv: [https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)\n- github: [https://github.com/ryankiros/layer-norm](https://github.com/ryankiros/layer-norm)\n- github(TensorFlow): [https://github.com/pbhatia243/tf-layer-norm](https://github.com/pbhatia243/tf-layer-norm)\n- github: [https://github.com/MycChiu/fast-LayerNorm-TF](https://github.com/MycChiu/fast-LayerNorm-TF)\n\n**Keras GRU with Layer Normalization**\n\n- gist: [https://gist.github.com/udibr/7f46e790c9e342d75dcbd9b1deb9d940](https://gist.github.com/udibr/7f46e790c9e342d75dcbd9b1deb9d940)\n\n**Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.05870](https://arxiv.org/abs/1702.05870)\n\n**Differentiable Learning-to-Normalize via Switchable Normalization**\n\n- arxiv: [https://arxiv.org/abs/1806.10779](https://arxiv.org/abs/1806.10779)\n- github: [https://github.com/switchablenorms/Switchable-Normalization](https://github.com/switchablenorms/Switchable-Normalization)\n\n## Group Normalization\n\n**Group Normalization**\n\n- intro: ECCV 2018 Best Paper Award Honorable Mention\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/1803.08494](https://arxiv.org/abs/1803.08494)\n\n## Batch-Instance Normalization\n\n**Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks**\n\n[https://arxiv.org/abs/1805.07925](https://arxiv.org/abs/1805.07925)\n\n**Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.09441](https://arxiv.org/abs/1807.09441)\n- github(official, Pytorch): [https://github.com/XingangPan/IBN-Net](https://github.com/XingangPan/IBN-Net)\n\n## Dynamic Normalization\n\n**Dynamic Normalization**\n\n[https://arxiv.org/abs/2101.06073](https://arxiv.org/abs/2101.06073)\n\n# Loss Function\n\n**The Loss Surfaces of Multilayer Networks**\n\n- arxiv: [http://arxiv.org/abs/1412.0233](http://arxiv.org/abs/1412.0233)\n\n**Direct Loss Minimization for Training Deep Neural Nets**\n\n- arxiv: [http://arxiv.org/abs/1511.06411](http://arxiv.org/abs/1511.06411)\n\n**Nonconvex Loss Functions for Classifiers and Deep Networks**\n\n- blog: [https://casmls.github.io/general/2016/10/27/NonconvexLosses.html](https://casmls.github.io/general/2016/10/27/NonconvexLosses.html)\n\n**Learning Deep Embeddings with Histogram Loss**\n\n- arxiv: [https://arxiv.org/abs/1611.00822](https://arxiv.org/abs/1611.00822)\n\n**Large-Margin Softmax Loss for Convolutional Neural Networks**\n\n- intro: ICML 2016\n- intro: Peking University & South China University of Technology & CMU & Shenzhen University\n- arxiv: [https://arxiv.org/abs/1612.02295](https://arxiv.org/abs/1612.02295)\n- github(Official. Caffe): [https://github.com/wy1iu/LargeMargin_Softmax_Loss](https://github.com/wy1iu/LargeMargin_Softmax_Loss)\n- github: [https://github.com/luoyetx/mx-lsoftmax](https://github.com/luoyetx/mx-lsoftmax)\n- github: [https://github.com/tpys/face-recognition-caffe2](https://github.com/tpys/face-recognition-caffe2)\n- github: [https://github.com/jihunchoi/lsoftmax-pytorch](https://github.com/jihunchoi/lsoftmax-pytorch)\n\n**An empirical analysis of the optimization of deep network loss surfaces**\n\n[https://arxiv.org/abs/1612.04010](https://arxiv.org/abs/1612.04010)\n\n**Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes**\n\n- intro: Peking University\n- arxiv: [https://arxiv.org/abs/1706.10239](https://arxiv.org/abs/1706.10239)\n\n**Hierarchical Softmax**\n\n[http://building-babylon.net/2017/08/01/hierarchical-softmax/](http://building-babylon.net/2017/08/01/hierarchical-softmax/)\n\n**Noisy Softmax: Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1708.03769](https://arxiv.org/abs/1708.03769)\n\n**DropMax: Adaptive Stochastic Softmax**\n\n- intro: UNIST & Postech & KAIST\n- arxiv: [https://arxiv.org/abs/1712.07834](https://arxiv.org/abs/1712.07834)\n\n**Rethinking Feature Distribution for Loss Functions in Image Classification**\n\n- intro: CVPR 2018 spotlight\n- arxiv: [https://arxiv.org/abs/1803.02988](https://arxiv.org/abs/1803.02988)\n\n**Ensemble Soft-Margin Softmax Loss for Image Classification**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1805.03922](https://arxiv.org/abs/1805.03922)\n\n**Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels**\n\n- intro: Cornell University\n- arxiv: [https://arxiv.org/abs/1805.07836](https://arxiv.org/abs/1805.07836)\n\n# Learning Rates\n\n**No More Pesky Learning Rates**\n\n- intro: Tom Schaul, Sixin Zhang, Yann LeCun\n- arxiv: [https://arxiv.org/abs/1206.1106](https://arxiv.org/abs/1206.1106)\n\n**Coupling Adaptive Batch Sizes with Learning Rates**\n\n- intro: Max Planck Institute for Intelligent Systems\n- intro: Tensorflow implementation of SGD with Coupled Adaptive Batch Size (CABS)\n- arxiv: [https://arxiv.org/abs/1612.05086](https://arxiv.org/abs/1612.05086)\n- github: [https://github.com/ProbabilisticNumerics/cabs](https://github.com/ProbabilisticNumerics/cabs)\n\n**Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates**\n\n[https://arxiv.org/abs/1708.07120](https://arxiv.org/abs/1708.07120)\n\n**Improving the way we work with learning rate.**\n\n[https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b](https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b)\n\n**WNGrad: Learn the Learning Rate in Gradient Descent**\n\n- intro: University of Texas at Austin & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1803.02865](https://arxiv.org/abs/1803.02865)\n\n**Learning with Random Learning Rates**\n\n- intro: Facebook AI Research & Universite Paris Sud\n- keywords: All Learning Rates At Once (Alrao)\n- project page: [https://leonardblier.github.io/alrao/](https://leonardblier.github.io/alrao/)\n- arxiv: [https://arxiv.org/abs/1810.01322](https://arxiv.org/abs/1810.01322)\n- github(PyTorch, official): [https://github.com/leonardblier/alrao](https://github.com/leonardblier/alrao)\n\n**Learning Rate Dropout**\n\n- intro: 1Xiamen University & Columbia University\n- arxiv: [https://arxiv.org/abs/1912.00144](https://arxiv.org/abs/1912.00144)\n\n# Convolution Filters\n\n**Non-linear Convolution Filters for CNN-based Learning**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.07038](https://arxiv.org/abs/1708.07038)\n\n# Pooling\n\n**Stochastic Pooling for Regularization of Deep Convolutional Neural Networks**\n\n- intro: ICLR 2013. Matthew D. Zeiler, Rob Fergus\n- paper: [http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf](http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf)\n\n**Multi-scale Orderless Pooling of Deep Convolutional Activation Features**\n\n- intro: ECCV 2014\n- intro: MOP-CNN, orderless VLAD pooling, image classification / instance-level retrieval\n- arxiv: [https://arxiv.org/abs/1403.1840](https://arxiv.org/abs/1403.1840)\n- paper: [http://web.engr.illinois.edu/~slazebni/publications/yunchao_eccv14_mopcnn.pdf](http://web.engr.illinois.edu/~slazebni/publications/yunchao_eccv14_mopcnn.pdf)\n\n**Fractional Max-Pooling**\n\n- arxiv: [https://arxiv.org/abs/1412.6071](https://arxiv.org/abs/1412.6071)\n- notes: [https://gist.github.com/shagunsodhani/ccfe3134f46fd3738aa0](https://gist.github.com/shagunsodhani/ccfe3134f46fd3738aa0)\n- github: [https://github.com/torch/nn/issues/371](https://github.com/torch/nn/issues/371)\n\n**TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks**\n\n- intro: CVPR 2016\n- paper: [http://dlaptev.org/papers/Laptev16_CVPR.pdf](http://dlaptev.org/papers/Laptev16_CVPR.pdf)\n- github: [https://github.com/dlaptev/TI-pooling](https://github.com/dlaptev/TI-pooling)\n\n**S3Pool: Pooling with Stochastic Spatial Sampling**\n\n- arxiv: [https://arxiv.org/abs/1611.05138](https://arxiv.org/abs/1611.05138)\n- github(Lasagne): [https://github.com/Shuangfei/s3pool](https://github.com/Shuangfei/s3pool)\n\n**Inductive Bias of Deep Convolutional Networks through Pooling Geometry**\n\n- arxiv: [https://arxiv.org/abs/1605.06743](https://arxiv.org/abs/1605.06743)\n- github: [https://github.com/HUJI-Deep/inductive-pooling](https://github.com/HUJI-Deep/inductive-pooling)\n\n**Improved Bilinear Pooling with CNNs**\n\n[https://arxiv.org/abs/1707.06772](https://arxiv.org/abs/1707.06772)\n\n**Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.08105](https://arxiv.org/abs/1707.08105)\n- github: [https://github.com/passalis/cbof](https://github.com/passalis/cbof)\n\n**A new kind of pooling layer for faster and sharper convergence**\n\n- blog: [https://medium.com/@singlasahil14/a-new-kind-of-pooling-layer-for-faster-and-sharper-convergence-1043c756a221](https://medium.com/@singlasahil14/a-new-kind-of-pooling-layer-for-faster-and-sharper-convergence-1043c756a221)\n- github: [https://github.com/singlasahil14/sortpool2d](https://github.com/singlasahil14/sortpool2d)\n\n**Statistically Motivated Second Order Pooling**\n\n[https://arxiv.org/abs/1801.07492](https://arxiv.org/abs/1801.07492)\n\n**Detail-Preserving Pooling in Deep Networks**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.04076](https://arxiv.org/abs/1804.04076)\n\n# Mini-Batch\n\n**Online Batch Selection for Faster Training of Neural Networks**\n\n- intro: Workshop paper at ICLR 2016\n- arxiv: [https://arxiv.org/abs/1511.06343](https://arxiv.org/abs/1511.06343)\n\n**On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima**\n\n- intro: ICLR 2017\n- arxiv: [https://arxiv.org/abs/1609.04836](https://arxiv.org/abs/1609.04836)\n\n**Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour**\n\n- intro: Facebook\n- keywords: Training with 256 GPUs, minibatches of 8192\n- arxiv: [https://arxiv.org/abs/1706.02677](https://arxiv.org/abs/1706.02677)\n\n**Scaling SGD Batch Size to 32K for ImageNet Training**\n\n**Large Batch Training of Convolutional Networks**\n\n[https://arxiv.org/abs/1708.03888](https://arxiv.org/abs/1708.03888)\n\n**ImageNet Training in 24 Minutes**\n\n[https://arxiv.org/abs/1709.05011](https://arxiv.org/abs/1709.05011)\n\n**Don't Decay the Learning Rate, Increase the Batch Size**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1711.00489](https://arxiv.org/abs/1711.00489)\n\n**Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes**\n\n- intro: NIPS 2017 Workshop: Deep Learning at Supercomputer Scale\n- arxiv: [https://arxiv.org/abs/1711.04325](https://arxiv.org/abs/1711.04325)\n\n**AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks**\n\n- intro: UC Berkeley & NVIDIA\n- arxiv: [https://arxiv.org/abs/1712.02029](https://arxiv.org/abs/1712.02029)\n\n**Hessian-based Analysis of Large Batch Training and Robustness to Adversaries**\n\n- intro: UC Berkeley & University of Texas\n- arxiv: [https://arxiv.org/abs/1802.08241](https://arxiv.org/abs/1802.08241)\n\n**Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling**\n\n- keywords: large batch, LARS, adaptive rate scaling\n- openreview: [https://openreview.net/forum?id=rJ4uaX2aW](https://openreview.net/forum?id=rJ4uaX2aW)\n\n**Revisiting Small Batch Training for Deep Neural Networks**\n\n[https://arxiv.org/abs/1804.07612](https://arxiv.org/abs/1804.07612)\n\n**Second-order Optimization Method for Large Mini-batch: Training ResNet-50 on ImageNet in 35 Epochs**\n\n[https://arxiv.org/abs/1811.12019](https://arxiv.org/abs/1811.12019)\n\n# Optimization Methods\n\n**On Optimization Methods for Deep Learning**\n\n- paper: [http://www.icml-2011.org/papers/210_icmlpaper.pdf](http://www.icml-2011.org/papers/210_icmlpaper.pdf)\n\n**Invariant backpropagation: how to train a transformation-invariant neural network**\n\n- arxiv: [http://arxiv.org/abs/1502.04434](http://arxiv.org/abs/1502.04434)\n- github: [https://github.com/sdemyanov/ConvNet](https://github.com/sdemyanov/ConvNet)\n\n**A practical theory for designing very deep convolutional neural network**\n\n- kaggle: [https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code/69284](https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code/69284)\n- paper: [https://kaggle2.blob.core.windows.net/forum-message-attachments/69182/2287/A%20practical%20theory%20for%20designing%20very%20deep%20convolutional%20neural%20networks.pdf?sv=2012-02-12&se=2015-12-05T15%3A40%3A02Z&sr=b&sp=r&sig=kfBQKduA1pDtu837Y9Iqyrp2VYItTV0HCgOeOok9E3E%3D](https://kaggle2.blob.core.windows.net/forum-message-attachments/69182/2287/A%20practical%20theory%20for%20designing%20very%20deep%20convolutional%20neural%20networks.pdf?sv=2012-02-12&se=2015-12-05T15%3A40%3A02Z&sr=b&sp=r&sig=kfBQKduA1pDtu837Y9Iqyrp2VYItTV0HCgOeOok9E3E%3D)\n- slides: [http://vdisk.weibo.com/s/3nFsznjLKn](http://vdisk.weibo.com/s/3nFsznjLKn)\n\n**Stochastic Optimization Techniques**\n\n- intro: SGD/Momentum/NAG/Adagrad/RMSProp/Adadelta/Adam/ESGD/Adasecant/vSGD/Rprop\n- blog: [http://colinraffel.com/wiki/stochastic_optimization_techniques](http://colinraffel.com/wiki/stochastic_optimization_techniques)\n\n**Alec Radford's animations for optimization algorithms**\n\n[http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html)\n\n**Faster Asynchronous SGD (FASGD)**\n\n- arxiv: [http://arxiv.org/abs/1601.04033](http://arxiv.org/abs/1601.04033)\n- github: [https://github.com/DoctorTeeth/fred](https://github.com/DoctorTeeth/fred)\n\n**An overview of gradient descent optimization algorithms (★★★★★)**\n\n![](http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif)\n\n- arxiv: [https://arxiv.org/abs/1609.04747](https://arxiv.org/abs/1609.04747)\n- blog: [http://sebastianruder.com/optimizing-gradient-descent/](http://sebastianruder.com/optimizing-gradient-descent/)\n\n**Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters**\n\n- arxiv: [http://arxiv.org/abs/1602.02151](http://arxiv.org/abs/1602.02151)\n\n**Writing fast asynchronous SGD/AdaGrad with RcppParallel**\n\n- blog: [http://gallery.rcpp.org/articles/rcpp-sgd/](http://gallery.rcpp.org/articles/rcpp-sgd/)\n\n**Quick Explanations Of Optimization Methods**\n\n- blog: [http://jxieeducation.com/2016-07-02/Quick-Explanations-of-Optimization-Methods/](http://jxieeducation.com/2016-07-02/Quick-Explanations-of-Optimization-Methods/)\n\n**Learning to learn by gradient descent by gradient descent**\n\n- intro: Google DeepMind\n- arxiv: [https://arxiv.org/abs/1606.04474](https://arxiv.org/abs/1606.04474)\n- github: [https://github.com/deepmind/learning-to-learn](https://github.com/deepmind/learning-to-learn)\n- github(TensorFlow): [https://github.com/runopti/Learning-To-Learn](https://github.com/runopti/Learning-To-Learn)\n- github(PyTorch): [https://github.com/ikostrikov/pytorch-meta-optimizer](https://github.com/ikostrikov/pytorch-meta-optimizer)\n\n**SGDR: Stochastic Gradient Descent with Restarts**\n\n- intro: ICLR 2017\n- keywords: cosine annealing strategy\n- arxiv: [http://arxiv.org/abs/1608.03983](http://arxiv.org/abs/1608.03983)\n- github: [https://github.com/loshchil/SGDR](https://github.com/loshchil/SGDR)\n\n**The zen of gradient descent**\n\n- blog: [http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html](http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html)\n\n**Big Batch SGD: Automated Inference using Adaptive Batch Sizes**\n\n- arxiv: [https://arxiv.org/abs/1610.05792](https://arxiv.org/abs/1610.05792)\n\n**Improving Stochastic Gradient Descent with Feedback**\n\n- arxiv: [https://arxiv.org/abs/1611.01505](https://arxiv.org/abs/1611.01505)\n- github: [https://github.com/jayanthkoushik/sgd-feedback](https://github.com/jayanthkoushik/sgd-feedback)\n- github: [https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Eve](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Eve)\n\n**Learning Gradient Descent: Better Generalization and Longer Horizons**\n\n- intro: Tsinghua University\n- arxiv: [https://arxiv.org/abs/1703.03633](https://arxiv.org/abs/1703.03633)\n- github(TensorFlow): [https://github.com/vfleaking/rnnprop](https://github.com/vfleaking/rnnprop)\n\n**Optimization Algorithms**\n\n- blog: [https://3dbabove.com/2017/11/14/optimizationalgorithms/](https://3dbabove.com/2017/11/14/optimizationalgorithms/)\n- github: [https://github.com//ManuelGonzalezRivero/3dbabove](https://github.com//ManuelGonzalezRivero/3dbabove)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/7ehxky/d_optimization_algorithms_math_and_code/](https://www.reddit.com/r/MachineLearning/comments/7ehxky/d_optimization_algorithms_math_and_code/)\n\n**Gradient Normalization & Depth Based Decay For Deep Learning**\n\n- intro: Columbia University\n- arxiv: [https://arxiv.org/abs/1712.03607](https://arxiv.org/abs/1712.03607)\n\n**Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks**\n\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/1712.03298](https://arxiv.org/abs/1712.03298)\n\n**Optimization for Deep Learning Highlights in 2017**\n\n[http://ruder.io/deep-learning-optimization-2017/index.html](http://ruder.io/deep-learning-optimization-2017/index.html)\n\n**Gradients explode - Deep Networks are shallow - ResNet explained**\n\n- intro: CMU & UC Berkeley\n- arxiv: [https://arxiv.org/abs/1712.05577](https://arxiv.org/abs/1712.05577)\n\n**A Sufficient Condition for Convergences of Adam and RMSProp**\n\n[https://arxiv.org/abs/1811.09358](https://arxiv.org/abs/1811.09358)\n\n## Adam\n\n**Adam: A Method for Stochastic Optimization**\n\n- intro: ICLR 2015\n- arxiv: [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980)\n\n**Fixing Weight Decay Regularization in Adam**\n\n- intro: University of Freiburg\n- arxiv: [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)\n- github: [https://github.com/loshchil/AdamW-and-SGDW](https://github.com/loshchil/AdamW-and-SGDW)\n- github: [https://github.com/fastai/fastai/pull/46/files](https://github.com/fastai/fastai/pull/46/files)\n\n**On the Convergence of Adam and Beyond**\n\n- intro: ICLR 2018 best paper award. CMU & IBM Research\n- paper: [https://openreview.net/pdf?id=ryQu7f-RZ](https://openreview.net/pdf?id=ryQu7f-RZ)\n- openreview: [https://openreview.net/forum?id=ryQu7f-RZ](https://openreview.net/forum?id=ryQu7f-RZ)\n\n# Tensor Methods\n\n**Tensorizing Neural Networks**\n\n- intro: TensorNet\n- arxiv: [http://arxiv.org/abs/1509.06569](http://arxiv.org/abs/1509.06569)\n- github(Matlab+Theano+Lasagne): [https://github.com/Bihaqo/TensorNet](https://github.com/Bihaqo/TensorNet)\n- github(TensorFlow): [https://github.com/timgaripov/TensorNet-TF](https://github.com/timgaripov/TensorNet-TF)\n\n**Tensor methods for training neural networks**\n\n- homepage: [http://newport.eecs.uci.edu/anandkumar/#home](http://newport.eecs.uci.edu/anandkumar/#home)\n- youtube: [https://www.youtube.com/watch?v=B4YvhcGaafw](https://www.youtube.com/watch?v=B4YvhcGaafw)\n- slides: [http://newport.eecs.uci.edu/anandkumar/slides/Strata-NY.pdf](http://newport.eecs.uci.edu/anandkumar/slides/Strata-NY.pdf)\n- talks: [http://newport.eecs.uci.edu/anandkumar/#talks](http://newport.eecs.uci.edu/anandkumar/#talks)\n\n# Regularization\n\n**DisturbLabel: Regularizing CNN on the Loss Layer**\n\n- intro:  University of California & MSR 2016\n- intro: \"an extremely simple algorithm which randomly replaces a part of labels as incorrect values in each iteration\"\n- paper: [http://research.microsoft.com/en-us/um/people/jingdw/pubs/cvpr16-disturblabel.pdf](http://research.microsoft.com/en-us/um/people/jingdw/pubs/cvpr16-disturblabel.pdf)\n\n**Robust Convolutional Neural Networks under Adversarial Noise**\n\n- intro:  ICLR 2016\n- arxiv: [http://arxiv.org/abs/1511.06306](http://arxiv.org/abs/1511.06306)\n\n**Adding Gradient Noise Improves Learning for Very Deep Networks**\n\n- intro:  ICLR 2016\n- arxiv: [http://arxiv.org/abs/1511.06807](http://arxiv.org/abs/1511.06807)\n\n**Stochastic Function Norm Regularization of Deep Networks**\n\n- arxiv: [http://arxiv.org/abs/1605.09085](http://arxiv.org/abs/1605.09085)\n- github: [https://github.com/AmalRT/DNN_Reg](https://github.com/AmalRT/DNN_Reg)\n\n**SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1609.06693](http://arxiv.org/abs/1609.06693)\n\n**Regularizing neural networks by penalizing confident predictions**\n\n- intro: Gabriel Pereyra, George Tucker, Lukasz Kaiser, Geoffrey Hinton [Google Brain\n- dropbox: [https://www.dropbox.com/s/8kqf4v2c9lbnvar/BayLearn%202016%20(gjt).pdf?dl=0](https://www.dropbox.com/s/8kqf4v2c9lbnvar/BayLearn%202016%20(gjt).pdf?dl=0)\n- mirror: [https://pan.baidu.com/s/1kUUtxdl](https://pan.baidu.com/s/1kUUtxdl)\n\n**Automatic Node Selection for Deep Neural Networks using Group Lasso Regularization**\n\n- arxiv: [https://arxiv.org/abs/1611.05527](https://arxiv.org/abs/1611.05527)\n\n**Regularization in deep learning**\n\n- blog: [https://medium.com/@cristina_scheau/regularization-in-deep-learning-f649a45d6e0#.py327hkuv](https://medium.com/@cristina_scheau/regularization-in-deep-learning-f649a45d6e0#.py327hkuv)\n- github: [https://github.com/cscheau/Examples/blob/master/iris_l1_l2.py](https://github.com/cscheau/Examples/blob/master/iris_l1_l2.py)\n\n**LDMNet: Low Dimensional Manifold Regularized Neural Networks**\n\n[https://arxiv.org/abs/1711.06246](https://arxiv.org/abs/1711.06246)\n\n**Learning Sparse Neural Networks through L0 Regularization**\n\n- intro: University of Amsterdam & OpenAI\n- arxiv: [https://arxiv.org/abs/1712.01312](https://arxiv.org/abs/1712.01312)\n\n**Regularization and Optimization strategies in Deep Convolutional Neural Network**\n\n[https://arxiv.org/abs/1712.04711](https://arxiv.org/abs/1712.04711)\n\n**Regularizing Deep Networks by Modeling and Predicting Label Structure**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.02009](https://arxiv.org/abs/1804.02009)\n\n**Adversarial Noise Layer: Regularize Neural Network By Adding Noise**\n\n- intro: Peking University & ‡University of Electronic Science and Technology of China & Australian National University\n- arxiv: [https://arxiv.org/abs/1805.08000](https://arxiv.org/abs/1805.08000)\n- github: [https://github.com/youzhonghui/ANL](https://github.com/youzhonghui/ANL)\n\n**Deep Bilevel Learning**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1809.01465](https://arxiv.org/abs/1809.01465)\n\n**Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.09102](https://arxiv.org/abs/1810.09102)\n\n**Gradient-Coherent Strong Regularization for Deep Neural Networks**\n\n[https://arxiv.org/abs/1811.08056](https://arxiv.org/abs/1811.08056)\n\n## Dropout\n\n**Improving neural networks by preventing co-adaptation of feature detectors**\n\n- intro: Dropout\n- arxiv: [http://arxiv.org/abs/1207.0580](http://arxiv.org/abs/1207.0580)\n\n**Dropout: A Simple Way to Prevent Neural Networks from Overfitting**\n\n- paper: [https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n\n**Fast dropout training**\n\n- paper: [http://jmlr.org/proceedings/papers/v28/wang13a.pdf](http://jmlr.org/proceedings/papers/v28/wang13a.pdf)\n- github: [https://github.com/sidaw/fastdropout](https://github.com/sidaw/fastdropout)\n\n**Dropout as data augmentation**\n\n- paper: [http://arxiv.org/abs/1506.08700](http://arxiv.org/abs/1506.08700)\n- notes: [https://www.evernote.com/shard/s189/sh/ef0c3302-21a4-40d7-b8b4-1c65b8ebb1c9/24ff553fcfb70a27d61ff003df75b5a9](https://www.evernote.com/shard/s189/sh/ef0c3302-21a4-40d7-b8b4-1c65b8ebb1c9/24ff553fcfb70a27d61ff003df75b5a9)\n\n**A Theoretically Grounded Application of Dropout in Recurrent Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1512.05287](http://arxiv.org/abs/1512.05287)\n- github: [https://github.com/yaringal/BayesianRNN](https://github.com/yaringal/BayesianRNN)\n\n**Improved Dropout for Shallow and Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1602.02220](http://arxiv.org/abs/1602.02220)\n\n**Dropout Regularization in Deep Learning Models With Keras**\n\n- blog: [http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/](http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)\n\n**Dropout with Expectation-linear Regularization**\n\n- arxiv: [http://arxiv.org/abs/1609.08017](http://arxiv.org/abs/1609.08017)\n\n**Dropout with Theano**\n\n- blog: [http://rishy.github.io/ml/2016/10/12/dropout-with-theano/](http://rishy.github.io/ml/2016/10/12/dropout-with-theano/)\n- ipn: [http://nbviewer.jupyter.org/github/rishy/rishy.github.io/blob/master/ipy_notebooks/Dropout-Theano.ipynb](http://nbviewer.jupyter.org/github/rishy/rishy.github.io/blob/master/ipy_notebooks/Dropout-Theano.ipynb)\n\n**Information Dropout: learning optimal representations through noise**\n\n- arxiv: [https://arxiv.org/abs/1611.01353](https://arxiv.org/abs/1611.01353)\n\n**Recent Developments in Dropout**\n\n- blog: [https://casmls.github.io/general/2016/11/11/dropout.html](https://casmls.github.io/general/2016/11/11/dropout.html)\n\n**Generalized Dropout**\n\n- arxiv: [https://arxiv.org/abs/1611.06791](https://arxiv.org/abs/1611.06791)\n\n**Analysis of Dropout**\n\n- blog: [https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/](https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/)\n\n**Variational Dropout Sparsifies Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1701.05369](https://arxiv.org/abs/1701.05369)\n\n**Learning Deep Networks from Noisy Labels with Dropout Regularization**\n\n- intro: 2016 IEEE 16th International Conference on Data Mining\n- arxiv: [https://arxiv.org/abs/1705.03419](https://arxiv.org/abs/1705.03419)\n\n**Concrete Dropout**\n\n- intro: University of Cambridge\n- arxiv: [https://arxiv.org/abs/1705.07832](https://arxiv.org/abs/1705.07832)\n- github: [https://github.com/yaringal/ConcreteDropout](https://github.com/yaringal/ConcreteDropout)\n\n**Analysis of dropout learning regarded as ensemble learning**\n\n- intro: Nihon University\n- arxiv: [https://arxiv.org/abs/1706.06859](https://arxiv.org/abs/1706.06859)\n\n**An Analysis of Dropout for Matrix Factorization**\n\n[https://arxiv.org/abs/1710.03487](https://arxiv.org/abs/1710.03487)\n\n**Analysis of Dropout in Online Learning**\n\n[https://arxiv.org/abs/1711.03343](https://arxiv.org/abs/1711.03343)\n\n**Regularization of Deep Neural Networks with Spectral Dropout**\n\n[https://arxiv.org/abs/1711.08591](https://arxiv.org/abs/1711.08591)\n\n**Data Dropout in Arbitrary Basis for Deep Network Regularization**\n\n[https://arxiv.org/abs/1712.00891](https://arxiv.org/abs/1712.00891)\n\n**A New Angle on L2 Regularization**\n\n- intro: An explorable explanation on the phenomenon of adversarial examples in linear classification and its relation to L2 regularization\n- blog: [https://thomas-tanay.github.io/post--L2-regularization/](https://thomas-tanay.github.io/post--L2-regularization/)\n- arxiv: [https://arxiv.org/abs/1806.11186](https://arxiv.org/abs/1806.11186)\n\n**Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning**\n\n- intro: Rutgers University\n- arxiv: [https://arxiv.org/abs/1808.03578](https://arxiv.org/abs/1808.03578)\n- github: [https://github.com/noahfl/densenet-sdr/](https://github.com/noahfl/densenet-sdr/)\n\n**Data Dropout: Optimizing Training Data for Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1809.00193](https://arxiv.org/abs/1809.00193)\n\n**DropFilter: Dropout for Convolutions**\n\n[https://arxiv.org/abs/1810.09849](https://arxiv.org/abs/1810.09849)\n\n**DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1811.06783](https://arxiv.org/abs/1811.06783)\n\n**Targeted Dropout**\n\n- intro: Google Brain & FOR.ai & University of Oxford\n- paper: [https://openreview.net/pdf?id=HkghWScuoQ](https://openreview.net/pdf?id=HkghWScuoQ)\n- github: [https://github.com/for-ai/TD](https://github.com/for-ai/TD)\n\n## DropConnect\n\n**Regularization of Neural Networks using DropConnect**\n\n- homepage: [http://cs.nyu.edu/~wanli/dropc/](http://cs.nyu.edu/~wanli/dropc/)\n- gitxiv: [http://gitxiv.com/posts/rJucpiQiDhQ7HkZoX/regularization-of-neural-networks-using-dropconnect](http://gitxiv.com/posts/rJucpiQiDhQ7HkZoX/regularization-of-neural-networks-using-dropconnect)\n- github: [https://github.com/iassael/torch-dropconnect](https://github.com/iassael/torch-dropconnect)\n\n**Regularizing neural networks with dropout and with DropConnect**\n\n- blog: [http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/](http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/)\n\n## DropNeuron\n\n**DropNeuron: Simplifying the Structure of Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1606.07326](http://arxiv.org/abs/1606.07326)\n- github: [https://github.com/panweihit/DropNeuron](https://github.com/panweihit/DropNeuron)\n\n## DropBlock\n\n**DropBlock: A regularization method for convolutional networks**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.12890](https://arxiv.org/abs/1810.12890)\n\n## Maxout\n\n**Maxout Networks**\n\n- intro: ICML 2013\n- intro: \"its output is the max of a set of inputs, a natural companion to dropout\"\n- project page: [http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html](http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html)\n- arxiv: [https://arxiv.org/abs/1302.4389](https://arxiv.org/abs/1302.4389)\n- github: [https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/models/maxout.py](https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/models/maxout.py)\n\n**Improving Deep Neural Networks with Probabilistic Maxout Units**\n\n- arxiv: [https://arxiv.org/abs/1312.6116](https://arxiv.org/abs/1312.6116)\n\n## Swapout\n\n**Swapout: Learning an ensemble of deep architectures**\n\n- arxiv: [https://arxiv.org/abs/1605.06465](https://arxiv.org/abs/1605.06465)\n- blog: [https://gab41.lab41.org/lab41-reading-group-swapout-learning-an-ensemble-of-deep-architectures-e67d2b822f8a#.9r2s4c58n](https://gab41.lab41.org/lab41-reading-group-swapout-learning-an-ensemble-of-deep-architectures-e67d2b822f8a#.9r2s4c58n)\n\n## Whiteout\n\n**Whiteout: Gaussian Adaptive Regularization Noise in Deep Neural Networks**\n\n- intro: University of Notre Dame & University of Science and Technology of China\n- arxiv: [https://arxiv.org/abs/1612.01490](https://arxiv.org/abs/1612.01490)\n\n**ShakeDrop regularization**\n\n[https://arxiv.org/abs/1802.02375](https://arxiv.org/abs/1802.02375)\n\n**Shakeout: A New Approach to Regularized Deep Neural Network Training**\n\n- intro: T-PAMI 2018\n- arxiv: [https://arxiv.org/abs/1904.06593](https://arxiv.org/abs/1904.06593)\n\n# Gradient Descent\n\n**RMSProp: Divide the gradient by a running average of its recent magnitude**\n\n![](/assets/train-dnn/rmsprop.jpg)\n\n- intro: it was not proposed in a paper, in fact it was just introduced in a slide in Geoffrey Hinton's Coursera class \n- slides: [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n\n**Fitting a model via closed-form equations vs. Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning. What is the difference?(Normal Equations vs. GD vs. SGD vs. MB-GD)**\n\n[http://sebastianraschka.com/faq/docs/closed-form-vs-gd.html](http://sebastianraschka.com/faq/docs/closed-form-vs-gd.html)\n\n**An Introduction to Gradient Descent in Python**\n\n- blog: [http://tillbergmann.com/blog/articles/python-gradient-descent.html](http://tillbergmann.com/blog/articles/python-gradient-descent.html)\n\n**Train faster, generalize better: Stability of stochastic gradient descent**\n\n- arxiv: [http://arxiv.org/abs/1509.01240](http://arxiv.org/abs/1509.01240)\n\n**A Variational Analysis of Stochastic Gradient Algorithms**\n\n- arxiv: [http://arxiv.org/abs/1602.02666](http://arxiv.org/abs/1602.02666)\n\n**The vanishing gradient problem: Oh no — an obstacle to deep learning!**\n\n- blog: [https://medium.com/a-year-of-artificial-intelligence/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.50hu5vwa8](https://medium.com/a-year-of-artificial-intelligence/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.50hu5vwa8)\n\n**Gradient Descent For Machine Learning**\n\n- blog: [http://machinelearningmastery.com/gradient-descent-for-machine-learning/](http://machinelearningmastery.com/gradient-descent-for-machine-learning/)\n\n**Revisiting Distributed Synchronous SGD**\n\n- arxiv: [http://arxiv.org/abs/1604.00981](http://arxiv.org/abs/1604.00981)\n\n**Convergence rate of gradient descent**\n\n- blog: [https://building-babylon.net/2016/06/23/convergence-rate-of-gradient-descent/](https://building-babylon.net/2016/06/23/convergence-rate-of-gradient-descent/)\n\n**A Robust Adaptive Stochastic Gradient Method for Deep Learning**\n\n- intro: IJCNN 2017 Accepted Paper, An extension of paper, \"ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient\"\n- intro: Universite de Montreal & University of Oxford\n- arxiv: [https://arxiv.org/abs/1703.00788](https://arxiv.org/abs/1703.00788)\n\n**Accelerating Stochastic Gradient Descent**\n\n[https://arxiv.org/abs/1704.08227](https://arxiv.org/abs/1704.08227)\n\n**Gentle Introduction to the Adam Optimization Algorithm for Deep Learning**\n\n- blog: [http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/](http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n\n**Understanding Generalization and Stochastic Gradient Descent**\n\n**A Bayesian Perspective on Generalization and Stochastic Gradient Descent**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1710.06451](https://arxiv.org/abs/1710.06451)\n\n**Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent**\n\n- intro: UC Berkeley & Microsoft Research, India\n- arxiv: [https://arxiv.org/abs/1711.10456](https://arxiv.org/abs/1711.10456)\n\n**Improving Generalization Performance by Switching from Adam to SGD**\n\n[https://arxiv.org/abs/1712.07628](https://arxiv.org/abs/1712.07628)\n\n**Laplacian Smoothing Gradient Descent**\n\n- intro: UCLA\n- arxiv: [https://arxiv.org/abs/1806.06317](https://arxiv.org/abs/1806.06317)\n\n## AdaGrad\n\n**Adaptive Subgradient Methods for Online Learning and Stochastic Optimization**\n\n- paper: [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n\n**ADADELTA: An Adaptive Learning Rate Method**\n\n- arxiv: [http://arxiv.org/abs/1212.5701](http://arxiv.org/abs/1212.5701)\n\n## Momentum\n\n**On the importance of initialization and momentum in deep learning**\n\n- intro:  NAG: Nesterov\n- paper: [http://www.cs.toronto.edu/~fritz/absps/momentum.pdf](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n- paper: [http://jmlr.org/proceedings/papers/v28/sutskever13.pdf](http://jmlr.org/proceedings/papers/v28/sutskever13.pdf)\n\n**YellowFin and the Art of Momentum Tuning**\n\n- intro: Stanford University\n- intro: auto-tuning momentum SGD optimizer\n- project page: [http://cs.stanford.edu/~zjian/project/YellowFin/](http://cs.stanford.edu/~zjian/project/YellowFin/)\n- arxiv: [https://arxiv.org/abs/1706.03471](https://arxiv.org/abs/1706.03471)\n- github(TensorFlow): [https://github.com/JianGoForIt/YellowFin](https://github.com/JianGoForIt/YellowFin)\n[https://github.com/JianGoForIt/YellowFin_Pytorch](https://github.com/JianGoForIt/YellowFin_Pytorch)\n\n# Backpropagation\n\n**Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks**\n\n- intro: ECCV 2016. first place of ILSVRC 2015 Scene Classification Challenge\n- arxiv: [https://arxiv.org/abs/1512.05830](https://arxiv.org/abs/1512.05830)\n- paper: [http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2016-ECCV-RelayBP.pdf](http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2016-ECCV-RelayBP.pdf)\n\n**Top-down Neural Attention by Excitation Backprop**\n\n![](http://cs-people.bu.edu/jmzhang/images/screen%20shot%202016-08-19%20at%2035847%20pm.jpg?crc=3911895888)\n\n- intro: ECCV, 2016 (oral)\n- projpage: [http://cs-people.bu.edu/jmzhang/excitationbp.html](http://cs-people.bu.edu/jmzhang/excitationbp.html)\n- arxiv: [http://arxiv.org/abs/1608.00507](http://arxiv.org/abs/1608.00507)\n- paper: [http://cs-people.bu.edu/jmzhang/EB/ExcitationBackprop.pdf](http://cs-people.bu.edu/jmzhang/EB/ExcitationBackprop.pdf)\n- github: [https://github.com/jimmie33/Caffe-ExcitationBP](https://github.com/jimmie33/Caffe-ExcitationBP)\n\n**Towards a Biologically Plausible Backprop**\n\n- arxiv: [http://arxiv.org/abs/1602.05179](http://arxiv.org/abs/1602.05179)\n- github: [https://github.com/bscellier/Towards-a-Biologically-Plausible-Backprop](https://github.com/bscellier/Towards-a-Biologically-Plausible-Backprop)\n\n**Sampled Backpropagation: Training Deep and Wide Neural Networks on Large Scale, User Generated Content Using Label Sampling**\n\n- blog: [https://medium.com/@karl1980.lab41/sampled-backpropagation-27ac58d5c51c#.xnbhyxtou](https://medium.com/@karl1980.lab41/sampled-backpropagation-27ac58d5c51c#.xnbhyxtou)\n\n**The Reversible Residual Network: Backpropagation Without Storing Activations**\n\n- intro: CoRR 2017. University of Toronto\n- arxiv: [https://arxiv.org/abs/1707.04585](https://arxiv.org/abs/1707.04585)\n- github: [https://github.com/renmengye/revnet-public](https://github.com/renmengye/revnet-public)\n\n**meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting**\n\n- intro: ICML 2017\n- arxiv: [https://arxiv.org/abs/1706.06197](https://arxiv.org/abs/1706.06197)\n- github: [https://github.com//jklj077/meProp](https://github.com//jklj077/meProp)\n\n# Accelerate Training\n\n**Neural Networks with Few Multiplications**\n\n- intro:  ICLR 2016\n- arxiv: [https://arxiv.org/abs/1510.03009](https://arxiv.org/abs/1510.03009)\n\n**Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices**\n\n- arxiv: [http://arxiv.org/abs/1603.07341](http://arxiv.org/abs/1603.07341)\n\n**Deep Q-Networks for Accelerating the Training of Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1606.01467](http://arxiv.org/abs/1606.01467)\n- github: [https://github.com/bigaidream-projects/qan](https://github.com/bigaidream-projects/qan)\n\n**Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs**\n\n- arxiv: [http://arxiv.org/abs/1606.04487](http://arxiv.org/abs/1606.04487)\n\n## Parallelism\n\n**One weird trick for parallelizing convolutional neural networks**\n\n- author: Alex Krizhevsky\n- arxiv: [http://arxiv.org/abs/1404.5997](http://arxiv.org/abs/1404.5997)\n\n**8-Bit Approximations for Parallelism in Deep Learning (ICLR 2016)**\n\n- arxiv: [http://arxiv.org/abs/1511.04561](http://arxiv.org/abs/1511.04561)\n\n# Handling Datasets\n\n## Data Augmentation\n\n**DataAugmentation ver1.0: Image data augmentation tool for training of image recognition algorithm**\n\n- github: [https://github.com/takmin/DataAugmentation](https://github.com/takmin/DataAugmentation)\n\n**Caffe-Data-Augmentation: a branc caffe with feature of Data Augmentation using a configurable stochastic combination of 7 data augmentation techniques**\n\n- github: [https://github.com/ShaharKatz/Caffe-Data-Augmentation](https://github.com/ShaharKatz/Caffe-Data-Augmentation)\n\n**Image Augmentation for Deep Learning With Keras**\n\n- blog: [http://machinelearningmastery.com/image-augmentation-deep-learning-keras/](http://machinelearningmastery.com/image-augmentation-deep-learning-keras/)\n\n**What you need to know about data augmentation for machine learning**\n\n- intro: keras Imagegenerator\n- blog: [https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/](https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/)\n\n**HZPROC: torch data augmentation toolbox (supports affine transform)**\n\n- github: [https://github.com/zhanghang1989/hzproc](https://github.com/zhanghang1989/hzproc)\n\n**AGA: Attribute Guided Augmentation**\n\n- intro: one-shot recognition\n- arxiv: [https://arxiv.org/abs/1612.02559](https://arxiv.org/abs/1612.02559)\n\n**Accelerating Deep Learning with Multiprocess Image Augmentation in Keras**\n\n- blog: [http://blog.stratospark.com/multiprocess-image-augmentation-keras.html](http://blog.stratospark.com/multiprocess-image-augmentation-keras.html)\n- github: [https://github.com/stratospark/keras-multiprocess-image-data-generator](https://github.com/stratospark/keras-multiprocess-image-data-generator)\n\n**Comprehensive Data Augmentation and Sampling for Pytorch**\n\n- github: [https://github.com/ncullen93/torchsample](https://github.com/ncullen93/torchsample)\n\n**Image augmentation for machine learning experiments.**\n\n[https://github.com/aleju/imgaug](https://github.com/aleju/imgaug)\n\n**Google/inception's data augmentation: scale and aspect ratio augmentation**\n\n[https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L130](https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L130)\n\n**Caffe Augmentation Extension**\n\n- intro: Data Augmentation for Caffe\n- github: [https://github.com/twtygqyy/caffe-augmentation](https://github.com/twtygqyy/caffe-augmentation)\n\n**Improving Deep Learning using Generic Data Augmentation**\n\n- intro: University of Cape Town\n- arxiv: [https://arxiv.org/abs/1708.06020](https://arxiv.org/abs/1708.06020)\n- github: [https://github.com/webstorms/AugmentedDatasets](https://github.com/webstorms/AugmentedDatasets)\n\n**Augmentor: An Image Augmentation Library for Machine Learning**\n\n- arxiv: [https://arxiv.org/abs/1708.04680](https://arxiv.org/abs/1708.04680)\n- github: [https://github.com/mdbloice/Augmentor](https://github.com/mdbloice/Augmentor)\n\n**Automatic Dataset Augmentation**\n\n- project page: [https://auto-da.github.io/](https://auto-da.github.io/)\n- arxiv: [https://arxiv.org/abs/1708.08201](https://arxiv.org/abs/1708.08201)\n\n**Learning to Compose Domain-Specific Transformations for Data Augmentation**\n\n[https://arxiv.org/abs/1709.01643](https://arxiv.org/abs/1709.01643)\n\n**Data Augmentation in Classification using GAN**\n\n[https://arxiv.org/abs/1711.00648](https://arxiv.org/abs/1711.00648)\n\n**Data Augmentation Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1711.04340](https://arxiv.org/abs/1711.04340)\n\n**Random Erasing Data Augmentation**\n\n- arxiv: [https://arxiv.org/abs/1708.04896](https://arxiv.org/abs/1708.04896)\n- github: [https://github.com/zhunzhong07/Random-Erasing](https://github.com/zhunzhong07/Random-Erasing)\n\n**Context Augmentation for Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1712.01653](https://arxiv.org/abs/1712.01653)\n\n**The Effectiveness of Data Augmentation in Image Classification using Deep Learning**\n\n[https://arxiv.org/abs/1712.04621](https://arxiv.org/abs/1712.04621)\n\n**MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels**\n\n- intro: Google Inc & Stanford University\n- arxiv: [https://arxiv.org/abs/1712.05055](https://arxiv.org/abs/1712.05055)\n\n**mixup: Beyond Empirical Risk Minimization**\n\n- intro: MIT & FAIR\n- arxiv: [https://arxiv.org/abs/1710.09412](https://arxiv.org/abs/1710.09412)\n- github: [https://github.com//leehomyc/mixup_pytorch](https://github.com//leehomyc/mixup_pytorch)\n- github: [https://github.com//unsky/mixup](https://github.com//unsky/mixup)\n\n**mixup: Data-Dependent Data Augmentation**\n\n[http://www.inference.vc/mixup-data-dependent-data-augmentation/](http://www.inference.vc/mixup-data-dependent-data-augmentation/)\n\n**Data Augmentation by Pairing Samples for Images Classification**\n\n- intro: IBM Research - Tokyo\n- arxiv: [https://arxiv.org/abs/1801.02929](https://arxiv.org/abs/1801.02929)\n\n**Feature Space Transfer for Data Augmentation**\n\n- keywords: eATure TransfEr Network (FATTEN)\n- arxiv: [https://arxiv.org/abs/1801.04356](https://arxiv.org/abs/1801.04356)\n\n**Visual Data Augmentation through Learning**\n\n[https://arxiv.org/abs/1801.06665](https://arxiv.org/abs/1801.06665)\n\n**Data Augmentation Generative Adversarial Networks**\n\n- arxiv: [https://arxiv.org/abs/1711.04340](https://arxiv.org/abs/1711.04340)\n- github: [https://github.com/AntreasAntoniou/DAGAN](https://github.com/AntreasAntoniou/DAGAN)\n\n**BAGAN: Data Augmentation with Balancing GAN**\n\n[https://arxiv.org/abs/1803.09655](https://arxiv.org/abs/1803.09655)\n\n**Parallel Grid Pooling for Data Augmentation**\n\n- intro: The University of Tokyo & NTT Communications Science Laboratories\n- arxiv: [https://arxiv.org/abs/1803.11370](https://arxiv.org/abs/1803.11370)\n- github(Chainer): [https://github.com/akitotakeki/pgp-chainer](https://github.com/akitotakeki/pgp-chainer)\n\n**AutoAugment: Learning Augmentation Policies from Data**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1805.09501](https://arxiv.org/abs/1805.09501)\n- github: [https://github.com/DeepVoltaire/AutoAugment](https://github.com/DeepVoltaire/AutoAugment)\n\n**Improved Mixed-Example Data Augmentation**\n\n[https://arxiv.org/abs/1805.11272](https://arxiv.org/abs/1805.11272)\n\n**Data augmentation instead of explicit regularization**\n\n[https://arxiv.org/abs/1806.03852](https://arxiv.org/abs/1806.03852)\n\n**Data Augmentation using Random Image Cropping and Patching for Deep CNNs**\n\n- intro: An extended version of a proceeding of ACML2018\n- keywords: random image cropping and patching (RICAP)\n- arxiv: [https://arxiv.org/abs/1811.09030](https://arxiv.org/abs/1811.09030)\n\n**GANsfer Learning: Combining labelled and unlabelled data for GAN based data augmentat**\n\n[https://arxiv.org/abs/1811.10669](https://arxiv.org/abs/1811.10669)\n\n**Adversarial Learning of General Transformations for Data Augmentation**\n\n- intro: Ecole de Technologie Sup ´ erieure & Element AI\n- arxiv: [https://arxiv.org/abs/1909.09801](https://arxiv.org/abs/1909.09801)\n\n**Implicit Semantic Data Augmentation for Deep Networks**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1909.12220](https://arxiv.org/abs/1909.12220)\n- github(official): [https://github.com/blackfeather-wang/ISDA-for-Deep-Networks](https://github.com/blackfeather-wang/ISDA-for-Deep-Networks)\n\n**Data Augmentation Revisited: Rethinking the Distribution Gap between Clean and Augmented Data**\n\n[https://arxiv.org/abs/1909.09148](https://arxiv.org/abs/1909.09148)\n\n**AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty**\n\n- intro: ICLR 2020\n- intro: Google & Deepmind\n- arxiv: [https://arxiv.org/abs/1912.02781](https://arxiv.org/abs/1912.02781)\n- github: [https://github.com/google-research/augmix](https://github.com/google-research/augmix)\n\n**GridMask Data Augmentation**\n\n[https://arxiv.org/abs/2001.04086](https://arxiv.org/abs/2001.04086)\n\n**On Feature Normalization and Data Augmentation**\n\n- intro: Cornell University & Cornell Tech & ASAPP Inc. & Facebook AI\n- keywords: MoEx (Moment Exchange)\n- arxiv: [https://arxiv.org/abs/2002.11102](https://arxiv.org/abs/2002.11102)\n- github: [https://github.com/Boyiliee/MoEx](https://github.com/Boyiliee/MoEx)\n\n**DADA: Differentiable Automatic Data Augmentation**\n\n[https://arxiv.org/abs/2003.03780](https://arxiv.org/abs/2003.03780)\n\n**Negative Data Augmentation**\n\n- intro: ICLR 2021\n- intro: Stanford University & Samsung Research America\n- arxiv: [https://arxiv.org/abs/2102.05113](https://arxiv.org/abs/2102.05113)\n\n## Imbalanced Datasets\n\n**Investigation on handling Structured & Imbalanced Datasets with Deep Learning**\n\n- intro: smote resampling, cost sensitive learning\n- blog: [https://www.analyticsvidhya.com/blog/2016/10/investigation-on-handling-structured-imbalanced-datasets-with-deep-learning/](https://www.analyticsvidhya.com/blog/2016/10/investigation-on-handling-structured-imbalanced-datasets-with-deep-learning/)\n\n**A systematic study of the class imbalance problem in convolutional neural networks**\n\n- intro: Duke University & Royal Institute of Technology (KTH)\n- arxiv: [https://arxiv.org/abs/1710.05381](https://arxiv.org/abs/1710.05381)\n\n**Class Rectification Hard Mining for Imbalanced Deep Learning**\n\n[https://arxiv.org/abs/1712.03162](https://arxiv.org/abs/1712.03162)\n\n**Bridging the Gap: Simultaneous Fine Tuning for Data Re-Balancing**\n\n- arxiv: [https://arxiv.org/abs/1801.02548](https://arxiv.org/abs/1801.02548)\n- github: [https://github.com/JohnMcKay/dataImbalance](https://github.com/JohnMcKay/dataImbalance)\n\n**Imbalanced Deep Learning by Minority Class Incremental Rectification**\n\n- intro: TPAMI\n- arxiv: [https://arxiv.org/abs/1804.10851](https://arxiv.org/abs/1804.10851)\n\n**Pseudo-Feature Generation for Imbalanced Data Analysis in Deep Learning**\n\n- intro: National Institute of Information and Communications Technology, Tokyo Japan\n- arxiv: [https://arxiv.org/abs/1807.06538](https://arxiv.org/abs/1807.06538)\n- slides: [https://www.slideshare.net/TomohikoKonno/pseudofeature-generation-for-imbalanced-data-analysis-in-deep-learning-tomohiko-105318569](https://www.slideshare.net/TomohikoKonno/pseudofeature-generation-for-imbalanced-data-analysis-in-deep-learning-tomohiko-105318569)\n\n**Max-margin Class Imbalanced Learning with Gaussian Affinity**\n\n[https://arxiv.org/abs/1901.07711](https://arxiv.org/abs/1901.07711)\n\n**Dynamic Curriculum Learning for Imbalanced Data Classification**\n\n- intro: ICCV 2019\n- intro: SenseTime\n- arxiv: [https://arxiv.org/abs/1901.06783](https://arxiv.org/abs/1901.06783)\n\n**Class Rectification Hard Mining for Imbalanced Deep Learning**\n\n- intro: ICCV 2017\n- paper: [https://www.eecs.qmul.ac.uk/~sgg/papers/DongEtAl_ICCV2017.pdf](https://www.eecs.qmul.ac.uk/~sgg/papers/DongEtAl_ICCV2017.pdf)\n\n## Noisy / Unlabelled Data\n\n**Data Distillation: Towards Omni-Supervised Learning**\n\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/1712.04440](https://arxiv.org/abs/1712.04440)\n\n**Learning From Noisy Singly-labeled Data**\n\n- intro: University of Illinois Urbana Champaign & CMU & Caltech & Amazon AI\n- arxiv: [https://arxiv.org/abs/1712.04577](https://arxiv.org/abs/1712.04577)\n\n# Low Numerical Precision\n\n**Training deep neural networks with low precision multiplications**\n\n- intro: ICLR 2015\n- intro: Maxout networks, 10-bit activations, 12-bit parameter updates\n- arxiv: [http://arxiv.org/abs/1412.7024](http://arxiv.org/abs/1412.7024)\n- github: [https://github.com/MatthieuCourbariaux/deep-learning-multipliers](https://github.com/MatthieuCourbariaux/deep-learning-multipliers)\n\n**Deep Learning with Limited Numerical Precision**\n\n- intro: ICML 2015\n- arxiv: [http://arxiv.org/abs/1502.02551](http://arxiv.org/abs/1502.02551)\n\n**BinaryConnect: Training Deep Neural Networks with binary weights during propagations**\n\n- paper: [http://papers.nips.cc/paper/5647-shape-and-illumination-from-shading-using-the-generic-viewpoint-assumption](http://papers.nips.cc/paper/5647-shape-and-illumination-from-shading-using-the-generic-viewpoint-assumption)\n- github: [https://github.com/MatthieuCourbariaux/BinaryConnect](https://github.com/MatthieuCourbariaux/BinaryConnect)\n\n**Binarized Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1602.02505](http://arxiv.org/abs/1602.02505)\n\n**BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**\n\n**Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**\n\n- arxiv: [http://arxiv.org/abs/1602.02830](http://arxiv.org/abs/1602.02830)\n- github: [https://github.com/MatthieuCourbariaux/BinaryNet](https://github.com/MatthieuCourbariaux/BinaryNet)\n- github: [https://github.com/codekansas/tinier-nn](https://github.com/codekansas/tinier-nn)\n\n**Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations**\n\n- arxiv: [http://arxiv.org/abs/1609.07061](http://arxiv.org/abs/1609.07061)\n\n# Distributed Training\n\n**Large Scale Distributed Systems for Training Neural Networks**\n\n- intro: By Jeff Dean & Oriol Vinyals, Google. NIPS 2015.\n- slides: [https://media.nips.cc/Conferences/2015/tutorialslides/Jeff-Oriol-NIPS-Tutorial-2015.pdf](https://media.nips.cc/Conferences/2015/tutorialslides/Jeff-Oriol-NIPS-Tutorial-2015.pdf)\n- video: [http://research.microsoft.com/apps/video/default.aspx?id=259564&l=i](http://research.microsoft.com/apps/video/default.aspx?id=259564&l=i)\n- mirror: [http://pan.baidu.com/s/1mgXV0hU](http://pan.baidu.com/s/1mgXV0hU)\n\n**Large Scale Distributed Deep Networks**\n\n- intro: distributed CPU training, data parallelism, model parallelism\n- paper: [http://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf](http://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf)\n- slides: [http://admis.fudan.edu.cn/~yfhuang/files/LSDDN_slide.pdf](http://admis.fudan.edu.cn/~yfhuang/files/LSDDN_slide.pdf)\n\n**Implementation of a Practical Distributed Calculation System with Browsers and JavaScript, and Application to Distributed Deep Learning**\n\n- project page: [http://mil-tokyo.github.io/](http://mil-tokyo.github.io/)\n- arxiv: [https://arxiv.org/abs/1503.05743](https://arxiv.org/abs/1503.05743)\n\n**SparkNet: Training Deep Networks in Spark**\n\n- arxiv: [http://arxiv.org/abs/1511.06051](http://arxiv.org/abs/1511.06051)\n- github: [https://github.com/amplab/SparkNet](https://github.com/amplab/SparkNet)\n- blog: [http://www.kdnuggets.com/2015/12/spark-deep-learning-training-with-sparknet.html](http://www.kdnuggets.com/2015/12/spark-deep-learning-training-with-sparknet.html)\n\n**A Scalable Implementation of Deep Learning on Spark**\n\n- intro: Alexander Ulanov\n- slides: [http://www.slideshare.net/AlexanderUlanov1/a-scalable-implementation-of-deep-learning-on-spark-alexander-ulanov](http://www.slideshare.net/AlexanderUlanov1/a-scalable-implementation-of-deep-learning-on-spark-alexander-ulanov)\n- mirror: [http://pan.baidu.com/s/1jHiNW5C](http://pan.baidu.com/s/1jHiNW5C)\n\n**TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems**\n\n- arxiv: [http://arxiv.org/abs/1603.04467](http://arxiv.org/abs/1603.04467)\n- gitxiv: [http://gitxiv.com/posts/57kjddp3AWt4y5K4h/tensorflow-large-scale-machine-learning-on-heterogeneous](http://gitxiv.com/posts/57kjddp3AWt4y5K4h/tensorflow-large-scale-machine-learning-on-heterogeneous)\n\n**Distributed Supervised Learning using Neural Networks**\n\n- intro: Ph.D. thesis\n- arxiv: [http://arxiv.org/abs/1607.06364](http://arxiv.org/abs/1607.06364)\n\n**Distributed Training of Deep Neuronal Networks: Theoretical and Practical Limits of Parallel Scalability**\n\n- arxiv: [http://arxiv.org/abs/1609.06870](http://arxiv.org/abs/1609.06870)\n\n**How to scale distributed deep learning?**\n\n- intro: Extended version of paper accepted at ML Sys 2016 (at NIPS 2016)\n- arxiv: [https://arxiv.org/abs/1611.04581](https://arxiv.org/abs/1611.04581)\n\n**Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training**\n\n- intro: Tsinghua University & Stanford University\n- comments: we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy\n- keywords: momentum correction, local gradient clipping, momentum factor masking, and warm-up training\n- arxiv: [https://arxiv.org/abs/1712.01887](https://arxiv.org/abs/1712.01887)\n\n**Distributed learning of CNNs on heterogeneous CPU/GPU architectures**\n\n[https://arxiv.org/abs/1712.02546](https://arxiv.org/abs/1712.02546)\n\n**Integrated Model and Data Parallelism in Training Neural Networks**\n\n- intro: UC Berkeley & Lawrence Berkeley National Laboratory\n- arxiv: [https://arxiv.org/abs/1712.04432](https://arxiv.org/abs/1712.04432)\n\n**Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training**\n\n- intro: ICLR 2018\n- intro: we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy\n- arxiv: [https://arxiv.org/abs/1712.01887](https://arxiv.org/abs/1712.01887)\n\n**RedSync : Reducing Synchronization Traffic for Distributed Deep Learning**\n\n[https://arxiv.org/abs/1808.04357](https://arxiv.org/abs/1808.04357)\n\n## Projects\n\n**Theano-MPI: a Theano-based Distributed Training Framework**\n\n- arxiv: [https://arxiv.org/abs/1605.08325](https://arxiv.org/abs/1605.08325)\n- github: [https://github.com/uoguelph-mlrg/Theano-MPI](https://github.com/uoguelph-mlrg/Theano-MPI)\n\n**CaffeOnSpark: Open Sourced for Distributed Deep Learning on Big Data Clusters**\n\n- intro: Yahoo Big ML Team\n- blog: [http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep](http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep)\n- github: [https://github.com/yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark)\n- youtube: [https://www.youtube.com/watch?v=bqj7nML-aHk](https://www.youtube.com/watch?v=bqj7nML-aHk)\n\n**Tunnel: Data Driven Framework for Distributed Computing in Torch 7**\n\n- github: [https://github.com/zhangxiangxiao/tunnel](https://github.com/zhangxiangxiao/tunnel)\n\n**Distributed deep learning with Keras and Apache Spark**\n\n- project page: [http://joerihermans.com/work/distributed-keras/](http://joerihermans.com/work/distributed-keras/)\n- github: [https://github.com/JoeriHermans/dist-keras](https://github.com/JoeriHermans/dist-keras)\n\n**BigDL: Distributed Deep learning Library for Apache Spark**\n\n- github: [https://github.com/intel-analytics/BigDL](https://github.com/intel-analytics/BigDL)\n\n## Videos\n\n**A Scalable Implementation of Deep Learning on Spark**\n\n- youtube: [https://www.youtube.com/watch?v=pNYBBhuK8yU](https://www.youtube.com/watch?v=pNYBBhuK8yU)\n- mirror: [http://pan.baidu.com/s/1mhzF1uK](http://pan.baidu.com/s/1mhzF1uK)\n\n**Distributed TensorFlow on Spark: Scaling Google's Deep Learning Library (Spark Summit)**\n\n- youtube: [https://www.youtube.com/watch?v=-QtcP3yRqyM](https://www.youtube.com/watch?v=-QtcP3yRqyM)\n- mirror: [http://pan.baidu.com/s/1mgOR1GG](http://pan.baidu.com/s/1mgOR1GG)\n\n**Deep Recurrent Neural Networks for Sequence Learning in Spark (Spark Summit)**\n\n- youtube: [https://www.youtube.com/watch?v=mUuqLcl8Jog](https://www.youtube.com/watch?v=mUuqLcl8Jog)\n- mirror: [http://pan.baidu.com/s/1sklHTPr](http://pan.baidu.com/s/1sklHTPr)\n\n**Distributed deep learning on Spark**\n\n- author: Alexander Ulanov July 12, 2016\n- intro: Alexander Ulanov offers an overview of tools and frameworks that have been proposed for performing deep learning on Spark.\n- video: [https://www.oreilly.com/learning/distributed-deep-learning-on-spark](https://www.oreilly.com/learning/distributed-deep-learning-on-spark)\n\n## Blogs\n\n**Distributed Deep Learning Reads**\n\n[https://github.com//tmulc18/DistributedDeepLearningReads](https://github.com//tmulc18/DistributedDeepLearningReads)\n\n**Hadoop, Spark, Deep Learning Mesh on Single GPU Cluster**\n\n[http://www.nextplatform.com/2016/02/24/hadoop-spark-deep-learning-mesh-on-single-gpu-cluster/](http://www.nextplatform.com/2016/02/24/hadoop-spark-deep-learning-mesh-on-single-gpu-cluster/)\n\n**The Unreasonable Effectiveness of Deep Learning on Spark**\n\n[https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html](https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html)\n\n**Distributed Deep Learning with Caffe Using a MapR Cluster**\n\n![](https://www.mapr.com/sites/default/files/spark-driver.jpg)\n\n[https://www.mapr.com/blog/distributed-deep-learning-caffe-using-mapr-cluster](https://www.mapr.com/blog/distributed-deep-learning-caffe-using-mapr-cluster)\n\n**Deep Learning with Apache Spark and TensorFlow**\n\n[https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)\n\n**Deeplearning4j on Spark**\n\n[http://deeplearning4j.org/spark](http://deeplearning4j.org/spark)\n\n**Distributed Deep Learning, Part 1: An Introduction to Distributed Training of Neural Networks**\n\n- blog: [http://engineering.skymind.io/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks](http://engineering.skymind.io/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks)\n\n**GPU Acceleration in Databricks: Speeding Up Deep Learning on Apache Spark**\n\n[https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html](https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html)\n\n**Distributed Deep Learning with Apache Spark and Keras**\n\n[https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras](https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras)\n\n# Adversarial Training\n\n**Learning from Simulated and Unsupervised Images through Adversarial Training**\n\n- intro: CVPR 2017 oral, best paper award. Apple Inc.\n- arxiv: [https://arxiv.org/abs/1612.07828](https://arxiv.org/abs/1612.07828)\n\n**The Robust Manifold Defense: Adversarial Training using Generative Models**\n\n[https://arxiv.org/abs/1712.09196](https://arxiv.org/abs/1712.09196)\n\n**DeepDefense: Training Deep Neural Networks with Improved Robustness**\n\n[https://arxiv.org/abs/1803.00404](https://arxiv.org/abs/1803.00404)\n\n**Gradient Adversarial Training of Neural Networks**\n\n- intro: Magic Leap\n- arxiv: [https://arxiv.org/abs/1806.08028](https://arxiv.org/abs/1806.08028)\n\n**Gray-box Adversarial Training**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.01753](https://arxiv.org/abs/1808.01753)\n\n**Universal Adversarial Training**\n\n[https://arxiv.org/abs/1811.11304](https://arxiv.org/abs/1811.11304)\n\n**MEAL: Multi-Model Ensemble via Adversarial Learning**\n\n- intro: AAAI 2019\n- intro: Fudan University & University of Illinois at Urbana-Champaign\n- arxiv: [https://arxiv.org/abs/1812.02425](https://arxiv.org/abs/1812.02425)\n- github(official): [https://github.com/AaronHeee/MEAL](https://github.com/AaronHeee/MEAL)\n\n**Regularized Ensembles and Transferability in Adversarial Learning**\n\n[https://arxiv.org/abs/1812.01821](https://arxiv.org/abs/1812.01821)\n\n**Feature denoising for improving adversarial robustness**\n\n- intro: Johns Hopkins University & Facebook AI Research\n- intro: ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018\n- arxiv: [https://arxiv.org/abs/1812.03411](https://arxiv.org/abs/1812.03411)\n- github: [https://github.com/facebookresearch/ImageNet-Adversarial-Training](https://github.com/facebookresearch/ImageNet-Adversarial-Training)\n\n**Second Rethinking of Network Pruning in the Adversarial Setting**\n\n[https://arxiv.org/abs/1903.12561](https://arxiv.org/abs/1903.12561)\n\n**Interpreting Adversarially Trained Convolutional Neural Networks**\n\n- intro: ICML 2019\n- arxiv: [https://arxiv.org/abs/1905.09797](https://arxiv.org/abs/1905.09797)\n\n**On Stabilizing Generative Adversarial Training with Noise**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1906.04612](https://arxiv.org/abs/1906.04612)\n\n**Adversarial Learning with Margin-based Triplet Embedding Regularization**\n\n- intro: ICCV 2019\n- intro: BUPT\n- arxiv: [https://arxiv.org/abs/1909.09481](https://arxiv.org/abs/1909.09481)\n- github: [https://github.com/zhongyy/Adversarial_MTER](https://github.com/zhongyy/Adversarial_MTER)\n\n**Bag of Tricks for Adversarial Training**\n\n- intro: Tsinghua University\n- arxiv: [https://arxiv.org/abs/2010.00467](https://arxiv.org/abs/2010.00467)\n\n# Low-Precision Training\n\n**Mixed Precision Training**\n\n- intro: ICLR 2018\n- arxiv: [https://arxiv.org/abs/1710.03740](https://arxiv.org/abs/1710.03740)\n\n**High-Accuracy Low-Precision Training**\n\n- intro: Cornell University & Stanford University\n- arxiv: [https://arxiv.org/abs/1803.03383](https://arxiv.org/abs/1803.03383)\n\n# Incremental Training\n\n**ClickBAIT: Click-based Accelerated Incremental Training of Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1709.05021](https://arxiv.org/abs/1709.05021)\n- dataset: [http://clickbait.crossmobile.info/](http://clickbait.crossmobile.info/)\n\n**ClickBAIT-v2: Training an Object Detector in Real-Time**\n\n[https://arxiv.org/abs/1803.10358](https://arxiv.org/abs/1803.10358)\n\n**Class-incremental Learning via Deep Model Consolidation**\n\n- intro: University of Southern California & Arizona State University & Samsung Research America\n- arxiv: [https://arxiv.org/abs/1903.07864](https://arxiv.org/abs/1903.07864)\n\n# Papers\n\n**Understanding the difficulty of training deep feed forward neural networks**\n\n- intro: Xavier initialization\n- paper: [http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n\n**Domain-Adversarial Training of Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1505.07818](https://arxiv.org/abs/1505.07818)\n- paper: [http://jmlr.org/papers/v17/15-239.html](http://jmlr.org/papers/v17/15-239.html)\n- github: [https://github.com/pumpikano/tf-dann](https://github.com/pumpikano/tf-dann)\n\n**Scalable and Sustainable Deep Learning via Randomized Hashing**\n\n- arxiv: [http://arxiv.org/abs/1602.08194](http://arxiv.org/abs/1602.08194)\n\n**Training Deep Nets with Sublinear Memory Cost**\n\n- arxiv: [https://arxiv.org/abs/1604.06174](https://arxiv.org/abs/1604.06174)\n- github: [https://github.com/dmlc/mxnet-memonger](https://github.com/dmlc/mxnet-memonger)\n- github: [https://github.com/Bihaqo/tf-memonger](https://github.com/Bihaqo/tf-memonger)\n\n**Improving the Robustness of Deep Neural Networks via Stability Training**\n\n- arxiv: [http://arxiv.org/abs/1604.04326](http://arxiv.org/abs/1604.04326)\n\n**Faster Training of Very Deep Networks Via p-Norm Gates**\n\n- arxiv: [http://arxiv.org/abs/1608.03639](http://arxiv.org/abs/1608.03639)\n\n**Fast Training of Convolutional Neural Networks via Kernel Rescaling**\n\n- arxiv: [https://arxiv.org/abs/1610.03623](https://arxiv.org/abs/1610.03623)\n\n**FreezeOut: Accelerate Training by Progressively Freezing Layers**\n\n- arxiv: [https://arxiv.org/abs/1706.04983](https://arxiv.org/abs/1706.04983)\n- github: [https://github.com/ajbrock/FreezeOut](https://github.com/ajbrock/FreezeOut)\n\n**Normalized Gradient with Adaptive Stepsize Method for Deep Neural Network Training**\n\n- intro: CMU & The University of Iowa\n- arxiv: [https://arxiv.org/abs/1707.04822](https://arxiv.org/abs/1707.04822)\n\n**Image Quality Assessment Guided Deep Neural Networks Training**\n\n[https://arxiv.org/abs/1708.03880](https://arxiv.org/abs/1708.03880)\n\n**An Effective Training Method For Deep Convolutional Neural Network**\n\n- intro: Beijing Institute of Technology & Tsinghua University\n- arxiv: [https://arxiv.org/abs/1708.01666](https://arxiv.org/abs/1708.01666)\n\n**On the Importance of Consistency in Training Deep Neural Networks**\n\n- intro: University of Maryland & Arizona State University\n- arxiv: [https://arxiv.org/abs/1708.00631](https://arxiv.org/abs/1708.00631)\n\n**Solving internal covariate shift in deep learning with linked neurons**\n\n- intro: Universitat de Barcelona\n- arxiv: [https://arxiv.org/abs/1712.02609](https://arxiv.org/abs/1712.02609)\n- github: [https://github.com/blauigris/linked_neurons](https://github.com/blauigris/linked_neurons)\n\n# Tools\n\n**pastalog: Simple, realtime visualization of neural network training performance**\n\n![](/assets/train-dnn/pastalog-main-big.gif)\n\n- github: [https://github.com/rewonc/pastalog](https://github.com/rewonc/pastalog)\n\n**torch-pastalog: A Torch interface for pastalog - simple, realtime visualization of neural network training performance**\n\n- github: [https://github.com/Kaixhin/torch-pastalog](https://github.com/Kaixhin/torch-pastalog)\n\n# Blogs\n\n**Important nuances to train deep learning models**\n\n[http://www.erogol.com/important-nuances-train-deep-learning-models/](http://www.erogol.com/important-nuances-train-deep-learning-models/)\n\n**Train your deep model faster and sharper — two novel techniques**\n\n[https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047](https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","title":"Machine Learning Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: machine_learning\ntitle: Machine Learning Resources\ndate: 2015-08-27\n---\n\n# Tutorials\n\n**Machine Learning for Developers**\n\n[http://xyclade.github.io/MachineLearning/](http://xyclade.github.io/MachineLearning/)\n\n**Logistic Regression Vs Decision Trees Vs SVM**\n\n- Part I: [http://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part1/](http://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part1/)\n- Part II: [http://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/](http://www.edvancer.in/logistic-regression-vs-decision-trees-vs-svm-part2/)\n\n**Machine learning: A practical introduction**\n\n- blog: [http://www.infoworld.com/article/3010401/big-data/machine-learning-a-practical-introduction.html](http://www.infoworld.com/article/3010401/big-data/machine-learning-a-practical-introduction.html)\n\n**Tutorials on Machine Learning (Tom Dietterich)**\n\n[http://web.engr.oregonstate.edu/~tgd/projects/tutorials.html](http://web.engr.oregonstate.edu/~tgd/projects/tutorials.html)\n\n**Machine Learning Tutorials**\n\n- intro: \"This repository contains a topic-wise curated list of Machine Learning and Deep Learning tutorials, articles and other resources. \nOther awesome lists can be found in this [list](https://github.com/sindresorhus/awesome).\"\n- homepage: [http://ujjwalkarn.github.io/Machine-Learning-Tutorials/](http://ujjwalkarn.github.io/Machine-Learning-Tutorials/)\n- github: [https://github.com/ujjwalkarn/Machine-Learning-Tutorials/blob/master/README.md](https://github.com/ujjwalkarn/Machine-Learning-Tutorials/blob/master/README.md)\n\n**A Visual Introduction to Machine Learning**\n\n- part 1: [http://www.r2d3.us/visual-intro-to-machine-learning-part-1/](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n\n**Machine Learning – A gentle & structured introduction**\n\n- blog: [http://blog.cambridgecoding.com/2016/02/14/machine-learning-a-gentle-structured-introduction/](http://blog.cambridgecoding.com/2016/02/14/machine-learning-a-gentle-structured-introduction/)\n- slides: [http://pan.baidu.com/s/1hqVGAl2](http://pan.baidu.com/s/1hqVGAl2)\n\n**A Comparison of Supervised Learning Algorithm**\n\n- blog: [http://blog.nycdatascience.com/students-work/a-comparison-of-supervised-learning-algorithm/](http://blog.nycdatascience.com/students-work/a-comparison-of-supervised-learning-algorithm/)\n\n**Statistical Learning and Kernel Methods**\n\n- slides: [http://matt.colorado.edu/compcogworkshop/talks/scholkopf.pdf](http://matt.colorado.edu/compcogworkshop/talks/scholkopf.pdf)\n\n**Getting Started with Machine Learning**\n\n[https://www.infoq.com/articles/getting-started-ml](https://www.infoq.com/articles/getting-started-ml)\n\n**Getting Started with Machine Learning: For the absolute beginners and fifth graders**\n\n[https://medium.com/@suffiyanz/getting-started-with-machine-learning-f15df1c283ea#.fqipdiyyn](https://medium.com/@suffiyanz/getting-started-with-machine-learning-f15df1c283ea#.fqipdiyyn)\n\n**Machine Learning Crash Course**\n\n- part 1: [https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/](https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/)\n- part 2: [https://ml.berkeley.edu/blog/2016/12/24/tutorial-2/](https://ml.berkeley.edu/blog/2016/12/24/tutorial-2/)\n\n**Rules of Machine Learning: Best Practices for ML Engineering**\n\n[http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf)\n\n## Machine Learning is Fun!\n\n**Machine Learning is Fun! - The world’s easiest introduction to Machine Learning**\n\n- blog: [https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.yy4r6gf6b](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.yy4r6gf6b)\n\n**Machine Learning is Fun! Part 2 - Using Machine Learning to generate Super Mario Maker levels**\n\n- blog: [https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3#.r3jx7zqro](https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3#.r3jx7zqro)\n\n**Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks**\n\n- blog: [https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.8jjnrfiix](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.8jjnrfiix)\n\n**Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning**\n\n- blog: [https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.fnu6ep6ac](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.fnu6ep6ac)\n\n## Machine Learning Theory\n\n**Machine Learning Theory - Part 1: Introduction**\n\n[https://mostafa-samir.github.io/ml-theory-pt1/](https://mostafa-samir.github.io/ml-theory-pt1/)\n\n**Machine Learning Theory - Part 2: Generalization Bounds**\n\n[https://mostafa-samir.github.io/ml-theory-pt2/](https://mostafa-samir.github.io/ml-theory-pt2/)\n\n# Boosting\n\n\"Quick Introduction to Boosting Algorithms in Machine Learning\"\n\n[http://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/](http://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/)\n\n**An Empirical Comparison of Three Boosting Algorithms on Real Data Sets with Artificial Class Noise(AdaBoost vs. LogitBoost vs. BrownBoost)**\n\n- paper: [http://www.lancs.ac.uk/~eckley/papers/McDonaldHandEckley2003.pdf](http://www.lancs.ac.uk/~eckley/papers/McDonaldHandEckley2003.pdf)\n\n**A (small) introduction to Boosting**\n\n- blog: [https://codesachin.wordpress.com/2016/03/06/a-small-introduction-to-boosting/](https://codesachin.wordpress.com/2016/03/06/a-small-introduction-to-boosting/)\n\n**Boosting and AdaBoost for Machine Learning**\n\n- blog: [http://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/](http://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/)\n\n# Gradient Boosting\n\n**Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python**\n\n- blog: [http://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/](http://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)\n\n**Understanding Gradient Boosting, Part 1**\n\n- blog: [http://rcarneva.github.io/understanding-gradient-boosting-part-1.html](http://rcarneva.github.io/understanding-gradient-boosting-part-1.html)\n\n**Gradient Boosting explained [demonstration]**\n\n- blog: [https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html]()https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html\n\n**A Kaggle Master Explains Gradient Boosting**\n\n[http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n\n**Performance of various open source GBM implementations**\n\n- intro: h2o VS. xgboost VS. lightgbm\n- github: [https://github.com/szilard/GBM-perf](https://github.com/szilard/GBM-perf)\n\n**arboretum - Gradient Boosting on GPU**\n\n- intro: Gradient Boosting powered by GPU(NVIDIA CUDA)\n- github: [https://github.com/sh1ng/arboretum](https://github.com/sh1ng/arboretum)\n\n**Gradient Boosting from scratch**\n\n[https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)\n\n## XGBoost\n\n**XGBoost: A Scalable Tree Boosting System**\n\n- arxiv: [http://arxiv.org/abs/1603.02754](http://arxiv.org/abs/1603.02754)\n\n**XGBoost: eXtreme Gradient Boosting**\n\n- intro: Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library, \nfor Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Flink and DataFlow\n- github: [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)\n\n**GPU Accelerated XGBoost**\n\n- blog: [http://dmlc.ml/2016/12/14/GPU-accelerated-xgboost.html](http://dmlc.ml/2016/12/14/GPU-accelerated-xgboost.html)\n\n**Awesome XGBoost**\n\n- intro: This page contains a curated list of examples, tutorials, blogs about XGBoost usecases. \n- github: [https://github.com/dmlc/xgboost/blob/master/demo/README.md](https://github.com/dmlc/xgboost/blob/master/demo/README.md)\n\n**Complete Guide to Parameter Tuning in XGBoost (with codes in Python)**\n\n- blog: [https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n- zh-blog: [http://blog.csdn.net/u010657489/article/details/51952785](http://blog.csdn.net/u010657489/article/details/51952785)\n\n**LinXGBoost: Extension of XGBoost to Generalized Local Linear Models**\n\n- arxiv: [https://arxiv.org/abs/1710.03634](https://arxiv.org/abs/1710.03634)\n- github: [https://github.com/ldv1/LinXGBoost](https://github.com/ldv1/LinXGBoost)\n\n**Tree Boosting With XGBoost - Why Does XGBoost Win \"Every\" Machine Learning Competition?**\n\n- intro: Master thesis\n- thesis page: [https://brage.bibsys.no/xmlui/handle/11250/2433761](https://brage.bibsys.no/xmlui/handle/11250/2433761)\n\n**XGBoost: Scalable GPU Accelerated Learning**\n\n- intro: describe the multi-GPU gradient boosting algorithm implemented in the XGBoost library\n- arxiv: [https://arxiv.org/abs/1806.11248](https://arxiv.org/abs/1806.11248)\n\n## LightGBM\n\n**LightGBM, Light Gradient Boosting Machine**\n\n- intro: LightGBM is a fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework \nbased on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n- github: [https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM)\n\n**pyLightGBM: Python binding for Microsoft LightGBM**\n\n- github: [https://github.com/ArdalanM/pyLightGBM](https://github.com/ArdalanM/pyLightGBM)\n\n**Benchmarking LightGBM: how fast is LightGBM vs xgboost?**\n\n- blog: [https://medium.com/data-design/benchmarking-lightgbm-how-fast-is-lightgbm-vs-xgboost-7b5484746ac4#.xwkhql46h](https://medium.com/data-design/benchmarking-lightgbm-how-fast-is-lightgbm-vs-xgboost-7b5484746ac4#.xwkhql46h)\n\n**GPU-acceleration for Large-scale Tree Boosting**\n\n- intro: University of California, Davis & Google Research\n- intro: GPU Accelerated LightGBM for Histogram-based GBDT Training\n- arxiv: [https://arxiv.org/abs/1706.08359](https://arxiv.org/abs/1706.08359)\n- github: [https://github.com/huanzhang12/lightgbm-gpu](https://github.com/huanzhang12/lightgbm-gpu)\n\n**Lessons Learned From Benchmarking Fast Machine Learning Algorithms**\n\n- intro: XGBoost and LightGBM\n- blog: [https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/](https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/)\n\n## CatBoost\n\n**CatBoost is an open-source gradient boosting library with categorical features support**\n\n- intro: CatBoost is a machine learning method based on gradient boosting over decision trees.\n- homepage: [https://catboost.yandex/](https://catboost.yandex/)\n- github: [https://github.com/catboost/catboost](https://github.com/catboost/catboost)\n\n# Bootstrap\n\n**Coding, Visualizing, and Animating Bootstrap Resampling**\n\n[http://minimaxir.com/2015/09/bootstrap-resample/](http://minimaxir.com/2015/09/bootstrap-resample/)\n\n**Can we trust the bootstrap in high-dimension?**\n\n- arxiv: [http://arxiv.org/abs/1608.00696](http://arxiv.org/abs/1608.00696)\n\n# Cascades\n\n**Making faces with Haar cascades and mixed integer linear programming**\n\n- blog: [http://matthewearl.github.io/2016/01/14/inverse-haar/](http://matthewearl.github.io/2016/01/14/inverse-haar/)\n- github: [https://github.com/matthewearl/inversehaar](https://github.com/matthewearl/inversehaar)\n\n# Classifiers\n\n**Measuring Performance of Classifiers**\n\n- blog: [http://shahramabyari.com/2016/02/22/measuring-performance-of-classifiers/](http://shahramabyari.com/2016/02/22/measuring-performance-of-classifiers/)\n\n# Convex Optimization\n\n**Convex Optimization: Algorithms and Complexity**\n\n- arxiv: [http://arxiv.org/abs/1405.4980](http://arxiv.org/abs/1405.4980)\n- blog: [https://blogs.princeton.edu/imabandit/2015/11/30/convex-optimization-algorithms-and-complexity/](https://blogs.princeton.edu/imabandit/2015/11/30/convex-optimization-algorithms-and-complexity/)\n\n**cvx-optim.torch: Torch library for convex optimization**\n\n- github: [https://github.com/bamos/cvx-optim.torch](https://github.com/bamos/cvx-optim.torch)\n\n# Decision Tree\n\n**Soft Decision Trees**\n\n- paper: [http://www.cmpe.boun.edu.tr/~ethem/files/papers/icpr2012_softtree.pdf](http://www.cmpe.boun.edu.tr/~ethem/files/papers/icpr2012_softtree.pdf)\n- project page: [http://www.cs.cornell.edu/~oirsoy/softtree.html](http://www.cs.cornell.edu/~oirsoy/softtree.html)\n- github: [https://github.com/oir/soft-tree](https://github.com/oir/soft-tree)\n\n**Canonical Correlation Forests**\n\n- arxiv: [http://arxiv.org/abs/1507.05444](http://arxiv.org/abs/1507.05444)\n- code: [https://bitbucket.org/twgr/ccf](https://bitbucket.org/twgr/ccf)\n\n**Decision Trees Tutorial**\n\n![](https://annalyzin.files.wordpress.com/2016/07/decision-tree-tutorial-animated3.gif?w=636&h=312&crop=1)\n\n- blog: [https://algobeans.com/2016/07/27/decision-trees-tutorial/](https://algobeans.com/2016/07/27/decision-trees-tutorial/)\n\n**End-to-end Learning of Deterministic Decision Trees**\n\n- intro: Heidelberg University\n- arxiv: [https://arxiv.org/abs/1712.02743](https://arxiv.org/abs/1712.02743)\n\n**Extremely Fast Decision Tree**\n\n- arxiv: [https://arxiv.org/abs/1802.08780](https://arxiv.org/abs/1802.08780)\n- github: [https://github.com/chaitanya-m/kdd2018](https://github.com/chaitanya-m/kdd2018)\n\n# Generative Models\n\n**A note on the evaluation of generative models**\n\n- arxiv: [http://arxiv.org/abs/1511.01844](http://arxiv.org/abs/1511.01844)\n\n# Markov Networks\n\n**Markov Logic Networks**\n\n- paper: [http://homes.cs.washington.edu/~pedrod/papers/mlj05.pdf](http://homes.cs.washington.edu/~pedrod/papers/mlj05.pdf)\n\n# Markov Chains\n\n**Evolution, Dynamical Systems and Markov Chains**\n\n[http://www.offconvex.org/2016/03/07/evolution-markov-chains/](http://www.offconvex.org/2016/03/07/evolution-markov-chains/)\n\n**Markov Chains: Explained Visually**\n\n- blog: [http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/)\n\n# Matrix Computations\n\n**Randomized Numerical Linear Algebra for Large Scale Data Analysis**\n\n[http://researcher.watson.ibm.com/researcher/view_group.php?id=5131](http://researcher.watson.ibm.com/researcher/view_group.php?id=5131)\n\n**Sketching-based Matrix Computations for Machine Learning**\n\n[http://xdata-skylark.github.io/libskylark/](http://xdata-skylark.github.io/libskylark/)\n\n# Matrix Factorization\n\n**Neural Network Matrix Factorization**\n\n- arxiv: [http://arxiv.org/abs/1511.06443](http://arxiv.org/abs/1511.06443)\n\n**Beyond Low Rank + Sparse: Multi-scale Low Rank Matrix Decomposition**\n\n- arxiv: [http://arxiv.org/abs/1507.08751](http://arxiv.org/abs/1507.08751)\n- github: [https://github.com/frankong/multi_scale_low_rank](https://github.com/frankong/multi_scale_low_rank)\n\n**k-Means Clustering Is Matrix Factorization**\n\n- arxiv: [http://arxiv.org/abs/1512.07548](http://arxiv.org/abs/1512.07548)\n- note: [http://blog.csdn.net/cyh_24/article/details/50408884](http://blog.csdn.net/cyh_24/article/details/50408884)\n\n**CuMF_SGD: Fast and Scalable Matrix Factorization**\n\n- arxiv: [https://arxiv.org/abs/1610.05838](https://arxiv.org/abs/1610.05838)\n- github: [https://github.com/CuMF/cumf_sgd](https://github.com/CuMF/cumf_sgd)\n\n# Gaussian Processes\n\n**The Gaussian Processes Web Site**\n\n- blog: [http://www.gaussianprocess.org/](http://www.gaussianprocess.org/)\n\n**Chained Gaussian Processes**\n\n- jmlr: [http://jmlr.org/proceedings/papers/v51/saul16.html](http://jmlr.org/proceedings/papers/v51/saul16.html)\n- arxiv: [http://arxiv.org/abs/1604.05263](http://arxiv.org/abs/1604.05263)\n- github: [https://github.com/SheffieldML/ChainedGP](https://github.com/SheffieldML/ChainedGP)\n\n**Introduction to Gaussian Processes**\n\n- slides: [http://learning.mpi-sws.org/mlss2016/slides/gp_mlss16.pdf](http://learning.mpi-sws.org/mlss2016/slides/gp_mlss16.pdf)\n\n# Multi-label Learning\n\n**Neural Network Models for Multilabel Learning**\n\n- paper: [http://pan.baidu.com/s/1bnFdYFX](http://pan.baidu.com/s/1bnFdYFX)\n- github: [https://github.com/abhishek-kumar/NNForMLL](https://github.com/abhishek-kumar/NNForMLL)\n\n**Conditional Bernoulli Mixtures for Multi-label Classification**\n\n- homepage: [http://www.chengli.io/publications/li2016conditional.html](http://www.chengli.io/publications/li2016conditional.html)\n- paper: [http://www.chengli.io/publications/li2016conditional.pdf](http://www.chengli.io/publications/li2016conditional.pdf)\n- slides: [http://www.chengli.io/publications/li2016conditional_slides.pdf](http://www.chengli.io/publications/li2016conditional_slides.pdf)\n- github: [https://github.com/cheng-li/pyramid](https://github.com/cheng-li/pyramid)\n- wiki: [https://github.com/cheng-li/pyramid/wiki/CBM](https://github.com/cheng-li/pyramid/wiki/CBM)\n\n**Multi-Label Learning with Label Enhancement**\n\n[https://arxiv.org/abs/1706.08323](https://arxiv.org/abs/1706.08323)\n\n# Multi-Task Learning\n\n**Multitask Learning**\n\n- intro: 1997\n- paper: [http://www.cs.cornell.edu/~caruana/mlj97.pdf](http://www.cs.cornell.edu/~caruana/mlj97.pdf)\n\n**Multi-Task Learning: Theory, Algorithms, and Applications (2012)**\n\n- slides: [http://www.public.asu.edu/~jye02/Software/MALSAR/MTL-SDM12.pdf](http://www.public.asu.edu/~jye02/Software/MALSAR/MTL-SDM12.pdf)\n\n# Nearest Neighbors\n\n**Annoy: Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk**\n\n![](https://camo.githubusercontent.com/d6bf20e534ab76b67c731b566859a24149a4bf80/68747470733a2f2f7261772e6769746875622e636f6d2f73706f746966792f616e6e6f792f6d61737465722f616e6e2e706e67)\n\n- github: [https://github.com/spotify/annoy](https://github.com/spotify/annoy)\n\n# Hidden Markov Models (HMM)\n\n**tensorflow_hmm: A tensorflow implementation of an HMM layer**\n\n- intro: Tensorflow and numpy implementations of the HMM viterbi and forward/backward algorithms\n- github: [https://github.com/dwiel/tensorflow_hmm](https://github.com/dwiel/tensorflow_hmm)\n\n# Online Learning\n\n**Lecture Notes on Online Learning**\n\n- notes: [http://www-stat.wharton.upenn.edu/~rakhlin/courses/stat991/papers/lecture_notes.pdf](http://www-stat.wharton.upenn.edu/~rakhlin/courses/stat991/papers/lecture_notes.pdf)\n\n**Scale-Free Online Learning**\n\n- arxiv: [http://arxiv.org/abs/1601.01974](http://arxiv.org/abs/1601.01974)\n\n**Online Learning with Expert Advice**\n\n- lecture notes: [http://courses.cs.washington.edu/courses/cse599s/14sp/scribes/lecture6/lecture6.pdf](http://courses.cs.washington.edu/courses/cse599s/14sp/scribes/lecture6/lecture6.pdf)\n\n# Stochastic Gradient Descent (SGD)\n\n**Stochastic Gradient Descent (v.2)**\n\n![](http://leon.bottou.org/_media/projects/crf-f1.png)\n\n- author: Leon Bottou\n- intro: SGD, ASGD, Stochastic Gradient SVM, Stochastic Gradient CRFs\n- homepage: [http://leon.bottou.org/projects/sgd](http://leon.bottou.org/projects/sgd)\n\n**Gradient descent with Python**\n\n- blog: [http://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/](http://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/)\n\n**Stochastic Gradient Descent (SGD) with Python**\n\n- blog: [http://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/](http://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/)\n\n**Gradient Descent Learns Linear Dynamical Systems**\n\n- blog: [http://www.offconvex.org/2016/10/13/gradient-descent-learns-dynamical-systems/](http://www.offconvex.org/2016/10/13/gradient-descent-learns-dynamical-systems/)\n\n**Why is gradient descent robust to non-linearly separable data?**\n\n- blog: [https://medium.com/@vivek.yadav/why-is-gradient-descent-robust-to-non-linearly-separable-data-a50c543e8f4a#.rhtf3xi79](https://medium.com/@vivek.yadav/why-is-gradient-descent-robust-to-non-linearly-separable-data-a50c543e8f4a#.rhtf3xi79)\n\n# Boosted Regression Trees\n\n**DART: Dropouts meet Multiple Additive Regression Trees**\n\n- paper: [http://www.jmlr.org/proceedings/papers/v38/korlakaivinayak15.html](http://www.jmlr.org/proceedings/papers/v38/korlakaivinayak15.html)\n- github: [https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dart.md](https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dart.md)\n\n# Visualization\n\n**Visualising High-Dimensional Data**\n\n![](http://blog.applied.ai/content/images/2015/06/Unknown-1.png)\n\n- blog: [http://blog.applied.ai/visualising-high-dimensional-data/](http://blog.applied.ai/visualising-high-dimensional-data/)\n- ipn(\"t-SNE Demo\"): [https://s3-eu-west-1.amazonaws.com/appliedai.static/tsnedemo/htmlrenders/01_EndToEnd_DataViz.html](https://s3-eu-west-1.amazonaws.com/appliedai.static/tsnedemo/htmlrenders/01_EndToEnd_DataViz.html)\n\n**Interactive demonstrations for ML courses**\n\n![](http://arogozhnikov.github.io/images/ml_demonstrations/gradient_boosting_explained.png)\n\n- blog: [http://arogozhnikov.github.io/2016/04/28/demonstrations-for-ml-courses.html](http://arogozhnikov.github.io/2016/04/28/demonstrations-for-ml-courses.html)\n\n**Comprehensive Guide on t-SNE algorithm with implementation in R & Python**\n\n[https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n\n# Tricks\n\n**Machine Learning Trick of the Day**\n\n- (1): Replica Trick: [http://blog.shakirm.com/2015/07/machine-learning-trick-of-the-day-1-replica-trick/](http://blog.shakirm.com/2015/07/machine-learning-trick-of-the-day-1-replica-trick/)\n- (2): Gaussian Integral Trick: [http://blog.shakirm.com/2015/08/machine-learning-trick-of-the-day-2-gaussian-integral-trick/](http://blog.shakirm.com/2015/08/machine-learning-trick-of-the-day-2-gaussian-integral-trick/)\n- (3): Hutchinson's Trick: [http://blog.shakirm.com/2015/09/machine-learning-trick-of-the-day-3-hutchinsons-trick/](http://blog.shakirm.com/2015/09/machine-learning-trick-of-the-day-3-hutchinsons-trick/)\n- (4): Reparameterisation Tricks: [http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/)\n- (5): Log Derivative Trick: [http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/)\n\n# Debug Machine Learning\n\n**Debugging Machine Learning Tasks**\n\n- arxiv: [http://arxiv.org/abs/1603.07292](http://arxiv.org/abs/1603.07292)\n\n# Tackle Unbalanced Classes\n\nClassic strategies:\n\n1. class re-sampling\n2. cost-sensitive training\n\n**Dealing with Unbalanced Classes ,Svm, Random Forests And Decision Trees In Python**\n\n![](http://www.bigdataexaminer.com/wp-content/uploads/2015/02/unbalanced01.jpg)\n\n- blog: [http://bigdataexaminer.com/data-science/dealing-with-unbalanced-classes-svm-random-forests-and-decision-trees-in-python/](http://bigdataexaminer.com/data-science/dealing-with-unbalanced-classes-svm-random-forests-and-decision-trees-in-python/)\n- blog: [http://www.kdnuggets.com/2016/04/unbalanced-classes-svm-random-forests-python.html](http://www.kdnuggets.com/2016/04/unbalanced-classes-svm-random-forests-python.html)\n\n**Fighting Class Unbalance Supervised ML Problem**\n\n[http://www.erogol.com/fighting-class-unbalance-supervised-ml-problem/](http://www.erogol.com/fighting-class-unbalance-supervised-ml-problem/)\n\n**Survey of resampling techniques for improving classification performance in unbalanced datasets**\n\n- arxiv: [http://arxiv.org/abs/1608.06048](http://arxiv.org/abs/1608.06048)\n\n**Learning from Imbalanced Classes**\n\n![](http://www.svds.com/wp-content/uploads/2016/08/ImbalancedClasses_fig5.jpg)\n\n- blog: [http://www.svds.com/learning-imbalanced-classes/](http://www.svds.com/learning-imbalanced-classes/)\n- github: [https://github.com/silicon-valley-data-science/learning-from-imbalanced-classes](https://github.com/silicon-valley-data-science/learning-from-imbalanced-classes)\n\n**Towards Competitive Classifiers for Unbalanced Classification Problems: A Study on the Performance Scores**\n\n- arxiv: [http://arxiv.org/abs/1608.08984](http://arxiv.org/abs/1608.08984)\n- github: [https://github.com/jonathanSS/ClassImbalanceStudies](https://github.com/jonathanSS/ClassImbalanceStudies)\n\n**This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume**\n\n[https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/](https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/)\n\n**Dealing with unbalanced data: Generating additional data by jittering the original image**\n\n- blog: [https://medium.com/@vivek.yadav/dealing-with-unbalanced-data-generating-additional-data-by-jittering-the-original-image-7497fe2119c3](https://medium.com/@vivek.yadav/dealing-with-unbalanced-data-generating-additional-data-by-jittering-the-original-image-7497fe2119c3)\n- ipynb: [https://nbviewer.jupyter.org/github/vxy10/SCND_notebooks/blob/master/preprocessing_stuff/img_transform_NB.ipynb](https://nbviewer.jupyter.org/github/vxy10/SCND_notebooks/blob/master/preprocessing_stuff/img_transform_NB.ipynb)\n\n**7 Techniques to Handle Imbalanced Data**\n\n[http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html](http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)\n\n# Mathematics\n\n**Some Notes on Applied Mathematics for Machine**\n\n- paper: [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/tr-2004-56.pdf](http://research.microsoft.com/en-us/um/people/cburges/tech_reports/tr-2004-56.pdf)\n\n**An extended collection of matrix derivative results for forward and reverse mode algorithmic differentiation**\n\n- paper: [https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf](https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf)\n\n**Probability Cheatsheet**\n\n- homepage: [http://www.wzchen.com/probability-cheatsheet](http://www.wzchen.com/probability-cheatsheet)\n- github: [https://github.com/wzchen/probability_cheatsheet](https://github.com/wzchen/probability_cheatsheet)\n\n**Probability Cheatsheet v2.0**\n[http://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf](http://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf)\n\n# Kalman Filter\n\n**How Kalman Filters Work**\n\n- part 1: [http://www.anuncommonlab.com/articles/how-kalman-filters-work/](http://www.anuncommonlab.com/articles/how-kalman-filters-work/)\n- part 2: [http://www.anuncommonlab.com/articles/how-kalman-filters-work/part2.html](http://www.anuncommonlab.com/articles/how-kalman-filters-work/part2.html)\n- part 3: [http://www.anuncommonlab.com/articles/how-kalman-filters-work/part3.html](http://www.anuncommonlab.com/articles/how-kalman-filters-work/part3.html)\n\n**Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation**\n\n- paper: [https://www.cl.cam.ac.uk/~rmf25/papers/Understanding%20the%20Basis%20of%20the%20Kalman%20Filter.pdf](https://www.cl.cam.ac.uk/~rmf25/papers/Understanding%20the%20Basis%20of%20the%20Kalman%20Filter.pdf)\n\n# L-BFGS\n\n# Code Stylometry\n\n**De-anonymizing Programmers via Code Stylometry**\n\n- keywords: source code authorship, random forests\n- paper: [http://www.princeton.edu/~aylinc/papers/caliskan-islam_deanonymizing.pdf](http://www.princeton.edu/~aylinc/papers/caliskan-islam_deanonymizing.pdf)\n\n# Recommendation / Recommender System\n\n**Master Recommender Systems**\n\n- intro: Learn how to design, building and evaluate recommender systems for commerce and content.\n- course page: [https://www.coursera.org/specializations/recommender-systems](https://www.coursera.org/specializations/recommender-systems)\n\n**Human Curation and Convnets: Powering Item-to-Item Recommendations on Pinterest**\n\n- paper: [https://engineering.pinterest.com/sites/engineering/files/article/fields/field_image/human-curation-convnets%20%281%29.pdf](https://engineering.pinterest.com/sites/engineering/files/article/fields/field_image/human-curation-convnets%20%281%29.pdf)\n\n**Top-N Recommendation with Novel Rank Approximation**\n\n- arxiv: [http://arxiv.org/abs/1602.07783](http://arxiv.org/abs/1602.07783)\n- github: [https://github.com/sckangz/SDM16](https://github.com/sckangz/SDM16)\n\n**On the Effectiveness of Linear Models for One-Class Collaborative Filtering**\n\n- paper: [http://www.cs.toronto.edu/~darius/papers/SedhainEtAl-AAAI2016.pdf](http://www.cs.toronto.edu/~darius/papers/SedhainEtAl-AAAI2016.pdf)\n- github: [https://github.com/mesuvash/LRec](https://github.com/mesuvash/LRec)\n\n**An Adaptive Matrix Factorization Approach for Personalized Recommender Systems**\n\n- arxiv: [http://arxiv.org/abs/1607.07607](http://arxiv.org/abs/1607.07607)\n\n**Implementing your own Recommender Systems in Python using Stochastic Gradient Descent**\n\n- blog: [http://online.cambridgecoding.com/notebooks/mhaller/implementing-your-own-recommender-systems-in-python-using-stochastic-gradient-descent-4#implementing-your-own-recommender-systems-in-python-using-stochastic-gradient-descent](http://online.cambridgecoding.com/notebooks/mhaller/implementing-your-own-recommender-systems-in-python-using-stochastic-gradient-descent-4#implementing-your-own-recommender-systems-in-python-using-stochastic-gradient-descent)\n\n**How to Write Your Own Recommendation System**\n\n- blog(part 1): [http://elliot.land/how-to-write-your-own-recommendation-system-part-1](http://elliot.land/how-to-write-your-own-recommendation-system-part-1)\n- blog(part 2): [http://elliot.land/how-to-write-your-own-recommendation-system-part-2](http://elliot.land/how-to-write-your-own-recommendation-system-part-2)\n\n**Addressing Cold Start for Next-song Recommendation**\n\n- intro: ACM Recsys 2016\n- paper: [http://mac.citi.sinica.edu.tw/~yang/pub/chou16recsys.pdf](http://mac.citi.sinica.edu.tw/~yang/pub/chou16recsys.pdf)\n- github: [https://github.com/fearofchou/ALMM](https://github.com/fearofchou/ALMM)\n\n**Using Navigation to Improve Recommendations in Real-Time**\n\n- paper: [http://dl.acm.org/citation.cfm?id=2959174](http://dl.acm.org/citation.cfm?id=2959174)\n\n**Local Item-Item Models For Top-N Recommendation**\n\n- paper: [http://dl.acm.org/citation.cfm?id=2959185](http://dl.acm.org/citation.cfm?id=2959185)\n\n**Lessons learned from building real-life recommender systems**\n\n- intro: Recsys 2016 tutorial\n- slides: [http://www.slideshare.net/xamat/recsys-2016-tutorial-lessons-learned-from-building-reallife-recommender-systems](http://www.slideshare.net/xamat/recsys-2016-tutorial-lessons-learned-from-building-reallife-recommender-systems)\n- mirror: [https://pan.baidu.com/s/1eSdWcue](https://pan.baidu.com/s/1eSdWcue)\n\n**Algorithms Aside: Recommendation As The Lens Of Life**\n\n- paper: [http://dl.acm.org/citation.cfm?doid=2959100.2959164](http://dl.acm.org/citation.cfm?doid=2959100.2959164)\n\n**Pairwise Preferences Based Matrix Factorization and Nearest Neighbor Recommendation Techniques**\n\n- paper: [http://dl.acm.org/citation.cfm?id=2959142](http://dl.acm.org/citation.cfm?id=2959142)\n- datasets: [http://www.inf.unibz.it/~kalloori/](http://www.inf.unibz.it/~kalloori/)\n\n**Mendeley: Recommendations for Researchers**\n\n- intro: RecSys 2016\n- slides: [http://saulvargas.es/slides/recsys2016/#/](http://saulvargas.es/slides/recsys2016/#/)\n\n**Past, Present and Future of Recommender Systems: an Industry Perspective**\n\n- intro: RecSys 2016\n- slides: [http://www.slideshare.net/xamat/past-present-and-future-of-recommender-systems-and-industry-perspective](http://www.slideshare.net/xamat/past-present-and-future-of-recommender-systems-and-industry-perspective)\n- mirror: [https://pan.baidu.com/s/1kVQ4SKZ](https://pan.baidu.com/s/1kVQ4SKZ)\n\n**TF-recomm: Tensorflow-based Recommendation systems**\n\n- github: [https://github.com/songgc/TF-recomm](https://github.com/songgc/TF-recomm)\n\n**List of Recommender Systems**\n\n- github: [https://github.com/grahamjenson/list_of_recommender_systems](https://github.com/grahamjenson/list_of_recommender_systems)\n\n**Related Pins at Pinterest: The Evolution of a Real-World Recommender System**\n\n- intro: Pinterest, Inc.\n- arxiv: [https://arxiv.org/abs/1702.07969](https://arxiv.org/abs/1702.07969)\n\n# Lifelong Learning\n\n**Lifelong Machine Learning**\n\n- book: [https://www.cs.uic.edu/~liub/lifelong-machine-learning.html](https://www.cs.uic.edu/~liub/lifelong-machine-learning.html)\n- pdf: [https://vk.com/doc-44016343_439142620?hash=a96978fe024d79e455&dl=2e154ea5883bbc8fd6](https://vk.com/doc-44016343_439142620?hash=a96978fe024d79e455&dl=2e154ea5883bbc8fd6)\n\n## NELL (Never Ending Language Learner)\n\n**Toward an architecture for neverending language learning**\n\n## NEIL (Never Ending Image Learner)\n\n**NEIL: Extracting Visual Knowledge from Web Data**\n\n- paper: [http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Chen_NEIL_Extracting_Visual_2013_ICCV_paper.pdf](http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Chen_NEIL_Extracting_Visual_2013_ICCV_paper.pdf)\n- slides: [http://web.cs.hacettepe.edu.tr/~nazli/courses/bil722/slides/week10_1.pdf](http://web.cs.hacettepe.edu.tr/~nazli/courses/bil722/slides/week10_1.pdf)\n- slides: [http://sglab.kaist.ac.kr/~sungeui/IR/Presentation/first/20141104%EC%9D%B4%EC%9C%A4%EC%84%9D.pdf](http://sglab.kaist.ac.kr/~sungeui/IR/Presentation/first/20141104%EC%9D%B4%EC%9C%A4%EC%84%9D.pdf)\n- talk: [http://techtalks.tv/talks/neil-extracting-visual-knowledge-from-web-data/59408/](http://techtalks.tv/talks/neil-extracting-visual-knowledge-from-web-data/59408/)\n- poster: [http://www.cs.cmu.edu/~xinleic/docs/neil/NEIL_poster.pdf](http://www.cs.cmu.edu/~xinleic/docs/neil/NEIL_poster.pdf)\n\n**Expert Gate: Lifelong Learning with a Network of Experts**\n\n- arxiv: [https://arxiv.org/abs/1611.06194](https://arxiv.org/abs/1611.06194)\n\n**Lifelong Machine Learning and Computer Reading the Web**\n\n- intro: KDD 2016 Tutorial\n- paper: [https://www.cs.uic.edu/~liub/Lifelong-Machine-Learning-Tutorial-KDD-2016.pdf](https://www.cs.uic.edu/~liub/Lifelong-Machine-Learning-Tutorial-KDD-2016.pdf)\n\n**Lifelong Machine Learning for Natural Language Processing**\n\n- intro: EMNLP 2016 Tutorial\n- slides: [http://www.emnlp2016.net/tutorials/chen-liu-t3.pdf](http://www.emnlp2016.net/tutorials/chen-liu-t3.pdf)\n\n# Zero-Shot Learning\n\n**An embarrassingly simple approach to zero-shot learning**\n\n- paper: [http://jmlr.org/proceedings/papers/v37/romera-paredes15.html](http://jmlr.org/proceedings/papers/v37/romera-paredes15.html)\n- github: [https://github.com/MLWave/extremely-simple-one-shot-learning](https://github.com/MLWave/extremely-simple-one-shot-learning)\n\n**Zero-Shot Learning - The Good, the Bad and the Ugly**\n\n- arxiv: [https://arxiv.org/abs/1703.04394](https://arxiv.org/abs/1703.04394)\n\n# One Shot Learning\n\n**Matching Networks for One Shot Learning**\n\n- arxiv: [http://arxiv.org/abs/1606.04080](http://arxiv.org/abs/1606.04080)\n- github: [https://github.com/zergylord/oneshot](https://github.com/zergylord/oneshot)\n\n# Maximum Entropy\n\n**Maximum entropy probability distribution**\n\n[https://www.wikiwand.com/en/Maximum_entropy_probability_distribution](https://www.wikiwand.com/en/Maximum_entropy_probability_distribution)\n\n# Metric Learning\n\n**Distance Metric Learning: A Comprehensive Survey**\n\n- intro: 2006\n- paper: [https://www.cs.cmu.edu/~liuy/frame_survey_v2.pdf](https://www.cs.cmu.edu/~liuy/frame_survey_v2.pdf)\n\n**Large Scale Metric Learning from Equivalence Constraints**\n\n- intro: CVPR 2012. KISSME\n- paper: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.384.2335&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.384.2335&rep=rep1&type=pdf)\n\n**Large Scale Strongly Supervised Ensemble Metric Learning, with Applications to Face Verification and Retrieval**\n\n- intro: NEC Laboratories America\n- arxiv: [https://arxiv.org/abs/1212.6094](https://arxiv.org/abs/1212.6094)\n\n# Finance and Trading\n\n**Efficient Portfolio optimisation by Hybridised Machine Learning**\n\n- intro: Thesis 2014\n- mirror: [http://pan.baidu.com/s/1eQvSyZ4](http://pan.baidu.com/s/1eQvSyZ4)\n\n**Feature Selection for Portfolio Optimization**\n\n- paper: [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2548800](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2548800)\n\n**The Efficient Frontier: Markowitz portfolio optimization in Python**\n\n- blog: [https://blog.quantopian.com/markowitz-portfolio-optimization-2/](https://blog.quantopian.com/markowitz-portfolio-optimization-2/)\n\n**Self-Study Plan for Becoming a Quantitative Trader**\n\n- part 1: [https://www.quantstart.com/articles/Self-Study-Plan-for-Becoming-a-Quantitative-Trader-Part-I](https://www.quantstart.com/articles/Self-Study-Plan-for-Becoming-a-Quantitative-Trader-Part-I)\n- part 2: [https://www.quantstart.com/articles/Self-Study-Plan-for-Becoming-a-Quantitative-Trader-Part-II](https://www.quantstart.com/articles/Self-Study-Plan-for-Becoming-a-Quantitative-Trader-Part-II)\n\n**Pyfolio -- a new Python library for performance and risk analysis**\n\n- blog: [https://blog.quantopian.com/pyfolio/](https://blog.quantopian.com/pyfolio/)\n- github: [https://github.com/quantopian/pyfolio](https://github.com/quantopian/pyfolio)\n\n**Application of Machine Learning: Automated Trading Informed by Event Driven Data**\n\n- intro: MIT master thesis\n- paper: [https://dspace.mit.edu/bitstream/handle/1721.1/105982/965785890-MIT.pdf](https://dspace.mit.edu/bitstream/handle/1721.1/105982/965785890-MIT.pdf)\n\n**Python Programming for Finance**\n\n- youtube: [https://www.youtube.com/playlist?list=PLQVvvaa0QuDcOdF96TBtRtuQksErCEBYZ](https://www.youtube.com/playlist?list=PLQVvvaa0QuDcOdF96TBtRtuQksErCEBYZ)\n\n**Algorithmic trading in less than 100 lines of Python code**\n\n[https://www.oreilly.com/learning/algorithmic-trading-in-less-than-100-lines-of-python-code](https://www.oreilly.com/learning/algorithmic-trading-in-less-than-100-lines-of-python-code)\n\n**Designing an Algorithmic Trading Strategy with Python**\n\n[https://www.youtube.com/watch?v=9XYjR6ge73M](https://www.youtube.com/watch?v=9XYjR6ge73M)\n\n# Different Interpretation about Same Model\n\n**Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning**\n\n- intro: ICML 2016\n- arxiv: [https://arxiv.org/abs/1506.02142](https://arxiv.org/abs/1506.02142)\n\n**Dropout as a Bayesian Approximation: Insights and Applications**\n\n[http://mlg.eng.cam.ac.uk/yarin/PDFs/Dropout_as_a_Bayesian_approximation.pdf](http://mlg.eng.cam.ac.uk/yarin/PDFs/Dropout_as_a_Bayesian_approximation.pdf)\n\n**k-Means Clustering Is Matrix Factorization**\n\n[https://arxiv.org/abs/1512.07548](https://arxiv.org/abs/1512.07548)\n\nword embedding as matrix factorization\n\n**Neural Word Embedding as Implicit Matrix Factorization**\n\n[https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf](https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf)\n\n**Deformable Part Models are Convolutional Neural Networks**\n\n- intro: CVPR 2015\n- arxiv: [https://arxiv.org/abs/1409.5403](https://arxiv.org/abs/1409.5403)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Girshick_Deformable_Part_Models_2015_CVPR_paper.pdf)\n\n**k-Means is a Variational EM Approximation of Gaussian Mixture Models**\n\n[https://arxiv.org/abs/1704.04812](https://arxiv.org/abs/1704.04812)\n\n**Steepest descent with momentum for quadratic functions is a version of the conjugate gradient method**\n\n[http://www.sciencedirect.com/science/article/pii/S0893608003001709](http://www.sciencedirect.com/science/article/pii/S0893608003001709)\n\n**On the momentum term in gradient descent learning algorithms**\n\n[http://www.sciencedirect.com/science/article/pii/S0893608098001166?np=y&npKey=142c3bf066ad1c36c5b4fd8713d0a8967413462675bae2f8d7b89933fa8cf228](http://www.sciencedirect.com/science/article/pii/S0893608098001166?np=y&npKey=142c3bf066ad1c36c5b4fd8713d0a8967413462675bae2f8d7b89933fa8cf228)\n\nEM as a coordinate descent\n\n**Backprop as Functor: A compositional perspective on supervised learning**\n\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1711.10455](https://arxiv.org/abs/1711.10455)\n\n# Papers\n\n**Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?**\n\n- intro: evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, \ndecision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, \ngeneralized linear models, nearest-neighbors, partial least squares and principal component regression, \nlogistic and multinomial regression, multiple adaptive regression splines and other methods), \nimplemented in Weka, R (with and without the caret package), C and Matlab, \nincluding all the relevant classifiers available today\n- intro: \"The random forest is clearly the best family of classifiers\"\n- paper: [http://www.jmlr.org/papers/volume15/delgado14a/delgado14a.pdf](http://www.jmlr.org/papers/volume15/delgado14a/delgado14a.pdf)\n\n**Are Random Forests Truly the Best Classifiers?**\n\n- intro: question the conclusion that random forests are the best classifiers\n- paper: [http://jmlr.org/papers/volume17/15-374/15-374.pdf](http://jmlr.org/papers/volume17/15-374/15-374.pdf)\n- notes: [http://weibo.com/ttarticle/p/show?id=2309404007876694808654](http://weibo.com/ttarticle/p/show?id=2309404007876694808654)\n- my notes: jeez, I love the above two papers..\n\n**An Empirical Evaluation of Supervised Learning in High Dimensions**\n\n- paper: [http://lowrank.net/nikos/pubs/empirical.pdf](http://lowrank.net/nikos/pubs/empirical.pdf)\n\n**Machine learning: Trends, perspectives, and prospects**\n\n- intro: M. I. Jordan and T. M. Mitchell. Science\n- paper: [http://www.cs.cmu.edu/~tom/pubs/Science-ML-2015.pdf](http://www.cs.cmu.edu/~tom/pubs/Science-ML-2015.pdf)\n\n**Debugging Machine Learning Tasks**\n\n- arxiv: [http://arxiv.org/abs/1603.07292](http://arxiv.org/abs/1603.07292)\n\n## LIME\n\n**\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier**\n\n- intro: Local Interpretable Model-Agnostic Explanations (LIME)\n- homepage: [http://homes.cs.washington.edu/~marcotcr/blog/lime/](http://homes.cs.washington.edu/~marcotcr/blog/lime/)\n- arxiv: [http://arxiv.org/abs/1602.04938](http://arxiv.org/abs/1602.04938)\n- github: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)\n- github: [https://github.com/marcotcr/lime-experiments](https://github.com/marcotcr/lime-experiments)\n- blog: [https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)\n- blog: [http://dataskeptic.com/epnotes/trusting-machine-learning-models-with-lime.php](http://dataskeptic.com/epnotes/trusting-machine-learning-models-with-lime.php)\n- notes: [https://blog.acolyer.org/2016/09/22/why-should-i-trust-you-explaining-the-predictions-of-any-classifier/](https://blog.acolyer.org/2016/09/22/why-should-i-trust-you-explaining-the-predictions-of-any-classifier/)\n\n# Datasets\n\n**Datasets for Machine Learning**\n\n- blog: [http://blog.webkid.io/datasets-for-machine-learning/](http://blog.webkid.io/datasets-for-machine-learning/)\n\n# Books\n\n**Machine Learning plus Intelligent Optimization: THE LION WAY, VERSION 2.0**\n\n- book: [http://intelligent-optimization.org/LIONbook/](http://intelligent-optimization.org/LIONbook/)\n- slides: [http://intelligent-optimization.org/LIONbook/LIONway-slides-chapter3.pdf](http://intelligent-optimization.org/LIONbook/LIONway-slides-chapter3.pdf)\n\n**Level-Up Your Machine Learning**\n\n[https://www.metacademy.org/roadmaps/cjrd/level-up-your-ml](https://www.metacademy.org/roadmaps/cjrd/level-up-your-ml)\n\n**An Introduction to the Science of Statistics: From Theory to Implementation (Preliminary Edition)**\n\n- book: [http://math.arizona.edu/~jwatkins/statbook.pdf](http://math.arizona.edu/~jwatkins/statbook.pdf)\n\n**Python Machine Learning**\n\n- github: [https://github.com/rasbt/python-machine-learning-book](https://github.com/rasbt/python-machine-learning-book)\n\n**Machine Learning for Hackers**\n\n- github: [https://github.com/johnmyleswhite/ML_for_Hackers](https://github.com/johnmyleswhite/ML_for_Hackers)\n\n**A Course in Machine Learning**\n\n- homepage: [http://ciml.info/](http://ciml.info/)\n- github: [https://github.com/hal3/ciml](https://github.com/hal3/ciml)\n\n**An Introduction to Statistical Learning: with Applications in R**\n\n- homepage: [http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/)\n- course page: [https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about)\n- unofficial solutions: [http://blog.princehonest.com/stat-learning/](http://blog.princehonest.com/stat-learning/)\n- github: [https://github.com/asadoughi/stat-learning](https://github.com/asadoughi/stat-learning)\n\n**Introduction to Machine Learning with Python**\n\n- github(Notebooks and code): [https://github.com/amueller/introduction_to_ml_with_python](https://github.com/amueller/introduction_to_ml_with_python)\n\n**Introduction to Machine Learning (Second Edition)**\n\n- author: Ethem Alpaydin\n- book: [https://static.aminer.org/upload/pdf/1821/326/1262/53e99a91b7602d9702304e89.pdf](https://static.aminer.org/upload/pdf/1821/326/1262/53e99a91b7602d9702304e89.pdf)\n\n# Videos\n\n**Video resources for machine learning**\n\n[http://dustintran.com/blog/video-resources-for-machine-learning/](http://dustintran.com/blog/video-resources-for-machine-learning/)\n\n# Blogs\n\n**10 More lessons learned from building real-life Machine Learning systems — Part I**\n\n[https://medium.com/@xamat/10-more-lessons-learned-from-building-real-life-ml-systems-part-i-b309cafc7b5e#.h7rh0gxlv](https://medium.com/@xamat/10-more-lessons-learned-from-building-real-life-ml-systems-part-i-b309cafc7b5e#.h7rh0gxlv)\n\n**Machine Learning: classifier comparison using Plotly**\n\n[http://nbviewer.jupyter.org/github/etpinard/plotly-misc-nbs/blob/master/ml-classifier-comp/ml-classifier-comp.ipynb](http://nbviewer.jupyter.org/github/etpinard/plotly-misc-nbs/blob/master/ml-classifier-comp/ml-classifier-comp.ipynb)\n\n**Fitting a model via closed-form equations vs. Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning. What is the difference?**\n\n![](https://raw.githubusercontent.com/rasbt/python-machine-learning-book/master/faq/closed-form-vs-gd/simple_regression.png)\n\n- github: [https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md)\n\n**A Friendly Introduction to Cross-Entropy Loss**\n\n[https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n\n**How to choose algorithms for Microsoft Azure Machine Learning**\n\n- blog: [https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-choice/](https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-choice/)\n\n**New to Machine Learning? Avoid these three mistakes**\n\n- blog: [https://medium.com/machine-intelligence-report/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4#.hi1iowlmf](https://medium.com/machine-intelligence-report/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4#.hi1iowlmf)\n\n**Machine Learning Exercises In Python**\n\n- part 1: [http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/)\n- part 2: [http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/)\n- part 3: [http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/)\n- part 4: [http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/)\n- part 5: [http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/)\n- part 6: [http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/)\n- part 7: [http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/)\n- part 8: [http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/)\n- github: [https://github.com/jdwittenauer/ipython-notebooks](https://github.com/jdwittenauer/ipython-notebooks)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/4xgkoa/all_of_andrew_ngs_machine_learning_class_in_python/](https://www.reddit.com/r/MachineLearning/comments/4xgkoa/all_of_andrew_ngs_machine_learning_class_in_python/)\n\n**Assessing Stability of K-Means Clusterings**\n\n- blog: [http://activisiongamescience.github.io/2016/08/19/Assessing-Stability-of-K-Means-Clusterings/](http://activisiongamescience.github.io/2016/08/19/Assessing-Stability-of-K-Means-Clusterings/)\n\n**Cross-Validation Gone Wrong**\n\n- blog: [http://betatim.github.io/posts/cross-validation-gone-wrong/](http://betatim.github.io/posts/cross-validation-gone-wrong/)\n- ipn: [http://nbviewer.jupyter.org/url/betatim.github.io//downloads/notebooks/cross_validation.ipynb](http://nbviewer.jupyter.org/url/betatim.github.io//downloads/notebooks/cross_validation.ipynb)\n\n**Probabilistic Machine Learning in PyMC3**\n\n- blog: [http://twiecki.github.io/ODSC_London_2016_Probabilistic_ML_Wiecki.slides.html#/](http://twiecki.github.io/ODSC_London_2016_Probabilistic_ML_Wiecki.slides.html#/)\n- slides: [https://docs.google.com/presentation/d/1puj4iN70MRVauUmIMAZS0pfANktjdQ5uCP7H8OLPKFk/edit#slide=id.p](https://docs.google.com/presentation/d/1puj4iN70MRVauUmIMAZS0pfANktjdQ5uCP7H8OLPKFk/edit#slide=id.p)\n- mirror: [https://pan.baidu.com/s/1pLJCya3](https://pan.baidu.com/s/1pLJCya3)\n- ipn: [https://twiecki.github.io/probabilistic_ml.ipynb](https://twiecki.github.io/probabilistic_ml.ipynb)\n\n**Bias in ML, and Teaching AI**\n\n- blog: [http://nlpers.blogspot.ru/2016/11/bias-in-ml-and-teaching-ai.html](http://nlpers.blogspot.ru/2016/11/bias-in-ml-and-teaching-ai.html)\n- slides: [http://www.umiacs.umd.edu/~hal/talks/16-11-diversity-bias.odp](http://www.umiacs.umd.edu/~hal/talks/16-11-diversity-bias.odp)\n- mirror: [https://pan.baidu.com/s/1bpqWIkB](https://pan.baidu.com/s/1bpqWIkB)\n\n**Solutions for Skilltest Machine Learning : Revealed**\n\n[https://www.analyticsvidhya.com/blog/2016/11/solution-for-skilltest-machine-learning-revealed/](https://www.analyticsvidhya.com/blog/2016/11/solution-for-skilltest-machine-learning-revealed/)\n\n**Machine Learning Performance Improvement Cheat Sheet**\n\n- intro: 32 Tips, Tricks and Hacks That You Can Use To Make Better Predictions.\n- blog: [http://machinelearningmastery.com/machine-learning-performance-improvement-cheat-sheet/](http://machinelearningmastery.com/machine-learning-performance-improvement-cheat-sheet/)\n\n**What is better: gradient-boosted trees, or a random forest?**\n\n[http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/](http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/)\n\n**A Practical Guide to Tree Based Learning Algorithms**\n\n[https://sadanand-singh.github.io/posts/treebasedmodels/](https://sadanand-singh.github.io/posts/treebasedmodels/)\n\n## Model evaluation, model selection, and algorithm selection in machine learning\n\n**Part I - The basics**\n\n[http://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html](http://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html)\n\n**Part II - Bootstrapping and uncertainties**\n\n[http://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html](http://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html)\n\n**Part III - Cross-validation and hyperparameter tuning**\n\n[http://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html](http://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html)\n\n## ROC / AUC\n\nROC: Receiver Operating Characteristic\n\nAUC: Area Under the Curve\n\n**Tutorials: Plotting AP and ROC curves**\n\n[http://www.vlfeat.org/overview/plots-rank.html](http://www.vlfeat.org/overview/plots-rank.html)\n\n**Beautiful Properties Of The Roc Curve**\n\n![](https://www.unc.edu/courses/2006spring/ecol/145/001/images/lectures/lecture37/fig4.png)\n\n[http://jxieeducation.com/2016-09-27/Beautiful-Properties-Of-The-ROC-Curve/](http://jxieeducation.com/2016-09-27/Beautiful-Properties-Of-The-ROC-Curve/)\n\n**On calculating AUC**\n\n- blog: [http://www.win-vector.com/blog/2016/10/on-calculating-auc/](http://www.win-vector.com/blog/2016/10/on-calculating-auc/)\n\n**ROC to precision-recall curve translator**\n\n[https://rafalab.shinyapps.io/roc-precision-recall/](https://rafalab.shinyapps.io/roc-precision-recall/)\n\n## t-SNE\n\n**How to Use t-SNE Effectively**\n\n- blog: [http://distill.pub/2016/misread-tsne/](http://distill.pub/2016/misread-tsne/)\n- github: [https://github.com/distillpub/post--misread-tsne](https://github.com/distillpub/post--misread-tsne)\n\n# Libraries\n\n**LambdaNet: Purely functional artificial neural network library implemented in Haskell**\n\n- github: [https://github.com/jbarrow/LambdaNet](https://github.com/jbarrow/LambdaNet)\n\n**rustlearn: Machine learning crate for Rust**\n\n- github: [https://github.com/maciejkula/rustlearn](https://github.com/maciejkula/rustlearn)\n\n**MILJS : Brand New JavaScript Libraries for Matrix Calculation and Machine Learning**\n\n- arxiv: [http://arxiv.org/abs/1503.05743v1](http://arxiv.org/abs/1503.05743v1)\n- github: [https://github.com/mil-tokyo](https://github.com/mil-tokyo)\n- homepage: [http://mil-tokyo.github.io/](http://mil-tokyo.github.io/)\n\n**machineJS: Automated machine learning- just give it a data file!**\n\n- github: [https://github.com/ClimbsRocks/machineJS](https://github.com/ClimbsRocks/machineJS)\n\n**Machine Learning for iOS: Tools and resources to create really smart iOS applications**\n\n- homepage: [http://alexsosn.github.io/ml/2015/11/05/iOS-ML.html](http://alexsosn.github.io/ml/2015/11/05/iOS-ML.html)\n\n**DynaML: Scala Library/REPL for Machine Learning Research**\n\n- homepage: [http://mandar2812.github.io/DynaML/](http://mandar2812.github.io/DynaML/)\n- github: [https://github.com/mandar2812/DynaML/](https://github.com/mandar2812/DynaML/)\n\n**Smile - Statistical Machine Intelligence and Learning Engine**\n\n- intro: Smile is a fast and comprehensive machine learning system.\n- homepage: [http://haifengl.github.io/smile/index.html](http://haifengl.github.io/smile/index.html)\n- github: [https://github.com/haifengl/smile](https://github.com/haifengl/smile)\n\n**benchm-ml**\n\n- intro: A minimal benchmark for scalability, speed and accuracy of commonly used open source implementations \n(R packages, Python scikit-learn, H2O, xgboost, Spark MLlib etc.) of the top machine learning algorithms \nfor binary classification (random forests, gradient boosted trees, deep neural networks etc.).\n- github: [https://github.com/szilard/benchm-ml](https://github.com/szilard/benchm-ml)\n\n**KeystoneML: Simplifying robust end-to-end machine learning on Apache Spark**\n\n- intro: a software framework, written in Scala, from the UC Berkeley AMPLab designed to \nsimplify the construction of large scale, end-to-end, machine learning pipelines with Apache Spark.\n- homepage: [http://keystone-ml.org/](http://keystone-ml.org/)\n- github: [https://github.com/amplab/keystone](https://github.com/amplab/keystone)\n\n**Talisman: A straightforward & modular NLP, machine learning & fuzzy matching library for JavaScript**\n\n- homepage: [http://yomguithereal.github.io/talisman/](http://yomguithereal.github.io/talisman/)\n- github: [https://github.com/Yomguithereal/talisman](https://github.com/Yomguithereal/talisman)\n\n**PRMLT: Pattern Recognition and Machine Learning Toolbox**\n\n- homepage: [http://prml.github.io/](http://prml.github.io/)\n- github: [https://github.com/PRML/PRMLT](https://github.com/PRML/PRMLT)\n\n**The Fido Project: An open source C++ machine learning library targeted towards embedded electronics and robotics**\n\n![](https://fidoproject.github.io/img/simulator.gif)\n\n- homepage: [https://fidoproject.github.io/](https://fidoproject.github.io/)\n- github: [https://github.com/FidoProject/Fido](https://github.com/FidoProject/Fido)\n\n**rusty-machine: Machine Learning library for Rust**\n\n- homepage: [https://crates.io/crates/rusty-machine/](https://crates.io/crates/rusty-machine/)\n- github: [https://github.com/AtheMathmo/rusty-machine](https://github.com/AtheMathmo/rusty-machine)\n\n**RoBO - a Robust Bayesian Optimization framework**\n\n- github: [https://github.com/automl/RoBO](https://github.com/automl/RoBO)\n- docs: [http://robo-fork.readthedocs.io/en/latest/](http://robo-fork.readthedocs.io/en/latest/)\n\n**Dlib: A toolkit for making real world machine learning and data analysis applications in C++**\n\n- intro: Dlib is a modern C++ toolkit containing machine learning algorithms and tools \nfor creating complex software in C++ to solve real world problems.\n- homepage: [http://dlib.net/](http://dlib.net/)\n- github: [https://github.com/davisking/dlib](https://github.com/davisking/dlib)\n\n**Bayesian Networks and Bayesian Classifier Software**\n\n- blog: [http://www.kdnuggets.com/software/bayesian.html](http://www.kdnuggets.com/software/bayesian.html)\n\n**ML-lib: An extensive machine learning library, made from scratch (Python)**\n\n- github: [https://github.com/christopherjenness/ML-lib](https://github.com/christopherjenness/ML-lib)\n\n**Top Machine Learning Projects for Julia**\n\n- blog: [http://www.kdnuggets.com/2016/08/top-machine-learning-projects-julia.html](http://www.kdnuggets.com/2016/08/top-machine-learning-projects-julia.html)\n\n**Helit: My machine learning/computer vision library for all of my recent papers, plus algorithms that I just like.**\n\n- github: [https://github.com/thaines/helit](https://github.com/thaines/helit)\n\n**Gorgonia: a library that helps facilitate machine learning in Go**\n\n- github: [https://github.com/chewxy/gorgonia](https://github.com/chewxy/gorgonia)\n- blog: [http://blog.chewxy.com/2016/09/19/gorgonia](http://blog.chewxy.com/2016/09/19/gorgonia)\n\n**GoLearn: Machine Learning for Go**\n\n- github: [https://github.com/sjwhitworth/golearn](https://github.com/sjwhitworth/golearn)\n\n**Cortex: Machine learning in Clojure**\n\n- intro: Neural networks, regression and feature learning in Clojure.\n- github: [https://github.com/thinktopic/cortex](https://github.com/thinktopic/cortex)\n\n**ELI5: A library for debugging machine learning classifiers and explaining their predictions**\n\n- github: [https://github.com/TeamHG-Memex/eli5](https://github.com/TeamHG-Memex/eli5)\n\n**PHP-ML - Machine Learning library for PHP**\n\n- github: [https://github.com/php-ai/php-ml](https://github.com/php-ai/php-ml)\n- github: [https://github.com/php-ai/php-ml-examples](https://github.com/php-ai/php-ml-examples)\n- docs: [http://php-ml.readthedocs.io/en/latest/](http://php-ml.readthedocs.io/en/latest/)\n\n**ml.js - Machine learning tools in JavaScript**\n\n[https://github.com/mljs/ml](https://github.com/mljs/ml)\n\n**Propel**\n\n- intro: A Machine Learning Framework for JavaScript / Differential Programming in JavaScript\n- homepage: [http://propelml.org/](http://propelml.org/)\n- github: [https://github.com/propelml/propel](https://github.com/propelml/propel)\n\n# Resources\n\n**Machine Learning Surveys: A list of literature surveys, reviews, and tutorials on Machine Learning and related topics**\n\n[http://www.mlsurveys.com/](http://www.mlsurveys.com/)\n\n**machine learning classifier gallery**\n\n[http://home.comcast.net/~tom.fawcett/public_html/ML-gallery/pages/](http://home.comcast.net/~tom.fawcett/public_html/ML-gallery/pages/)\n\n**Machine Learning and Computer Vision Resources**\n\n[http://zhengrui.github.io/zerryland/ML-CV-Resource.html](http://zhengrui.github.io/zerryland/ML-CV-Resource.html)\n\n**A Huge List of Machine Learning And Statistics Repositories**\n\n[http://blog.josephmisiti.com/a-huge-list-of-machine-learning-repositories/](http://blog.josephmisiti.com/a-huge-list-of-machine-learning-repositories/)\n\n**Machine Learning in Python Course**\n\n[https://www.springboard.com/learning-paths/machine-learning-python/](https://www.springboard.com/learning-paths/machine-learning-python/)\n\n**机器学习(Machine Learning)&深度学习(Deep Learning)资料(Chapter 1)**\n\n[https://github.com/ty4z2008/Qix/blob/master/dl.md](https://github.com/ty4z2008/Qix/blob/master/dl.md)\n\n**The Spectator: Shakir's Machine Learning Blog**\n\n[http://blog.shakirm.com/](http://blog.shakirm.com/)\n\n**Useful Inequalities**\n\n[http://www.lkozma.net/inequalities_cheat_sheet/ineq.pdf](http://www.lkozma.net/inequalities_cheat_sheet/ineq.pdf)\n\n**Math for Machine Learning**\n\n[http://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf](http://www.umiacs.umd.edu/~hal/courses/2013S_ML/math4ml.pdf)\n\n**Cheat Sheet: Algorithms for Supervised- and Unsupervised Learning**\n\n- blog: [http://eferm.com/machine-learning-cheat-sheet/](http://eferm.com/machine-learning-cheat-sheet/)\n\n**Annalyzin: Analytics For Layman, with Tutorials & Experiments**\n\n[https://annalyzin.wordpress.com/](https://annalyzin.wordpress.com/)\n\n**ALGORITHMS: AI, Data Mining, Clustering, Data Structures, Machine Learning, Neural, NLP, ...**\n\n- github: [https://github.com/svaksha/pythonidae/blob/master/AI.md](https://github.com/svaksha/pythonidae/blob/master/AI.md)\n\n**Awesome Machine Learning: A curated list of awesome machine learning frameworks, libraries and software (by language)**\n\n- github: [https://github.com/josephmisiti/awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning)\n\n**awesome-machine-learning-cn: 机器学习资源大全中文版**\n\n- intro: 机器学习资源大全中文版，包括机器学习领域的框架、库以及软件\n- github: [https://github.com/jobbole/awesome-machine-learning-cn](https://github.com/jobbole/awesome-machine-learning-cn)\n\n**Machine and Deep Learning with Python**\n\n- github: [https://github.com/szwed/awesome-machine-learning-python](https://github.com/szwed/awesome-machine-learning-python)\n\n**useR! 2016 Tutorial: Machine Learning Algorithmic Deep Dive**\n\n![](https://raw.githubusercontent.com/ledell/useR-machine-learning-tutorial/master/images/user2016.png)\n\n- homepage: [http://user2016.org/tutorials/10.html](http://user2016.org/tutorials/10.html)\n- github: [https://github.com/ledell/useR-machine-learning-tutorial](https://github.com/ledell/useR-machine-learning-tutorial)\n\n**Top-down learning path: Machine Learning for Software Engineers**\n\n- intro: A complete daily plan for studying to become a machine learning engineer.\n- github: [https://github.com/ZuzooVn/machine-learning-for-software-engineers](https://github.com/ZuzooVn/machine-learning-for-software-engineers)\n\n**30 Top Videos, Tutorials & Courses on Machine Learning & Artificial Intelligence from 2016**\n[https://www.analyticsvidhya.com/blog/2016/12/30-top-videos-tutorials-courses-on-machine-learning-artificial-intelligence-from-2016/](https://www.analyticsvidhya.com/blog/2016/12/30-top-videos-tutorials-courses-on-machine-learning-artificial-intelligence-from-2016/)\n\n**Machine Learning Problem Bible (MLPB)**\n\n- github: [https://github.com/ben519/MLPB](https://github.com/ben519/MLPB)\n\n**The most shared Machine Learning conten on Twitter from the past 7 days**\n\n- Based on the millions of #machinelearning tweets already processed by [The Herd Locker](http://theherdlocker.com/tweet/popularity/machinelearning), noise is a little over 94% of the conversation. Tracking the 8,000 daily tweets that are tagged #machineLearning, the platform filters and ranks the most popular shared content in realtime. Machine learning’s zeitgeist, you might say. It's been running for over a year, monitoring half a billion tweets a day, and will always be free to use. No ads. No BS. [http://theherdlocker.com/tweet/popularity/machinelearning](http://theherdlocker.com/tweet/popularity/machinelearning)\n\n# Projects\n\n**Machine learning algorithms： Minimal and clean examples of machine learning algorithms**\n\n- intro: A collection of minimal and clean implementations of machine learning algorithms.\n- github: [https://github.com/rushter/MLAlgorithms](https://github.com/rushter/MLAlgorithms)\n\n**Plotting high-dimensional decision boundaries**\n\n![](https://raw.githubusercontent.com/tmadl/highdimensional-decision-boundary-plot/master/img/dr_model_overview.png)\n\n- github: [https://github.com/tmadl/highdimensional-decision-boundary-plot](https://github.com/tmadl/highdimensional-decision-boundary-plot)\n\n**Flappy Learning: Program that learns to play Flappy Bird by machine learning (Neuroevolution)**\n\n- blog: [https://xviniette.github.io/FlappyLearning/](https://xviniette.github.io/FlappyLearning/)\n- github: [https://github.com/xviniette/FlappyLearning](https://github.com/xviniette/FlappyLearning)\n\n# Readings /  Questions / Discussions\n\n**A Super Harsh Guide to Machine Learning**\n\n[https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/](https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/)\n\n**(Quora): What are the top 10 data mining or machine learning algorithms?**\n\n[https://www.quora.com/What-are-the-top-10-data-mining-or-machine-learning-algorithms/answer/Xavier-Amatriain](https://www.quora.com/What-are-the-top-10-data-mining-or-machine-learning-algorithms/answer/Xavier-Amatriain)\n\n**(Quora): What are the must read papers on data mining and machine learning?**\n\n[https://www.quora.com/What-are-the-must-read-papers-on-data-mining-and-machine-learning](https://www.quora.com/What-are-the-must-read-papers-on-data-mining-and-machine-learning)\n\n**(Quora): What would be your advice to a software engineer who wants to learn machine learning?**\n[https://www.quora.com/What-would-be-your-advice-to-a-software-engineer-who-wants-to-learn-machine-learning-3/answer/Alex-Smola-1](https://www.quora.com/What-would-be-your-advice-to-a-software-engineer-who-wants-to-learn-machine-learning-3/answer/Alex-Smola-1)\n\n**Machine Learning FAQ**\n\n- homepage: [http://sebastianraschka.com/faq/index.html](http://sebastianraschka.com/faq/index.html)\n\n**MLNotes: Very concise notes on machine learning and statistics**\n\n- github: [https://github.com/johnmyleswhite/MLNotes](https://github.com/johnmyleswhite/MLNotes)\n\n**Machine Learning Problem Bible (MLPB)**\n\n- github: [https://github.com/ben519/MLPB](https://github.com/ben519/MLPB)\n\n**List of machine learning concepts**\n\n- wiki: [https://en.wikipedia.org/wiki/List_of_machine_learning_concepts](https://en.wikipedia.org/wiki/List_of_machine_learning_concepts)\n\n**What is the relation between Logistic Regression and Neural Networks and when to use which?**\n\n- blog: [http://sebastianraschka.com/faq/docs/logisticregr-neuralnet.html](http://sebastianraschka.com/faq/docs/logisticregr-neuralnet.html)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","title":"Re-ID"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Re-ID\ndate: 2015-10-09\n---\n\n# Leaderboard\n\n## ReID\n\n| Method         | backbone   | test size | Market1501    | CUHK03 (detected)     | CUHK03 (detected/new) | CUHK03 (labeled/new) | CUHK-SYSU   | DukeMTMC-reID | MARS          |\n| :------------: | :-----:    | :-----:   | :-----:       | :-----:               | :----------:          | :---------:          | :--------:  | :----------:  | :--------:    |\n|                |            |           | rank1 / mAP   | rank1/rank5/rank10    | rank1 / mAP           | rank1 / mAP          | rank1 / mAP |               | rank1 / mAP   |\n| [DG-Net](https://github.com/NVlabs/DG-Net)    | ResNet-50  | 256×128   |  94.8/ 86.0 |     |      65.6/61.1          |                      |             |         86.6/74.8      |           |\n| AlignedReID    | ResNet50-X |           | 92.6 / 82.3   | 91.9 / 98.7 / 99.4    |                       | 86.8 / 79.1          |             |               | 95.3 / 93.7   |\n| Deep-Person    | ResNet-50  | 256×128   | 92.31 / 79.58 | 89.4 / 98.2 / 99.1    |                       |                      |             |               | 80.90 / 64.80 |\n| PCB            | ResNet-50  | 384x128   | 92.4 / 77.3   |                       | 61.3 / 54.2           |                      |             | 81.9 / 65.3   |               |\n| PCB+RPP        | ResNet-50  | 384x128   | 93.8 / 81.6   |                       | 63.7 / 57.5           |                      |             | 83.3 / 69.2   |               |\n| PN-GAN         | ResNet-50  |           | 89.43 / 72.58 | 79.76 / 96.24 / 98.56 |                       |                      |             | 73.58 / 53.20 |               |\n| MGN            | ResNet-50  |           | 95.7 /  86.9  |                       | 66.8 / 66.0           | 68.0 / 67.4          |             | 88.7 / 78.4   |               |\n| HPM            | ResNet-50  | 384x128   | 94.2 /  82.7  |                       | 63.1 / 57.5           |                      |             | 86.6 / 74.3   |               |\n| HPM+HRE        | ResNet-50  | 384x128   | 93.9 /  83.1  |                       | 63.2 / 59.7           |                      |             | 86.3 / 74.5   |               |\n| SphereReID     | ResNet-50  | 288×144   | 94.4 /  83.6  | 93.1 / 98.7 / 99.4    | 63.2 / 59.7           |                      | 95.4 / 93.9 | 83.9 / 68.5   |               |\n| Auto-ReID      |            | 384x128   | 94.5 /  85.1  |                       | 73.3 / 69.3           | 77.9 / 73.0          |             | 88.5 / 75.1   |               |\n\n## PersonSearch\n\n| Method                                                                                                    | backbone  | CUHK-SYSU     | PRW          | PRW-mini    |\n| :------------:                                                                                            | :-----:   | :-----:       | :-----:      | :---------: |\n|                                                                                                           |           | top1 / mAP    | top1 / mAP   | top1 / mAP  |\n| [Joint Detection and Identification Feature Learning for Person Search](https://arxiv.org/abs/1604.01850) | ResNet-50 | 78.7 / 75.5   | -- / --      | -- / --     |\n| [Learning Context Graph for Person Search](https://arxiv.org/abs/1904.01830)                              | ResNet-50 | 86.5 / 84.1   | 73.6 / 33.4  | -- / --     |\n| [Knowledge Distillation for End-to-End Person Search](https://arxiv.org/abs/1909.01058)                   | ResNet-50 | 88.5 / 87.2   | -- / --      | 70.0 / 33.1 |\n| [Query-guided End-to-End Person Search](https://arxiv.org/abs/1905.01203)                                 | ResNet-50 | 89.1 / 88.9   | 76.7 / 37.1  | 80.0 / 39.1 |\n| [Fast Person Search Pipeline](https://ieeexplore.ieee.org/abstract/document/8784982/)                     | ResNet-50 | 89.87 / 86.99 | 70.58/ 44.45 | -- / --     |\n| End-to-End Thorough Body Perception for Person Search                                                     | ResNet-50 | 90.5 / 88.4   | 68.9 / 42.9  | -- / --     |\n| [Re-ID Driven Localization Refinement for Person Search](https://arxiv.org/abs/1909.08580)                | ResNet-50 | 94.2 / 93.0   | 70.2 / 42.9  | -- / --     |\n\n# Papers\n\n**Uncertainty-Aware Multi-Shot Knowledge Distillation for Image-Based Object Re-Identification**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/2001.05197](https://arxiv.org/abs/2001.05197)\n\n**Robust Re-Identification by Multiple Views Knowledge Distillation**\n\n- intro: ECCV 2020\n- intro: University of Modena and Reggio Emilia\n- arxiv: [https://arxiv.org/abs/2007.04174](https://arxiv.org/abs/2007.04174)\n- github: [https://github.com/aimagelab/VKD](https://github.com/aimagelab/VKD)\n\n**Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID**\n\n- intro: NeurIPS 2020\n- intro: CUHK\n- project apge: [https://geyixiao.com/projects/spcl.html](https://geyixiao.com/projects/spcl.html)\n- arxiv: [https://arxiv.org/abs/2006.02713](https://arxiv.org/abs/2006.02713)\n- github: [https://github.com/yxgeee/SpCL](https://github.com/yxgeee/SpCL)\n- github: [https://github.com/open-mmlab/OpenUnReID](https://github.com/open-mmlab/OpenUnReID)\n\n**Context-Aware Graph Convolution Network for Target Re-identification**\n\n- intro: AAAI 2021\n- intro: SenseTime Research & Xidian University & Peking University\n- arxiv: [https://arxiv.org/abs/2012.04298](https://arxiv.org/abs/2012.04298)\n\n# Person Re-identification / Person Retrieval\n\n**DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification**\n\n- intro: CVPR 2014\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Li_DeepReID_Deep_Filter_2014_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Li_DeepReID_Deep_Filter_2014_CVPR_paper.pdf)\n\n**An Improved Deep Learning Architecture for Person Re-Identification**\n\n- intro: CVPR 2015\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf)\n- github: [https://github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID](https://github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID)\n\n**Deep Ranking for Person Re-identification via Joint Representation Learning**\n\n- intro: IEEE Transactions on Image Processing (TIP), 2016\n- arxiv: [https://arxiv.org/abs/1505.06821](https://arxiv.org/abs/1505.06821)\n\n**PersonNet: Person Re-identification with Deep Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1601.07255](http://arxiv.org/abs/1601.07255)\n\n**Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification**\n\n- intro: CVPR 2016\n- arxiv: [https://arxiv.org/abs/1604.07528](https://arxiv.org/abs/1604.07528)\n- github: [https://github.com/Cysu/dgd_person_reid](https://github.com/Cysu/dgd_person_reid)\n\n**Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function**\n\n- intro: CVPR 2016\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf)\n\n**Joint Learning of Single-image and Cross-image Representations for Person Re-identification**\n\n- intro: CVPR 2016\n- paper: [http://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Joint_Learning_of_CVPR_2016_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Joint_Learning_of_CVPR_2016_paper.pdf)\n\n**End-to-End Comparative Attention Networks for Person Re-identification**\n\n[https://arxiv.org/abs/1606.04404](https://arxiv.org/abs/1606.04404)\n\n**A Multi-task Deep Network for Person Re-identification**\n\n- intro: AAAI 2017\n- arxiv: [http://arxiv.org/abs/1607.05369](http://arxiv.org/abs/1607.05369)\n\n**A Siamese Long Short-Term Memory Architecture for Human Re-Identification**\n\n- arxiv: [http://arxiv.org/abs/1607.08381](http://arxiv.org/abs/1607.08381)\n\n**Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification**\n\n- intro: ECCV 2016\n- keywords: Market1501 rank1 = 65.9%\n- arxiv: [https://arxiv.org/abs/1607.08378](https://arxiv.org/abs/1607.08378)\n\n**Deep Neural Networks with Inexact Matching for Person Re-Identification**\n\n- intro: NIPS 2016\n- keywords: Normalized correlation layer, CUHK03/CUHK01/QMULGRID\n- paper: [https://papers.nips.cc/paper/6367-deep-neural-networks-with-inexact-matching-for-person-re-identification](https://papers.nips.cc/paper/6367-deep-neural-networks-with-inexact-matching-for-person-re-identification)\n- github: [https://github.com/InnovArul/personreid_normxcorr](https://github.com/InnovArul/personreid_normxcorr)\n\n**Person Re-identification: Past, Present and Future**\n\n[https://arxiv.org/abs/1610.02984](https://arxiv.org/abs/1610.02984)\n\n**Deep Learning Prototype Domains for Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/1610.05047](https://arxiv.org/abs/1610.05047)\n\n**Deep Transfer Learning for Person Re-identification**\n\n- arxiv: [https://arxiv.org/abs/1611.05244](https://arxiv.org/abs/1611.05244)\n\n**A Discriminatively Learned CNN Embedding for Person Re-identification**\n\n- intro: TOMM 2017\n- arxiv: [https://arxiv.org/abs/1611.05666](https://arxiv.org/abs/1611.05666)\n- github(official, MatConvnet): [https://github.com/layumi/2016_person_re-ID](https://github.com/layumi/2016_person_re-ID)\n- github: [https://github.com/D-X-Y/caffe-reid](https://github.com/D-X-Y/caffe-reid)\n\n**Person Re-Identification via Recurrent Feature Aggregation**\n\n- intro: ECCV 2016\n- keywords: recurrent feature aggregation network (RFA-Net)\n- arxiv: [https://arxiv.org/abs/1701.06351](https://arxiv.org/abs/1701.06351)\n- code: [https://sites.google.com/site/yanyichao91sjtu/](https://sites.google.com/site/yanyichao91sjtu/)\n- github(official): [https://github.com/daodaofr/caffe-re-id](https://github.com/daodaofr/caffe-re-id)\n\n**Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification**\n\n- arxiv: [https://arxiv.org/abs/1702.04179](https://arxiv.org/abs/1702.04179)\n\n**SVDNet for Pedestrian Retrieval**\n\n- intro: ICCV 2017 spotlight\n- intro: On the Market-1501 dataset, rank-1 accuracy is improved from 55.2% to 80.5% for CaffeNet, \nand from 73.8% to 83.1% for ResNet-50\n- arxiv: [https://arxiv.org/abs/1703.05693](https://arxiv.org/abs/1703.05693)\n- github: [https://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval](https://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval)\n\n**In Defense of the Triplet Loss for Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/1703.07737](https://arxiv.org/abs/1703.07737)\n- github(official, Theano): [https://github.com/VisualComputingInstitute/triplet-reid](https://github.com/VisualComputingInstitute/triplet-reid)\n\n**Beyond triplet loss: a deep quadruplet network for person re-identification**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.01719](https://arxiv.org/abs/1704.01719)\n- ppaper: [http://cvip.computing.dundee.ac.uk/papers/Chen_CVPR_2017_paper.pdf](http://cvip.computing.dundee.ac.uk/papers/Chen_CVPR_2017_paper.pdf)\n\n**Quality Aware Network for Set to Set Recognition**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.03373](https://arxiv.org/abs/1704.03373)\n- github: [https://github.com/sciencefans/Quality-Aware-Network](https://github.com/sciencefans/Quality-Aware-Network)\n\n**Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification**\n\n- intro: CVPR 2017. CASIA\n- keywords: Multi-Scale Context-Aware Network (MSCAN)\n- arxiv: [https://arxiv.org/abs/1710.06555](https://arxiv.org/abs/1710.06555)\n- supplemental: [http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Learning_Deep_Context-Aware_2017_CVPR_supplemental.pdf](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Learning_Deep_Context-Aware_2017_CVPR_supplemental.pdf)\n\n**Point to Set Similarity Based Deep Feature Learning for Person Re-identification**\n\n- intro: CVPR 2017\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf)\n- github(stay tuned): [https://github.com/samaonline/Point-to-Set-Similarity-Based-Deep-Feature-Learning-for-Person-Re-identification](https://github.com/samaonline/Point-to-Set-Similarity-Based-Deep-Feature-Learning-for-Person-Re-identification)\n\n**Scalable Person Re-identification on Supervised Smoothed Manifold**\n\n- intro: CVPR 2017 spotlight\n- arxiv: [https://arxiv.org/abs/1703.08359](https://arxiv.org/abs/1703.08359)\n- youtube: [https://www.youtube.com/watch?v=bESdJgalQrg](https://www.youtube.com/watch?v=bESdJgalQrg)\n\n**Attention-based Natural Language Person Retrieval**\n\n- intro: CVPR 2017 Workshop (vision meets cognition)\n- keywords: Bidirectional Long Short- Term Memory (BLSTM)\n- arxiv: [https://arxiv.org/abs/1705.08923](https://arxiv.org/abs/1705.08923)\n\n**Part-based Deep Hashing for Large-scale Person Re-identification**\n\n- intro: IEEE Transactions on Image Processing, 2017\n- arxiv: [https://arxiv.org/abs/1705.02145](https://arxiv.org/abs/1705.02145)\n\n**Deep Person Re-Identification with Improved Embedding**\n\n**Deep Person Re-Identification with Improved Embedding and Efficient Training**\n\n- intro: IJCB 2017\n- arxiv: [https://arxiv.org/abs/1705.03332](https://arxiv.org/abs/1705.03332)\n\n**Towards a Principled Integration of Multi-Camera Re-Identification and Tracking through Optimal Bayes Filters**\n\n- arxiv: [https://arxiv.org/abs/1705.04608](https://arxiv.org/abs/1705.04608)\n- github: [https://github.com/VisualComputingInstitute/towards-reid-tracking](https://github.com/VisualComputingInstitute/towards-reid-tracking)\n\n**Person Re-Identification by Deep Joint Learning of Multi-Loss Classification**\n\n- intro: IJCAI 2017\n- arxiv: [https://arxiv.org/abs/1705.04724](https://arxiv.org/abs/1705.04724)\n\n**Deep Representation Learning with Part Loss for Person Re-Identification**\n\n- keywords: Part Loss Networks\n- arxiv: [https://arxiv.org/abs/1707.00798](https://arxiv.org/abs/1707.00798)\n\n**Pedestrian Alignment Network for Large-scale Person Re-identification**\n\n- arxiv: [https://arxiv.org/abs/1707.00408](https://arxiv.org/abs/1707.00408)\n- github: [https://github.com/layumi/Pedestrian_Alignment](https://github.com/layumi/Pedestrian_Alignment)\n\n**Learning Efficient Image Representation for Person Re-Identification**\n\n[https://arxiv.org/abs/1707.02319](https://arxiv.org/abs/1707.02319)\n\n**Person Re-identification Using Visual Attention**\n\n- intro: ICIP 2017\n- arxiv: [https://arxiv.org/abs/1707.07336](https://arxiv.org/abs/1707.07336)\n\n**What-and-Where to Match: Deep Spatially Multiplicative Integration Networks for Person Re-identification**\n\n[https://arxiv.org/abs/1707.07074](https://arxiv.org/abs/1707.07074)\n\n**Deep Feature Learning via Structured Graph Laplacian Embedding for Person Re-Identification**\n\n[https://arxiv.org/abs/1707.07791](https://arxiv.org/abs/1707.07791)\n\n**Large Margin Learning in Set to Set Similarity Comparison for Person Re-identification**\n\n- intro: IEEE Transactions on Multimedia\n- arxiv: [https://arxiv.org/abs/1708.05512](https://arxiv.org/abs/1708.05512)\n\n**Multi-scale Deep Learning Architectures for Person Re-identification**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.05165](https://arxiv.org/abs/1709.05165)\n\n**Person Re-Identification by Deep Learning Multi-Scale Representations**\n\n- intro: ICCV 2017\n- keywords: Deep Pyramid Feature Learning (DPFL)\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w37/Chen_Person_Re-Identification_by_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w37/Chen_Person_Re-Identification_by_ICCV_2017_paper.pdf)\n- paper: [http://www.eecs.qmul.ac.uk/~sgg/papers/ChenEtAl_ICCV2017WK_CHI.pdf](http://www.eecs.qmul.ac.uk/~sgg/papers/ChenEtAl_ICCV2017WK_CHI.pdf)\n\n**Person Re-Identification with Vision and Language**\n\n[https://arxiv.org/abs/1710.01202](https://arxiv.org/abs/1710.01202)\n\n**Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification**\n\n[https://arxiv.org/abs/1710.00478](https://arxiv.org/abs/1710.00478)\n\n**Pseudo-positive regularization for deep person re-identification**\n\n[https://arxiv.org/abs/1711.06500](https://arxiv.org/abs/1711.06500)\n\n**Let Features Decide for Themselves: Feature Mask Network for Person Re-identification**\n\n- keywords: Feature Mask Network (FMN)\n- arxiv: [https://arxiv.org/abs/1711.07155](https://arxiv.org/abs/1711.07155)\n\n**AlignedReID: Surpassing Human-Level Performance in Person Re-Identification**\n\n- intro: Megvii Inc & Zhejiang University\n- arxiv: [https://arxiv.org/abs/1711.08184](https://arxiv.org/abs/1711.08184)\n- evaluation website: (Market1501): [http://reid-challenge.megvii.com/](http://reid-challenge.megvii.com/)\n- evaluation website: (CUHK03): [http://reid-challenge.megvii.com/cuhk03](http://reid-challenge.megvii.com/cuhk03)\n- github: [https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch](https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch)\n\n**Deep Cosine Metric Learning for Person Re-Identification**\n\n- intro: WACV 2018\n- paper: [https://elib.dlr.de/116408/1/WACV2018.pdf](https://elib.dlr.de/116408/1/WACV2018.pdf)\n- github: [https://github.com/nwojke/cosine_metric_learning](https://github.com/nwojke/cosine_metric_learning)\n\n**Region-based Quality Estimation Network for Large-scale Person Re-identification**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1711.08766](https://arxiv.org/abs/1711.08766)\n\n**Beyond Part Models: Person Retrieval with Refined Part Pooling**\n\n- intro: ECCV 2018\n- keywords: Part-based Convolutional Baseline (PCB), Refined Part Pooling (RPP)\n- arxiv: [https://arxiv.org/abs/1711.09349](https://arxiv.org/abs/1711.09349)\n- github: [https://github.com/syfafterzy/PCB_RPP_for_reID](https://github.com/syfafterzy/PCB_RPP_for_reID)\n\n**Deep-Person: Learning Discriminative Deep Features for Person Re-Identification**\n\n[https://arxiv.org/abs/1711.10658](https://arxiv.org/abs/1711.10658)\n\n**Hierarchical Cross Network for Person Re-identification**\n\n[https://arxiv.org/abs/1712.06820](https://arxiv.org/abs/1712.06820)\n\n**Re-ID done right: towards good practices for person re-identification**\n\n[https://arxiv.org/abs/1801.05339](https://arxiv.org/abs/1801.05339)\n\n**Triplet-based Deep Similarity Learning for Person Re-Identification**\n\n- intro: ICCV Workshops 2017\n- arxiv: [https://arxiv.org/abs/1802.03254](https://arxiv.org/abs/1802.03254)\n\n**Group Consistent Similarity Learning via Deep CRFs for Person Re-Identification**\n\n- intro: CVPR 2018 oral\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Group_Consistent_Similarity_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Group_Consistent_Similarity_CVPR_2018_paper.pdf)\n- github(official, PyTorch): [https://github.com/dapengchen123/crf_affinity](https://github.com/dapengchen123/crf_affinity)\n\n**Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification**\n\n- intro: CVPR 2018\n- keywords: similarity preserving generative adversarial network (SPGAN), Siamese network, CycleGAN, domain adaptation\n- arxiv: [https://arxiv.org/abs/1711.07027](https://arxiv.org/abs/1711.07027)\n\n**Similarity-preserving Image-image Domain Adaptation for Person Re-identification**\n\n[https://arxiv.org/abs/1811.10551](https://arxiv.org/abs/1811.10551)\n\n**Harmonious Attention Network for Person Re-Identification**\n\n- intro: CVPR 2018\n- keywords: Harmonious Attention CNN (HA-CNN)\n- arxiv: [https://arxiv.org/abs/1802.08122](https://arxiv.org/abs/1802.08122)\n\n**Camera Style Adaptation for Person Re-identfication**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1711.10295](https://arxiv.org/abs/1711.10295)\n- github: [https://github.com/zhunzhong07/CamStyle](https://github.com/zhunzhong07/CamStyle)\n\n**Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1711.07027](https://arxiv.org/abs/1711.07027)\n\n**Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.09937](https://arxiv.org/abs/1803.09937)\n\n**Multi-Level Factorisation Net for Person Re-Identification**\n\n- intro: CVPR 2018\n- keywords: Multi-Level Factorisation Net (MLFN)\n- arxiv: [https://arxiv.org/abs/1803.09132](https://arxiv.org/abs/1803.09132)\n\n**Features for Multi-Target Multi-Camera Tracking and Re-Identification**\n\n**Good Appearance Features for Multi-Target Multi-Camera Tracking**\n\n- intro: CVPR 2018 spotlight\n- intro: Duke University\n- keywords: adaptive weighted triplet loss, hard-identity mining\n- project page: [http://vision.cs.duke.edu/DukeMTMC/](http://vision.cs.duke.edu/DukeMTMC/)\n- arxiv: [https://arxiv.org/abs/1803.10859](https://arxiv.org/abs/1803.10859)\n\n**Mask-guided Contrastive Attention Model for Person Re-Identification**\n\n- intro: CVPR 2018\n- keywords: MGCAM\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf)\n- github: [https://github.com/developfeng/MGCAM](https://github.com/developfeng/MGCAM)\n\n**Efficient and Deep Person Re-Identification using Multi-Level Similarity**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.11353](https://arxiv.org/abs/1803.11353)\n\n**Person Re-identification with Cascaded Pairwise Convolutions**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Person_Re-Identification_With_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Person_Re-Identification_With_CVPR_2018_paper.pdf)\n\n**Attention-Aware Compositional Network for Person Re-identification**\n\n- intro: CVPR 2018\n- intro: Sensets Technology Limited & University of Sydney\n- keywords: Attention-Aware Compositional Network (AACN), Pose-guided Part Attention (PPA), Attention-aware Feature Composition (AFC)\n- arxiv: [https://arxiv.org/abs/1805.03344](https://arxiv.org/abs/1805.03344)\n\n**Deep Group-shuffling Random Walk for Person Re-identification**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.pdf)\n- github: [https://github.com/YantaoShen/kpm_rw_person_reid](https://github.com/YantaoShen/kpm_rw_person_reid)\n\n**Adversarially Occluded Samples for Person Re-identification**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.pdf)\n- github: [https://github.com/huanghoujing/AOS4ReID](https://github.com/huanghoujing/AOS4ReID)\n\n**Easy Identification from Better Constraints: Multi-Shot Person Re-Identification from Reference Constraints**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Easy_Identification_From_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Easy_Identification_From_CVPR_2018_paper.pdf)\n\n**Eliminating Background-bias for Robust Person Re-identification**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.pdf)\n\n**End-to-End Deep Kronecker-Product Matching for Person Re-identification**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.pdf)\n- github: [https://github.com/YantaoShen/kpm_rw_person_reid](https://github.com/YantaoShen/kpm_rw_person_reid)\n\n**Exploiting Transitivity for Learning Person Re-identification Models on a Budget**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.pdf)\n\n**Resource Aware Person Re-identification across Multiple Resolutions**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Resource_Aware_Person_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Resource_Aware_Person_CVPR_2018_paper.pdf)\n- arxiv: [https://arxiv.org/abs/1805.08805](https://arxiv.org/abs/1805.08805)\n\n**Multi-Channel Pyramid Person Matching Network for Person Re-Identification**\n\n- intro: 32nd AAAI Conference on Artificial Intelligence\n- keywords: Multi-Channel deep convolutional Pyramid Person Matching Network (MC-PPMN)\n- arxiv: [https://arxiv.org/abs/1803.02558](https://arxiv.org/abs/1803.02558)\n\n**Pyramid Person Matching Network for Person Re-identification**\n\n- intro: 9th Asian Conference on Machine Learning (ACML2017) JMLR Workshop and Conference Proceedings\n- arxiv: [https://arxiv.org/abs/1803.02547](https://arxiv.org/abs/1803.02547)\n\n**Virtual CNN Branching: Efficient Feature Ensemble for Person Re-Identification**\n\n[https://arxiv.org/abs/1803.05872](https://arxiv.org/abs/1803.05872)\n\n**Weighted Bilinear Coding over Salient Body Parts for Person Re-identification**\n\n[https://arxiv.org/abs/1803.08580](https://arxiv.org/abs/1803.08580)\n\n**Adversarial Binary Coding for Efficient Person Re-identification**\n\n[https://arxiv.org/abs/1803.10914](https://arxiv.org/abs/1803.10914)\n\n**Learning View-Specific Deep Networks for Person Re-Identification**\n\n- intro: IEEE Transactions on image processing. Sun Yat-Sen University\n- keywords: cross-view Euclidean constraint (CV-EC), cross-view center loss (CV-CL)\n- arxiv: [https://arxiv.org/abs/1803.11333](https://arxiv.org/abs/1803.11333)\n\n**Learning Discriminative Features with Multiple Granularities for Person Re-Identification**\n\n- intro: ACM Multimedia 2018\n- intro: Shanghai Jiao Tong University & CloudWalk\n- keywords: Multiple Granularity Network (MGN)\n- arxiv: [https://arxiv.org/abs/1804.01438](https://arxiv.org/abs/1804.01438)\n- video: [https://edu.csdn.net/course/play/8426](https://edu.csdn.net/course/play/8426)\n\n**Recurrent Neural Networks for Person Re-identification Revisited**\n\n- intro: Stanford University & Google AI\n- arxiv: [https://arxiv.org/abs/1804.03281](https://arxiv.org/abs/1804.03281)\n\n**MaskReID: A Mask Based Deep Ranking Neural Network for Person Re-identification**\n\n[https://arxiv.org/abs/1804.03864](https://arxiv.org/abs/1804.03864)\n\n**Horizontal Pyramid Matching for Person Re-identification**\n\n- intro: AAAI 2019\n- intro: UIUC & IBM Research & Cornell University & Stevens Institute of Technology & CloudWalk Technology\n- keywords: Horizontal Pyramid Matching (HPM), Horizontal Pyramid Pooling (HPP), horizontal random erasing (HRE)\n- arxiv: [https://arxiv.org/abs/1804.05275](https://arxiv.org/abs/1804.05275)\n- github: [https://github.com/OasisYang/HPM](https://github.com/OasisYang/HPM)\n\n**Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification**\n\n[https://arxiv.org/abs/1804.11027](https://arxiv.org/abs/1804.11027)\n\n**Feature Affinity based Pseudo Labeling for Semi-supervised Person Re-identification**\n\n[https://arxiv.org/abs/1805.06118](https://arxiv.org/abs/1805.06118)\n\n**Semantically Selective Augmentation for Deep Compact Person Re-Identification**\n\n[https://arxiv.org/abs/1806.04074](https://arxiv.org/abs/1806.04074)\n\n**SphereReID: Deep Hypersphere Manifold Embedding for Person Re-Identification**\n\n- keywords: Sphere Loss, feature normalization, weight normalization, balanced sampling\n- arxiv: [https://arxiv.org/abs/1807.00537](https://arxiv.org/abs/1807.00537)\n\n**Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification**\n\n- intro: BMVC 2018\n- intro: University of Warwick & Nanyang Technological University & Charles Sturt University\n- arxiv: [https://arxiv.org/abs/1807.01440](https://arxiv.org/abs/1807.01440)\n\n**Discriminative Feature Learning with Foreground Attention for Person Re-Identification**\n\n[https://arxiv.org/abs/1807.01455](https://arxiv.org/abs/1807.01455)\n\n**Part-Aligned Bilinear Representations for Person Re-identification**\n\n- intro: ECCV 2018\n- intro: Seoul National University & Microsoft Research & Max Planck Institute & University of Tubingen & JD.COM\n- arxiv: [https://arxiv.org/abs/1804.07094](https://arxiv.org/abs/1804.07094)\n- github: [https://github.com/yuminsuh/part_bilinear_reid](https://github.com/yuminsuh/part_bilinear_reid)\n\n**Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-identification**\n\n- intro: ECCV 2018. Huazhong University of Science and Technology & Horizon Robotics Inc.\n\n**Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.01571](https://arxiv.org/abs/1808.01571)\n\n**Deep Sequential Multi-camera Feature Fusion for Person Re-identification**\n\n[https://arxiv.org/abs/1807.07295](https://arxiv.org/abs/1807.07295)\n\n**Improving Deep Models of Person Re-identification for Cross-Dataset Usage**\n\n- intro: AIAI 2018 (14th International Conference on Artificial Intelligence Applications and Innovations) proceeding\n- arxiv: [https://arxiv.org/abs/1807.08526](https://arxiv.org/abs/1807.08526)\n\n**Measuring the Temporal Behavior of Real-World Person Re-Identification**\n\n[https://arxiv.org/abs/1808.05499](https://arxiv.org/abs/1808.05499)\n\n**Alignedreid＋+: Dynamically Matching Local Information for Person Re-Identification**\n\n[https://github.com/michuanhaohao/AlignedReID](https://github.com/michuanhaohao/AlignedReID)\n\n**Sparse Label Smoothing for Semi-supervised Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/1809.04976](https://arxiv.org/abs/1809.04976)\n- github: [https://github.com/jpainam/SLS_ReID](https://github.com/jpainam/SLS_ReID)\n\n**In Defense of the Classification Loss for Person Re-Identification**\n\n- intro: University of Science and Technology of China & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/1809.05864](https://arxiv.org/abs/1809.05864)\n\n**FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.02936](https://arxiv.org/abs/1810.02936)\n- github(Pytorch, official): [https://github.com/yxgeee/FD-GAN](https://github.com/yxgeee/FD-GAN)\n\n**Image-to-Video Person Re-Identification by Reusing Cross-modal Embeddings**\n\n[https://arxiv.org/abs/1810.03989](https://arxiv.org/abs/1810.03989)\n\n**Attention Driven Person Re-identification**\n\n- intro: Pattern Recognition (PR)\n- arxiv: [https://arxiv.org/abs/1810.05866](https://arxiv.org/abs/1810.05866)\n\n**A Coarse-to-fine Pyramidal Model for Person Re-identification via Multi-Loss Dynamic Training**\n\n- intro: YouTu Lab, Tencent\n- arxiv: [https://arxiv.org/abs/1810.12193](https://arxiv.org/abs/1810.12193)\n\n**M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for Person Re-Identification**\n\n[https://arxiv.org/abs/1811.03768](https://arxiv.org/abs/1811.03768)\n\n**Batch Feature Erasing for Person Re-identification and Beyond**\n\n- arxiv: [https://arxiv.org/abs/1811.07130](https://arxiv.org/abs/1811.07130)\n- github(official, Pytorch): [https://github.com/daizuozhuo/batch-feature-erasing-network](https://github.com/daizuozhuo/batch-feature-erasing-network)\n\n**Re-Identification with Consistent Attentive Siamese Networks**\n\n[https://arxiv.org/abs/1811.07487](https://arxiv.org/abs/1811.07487)\n\n**One Shot Domain Adaptation for Person Re-Identification**\n\n[https://arxiv.org/abs/1811.10144](https://arxiv.org/abs/1811.10144)\n\n**Parameter-Free Spatial Attention Network for Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/1811.12150](https://arxiv.org/abs/1811.12150)\n- github: [https://github.com/HRanWang/Spatial-Attention](https://github.com/HRanWang/Spatial-Attention)\n\n**Spectral Feature Transformation for Person Re-identification**\n\n- intro: University of Chinese Academy of Sciences & TuSimple\n- arxiv: [https://arxiv.org/abs/1811.11405](https://arxiv.org/abs/1811.11405)\n\n**Identity Preserving Generative Adversarial Network for Cross-Domain Person Re-identification**\n\n[https://arxiv.org/abs/1811.11510](https://arxiv.org/abs/1811.11510)\n\n**Dissecting Person Re-identification from the Viewpoint of Viewpoint**\n\n[https://arxiv.org/abs/1812.02162](https://arxiv.org/abs/1812.02162)\n\n**Fast and Accurate Person Re-Identification with RMNet**\n\n- intro: IOTG Computer Vision (ICV), Intel\n- arxiv: [https://arxiv.org/abs/1812.02465](https://arxiv.org/abs/1812.02465)\n\n**Spatial-Temporal Person Re-identification**\n\n- intro: AAAI 2019\n- intro: Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/1812.03282](https://arxiv.org/abs/1812.03282)\n- github: [https://github.com/Wanggcong/Spatial-Temporal-Re-identification](https://github.com/Wanggcong/Spatial-Temporal-Re-identification)\n\n**Omni-directional Feature Learning for Person Re-identification**\n\n- intro: Tongji University\n- keywords: OIM loss\n- arxiv: [https://arxiv.org/abs/1812.05319](https://arxiv.org/abs/1812.05319)\n\n**Learning Incremental Triplet Margin for Person Re-identification**\n\n- intro: AAAI 2019 spotlight\n- intro: Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1812.06576](https://arxiv.org/abs/1812.06576)\n\n**Densely Semantically Aligned Person Re-Identification**\n\n- intro: USTC & MSRA\n- arxiv: [https://arxiv.org/abs/1812.08967](https://arxiv.org/abs/1812.08967)\n\n**EANet: Enhancing Alignment for Cross-Domain Person Re-identification**\n\n- intro: CRISE & CASIA & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1812.11369](https://arxiv.org/abs/1812.11369)\n- github(official, Pytorch): [https://github.com/huanghoujing/EANet](https://github.com/huanghoujing/EANet)\n- blog: [https://zhuanlan.zhihu.com/p/53660395](https://zhuanlan.zhihu.com/p/53660395)\n\n**Backbone Can Not be Trained at Once: Rolling Back to Pre-trained Network for Person Re-Identification**\n\n- intro: AAAI 2019\n- intro: Seoul National University & Samsung SDS\n- arxiv: [https://arxiv.org/abs/1901.06140](https://arxiv.org/abs/1901.06140)\n\n**Ensemble Feature for Person Re-Identification**\n\n- keywords: EnsembleNet\n- arxiv: [https://arxiv.org/abs/1901.05798](https://arxiv.org/abs/1901.05798)\n\n**Adversarial Metric Attack for Person Re-identification**\n\n- intro: University of Oxford & Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/1901.10650](https://arxiv.org/abs/1901.10650)\n\n**Discovering Underlying Person Structure Pattern with Relative Local Distance for Person Re-identification**\n\n- intro: SYSU\n- arxiv: [https://arxiv.org/abs/1901.10100](https://arxiv.org/abs/1901.10100)\n- github: [https://github.com/Wanggcong/RLD_codes](https://github.com/Wanggcong/RLD_codes)\n\n**Attributes-aided Part Detection and Refinement for Person Re-identification**\n\n[https://arxiv.org/abs/1902.10528](https://arxiv.org/abs/1902.10528)\n\nB**ags of Tricks and A Strong Baseline for Deep Person Re-identification**\n\n- intro: CVPR 2019 Workshop\n- arxiv: [https://arxiv.org/abs/1903.07071](https://arxiv.org/abs/1903.07071)\n- paper: [http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf](http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf)\n- slides: [https://drive.google.com/file/d/1h9SgdJenvfoNp9PTUxPiz5_K5HFCho-V/view](https://drive.google.com/file/d/1h9SgdJenvfoNp9PTUxPiz5_K5HFCho-V/view)\n- github: [https://github.com/michuanhaohao/reid-strong-baseline](https://github.com/michuanhaohao/reid-strong-baseline)\n\n**Auto-ReID: Searching for a Part-aware ConvNet for Person Re-Identification**\n\n- keywords: NAS\n- arxiv: [https://arxiv.org/abs/1903.09776](https://arxiv.org/abs/1903.09776)\n- github(official): [https://github.com/D-X-Y/Auto-ReID-and-Others](https://github.com/D-X-Y/Auto-ReID-and-Others)\n\n**Perceive Where to Focus: Learning Visibility-aware Part-level Features for Partial Person Re-identification**\n\n- intro: CVPR 2019\n- intro: Tsinghua University & Megvii Technology\n- keywords: Visibility-aware Part Model (VPM)\n- arxiv: [https://arxiv.org/abs/1904.00537](https://arxiv.org/abs/1904.00537)\n\n**Pedestrian re-identification based on Tree branch network with local and global learning**\n\n- intro: ICME 2019 oral\n- arxiv: [https://arxiv.org/abs/1904.00355](https://arxiv.org/abs/1904.00355)\n\n**Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.01990](https://arxiv.org/abs/1904.01990)\n- github: [https://github.com/zhunzhong07/ECN](https://github.com/zhunzhong07/ECN)\n\n**Person Re-identification with Bias-controlled Adversarial Training**\n\n[https://arxiv.org/abs/1904.00244](https://arxiv.org/abs/1904.00244)\n\n**Relation-Aware Global Attention for Person Re-identification**\n\n- intro: CVPR 2020\n- intro: University of Science and Technology of China & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/1904.02998](https://arxiv.org/abs/1904.02998)\n- github: [https://github.com/microsoft/Relation-Aware-Global-Attention-Networks](https://github.com/microsoft/Relation-Aware-Global-Attention-Networks)\n\n**Person Re-identification with Metric Learning using Privileged Information**\n\n- intro: IEEE TIP\n- arxiv: [https://arxiv.org/abs/1904.05005](https://arxiv.org/abs/1904.05005)\n\n**Multi-Scale Body-Part Mask Guided Attention for Person Re-identification**\n\n- intro: Suning R&D Center USA\n- arxiv: [https://arxiv.org/abs/1904.11041](https://arxiv.org/abs/1904.11041)\n\n**Deep Constrained Dominant Sets for Person Re-identification**\n\n[https://arxiv.org/abs/1904.11397](https://arxiv.org/abs/1904.11397)\n\n**Illumination-Adaptive Person Re-identification**\n\n- intro: 1The University of Tokyo & National Taiwan University & National Institute of Informatics\n- arxiv: [https://arxiv.org/abs/1905.04525](https://arxiv.org/abs/1905.04525)\n\n**Domain Adaptive Person Re-Identification via Camera Style Generation and Label Propagation**\n\n- intro: Sun Yat-Sen University & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1905.05382](https://arxiv.org/abs/1905.05382)\n\n**Beyond Intra-modality Discrepancy: A Comprehensive Survey of Heterogeneous Person Re-identification**\n\n[https://arxiv.org/abs/1905.10048](https://arxiv.org/abs/1905.10048)\n\n**Deep Multi-Index Hashing for Person Re-Identification**\n\n- intro: Nanjing University\n- arxiv: [https://arxiv.org/abs/1905.10980](https://arxiv.org/abs/1905.10980)\n\n**Attention: A Big Surprise for Cross-Domain Person Re-Identification**\n\n[https://arxiv.org/abs/1905.12830](https://arxiv.org/abs/1905.12830)\n\n**Semantics-Aligned Representation Learning for Person Re-identification**\n\n- intro: USTC & MSRA\n- keywords: Semantics Aligning Network (SAN)\n- arxiv: [https://arxiv.org/abs/1905.13143](https://arxiv.org/abs/1905.13143)\n\n**Rethinking Person Re-Identification with Confidence**\n\n- intro: EPFL\n- arxiv: [https://arxiv.org/abs/1906.04692](https://arxiv.org/abs/1906.04692)\n\n**CDPM: Convolutional Deformable Part Models for Person Re-identification**\n\n[https://arxiv.org/abs/1906.04976](https://arxiv.org/abs/1906.04976)\n\n**Resolution-invariant Person Re-Identification**\n\n- intro: IJCAI 2019\n- intro: Peking University & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1906.09748](https://arxiv.org/abs/1906.09748)\n\n**Interaction-and-Aggregation Network for Person Re-identification**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1907.08435](https://arxiv.org/abs/1907.08435)\n\n**Distilled Person Re-identification: Towards a More Scalable System**\n\n- intro: CVPR 2019\n- intro: Sun Yat-sen University & YouTu Lab, Tencent\n- keywords: Multi-teacher Adaptive Similarity Distillation Framework, Log-Euclidean Similarity Distillation Loss, \nAdaptive Knowledge Aggregator\n- paper: [http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf)\n\n**Universal Person Re-Identification**\n\n- intro: Queen Mary University of London & Vision Semantics Ltd.\n- arxiv: [https://arxiv.org/abs/1907.09511](https://arxiv.org/abs/1907.09511)\n\n**ABD-Net: Attentive but Diverse Person Re-Identification**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.01114](https://arxiv.org/abs/1908.01114)\n- github: [https://github.com/TAMU-VITA/ABD-Net](https://github.com/TAMU-VITA/ABD-Net)\n\n**Fairest of Them All: Establishing a Strong Baseline for Cross-Domain Person ReID**\n\n- intro: University of Waterloo & SPORTLOGiQ Inc.\n- arxiv: [https://arxiv.org/abs/1907.12016](https://arxiv.org/abs/1907.12016)\n\n**Progressive Cross-camera Soft-label Learning for Semi-supervised Person Re-identification**\n\n[https://arxiv.org/abs/1908.05669](https://arxiv.org/abs/1908.05669)\n\n**Mixed High-Order Attention Network for Person Re-Identification**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.05819](https://arxiv.org/abs/1908.05819)\n- github: [https://github.com/chenbinghui1/MHN](https://github.com/chenbinghui1/MHN)\n\n**Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.06052](https://arxiv.org/abs/1908.06052)\n\n**HorNet: A Hierarchical Offshoot Recurrent Network for Improving Person Re-ID via Image Captioning**\n\n- intro: IJCAI 2019\n- intro: Nanjing Institute of Advanced Artificial Intelligence & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1908.04915](https://arxiv.org/abs/1908.04915)\n\n**Temporal Knowledge Propagation for Image-to-Video Person Re-identification**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.03885](https://arxiv.org/abs/1908.03885)\n\n**SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.09086](https://arxiv.org/abs/1908.09086)\n\n**Learning Deep Representations by Mutual Information for Person Re-identification**\n\n[https://arxiv.org/abs/1908.05860](https://arxiv.org/abs/1908.05860)\n\n**Orthogonal Center Learning with Subspace Masking for Person Re-Identification**\n\n- intro: Youtu X-lab, Tencent\n- arxiv: [https://arxiv.org/abs/1908.10535](https://arxiv.org/abs/1908.10535)\n\n**Adaptive Graph Representation Learning for Video Person Re-identification**\n\n[https://arxiv.org/abs/1909.02240](https://arxiv.org/abs/1909.02240)\n\n**POD: Practical Object Detection with Scale-Sensitive Network**\n\n[https://arxiv.org/abs/1909.02225](https://arxiv.org/abs/1909.02225)\n\n**Second-order Non-local Attention Networks for Person Re-identification**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.00295](https://arxiv.org/abs/1909.00295)\n\n**Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.09675](https://arxiv.org/abs/1909.09675)\n\n**View Confusion Feature Learning for Person Re-identification**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.03849](https://arxiv.org/abs/1910.03849)\n\n**Augmented Hard Example Mining for Generalizable Person Re-Identification**\n\n[https://arxiv.org/abs/1910.05280](https://arxiv.org/abs/1910.05280)\n\n**Learning Generalisable Omni-Scale Representations for Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/1910.06827](https://arxiv.org/abs/1910.06827)\n- github: [https://github.com/KaiyangZhou/deep-person-reid](https://github.com/KaiyangZhou/deep-person-reid)\n\n**Attention Network Robustification for Person ReID**\n\n- intro: DAMO Academy, Alibaba Group\n- arxiv: [https://arxiv.org/abs/1910.07038](https://arxiv.org/abs/1910.07038)\n\n**Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification**\n\n- intro: ICCV 2019\n- keywords: dual part-aligned block (DPB)\n- arxiv: [https://arxiv.org/abs/1910.10111](https://arxiv.org/abs/1910.10111)\n- github: [https://github.com/ggjy/P2Net.pytorch](https://github.com/ggjy/P2Net.pytorch)\n\n**An End-to-End Foreground-Aware Network for Person Re-Identification**\n\n[https://arxiv.org/abs/1910.11547](https://arxiv.org/abs/1910.11547)\n\n**Learning Disentangled Representation for Robust Person Re-identification**\n\n- intro: NeurIPS 2019\n- keywords: identity shuffle GAN (IS-GAN)\n- project page: [https://cvlab.yonsei.ac.kr/projects/ISGAN/](https://cvlab.yonsei.ac.kr/projects/ISGAN/)\n- arxiv: [https://arxiv.org/abs/1910.12003](https://arxiv.org/abs/1910.12003)\n\n**ID-aware Quality for Set-based Person Re-identification**\n\n[https://arxiv.org/abs/1911.09143](https://arxiv.org/abs/1911.09143)\n\n**Calibrated Domain-Invariant Learning for Highly Generalizable Large Scale Re-Identification**\n\n- intro: WACV 2020\n- arxiv: [https://arxiv.org/abs/1911.11314](https://arxiv.org/abs/1911.11314)\n\n**Collaborative Attention Network for Person Re-identification**\n\n[https://arxiv.org/abs/1911.13008](https://arxiv.org/abs/1911.13008)\n\n**Viewpoint-Aware Loss with Angular Regularization for Person Re-Identification**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1912.01300](https://arxiv.org/abs/1912.01300)\n- github: [https://github.com/zzhsysu/VA-ReID](https://github.com/zzhsysu/VA-ReID)\n\n**AANet: Attribute Attention Network for Person Re-Identifications**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1912.09021](https://arxiv.org/abs/1912.09021)\n\n**In Defense of the Triplet Loss Again: Learning Robust Person Re-Identification with Fast Approximated Triplet Loss and Label Distillation**\n\n- arxiv: [https://arxiv.org/abs/1912.07863](https://arxiv.org/abs/1912.07863)\n- github: [https://github.com/TAMU-VITA/FAT](https://github.com/TAMU-VITA/FAT)\n\n**Rethinking the Distribution Gap of Person Re-identification with Camera-based Batch Normalization**\n\n- intro: ECCV 2020\n- intro: Tsinghua University & Huawei Inc. & Hefei University of Technology & University of Science and Technology of China\n- arxiv: [https://arxiv.org/abs/2001.08680](https://arxiv.org/abs/2001.08680)\n- github: [https://github.com/automan000/Camera-based-Person-ReID](https://github.com/automan000/Camera-based-Person-ReID)\n\n**Intra-Camera Supervised Person Re-Identification**\n\n[https://arxiv.org/abs/2002.05046](https://arxiv.org/abs/2002.05046)\n\n**Towards Precise Intra-camera Supervised Person Re-identification**\n\n- intro: Zhejiang University & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2002.04932](https://arxiv.org/abs/2002.04932)\n\n**Diversity-Achieving Slow-DropBlock Network for Person Re-Identification**\n\n[https://arxiv.org/abs/2002.04414](https://arxiv.org/abs/2002.04414)\n\n**MagnifierNet: Towards Semantic Regularization and Fusion for Person Re-identification**\n\n[https://arxiv.org/abs/2002.10979](https://arxiv.org/abs/2002.10979)\n\n**Triplet Online Instance Matching Loss for Person Re-identification**\n\n[https://arxiv.org/abs/2002.10560](https://arxiv.org/abs/2002.10560)\n\n**When Person Re-identification Meets Changing Clothes**\n\n[https://arxiv.org/abs/2003.04070](https://arxiv.org/abs/2003.04070)\n\n**Multi-task Learning with Coarse Priors for Robust Part-aware Person Re-identification**\n\n- intro: South China University of Technolog & University of Sydney\n- keywords: Multi-task Part-aware Network (MPN)\n- arxiv: [https://arxiv.org/abs/2003.08069](https://arxiv.org/abs/2003.08069)\n\n**Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking**\n\n- intro: CVPR 2020\n- intro: To attack ReID, we propose a learning-to-mis-rank formulation to perturb the ranking of the system output\n- arxiv: [https://arxiv.org/abs/2004.04199](https://arxiv.org/abs/2004.04199)\n- github: [https://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking](https://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking)\n\n**Real-world Person Re-Identification via Degradation Invariance Learning**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2004.04933](https://arxiv.org/abs/2004.04933)\n\n**Person Re-identification in the 3D Space**\n\n- intro: University of Technology Sydney\n- arxiv: [https://arxiv.org/abs/2006.04569](https://arxiv.org/abs/2006.04569)\n- github(official): [https://github.com/layumi/person-reid-3d](https://github.com/layumi/person-reid-3d)\n\n**Rethinking Classification Loss Designs for Person Re-identification with a Unified View**\n\n- intro: University of Science and Technology of China & MicroSoft Research Asia & Columbia University\n- arxiv: [https://arxiv.org/abs/2006.04991](https://arxiv.org/abs/2006.04991)\n\n**Multiple Expert Brainstorming for Domain Adaptive Person Re-identification**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.01546](https://arxiv.org/abs/2007.01546)\n- github: [https://github.com/YunpengZhai/MEB-Net](https://github.com/YunpengZhai/MEB-Net)\n\n**ESA-ReID: Entropy-Based Semantic Feature Alignment for Person re-ID**\n\n- intro: Alibaba Group\n- arxiv: [https://arxiv.org/abs/2007.04644](https://arxiv.org/abs/2007.04644)\n\n**Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification**\n\n- intro: ECCV 2020 oral\n- intro: Carnegie Mellon University & NVIDIA\n- arxiv: [https://arxiv.org/abs/2007.10315](https://arxiv.org/abs/2007.10315)\n\n**Dual Distribution Alignment Network for Generalizable Person Re-Identification**\n\n[https://arxiv.org/abs/2007.13249](https://arxiv.org/abs/2007.13249)\n\n**Hierarchical Bi-Directional Feature Perception Network for Person Re-Identification**\n\n- intro: ACM MM 2020\n- arxiv: [https://arxiv.org/abs/2008.03509](https://arxiv.org/abs/2008.03509)\n\n**Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/2008.08528](https://arxiv.org/abs/2008.08528)\n- github: [https://github.com/xbq1994/HAA](https://github.com/xbq1994/HAA)\n\n**Faster Person Re-Identification**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.06826](https://arxiv.org/abs/2008.06826)\n- github: [https://github.com/wangguanan/light-reid](https://github.com/wangguanan/light-reid)\n\n**Do Not Disturb Me: Person Re-identification Under the Interference of Other Pedestrians**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.06963](https://arxiv.org/abs/2008.06963)\n- github(official): [https://github.com/X-BrainLab/PI-ReID](https://github.com/X-BrainLab/PI-ReID)\n\n**Apparel-invariant Feature Learning for Apparel-changed Person Re-identification**\n\n[https://arxiv.org/abs/2008.06181](https://arxiv.org/abs/2008.06181)\n\n**Cluster-level Feature Alignment for Person Re-identification**\n\n[https://arxiv.org/abs/2008.06810](https://arxiv.org/abs/2008.06810)\n\n**Self-Supervised Gait Encoding with Locality-Aware Attention for Person Re-Identification**\n\n- intro: IJCAI 2020\n- arxiv: [https://arxiv.org/abs/2008.09435](https://arxiv.org/abs/2008.09435)\n- github: [https://github.com/Kali-Hac/SGE-LA](https://github.com/Kali-Hac/SGE-LA)\n\n**Receptive Multi-granularity Representation for Person Re-Identification**\n\n- intro: Championship solution of NAIC 2019 Person re-ID Track\n- arxiv: [https://arxiv.org/abs/2008.13450](https://arxiv.org/abs/2008.13450)\n\n**Proxy Task Learning For Cross-Domain Person Re-Identification**\n\n- intro: ICME 2020\n- paper: [https://scihub.wikicn.top/10.1109/ICME46284.2020.9102898](https://scihub.wikicn.top/10.1109/ICME46284.2020.9102898)\n- paper: [https://ieeexplore.ieee.org/document/9102898](https://ieeexplore.ieee.org/document/9102898)\n- github: [https://github.com/huanghoujing/PTL](https://github.com/huanghoujing/PTL)\n\n**Progressive Bilateral-Context Driven Model for Post-Processing Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/2009.03098](https://arxiv.org/abs/2009.03098)\n- github: [https://github.com/123ci/PBCmodel](https://github.com/123ci/PBCmodel)\n\n**Devil's in the Detail: Graph-based Key-point Alignment and Embedding for Person Re-ID**\n\n- intro: Tencent Youtu Lab &  Sun Yat-sen University & Huazhong University of Science and Technology\n- arxiv: [https://arxiv.org/abs/2009.05250](https://arxiv.org/abs/2009.05250)\n\n**Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/2009.07536](https://arxiv.org/abs/2009.07536)\n- github: [https://github.com/libraflower/MutipleFeature-for-PRID](https://github.com/libraflower/MutipleFeature-for-PRID)\n\n**Beyond Triplet Loss: Person Re-identification with Fine-grained Difference-aware Pairwise Loss**\n\n[https://arxiv.org/abs/2009.10295](https://arxiv.org/abs/2009.10295)\n\n**Batch Coherence-Driven Network for Part-aware Person Re-Identification**\n\n[https://arxiv.org/abs/2009.09692](https://arxiv.org/abs/2009.09692)\n\n**FTN: Foreground-Guided Texture-Focused Person Re-Identification**\n\n[https://arxiv.org/abs/2009.11425](https://arxiv.org/abs/2009.11425)\n\n**Improve Person Re-Identification With Part Awareness Learning**\n\n- intro: TIP 2020\n- intro: Institute of Automation, Chinese Academy of Sciences & Horizon Robotics, Inc\n- paper: [https://ieeexplore.ieee.org/document/9124699](https://ieeexplore.ieee.org/document/9124699)\n- github: [https://github.com/huanghoujing/PAL-MGN](https://github.com/huanghoujing/PAL-MGN)\n\n**Performance Optimization for Federated Person Re-identification via Benchmark Analysis**\n\n- intro: ACMMM 2020\n- arxiv: [https://arxiv.org/abs/2008.11560](https://arxiv.org/abs/2008.11560)\n\n**DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations**\n\n[https://arxiv.org/abs/2011.11953](https://arxiv.org/abs/2011.11953)\n\n**Multi-Domain Adversarial Feature Generalization for Person Re-Identification**\n\n[https://arxiv.org/abs/2011.12563](https://arxiv.org/abs/2011.12563)\n\n**Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification**\n\n[https://arxiv.org/abs/2012.00417](https://arxiv.org/abs/2012.00417)\n\n**Unsupervised Pre-training for Person Re-identification**\n\n- intro: University of Science and Technology of China & Microsoft Research\n- intro: LUPerson dataset\n- arxiv: [https://arxiv.org/abs/2012.03753](https://arxiv.org/abs/2012.03753)\n\n**UnrealPerson: An Adaptive Pipeline towards Costless Person Re-identification**\n\n- arxov: [https://arxiv.org/abs/2012.04268](https://arxiv.org/abs/2012.04268)\n- github: [https://github.com/FlyHighest/UnrealPerson](https://github.com/FlyHighest/UnrealPerson)\n\n**One for More: Selecting Generalizable Samples for Generalizable ReID Model**\n\n- intro: Tencent Youtu Lab & Sun Yat-sen University & Southern University of Science and Technology\n- arxiv: [https://arxiv.org/abs/2012.05475](https://arxiv.org/abs/2012.05475)\n\n**Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification**\n\n- intro: AAAI 2021\n- arxiv: [https://arxiv.org/abs/2012.08733](https://arxiv.org/abs/2012.08733)\n\n**Camera-aware Proxies for Unsupervised Person Re-Identification**\n\n- intro: AAAI 2021\n- intro: 1Zhejiang University & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2012.10674](https://arxiv.org/abs/2012.10674)\n\n**1st Place Solution to VisDA-2020: Bias Elimination for Domain Adaptive Pedestrian Re-identification**\n\n- intro: 1st place solution to VisDA-2020 Challenge (ECCVW 2020)\n- intro: Zhejiang University & Alibaba Group\n- project page: [http://ai.bu.edu/visda-2020/](http://ai.bu.edu/visda-2020/)\n- arxiv: [https://arxiv.org/abs/2012.13498](https://arxiv.org/abs/2012.13498)\n- github: [https://github.com/vimar-gu/Bias-Eliminate-DA-ReID](https://github.com/vimar-gu/Bias-Eliminate-DA-ReID)\n\n**HAVANA: Hierarchical and Variation-Normalized Autoencoder for Person Re-identification**\n\n[https://arxiv.org/abs/2101.02568](https://arxiv.org/abs/2101.02568)\n\n**AXM-Net: Cross-Modal Context Sharing Attention Network for Person Re-ID**\n\n[https://arxiv.org/abs/2101.08238](https://arxiv.org/abs/2101.08238)\n\n**TransReID: Transformer-based Object Re-Identification**\n\n- intro: Alibaba Group & Zhejiang University\n- arxiv: [https://arxiv.org/abs/2102.04378](https://arxiv.org/abs/2102.04378)\n\n**Deep Miner: A Deep and Multi-branch Network which Mines Rich and Diverse Features for Person Re-identification**\n\n- intro: Digeiz AI Lab & Ecole Polytechnique\n- arxiv: [https://arxiv.org/abs/2102.09321](https://arxiv.org/abs/2102.09321)\n\n**AttriMeter: An Attribute-guided Metric Interpreter for Person Re-Identification**\n\n- intro: University of Science and Technology of China & JD AI Research & Ryerson University\n- arxiv: [https://arxiv.org/abs/2103.01451](https://arxiv.org/abs/2103.01451)\n\n**Watching You: Global-guided Reciprocal Learning for Video-based Person Re-identification**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.04337](https://arxiv.org/abs/2103.04337)\n\n**Lifelong Person Re-Identification via Adaptive Knowledge Accumulation**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.12462](https://arxiv.org/abs/2103.12462)\n\n**Spatiotemporal Transformer for Video-based Person Re-identification**\n\n- intro: Beihang University & Pengcheng Laboratory & Tsinghua University & University of Science and Technology of China & Xidian University\n- arxiv: [https://arxiv.org/abs/2103.16469](https://arxiv.org/abs/2103.16469)\n\n**AAformer: Auto-Aligned Transformer for Person Re-Identification**\n\n- intro: Chinese Academy of Sciences & Peng Cheng Laboratory & Peking University & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2104.00921](https://arxiv.org/abs/2104.00921)\n\n**Combined Depth Space based Architecture Search For Person Re-identification**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.04163](https://arxiv.org/abs/2104.04163)\n\n**Graph-based Person Signature for Person Re-Identifications**\n\n- intro: CVPR 2021 Workshops\n- arxiv: [https://arxiv.org/abs/2104.06770](https://arxiv.org/abs/2104.06770)\n\n**Unsupervised Multi-Source Domain Adaptation for Person Re-Identification**\n\n- intro: CVPR 2021 oral\n- arxiv: [https://arxiv.org/abs/2104.12961](https://arxiv.org/abs/2104.12961)\n\n**Generalizable Person Re-identification with Relevance-aware Mixture of Experts**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2105.09156](https://arxiv.org/abs/2105.09156)\n\n**Transformer-Based Deep Image Matching for Generalizable Person Re-identification**\n\n[https://arxiv.org/abs/2105.14432](https://arxiv.org/abs/2105.14432)\n\n**Person Re-Identification with a Locally Aware Transformer**\n\n- intro: University of Maryland Baltimore County\n- arxiv: [https://arxiv.org/abs/2106.03720](https://arxiv.org/abs/2106.03720)\n\n**Learning Instance-level Spatial-Temporal Patterns for Person Re-identification**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.00171](https://arxiv.org/abs/2108.00171)\n- github: [https://github.com/RenMin1991/cleaned-DukeMTMC-reID/](https://github.com/RenMin1991/cleaned-DukeMTMC-reID/)\n\n**IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID**\n\n- intro: ICCV 2021 oral\n- keywords: Peking University & Singapore University of Technology and Design & Megvii Technology & National University of Singapore & Peng Cheng Laboratory\n- arxiv: [https://arxiv.org/abs/2108.02413](https://arxiv.org/abs/2108.02413)\n- github: [https://github.com/SikaStar/IDM](https://github.com/SikaStar/IDM)\n\n**Benchmarks for Corruption Invariant Person Re-identification**\n\n- intro: NeurIPS 2021 Track on Datasets and Benchmarks\n- arxiv: [https://arxiv.org/abs/2111.00880](https://arxiv.org/abs/2111.00880)\n- github: [https://github.com/MinghuiChen43/CIL-ReID](https://github.com/MinghuiChen43/CIL-ReID)\n\n**Self-Supervised Pre-Training for Transformer-Based Person Re-Identification**\n\n- intro: Alibaba Group\n- arxiv: [https://arxiv.org/abs/2111.12084](https://arxiv.org/abs/2111.12084)\n- github: [https://github.com/michuanhaohao/TransReID-SSL](https://github.com/michuanhaohao/TransReID-SSL)\n\n**Stronger Baseline for Person Re-Identification**\n\n- intro: The third-place solution for ICCV2021 VIPriors Re-identification Challenge\n- arxiv: [https://arxiv.org/abs/2112.01059](https://arxiv.org/abs/2112.01059)\n\n**Dual Cluster Contrastive learning for Person Re-Identification**\n\n- intro: Institute of Automation, CAS\n- arxiv: [https://arxiv.org/abs/2112.04662](https://arxiv.org/abs/2112.04662)\n- github: [https://github.com/htyao89/Dual-Cluster-Contrastive/](https://github.com/htyao89/Dual-Cluster-Contrastive/)\n\n**META: Mimicking Embedding via oThers' Aggregation for Generalizable Person Re-identification**\n\n- intro: CASIA & UCAS & JD AI Reasearch\n- arxiv: [https://arxiv.org/abs/2112.08684](https://arxiv.org/abs/2112.08684)\n\n**Multi-Centroid Representation Network for Domain Adaptive Person Re-ID**\n\n- intro: AAAI 2022\n- arxiv: [https://arxiv.org/abs/2112.11689](https://arxiv.org/abs/2112.11689)\n\n**Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.15210](https://arxiv.org/abs/2203.15210)\n\n**Large-Scale Pre-training for Person Re-identification with Noisy Labels**\n\n- intro: CVPR 2022\n- intro: University of Science and Technology of China & Microsoft Research & Microsoft Cloud AI & IDEA\n- arxiv: [https://arxiv.org/abs/2203.16533](https://arxiv.org/abs/2203.16533)\n- github: [https://github.com/DengpanFu/LUPerson-NL](https://github.com/DengpanFu/LUPerson-NL)\n\n**A Free Lunch to Person Re-identification: Learning from Automatically Generated Noisy Tracklets**\n\n- intro: Tsinghua University & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2204.00891](https://arxiv.org/abs/2204.00891)\n\n**Clothes-Changing Person Re-identification with RGB Modality Only**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.06890](https://arxiv.org/abs/2204.06890)\n- github: [https://github.com/guxinqian/Simple-CCReID](https://github.com/guxinqian/Simple-CCReID)\n\n**NFormer: Robust Person Re-identification with Neighbor Transformer**\n\n- intro: CVPR 2022 poster\n- arxiv: [https://arxiv.org/abs/2204.09331](https://arxiv.org/abs/2204.09331)\n- github: [https://github.com/haochenheheda/NFormer](https://github.com/haochenheheda/NFormer)\n\n**Feature-Distribution Perturbation and Calibration for Generalized Person ReID**\n\n- iintro: Queen Mary University of London\n- arxiv: [https://arxiv.org/abs/2205.11197](https://arxiv.org/abs/2205.11197)\n\n# Person Search\n\n**End-to-End Deep Learning for Person Search**\n\n- arxiv: [https://arxiv.org/abs/1604.01850v1](https://arxiv.org/abs/1604.01850v2)\n- paper: [http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf](http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf)\n\n**Joint Detection and Identification Feature Learning for Person Search**\n\n- intro: CVPR 2017 Spotlight\n- intro: The Chinese University of Hong Kong & Sun Yat-Sen University & SenseTime Group Limited\n- keywords: Online Instance Matching (OIM) loss function\n- homepage(dataset+code):[http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)\n- arxiv: [https://arxiv.org/abs/1604.01850](https://arxiv.org/abs/1604.01850)\n- paper: [http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf](http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf)\n- github(official. Caffe): [https://github.com/ShuangLI59/person_search](https://github.com/ShuangLI59/person_search)\n\n**Person Re-identification in the Wild**\n\n- intro: CVPR 2017 spotlight\n- intro: University of Technology Sydney & UTSA & USTC & UCSD\n- keywords: PRW dataset\n- project page: [http://www.liangzheng.com.cn/Project/project_prw.html](http://www.liangzheng.com.cn/Project/project_prw.html)\n- arxiv: [https://arxiv.org/abs/1604.02531](https://arxiv.org/abs/1604.02531)\n- github: [https://github.com/liangzheng06/PRW-baseline](https://github.com/liangzheng06/PRW-baseline)\n- youtube: [https://www.youtube.com/watch?v=dbOGwBITJqo](https://www.youtube.com/watch?v=dbOGwBITJqo)\n\n**IAN: The Individual Aggregation Network for Person Search**\n\n- intro: Xi’an Jiaotong-Liverpool University & National University of Singapore\n- arxiv: [https://arxiv.org/abs/1705.05552](https://arxiv.org/abs/1705.05552)\n\n**Neural Person Search Machines**\n\n- intro: ICCV 2017\n- intro: Hefei University of Technology & National University of Singapore & Tencent AI Lab & Panasonic R&D Center Singapore & Southwest Jiaotong University & 360 AI Institute\n- arxiv: [https://arxiv.org/abs/1707.06777](https://arxiv.org/abs/1707.06777)\n- paper: [https://ai.tencent.com/ailab/media/publications/Neural_Person_Search_Machines.pdf](https://ai.tencent.com/ailab/media/publications/Neural_Person_Search_Machines.pdf)\n\n**Efficient Person Search via Expert-Guided Knowledge Distillation**\n\n- paper: [https://ieeexplore.ieee.org/document/8759990](https://ieeexplore.ieee.org/document/8759990)\n\n**End-to-End Detection and Re-identification Integrated Net for Person Search**\n\n- intro: Chongqing University & Hefei University of Technology\n- keywords: I-Net\n- arxiv: [https://arxiv.org/abs/1804.00376](https://arxiv.org/abs/1804.00376)\n\n**Person Search via A Mask-guided Two-stream CNN Model**\n\n- intro: ECCV 2018\n- intro: Nanjing University of Science and Technology & Youtu Lab, Tencent\n- arxiv: [https://arxiv.org/abs/1807.08107](https://arxiv.org/abs/1807.08107)\n\n**Person Search by Multi-Scale Matching**\n\n- intro: ECCV 2018\n- intro: Queen Mary University of London & Vision Semantics Ltd\n- keywords: Cross-Level Semantic Alignment (CLSA)\n- arxiv: [https://arxiv.org/abs/1807.08582](https://arxiv.org/abs/1807.08582)\n\n**RCAA: Relational Context-Aware Agents for Person Search**\n\n- intro: ECCV 2018\n- intro: Carnegie Mellon University & Chinese Academy of Sciences & University of Technology Sydney\n- paper: [http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaojun_Chang_RCAA_Relational_Context-Aware_ECCV_2018_paper.pdf](http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiaojun_Chang_RCAA_Relational_Context-Aware_ECCV_2018_paper.pdf)\n\n**Fast Person Search Pipeline**\n\n- intro: ICME 2019\n- intro: Sun Yat-sen University\n- paper: [https://ieeexplore.ieee.org/abstract/document/8784982/](https://ieeexplore.ieee.org/abstract/document/8784982/)\n- paper: [https://sci-hub.tw/10.1109/ICME.2019.00195](https://sci-hub.tw/10.1109/ICME.2019.00195)\n\n**Learning Context Graph for Person Search**\n\n- intro: CVPR 2019 Oral\n- intro: Shanghai Jiao Tong University & Tencent YouTu Lab &  Inception Institute of Artificial Intelligence, UAE\n- arxiv: [https://arxiv.org/abs/1904.01830](https://arxiv.org/abs/1904.01830)\n- github(official, Pytorch): [https://github.com/sjtuzq/person_search_gcn](https://github.com/sjtuzq/person_search_gcn)\n\n**Query-guided End-to-End Person Search**\n\n- intro: CVPR 2019 poster\n- intro: OSRAM GmbH & Technische Universität München\n- keywords: query-guided end-to-end person search network (QEEPS)\n- arxiv: [https://arxiv.org/abs/1905.01203](https://arxiv.org/abs/1905.01203)\n- github(official): [https://github.com/munjalbharti/Query-guided-End-to-End-Person-Search](https://github.com/munjalbharti/Query-guided-End-to-End-Person-Search)\n\n**Knowledge Distillation for End-to-End Person Search**\n\n- intro: BMVC 2019\n- intro: Technische Universität München & OSRAM GmbH\n- arxiv: [https://arxiv.org/abs/1909.01058](https://arxiv.org/abs/1909.01058)\n\n**Re-ID Driven Localization Refinement for Person Search**\n\n- intro: ICCV 2019\n- intro: Huazhong University of Science and Technology & Peking University & Shanghai Jiao Tong University & Megvii Technology\n- arxiv: [https://arxiv.org/abs/1909.08580](https://arxiv.org/abs/1909.08580)\n\n**End-to-End Thorough Body Perception for Person Search**\n\n- intro: AAAI 2020\n- intro: Horizon Robotics & Chinese Academy of Sciences\n- paper: [https://aaai.org/Papers/AAAI/2020GB/AAAI-TianK.2778.pdf](https://aaai.org/Papers/AAAI/2020GB/AAAI-TianK.2778.pdf)\n\n**Hierarchical Online Instance Matching for Person Search**\n\n- intro: AAAI 2020\n- paper: [https://aaai.org/Papers/AAAI/2020GB/AAAI-ChenD.1557.pdf](https://aaai.org/Papers/AAAI/2020GB/AAAI-ChenD.1557.pdf)\n- github: [https://github.com/DeanChan/HOIM-PyTorch](https://github.com/DeanChan/HOIM-PyTorch)\n\n**Robust Partial Matching for Person Search in the Wild**\n\n- intro: CVPR 2020\n- intro: Peking University & Intellifusion & CUHK\n- keywords: Align-to-Part Network (APNet)\n- arxiv: [https://arxiv.org/abs/2004.09329](https://arxiv.org/abs/2004.09329)\n\n**Joint Person Objectness and Repulsion for Person Search**\n\n- intro:  Chinese Academy of Sciences\n- aarxiv: [https://arxiv.org/abs/2006.00155](https://arxiv.org/abs/2006.00155)\n\n**Norm-Aware Embedding for Efﬁcient Person Search**\n\n- intro: CVPR 2020\n- paper: [http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Norm-Aware_Embedding_for_Efficient_Person_Search_CVPR_2020_paper.pdf](http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Norm-Aware_Embedding_for_Efficient_Person_Search_CVPR_2020_paper.pdf)\n- arxiv: [https://github.com/DeanChan/NAE4PS](https://github.com/DeanChan/NAE4PS)\n\n**A Multi-task Joint Framework for Real-time Person Search**\n\n[https://arxiv.org/abs/2012.06418](https://arxiv.org/abs/2012.06418)\n\n**Diverse Knowledge Distillation for End-to-End Person Search**\n\n- intro: AAAI 2021\n- intro: Tongji University & The University of Adelaide & Monash University\n- arxiv: [https://arxiv.org/abs/2012.11187](https://arxiv.org/abs/2012.11187)\n- github: [https://github.com/zhangxinyu-xyz/DKD-PersonSearch](https://github.com/zhangxinyu-xyz/DKD-PersonSearch)\n\n**Multi-Attribute Enhancement Network for Person Search**\n\n- arxiv: [https://arxiv.org/abs/2102.07968](https://arxiv.org/abs/2102.07968)\n- gihtub: [https://github.com/chenlq123/MAE](https://github.com/chenlq123/MAE)\n\n**Sequential End-to-end Network for Efficient Person Search**\n\n- intro: Tongji University\n- arxiv: [https://arxiv.org/abs/2103.10148](https://arxiv.org/abs/2103.10148)\n\n**Anchor-Free Person Search**\n\n- intro: CVPR 2021\n- intro: Inception Institute of Artificial Intelligence (IIAI) & University of Oxford\n- arxiv: [https://arxiv.org/abs/2103.11617](https://arxiv.org/abs/2103.11617)\n- github: [https://github.com/daodaofr/AlignPS](https://github.com/daodaofr/AlignPS)\n\n**Exploring Visual Context for Weakly Supervised Person Search**\n\n- arxiv: [https://arxiv.org/abs/2106.10506](https://arxiv.org/abs/2106.10506)\n- github: [https://github.com/ljpadam/CGPS](https://github.com/ljpadam/CGPS)\n\n**Making Person Search Enjoy the Merits of Person Re-identification**\n\n- intro: Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/2108.10536](https://arxiv.org/abs/2108.10536)\n\n**Context-Aware Unsupervised Clustering for Person Search**\n\n[https://arxiv.org/abs/2110.01341](https://arxiv.org/abs/2110.01341)\n\n**Learning Context-Aware Embedding for Person Search**\n\n- intro: Beihang University & MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2111.14316](https://arxiv.org/abs/2111.14316)\n\n**Subtask-dominated Transfer Learning for Long-tail Person Search**\n\n- intro: Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/2112.00527](https://arxiv.org/abs/2112.00527)\n\n**Global-Local Context Network for Person Search**\n\n- arxiv: [https://arxiv.org/abs/2112.02500](https://arxiv.org/abs/2112.02500)\n- github: [https://github.com/ZhengPeng7/GLCNet](https://github.com/ZhengPeng7/GLCNet)\n\n**Cascade Transformers for End-to-End Person Search**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.09642](https://arxiv.org/abs/2203.09642)\n- github: [https://github.com/Kitware/COAT](https://github.com/Kitware/COAT)\n\n**CGUA: Context-Guided and Unpaired-Assisted Weakly Supervised Person Search**\n\n[https://arxiv.org/abs/2203.14307](https://arxiv.org/abs/2203.14307)\n\n**PSTR: End-to-End One-Step Person Search With Transformers**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.03340](https://arxiv.org/abs/2204.03340)\n- github: [https://github.com/JialeCao001/PSTR](https://github.com/JialeCao001/PSTR)\n\n**OIMNet++: Prototypical Normalization and Localization-aware Learning for Person Search**\n\n- intro: ECCV 2022\n- intro: Yonsei University & Korea Institute of Science and Technology\n- project page: [https://cvlab.yonsei.ac.kr/projects/OIMNetPlus/](https://cvlab.yonsei.ac.kr/projects/OIMNetPlus/)\n- arxiv: [https://arxiv.org/abs/2207.10320](https://arxiv.org/abs/2207.10320)\n- github: [https://github.com/cvlab-yonsei/OIMNetPlus](https://github.com/cvlab-yonsei/OIMNetPlus)\n\n**Domain Adaptive Person Search**\n\n- intro: ECCV 2022 Oral\n- intro: Shanghai Jiao Tong University & Tencent Youtu Lab\n- arxiv: [https://arxiv.org/abs/2207.11898](https://arxiv.org/abs/2207.11898)\n- github: [https://github.com/caposerenity/DAPS](https://github.com/caposerenity/DAPS)\n\n# Pose / Viewpoint for Re-ID\n\n**Pose Invariant Embedding for Deep Person Re-identification**\n\n- keywords: pose invariant embedding (PIE), PoseBox fusion (PBF) CNN\n- arixv: [https://arxiv.org/abs/1701.07732](https://arxiv.org/abs/1701.07732)\n\n**Deeply-Learned Part-Aligned Representations for Person Re-Identification**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.07256](https://arxiv.org/abs/1707.07256)\n- github(official, Caffe): [https://github.com/zlmzju/part_reid](https://github.com/zlmzju/part_reid)\n\n**Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion**\n\n- intro: CVPR 2017\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf)\n- github: [https://github.com/yokattame/SpindleNet](https://github.com/yokattame/SpindleNet)\n\n**Pose-driven Deep Convolutional Model for Person Re-identification**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.08325](https://arxiv.org/abs/1709.08325)\n\n**A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1711.10378](https://arxiv.org/abs/1711.10378)\n- github(official): [https://github.com/pse-ecn/pose-sensitive-embedding](https://github.com/pse-ecn/pose-sensitive-embedding)\n\n**Pose-Driven Deep Models for Person Re-Identification**\n\n- intro: Masters thesis\n- arxiv: [https://arxiv.org/abs/1803.08709](https://arxiv.org/abs/1803.08709)\n\n**Pose Transferrable Person Re-Identification**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf)\n\n**Person re-identification with fusion of hand-crafted and deep pose-based body region features**\n\n[https://arxiv.org/abs/1803.10630](https://arxiv.org/abs/1803.10630)\n\n# GAN for Re-ID\n\n**Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1701.07717](https://arxiv.org/abs/1701.07717)\n- github(official, Matlab): [https://github.com/layumi/Person-reID_GAN](https://github.com/layumi/Person-reID_GAN)\n- github: [https://github.com/qiaoguan/Person-reid-GAN-pytorch](https://github.com/qiaoguan/Person-reid-GAN-pytorch)\n\n**Person Transfer GAN to Bridge Domain Gap for Person Re-Identification**\n\n- intro: CVPR 2018 spotlight\n- intro: PTGAN\n- arxiv: [https://arxiv.org/abs/1711.08565](https://arxiv.org/abs/1711.08565)\n- github: [https://github.com/JoinWei-PKU/PTGAN](https://github.com/JoinWei-PKU/PTGAN)\n\n**Pose-Normalized Image Generation for Person Re-identification**\n\n- keywords: PN-GAN\n- arxiv: [https://arxiv.org/abs/1712.02225](https://arxiv.org/abs/1712.02225)\n- github: [https://github.com/naiq/PN_GAN](https://github.com/naiq/PN_GAN)\n\n**Multi-pseudo Regularized Label for Generated Samples in Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/1801.06742](https://arxiv.org/abs/1801.06742)\n- github: [https://github.com/Huang-3/MpRL-for-person-re-ID](https://github.com/Huang-3/MpRL-for-person-re-ID)\n\n**Joint Discriminative and Generative Learning for Person Re-identification**\n\n- intro: CVPR 2019 oral\n- intro: NVIDIA & University of Technology Sydney & Australian National University\n- arxiv: [https://arxiv.org/abs/1904.07223](https://arxiv.org/abs/1904.07223)\n- github: [https://github.com/NVlabs/DG-Net](https://github.com/NVlabs/DG-Net)\n- youtube: [https://www.youtube.com/watch?v=ubCrEAIpQs4](https://www.youtube.com/watch?v=ubCrEAIpQs4)\n- bilibili: [https://www.bilibili.com/video/av51439240](https://www.bilibili.com/video/av51439240)\n\n# Human Parsing for Re-ID\n\n**Human Semantic Parsing for Person Re-identification**\n\n- intro: CVPR 2018. SPReID\n- arxiv: [https://arxiv.org/abs/1804.00216](https://arxiv.org/abs/1804.00216)\n- github: [https://github.com/emrahbasaran/SPReID](https://github.com/emrahbasaran/SPReID)\n\n**Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models**\n\n- keywords: Saliency-Semantic Parsing Re-Identification (SSP-ReID)\n- arxiv: [https://arxiv.org/abs/1807.05618](https://arxiv.org/abs/1807.05618)\n\n**Identity-Guided Human Semantic Parsing for Person Re-Identification**\n\n- intro: ECCV 2020 spotlight\n- arxiv: [https://arxiv.org/abs/2007.13467](https://arxiv.org/abs/2007.13467)\n- github: [https://github.com/CASIA-IVA-Lab/ISP-reID](https://github.com/CASIA-IVA-Lab/ISP-reID)\n\n**Human Parsing Based Alignment with Multi-task Learning for Occluded Person Re-identification**\n\n- intro: ICME 2020 Oral\n- paper; [https://scihub.wikicn.top/10.1109/ICME46284.2020.9102789](https://scihub.wikicn.top/10.1109/ICME46284.2020.9102789)\n- paper: [https://ieeexplore.ieee.org/abstract/document/9102789](https://ieeexplore.ieee.org/abstract/document/9102789)\n- github: [https://github.com/huanghoujing/HPNet](https://github.com/huanghoujing/HPNet)\n\n# Occluded Person Re-ID / Partial Person Re-ID\n\n**Partial Person Re-identification**\n\n- intro: ICCV 2015\n- arxiv: [https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.pdf)\n\n**Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-Free Approach**\n\n- intro: CVPR 2018\n- intro: Market1501 rank1=83.58%\n- keywords: Deep Spatial feature Reconstruction (DSR)\n- arxiv: [https://arxiv.org/abs/1801.00881](https://arxiv.org/abs/1801.00881)\n- github(official, PyTorch): [https://github.com/lingxiao-he/Partial-Person-ReID](https://github.com/lingxiao-he/Partial-Person-ReID)\n\n**Occluded Person Re-identification**\n\n- intro: ICME 2018\n- intro: Sun Yat-Sen University\n- arxiv: [https://arxiv.org/abs/1804.02792](https://arxiv.org/abs/1804.02792)\n\n**Partial Person Re-identification with Alignment and Hallucination**\n\n- intro: Imperial College London\n- keywords: Partial Matching Net (PMN)\n- arxiv: [https://arxiv.org/abs/1807.09162](https://arxiv.org/abs/1807.09162)\n\n**SCPNet: Spatial-Channel Parallelism Network for Joint Holistic and Partial Person Re-Identification**\n\n- intro: ACCV 2018\n- intro: Megvii Inc. (Face++) & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1810.06996](https://arxiv.org/abs/1810.06996)\n- github: [https://github.com/xfanplus/Open-SCPNet](https://github.com/xfanplus/Open-SCPNet)\n\n**STNReID : Deep Convolutional Networks with Pairwise Spatial Transformer Networks for Partial Person Re-identification**\n\n- intro: Zhejiang University & Megvii Inc\n- arxiv: [https://arxiv.org/abs/1903.07072](https://arxiv.org/abs/1903.07072)\n\n**Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification**\n\n[https://arxiv.org/abs/1904.04975](https://arxiv.org/abs/1904.04975)\n\n**A Novel Teacher-Student Learning Framework For Occluded Person Re-Identification**\n\n[https://arxiv.org/abs/1907.03253](https://arxiv.org/abs/1907.03253)\n\n**VRSTC: Occlusion-Free Video Person Re-Identification**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1907.08427](https://arxiv.org/abs/1907.08427)\n\n**Pose-Guided Feature Alignment for Occluded Person Re-Identification**\n\n- intro: ICCV 2019\n- intro: Baidu Research & University of Technology Sydney\n- keywords: Pose-Guided Feature Alignment (PGFA)\n- paper: [http://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf)\n- paper: [https://yu-wu.net/pdf/ICCV2019_Occluded-reID.pdf](https://yu-wu.net/pdf/ICCV2019_Occluded-reID.pdf)\n- github: [https://github.com/lightas/Occluded-DukeMTMC-Dataset](https://github.com/lightas/Occluded-DukeMTMC-Dataset)\n\n**High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification**\n\n- intro: CVPR 2020\n- intro: Institute of Automation, CAS & Megvii Inc. & Beijing Institute of Technology\n- arxiv: [https://arxiv.org/abs/2003.08177](https://arxiv.org/abs/2003.08177)\n- github: [https://github.com/wangguanan/HOReID](https://github.com/wangguanan/HOReID)\n\n**Pose-guided Visible Part Matching for Occluded Person ReID**\n\n- intro: CVPR 2020\n- intro: Dalian University of Technology & Pengcheng Lab & The University of Sydney\n- arxiv: [https://arxiv.org/abs/2004.00230](https://arxiv.org/abs/2004.00230)\n- github(official, pytorch): [https://github.com/hh23333/PVPM](https://github.com/hh23333/PVPM)\n\n**MHSA-Net: Multi-Head Self-Attention Network for Occluded Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/2008.04015](https://arxiv.org/abs/2008.04015)\n- github: [https://github.com/hongchenphd/MHSA-Net](https://github.com/hongchenphd/MHSA-Net)\n\n**Holistic Guidance for Occluded Person Re-Identification**\n\n- intro: École de technologie supérieure & Genetec Inc\n- arxiv: [https://arxiv.org/abs/2104.06524](https://arxiv.org/abs/2104.06524)\n\n**Neighbourhood-guided Feature Reconstruction for Occluded Person Re-Identification**\n\n[https://arxiv.org/abs/2105.07345](https://arxiv.org/abs/2105.07345)\n\n**Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2106.04095](https://arxiv.org/abs/2106.04095)\n\n**Feature Completion for Occluded Person Re-Identification**\n\n- intro: TPAMI\n- arxiv: [https://arxiv.org/abs/2106.12733](https://arxiv.org/abs/2106.12733)\n- github: [https://github.com/blue-blue272/OccludedReID-RFCnet](https://github.com/blue-blue272/OccludedReID-RFCnet)\n\n**Learning Disentangled Representation Implicitly via Transformer for Occluded Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/2107.02380](https://arxiv.org/abs/2107.02380)\n- github: [https://github.com/Anonymous-release-code/DRL-Net](https://github.com/Anonymous-release-code/DRL-Net)\n\n**Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification**\n\n- intro: ACM MM 2021\n- arxiv: [https://arxiv.org/abs/2108.00139](https://arxiv.org/abs/2108.00139)\n\n**Feature Erasing and Diffusion Network for Occluded Person Re-Identification**\n\n- intro: Monash University & SenseTime & The University of Sydney & Xidian University\n- arxiv: [https://arxiv.org/abs/2112.08740](https://arxiv.org/abs/2112.08740)\n\n**Quality-aware Part Models for Occluded Person Re-identification**\n\n- intro: South China University of Technology & Baidu Inc & Shenzhen University & JD Explore Academy\n- arxiv: [https://arxiv.org/abs/2201.00107](https://arxiv.org/abs/2201.00107)\n\n**Motion-Aware Transformer For Occluded Person Re-identification**\n\n- intro: NetEase & China JiLiang University & Hunan Institute of Engineering\n- arxiv: [https://arxiv.org/abs/2202.04243](https://arxiv.org/abs/2202.04243)\n\n# Cross-modality Re-ID\n\n## RGB-IR Re-ID\n\n**RGB-Infrared Cross-Modality Person Re-Identification**\n\n- intro: ICCV 2017\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.pdf)\n\n**RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment**\n\n- intro: ICCV 2019\n- keywords: Alignment Generative Adversarial Network (AlignGAN)\n- arxiv: [https://arxiv.org/abs/1910.05839](https://arxiv.org/abs/1910.05839)\n\n**Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences**\n\n- intro: ICCV 2021\n- intro: Yonsei University\n- project page: [https://cvlab.yonsei.ac.kr/projects/LbA/](https://cvlab.yonsei.ac.kr/projects/LbA/)\n- arxiv: [https://arxiv.org/abs/2108.07422](https://arxiv.org/abs/2108.07422)\n\n## Depth-Based Re-ID\n\n**Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification**\n\n- intro: ECCV 2018\n- arxiv: [http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Karianakis_Reinforced_Temporal_Attention_ECCV_2018_paper.pdf](http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Karianakis_Reinforced_Temporal_Attention_ECCV_2018_paper.pdf)\n\n**A Cross-Modal Distillation Network for Person Re-identification in RGB-Depth**\n\n[https://arxiv.org/abs/1810.11641](https://arxiv.org/abs/1810.11641)\n\n# Low Resolution Re-ID\n\n**Multi-scale Learning for Low-resolution Person Re-identification**\n\n- intro: ICCV 2015\n- arxiv: [https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Li_Multi-Scale_Learning_for_ICCV_2015_paper.pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Li_Multi-Scale_Learning_for_ICCV_2015_paper.pdf)\n\n**Cascaded SR-GAN for Scale-Adaptive Low Resolution Person Re-identification**\n\n- intro: IJCAI 2018\n- arxiv: [https://www.ijcai.org/proceedings/2018/0541.pdf](https://www.ijcai.org/proceedings/2018/0541.pdf)\n\n**Deep Low-Resolution Person Re-Identification**\n\n- intro: AAAI 2018\n- keywords: Super resolution and Identity joiNt learninG (SING)\n- paper: [http://www.eecs.qmul.ac.uk/~xiatian/papers/JiaoEtAl_2018AAAI.pdf](http://www.eecs.qmul.ac.uk/~xiatian/papers/JiaoEtAl_2018AAAI.pdf)\n\n# Reinforcement Learning for Re-ID\n\n**Deep Reinforcement Learning Attention Selection for Person Re-Identification**\n\n- intro: BMVC 2017\n- arxiv: [https://arxiv.org/abs/1707.02785](https://arxiv.org/abs/1707.02785)\n- paper: [http://www.eecs.qmul.ac.uk/~sgg/papers/LanEtAl_2017BMVC.pdf](http://www.eecs.qmul.ac.uk/~sgg/papers/LanEtAl_2017BMVC.pdf)\n\n# Attributes Prediction for Re-ID\n\n**Multi-Task Learning with Low Rank Attribute Embedding for Person Re-identification**\n\n- intro: ICCV 2015\n- paper: [http://legacydirs.umiacs.umd.edu/~fyang/papers/iccv15.pdf](http://legacydirs.umiacs.umd.edu/~fyang/papers/iccv15.pdf)\n\n**Deep Attributes Driven Multi-Camera Person Re-identification**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1605.03259](https://arxiv.org/abs/1605.03259)\n\n**Improving Person Re-identification by Attribute and Identity Learning**\n\n[https://arxiv.org/abs/1703.07220](https://arxiv.org/abs/1703.07220)\n\n**Person Re-identification by Deep Learning Attribute-Complementary Information**\n\n- intro: CVPR 2017 workshop\n- paper: [https://sci-hub.tw/10.1109/CVPRW.2017.186](https://sci-hub.tw/10.1109/CVPRW.2017.186)\n\n**CA3Net: Contextual-Attentional Attribute-Appearance Network for Person Re-Identification**\n\n[https://arxiv.org/abs/1811.07544](https://arxiv.org/abs/1811.07544)\n\n**Attribute analysis with synthetic dataset for person re-identification**\n\n- intro: Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/2006.07139](https://arxiv.org/abs/2006.07139)\n\n**Taking A Closer Look at Synthesis: Fine-grained Attribute Analysis for Person Re-Identification**\n\n- intro: Shanghai Jiao Tong University & National University of Defense Technology\n- arxiv: [https://arxiv.org/abs/2010.08145](https://arxiv.org/abs/2010.08145)\n\n# Video Person Re-Identification\n\n**Recurrent Convolutional Network for Video-based Person Re-Identification**\n\n- intro: CVPR 2016\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.pdf)\n- github: [https://github.com/niallmcl/Recurrent-Convolutional-Video-ReID](https://github.com/niallmcl/Recurrent-Convolutional-Video-ReID)\n\n**Deep Recurrent Convolutional Networks for Video-based Person Re-identification: An End-to-End Approach**\n\n[https://arxiv.org/abs/1606.01609](https://arxiv.org/abs/1606.01609)\n\n**Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.02286](https://arxiv.org/abs/1708.02286)\n\n**Three-Stream Convolutional Networks for Video-based Person Re-Identification**\n\n[https://arxiv.org/abs/1712.01652](https://arxiv.org/abs/1712.01652)\n\n**LVreID: Person Re-Identification with Long Sequence Videos**\n\n[https://arxiv.org/abs/1712.07286](https://arxiv.org/abs/1712.07286)\n\n**Multi-shot Pedestrian Re-identification via Sequential Decision Making**\n\n- intro: CVPR 2018. TuSimple\n- keywords: reinforcement learning\n- arxiv: [https://arxiv.org/abs/1712.07257](https://arxiv.org/abs/1712.07257)\n- github: [https://github.com/TuSimple/rl-multishot-reid](https://github.com/TuSimple/rl-multishot-reid)\n\n**Learning Distributional Representation and Set Distance for Multi-shot Person Re-identification**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1808.01119](https://arxiv.org/abs/1808.01119)\n\n**LVreID: Person Re-Identification with Long Sequence Videos**\n\n[https://arxiv.org/abs/1712.07286](https://arxiv.org/abs/1712.07286)\n\n**Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification**\n\n- intro: CUHK-SenseTime & Argo AI\n- arxiv: [https://arxiv.org/abs/1803.09882](https://arxiv.org/abs/1803.09882)\n\n**Video Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding**\n\n- intro: CVPR 2018 Poster\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf)\n\n**Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf)\n\n**Video Person Re-Identification With Competitive Snippet-Similarity Aggregation and Co-Attentive Snippet Embedding**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf)\n\n**Revisiting Temporal Modeling for Video-based Person ReID**\n\n- arxiv: [https://arxiv.org/abs/1805.02104](https://arxiv.org/abs/1805.02104)\n- github: [https://github.com/jiyanggao/Video-Person-ReID](https://github.com/jiyanggao/Video-Person-ReID)\n\n**Video Person Re-identification by Temporal Residual Learning**\n\n[https://arxiv.org/abs/1802.07918](https://arxiv.org/abs/1802.07918)\n\n**A Spatial and Temporal Features Mixture Model with Body Parts for Video-based Person Re-Identification**\n\n[https://arxiv.org/abs/1807.00975](https://arxiv.org/abs/1807.00975)\n\n**Video-based Person Re-identification via 3D Convolutional Networks and Non-local Attention**\n\n- intro: University of Science and Technology of China & University of Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1807.05073](https://arxiv.org/abs/1807.05073)\n\n**Spatial-Temporal Synergic Residual Learning for Video Person Re-Identification**\n\n[https://arxiv.org/abs/1807.05799](https://arxiv.org/abs/1807.05799)\n\n**SCAN: Self-and-Collaborative Attention Network for Video Person Re-identification**\n\n[https://arxiv.org/abs/1807.05688](https://arxiv.org/abs/1807.05688)\n\n**Where-and-When to Look: Deep Siamese Attention Networks for Video-based Person Re-identification**\n\n- intro: IEEE Transactions on Multimedia\n- arxiv: [https://arxiv.org/abs/1808.01911](https://arxiv.org/abs/1808.01911)\n\n**STA: Spatial-Temporal Attention for Large-Scale Video-based Person Re-Identification**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.04129](https://arxiv.org/abs/1811.04129)\n\n**Multi-scale 3D Convolution Network for Video Based Person Re-Identification**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.07468](https://arxiv.org/abs/1811.07468)\n\n**Deep Active Learning for Video-based Person Re-identification**\n\n[https://arxiv.org/abs/1812.05785](https://arxiv.org/abs/1812.05785)\n\n**Spatial and Temporal Mutual Promotion for Video-based Person Re-identification**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1812.10305](https://arxiv.org/abs/1812.10305)\n\n**3D PersonVLAD: Learning Deep Global Representations for Video-based Person Re-identification**\n\n[https://arxiv.org/abs/1812.10222](https://arxiv.org/abs/1812.10222)\n\n**GAN-based Pose-aware Regulation for Video-based Person Re-identification**\n\n- intro: Heriot-Watt University & University of Edinburgh & Queen’s University Belfast & Anyvision\n- keywords: Weighted Fusion (WF) & Weighted-Pose Regulation (WPR)\n- arxiv: [https://arxiv.org/abs/1903.11552](https://arxiv.org/abs/1903.11552)\n\n**Convolutional Temporal Attention Model for Video-based Person Re-identification**\n\n- intro: ICME 2019\n- arxiv: [https://arxiv.org/abs/1904.04492](https://arxiv.org/abs/1904.04492)\n\n**Video Person Re-Identification using Learned Clip Similarity Aggregation**\n\n[https://arxiv.org/abs/1910.08055](https://arxiv.org/abs/1910.08055)\n\n**Video Person Re-ID: Fantastic Techniques and Where to Find Them**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1912.05295](https://arxiv.org/abs/1912.05295)\n- github: [https://github.com/ppriyank/Video-Person-Re-ID-Fantastic-Techniques-and-Where-to-Find-Them](https://github.com/ppriyank/Video-Person-Re-ID-Fantastic-Techniques-and-Where-to-Find-Them)\n\n**Multi-Granularity Reference-Aided Attentive Feature Aggregation for Video-based Person Re-identification**\n\n- intro: CVPR 2020\n- intro: 1University of Science and Technology of China & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/2003.12224](https://arxiv.org/abs/2003.12224)\n\n**Appearance-Preserving 3D Convolution for Video-based Person Re-identification**\n\n- intro: ECCV 2020 Oral\n- arxiv: [https://arxiv.org/abs/2007.08434](https://arxiv.org/abs/2007.08434)\n- github: [https://github.com/guxinqian/AP3D](https://github.com/guxinqian/AP3D)\n\n**Temporal Complementary Learning for Video Person Re-Identification**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.09357](https://arxiv.org/abs/2007.09357)\n- github: [https://github.com/blue-blue272/VideoReID-TCLNet](https://github.com/blue-blue272/VideoReID-TCLNet)\n\n**Reference-Aided Part-Aligned Feature Disentangling for Video Person Re-Identification**\n\n- intro: ICME 2021\n- arxiv: [https://arxiv.org/abs/2103.11319](https://arxiv.org/abs/2103.11319)\n\n**Spatial-Temporal Correlation and Topology Learning for Person Re-Identification in Videos**\n\n[https://arxiv.org/abs/2104.08241](https://arxiv.org/abs/2104.08241)\n\n**BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.14783](https://arxiv.org/abs/2104.14783)\n\n**Video-based Person Re-identification without Bells and Whistles**\n\n- intro: CVPR 2021 Biometrics Workshop\n- arxiv: [https://arxiv.org/abs/2105.10678](https://arxiv.org/abs/2105.10678)\n- github: [https://github.com/jackie840129/CF-AAN](https://github.com/jackie840129/CF-AAN)\n\n**Keypoint Message Passing for Video-based Person Re-Identification**\n\n[https://arxiv.org/abs/2111.08279](https://arxiv.org/abs/2111.08279)\n\n# Re-ranking\n\n**Divide and Fuse: A Re-ranking Approach for Person Re-identification**\n\n- intro: BMVC 2017\n- arxiv: [https://arxiv.org/abs/1708.04169](https://arxiv.org/abs/1708.04169)\n\n**Re-ranking Person Re-identification with k-reciprocal Encoding**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1701.08398](https://arxiv.org/abs/1701.08398)\n- github: [https://github.com/zhunzhong07/person-re-ranking](https://github.com/zhunzhong07/person-re-ranking)\n\n**A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1711.10378](https://arxiv.org/abs/1711.10378)\n- github(official): [https://github.com/pse-ecn/expanded-cross-neighborhood](https://github.com/pse-ecn/expanded-cross-neighborhood)\n\n**Adaptive Re-ranking of Deep Feature for Person Re-identification**\n\n[https://arxiv.org/abs/1811.08561](https://arxiv.org/abs/1811.08561)\n\n**Graph Convolution for Re-ranking in Person Re-identification**\n\n[https://arxiv.org/abs/2107.02220](https://arxiv.org/abs/2107.02220)\n\n# Unsupervised Re-ID\n\n**Unsupervised Person Re-identification: Clustering and Fine-tuning**\n\n- arxiv: [https://arxiv.org/abs/1705.10444](https://arxiv.org/abs/1705.10444)\n- github: [https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning](https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning)\n\n**Stepwise Metric Promotion for Unsupervised Video Person Re-identification**\n\n- intro: ICCV 2017\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.pdf)\n- github: [https://github.com/lilithliu/StepwiseMetricPromotion-code](https://github.com/lilithliu/StepwiseMetricPromotion-code)\n\n**Dynamic Label Graph Matching for Unsupervised Video Re-Identification**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.09297](https://arxiv.org/abs/1709.09297)\n- github: [https://github.com/mangye16/dgm_re-id](https://github.com/mangye16/dgm_re-id)\n\n**Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.07293](https://arxiv.org/abs/1803.07293)\n- github: [https://github.com/ahangchen/TFusion](https://github.com/ahangchen/TFusion)\n- blog: [https://zhuanlan.zhihu.com/p/34778414](https://zhuanlan.zhihu.com/p/34778414)\n\n**Cross-dataset Person Re-Identification Using Similarity Preserved Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1806.04533](https://arxiv.org/abs/1806.04533)\n\n**Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.07293](https://arxiv.org/abs/1803.07293)\n- github: [https://github.com/ahangchen/rank-reid](https://github.com/ahangchen/rank-reid)\n\n**Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.09786](https://arxiv.org/abs/1803.09786)\n\n**Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification**\n\n- intro: CVPR 2018 workshop. National Taiwan University & Umbo Computer Vision\n- keywords: adaptation and re-identification network (ARN)\n- arxiv: [https://arxiv.org/abs/1804.09347](https://arxiv.org/abs/1804.09347)\n\n**Domain Adaptation through Synthesis for Unsupervised Person Re-identification**\n\n[https://arxiv.org/abs/1804.10094](https://arxiv.org/abs/1804.10094)\n\n**Unsupervised Domain Adaptive Re-Identification: Theory and Practice**\n\n- intro: Wuhan University & Horizon Robotics & Huazhong Univ. of Science and Technology\n- arxiv: [https://arxiv.org/abs/1807.11334](https://arxiv.org/abs/1807.11334)\n- github: [https://github.com/LcDog/DomainAdaptiveReID](https://github.com/LcDog/DomainAdaptiveReID)\n\n**Deep Association Learning for Unsupervised Video Person Re-identification**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1808.07301](https://arxiv.org/abs/1808.07301)\n\n**Support Neighbor Loss for Person Re-Identification**\n\n- intro: ACM Multimedia (ACM MM) 2018\n- arxiv: [https://arxiv.org/abs/1808.06030](https://arxiv.org/abs/1808.06030)\n\n**Unsupervised Person Re-identification by Deep Learning Tracklet Association**\n\n- intro: ECCV 2018 Oral\n- keywords: Tracklet Association Unsupervised Deep Learning (TAUDL)\n- arxiv: [https://arxiv.org/abs/1809.02874](https://arxiv.org/abs/1809.02874)\n\n**Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification**\n\n- intro: ICCV 2019 oral\n- arxiv: [https://arxiv.org/abs/1811.10144](https://arxiv.org/abs/1811.10144)\n- github: [https://github.com/OasisYang/SSG](https://github.com/OasisYang/SSG)\n\n**Unsupervised Tracklet Person Re-Identification**\n\n- intro: TPAMI 2019\n- arxiv: [https://arxiv.org/abs/1903.00535](https://arxiv.org/abs/1903.00535)\n- github: [https://github.com/liminxian/DukeMTMC-SI-Tracklet](https://github.com/liminxian/DukeMTMC-SI-Tracklet)\n\n**Unsupervised Person Re-identification by Deep Asymmetric Metric Embedding**\n\n- intro: TPAMI\n- keywords: DEep Clustering-based Asymmetric MEtric Learning (DECAMEL)\n- arxiv: [https://arxiv.org/abs/1901.10177](https://arxiv.org/abs/1901.10177)\n- github: [https://github.com/KovenYu/DECAMEL](https://github.com/KovenYu/DECAMEL)\n\n**Unsupervised Person Re-identification by Soft Multilabel Learning**\n\n- intro: CVPR 2019 oral\n- intro: Sun Yat-sen University & YouTu Lab & Queen Mary University of London\n- keywords: MAR (MultilAbel Reference learning), soft multilabel-guided hard negative mining\n- project page: [https://kovenyu.com/publication/2019-cvpr-mar/](https://kovenyu.com/publication/2019-cvpr-mar/)\n- arxiv: [https://arxiv.org/abs/1903.06325](https://arxiv.org/abs/1903.06325)\n- github(official, Pytorch): [https://github.com/KovenYu/MAR](https://github.com/KovenYu/MAR)\n\n**A Bottom-up Clustering Approach to Unsupervised Person Re-identification**\n\n- intro: AAAI 2019 oral\n- paper: [https://vana77.github.io/vana77.github.io/images/AAAI19.pdf](https://vana77.github.io/vana77.github.io/images/AAAI19.pdf)\n- github: [https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification](https://github.com/vana77/Bottom-up-Clustering-Person-Re-identification)\n\n**A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification**\n\n[https://arxiv.org/abs/1904.03425](https://arxiv.org/abs/1904.03425)\n\n**Towards better Validity: Dispersion based Clustering for Unsupervised Person Re-identification**\n\n- arxiv: [https://arxiv.org/abs/1906.01308](https://arxiv.org/abs/1906.01308)\n- github: [https://github.com/gddingcs/Dispersion-based-Clustering](https://github.com/gddingcs/Dispersion-based-Clustering)\n\n**PAC-GAN: An Effective Pose Augmentation Scheme for Unsupervised Cross-View Person Re-identification**\n\n[https://arxiv.org/abs/1906.01792](https://arxiv.org/abs/1906.01792)\n\n**Adaptive Exploration for Unsupervised Person Re-Identification**\n\n- arxiv: [https://arxiv.org/abs/1907.04194](https://arxiv.org/abs/1907.04194)\n- github: [https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification](https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification)\n\n**Learning to Adapt Invariance in Memory for Person Re-identification**\n\n[https://arxiv.org/abs/1908.00485](https://arxiv.org/abs/1908.00485)\n\n**Consistent Cross-view Matching for Unsupervised Person Re-identification**\n\n[https://arxiv.org/abs/1908.10486](https://arxiv.org/abs/1908.10486)\n\n**Learning to Align Multi-Camera Domain for Unsupervised Video Person Re-Identification**\n\n[https://arxiv.org/abs/1909.13248](https://arxiv.org/abs/1909.13248)\n\n**Progressive Unsupervised Person Re-identification by Tracklet Association with Spatio-Temporal Regularization**\n\n[https://arxiv.org/abs/1910.11560](https://arxiv.org/abs/1910.11560)\n\n**Asymmetric Co-Teaching for Unsupervised Cross Domain Person Re-Identification**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1912.01349](https://arxiv.org/abs/1912.01349)\n- github: [https://github.com/FlyingRoastDuck/ACT_AAAI20](https://github.com/FlyingRoastDuck/ACT_AAAI20)\n\n**Memorizing Comprehensively to Learn Adaptively: Unsupervised Cross-Domain Person Re-ID with Multi-level Memory**\n\n[https://arxiv.org/abs/2001.04123](https://arxiv.org/abs/2001.04123)\n\n**Unsupervised Domain Adaptation in the Dissimilarity Space for Person Re-identification**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.13890](https://arxiv.org/abs/2007.13890)\n\n**Unsupervised Attention Based Instance Discriminative Learning for Person Re-Identification**\n\n- intro: WACV 2021\n- arxiv: [https://arxiv.org/abs/2011.01888](https://arxiv.org/abs/2011.01888)\n\n**Joint Generative and Contrastive Learning for Unsupervised Person Re-identification**\n\n- arxiv: [https://arxiv.org/abs/2012.09071](https://arxiv.org/abs/2012.09071)\n- github: [https://github.com/chenhao2345/GCL](https://github.com/chenhao2345/GCL)\n\n**Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification**\n\n- intro: CVPR 2021\n- intro: Xiamen University & University of Trento & Minjiang Universit & Minnan Normal University\n- arxiv: [https://arxiv.org/abs/2103.04618](https://arxiv.org/abs/2103.04618)\n- github: [https://github.com/FlyingRoastDuck/MetaCam_DSCE](https://github.com/FlyingRoastDuck/MetaCam_DSCE)\n\n**Cluster Contrast for Unsupervised Person Re-Identification**\n\n- intro: Alibaba A.I. Labs & Simon Fraser University\n- arxiv: [https://arxiv.org/abs/2103.11568](https://arxiv.org/abs/2103.11568)\n- github: [https://github.com/wangguangyuan/ClusterContrast](https://github.com/wangguangyuan/ClusterContrast)\n\n**Intra-Inter Camera Similarity for Unsupervised Person Re-Identification**\n\n- intro: CVPR 2021\n- intro: Peking University\n- arxiv: [https://arxiv.org/abs/2103.11658](https://arxiv.org/abs/2103.11658)\n\n**Group-aware Label Transfer for Domain Adaptive Person Re-identification**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.12366](https://arxiv.org/abs/2103.12366)\n\n**Large-Scale Unsupervised Person Re-Identification with Contrastive Learning**\n\n- intro: Tongji University\n- arxiv: [https://arxiv.org/abs/2105.07914](https://arxiv.org/abs/2105.07914)\n\n**Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification**\n\n- intro: Beijing University of Posts and Telecommunications\n- arxiv: [https://arxiv.org/abs/2106.07846](https://arxiv.org/abs/2106.07846)\n\n**Towards Discriminative Representation Learning for Unsupervised Person Re-identification**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.03439](https://arxiv.org/abs/2108.03439)\n\n**Unleashing the Potential of Unsupervised Pre-Training with Intra-Identity Regularization for Person Re-Identification**\n\n- intro: University of Science and Technology of China\n- keywords: UP-ReID\n- arxiv: [https://arxiv.org/abs/2112.00317](https://arxiv.org/abs/2112.00317)\n- github: [https://github.com/Frost-Yang-99/UP-ReID](https://github.com/Frost-Yang-99/UP-ReID)\n\n**Part-based Pseudo Label Refinement for Unsupervised Person Re-identification**\n\n- intro: CVPR 2022\n- intro: KAIST\n- arxiv: [https://arxiv.org/abs/2203.14675](https://arxiv.org/abs/2203.14675)\n\n**Implicit Sample Extension for Unsupervised Person Re-Identification**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.06892](https://arxiv.org/abs/2204.06892)\n- github: [https://github.com/PaddlePaddle/PaddleClas](https://github.com/PaddlePaddle/PaddleClas)\n\n# Weakly Supervised Person Re-identification\n\n**Weakly Supervised Person Re-Identification**\n\n- intro: CVPR 2019\n- keywords: multi-instance multi-label learning (MIML), Cross-View MIML (CV-MIML)\n- arxiv: [https://arxiv.org/abs/1904.03832](https://arxiv.org/abs/1904.03832)\n\n**Weakly Supervised Person Re-identification: Cost-effective Learning with A New Benchmark**\n\n- keywords: SYSU-30k\n- arxiv: [https://arxiv.org/abs/1904.03845](https://arxiv.org/abs/1904.03845)\n\n**Weakly Supervised Tracklet Person Re-Identification by Deep Feature-wise Mutual Learning**\n\n[https://arxiv.org/abs/1910.14333](https://arxiv.org/abs/1910.14333)\n\n# Vehicle Re-ID\n\n**Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.03918](https://arxiv.org/abs/1708.03918)\n\n**Viewpoint-Aware Attentive Multi-View Inference for Vehicle Re-Identification**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.pdf)\n\n**RAM: A Region-Aware Deep Model for Vehicle Re-Identification**\n\n- intro: ICME 2018\n- arxiv: [https://arxiv.org/abs/1806.09283](https://arxiv.org/abs/1806.09283)\n\n**Vehicle Re-Identification in Context**\n\n- intro: Pattern Recognition - 40th German Conference, (GCPR) 2018, Stuttgart\n- project page: [https://qmul-vric.github.io/](https://qmul-vric.github.io/)\n- arxiv: [https://arxiv.org/abs/1809.09409](https://arxiv.org/abs/1809.09409)\n\n**Vehicle Re-identification Using Quadruple Directional Deep Learning Features**\n\n[https://arxiv.org/abs/1811.05163](https://arxiv.org/abs/1811.05163)\n\n**Coarse-to-fine: A RNN-based hierarchical attention model for vehicle re-identification**\n\n- intro: ACCV 2018\n- arxiv: [https://arxiv.org/abs/1812.04239](https://arxiv.org/abs/1812.04239)\n\n**Vehicle Re-Identification: an Efficient Baseline Using Triplet Embedding**\n\n[https://arxiv.org/abs/1901.01015](https://arxiv.org/abs/1901.01015)\n\n**A Two-Stream Siamese Neural Network for Vehicle Re-Identification by Using Non-Overlapping Cameras**\n\n- intro: ICIP 2019\n- arxiv: [https://arxiv.org/abs/1902.01496](https://arxiv.org/abs/1902.01496)\n\n**CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification**\n\n- intro: Accepted for oral presentation at CVPR 2019 with review ratings of 2 strong accepts and 1 accept (work done during an internship at NVIDIA)\n- arxiv: [https://arxiv.org/abs/1903.09254](https://arxiv.org/abs/1903.09254)\n\n**Vehicle Re-identification in Aerial Imagery: Dataset and Approach**\n\n- intro: Northwestern Polytechnical University\n- arxiv: [https://arxiv.org/abs/1904.01400](https://arxiv.org/abs/1904.01400)\n\n**Attributes Guided Feature Learning for Vehicle Re-identification**\n\n[https://arxiv.org/abs/1905.08997](https://arxiv.org/abs/1905.08997)\n\n**A unified neural network for object detection, multiple object tracking and vehicle re-identification**\n\n[https://arxiv.org/abs/1907.03465](https://arxiv.org/abs/1907.03465)\n\n**Part-Guided Attention Learning for Vehicle Re-Identification**\n\n[https://arxiv.org/abs/1909.06023](https://arxiv.org/abs/1909.06023)\n\n**Vehicle Re-identification with Viewpoint-aware Metric Learning**\n\n- intro: ICCV 2019\n- intro: Beihang University & Tsinghua University & Megvii Technology\n- keywords: Viewpoint-Aware Network (VANet)\n- arxiv: [https://arxiv.org/abs/1910.04104](https://arxiv.org/abs/1910.04104)\n\n**PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/2005.00673](https://arxiv.org/abs/2005.00673)\n- github: [https://github.com/NVlabs/PAMTRI](https://github.com/NVlabs/PAMTRI)\n\n**Stripe-based and Attribute-aware Network: A Two-Branch Deep Model for Vehicle Re-identification**\n\n[https://arxiv.org/abs/1910.05549](https://arxiv.org/abs/1910.05549)\n\n**Background Segmentation for Vehicle Re-Identification**\n\n[https://arxiv.org/abs/1910.06613](https://arxiv.org/abs/1910.06613)\n\n**Vehicle Re-ID Collection**\n\n[https://github.com/layumi/Vehicle_reID-Collection](https://github.com/layumi/Vehicle_reID-Collection)\n\n**awesome-vehicle-re-identification**\n\n[https://github.com/knwng/awesome-vehicle-re-identification](https://github.com/knwng/awesome-vehicle-re-identification)\n\n**DCDLearn: Multi-order Deep Cross-distance Learning for Vehicle Re-Identification**\n\n[https://arxiv.org/abs/2003.11315](https://arxiv.org/abs/2003.11315)\n\n**Parsing-based View-aware Embedding Network for Vehicle Re-Identification**\n\n- intro: CVPR 2020 poster\n- arxiv: [https://arxiv.org/abs/2004.05021](https://arxiv.org/abs/2004.05021)\n- github(official): [https://github.com/silverbulletmdc/PVEN](https://github.com/silverbulletmdc/PVEN)\n\n**Multi-Domain Learning and Identity Mining for Vehicle Re-Identification**\n\n- intro: Solution for AI City Challenge, CVPR2020 Workshop\n- intro: The 3rd Place Submission to AICity Challenge 2020 Track2\n- arxiv: [https://arxiv.org/abs/2004.10547](https://arxiv.org/abs/2004.10547)\n- github: [https://github.com/heshuting555/AICITY2020_DMT_VehicleReID](https://github.com/heshuting555/AICITY2020_DMT_VehicleReID)\n\n**VOC-ReID: Vehicle Re-identification based on Vehicle-Orientation-Camera**\n\n- intro: CVPR 2020 workshop, 2nd place solution for AICity2020 Challenge ReID track\n- intro: RuiyanAI (睿沿科技)\n- arxiv: [https://arxiv.org/abs/2004.09164](https://arxiv.org/abs/2004.09164)\n- github: [https://github.com/Xiangyu-CAS/AICity2020-VOC-ReID](https://github.com/Xiangyu-CAS/AICity2020-VOC-ReID)\n\n**VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification**\n\n- intro: The 1st Place Submission to AICity Challenge 2020 re-id track (Baidu-UTS submission)\n- arxiv: [https://arxiv.org/abs/2004.06305](https://arxiv.org/abs/2004.06305)\n- gtihub: [https://github.com/layumi/AICIty-reID-2020](https://github.com/layumi/AICIty-reID-2020)\n- blog: [https://zhuanlan.zhihu.com/p/186905783](https://zhuanlan.zhihu.com/p/186905783)\n\n**Viewpoint-Aware Channel-Wise Attentive Network for Vehicle Re-Identification**\n\n- intro: CVPR Workshop 2020\n- arxiv: [https://arxiv.org/abs/2010.05810](https://arxiv.org/abs/2010.05810)\n\n**Self-Supervised Visual Attention Learning for Vehicle Re-Identification**\n\n[https://arxiv.org/abs/2010.09221](https://arxiv.org/abs/2010.09221)\n\n**AttributeNet: Attribute Enhanced Vehicle Re-Identification**\n\n[https://arxiv.org/abs/2102.03898](https://arxiv.org/abs/2102.03898)\n\n**A Strong Baseline for Vehicle Re-Identification**\n\n- intro: CVPR Workshop 2021, 5th AI City Challenge\n- intro: Cybercore AI\n- arxiv: [https://arxiv.org/abs/2104.10850](https://arxiv.org/abs/2104.10850)\n- github: [https://github.com/cybercore-co-ltd/track2_aicity_2021](https://github.com/cybercore-co-ltd/track2_aicity_2021)\n\n# Deep Metric Learning\n\n**Deep Metric Learning for Person Re-Identification**\n\n- intro: ICPR 2014\n- paper: [http://www.cbsr.ia.ac.cn/users/zlei/papers/ICPR2014/Yi-ICPR-14.pdf](http://www.cbsr.ia.ac.cn/users/zlei/papers/ICPR2014/Yi-ICPR-14.pdf)\n\n**Deep Metric Learning for Practical Person Re-Identification**\n\n[https://arxiv.org/abs/1407.4979](https://arxiv.org/abs/1407.4979)\n\n**Constrained Deep Metric Learning for Person Re-identification**\n\n[https://arxiv.org/abs/1511.07545](https://arxiv.org/abs/1511.07545)\n\n**Embedding Deep Metric for Person Re-identication A Study Against Large Variations**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1611.00137](https://arxiv.org/abs/1611.00137)\n\n**DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer**\n\n- intro: TuSimple\n- keywords: pedestrian re-identification\n- arxiv: [https://arxiv.org/abs/1707.01220](https://arxiv.org/abs/1707.01220)\n\n# Projects\n\n**Open-ReID: Open source person re-identification library in python**\n\n- intro: Open-ReID is a lightweight library of person re-identification for research purpose. It aims to provide a uniform interface for different datasets, a full set of models and evaluation metrics, as well as examples to reproduce (near) state-of-the-art results.\n- project page: [https://cysu.github.io/open-reid/](https://cysu.github.io/open-reid/)\n- github(PyTorch): [https://github.com/Cysu/open-reid](https://github.com/Cysu/open-reid)\n- examples: [https://cysu.github.io/open-reid/examples/training_id.html](https://cysu.github.io/open-reid/examples/training_id.html)\n- benchmarks: [https://cysu.github.io/open-reid/examples/benchmarks.html](https://cysu.github.io/open-reid/examples/benchmarks.html)\n\n**FastReID: A Pytorch Toolbox for Real-world Person Re-identification**\n\n- intro: SOTA ReID Methods and Toolbox\n- intro: JD AI Research\n- arxiv: [https://arxiv.org/abs/2006.02631](https://arxiv.org/abs/2006.02631)\n- github: [https://github.com/JDAI-CV/fast-reid](https://github.com/JDAI-CV/fast-reid)\n\n**light-reid: a toolbox of light reid for fast feature extraction and search**\n\n- intro: a toolbox of light-reid learning for faster inference, speed both feature extraction and retrieval stages up to >30x\n- github: [https://github.com/wangguanan/light-reid](https://github.com/wangguanan/light-reid)\n\n**caffe-PersonReID**\n\n- intro: Person Re-Identification: Multi-Task Deep CNN with Triplet Loss\n- gtihub: [https://github.com/agjayant/caffe-Person-ReID](https://github.com/agjayant/caffe-Person-ReID)\n\n**Person_reID_baseline_pytorch**\n\n- intro: Pytorch implement of Person re-identification baseline\n- arxiv: [https://github.com/layumi/Person_reID_baseline_pytorch](https://github.com/layumi/Person_reID_baseline_pytorch)\n\n**deep-person-reid**\n\n- intro: Pytorch implementation of deep person re-identification models.\n- github: [https://github.com/KaiyangZhou/deep-person-reid](https://github.com/KaiyangZhou/deep-person-reid)\n\n**ReID_baseline**\n\n- intro: Baseline model (with bottleneck) for person ReID (using softmax and triplet loss).\n- intro: 一个强力的ReID basemodel\n- github: [https://github.com/L1aoXingyu/reid_baseline](https://github.com/L1aoXingyu/reid_baseline)\n- blog: [https://zhuanlan.zhihu.com/p/40514536](https://zhuanlan.zhihu.com/p/40514536)\n\n**gluon-reid**\n\n- intro: A code gallery for person re-identification with mxnet-gluon, and I will reproduce many STOA algorithm.\n- github: [https://github.com/xiaolai-sqlai/gluon-reid](https://github.com/xiaolai-sqlai/gluon-reid)\n\n**A PyTorch based strong baseline for person search**\n\n[https://github.com/DeepAlchemist/deep-person-search](https://github.com/DeepAlchemist/deep-person-search)\n\n**OpenUnReID**\n\n- intro: PyTorch open-source toolbox for unsupervised or domain adaptive object re-ID\n- github: [https://github.com/open-mmlab/OpenUnReID](https://github.com/open-mmlab/OpenUnReID)\n\n# Evaluation\n\n**DukeMTMC-reID**\n\n- intro: The Person re-ID Evaluation Code for DukeMTMC-reID Dataset (Including Dataset Download)\n- github: [https://github.com/layumi/DukeMTMC-reID_evaluation](https://github.com/layumi/DukeMTMC-reID_evaluation)\n\n**DukeMTMC-reID_baseline (Matlab)**\n\n[https://github.com/layumi/DukeMTMC-reID_baseline](https://github.com/layumi/DukeMTMC-reID_baseline)\n\n**Code for IDE baseline on Market-1501**\n\n[https://github.com/zhunzhong07/IDE-baseline-Market-1501](https://github.com/zhunzhong07/IDE-baseline-Market-1501)\n\n# Talks\n\n**1st Workshop on Target Re-Identification and Multi-Target Multi-Camera Tracking**\n\n[https://reid-mct.github.io/](https://reid-mct.github.io/)\n\n**Target Re-Identification and Multi-Target Multi-Camera Tracking**\n\n[http://openaccess.thecvf.com/CVPR2017_workshops/CVPR2017_W17.py](http://openaccess.thecvf.com/CVPR2017_workshops/CVPR2017_W17.py)\n\n# Resources\n\n**Re-id Resources**\n\n[https://wangzwhu.github.io/home/re_id_resources.html](https://wangzwhu.github.io/home/re_id_resources.html)\n\n**Person Re-Identification - Papers With Code**\n\n[https://paperswithcode.com/task/person-re-identification](https://paperswithcode.com/task/person-re-identification)\n\n**Awesome Person Re-Identification**\n\n[https://github.com/FDU-VTS/Awesome-Person-Re-Identification](https://github.com/FDU-VTS/Awesome-Person-Re-Identification)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","title":"Deep Learning Resources"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Deep Learning Resources\ndate: 2015-10-09\n---\n\n# ImageNet\n\nSingle-model on 224x224\n\n| Method              | top1        | top5        | Model Size  | Speed       |\n|:-------------------:|:-----------:|:-----------:|:-----------:|:-----------:|\n| ResNet-101          | 78.0%       | 94.0%       |             |             |\n| ResNet-200          | 78.3%       | 94.2%       |             |             |\n| Inception-v3        |             |             |             |             |\n| Inception-v4        |             |             |             |             |\n| Inception-ResNet-v2 |             |             |             |             |\n| ResNet-50           | 77.8%       |             |             |             |\n| ResNet-101          | 79.6%       | 94.7%       |             |             |\n\nSingle-model on 320×320 / 299×299\n\n| Method              | top1        | top5        | Model Size  | Speed       |\n|:-------------------:|:-----------:|:-----------:|:-----------:|:-----------:|\n| ResNet-101          |             |             |             |             |\n| ResNet-200          | 79.9%       | 95.2%       |             |             |\n| Inception-v3        | 78.8%       | 94.4%       |             |             |\n| Inception-v4        | 80.0%       | 95.0%       |             |             |\n| Inception-ResNet-v2 | 80.1%       | 95.1%       |             |             |\n| ResNet-50           |             |             |             |             |\n| ResNet-101          | 80.9%       | 95.6%       |             |             |\n\n## AlexNet\n\n**ImageNet Classification with Deep Convolutional Neural Networks**\n\n- nips-page: [http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-)\n- paper: [http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n- slides: [http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)\n- code: [https://code.google.com/p/cuda-convnet/](https://code.google.com/p/cuda-convnet/)\n- github: [https://github.com/dnouri/cuda-convnet](https://github.com/dnouri/cuda-convnet)\n- code: [https://code.google.com/p/cuda-convnet2/](https://code.google.com/p/cuda-convnet2/)\n\n## Network In Network\n\n**Network In Network**\n\n- intro: ICLR 2014\n- arxiv: [http://arxiv.org/abs/1312.4400](http://arxiv.org/abs/1312.4400)\n- gitxiv: [http://gitxiv.com/posts/PA98qGuMhsijsJzgX/network-in-network-nin](http://gitxiv.com/posts/PA98qGuMhsijsJzgX/network-in-network-nin)\n- code(Caffe, official): [https://gist.github.com/mavenlin/d802a5849de39225bcc6](https://gist.github.com/mavenlin/d802a5849de39225bcc6)\n\n**Batch-normalized Maxout Network in Network**\n\n- arxiv: [http://arxiv.org/abs/1511.02583](http://arxiv.org/abs/1511.02583)\n\n## GoogLeNet (Inception V1)\n\n**Going Deeper with Convolutions**\n\n- arxiv: [http://arxiv.org/abs/1409.4842](http://arxiv.org/abs/1409.4842)\n- github: [https://github.com/google/inception](https://github.com/google/inception)\n- github: [https://github.com/soumith/inception.torch](https://github.com/soumith/inception.torch)\n\n**Building a deeper understanding of images**\n\n- blog: [http://googleresearch.blogspot.jp/2014/09/building-deeper-understanding-of-images.html](http://googleresearch.blogspot.jp/2014/09/building-deeper-understanding-of-images.html)\n\n## VGGNet\n\n**Very Deep Convolutional Networks for Large-Scale Image Recognition**\n\n- homepage: [http://www.robots.ox.ac.uk/~vgg/research/very_deep/](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n- arxiv: [http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556)\n- slides: [http://llcao.net/cu-deeplearning15/presentation/cc3580_Simonyan.pptx](http://llcao.net/cu-deeplearning15/presentation/cc3580_Simonyan.pptx)\n- slides: [http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf](http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf)\n- slides: [http://deeplearning.cs.cmu.edu/slides.2015/25.simonyan.pdf](http://deeplearning.cs.cmu.edu/slides.2015/25.simonyan.pdf)\n- github(official, deprecated Caffe API): [https://gist.github.com/ksimonyan/211839e770f7b538e2d8](https://gist.github.com/ksimonyan/211839e770f7b538e2d8)\n- github: [https://github.com/ruimashita/caffe-train](https://github.com/ruimashita/caffe-train)\n\n**Tensorflow VGG16 and VGG19**\n\n- github: [https://github.com/machrisaa/tensorflow-vgg](https://github.com/machrisaa/tensorflow-vgg)\n\n**RepVGG: Making VGG-style ConvNets Great Again**\n\n- intro: BNRist & Tsinghua University & MEGVII Technology & Hong Kong University of Science and Technology & Aberystwyth University\n- arxiv: [https://arxiv.org/abs/2101.03697](https://arxiv.org/abs/2101.03697)\n- github: [https://github.com/DingXiaoH/RepVGG](https://github.com/DingXiaoH/RepVGG)\n- github: [https://github.com/megvii-model/RepVGG](https://github.com/megvii-model/RepVGG)\n\n## Inception-V2\n\n**Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n\n- intro: ImageNet top-5 error: 4.82%\n- keywords: internal covariate shift problem\n- arxiv: [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167)\n- blog: [https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/](https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/)\n- notes: [http://blog.csdn.net/happynear/article/details/44238541](http://blog.csdn.net/happynear/article/details/44238541)\n- github: [https://github.com/lim0606/caffe-googlenet-bn](https://github.com/lim0606/caffe-googlenet-bn)\n\n**ImageNet pre-trained models with batch normalization**\n\n- arxiv: [https://arxiv.org/abs/1612.01452](https://arxiv.org/abs/1612.01452)\n- project page: [http://www.inf-cv.uni-jena.de/Research/CNN+Models.html](http://www.inf-cv.uni-jena.de/Research/CNN+Models.html)\n- github: [https://github.com/cvjena/cnn-models](https://github.com/cvjena/cnn-models)\n\n## Inception-V3\n\nInception-V3 = Inception-V2 + BN-auxiliary (fully connected layer of the auxiliary classifier is also batch-normalized, \nnot just the convolutions)\n\n**Rethinking the Inception Architecture for Computer Vision**\n\n- intro: \"21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network; \n3.5% top-5 error and 17.3% top-1 error With an ensemble of 4 models and multi-crop evaluation.\"\n- arxiv: [http://arxiv.org/abs/1512.00567](http://arxiv.org/abs/1512.00567)\n- github: [https://github.com/Moodstocks/inception-v3.torch](https://github.com/Moodstocks/inception-v3.torch)\n\n**Inception in TensorFlow**\n\n- intro: demonstrate how to train the Inception v3 architecture\n- github: [https://github.com/tensorflow/models/tree/master/inception](https://github.com/tensorflow/models/tree/master/inception)\n\n**Train your own image classifier with Inception in TensorFlow**\n\n- intro: Inception-v3\n- blog: [https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html](https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html)\n\n**Notes on the TensorFlow Implementation of Inception v3**\n\n[https://pseudoprofound.wordpress.com/2016/08/28/notes-on-the-tensorflow-implementation-of-inception-v3/](https://pseudoprofound.wordpress.com/2016/08/28/notes-on-the-tensorflow-implementation-of-inception-v3/)\n\n**Training an InceptionV3-based image classifier with your own dataset**\n\n- github: [https://github.com/danielvarga/keras-finetuning](https://github.com/danielvarga/keras-finetuning)\n\n**Inception-BN full for Caffe: Inception-BN ImageNet (21K classes) model for Caffe**\n\n- github: [https://github.com/pertusa/InceptionBN-21K-for-Caffe](https://github.com/pertusa/InceptionBN-21K-for-Caffe)\n\n## ResNet\n\n**Deep Residual Learning for Image Recognition**\n\n- intro: CVPR 2016 Best Paper Award\n- arxiv: [http://arxiv.org/abs/1512.03385](http://arxiv.org/abs/1512.03385)\n- slides: [http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf](http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)\n- gitxiv: [http://gitxiv.com/posts/LgPRdTY3cwPBiMKbm/deep-residual-learning-for-image-recognition](http://gitxiv.com/posts/LgPRdTY3cwPBiMKbm/deep-residual-learning-for-image-recognition)\n- github: [https://github.com/KaimingHe/deep-residual-networks](https://github.com/KaimingHe/deep-residual-networks)\n- github: [https://github.com/ry/tensorflow-resnet](https://github.com/ry/tensorflow-resnet)\n\n**Third-party re-implementations**\n\n[https://github.com/KaimingHe/deep-residual-networks#third-party-re-implementations](https://github.com/KaimingHe/deep-residual-networks#third-party-re-implementations)\n\n**Training and investigating Residual Nets**\n\n- intro: Facebook AI Research\n- blog: [http://torch.ch/blog/2016/02/04/resnets.html](http://torch.ch/blog/2016/02/04/resnets.html)\n- github: [https://github.com/facebook/fb.resnet.torch](https://github.com/facebook/fb.resnet.torch)\n\n**resnet.torch: an updated version of fb.resnet.torch with many changes.**\n\n- github: [https://github.com/erogol/resnet.torch](https://github.com/erogol/resnet.torch)\n\n**Highway Networks and Deep Residual Networks**\n\n- blog: [http://yanran.li/peppypapers/2016/01/10/highway-networks-and-deep-residual-networks.html](http://yanran.li/peppypapers/2016/01/10/highway-networks-and-deep-residual-networks.html)\n\n**Interpretating Deep Residual Learning Blocks as Locally Recurrent Connections**\n\n- blog: [https://matrixmashing.wordpress.com/2016/01/29/interpretating-deep-residual-learning-blocks-as-locally-recurrent-connections/](https://matrixmashing.wordpress.com/2016/01/29/interpretating-deep-residual-learning-blocks-as-locally-recurrent-connections/)\n\n**Lab41 Reading Group: Deep Residual Learning for Image Recognition**\n\n- blog: [https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f](https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f)\n\n**50-layer ResNet, trained on ImageNet, classifying webcam**\n\n- homepage: [https://ml4a.github.io/demos/keras.js/](https://ml4a.github.io/demos/keras.js/)\n\n**Reproduced ResNet on CIFAR-10 and CIFAR-100 dataset.**\n\n- github: [https://github.com/tensorflow/models/tree/master/resnet](https://github.com/tensorflow/models/tree/master/resnet)\n\n## ResNet-V2\n\n**Identity Mappings in Deep Residual Networks**\n\n- intro: ECCV 2016. ResNet-v2\n- arxiv: [http://arxiv.org/abs/1603.05027](http://arxiv.org/abs/1603.05027)\n- github: [https://github.com/KaimingHe/resnet-1k-layers](https://github.com/KaimingHe/resnet-1k-layers)\n- github: [https://github.com/tornadomeet/ResNet](https://github.com/tornadomeet/ResNet)\n\n**Deep Residual Networks for Image Classification with Python + NumPy**\n\n![](https://dnlcrl.github.io/assets/thesis-post/Diagramma.png)\n\n- blog: [https://dnlcrl.github.io/projects/2016/06/22/Deep-Residual-Networks-for-Image-Classification-with-Python+NumPy.html](https://dnlcrl.github.io/projects/2016/06/22/Deep-Residual-Networks-for-Image-Classification-with-Python+NumPy.html)\n\n## Inception-V4 / Inception-ResNet-V2\n\n**Inception-V4, Inception-Resnet And The Impact Of Residual Connections On Learning**\n\n- intro: Workshop track - ICLR 2016. 3.08 % top-5 error on ImageNet CLS\n- intro: \"achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge\"\n- arxiv: [http://arxiv.org/abs/1602.07261](http://arxiv.org/abs/1602.07261)\n- github(Keras): [https://github.com/kentsommer/keras-inceptionV4](https://github.com/kentsommer/keras-inceptionV4)\n\n**The inception-resnet-v2 models trained from scratch via torch**\n\n- github: [https://github.com/lim0606/torch-inception-resnet-v2](https://github.com/lim0606/torch-inception-resnet-v2)\n\n**Inception v4 in Keras**\n\n- intro: Inception-v4, Inception - Resnet-v1 and v2\n- github: [https://github.com/titu1994/Inception-v4](https://github.com/titu1994/Inception-v4)\n\n## ResNeXt\n\n**Aggregated Residual Transformations for Deep Neural Networks**\n\n- intro: CVPR 2017. UC San Diego & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1611.05431](https://arxiv.org/abs/1611.05431)\n- github(Torch): [https://github.com/facebookresearch/ResNeXt](https://github.com/facebookresearch/ResNeXt)\n- github: [https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol/resnext.py](https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol/resnext.py)\n- dataset: [http://data.dmlc.ml/models/imagenet/resnext/](http://data.dmlc.ml/models/imagenet/resnext/)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/5haml9/p_implementation_of_aggregated_residual/](https://www.reddit.com/r/MachineLearning/comments/5haml9/p_implementation_of_aggregated_residual/)\n\n## ResNeSt\n\n**ResNeSt: Split-Attention Networks**\n\n- intro: Amazon & University of California\n- arxiv: [https://arxiv.org/abs/2004.08955](https://arxiv.org/abs/2004.08955)\n- github: [https://github.com/zhanghang1989/ResNeSt](https://github.com/zhanghang1989/ResNeSt)\n\n## Residual Networks Variants\n\n**Resnet in Resnet: Generalizing Residual Architectures**\n\n- paper: [http://beta.openreview.net/forum?id=lx9l4r36gU2OVPy8Cv9g](http://beta.openreview.net/forum?id=lx9l4r36gU2OVPy8Cv9g)\n- arxiv: [http://arxiv.org/abs/1603.08029](http://arxiv.org/abs/1603.08029)\n\n**Residual Networks are Exponential Ensembles of Relatively Shallow Networks**\n\n- arxiv: [http://arxiv.org/abs/1605.06431](http://arxiv.org/abs/1605.06431)\n\n**Wide Residual Networks**\n\n- intro: BMVC 2016\n- arxiv: [http://arxiv.org/abs/1605.07146](http://arxiv.org/abs/1605.07146)\n- github: [https://github.com/szagoruyko/wide-residual-networks](https://github.com/szagoruyko/wide-residual-networks)\n- github: [https://github.com/asmith26/wide_resnets_keras](https://github.com/asmith26/wide_resnets_keras)\n- github: [https://github.com/ritchieng/wideresnet-tensorlayer](https://github.com/ritchieng/wideresnet-tensorlayer)\n- github: [https://github.com/xternalz/WideResNet-pytorch](https://github.com/xternalz/WideResNet-pytorch)\n- github(Torch): [https://github.com/meliketoy/wide-residual-network](https://github.com/meliketoy/wide-residual-network)\n\n**Residual Networks of Residual Networks: Multilevel Residual Networks**\n\n- arxiv: [http://arxiv.org/abs/1608.02908](http://arxiv.org/abs/1608.02908)\n\n**Multi-Residual Networks**\n\n- arxiv: [http://arxiv.org/abs/1609.05672](http://arxiv.org/abs/1609.05672)\n- github: [https://github.com/masoudabd/multi-resnet](https://github.com/masoudabd/multi-resnet)\n\n**Deep Pyramidal Residual Networks**\n\n- intro: PyramidNet\n- arxiv: [https://arxiv.org/abs/1610.02915](https://arxiv.org/abs/1610.02915)\n- github: [https://github.com/jhkim89/PyramidNet](https://github.com/jhkim89/PyramidNet)\n\n**Learning Identity Mappings with Residual Gates**\n\n- arxiv: [https://arxiv.org/abs/1611.01260](https://arxiv.org/abs/1611.01260)\n\n**Wider or Deeper: Revisiting the ResNet Model for Visual Recognition**\n\n- intro: image classification, semantic image segmentation\n- arxiv: [https://arxiv.org/abs/1611.10080](https://arxiv.org/abs/1611.10080)\n- github: [https://github.com/itijyou/ademxapp](https://github.com/itijyou/ademxapp)\n\n**Deep Pyramidal Residual Networks with Separated Stochastic Depth**\n\n- arxiv: [https://arxiv.org/abs/1612.01230](https://arxiv.org/abs/1612.01230)\n\n**Spatially Adaptive Computation Time for Residual Networks**\n\n- intro: Higher School of Economics & Google & CMU\n- arxiv: [https://arxiv.org/abs/1612.02297](https://arxiv.org/abs/1612.02297)\n\n**ShaResNet: reducing residual network parameter number by sharing weights**\n\n- arxiv: [https://arxiv.org/abs/1702.08782](https://arxiv.org/abs/1702.08782)\n- github: [https://github.com/aboulch/sharesnet](https://github.com/aboulch/sharesnet)\n\n**Sharing Residual Units Through Collective Tensor Factorization in Deep Neural Networks**\n\n- intro: Collective Residual Networks\n- arxiv: [https://arxiv.org/abs/1703.02180](https://arxiv.org/abs/1703.02180)\n- github(MXNet): [https://github.com/cypw/CRU-Net](https://github.com/cypw/CRU-Net)\n\n**Residual Attention Network for Image Classification**\n\n- intro: CVPR 2017 Spotlight. SenseTime Group Limited & Tsinghua University & The Chinese University of Hong Kong\n- intro: ImageNet (4.8% single model and single crop, top-5 error)\n- arxiv: [https://arxiv.org/abs/1704.06904](https://arxiv.org/abs/1704.06904)\n- github(Caffe): [https://github.com/buptwangfei/residual-attention-network](https://github.com/buptwangfei/residual-attention-network)\n\n**Dilated Residual Networks**\n\n- intro: CVPR 2017. Princeton University & Intel Labs\n- keywords: Dilated Residual Networks (DRN)\n- project page: [http://vladlen.info/publications/dilated-residual-networks/](http://vladlen.info/publications/dilated-residual-networks/)\n- arxiv: [https://arxiv.org/abs/1705.09914](https://arxiv.org/abs/1705.09914)\n- paper: [http://vladlen.info/papers/DRN.pdf](http://vladlen.info/papers/DRN.pdf)\n\n**Dynamic Steerable Blocks in Deep Residual Networks**\n\n- intro: University of Amsterdam & ESAT-PSI\n- arxiv: [https://arxiv.org/abs/1706.00598](https://arxiv.org/abs/1706.00598)\n\n**Learning Deep ResNet Blocks Sequentially using Boosting Theory**\n\n- intro: Microsoft Research & Princeton University\n- arxiv: [https://arxiv.org/abs/1706.04964](https://arxiv.org/abs/1706.04964)\n\n**Learning Strict Identity Mappings in Deep Residual Networks**\n\n- keywords: epsilon-ResNet\n- arxiv: [https://arxiv.org/abs/1804.01661](https://arxiv.org/abs/1804.01661)\n\n**Spiking Deep Residual Network**\n\n[https://arxiv.org/abs/1805.01352](https://arxiv.org/abs/1805.01352)\n\n**Norm-Preservation: Why Residual Networks Can Become Extremely Deep?**\n\n- intro: University of Central Florida\n- arxiv: [https://arxiv.org/abs/1805.07477](https://arxiv.org/abs/1805.07477)\n\n**MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks**\n\n- intro: Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/2009.08453](https://arxiv.org/abs/2009.08453)\n- github: [https://github.com/szq0214/MEAL-V2](https://github.com/szq0214/MEAL-V2)\n\n## DenseNet\n\n**Densely Connected Convolutional Networks**\n\n![](https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg)\n\n- intro: CVPR 2017 best paper. Cornell University & Tsinghua University. DenseNet\n- arxiv: [http://arxiv.org/abs/1608.06993](http://arxiv.org/abs/1608.06993)\n- github: [https://github.com/liuzhuang13/DenseNet](https://github.com/liuzhuang13/DenseNet)\n- github(Lasagne): [https://github.com/Lasagne/Recipes/tree/master/papers/densenet](https://github.com/Lasagne/Recipes/tree/master/papers/densenet)\n- github(Keras): [https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet)\n- github(Caffe): [https://github.com/liuzhuang13/DenseNetCaffe](https://github.com/liuzhuang13/DenseNetCaffe)\n- github(Tensorflow): [https://github.com/YixuanLi/densenet-tensorflow](https://github.com/YixuanLi/densenet-tensorflow)\n- github(Keras): [https://github.com/titu1994/DenseNet](https://github.com/titu1994/DenseNet)\n- github(PyTorch): [https://github.com/bamos/densenet.pytorch](https://github.com/bamos/densenet.pytorch)\n- github(PyTorch): [https://github.com/andreasveit/densenet-pytorch](https://github.com/andreasveit/densenet-pytorch)\n- github(Tensorflow): [https://github.com/ikhlestov/vision_networks](https://github.com/ikhlestov/vision_networks)\n\n**Memory-Efficient Implementation of DenseNets**\n\n- intro: Cornell University & Fudan University & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1707.06990](https://arxiv.org/abs/1707.06990)\n- github: [https://github.com/liuzhuang13/DenseNet/tree/master/models](https://github.com/liuzhuang13/DenseNet/tree/master/models)\n- github: [https://github.com/gpleiss/efficient_densenet_pytorch](https://github.com/gpleiss/efficient_densenet_pytorch)\n- github: [https://github.com/taineleau/efficient_densenet_mxnet](https://github.com/taineleau/efficient_densenet_mxnet)\n- github: [https://github.com/Tongcheng/DN_CaffeScript](https://github.com/Tongcheng/DN_CaffeScript)\n\n## DenseNet 2.0\n\n**CondenseNet: An Efficient DenseNet using Learned Group Convolutions**\n\n- arxiv: [https://arxiv.org/abs/1711.09224](https://arxiv.org/abs/1711.09224)\n- github: [https://github.com//ShichenLiu/CondenseNet](https://github.com//ShichenLiu/CondenseNet)\n\n**Multimodal Densenet**\n\n[https://arxiv.org/abs/1811.07407](https://arxiv.org/abs/1811.07407)\n\n## Xception\n\n**Deep Learning with Separable Convolutions**\n\n**Xception: Deep Learning with Depthwise Separable Convolutions**\n\n- intro: CVPR 2017. Extreme Inception\n- arxiv: [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)\n- code: [https://keras.io/applications/#xception](https://keras.io/applications/#xception)\n- github(Keras): [https://github.com/fchollet/deep-learning-models/blob/master/xception.py](https://github.com/fchollet/deep-learning-models/blob/master/xception.py)\n- github: [https://gist.github.com/culurciello/554c8e56d3bbaf7c66bf66c6089dc221](https://gist.github.com/culurciello/554c8e56d3bbaf7c66bf66c6089dc221)\n- github: [https://github.com/kwotsin/Tensorflow-Xception](https://github.com/kwotsin/Tensorflow-Xception)\n- github: [https://github.com//bruinxiong/xception.mxnet](https://github.com//bruinxiong/xception.mxnet)\n- notes: [http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1610.02357](http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1610.02357)\n\n**Towards a New Interpretation of Separable Convolutions**\n\n- arxiv: [https://arxiv.org/abs/1701.04489](https://arxiv.org/abs/1701.04489)\n\n## MobileNets\n\n**MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications**\n\n- intro: Google\n- arxiv: [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)\n- github: [https://github.com/rcmalli/keras-mobilenet](https://github.com/rcmalli/keras-mobilenet)\n- github: [https://github.com/marvis/pytorch-mobilenet](https://github.com/marvis/pytorch-mobilenet)\n- github(Tensorflow): [https://github.com/Zehaos/MobileNet](https://github.com/Zehaos/MobileNet)\n- github: [https://github.com/shicai/MobileNet-Caffe](https://github.com/shicai/MobileNet-Caffe)\n- github: [https://github.com/hollance/MobileNet-CoreML](https://github.com/hollance/MobileNet-CoreML)\n- github: [https://github.com/KeyKy/mobilenet-mxnet](https://github.com/KeyKy/mobilenet-mxnet)\n\n**MobileNets: Open-Source Models for Efficient On-Device Vision**\n\n- blog: [https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html)\n- github: [https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md)\n\n**Google’s MobileNets on the iPhone**\n\n- blog: [http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/](http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)\n- github: [https://github.com/hollance/MobileNet-CoreML](https://github.com/hollance/MobileNet-CoreML)\n\n**Depth_conv-for-mobileNet**\n\n[https://github.com//LamHoCN/Depth_conv-for-mobileNet](https://github.com//LamHoCN/Depth_conv-for-mobileNet)\n\n**The Enhanced Hybrid MobileNet**\n\n[https://arxiv.org/abs/1712.04698](https://arxiv.org/abs/1712.04698)\n\n**FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy**\n\n[https://arxiv.org/abs/1802.03750](https://arxiv.org/abs/1802.03750)\n\n**A Quantization-Friendly Separable Convolution for MobileNets**\n\n- intro: THE 1ST WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND COGNITIVE COMPUTING FOR EMBEDDED APPLICATIONS (EMC2)\n- arxiv: [https://arxiv.org/abs/1803.08607](https://arxiv.org/abs/1803.08607)\n\n## MobileNetV2\n\n**Inverted Residuals and Linear Bottlenecks: Mobile Networks forClassification, Detection and Segmentation**\n\n- intro: Google\n- keywords: MobileNetV2, SSDLite, DeepLabv3\n- arxiv: [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)\n- github: [https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)\n- github: [https://github.com/liangfu/mxnet-mobilenet-v2](https://github.com/liangfu/mxnet-mobilenet-v2)\n- blog: [https://research.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html](https://research.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html)\n\n**PydMobileNet: Improved Version of MobileNets with Pyramid Depthwise Separable Convolution**\n\n[https://arxiv.org/abs/1811.07083](https://arxiv.org/abs/1811.07083)\n\n**Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.13549](https://arxiv.org/abs/2003.13549)\n- github: [https://github.com/zeiss-microscopy/BSConv](https://github.com/zeiss-microscopy/BSConv)\n\n**Rethinking Bottleneck Structure for Efficient Mobile Network Design**\n\n- intro: ECCV 2020\n- intro: National University of Singapore & Yitu Technology\n- arxiv: [https://arxiv.org/abs/2007.02269](https://arxiv.org/abs/2007.02269)\n- github: [https://github.com/zhoudaquan/rethinking_bottleneck_design](https://github.com/zhoudaquan/rethinking_bottleneck_design)\n\n**Mobile-Former: Bridging MobileNet and Transformer**\n\n- intro: Microsoft & University of Science and Technology of China\n- arxiv: [https://arxiv.org/abs/2108.05895](https://arxiv.org/abs/2108.05895)\n\n## ShuffleNet\n\n**ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices**\n\n- intro: Megvii Inc (Face++)\n- arxiv: [https://arxiv.org/abs/1707.01083](https://arxiv.org/abs/1707.01083)\n\n## ShuffleNet V2\n\n**ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design**\n\n- intro: ECCV 2018. Megvii Inc (Face++) & Tsinghua University\n- arxiv: [https://arxiv.org/abs/1807.11164]（https://arxiv.org/abs/1807.11164\n\n## SENet\n\n**Squeeze-and-Excitation Networks**\n\n- intro: CVPR 2018\n- intro: ILSVRC 2017 image classification winner. Momenta & University of Oxford\n- arxiv: [https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)\n- github(official, Caffe): [https://github.com/hujie-frank/SENet](https://github.com/hujie-frank/SENet)\n- github: [https://github.com/bruinxiong/SENet.mxnet](https://github.com/bruinxiong/SENet.mxnet)\n\n**Competitive Inner-Imaging Squeeze and Excitation for Residual Network**\n\n- arxiv: [https://arxiv.org/abs/1807.08920](https://arxiv.org/abs/1807.08920)\n- github: [https://github.com/scut-aitcm/CompetitiveSENet](https://github.com/scut-aitcm/CompetitiveSENet)\n\n## GENet\n\n**Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks**\n\n- intro: NIPS 2018\n- github: [https://github.com/hujie-frank/GENet](https://github.com/hujie-frank/GENet)\n\n**A ConvNet for the 2020s**\n\n- intro: Facebook AI Research (FAIR) & UC Berkeley\n- arxiv: [https://arxiv.org/abs/2201.03545](https://arxiv.org/abs/2201.03545)\n- github: [https://github.com/facebookresearch/ConvNeXt](https://github.com/facebookresearch/ConvNeXt)\n\n## ImageNet Projects\n\n**Training an Object Classifier in Torch-7 on multiple GPUs over ImageNet**\n\n- intro: an imagenet example in torch\n- github: [https://github.com/soumith/imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch)\n\n# Pre-training\n\n**Exploring the Limits of Weakly Supervised Pretraining**\n\n- intro: report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5)\n- paper: [https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/](https://research.fb.com/publications/exploring-the-limits-of-weakly-supervised-pretraining/)\n\n**Rethinking ImageNet Pre-training**\n\n- intro: Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1811.08883](https://arxiv.org/abs/1811.08883)\n\n**Revisiting Pre-training: An Efficient Training Method for Image Classification**\n\n[https://arxiv.org/abs/1811.09347](https://arxiv.org/abs/1811.09347)\n\n**Rethinking Pre-training and Self-training**\n\n- intro: NeurIPS 2020\n- intro: Google Research, Brain Team\n- arxiv: [https://arxiv.org/abs/2006.06882](https://arxiv.org/abs/2006.06882)\n- github: [https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/self_training](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/self_training)\n\n**Exploring the Limits of Large Scale Pre-training**\n\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/2110.02095](https://arxiv.org/abs/2110.02095)\n\n# Transformers\n\n**Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet**\n\n- intro: National University of Singapore & YITU Technology\n- arxiv: [https://arxiv.org/abs/2101.11986](https://arxiv.org/abs/2101.11986)\n- github: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)\n\n**Incorporating Convolution Designs into Visual Transformers**\n\n- intro: SenseTime Research & Nanyang Technological University\n- arxiv: [https://arxiv.org/abs/2103.11816](https://arxiv.org/abs/2103.11816)\n\n**DeepViT: Towards Deeper Vision Transformer**\n\n- intro: National University of Singapore & ByteDance US AI Lab\n- arxiv: [https://arxiv.org/abs/2103.11886](https://arxiv.org/abs/2103.11886)\n\n**Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**\n\n- intro: ICCV 2021 best paper\n- intro: Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)\n- github: [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)\n\n**Rethinking the Design Principles of Robust Vision Transformer**\n\n- arxiv: [https://arxiv.org/abs/2105.07926](https://arxiv.org/abs/2105.07926)\n- github: [https://github.com/vtddggg/Robust-Vision-Transformer](https://github.com/vtddggg/Robust-Vision-Transformer)\n\n**Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers**\n\n- intro: Google Research & DeepMind\n- arxiv: [https://arxiv.org/abs/2109.10686](https://arxiv.org/abs/2109.10686)\n\n**How Do Vision Transformers Work?**\n\n- intro: ICLR 2022 Spotlight\n- intro: Yonsei University & NAVER AI Lab\n- arxiv: [https://arxiv.org/abs/2202.06709](https://arxiv.org/abs/2202.06709)\n- github: [https://github.com/xxxnell/how-do-vits-work](https://github.com/xxxnell/how-do-vits-work)\n\n**MulT: An End-to-End Multitask Learning Transformer**\n\n- intro: CVPR 2022\n- project page: [https://ivrl.github.io/MulT/](https://ivrl.github.io/MulT/)\n- arxiv: [https://arxiv.org/abs/2205.08303](https://arxiv.org/abs/2205.08303)\n\n**EfficientFormer: Vision Transformers at MobileNet Speed**\n\n- intro: Snap Inc. & Northeastern University\n- arxiv: [https://arxiv.org/abs/2206.01191](https://arxiv.org/abs/2206.01191)\n- github: [https://github.com/snap-research/EfficientFormer](https://github.com/snap-research/EfficientFormer)\n\n**SimA: Simple Softmax-free Attention for Vision Transformers**\n\n- intro: University of Maryland & University of California\n- arxiv: [https://arxiv.org/abs/2206.08898](https://arxiv.org/abs/2206.08898)\n- gihtub: [https://github.com/UCDvision/sima](https://github.com/UCDvision/sima)\n\n# Semi-Supervised Learning\n\n**Semi-Supervised Learning with Graphs**\n\n- intro: Label Propagation\n- paper: [http://pages.cs.wisc.edu/~jerryzhu/pub/thesis.pdf](http://pages.cs.wisc.edu/~jerryzhu/pub/thesis.pdf)\n- blog(\"标签传播算法（Label Propagation）及Python实现\"): [http://blog.csdn.net/zouxy09/article/details/49105265](http://blog.csdn.net/zouxy09/article/details/49105265)\n\n**Semi-Supervised Learning with Ladder Networks**\n\n- arxiv: [http://arxiv.org/abs/1507.02672](http://arxiv.org/abs/1507.02672)\n- github: [https://github.com/CuriousAI/ladder](https://github.com/CuriousAI/ladder)\n- github: [https://github.com/rinuboney/ladder](https://github.com/rinuboney/ladder)\n\n**Semi-supervised Feature Transfer: The Practical Benefit of Deep Learning Today?**\n\n- blog: [http://www.kdnuggets.com/2016/07/semi-supervised-feature-transfer-deep-learning.html](http://www.kdnuggets.com/2016/07/semi-supervised-feature-transfer-deep-learning.html)\n\n**Temporal Ensembling for Semi-Supervised Learning**\n\n- intro: ICLR 2017\n- arxiv: [https://arxiv.org/abs/1610.02242](https://arxiv.org/abs/1610.02242)\n- github: [https://github.com/smlaine2/tempens](https://github.com/smlaine2/tempens)\n\n**Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data**\n\n- intro: ICLR 2017 best paper award\n- arxiv: [https://arxiv.org/abs/1610.05755](https://arxiv.org/abs/1610.05755)\n- github: [https://github.com/tensorflow/models/tree/8505222ea1f26692df05e65e35824c6c71929bb5/privacy](https://github.com/tensorflow/models/tree/8505222ea1f26692df05e65e35824c6c71929bb5/privacy)\n\n**Infinite Variational Autoencoder for Semi-Supervised Learning**\n\n- arxiv: [https://arxiv.org/abs/1611.07800](https://arxiv.org/abs/1611.07800)\n\n# Multi-label Learning\n\n**CNN: Single-label to Multi-label**\n\n- arxiv: [http://arxiv.org/abs/1406.5726](http://arxiv.org/abs/1406.5726)\n\n**Deep Learning for Multi-label Classification**\n\n- arxiv: [http://arxiv.org/abs/1502.05988](http://arxiv.org/abs/1502.05988)\n- github: [http://meka.sourceforge.net](http://meka.sourceforge.net)\n\n**Predicting Unseen Labels using Label Hierarchies in Large-Scale Multi-label Learning**\n\n- intro: ECML 2015\n- paper: [https://www.kdsl.tu-darmstadt.de/fileadmin/user_upload/Group_KDSL/PUnL_ECML2015_camera_ready.pdf](https://www.kdsl.tu-darmstadt.de/fileadmin/user_upload/Group_KDSL/PUnL_ECML2015_camera_ready.pdf)\n\n**Learning with a Wasserstein Loss**\n\n- project page: [http://cbcl.mit.edu/wasserstein/](http://cbcl.mit.edu/wasserstein/)\n- arxiv: [http://arxiv.org/abs/1506.05439](http://arxiv.org/abs/1506.05439)\n- code: [http://cbcl.mit.edu/wasserstein/yfcc100m_labels.tar.gz](http://cbcl.mit.edu/wasserstein/yfcc100m_labels.tar.gz)\n- MIT news: [http://news.mit.edu/2015/more-flexible-machine-learning-1001](http://news.mit.edu/2015/more-flexible-machine-learning-1001)\n\n**From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification**\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1602.02068](http://arxiv.org/abs/1602.02068)\n- github: [https://github.com/gokceneraslan/SparseMax.torch](https://github.com/gokceneraslan/SparseMax.torch)\n- github: [https://github.com/Unbabel/sparsemax](https://github.com/Unbabel/sparsemax)\n\n**CNN-RNN: A Unified Framework for Multi-label Image Classification**\n\n- arxiv: [http://arxiv.org/abs/1604.04573](http://arxiv.org/abs/1604.04573)\n\n**Improving Multi-label Learning with Missing Labels by Structured Semantic Correlations**\n\n- arxiv: [http://arxiv.org/abs/1608.01441](http://arxiv.org/abs/1608.01441)\n\n**Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking & Other Missing Label Applications**\n\n- intro: Indian Institute of Technology Delhi & MSR\n- paper: [https://manikvarma.github.io/pubs/jain16.pdf](https://manikvarma.github.io/pubs/jain16.pdf)\n\n**Multi-Label Image Classification with Regional Latent Semantic Dependencies**\n\n- intro: Regional Latent Semantic Dependencies model (RLSD), RNN, RPN\n- arxiv: [https://arxiv.org/abs/1612.01082](https://arxiv.org/abs/1612.01082)\n\n**Privileged Multi-label Learning**\n\n- intro: Peking University & University of Technology Sydney & University of Sydney\n- arxiv: [https://arxiv.org/abs/1701.07194](https://arxiv.org/abs/1701.07194)\n\n# Multi-task Learning\n\n**Multitask Learning / Domain Adaptation**\n\n- homepage: [http://www.cs.cornell.edu/~kilian/research/multitasklearning/multitasklearning.html](http://www.cs.cornell.edu/~kilian/research/multitasklearning/multitasklearning.html)\n\n**multi-task learning**\n\n- discussion: [https://github.com/memect/hao/issues/93](https://github.com/memect/hao/issues/93)\n\n**Learning and Transferring Multi-task Deep Representation for Face Alignment**\n\n- arxiv: [http://arxiv.org/abs/1408.3967](http://arxiv.org/abs/1408.3967)\n\n**Multi-task learning of facial landmarks and expression**\n\n- paper: [http://www.uoguelph.ca/~gwtaylor/publications/gwtaylor_crv2014.pdf](http://www.uoguelph.ca/~gwtaylor/publications/gwtaylor_crv2014.pdf)\n\n**Multi-Task Deep Visual-Semantic Embedding for Video Thumbnail Selection**\n\n- intro:  CVPR 2015\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Multi-Task_Deep_Visual-Semantic_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Multi-Task_Deep_Visual-Semantic_2015_CVPR_paper.pdf)\n\n**Learning Multiple Tasks with Deep Relationship Networks**\n\n- arxiv: [https://arxiv.org/abs/1506.02117](https://arxiv.org/abs/1506.02117)\n\n**Learning deep representation of multityped objects and tasks**\n\n- arxiv: [http://arxiv.org/abs/1603.01359](http://arxiv.org/abs/1603.01359)\n\n**Cross-stitch Networks for Multi-task Learning**\n\n- arxiv: [http://arxiv.org/abs/1604.03539](http://arxiv.org/abs/1604.03539)\n\n**Multi-Task Learning in Tensorflow (Part 1)**\n\n- blog: [https://jg8610.github.io/Multi-Task/](https://jg8610.github.io/Multi-Task/)\n\n**Deep Multi-Task Learning with Shared Memory**\n\n- intro: EMNLP 2016\n- arxiv: [http://arxiv.org/abs/1609.07222](http://arxiv.org/abs/1609.07222)\n\n**Learning to Push by Grasping: Using multiple tasks for effective learning**\n\n- arxiv: [http://arxiv.org/abs/1609.09025](http://arxiv.org/abs/1609.09025)\n\n**Identifying beneficial task relations for multi-task learning in deep neural networks**\n\n- intro: EACL 2017\n- arxiv: [https://arxiv.org/abs/1702.08303](https://arxiv.org/abs/1702.08303)\n- github: [https://github.com/jbingel/eacl2017_mtl](https://github.com/jbingel/eacl2017_mtl)\n\n**Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics**\n\n- intro: University of Cambridge\n- arxiv: [https://arxiv.org/abs/1705.07115](https://arxiv.org/abs/1705.07115)\n\n**One Model To Learn Them All**\n\n- intro: Google Brain & University of Toronto\n- arxiv: [https://arxiv.org/abs/1706.05137](https://arxiv.org/abs/1706.05137)\n- github: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n\n**MultiModel: Multi-Task Machine Learning Across Domains**\n\n[https://research.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html](https://research.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html)\n\n**An Overview of Multi-Task Learning in Deep Neural Networks**\n\n- intro: Aylien Ltd\n- arxiv: [https://arxiv.org/abs/1706.05098](https://arxiv.org/abs/1706.05098)\n\n**PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning**\n\n- arxiv: [https://arxiv.org/abs/1711.05769](https://arxiv.org/abs/1711.05769)\n- github: [https://github.com/arunmallya/packnet](https://github.com/arunmallya/packnet)\n\n**End-to-End Multi-Task Learning with Attention**\n\n- intro: Imperial College London\n- arxiv: [https://arxiv.org/abs/1803.10704](https://arxiv.org/abs/1803.10704)\n\n**Cross-connected Networks for Multi-task Learning of Detection and Segmentation**\n\n[https://arxiv.org/abs/1805.05569](https://arxiv.org/abs/1805.05569)\n\n**Auxiliary Tasks in Multi-task Learning**\n\n[https://arxiv.org/abs/1805.06334](https://arxiv.org/abs/1805.06334)\n\n**K For The Price Of 1: Parameter Efficient Multi-task And Transfer Learning**\n\n- intro: The University of Chicago & Google\n- arxiv: [https://arxiv.org/abs/1810.10703](https://arxiv.org/abs/1810.10703)\n\n**Which Tasks Should Be Learned Together in Multi-task Learning?**\n\n- intro: ICML 2020\n- intro: Stanford\n- project page: [http://taskgrouping.stanford.edu/](http://taskgrouping.stanford.edu/)\n- arxiv: [https://arxiv.org/abs/1905.07553](https://arxiv.org/abs/1905.07553)\n\n**OmniNet: A unified architecture for multi-modal multi-task learning**\n\n- arxiv: [https://arxiv.org/abs/1907.07804](https://arxiv.org/abs/1907.07804)\n- github: [https://github.com/subho406/OmniNet](https://github.com/subho406/OmniNet)\n\n**Deep Elastic Networks with Model Selection for Multi-Task Learning**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.04860](https://arxiv.org/abs/1909.04860)\n\n**AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning**\n\n- intro: Boston University & IBM Research & MIT-IBM Watson AI Lab\n- arxiv: [https://arxiv.org/abs/1911.12423](https://arxiv.org/abs/1911.12423)\n\n**Multi-Task Learning for Dense Prediction Tasks: A Survey**\n\n- intro: T-PAMI\n- arxiv: [https://arxiv.org/abs/2004.13379](https://arxiv.org/abs/2004.13379)\n- github: [https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch](https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch)\n\n**MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning**\n\n- intro: ECCV 2020 spotlight\n- keywords: MTI-Net\n- arxiv: [https://arxiv.org/abs/2001.06902](https://arxiv.org/abs/2001.06902)\n- github: [https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch](https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch)\n\n**Exploring Relational Context for Multi-Task Dense Prediction**\n\n- intro: ETH Zurich\n- arxiv: [https://arxiv.org/abs/2104.13874](https://arxiv.org/abs/2104.13874)\n\n**Cross-task Attention Mechanism for Dense Multi-task Learning**\n\n- intro: Inria, France & Valeo.ai, France\n- arxiv: [https://arxiv.org/abs/2206.08927](https://arxiv.org/abs/2206.08927)\n- github: [https://github.com/cv-rits/DenseMTL](https://github.com/cv-rits/DenseMTL)\n\n# Multi-modal Learning\n\n**Multimodal Deep Learning**\n\n- paper: [http://ai.stanford.edu/~ang/papers/nipsdlufl10-MultimodalDeepLearning.pdf](http://ai.stanford.edu/~ang/papers/nipsdlufl10-MultimodalDeepLearning.pdf)\n\n**Multimodal Convolutional Neural Networks for Matching Image and Sentence**\n\n- homepage: [http://mcnn.noahlab.com.hk/project.html](http://mcnn.noahlab.com.hk/project.html)\n- paper: [http://mcnn.noahlab.com.hk/ICCV2015.pdf](http://mcnn.noahlab.com.hk/ICCV2015.pdf)\n- arxiv: [http://arxiv.org/abs/1504.06063](http://arxiv.org/abs/1504.06063)\n\n**A C++ library for Multimodal Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1512.06927](http://arxiv.org/abs/1512.06927)\n- github: [https://github.com/Jian-23/Deep-Learning-Library](https://github.com/Jian-23/Deep-Learning-Library)\n\n**Multimodal Learning for Image Captioning and Visual Question Answering**\n\n- slides: [http://research.microsoft.com/pubs/264769/UCB_XiaodongHe.6.pdf](http://research.microsoft.com/pubs/264769/UCB_XiaodongHe.6.pdf)\n\n**Multi modal retrieval and generation with deep distributed models**\n\n- slides: [http://www.slideshare.net/roelofp/multi-modal-retrieval-and-generation-with-deep-distributed-models](http://www.slideshare.net/roelofp/multi-modal-retrieval-and-generation-with-deep-distributed-models)\n- slides: [http://pan.baidu.com/s/1kUSjn4z](http://pan.baidu.com/s/1kUSjn4z)\n\n**Learning Aligned Cross-Modal Representations from Weakly Aligned Data**\n\n![](http://projects.csail.mit.edu/cmplaces/imgs/teaser.png)\n\n- homepage: [http://projects.csail.mit.edu/cmplaces/index.html](http://projects.csail.mit.edu/cmplaces/index.html)\n- paper: [http://projects.csail.mit.edu/cmplaces/content/paper.pdf](http://projects.csail.mit.edu/cmplaces/content/paper.pdf)\n\n**Variational methods for Conditional Multimodal Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1603.01801](http://arxiv.org/abs/1603.01801)\n\n**Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images**\n\n- intro: NIPS 2016. University of California & Pinterest\n- project page: [http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html](http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html)\n- arxiv: [https://arxiv.org/abs/1611.08321](https://arxiv.org/abs/1611.08321)\n\n**Deep Multi-Modal Image Correspondence Learning**\n\n- arxiv: [https://arxiv.org/abs/1612.01225](https://arxiv.org/abs/1612.01225)\n\n**Multimodal Deep Learning (D4L4 Deep Learning for Speech and Language UPC 2017)**\n\n- slides: [http://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017](http://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017)\n\n**Multimodal Learning with Transformers: A Survey**\n\n- intro: University of Oxford & University of Surrey\n- arxiv: [https://arxiv.org/abs/2206.06488](https://arxiv.org/abs/2206.06488)\n\n# Debugging Deep Learning\n\n**Some tips for debugging deep learning**\n\n- blog: [http://www.lab41.org/some-tips-for-debugging-in-deep-learning-2/](http://www.lab41.org/some-tips-for-debugging-in-deep-learning-2/)\n\n**Introduction to debugging neural networks**\n\n- blog: [http://russellsstewart.com/notes/0.html](http://russellsstewart.com/notes/0.html)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/4du7gv/introduction_to_debugging_neural_networks](https://www.reddit.com/r/MachineLearning/comments/4du7gv/introduction_to_debugging_neural_networks)\n\n**How to Visualize, Monitor and Debug Neural Network Learning**\n\n- blog: [http://deeplearning4j.org/visualization](http://deeplearning4j.org/visualization)\n\n**Learning from learning curves**\n\n- intro: Kaggle\n- blog: [https://medium.com/@dsouza.amanda/learning-from-learning-curves-1a82c6f98f49#.o5synrvvl](https://medium.com/@dsouza.amanda/learning-from-learning-curves-1a82c6f98f49#.o5synrvvl)\n\n# Understanding CNN\n\n**Understanding the Effective Receptive Field in Deep Convolutional Neural Networks**\n\n- intro: NIPS 2016\n- paper: [http://www.cs.toronto.edu/~wenjie/papers/nips16/top.pdf](http://www.cs.toronto.edu/~wenjie/papers/nips16/top.pdf)\n\n# Deep Learning Networks\n\n**PCANet: A Simple Deep Learning Baseline for Image Classification?**\n\n- arixv: [http://arxiv.org/abs/1404.3606](http://arxiv.org/abs/1404.3606)\n- code(Matlab): [http://mx.nthu.edu.tw/~tsunghan/download/PCANet_demo_pyramid.rar](http://mx.nthu.edu.tw/~tsunghan/download/PCANet_demo_pyramid.rar)\n- mirror: [http://pan.baidu.com/s/1mg24b3a](http://pan.baidu.com/s/1mg24b3a)\n- github(C++): [https://github.com/Ldpe2G/PCANet](https://github.com/Ldpe2G/PCANet)\n- github(Python): [https://github.com/IshitaTakeshi/PCANet](https://github.com/IshitaTakeshi/PCANet)\n\n**Convolutional Kernel Networks**\n\n- intro: NIPS 2014\n- arxiv: [http://arxiv.org/abs/1406.3332](http://arxiv.org/abs/1406.3332)\n\n**Deeply-supervised Nets**\n\n- intro: DSN\n- arxiv: [http://arxiv.org/abs/1409.5185](http://arxiv.org/abs/1409.5185)\n- homepage: [http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/](http://vcl.ucsd.edu/~sxie/2014/09/12/dsn-project/)\n- github: [https://github.com/s9xie/DSN](https://github.com/s9xie/DSN)\n- notes: [http://zhangliliang.com/2014/11/02/paper-note-dsn/](http://zhangliliang.com/2014/11/02/paper-note-dsn/)\n\n**FitNets: Hints for Thin Deep Nets**\n\n- arxiv: [https://arxiv.org/abs/1412.6550](https://arxiv.org/abs/1412.6550)\n- github: [https://github.com/adri-romsor/FitNets](https://github.com/adri-romsor/FitNets)\n\n**Striving for Simplicity: The All Convolutional Net**\n\n- intro: ICLR-2015 workshop\n- arxiv: [http://arxiv.org/abs/1412.6806](http://arxiv.org/abs/1412.6806)\n\n**How these researchers tried something unconventional to come out with a smaller yet better Image Recognition.**\n\n- intro: All Convolutional Network: (https://arxiv.org/abs/1412.6806#) implementation in Keras\n- blog: [https://medium.com/@matelabs_ai/how-these-researchers-tried-something-unconventional-to-came-out-with-a-smaller-yet-better-image-544327f30e72#.pfdbvdmuh](https://medium.com/@matelabs_ai/how-these-researchers-tried-something-unconventional-to-came-out-with-a-smaller-yet-better-image-544327f30e72#.pfdbvdmuh)\n- blog: [https://github.com/MateLabs/All-Conv-Keras](https://github.com/MateLabs/All-Conv-Keras)\n\n**Pointer Networks**\n\n- arxiv: [https://arxiv.org/abs/1506.03134](https://arxiv.org/abs/1506.03134)\n- github: [https://github.com/vshallc/PtrNets](https://github.com/vshallc/PtrNets)\n- github(TensorFlow): [https://github.com/ikostrikov/TensorFlow-Pointer-Networks](https://github.com/ikostrikov/TensorFlow-Pointer-Networks)\n- github(TensorFlow): [https://github.com/devsisters/pointer-network-tensorflow](https://github.com/devsisters/pointer-network-tensorflow)\n- notes: [https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/pointer-networks.md](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/pointer-networks.md)\n\n**Pointer Networks in TensorFlow (with sample code)**\n\n- blog: [https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264#.sxipqfj30](https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264#.sxipqfj30)\n- github: [https://github.com/devnag/tensorflow-pointer-networks](https://github.com/devnag/tensorflow-pointer-networks)\n\n**Rectified Factor Networks**\n\n- arxiv: [http://arxiv.org/abs/1502.06464](http://arxiv.org/abs/1502.06464)\n- github: [https://github.com/untom/librfn](https://github.com/untom/librfn)\n\n**Correlational Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1504.07225](http://arxiv.org/abs/1504.07225)\n- github: [https://github.com/apsarath/CorrNet](https://github.com/apsarath/CorrNet)\n\n**Diversity Networks**\n\n- arxiv: [http://arxiv.org/abs/1511.05077](http://arxiv.org/abs/1511.05077)\n\n**Competitive Multi-scale Convolution**\n\n- arxiv: [http://arxiv.org/abs/1511.05635](http://arxiv.org/abs/1511.05635)\n- blog: [https://zhuanlan.zhihu.com/p/22377389](https://zhuanlan.zhihu.com/p/22377389)\n\n**A Unified Approach for Learning the Parameters of Sum-Product Networks (SPN)**\n\n- intro: \"The Sum-Product Network (SPN) is a new type of machine learning model \nwith fast exact probabilistic inference over many layers.\"\n- arxiv: [http://arxiv.org/abs/1601.00318](http://arxiv.org/abs/1601.00318)\n- homepage: [http://spn.cs.washington.edu/index.shtml](http://spn.cs.washington.edu/index.shtml)\n- code: [http://spn.cs.washington.edu/code.shtml](http://spn.cs.washington.edu/code.shtml)\n\n**Awesome Sum-Product Networks**\n\n- github: [https://github.com/arranger1044/awesome-spn](https://github.com/arranger1044/awesome-spn)\n\n**Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1511.07356](http://arxiv.org/abs/1511.07356)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Honari_Recombinator_Networks_Learning_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Honari_Recombinator_Networks_Learning_CVPR_2016_paper.pdf)\n- github: [https://github.com/SinaHonari/RCN](https://github.com/SinaHonari/RCN)\n\n**Dynamic Capacity Networks**\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1511.07838](http://arxiv.org/abs/1511.07838)\n- github(Tensorflow): [https://github.com/beopst/dcn.tf](https://github.com/beopst/dcn.tf)\n- review: [http://www.erogol.com/1314-2/](http://www.erogol.com/1314-2/)\n\n**Bitwise Neural Networks**\n\n- paper: [http://paris.cs.illinois.edu/pubs/minje-icmlw2015.pdf](http://paris.cs.illinois.edu/pubs/minje-icmlw2015.pdf)\n- demo: [http://minjekim.com/demo_bnn.html](http://minjekim.com/demo_bnn.html)\n\n**Learning Discriminative Features via Label Consistent Neural Network**\n\n- arxiv: [http://arxiv.org/abs/1602.01168](http://arxiv.org/abs/1602.01168)\n\n**A Theory of Generative ConvNet**\n\n- project page: [http://www.stat.ucla.edu/~ywu/GenerativeConvNet/main.html](http://www.stat.ucla.edu/~ywu/GenerativeConvNet/main.html)\n- arxiv: [http://arxiv.org/abs/1602.03264](http://arxiv.org/abs/1602.03264)\n- code: [http://www.stat.ucla.edu/~ywu/GenerativeConvNet/doc/code.zip](http://www.stat.ucla.edu/~ywu/GenerativeConvNet/doc/code.zip)\n\n**How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks**\n\n- arxiv: [http://arxiv.org/abs/1602.02282](http://arxiv.org/abs/1602.02282)\n\n**Group Equivariant Convolutional Networks (G-CNNs)**\n\n- arxiv: [http://arxiv.org/abs/1602.07576](http://arxiv.org/abs/1602.07576)\n\n**Deep Spiking Networks**\n\n- arxiv: [http://arxiv.org/abs/1602.08323](http://arxiv.org/abs/1602.08323)\n- github: [https://github.com/petered/spiking-mlp](https://github.com/petered/spiking-mlp)\n\n**Low-rank passthrough neural networks**\n\n- arxiv: [http://arxiv.org/abs/1603.03116](http://arxiv.org/abs/1603.03116)\n- github: [https://github.com/Avmb/lowrank-gru](https://github.com/Avmb/lowrank-gru)\n\n**Single Image 3D Interpreter Network**\n\n- intro: ECCV 2016 (oral)\n- arxiv: [https://arxiv.org/abs/1604.08685](https://arxiv.org/abs/1604.08685)\n\n**Deeply-Fused Nets**\n\n- arxiv: [http://arxiv.org/abs/1605.07716](http://arxiv.org/abs/1605.07716)\n\n**SNN: Stacked Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1605.08512](http://arxiv.org/abs/1605.08512)\n\n**Universal Correspondence Network**\n\n- intro: NIPS 2016 full oral presentation. Stanford University & NEC Laboratories America\n- project page: [http://cvgl.stanford.edu/projects/ucn/](http://cvgl.stanford.edu/projects/ucn/)\n- arxiv: [https://arxiv.org/abs/1606.03558](https://arxiv.org/abs/1606.03558)\n\n**Progressive Neural Networks**\n\n- intro: Google DeepMind\n- arxiv: [https://arxiv.org/abs/1606.04671](https://arxiv.org/abs/1606.04671)\n- github: [https://github.com/synpon/prog_nn](https://github.com/synpon/prog_nn)\n- github: [https://github.com/yao62995/A3C](https://github.com/yao62995/A3C)\n\n**Holistic SparseCNN: Forging the Trident of Accuracy, Speed, and Size**\n\n- arxiv: [http://arxiv.org/abs/1608.01409](http://arxiv.org/abs/1608.01409)\n\n**Mollifying Networks**\n\n- author: Caglar Gulcehre, Marcin Moczulski, Francesco Visin, Yoshua Bengio\n- arxiv: [http://arxiv.org/abs/1608.04980](http://arxiv.org/abs/1608.04980)\n\n**Domain Separation Networks**\n\n- intro: NIPS 2016\n- intro: Google Brain & Imperial College London & Google Research\n- arxiv: [https://arxiv.org/abs/1608.06019](https://arxiv.org/abs/1608.06019)\n- github: [https://github.com/tensorflow/models/tree/master/domain_adaptation](https://github.com/tensorflow/models/tree/master/domain_adaptation)\n\n**Local Binary Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1608.06049](http://arxiv.org/abs/1608.06049)\n\n**CliqueCNN: Deep Unsupervised Exemplar Learning**\n\n- intro: NIPS 2016\n- arxiv: [http://arxiv.org/abs/1608.08792](http://arxiv.org/abs/1608.08792)\n- github: [https://github.com/asanakoy/cliquecnn](https://github.com/asanakoy/cliquecnn)\n\n**Convexified Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1609.01000](http://arxiv.org/abs/1609.01000)\n\n**Multi-scale brain networks**\n\n- arxiv: [http://arxiv.org/abs/1608.08828](http://arxiv.org/abs/1608.08828)\n\n[https://arxiv.org/abs/1711.11473](https://arxiv.org/abs/1711.11473)\n\n**Input Convex Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1609.07152](http://arxiv.org/abs/1609.07152)\n- github: [https://github.com/locuslab/icnn](https://github.com/locuslab/icnn)\n\n**HyperNetworks**\n\n- arxiv: [https://arxiv.org/abs/1609.09106](https://arxiv.org/abs/1609.09106)\n- blog: [http://blog.otoro.net/2016/09/28/hyper-networks/](http://blog.otoro.net/2016/09/28/hyper-networks/)\n- github: [https://github.com/hardmaru/supercell/blob/master/assets/MNIST_Static_HyperNetwork_Example.ipynb](https://github.com/hardmaru/supercell/blob/master/assets/MNIST_Static_HyperNetwork_Example.ipynb)\n\n**HyperLSTM**\n\n- github: [https://github.com/hardmaru/supercell/blob/master/supercell.py](https://github.com/hardmaru/supercell/blob/master/supercell.py)\n\n**X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets**\n\n- arxiv: [https://arxiv.org/abs/1610.00163](https://arxiv.org/abs/1610.00163)\n\n**Tensor Switching Networks**\n\n- intro: NIPS 2016\n- arixiv: [https://arxiv.org/abs/1610.10087](https://arxiv.org/abs/1610.10087)\n- github: [https://github.com/coxlab/tsnet](https://github.com/coxlab/tsnet)\n\n**BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks**\n\n- intro: Harvard University\n- paper: [http://www.eecs.harvard.edu/~htk/publication/2016-icpr-teerapittayanon-mcdanel-kung.pdf](http://www.eecs.harvard.edu/~htk/publication/2016-icpr-teerapittayanon-mcdanel-kung.pdf)\n- github: [https://github.com/kunglab/branchynet](https://github.com/kunglab/branchynet)\n\n**Spectral Convolution Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.05378](https://arxiv.org/abs/1611.05378)\n\n**DelugeNets: Deep Networks with Massive and Flexible Cross-layer Information Inflows**\n\n- arxiv: [https://arxiv.org/abs/1611.05552](https://arxiv.org/abs/1611.05552)\n- github: [https://github.com/xternalz/DelugeNets](https://github.com/xternalz/DelugeNets)\n\n**PolyNet: A Pursuit of Structural Diversity in Very Deep Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.05725](https://arxiv.org/abs/1611.05725)\n- poster: [http://mmlab.ie.cuhk.edu.hk/projects/cu_deeplink/polynet_poster.pdf](http://mmlab.ie.cuhk.edu.hk/projects/cu_deeplink/polynet_poster.pdf)\n\n**Weakly Supervised Cascaded Convolutional Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.08258](https://arxiv.org/abs/1611.08258)\n\n**DeepSetNet: Predicting Sets with Deep Neural Networks**\n\n- intro: multi-class image classification and pedestrian detection\n- arxiv: [https://arxiv.org/abs/1611.08998](https://arxiv.org/abs/1611.08998)\n\n**Steerable CNNs**\n\n- intro: University of Amsterdam\n- arxiv: [https://arxiv.org/abs/1612.08498](https://arxiv.org/abs/1612.08498)\n\n**Feedback Networks**\n\n- project page: [http://feedbacknet.stanford.edu/](http://feedbacknet.stanford.edu/)\n- arxiv: [https://arxiv.org/abs/1612.09508](https://arxiv.org/abs/1612.09508)\n- youtube: [https://youtu.be/MY5Uhv38Ttg](https://youtu.be/MY5Uhv38Ttg)\n\n**Oriented Response Networks**\n\n- arxiv: [https://arxiv.org/abs/1701.01833](https://arxiv.org/abs/1701.01833)\n\n**OptNet: Differentiable Optimization as a Layer in Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1703.00443](https://arxiv.org/abs/1703.00443)\n- github: [https://github.com/locuslab/optnet](https://github.com/locuslab/optnet)\n\n**A fast and differentiable QP solver for PyTorch**\n\n- github: [https://github.com/locuslab/qpth](https://github.com/locuslab/qpth)\n\n**Meta Networks**\n\n[https://arxiv.org/abs/1703.00837](https://arxiv.org/abs/1703.00837)\n\n**Deformable Convolutional Networks**\n\n- intro: ICCV 2017 oral. Microsoft Research Asia\n- keywords: deformable convolution, deformable RoI pooling\n- arxiv: [https://arxiv.org/abs/1703.06211](https://arxiv.org/abs/1703.06211)\n- sliedes: [http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf](http://www.jifengdai.org/slides/Deformable_Convolutional_Networks_Oral.pdf)\n- github(official): [https://github.com/msracver/Deformable-ConvNets](https://github.com/msracver/Deformable-ConvNets)\n- github: [https://github.com/felixlaumon/deform-conv](https://github.com/felixlaumon/deform-conv)\n- github: [https://github.com/oeway/pytorch-deform-conv](https://github.com/oeway/pytorch-deform-conv)\n\nDeformable ConvNets v2: More Deformable, Better Results**\n\n- intro: University of Science and Technology of China & Microsoft Research Asia\n- keywords: DCNv2\n- arxiv: [https://arxiv.org/abs/1811.11168](https://arxiv.org/abs/1811.11168)\n- github: [https://github.com/msracver/Deformable-ConvNets/tree/master/DCNv2_op](https://github.com/msracver/Deformable-ConvNets/tree/master/DCNv2_op)\n\n**Second-order Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1703.06817](https://arxiv.org/abs/1703.06817)\n\n**Gabor Convolutional Networks**\n\n[https://arxiv.org/abs/1705.01450](https://arxiv.org/abs/1705.01450)\n\n**Deep Rotation Equivariant Network**\n\n[https://arxiv.org/abs/1705.08623](https://arxiv.org/abs/1705.08623)\n\n**Dense Transformer Networks**\n\n- intro: Washington State University & University of California, Davis\n- arxiv: [https://arxiv.org/abs/1705.08881](https://arxiv.org/abs/1705.08881)\n- github: [https://github.com/divelab/dtn](https://github.com/divelab/dtn)\n\n**Deep Complex Networks**\n\n- intro: [Université de Montréal & INRS-EMT & Microsoft Maluuba\n- arxiv: [https://arxiv.org/abs/1705.09792](https://arxiv.org/abs/1705.09792)\n- github: [https://github.com/ChihebTrabelsi/deep_complex_networks](https://github.com/ChihebTrabelsi/deep_complex_networks)\n\n**Deep Quaternion Networks**\n\n- intro: University of Louisiana\n- arxiv: [https://arxiv.org/abs/1712.04604](https://arxiv.org/abs/1712.04604)\n\n**DiracNets: Training Very Deep Neural Networks Without Skip-Connections**\n\n- intro: Université Paris-Est\n- arxiv: [https://arxiv.org/abs/1706.00388](https://arxiv.org/abs/1706.00388)\n- github: [https://github.com/szagoruyko/diracnets](https://github.com/szagoruyko/diracnets)\n\n**Dual Path Networks**\n\n- intro: National University of Singapore\n- arxiv: [https://arxiv.org/abs/1707.01629](https://arxiv.org/abs/1707.01629)\n- github(MXNet): [https://github.com/cypw/DPNs](https://github.com/cypw/DPNs)\n\n**Primal-Dual Group Convolutions for Deep Neural Networks**\n\n**Interleaved Group Convolutions for Deep Neural Networks**\n\n- intro: ICCV 2017\n- keywords: interleaved group convolutional neural networks (IGCNets), IGCV1\n- arxiv: [https://arxiv.org/abs/1707.02725](https://arxiv.org/abs/1707.02725)\n- gihtub: [https://github.com/hellozting/InterleavedGroupConvolutions](https://github.com/hellozting/InterleavedGroupConvolutions)\n\n**IGCV2: Interleaved Structured Sparse Convolutional Neural Networks**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.06202](https://arxiv.org/abs/1804.06202)\n\n**IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks**\n\n- intro: University of Scinence and Technology of China & Microsoft Reserach Asia\n- arxiv: [https://arxiv.org/abs/1806.00178](https://arxiv.org/abs/1806.00178)\n- github(official): [https://github.com/homles11/IGCV3](https://github.com/homles11/IGCV3)\n\n**Sensor Transformation Attention Networks**\n\n[https://arxiv.org/abs/1708.01015](https://arxiv.org/abs/1708.01015)\n\n**Sparsity Invariant CNNs**\n\n[https://arxiv.org/abs/1708.06500](https://arxiv.org/abs/1708.06500)\n\n**SPARCNN: SPAtially Related Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1708.07522](https://arxiv.org/abs/1708.07522)\n\n**BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks**\n\n[https://arxiv.org/abs/1709.01686](https://arxiv.org/abs/1709.01686)\n\n**Polar Transformer Networks**\n\n[https://arxiv.org/abs/1709.01889](https://arxiv.org/abs/1709.01889)\n\n**Tensor Product Generation Networks**\n\n[https://arxiv.org/abs/1709.09118](https://arxiv.org/abs/1709.09118)\n\n**Deep Competitive Pathway Networks**\n\n- intro: ACML 2017\n- arxiv: [https://arxiv.org/abs/1709.10282](https://arxiv.org/abs/1709.10282)\n- github: [https://github.com/JiaRenChang/CoPaNet](https://github.com/JiaRenChang/CoPaNet)\n\n**Context Embedding Networks**\n\n[https://arxiv.org/abs/1710.01691](https://arxiv.org/abs/1710.01691)\n\n**Generalization in Deep Learning**\n\n- intro: MIT & University of Montreal\n- arxiv: [https://arxiv.org/abs/1710.05468](https://arxiv.org/abs/1710.05468)\n\n**Understanding Deep Learning Generalization by Maximum Entropy**\n\n- intro: University of Science and Technology of China & Beijing Jiaotong University & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1711.07758](https://arxiv.org/abs/1711.07758)\n\n**Do Convolutional Neural Networks Learn Class Hierarchy?**\n\n- intro: Bosch Research North America & Michigan State University\n- arxiv: [https://arxiv.org/abs/1710.06501](https://arxiv.org/abs/1710.06501)\n- video demo: [https://vimeo.com/228263798](https://vimeo.com/228263798)\n\n**Deep Hyperspherical Learning**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1711.03189](https://arxiv.org/abs/1711.03189)\n\n**Beyond Sparsity: Tree Regularization of Deep Models for Interpretability**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1711.06178](https://arxiv.org/abs/1711.06178)\n\n**Neural Motifs: Scene Graph Parsing with Global Context**\n\n- keywords: Stacked Motif Networks\n- arxiv: [https://arxiv.org/abs/1711.06640](https://arxiv.org/abs/1711.06640)\n\n**Priming Neural Networks**\n\n[https://arxiv.org/abs/1711.05918](https://arxiv.org/abs/1711.05918)\n\n**Three Factors Influencing Minima in SGD**\n\n[https://arxiv.org/abs/1711.04623](https://arxiv.org/abs/1711.04623)\n\n**BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning**\n\n[https://arxiv.org/abs/1711.06959](https://arxiv.org/abs/1711.06959)\n\n**BlockDrop: Dynamic Inference Paths in Residual Networks**\n\n- intro: UMD & UT Austin & IBM Research & Fusemachines Inc.\n- arxiv: [https://arxiv.org/abs/1711.08393](https://arxiv.org/abs/1711.08393)\n\n**Wasserstein Introspective Neural Networks**\n\n[https://arxiv.org/abs/1711.08875](https://arxiv.org/abs/1711.08875)\n\n**SkipNet: Learning Dynamic Routing in Convolutional Networks**\n\n[https://arxiv.org/abs/1711.09485](https://arxiv.org/abs/1711.09485)\n\n**Do Convolutional Neural Networks act as Compositional Nearest Neighbors?**\n\n- intro: CMU & West Virginia University\n- arxiv: [https://arxiv.org/abs/1711.10683](https://arxiv.org/abs/1711.10683)\n\n**ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism**\n\n- intro: Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1711.11443](https://arxiv.org/abs/1711.11443)\n\n**Broadcasting Convolutional Network**\n\n[https://arxiv.org/abs/1712.02517](https://arxiv.org/abs/1712.02517)\n\n**Point-wise Convolutional Neural Network**\n\n- intro: Singapore University of Technology and Design\n- arxiv: [https://arxiv.org/abs/1712.05245](https://arxiv.org/abs/1712.05245)\n\n**ScreenerNet: Learning Curriculum for Neural Networks**\n\n- intro: Intel Corporation & Allen Institute for Artificial Intelligence\n- keywords: curricular learning, deep learning, deep q-learning\n- arxiv: [https://arxiv.org/abs/1801.00904](https://arxiv.org/abs/1801.00904)\n\n**Sparsely Connected Convolutional Networks**\n\n[https://arxiv.org/abs/1801.05895](https://arxiv.org/abs/1801.05895)\n\n**Spherical CNNs**\n\n- intro: ICLR 2018 best paper award. University of Amsterdam & EPFL\n- arxiv: [https://arxiv.org/abs/1801.10130](https://arxiv.org/abs/1801.10130)\n- github(official, PyTorch): [https://github.com/jonas-koehler/s2cnn](https://github.com/jonas-koehler/s2cnn)\n\n**Going Deeper in Spiking Neural Networks: VGG and Residual Architectures**\n\n- intro: Purdue University & Oculus Research & Facebook Research\n- arxiv: [https://arxiv.org/abs/1802.02627](https://arxiv.org/abs/1802.02627)\n\n**Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting**\n\n[https://arxiv.org/abs/1802.02950](https://arxiv.org/abs/1802.02950)\n\n**Convolutional Neural Networks with Alternately Updated Clique**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1802.10419](https://arxiv.org/abs/1802.10419)\n- github: [https://github.com/iboing/CliqueNet](https://github.com/iboing/CliqueNet)\n\n**Decoupled Networks**\n\n- intro: CVPR 2018 (Spotlight)\n- arxiv: [https://arxiv.org/abs/1804.08071](https://arxiv.org/abs/1804.08071)\n\n**Optical Neural Networks**\n\n[https://arxiv.org/abs/1805.06082](https://arxiv.org/abs/1805.06082)\n\n**Regularization Learning Networks**\n\n- intro: Weizmann Institute of Science\n- keywords: Regularization Learning Networks (RLNs), Counterfactual Loss, tabular datasets\n- arxiv: [https://arxiv.org/abs/1805.06440](https://arxiv.org/abs/1805.06440)\n\n**Bilinear Attention Networks**\n\n[https://arxiv.org/abs/1805.07932](https://arxiv.org/abs/1805.07932)\n\n**Cautious Deep Learning**\n\n[https://arxiv.org/abs/1805.09460](https://arxiv.org/abs/1805.09460)\n\n**Perturbative Neural Networks**\n\n- intro: CVPR 2018\n- intro: We introduce a very simple, yet effective, module called a perturbation layer as an alternative to a convolutional layer\n- project page: [http://xujuefei.com/pnn.html](http://xujuefei.com/pnn.html)\n- arxiv: [https://arxiv.org/abs/1806.01817](https://arxiv.org/abs/1806.01817)\n\n**Lightweight Probabilistic Deep Networks**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1805.11327](https://arxiv.org/abs/1805.11327)\n\n**Channel Gating Neural Networks**\n\n[https://arxiv.org/abs/1805.12549](https://arxiv.org/abs/1805.12549)\n\n**Evenly Cascaded Convolutional Networks**\n\n[https://arxiv.org/abs/1807.00456](https://arxiv.org/abs/1807.00456)\n\n**SGAD: Soft-Guided Adaptively-Dropped Neural Network**\n\n[https://arxiv.org/abs/1807.01430](https://arxiv.org/abs/1807.01430)\n\n**Explainable Neural Computation via Stack Neural Module Networks**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.08556](https://arxiv.org/abs/1807.08556)\n\n**Rank-1 Convolutional Neural Network**\n\n[https://arxiv.org/abs/1808.04303](https://arxiv.org/abs/1808.04303)\n\n**Neural Network Encapsulation**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.03749](https://arxiv.org/abs/1808.03749)\n\n**Penetrating the Fog: the Path to Efficient CNN Models**\n\n[https://arxiv.org/abs/1810.04231](https://arxiv.org/abs/1810.04231)\n\n**A2-Nets: Double Attention Networks**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.11579](https://arxiv.org/abs/1810.11579)\n\n**Global Second-order Pooling Neural Networks**\n\n[https://arxiv.org/abs/1811.12006](https://arxiv.org/abs/1811.12006)\n\n**ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network**\n\n- intro: University of Washington & Allen Institute for AI (AI2) & XNOR.AI\n- arxiv: [https://arxiv.org/abs/1811.11431](https://arxiv.org/abs/1811.11431)\n- github: [https://github.com/sacmehta/ESPNetv2](https://github.com/sacmehta/ESPNetv2)\n\n**Kernel Transformer Networks for Compact Spherical Convolution**\n\n[https://arxiv.org/abs/1812.03115](https://arxiv.org/abs/1812.03115)\n\n**UAN: Unified Attention Network for Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1901.05376](https://arxiv.org/abs/1901.05376)\n\n**One-Class Convolutional Neural Network**\n\n- intro: Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/1901.08688](https://arxiv.org/abs/1901.08688)\n- github: [https://github.com/otkupjnoz/oc-cnn](https://github.com/otkupjnoz/oc-cnn)\n\n**Selective Kernel Networks**\n\n- intro: CVPR 2019\n- inrtro: Nanjing University of Science and Technology & Momenta & Nanjing University & Tsinghua University]\n- arxiv: [https://arxiv.org/abs/1903.06586](https://arxiv.org/abs/1903.06586)\n- github: [https://github.com/implus/SKNet](https://github.com/implus/SKNet)\n\n**Universally Slimmable Networks and Improved Training Techniques**\n\n- intro: ICLR 2019\n- arxiv: [https://arxiv.org/abs/1903.05134](https://arxiv.org/abs/1903.05134)\n\n**Dynamic Slimmable Network**\n\n- intro: CVPR 2021 oral\n- arxiv: [https://arxiv.org/abs/2103.13258](https://arxiv.org/abs/2103.13258)\n- github: [https://github.com/changlin31/DS-Net](https://github.com/changlin31/DS-Net)\n\n**Adaptively Connected Neural Networks**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.03579](https://arxiv.org/abs/1904.03579)\n- github: [https://github.com/wanggrun/Adaptively-Connected-Neural-Networks](https://github.com/wanggrun/Adaptively-Connected-Neural-Networks)\n\n**Transformable Bottleneck Networks**\n\n[https://arxiv.org/abs/1904.06458](https://arxiv.org/abs/1904.06458)\n\n**Pixel-Adaptive Convolutional Neural Networks**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.05373](https://arxiv.org/abs/1904.05373)\n\n**Attention Augmented Convolutional Networks**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1904.09925](https://arxiv.org/abs/1904.09925)\n\n**Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks**\n\n- arxiv: [https://arxiv.org/abs/1905.09646](https://arxiv.org/abs/1905.09646)\n- github: [https://github.com/implus/PytorchInsight](https://github.com/implus/PytorchInsight)\n\n**EnsembleNet: End-to-End Optimization of Multi-headed Models**\n\n- intro: Google AI\n- arxiv: [https://arxiv.org/abs/1905.09979](https://arxiv.org/abs/1905.09979)\n\n**MixNet: Mixed Depthwise Convolutional Kernels**\n\n- intro: BMVC 2019\n- arxiv: [https://arxiv.org/abs/1907.09595](https://arxiv.org/abs/1907.09595)\n- github: [https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet)\n\n**HarDNet: A Low Memory Traffic Network**\n\n- intro: ICCV 2019\n- intro: National Tsing Hua University & University of Michigan\n- arxiv: [https://arxiv.org/abs/1909.00948](https://arxiv.org/abs/1909.00948)\n\n**Π− nets: Deep Polynomial Neural Networks**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.03828](https://arxiv.org/abs/2003.03828)\n\n**Circle Loss: A Unified Perspective of Pair Similarity Optimization**\n\n- intro: 1Megvii Inc. & Beihang University & Australian National University & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2002.10857](https://arxiv.org/abs/2002.10857)\n\n**Designing Network Design Spaces**\n\n- intro: CVPR 2020\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/2003.13678](https://arxiv.org/abs/2003.13678)\n\n**WeightNet: Revisiting the Design Space of Weight Networks**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.11823](https://arxiv.org/abs/2007.11823)\n- github: [https://github.com/megvii-model/WeightNet](https://github.com/megvii-model/WeightNet)\n\n**Disentangled Non-Local Neural Networks**\n\n[https://arxiv.org/abs/2006.06668](https://arxiv.org/abs/2006.06668)\n\n**Dynamic Neural Networks: A Survey**\n\n- intro: Tsinghua University\n- arxiv: [https://arxiv.org/abs/2102.04906](https://arxiv.org/abs/2102.04906)\n\n## Convolutions / Filters\n\n**Warped Convolutions: Efficient Invariance to Spatial Transformations**\n\n- arxiv: [http://arxiv.org/abs/1609.04382](http://arxiv.org/abs/1609.04382)\n\n**Coordinating Filters for Faster Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1703.09746](https://arxiv.org/abs/1703.09746)\n- github: [https://github.com/wenwei202/caffe/tree/sfm](https://github.com/wenwei202/caffe/tree/sfm)\n\n**Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions**\n\n- intro: UC Berkeley\n- arxiv: [https://arxiv.org/abs/1711.08141](https://arxiv.org/abs/1711.08141)\n\n**Spatially-Adaptive Filter Units for Deep Neural Networks**\n\n- intro: University of Ljubljana & University of Birmingham\n- arxiv: [https://arxiv.org/abs/1711.11473](https://arxiv.org/abs/1711.11473)\n\n**clcNet: Improving the Efficiency of Convolutional Neural Network using Channel Local Convolutions**\n\n[https://arxiv.org/abs/1712.06145](https://arxiv.org/abs/1712.06145)\n\n**DCFNet: Deep Neural Network with Decomposed Convolutional Filters**\n\n[https://arxiv.org/abs/1802.04145](https://arxiv.org/abs/1802.04145)\n\n**Fast End-to-End Trainable Guided Filter**\n\n- intro: CVPR 2018\n- project page: [http://wuhuikai.me/DeepGuidedFilterProject/](http://wuhuikai.me/DeepGuidedFilterProject/)\n- gtihub(official, PyTorch): [https://github.com/wuhuikai/DeepGuidedFilter](https://github.com/wuhuikai/DeepGuidedFilter)\n\n**Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions**\n\n- arxiv: [https://arxiv.org/abs/1803.09926](https://arxiv.org/abs/1803.09926)\n- github: [https://github.com/clavichord93/diagonalwise-refactorization-tensorflow](https://github.com/clavichord93/diagonalwise-refactorization-tensorflow)\n\n**Use of symmetric kernels for convolutional neural networks**\n\n- intro: ICDSIAI 2018\n- arxiv: [https://arxiv.org/abs/1805.09421](https://arxiv.org/abs/1805.09421)\n\n**EasyConvPooling: Random Pooling with Easy Convolution for Accelerating Training and Testing**\n\n[https://arxiv.org/abs/1806.01729](https://arxiv.org/abs/1806.01729)\n\n**Targeted Kernel Networks: Faster Convolutions with Attentive Regularization**\n\n[https://arxiv.org/abs/1806.00523](https://arxiv.org/abs/1806.00523)\n\n**An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution**\n\n- intro: NeurIPS 2018\n- intro: Uber AI Labs & Uber Technologies\n- arxiv: [https://arxiv.org/abs/1807.03247](https://arxiv.org/abs/1807.03247)\n- github: [https://github.com/uber-research/CoordConv](https://github.com/uber-research/CoordConv)\n- youtube: [https://www.youtube.com/watch?v=8yFQc6elePA](https://www.youtube.com/watch?v=8yFQc6elePA)\n\n**Network Decoupling: From Regular to Depthwise Separable Convolutions**\n\n[https://arxiv.org/abs/1808.05517](https://arxiv.org/abs/1808.05517)\n\n**Partial Convolution based Padding**\n\n- intro: NVIDIA Corporation\n- arxiv; [https://arxiv.org/abs/1811.11718](https://arxiv.org/abs/1811.11718)\n- github: [https://github.com/NVIDIA/partialconv](https://github.com/NVIDIA/partialconv)\n\n**DSConv: Efficient Convolution Operator**\n\n[https://arxiv.org/abs/1901.01928](https://arxiv.org/abs/1901.01928)\n\n**CircConv: A Structured Convolution with Low Complexity**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1902.11268](https://arxiv.org/abs/1902.11268)\n\n**Accelerating Large-Kernel Convolution Using Summed-Area Tables**\n\n- intro: Princeton University\n- arxiv: [https://arxiv.org/abs/1906.11367](https://arxiv.org/abs/1906.11367)\n\n**Mapped Convolutions**\n\n- intro: University of North Carolina at Chapel Hill\n- arxiv: [https://arxiv.org/abs/1906.11096](https://arxiv.org/abs/1906.11096)\n\n**Universal Pooling -- A New Pooling Method for Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1907.11440](https://arxiv.org/abs/1907.11440)\n\n**Dilated Point Convolutions: On the Receptive Field of Point Convolutions**\n\n[https://arxiv.org/abs/1907.12046](https://arxiv.org/abs/1907.12046)\n\n**LIP: Local Importance-based Pooling**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.04156](https://arxiv.org/abs/1908.04156)\n\n**Deep Generalized Max Pooling**\n\n- intro: ICDAR\n- arxiv: [https://arxiv.org/abs/1908.05040](https://arxiv.org/abs/1908.05040)\n\n**MixConv: Mixed Depthwise Convolutional Kernels**\n\n- intro:BMVC 2019\n- arxiv: [https://arxiv.org/abs/1907.09595](https://arxiv.org/abs/1907.09595)\n- github: [https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet](https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet)\n\n**Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation**\n\n- intro: UC Berkeley & USTC & MSRA\n- arxiv: [https://arxiv.org/abs/1910.02940](https://arxiv.org/abs/1910.02940)\n\n**Dynamic Convolution: Attention over Convolution Kernels**\n\n- intro: CVPR 2020 oral\n- intro: Microsoft\n- arxiv: [https://arxiv.org/abs/1912.03458](https://arxiv.org/abs/1912.03458)\n\n**Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition**\n\n- arxiv: [https://arxiv.org/abs/2006.11538](https://arxiv.org/abs/2006.11538)\n- github: [https://github.com/iduta/pyconv](https://github.com/iduta/pyconv)\n- gihtub: [https://github.com/iduta/pyconvsegnet](https://github.com/iduta/pyconvsegnet)\n\n## Highway Networks\n\n**Highway Networks**\n\n- intro: ICML 2015 Deep Learning workshop\n- intro: shortcut connections with gating functions. These gates are data-dependent and have parameters\n- arxiv: [http://arxiv.org/abs/1505.00387](http://arxiv.org/abs/1505.00387)\n- github(PyTorch): [https://github.com/analvikingur/pytorch_Highway](https://github.com/analvikingur/pytorch_Highway)\n\n**Highway Networks with TensorFlow**\n\n- blog: [https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa#.71fgztsb6](https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa#.71fgztsb6)\n\n**Very Deep Learning with Highway Networks**\n\n- homepage(papers+code+FAQ): [http://people.idsia.ch/~rupesh/very_deep_learning/](http://people.idsia.ch/~rupesh/very_deep_learning/)\n\n**Training Very Deep Networks**\n\n- intro: Extends [Highway Networks](https://arxiv.org/abs/1505.00387)\n- project page: [http://people.idsia.ch/~rupesh/very_deep_learning/](http://people.idsia.ch/~rupesh/very_deep_learning/)\n- arxiv: [http://arxiv.org/abs/1507.06228](http://arxiv.org/abs/1507.06228)\n\n## Spatial Transformer Networks\n\n**Spatial Transformer Networks**\n\n![](https://camo.githubusercontent.com/bb81d6267f2123d59979453526d958a58899bb4f/687474703a2f2f692e696d6775722e636f6d2f4578474456756c2e706e67)\n\n- intro: NIPS 2015\n- arxiv: [http://arxiv.org/abs/1506.02025](http://arxiv.org/abs/1506.02025)\n- gitxiv: [http://gitxiv.com/posts/5WTXTLuEA4Hd8W84G/spatial-transformer-networks](http://gitxiv.com/posts/5WTXTLuEA4Hd8W84G/spatial-transformer-networks)\n- github: [https://github.com/daerduoCarey/SpatialTransformerLayer](https://github.com/daerduoCarey/SpatialTransformerLayer)\n- github: [https://github.com/qassemoquab/stnbhwd](https://github.com/qassemoquab/stnbhwd)\n- github: [https://github.com/skaae/transformer_network](https://github.com/skaae/transformer_network)\n- github(Caffe): [https://github.com/happynear/SpatialTransformerLayer](https://github.com/happynear/SpatialTransformerLayer)\n- github: [https://github.com/daviddao/spatial-transformer-tensorflow](https://github.com/daviddao/spatial-transformer-tensorflow)\n- caffe-issue: [https://github.com/BVLC/caffe/issues/3114](https://github.com/BVLC/caffe/issues/3114)\n- code: [https://lasagne.readthedocs.org/en/latest/modules/layers/special.html#lasagne.layers.TransformerLayer](https://lasagne.readthedocs.org/en/latest/modules/layers/special.html#lasagne.layers.TransformerLayer)\n- ipn(Lasagne): [http://nbviewer.jupyter.org/github/Lasagne/Recipes/blob/master/examples/spatial_transformer_network.ipynb](http://nbviewer.jupyter.org/github/Lasagne/Recipes/blob/master/examples/spatial_transformer_network.ipynb)\n- notes: [https://www.evernote.com/shard/s189/sh/ad8a38de-9e98-4e06-b09e-574bd62893ff/32f72798c095dd7672f4cb017a32d9b4](https://www.evernote.com/shard/s189/sh/ad8a38de-9e98-4e06-b09e-574bd62893ff/32f72798c095dd7672f4cb017a32d9b4)\n- youtube: [https://www.youtube.com/watch?v=6NOQC_fl1hQ](https://www.youtube.com/watch?v=6NOQC_fl1hQ)\n\n**The power of Spatial Transformer Networks**\n\n- blog: [http://torch.ch/blog/2015/09/07/spatial_transformers.html](http://torch.ch/blog/2015/09/07/spatial_transformers.html)\n- github: [https://github.com/moodstocks/gtsrb.torch](https://github.com/moodstocks/gtsrb.torch)\n\n**Recurrent Spatial Transformer Networks**\n\n- paper: [http://arxiv.org/abs/1509.05329](http://arxiv.org/abs/1509.05329)\n\n**Deep Learning Paper Implementations: Spatial Transformer Networks - Part I**\n\n- blog: [https://kevinzakka.github.io/2017/01/10/stn-part1/](https://kevinzakka.github.io/2017/01/10/stn-part1/)\n- github: [https://github.com/kevinzakka/blog-code/tree/master/spatial_transformer](https://github.com/kevinzakka/blog-code/tree/master/spatial_transformer)\n\n**Top-down Flow Transformer Networks**\n\n[https://arxiv.org/abs/1712.02400](https://arxiv.org/abs/1712.02400)\n\n**Non-Parametric Transformation Networks**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1801.04520](https://arxiv.org/abs/1801.04520)\n\n**Hierarchical Spatial Transformer Network**\n\n[https://arxiv.org/abs/1801.09467](https://arxiv.org/abs/1801.09467)\n\n**Spatial Transformer Introspective Neural Network**\n\n- intro: Johns Hopkins University & Shanghai University\n- arxiv: [https://arxiv.org/abs/1805.06447](https://arxiv.org/abs/1805.06447)\n\n**DeSTNet: Densely Fused Spatial Transformer Networks**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.04050](https://arxiv.org/abs/1807.04050)\n\n**MIST: Multiple Instance Spatial Transformer Network**\n\n[https://arxiv.org/abs/1811.10725](https://arxiv.org/abs/1811.10725)\n\n## FractalNet\n\n**FractalNet: Ultra-Deep Neural Networks without Residuals**\n\n![](http://people.cs.uchicago.edu/~larsson/fractalnet/overview.png)\n\n- project: [http://people.cs.uchicago.edu/~larsson/fractalnet/](http://people.cs.uchicago.edu/~larsson/fractalnet/)\n- arxiv: [http://arxiv.org/abs/1605.07648](http://arxiv.org/abs/1605.07648)\n- github: [https://github.com/gustavla/fractalnet](https://github.com/gustavla/fractalnet)\n- github: [https://github.com/edgelord/FractalNet](https://github.com/edgelord/FractalNet)\n- github(Keras): [https://github.com/snf/keras-fractalnet](https://github.com/snf/keras-fractalnet)\n\n# Generative Models\n\n**Max-margin Deep Generative Models**\n\n- intro: NIPS 2015\n- arxiv: [http://arxiv.org/abs/1504.06787](http://arxiv.org/abs/1504.06787)\n- github: [https://github.com/zhenxuan00/mmdgm](https://github.com/zhenxuan00/mmdgm)\n\n**Discriminative Regularization for Generative Models**\n\n- arxiv: [http://arxiv.org/abs/1602.03220](http://arxiv.org/abs/1602.03220)\n- github: [https://github.com/vdumoulin/discgen](https://github.com/vdumoulin/discgen)\n\n**Auxiliary Deep Generative Models**\n\n- arxiv: [http://arxiv.org/abs/1602.05473](http://arxiv.org/abs/1602.05473)\n- github: [https://github.com/larsmaaloee/auxiliary-deep-generative-models](https://github.com/larsmaaloee/auxiliary-deep-generative-models)\n\n**Sampling Generative Networks: Notes on a Few Effective Techniques**\n\n- arxiv: [http://arxiv.org/abs/1609.04468](http://arxiv.org/abs/1609.04468)\n- paper: [https://github.com/dribnet/plat](https://github.com/dribnet/plat)\n\n**Conditional Image Synthesis With Auxiliary Classifier GANs**\n\n- arxiv: [https://arxiv.org/abs/1610.09585](https://arxiv.org/abs/1610.09585)\n- github: [https://github.com/buriburisuri/ac-gan](https://github.com/buriburisuri/ac-gan)\n- github(Keras): [https://github.com/lukedeo/keras-acgan](https://github.com/lukedeo/keras-acgan)\n\n**On the Quantitative Analysis of Decoder-Based Generative Models**\n\n- intro: University of Toronto & OpenAI & CMU\n- arxiv: [https://arxiv.org/abs/1611.04273](https://arxiv.org/abs/1611.04273)\n- github: [https://github.com/tonywu95/eval_gen](https://github.com/tonywu95/eval_gen)\n\n**Boosted Generative Models**\n\n- arxiv: [https://arxiv.org/abs/1702.08484](https://arxiv.org/abs/1702.08484)\n- paper: [https://openreview.net/pdf?id=HyY4Owjll](https://openreview.net/pdf?id=HyY4Owjll)\n\n**An Architecture for Deep, Hierarchical Generative Models**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1612.04739](https://arxiv.org/abs/1612.04739)\n- github: [https://github.com/Philip-Bachman/MatNets-NIPS](https://github.com/Philip-Bachman/MatNets-NIPS)\n\n**Deep Learning and Hierarchal Generative Models**\n\n- intro: NIPS 2016. MIT\n- arxiv: [https://arxiv.org/abs/1612.09057](https://arxiv.org/abs/1612.09057)\n\n**Probabilistic Torch**\n\n- intro: Probabilistic Torch is library for deep generative models that extends PyTorch\n- github: [https://github.com/probtorch/probtorch](https://github.com/probtorch/probtorch)\n\n**Tutorial on Deep Generative Models**\n\n- intro: UAI 2017 Tutorial: Shakir Mohamed & Danilo Rezende (DeepMind)\n- youtube: [https://www.youtube.com/watch?v=JrO5fSskISY](https://www.youtube.com/watch?v=JrO5fSskISY)\n- mirror: [https://www.bilibili.com/video/av16428277/](https://www.bilibili.com/video/av16428277/)\n- slides: [http://www.shakirm.com/slides/DeepGenModelsTutorial.pdf](http://www.shakirm.com/slides/DeepGenModelsTutorial.pdf)\n\n**A Note on the Inception Score**\n\n- intro: Stanford University\n- arxiv: [https://arxiv.org/abs/1801.01973](https://arxiv.org/abs/1801.01973)\n\n**Gradient Layer: Enhancing the Convergence of Adversarial Training for Generative Models**\n\n- intro: AISTATS 2018. The University of Tokyo\n- arxiv: [https://arxiv.org/abs/1801.02227](https://arxiv.org/abs/1801.02227)\n\n**Batch Normalization in the final layer of generative networks**\n\n[https://arxiv.org/abs/1805.07389](https://arxiv.org/abs/1805.07389)\n\n**Deep Structured Generative Models**\n\n- intro: Tsinghua University\n- arxiv: [https://arxiv.org/abs/1807.03877](https://arxiv.org/abs/1807.03877)\n\n**VFunc: a Deep Generative Model for Functions**\n\n- intro: ICML 2018 workshop on Prediction and Generative Modeling in Reinforcement Learning. Microsoft Research & McGill University\n- arxiv: [https://arxiv.org/abs/1807.04106](https://arxiv.org/abs/1807.04106)\n\n# Deep Learning and Robots\n\n**Robot Learning Manipulation Action Plans by \"Watching\" Unconstrained Videos from the World Wide Web**\n\n- intro: AAAI 2015\n- paper: [http://www.umiacs.umd.edu/~yzyang/paper/YouCookMani_CameraReady.pdf](http://www.umiacs.umd.edu/~yzyang/paper/YouCookMani_CameraReady.pdf)\n- author page: [http://www.umiacs.umd.edu/~yzyang/](http://www.umiacs.umd.edu/~yzyang/)\n\n**End-to-End Training of Deep Visuomotor Policies**\n\n- arxiv: [http://arxiv.org/abs/1504.00702](http://arxiv.org/abs/1504.00702)\n\n**Comment on Open AI’s Efforts to Robot Learning**\n\n- blog: [https://gridworld.wordpress.com/2016/07/28/comment-on-open-ais-efforts-to-robot-learning/](https://gridworld.wordpress.com/2016/07/28/comment-on-open-ais-efforts-to-robot-learning/)\n\n**The Curious Robot: Learning Visual Representations via Physical Interactions**\n\n- arxiv: [http://arxiv.org/abs/1604.01360](http://arxiv.org/abs/1604.01360)\n\n**How to build a robot that “sees” with $100 and TensorFlow**\n\n![](https://d3ansictanv2wj.cloudfront.net/Figure_5-5b104cf7a53a9c1ee95110b78fb14256.jpg)\n\n- blog: [https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with-100-and-tensorflow](https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with-100-and-tensorflow)\n\n**Deep Visual Foresight for Planning Robot Motion**\n\n- project page: [https://sites.google.com/site/brainrobotdata/](https://sites.google.com/site/brainrobotdata/)\n- arxiv: [https://arxiv.org/abs/1610.00696](https://arxiv.org/abs/1610.00696)\n- video: [https://sites.google.com/site/robotforesight/](https://sites.google.com/site/robotforesight/)\n\n**Sim-to-Real Robot Learning from Pixels with Progressive Nets**\n\n- intro: Google DeepMind\n- arxiv: [https://arxiv.org/abs/1610.04286](https://arxiv.org/abs/1610.04286)\n\n**Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics**\n\n- arxiv: [https://arxiv.org/abs/1611.00201](https://arxiv.org/abs/1611.00201)\n\n**A Differentiable Physics Engine for Deep Learning in Robotics**\n\n- paper: [http://openreview.net/pdf?id=SyEiHNKxx](http://openreview.net/pdf?id=SyEiHNKxx)\n\n**Deep-learning in Mobile Robotics - from Perception to Control Systems: A Survey on Why and Why not**\n\n- intro: City University of Hong Kong & Hong Kong University of Science and Technology\n- arxiv: [https://arxiv.org/abs/1612.07139](https://arxiv.org/abs/1612.07139)\n\n**Deep Robotic Learning**\n\n- intro: [https://simons.berkeley.edu/talks/sergey-levine-01-24-2017-1](https://simons.berkeley.edu/talks/sergey-levine-01-24-2017-1)\n- youtube: [https://www.youtube.com/watch?v=jtjW5Pye_44](https://www.youtube.com/watch?v=jtjW5Pye_44)\n\n**Deep Learning in Robotics: A Review of Recent Research**\n\n[https://arxiv.org/abs/1707.07217](https://arxiv.org/abs/1707.07217)\n\n**Deep Learning for Robotics**\n\n- intro: by Pieter Abbeel\n- video: [https://www.facebook.com/nipsfoundation/videos/1554594181298482/](https://www.facebook.com/nipsfoundation/videos/1554594181298482/)\n- mirror: [https://www.bilibili.com/video/av17078186/](https://www.bilibili.com/video/av17078186/)\n- slides: [https://www.dropbox.com/s/4fhczb9cxkuqalf/2017_11_xx_BARS-Abbeel.pdf?dl=0](https://www.dropbox.com/s/4fhczb9cxkuqalf/2017_11_xx_BARS-Abbeel.pdf?dl=0)\n\n**DroNet: Learning to Fly by Driving**\n\n- project page: [http://rpg.ifi.uzh.ch/dronet.html](http://rpg.ifi.uzh.ch/dronet.html)\n- paper: [http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf](http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf)\n- github: [https://github.com/uzh-rpg/rpg_public_dronet](https://github.com/uzh-rpg/rpg_public_dronet)\n\n**A Survey on Deep Learning Methods for Robot Vision**\n\n[https://arxiv.org/abs/1803.10862](https://arxiv.org/abs/1803.10862)\n\n# Deep Learning on Mobile / Embedded Devices\n\n**Convolutional neural networks on the iPhone with VGGNet**\n\n- blog: [http://matthijshollemans.com/2016/08/30/vggnet-convolutional-neural-network-iphone/](http://matthijshollemans.com/2016/08/30/vggnet-convolutional-neural-network-iphone/)\n- github: [https://github.com/hollance/VGGNet-Metal](https://github.com/hollance/VGGNet-Metal)\n\n**TensorFlow for Mobile Poets**\n\n- blog: [https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/)\n\n**The Convolutional Neural Network(CNN) for Android**\n\n- intro: CnnForAndroid:A Classification Project using Convolutional Neural Network(CNN) in Android platform。It also support Caffe Model\n- github: [https://github.com/zhangqianhui/CnnForAndroid](https://github.com/zhangqianhui/CnnForAndroid)\n\n**TensorFlow on Android**\n\n- blog: [https://www.oreilly.com/learning/tensorflow-on-android](https://www.oreilly.com/learning/tensorflow-on-android)\n\n**Experimenting with TensorFlow on Android**\n\n- part 1: [https://medium.com/@mgazar/experimenting-with-tensorflow-on-android-pt-1-362683b31838#.5gbp2d4st](https://medium.com/@mgazar/experimenting-with-tensorflow-on-android-pt-1-362683b31838#.5gbp2d4st)\n- part 2: [https://medium.com/@mgazar/experimenting-with-tensorflow-on-android-part-2-12f3dc294eaf#.2gx3o65f5](https://medium.com/@mgazar/experimenting-with-tensorflow-on-android-part-2-12f3dc294eaf#.2gx3o65f5)\n- github: [https://github.com/MostafaGazar/tensorflow](https://github.com/MostafaGazar/tensorflow)\n\n**XNOR.ai frees AI from the prison of the supercomputer**\n\n- blog: [https://techcrunch.com/2017/01/19/xnor-ai-frees-ai-from-the-prison-of-the-supercomputer/](https://techcrunch.com/2017/01/19/xnor-ai-frees-ai-from-the-prison-of-the-supercomputer/)\n\n**Embedded and mobile deep learning research resources**\n\n[https://github.com/csarron/emdl](https://github.com/csarron/emdl)\n\n**Modeling the Resource Requirements of Convolutional Neural Networks on Mobile Devices**\n\n[https://arxiv.org/abs/1709.09118](https://arxiv.org/abs/1709.09118)\n\n# Benchmarks\n\n**Deep Learning’s Accuracy**\n\n- blog: [http://deeplearning4j.org/accuracy.html](http://deeplearning4j.org/accuracy.html)\n\n**Benchmarks for popular CNN models**\n\n- intro: Benchmarks for popular convolutional neural network models on CPU and different GPUs, with and without cuDNN.\n- github: [https://github.com/jcjohnson/cnn-benchmarks](https://github.com/jcjohnson/cnn-benchmarks)\n\n**Deep Learning Benchmarks**\n\n[http://add-for.com/deep-learning-benchmarks/](http://add-for.com/deep-learning-benchmarks/)\n\n**cudnn-rnn-benchmarks**\n\n- github: [https://github.com/MaximumEntropy/cudnn_rnn_theano_benchmarks](https://github.com/MaximumEntropy/cudnn_rnn_theano_benchmarks)\n\n# Papers\n\n**Reweighted Wake-Sleep**\n\n- paper: [http://arxiv.org/abs/1406.2751](http://arxiv.org/abs/1406.2751)\n- github: [https://github.com/jbornschein/reweighted-ws](https://github.com/jbornschein/reweighted-ws)\n\n**Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks**\n\n- paper: [http://arxiv.org/abs/1502.05336](http://arxiv.org/abs/1502.05336)\n- github: [https://github.com/HIPS/Probabilistic-Backpropagation](https://github.com/HIPS/Probabilistic-Backpropagation)\n\n**Deeply-Supervised Nets**\n\n- paper: [http://arxiv.org/abs/1409.5185](http://arxiv.org/abs/1409.5185)\n- github: [https://github.com/mbhenaff/spectral-lib](https://github.com/mbhenaff/spectral-lib)\n\n**Deep learning**\n\n- intro: Nature 2015\n- author: Yann LeCun, Yoshua Bengio & Geoffrey Hinton\n- paper: [http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)\n\n**On the Expressive Power of Deep Learning: A Tensor Analysis**\n\n- paper: [http://arxiv.org/abs/1509.05009](http://arxiv.org/abs/1509.05009)\n\n**Understanding and Predicting Image Memorability at a Large Scale**\n\n- intro: MIT. ICCV 2015\n- homepage: [http://memorability.csail.mit.edu/](http://memorability.csail.mit.edu/)\n- paper: [https://people.csail.mit.edu/khosla/papers/iccv2015_khosla.pdf](https://people.csail.mit.edu/khosla/papers/iccv2015_khosla.pdf)\n- code: [http://memorability.csail.mit.edu/download.html](http://memorability.csail.mit.edu/download.html)\n- reviews: [http://petapixel.com/2015/12/18/how-memorable-are-times-top-10-photos-of-2015-to-a-computer/](http://petapixel.com/2015/12/18/how-memorable-are-times-top-10-photos-of-2015-to-a-computer/)\n\n**Towards Open Set Deep Networks**\n\n- arxiv: [http://arxiv.org/abs/1511.06233](http://arxiv.org/abs/1511.06233)\n- github: [https://github.com/abhijitbendale/OSDN](https://github.com/abhijitbendale/OSDN)\n\n**Structured Prediction Energy Networks**\n\n- intro: ICML 2016. SPEN\n- arxiv: [http://arxiv.org/abs/1511.06350](http://arxiv.org/abs/1511.06350)\n- github: [https://github.com/davidBelanger/SPEN](https://github.com/davidBelanger/SPEN)\n\n**Deep Neural Networks predict Hierarchical Spatio-temporal Cortical Dynamics of Human Visual Object Recognition**\n\n- arxiv: [http://arxiv.org/abs/1601.02970](http://arxiv.org/abs/1601.02970)\n- demo: [http://brainmodels.csail.mit.edu/dnn/drawCNN/](http://brainmodels.csail.mit.edu/dnn/drawCNN/)\n\n**Recent Advances in Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1512.07108](http://arxiv.org/abs/1512.07108)\n\n**Understanding Deep Convolutional Networks**\n\n- arxiv: [http://arxiv.org/abs/1601.04920](http://arxiv.org/abs/1601.04920)\n\n**DeepCare: A Deep Dynamic Memory Model for Predictive Medicine**\n\n- arxiv: [http://arxiv.org/abs/1602.00357](http://arxiv.org/abs/1602.00357)\n\n**Exploiting Cyclic Symmetry in Convolutional Neural Networks**\n\n![](http://benanne.github.io/images/cyclicroll.png)\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1602.02660](http://arxiv.org/abs/1602.02660)\n- github(Winning solution for the National Data Science Bowl competition on Kaggle (plankton classification)): [https://github.com/benanne/kaggle-ndsb](https://github.com/benanne/kaggle-ndsb)\n- ref(use Cyclic pooling): [http://benanne.github.io/2015/03/17/plankton.html](http://benanne.github.io/2015/03/17/plankton.html)\n\n**Cross-dimensional Weighting for Aggregated Deep Convolutional Features**\n\n- arxiv: [http://arxiv.org/abs/1512.04065](http://arxiv.org/abs/1512.04065)\n- github: [https://github.com/yahoo/crow](https://github.com/yahoo/crow)\n\n**Understanding Visual Concepts with Continuation Learning**\n\n- project page: [http://willwhitney.github.io/understanding-visual-concepts/](http://willwhitney.github.io/understanding-visual-concepts/)\n- arxiv: [http://arxiv.org/abs/1602.06822](http://arxiv.org/abs/1602.06822)\n- github: [https://github.com/willwhitney/understanding-visual-concepts](https://github.com/willwhitney/understanding-visual-concepts)\n\n**Learning Efficient Algorithms with Hierarchical Attentive Memory**\n\n- arxiv: [http://arxiv.org/abs/1602.03218](http://arxiv.org/abs/1602.03218)\n- github: [https://github.com/Smerity/tf-ham](https://github.com/Smerity/tf-ham)\n\n**DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1601.00917](http://arxiv.org/abs/1601.00917)\n- github: [https://github.com/bigaidream-projects/drmad](https://github.com/bigaidream-projects/drmad)\n\n**Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?**\n\n- arxiv: [http://arxiv.org/abs/1603.05691](http://arxiv.org/abs/1603.05691)\n- review: [http://www.erogol.com/paper-review-deep-convolutional-nets-really-need-deep-even-convolutional/](http://www.erogol.com/paper-review-deep-convolutional-nets-really-need-deep-even-convolutional/)\n\n**Harnessing Deep Neural Networks with Logic Rules**\n\n- arxiv: [http://arxiv.org/abs/1603.06318](http://arxiv.org/abs/1603.06318)\n\n**Degrees of Freedom in Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1603.09260](http://arxiv.org/abs/1603.09260)\n\n**Deep Networks with Stochastic Depth**\n\n- arxiv: [http://arxiv.org/abs/1603.09382](http://arxiv.org/abs/1603.09382)\n- github: [https://github.com/yueatsprograms/Stochastic_Depth](https://github.com/yueatsprograms/Stochastic_Depth)\n- notes(\"Stochastic Depth Networks will Become the New Normal\"): [http://deliprao.com/archives/134](http://deliprao.com/archives/134)\n- github: [https://github.com/dblN/stochastic_depth_keras](https://github.com/dblN/stochastic_depth_keras)\n- github: [https://github.com/yasunorikudo/chainer-ResDrop](https://github.com/yasunorikudo/chainer-ResDrop)\n- review: [https://medium.com/@tim_nth/review-deep-networks-with-stochastic-depth-51bd53acfe72](https://medium.com/@tim_nth/review-deep-networks-with-stochastic-depth-51bd53acfe72)\n\n**LIFT: Learned Invariant Feature Transform**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1603.09114](http://arxiv.org/abs/1603.09114)\n- github(official): [https://github.com/cvlab-epfl/LIFT](https://github.com/cvlab-epfl/LIFT)\n\n**Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex**\n\n- arxiv: [https://arxiv.org/abs/1604.03640](https://arxiv.org/abs/1604.03640)\n- slides: [http://prlab.tudelft.nl/sites/default/files/rnnResnetCortex.pdf](http://prlab.tudelft.nl/sites/default/files/rnnResnetCortex.pdf)\n\n**Understanding How Image Quality Affects Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.04004](http://arxiv.org/abs/1604.04004)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/4exk3u/dcnns_are_more_sensitive_to_blur_and_noise_than/](https://www.reddit.com/r/MachineLearning/comments/4exk3u/dcnns_are_more_sensitive_to_blur_and_noise_than/)\n\n**Deep Embedding for Spatial Role Labeling**\n\n- arxiv: [http://arxiv.org/abs/1603.08474](http://arxiv.org/abs/1603.08474)\n- github: [https://github.com/oswaldoludwig/visually-informed-embedding-of-word-VIEW-](https://github.com/oswaldoludwig/visually-informed-embedding-of-word-VIEW-)\n\n**Unreasonable Effectiveness of Learning Neural Nets: Accessible States and Robust Ensembles**\n\n- arxiv: [http://arxiv.org/abs/1605.06444](http://arxiv.org/abs/1605.06444)\n\n**Learning Deep Representation for Imbalanced Classification**\n\n![](http://mmlab.ie.cuhk.edu.hk/projects/LMLE/method.png)\n\n- intro: CVPR 2016\n- keywords: Deep Learning Large Margin Local Embedding (LMLE)\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/LMLE.html](http://mmlab.ie.cuhk.edu.hk/projects/LMLE.html)\n- paper: [http://personal.ie.cuhk.edu.hk/~ccloy/files/cvpr_2016_imbalanced.pdf](http://personal.ie.cuhk.edu.hk/~ccloy/files/cvpr_2016_imbalanced.pdf)\n- code: [http://mmlab.ie.cuhk.edu.hk/projects/LMLE/lmle_code.zip](http://mmlab.ie.cuhk.edu.hk/projects/LMLE/lmle_code.zip)\n\n**Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images**\n\n![](http://allenai.org/images/projects/plato_newton.png?cb=1466683222538)\n\n- homepage: [http://allenai.org/plato/newtonian-understanding/](http://allenai.org/plato/newtonian-understanding/)\n- arxiv: [http://arxiv.org/abs/1511.04048](http://arxiv.org/abs/1511.04048)\n- github: [https://github.com/roozbehm/newtonian](https://github.com/roozbehm/newtonian)\n\n**DeepMath - Deep Sequence Models for Premise Selection**\n\n- arxiv: [https://arxiv.org/abs/1606.04442](https://arxiv.org/abs/1606.04442)\n- github: [https://github.com/tensorflow/deepmath](https://github.com/tensorflow/deepmath)\n\n**Convolutional Neural Networks Analyzed via Convolutional Sparse Coding**\n\n- arxiv: [http://arxiv.org/abs/1607.08194](http://arxiv.org/abs/1607.08194)\n\n**Systematic evaluation of CNN advances on the ImageNet**\n\n- arxiv: [http://arxiv.org/abs/1606.02228](http://arxiv.org/abs/1606.02228)\n- github: [https://github.com/ducha-aiki/caffenet-benchmark](https://github.com/ducha-aiki/caffenet-benchmark)\n\n**Why does deep and cheap learning work so well?**\n\n- intro: Harvard and MIT\n- arxiv: [http://arxiv.org/abs/1608.08225](http://arxiv.org/abs/1608.08225)\n- review: [https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe/](https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe/)\n\n**A scalable convolutional neural network for task-specified scenarios via knowledge distillation**\n\n- arxiv: [http://arxiv.org/abs/1609.05695](http://arxiv.org/abs/1609.05695)\n\n**Alternating Back-Propagation for Generator Network**\n\n- project page(code+data): [http://www.stat.ucla.edu/~ywu/ABP/main.html](http://www.stat.ucla.edu/~ywu/ABP/main.html)\n- paper: [http://www.stat.ucla.edu/~ywu/ABP/doc/arXivABP.pdf](http://www.stat.ucla.edu/~ywu/ABP/doc/arXivABP.pdf)\n\n**A Novel Representation of Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1610.01549](https://arxiv.org/abs/1610.01549)\n\n**Optimization of Convolutional Neural Network using Microcanonical Annealing Algorithm**\n\n- intro: IEEE ICACSIS 2016\n- arxiv: [https://arxiv.org/abs/1610.02306](https://arxiv.org/abs/1610.02306)\n\n**Uncertainty in Deep Learning**\n\n- intro: PhD Thesis. Cambridge Machine Learning Group\n- blog: [http://mlg.eng.cam.ac.uk/yarin/blog_2248.html](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)\n- thesis: [http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf](http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf)\n\n**Deep Convolutional Neural Network Design Patterns**\n\n- arxiv: [https://arxiv.org/abs/1611.00847](https://arxiv.org/abs/1611.00847)\n- github: [https://github.com/iPhysicist/CNNDesignPatterns](https://github.com/iPhysicist/CNNDesignPatterns)\n\n**Extensions and Limitations of the Neural GPU**\n\n- arxiv: [https://arxiv.org/abs/1611.00736](https://arxiv.org/abs/1611.00736)\n- github: [https://github.com/openai/ecprice-neural-gpu](https://github.com/openai/ecprice-neural-gpu)\n\n**Neural Functional Programming**\n\n- arxiv: [https://arxiv.org/abs/1611.01988](https://arxiv.org/abs/1611.01988)\n\n**Deep Information Propagation**\n\n- arxiv: [https://arxiv.org/abs/1611.01232](https://arxiv.org/abs/1611.01232)\n\n**Compressed Learning: A Deep Neural Network Approach**\n\n- arxiv: [https://arxiv.org/abs/1610.09615](https://arxiv.org/abs/1610.09615)\n\n**A backward pass through a CNN using a generative model of its activations**\n\n- arxiv: [https://arxiv.org/abs/1611.02767](https://arxiv.org/abs/1611.02767)\n\n**Understanding deep learning requires rethinking generalization**\n\n- intro: ICLR 2017 best paper. MIT & Google Brain & UC Berkeley & Google DeepMind\n- arxiv: [https://arxiv.org/abs/1611.03530](https://arxiv.org/abs/1611.03530)\n- example code: [https://github.com/pluskid/fitting-random-labels](https://github.com/pluskid/fitting-random-labels)\n- notes: [https://theneuralperspective.com/2017/01/24/understanding-deep-learning-requires-rethinking-generalization/](https://theneuralperspective.com/2017/01/24/understanding-deep-learning-requires-rethinking-generalization/)\n\n**Learning the Number of Neurons in Deep Networks**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1611.06321](https://arxiv.org/abs/1611.06321)\n\n**Survey of Expressivity in Deep Neural Networks**\n\n- intro: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems\n- intro: Google Brain & Cornell University & Stanford University\n- arxiv: [https://arxiv.org/abs/1611.08083](https://arxiv.org/abs/1611.08083)\n\n**Designing Neural Network Architectures using Reinforcement Learning**\n\n- intro: MIT\n- project page: [https://bowenbaker.github.io/metaqnn/](https://bowenbaker.github.io/metaqnn/)\n- arxiv: [https://arxiv.org/abs/1611.02167](https://arxiv.org/abs/1611.02167)\n\n**Towards Robust Deep Neural Networks with BANG**\n\n- intro: University of Colorado\n- arxiv: [https://arxiv.org/abs/1612.00138](https://arxiv.org/abs/1612.00138)\n\n**Deep Quantization: Encoding Convolutional Activations with Deep Generative Model**\n\n- intro: University of Science and Technology of China & MSR\n- arxiv: [https://arxiv.org/abs/1611.09502](https://arxiv.org/abs/1611.09502)\n\n**A Probabilistic Theory of Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1504.00641](https://arxiv.org/abs/1504.00641)\n\n**A Probabilistic Framework for Deep Learning**\n\n- intro: Rice University\n- arxiv: [https://arxiv.org/abs/1612.01936](https://arxiv.org/abs/1612.01936)\n\n**Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer**\n\n- arxiv: [https://arxiv.org/abs/1612.03928](https://arxiv.org/abs/1612.03928)\n- github(PyTorch): [https://github.com/szagoruyko/attention-transfer](https://github.com/szagoruyko/attention-transfer)\n\n**Risk versus Uncertainty in Deep Learning: Bayes, Bootstrap and the Dangers of Dropout**\n\n- intro: Google Deepmind\n- paper: [http://bayesiandeeplearning.org/papers/BDL_4.pdf](http://bayesiandeeplearning.org/papers/BDL_4.pdf)\n\n**Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer**\n\n- intro: Google Brain & Jagiellonian University\n- keywords: Sparsely-Gated Mixture-of-Experts layer (MoE), language modeling and machine translation\n- arxiv: [https://arxiv.org/abs/1701.06538](https://arxiv.org/abs/1701.06538)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/5pud72/research_outrageously_large_neural_networks_the/](https://www.reddit.com/r/MachineLearning/comments/5pud72/research_outrageously_large_neural_networks_the/)\n\n**Deep Network Guided Proof Search**\n\n- intro: Google Research & University of Innsbruck\n- arxiv: [https://arxiv.org/abs/1701.06972](https://arxiv.org/abs/1701.06972)\n\n**PathNet: Evolution Channels Gradient Descent in Super Neural Networks**\n\n- intro: Google DeepMind & Google Brain\n- arxiv: [https://arxiv.org/abs/1701.08734](https://arxiv.org/abs/1701.08734)\n- notes: [https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273#.8f0o6w3en](https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273#.8f0o6w3en)\n\n**Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.01135](https://arxiv.org/abs/1702.01135)\n\n**The Power of Sparsity in Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.06257](https://arxiv.org/abs/1702.06257)\n\n**Learning across scales - A multiscale method for Convolution Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1703.02009](https://arxiv.org/abs/1703.02009)\n\n**Stacking-based Deep Neural Network: Deep Analytic Network on Convolutional Spectral Histogram Features**\n\n- arxiv: [https://arxiv.org/abs/1703.01396](https://arxiv.org/abs/1703.01396)\n\n**A Compositional Object-Based Approach to Learning Physical Dynamics**\n\n- intro: ICLR 2017. Neural Physics Engine\n- paper: [https://openreview.net/pdf?id=Bkab5dqxe](https://openreview.net/pdf?id=Bkab5dqxe)\n- github: [https://github.com/mbchang/dynamics](https://github.com/mbchang/dynamics)\n\n**Genetic CNN**\n\n- arxiv: [https://arxiv.org/abs/1703.01513](https://arxiv.org/abs/1703.01513)\n- github(Tensorflow): [https://github.com/aqibsaeed/Genetic-CNN](https://github.com/aqibsaeed/Genetic-CNN)\n\n**Deep Sets**\n\n- intro: Amazon Web Services & CMU\n- keywords: statistic estimation, point cloud classification, set expansion, and image tagging\n- arxiv: [https://arxiv.org/abs/1703.06114](https://arxiv.org/abs/1703.06114)\n\n**Multiscale Hierarchical Convolutional Networks**\n\n- arxiv: [https://arxiv.org/abs/1703.04140](https://arxiv.org/abs/1703.04140)\n- github: [https://github.com/jhjacobsen/HierarchicalCNN](https://github.com/jhjacobsen/HierarchicalCNN)\n\n**Deep Neural Networks Do Not Recognize Negative Images**\n\n[https://arxiv.org/abs/1703.06857](https://arxiv.org/abs/1703.06857)\n\n**Failures of Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1703.07950](https://arxiv.org/abs/1703.07950)\n- github: [https://github.com/shakedshammah/failures_of_DL](https://github.com/shakedshammah/failures_of_DL)\n\n**Multi-Scale Dense Convolutional Networks for Efficient Prediction**\n\n- intro: Cornell University & Tsinghua University & Fudan University & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1703.09844](https://arxiv.org/abs/1703.09844)\n- github: [https://github.com/gaohuang/MSDNet](https://github.com/gaohuang/MSDNet)\n\n**Scaling the Scattering Transform: Deep Hybrid Networks**\n\n- arxiv: [https://arxiv.org/abs/1703.08961](https://arxiv.org/abs/1703.08961)\n- github: [https://github.com/edouardoyallon/scalingscattering](https://github.com/edouardoyallon/scalingscattering)\n- github(CuPy/PyTorch): [https://github.com/edouardoyallon/pyscatwave](https://github.com/edouardoyallon/pyscatwave)\n\n**Deep Learning is Robust to Massive Label Noise**\n\n[https://arxiv.org/abs/1705.10694](https://arxiv.org/abs/1705.10694)\n\n**Input Fast-Forwarding for Better Deep Learning**\n\n- intro: ICIAR 2017\n- keywords: Fast-Forward Network (FFNet)\n- arxiv: [https://arxiv.org/abs/1705.08479](https://arxiv.org/abs/1705.08479)\n\n**Deep Mutual Learning**\n\n- intro: CVPR 2018\n- keywords: deep mutual learning (DML)\n- arxiv: [https://arxiv.org/abs/1706.00384](https://arxiv.org/abs/1706.00384)\n- github(official, TensorFlow): [https://github.com/YingZhangDUT/Deep-Mutual-Learning](https://github.com/YingZhangDUT/Deep-Mutual-Learning)\n\n**Automated Problem Identification: Regression vs Classification via Evolutionary Deep Networks**\n\n- intro: University of Cape Town\n- arxiv: [https://arxiv.org/abs/1707.00703](https://arxiv.org/abs/1707.00703)\n\n**Revisiting Unreasonable Effectiveness of Data in Deep Learning Era**\n\n- intro: Google Research & CMU\n- arxiv: [https://arxiv.org/abs/1707.02968](https://arxiv.org/abs/1707.02968)\n- blog: [https://research.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html](https://research.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)\n\n**Deep Layer Aggregation**\n\n- intro: UC Berkeley\n- arxiv: [https://arxiv.org/abs/1707.06484](https://arxiv.org/abs/1707.06484)\n\n**Improving Robustness of Feature Representations to Image Deformations using Powered Convolution in CNNs**\n\n[https://arxiv.org/abs/1707.07830](https://arxiv.org/abs/1707.07830)\n\n**Learning uncertainty in regression tasks by deep neural networks**\n\n- intro: Free University of Berlin\n- arxiv: [https://arxiv.org/abs/1707.07287](https://arxiv.org/abs/1707.07287)\n\n**Generalizing the Convolution Operator in Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1707.09864](https://arxiv.org/abs/1707.09864)\n\n**Convolution with Logarithmic Filter Groups for Efficient Shallow CNN**\n\n[https://arxiv.org/abs/1707.09855](https://arxiv.org/abs/1707.09855)\n\n**Deep Multi-View Learning with Stochastic Decorrelation Loss**\n\n[https://arxiv.org/abs/1707.09669](https://arxiv.org/abs/1707.09669)\n\n**Take it in your stride: Do we need striding in CNNs?**\n\n[https://arxiv.org/abs/1712.02502](https://arxiv.org/abs/1712.02502)\n\n**Security Risks in Deep Learning Implementation**\n\n- intro: Qihoo 360 Security Research Lab & University of Georgia & University of Virginia\n- arxiv: [https://arxiv.org/abs/1711.11008](https://arxiv.org/abs/1711.11008)\n\n**Online Learning with Gated Linear Networks**\n\n- intro: DeepMind\n- arxiv: [https://arxiv.org/abs/1712.01897](https://arxiv.org/abs/1712.01897)\n\n**On the Information Bottleneck Theory of Deep Learning**\n\n[https://openreview.net/forum?id=ry_WPG-A-&noteId=ry_WPG-A](https://openreview.net/forum?id=ry_WPG-A-&noteId=ry_WPG-A)\n\n**The Unreasonable Effectiveness of Deep Features as a Perceptual Metric**\n\n- project page: [https://richzhang.github.io/PerceptualSimilarity/](https://richzhang.github.io/PerceptualSimilarity/)\n- arxiv: [https://arxiv.org/abs/1801.03924](https://arxiv.org/abs/1801.03924)\n- github: [https://github.com//richzhang/PerceptualSimilarity](https://github.com//richzhang/PerceptualSimilarity)\n\n**Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks**\n\n- intro: University of California, Davis\n- arxiv: [https://arxiv.org/abs/1801.02850](https://arxiv.org/abs/1801.02850)\n\n**Towards an Understanding of Neural Networks in Natural-Image Spaces**\n\n[https://arxiv.org/abs/1801.09097](https://arxiv.org/abs/1801.09097)\n\n**Deep Private-Feature Extraction**\n\n[https://arxiv.org/abs/1802.03151](https://arxiv.org/abs/1802.03151)\n\n**Not All Samples Are Created Equal: Deep Learning with Importance Sampling**\n\n- intro: Idiap Research Institute\n- arxiv: [https://arxiv.org/abs/1803.00942](https://arxiv.org/abs/1803.00942)\n\n**Label Refinery: Improving ImageNet Classification through Label Progression**\n\n- intro: Using a Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNet from 59.3 to 67.2, \n(2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from 50.6 to 55.59, \n(4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to 74.47.\n- intro: XNOR AI, University of Washington, Allen AI\n- arxiv: [https://arxiv.org/abs/1805.02641](https://arxiv.org/abs/1805.02641)\n- github: [https://github.com/hessamb/label-refinery](https://github.com/hessamb/label-refinery)\n\n**How Many Samples are Needed to Learn a Convolutional Neural Network?**\n\n[https://arxiv.org/abs/1805.07883](https://arxiv.org/abs/1805.07883)\n\n**VisualBackProp for learning using privileged information with CNNs**\n\n[https://arxiv.org/abs/1805.09474](https://arxiv.org/abs/1805.09474)\n\n**BAM: Bottleneck Attention Module**\n\n- intro: BMVC 2018 (oral). Lunit Inc. & Adobe Research\n- arxiv: [https://arxiv.org/abs/1807.06514](https://arxiv.org/abs/1807.06514)\n\n**CBAM: Convolutional Block Attention Module**\n\n- intro: ECCV 2018. Lunit Inc. & Adobe Research\n- arxiv: [https://arxiv.org/abs/1807.06521](https://arxiv.org/abs/1807.06521)\n\n**Scale equivariance in CNNs with vector fields**\n\n- intro: ICML/FAIM 2018 workshop on Towards learning with limited labels: Equivariance, Invariance, and Beyond (oral presentation)\n- arxiv: [https://arxiv.org/abs/1807.11783](https://arxiv.org/abs/1807.11783)\n\n**Downsampling leads to Image Memorization in Convolutional Autoencoders**\n\n[https://arxiv.org/abs/1810.10333](https://arxiv.org/abs/1810.10333)\n\n**Do Normalization Layers in a Deep ConvNet Really Need to Be Distinct?**\n\n[https://arxiv.org/abs/1811.07727](https://arxiv.org/abs/1811.07727)\n\n**Are All Training Examples Created Equal? An Empirical Study**\n\n[https://arxiv.org/abs/1811.12569](https://arxiv.org/abs/1811.12569)\n\n**ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness**\n\n[https://arxiv.org/abs/1811.12231](https://arxiv.org/abs/1811.12231)\n\n**DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images**\n\n- intro: The Chinese University of Hong Kong & SenseTime Research\n- keywords: Match R-CNN\n- arxiv: [https://arxiv.org/abs/1901.07973](https://arxiv.org/abs/1901.07973)\n\n**A Comprehensive Overhaul of Feature Distillation**\n\n[https://arxiv.org/abs/1904.01866](https://arxiv.org/abs/1904.01866)\n\n**Mesh R-CNN**\n\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/1906.02739](https://arxiv.org/abs/1906.02739)\n\n**ViP: Virtual Pooling for Accelerating CNN-based Image Classification and Object Detection**\n\n[https://arxiv.org/abs/1906.07912](https://arxiv.org/abs/1906.07912)\n\n**VarGNet: Variable Group Convolutional Neural Network for Efficient Embedded Computing**\n\n- intro: Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1907.05653](https://arxiv.org/abs/1907.05653)\n\n**Anchor Loss: Modulating Loss Scale based on Prediction Difficulty**\n\n- intro: ICCV 2019 oral\n- arxiv: [https://arxiv.org/abs/1909.11155](https://arxiv.org/abs/1909.11155)\n\n**Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem**\n\n- intro: CVPR 2019 oral\n- arxiv: [https://arxiv.org/abs/1812.05720](https://arxiv.org/abs/1812.05720)\n- github: [https://github.com/max-andr/relu_networks_overconfident](https://github.com/max-andr/relu_networks_overconfident)\n\n**Feature Space Augmentation for Long-Tailed Data**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.03673](https://arxiv.org/abs/2008.03673)\n\n## Tutorials and Surveys\n\n**A Survey: Time Travel in Deep Learning Space: An Introduction to Deep Learning Models and How Deep Learning Models Evolved from the Initial Ideas**\n\n- arxiv: [http://arxiv.org/abs/1510.04781](http://arxiv.org/abs/1510.04781)\n\n**On the Origin of Deep Learning**\n\n- intro: CMU. 70 pages, 200 references\n- arxiv: [https://arxiv.org/abs/1702.07800](https://arxiv.org/abs/1702.07800)\n\n**Efficient Processing of Deep Neural Networks: A Tutorial and Survey**\n\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1703.09039](https://arxiv.org/abs/1703.09039)\n\n**The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches**\n\n{https://arxiv.org/abs/1803.01164}(https://arxiv.org/abs/1803.01164)\n\n## Mathematics of Deep Learning\n\n**A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction**\n\n- arxiv: [http://arxiv.org/abs/1512.06293](http://arxiv.org/abs/1512.06293)\n\n**Mathematics of Deep Learning**\n\n- intro: Johns Hopkins University & New York University & Tel-Aviv University & University of California, Los Angeles\n- arxiv: [https://arxiv.org/abs/1712.04741](https://arxiv.org/abs/1712.04741)\n\n## Local Minima\n\n**Local minima in training of deep networks**\n\n- intro: DeepMind\n- arxiv: [https://arxiv.org/abs/1611.06310](https://arxiv.org/abs/1611.06310)\n\n**Deep linear neural networks with arbitrary loss: All local minima are global**\n\n- intro: CMU & University of Southern California & Facebook Artificial Intelligence Research\n- arxiv: [https://arxiv.org/abs/1712.00779](https://arxiv.org/abs/1712.00779)\n\n**Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima**\n\n- intro: Loyola Marymount University & California State University\n- arxiv: [https://arxiv.org/abs/1712.01473](https://arxiv.org/abs/1712.01473)\n\n**CNNs are Globally Optimal Given Multi-Layer Support**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1712.02501](https://arxiv.org/abs/1712.02501)\n\n**Spurious Local Minima are Common in Two-Layer ReLU Neural Networks**\n\n[https://arxiv.org/abs/1712.08968](https://arxiv.org/abs/1712.08968)\n\n## Dive Into CNN\n\n**Structured Receptive Fields in CNNs**\n\n- arxiv: [https://arxiv.org/abs/1605.02971](https://arxiv.org/abs/1605.02971)\n- github: [https://github.com/jhjacobsen/RFNN](https://github.com/jhjacobsen/RFNN)\n\n**How ConvNets model Non-linear Transformations**\n\n- arxiv: [https://arxiv.org/abs/1702.07664](https://arxiv.org/abs/1702.07664)\n\n## Separable Convolutions / Grouped Convolutions\n\n**Factorized Convolutional Neural Networks**\n\n**Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial \"Bottleneck\" Structure**\n\n- arxiv: [http://arxiv.org/abs/1608.04337](http://arxiv.org/abs/1608.04337)\n\n**XSepConv: Extremely Separated Convolution**\n\n- intro: Tsinghua University & University College London\n- arxiv: [https://arxiv.org/abs/2002.12046](https://arxiv.org/abs/2002.12046)\n\n## STDP\n\n**A biological gradient descent for prediction through a combination of STDP and homeostatic plasticity**\n\n- arxiv: [http://arxiv.org/abs/1206.4812](http://arxiv.org/abs/1206.4812)\n\n**An objective function for STDP**\n\n- arxiv: [http://arxiv.org/abs/1509.05936](http://arxiv.org/abs/1509.05936)\n\n**Towards a Biologically Plausible Backprop**\n\n- arxiv: [http://arxiv.org/abs/1602.05179](http://arxiv.org/abs/1602.05179)\n\n## Target Propagation\n\n**How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation**\n\n- arxiv: [http://arxiv.org/abs/1407.7906](http://arxiv.org/abs/1407.7906)\n\n**Difference Target Propagation**\n\n- arxiv: [http://arxiv.org/abs/1412.7525](http://arxiv.org/abs/1412.7525)\n- github: [https://github.com/donghyunlee/dtp](https://github.com/donghyunlee/dtp)\n\n## Zero Shot Learning\n\n**Learning a Deep Embedding Model for Zero-Shot Learning**\n\n- arxiv: [https://arxiv.org/abs/1611.05088](https://arxiv.org/abs/1611.05088)\n\n**Zero-Shot (Deep) Learning**\n\n[https://amundtveit.com/2016/11/18/zero-shot-deep-learning/](https://amundtveit.com/2016/11/18/zero-shot-deep-learning/)\n\n**Zero-shot learning experiments by deep learning.**\n\n[https://github.com/Elyorcv/zsl-deep-learning](https://github.com/Elyorcv/zsl-deep-learning)\n\n**Zero-Shot Learning - The Good, the Bad and the Ugly**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1703.04394](https://arxiv.org/abs/1703.04394)\n\n**Semantic Autoencoder for Zero-Shot Learning**\n\n- intro: CVPR 2017\n- project page: [https://elyorcv.github.io/projects/sae](https://elyorcv.github.io/projects/sae)\n- arxiv: [https://arxiv.org/abs/1704.08345](https://arxiv.org/abs/1704.08345)\n- github: [https://github.com/Elyorcv/SAE](https://github.com/Elyorcv/SAE)\n\n**Zero-Shot Learning via Category-Specific Visual-Semantic Mapping**\n\n[https://arxiv.org/abs/1711.06167](https://arxiv.org/abs/1711.06167)\n\n**Zero-Shot Learning via Class-Conditioned Deep Generative Models**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1711.05820](https://arxiv.org/abs/1711.05820)\n\n**Feature Generating Networks for Zero-Shot Learning**\n\n[https://arxiv.org/abs/1712.00981](https://arxiv.org/abs/1712.00981)\n\n**Zero-Shot Visual Recognition using Semantics-Preserving Adversarial Embedding Network**\n\n[https://arxiv.org/abs/1712.01928](https://arxiv.org/abs/1712.01928)\n\n**Combining Deep Universal Features, Semantic Attributes, and Hierarchical Classification for Zero-Shot Learning**\n\n- intro: extension to work published in conference proceedings of 2017 IAPR MVA Conference\n- arxiv: [https://arxiv.org/abs/1712.03151](https://arxiv.org/abs/1712.03151)\n\n**Multi-Context Label Embedding**\n\n- keywords: Multi-Context Label Embedding (MCLE) \n- arxiv: [https://arxiv.org/abs/1805.01199](https://arxiv.org/abs/1805.01199)\n\n## Incremental Learning\n\n**iCaRL: Incremental Classifier and Representation Learning**\n\n- arxiv: [https://arxiv.org/abs/1611.07725](https://arxiv.org/abs/1611.07725)\n\n**FearNet: Brain-Inspired Model for Incremental Learning**\n\n[https://arxiv.org/abs/1711.10563](https://arxiv.org/abs/1711.10563)\n\n**Incremental Learning in Deep Convolutional Neural Networks Using Partial Network Sharing**\n\n- intro: Purdue University\n- arxiv: [https://arxiv.org/abs/1712.02719](https://arxiv.org/abs/1712.02719)\n\n**Incremental Classifier Learning with Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1802.00853](https://arxiv.org/abs/1802.00853)\n\n**Learn the new, keep the old: Extending pretrained models with new anatomy and images**\n\n- intro: MICCAI 2018\n- arxiv: [https://arxiv.org/abs/1806.00265](https://arxiv.org/abs/1806.00265)\n\n## Ensemble Deep Learning\n\n**Convolutional Neural Fabrics**\n\n- intro: NIPS 2016\n- arxiv: [http://arxiv.org/abs/1606.02492](http://arxiv.org/abs/1606.02492)\n- github: [https://github.com/shreyassaxena/convolutional-neural-fabrics](https://github.com/shreyassaxena/convolutional-neural-fabrics)\n\n**Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles**\n\n- arxiv: [https://arxiv.org/abs/1606.07839](https://arxiv.org/abs/1606.07839)\n- youtube: [https://www.youtube.com/watch?v=KjUfMtZjyfg&feature=youtu.be](https://www.youtube.com/watch?v=KjUfMtZjyfg&feature=youtu.be)\n\n**Snapshot Ensembles: Train 1, Get M for Free**\n\n- paper: [http://openreview.net/pdf?id=BJYwwY9ll](http://openreview.net/pdf?id=BJYwwY9ll)\n- github(Torch): [https://github.com/gaohuang/SnapshotEnsemble](https://github.com/gaohuang/SnapshotEnsemble)\n- github: [https://github.com/titu1994/Snapshot-Ensembles](https://github.com/titu1994/Snapshot-Ensembles)\n\n**Ensemble Deep Learning**\n\n- blog: [https://amundtveit.com/2016/12/02/ensemble-deep-learning/](https://amundtveit.com/2016/12/02/ensemble-deep-learning/)\n\n## Domain Adaptation\n\n**Adversarial Discriminative Domain Adaptation**\n\n- intro: UC Berkeley & Stanford University & Boston University\n- arxiv: [https://arxiv.org/abs/1702.05464](https://arxiv.org/abs/1702.05464)\n- github: [https://github.com//corenel/pytorch-adda](https://github.com//corenel/pytorch-adda)\n\n**Parameter Reference Loss for Unsupervised Domain Adaptation**\n\n[https://arxiv.org/abs/1711.07170](https://arxiv.org/abs/1711.07170)\n\n**Residual Parameter Transfer for Deep Domain Adaptation**\n\n[https://arxiv.org/abs/1711.07714](https://arxiv.org/abs/1711.07714)\n\n**Adversarial Feature Augmentation for Unsupervised Domain Adaptation**\n\n[https://arxiv.org/abs/1711.08561](https://arxiv.org/abs/1711.08561)\n\n**Image to Image Translation for Domain Adaptation**\n\n[https://arxiv.org/abs/1712.00479](https://arxiv.org/abs/1712.00479)\n\n**Incremental Adversarial Domain Adaptation**\n\n[https://arxiv.org/abs/1712.07436](https://arxiv.org/abs/1712.07436)\n\n**Deep Visual Domain Adaptation: A Survey**\n\n[https://arxiv.org/abs/1802.03601](https://arxiv.org/abs/1802.03601)\n\n**Unsupervised Domain Adaptation: A Multi-task Learning-based Method**\n\n[https://arxiv.org/abs/1803.09208](https://arxiv.org/abs/1803.09208)\n\n**Importance Weighted Adversarial Nets for Partial Domain Adaptation**\n\n[https://arxiv.org/abs/1803.09210](https://arxiv.org/abs/1803.09210)\n\n**Open Set Domain Adaptation by Backpropagation**\n\n[https://arxiv.org/abs/1804.10427](https://arxiv.org/abs/1804.10427)\n\n**Learning Sampling Policies for Domain Adaptation**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1805.07641](https://arxiv.org/abs/1805.07641)\n\n**Multi-Adversarial Domain Adaptation**\n\n- intro: AAAI 2018 Oral.\n- arxiv: [https://arxiv.org/abs/1809.02176](https://arxiv.org/abs/1809.02176)\n\n**Unsupervised Domain Adaptation: An Adaptive Feature Norm Approach**\n\n- intro: Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/1811.07456](https://arxiv.org/abs/1811.07456)\n- github: [https://github.com/jihanyang/AFN/](https://github.com/jihanyang/AFN/)\n\n**Multi-source Distilling Domain Adaptation**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.11554](https://arxiv.org/abs/1911.11554)\n- github: [https://github.com/daoyuan98/MDDA](https://github.com/daoyuan98/MDDA)\n\n**awsome-domain-adaptation**\n\n[https://github.com/zhaoxin94/awsome-domain-adaptation](https://github.com/zhaoxin94/awsome-domain-adaptation)\n\n## Embedding\n\n**Learning Deep Embeddings with Histogram Loss**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1611.00822](https://arxiv.org/abs/1611.00822)\n\n**Full-Network Embedding in a Multimodal Embedding Pipeline**\n\n[https://arxiv.org/abs/1707.09872](https://arxiv.org/abs/1707.09872)\n\n**Clustering-driven Deep Embedding with Pairwise Constraints**\n\n[https://arxiv.org/abs/1803.08457](https://arxiv.org/abs/1803.08457)\n\n**Deep Mixture of Experts via Shallow Embedding**\n\n[https://arxiv.org/abs/1806.01531](https://arxiv.org/abs/1806.01531)\n\n**Learning to Learn from Web Data through Deep Semantic Embeddings**\n\n- intro: ECCV MULA Workshop 2018\n- arxiv: [https://arxiv.org/abs/1808.06368](https://arxiv.org/abs/1808.06368)\n\n**Heated-Up Softmax Embedding**\n\n[https://arxiv.org/abs/1809.04157](https://arxiv.org/abs/1809.04157)\n\n**Virtual Class Enhanced Discriminative Embedding Learning**\n\n- intro: NeurIPS 2018\n- arxiv: [https://arxiv.org/abs/1811.12611](https://arxiv.org/abs/1811.12611)\n\n## Regression\n\n**A Comprehensive Analysis of Deep Regression**\n\n[https://arxiv.org/abs/1803.08450](https://arxiv.org/abs/1803.08450)\n\n**Neural Motifs: Scene Graph Parsing with Global Context**\n\n- intro: CVPR 2018. University of Washington\n- project page: [http://rowanzellers.com/neuralmotifs/](http://rowanzellers.com/neuralmotifs/)\n- arxiv: [https://arxiv.org/abs/1711.06640](https://arxiv.org/abs/1711.06640)\n- github: [https://github.com/rowanz/neural-motifs](https://github.com/rowanz/neural-motifs)\n- demo: [https://rowanzellers.com/scenegraph2/](https://rowanzellers.com/scenegraph2/)\n\n## CapsNets\n\n**Dynamic Routing Between Capsules**\n\n- intro: Sara Sabour, Nicholas Frosst, Geoffrey E Hinton\n- intro: Google Brain, Toronto\n- arxiv: [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)\n- github(official, Tensorflow): [https://github.com/Sarasra/models/tree/master/research/capsules](https://github.com/Sarasra/models/tree/master/research/capsules)\n\n**Capsule Networks (CapsNets) – Tutorial**\n\n- youtube: [https://www.youtube.com/watch?v=pPN8d0E3900](https://www.youtube.com/watch?v=pPN8d0E3900)\n- mirror: [http://www.bilibili.com/video/av16594836/](http://www.bilibili.com/video/av16594836/)\n\n**Improved Explainability of Capsule Networks: Relevance Path by Agreement**\n\n- intro: Concordia University & University of Toronto\n- arxiv: [https://arxiv.org/abs/1802.10204](https://arxiv.org/abs/1802.10204)\n\n## Low Light\n\n**Exploring Image Enhancement for Salient Object Detection in Low Light Images**\n\n- intro: ACM Transactions on Multimedia Computing, Communications, and Applications\n- arxiv: [https://arxiv.org/abs/2007.16124](https://arxiv.org/abs/2007.16124)\n\n**NOD: Taking a Closer Look at Detection under Extreme Low-Light Conditions with Night Object Detection Dataset**\n\n- intro: BMVC 2021\n- arxiv: [https://arxiv.org/abs/2110.10364](https://arxiv.org/abs/2110.10364)\n\n## Computer Vision\n\n**A Taxonomy of Deep Convolutional Neural Nets for Computer Vision**\n\n- arxiv: [http://arxiv.org/abs/1601.06615](http://arxiv.org/abs/1601.06615)\n\n**On the usability of deep networks for object-based image analysis**\n\n- intro: GEOBIA 2016\n- arxiv: [http://arxiv.org/abs/1609.06845](http://arxiv.org/abs/1609.06845)\n\n**Learning Recursive Filters for Low-Level Vision via a Hybrid Neural Network**\n\n- intro: ECCV 2016\n- project page: [http://www.sifeiliu.net/linear-rnn](http://www.sifeiliu.net/linear-rnn)\n- paper: [http://faculty.ucmerced.edu/mhyang/papers/eccv16_rnn_filter.pdf](http://faculty.ucmerced.edu/mhyang/papers/eccv16_rnn_filter.pdf)\n- poster: [http://www.eccv2016.org/files/posters/O-3A-03.pdf](http://www.eccv2016.org/files/posters/O-3A-03.pdf)\n- github: [https://github.com/Liusifei/caffe-lowlevel](https://github.com/Liusifei/caffe-lowlevel)\n\n**Toward Geometric Deep SLAM**\n\n- intro: Magic Leap, Inc\n- arxiv: [https://arxiv.org/abs/1707.07410](https://arxiv.org/abs/1707.07410)\n\n**Learning Dual Convolutional Neural Networks for Low-Level Vision**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1805.05020](https://arxiv.org/abs/1805.05020)\n\n**Not just a matter of semantics: the relationship between visual similarity and semantic similarity**\n\n[https://arxiv.org/abs/1811.07120](https://arxiv.org/abs/1811.07120)\n\n**DF-SLAM: A Deep-Learning Enhanced Visual SLAM System based on Deep Local Features**\n\n- intro: BUPT & Megvii\n- arxiv: [https://arxiv.org/abs/1901.07223](https://arxiv.org/abs/1901.07223)\n\n**GN-Net: The Gauss-Newton Loss for Deep Direct SLAM**\n\n- intro: Technical University of Munich & Artisense\n- arxiv: [https://arxiv.org/abs/1904.11932](https://arxiv.org/abs/1904.11932)\n\n### All-In-One Network\n\n**HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition**\n\n- arxiv: [https://arxiv.org/abs/1603.01249](https://arxiv.org/abs/1603.01249)\n- summary: [https://github.com/aleju/papers/blob/master/neural-nets/HyperFace.md](https://github.com/aleju/papers/blob/master/neural-nets/HyperFace.md)\n\n**UberNet: Training a `Universal' Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory**\n\n- arxiv: [http://arxiv.org/abs/1609.02132](http://arxiv.org/abs/1609.02132)\n- demo: [http://cvn.ecp.fr/ubernet/](http://cvn.ecp.fr/ubernet/)\n\n**An All-In-One Convolutional Neural Network for Face Analysis**\n\n- intro: simultaneous face detection, face alignment, pose estimation, gender recognition, smile detection, age estimation and face recognition \n- arxiv: [https://arxiv.org/abs/1611.00851](https://arxiv.org/abs/1611.00851)\n\n**MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving**\n\n- intro: first place on Kitti Road Segmentation. \njoint classification, detection and semantic segmentation via a unified architecture, less than 100 ms to perform all tasks\n- arxiv: [https://arxiv.org/abs/1612.07695](https://arxiv.org/abs/1612.07695)\n- github: [https://github.com/MarvinTeichmann/MultiNet](https://github.com/MarvinTeichmann/MultiNet)\n\n**Adversarial Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation**\n\n[https://arxiv.org/abs/1805.09806](https://arxiv.org/abs/1805.09806)\n\n**Visual Person Understanding through Multi-Task and Multi-Dataset Learning**\n\n- intro: RWTH Aachen University\n- arxiv: [https://arxiv.org/abs/1906.03019](https://arxiv.org/abs/1906.03019)\n\n### Deep Learning for Data Structures\n\n**The Case for Learned Index Structures**\n\n- intro: MIT & Google\n- keywords: B-Tree-Index, Hash-Index, BitMap-Index\n- arxiv: [https://arxiv.org/abs/1712.01208](https://arxiv.org/abs/1712.01208)\n\n# Projects\n\n**Top Deep Learning Projects**\n\n- github: [https://github.com/aymericdamien/TopDeepLearning](https://github.com/aymericdamien/TopDeepLearning)\n\n**deepnet: Implementation of some deep learning algorithms**\n\n- github: [https://github.com/nitishsrivastava/deepnet](https://github.com/nitishsrivastava/deepnet)\n\n**DeepNeuralClassifier(Julia): Deep neural network using rectified linear units to classify hand written digits from the MNIST dataset**\n\n- github: [https://github.com/jostmey/DeepNeuralClassifier](https://github.com/jostmey/DeepNeuralClassifier)\n\n**Clarifai Node.js Demo**\n\n- github: [https://github.com/patcat/Clarifai-Node-Demo](https://github.com/patcat/Clarifai-Node-Demo)\n- blog(\"How to Make Your Web App Smarter with Image Recognition\"): [http://www.sitepoint.com/how-to-make-your-web-app-smarter-with-image-recognition/](http://www.sitepoint.com/how-to-make-your-web-app-smarter-with-image-recognition/)\n\n**Deep Learning in Rust**\n\n- blog(\"baby steps\"): [https://medium.com/@tedsta/deep-learning-in-rust-7e228107cccc#.t0pskuwkm](https://medium.com/@tedsta/deep-learning-in-rust-7e228107cccc#.t0pskuwkm)\n- blog(\"a walk in the park\"): [https://medium.com/@tedsta/deep-learning-in-rust-a-walk-in-the-park-fed6c87165ea#.pucj1l5yx](https://medium.com/@tedsta/deep-learning-in-rust-a-walk-in-the-park-fed6c87165ea#.pucj1l5yx)\n- github: [https://github.com/tedsta/deeplearn-rs](https://github.com/tedsta/deeplearn-rs)\n\n**Implementation of state-of-art models in Torch**\n\n- github: [https://github.com/aciditeam/torch-models](https://github.com/aciditeam/torch-models)\n\n**Deep Learning (Python, C, C++, Java, Scala, Go)**\n\n- github: [https://github.com/yusugomori/DeepLearning](https://github.com/yusugomori/DeepLearning)\n\n**deepmark: THE Deep Learning Benchmarks**\n\n- github: [https://github.com/DeepMark/deepmark](https://github.com/DeepMark/deepmark)\n\n**Siamese Net**\n\n- intro: \"This package shows how to train a siamese network using Lasagne and Theano and includes network definitions \nfor state-of-the-art networks including: DeepID, DeepID2, Chopra et. al, and Hani et. al. \nWe also include one pre-trained model using a custom convolutional network.\"\n- github: [https://github.com/Kadenze/siamese_net](https://github.com/Kadenze/siamese_net)\n\n**PRE-TRAINED CONVNETS AND OBJECT LOCALISATION IN KERAS**\n\n- blog: [https://blog.heuritech.com/2016/04/26/pre-trained-convnets-and-object-localisation-in-keras/](https://blog.heuritech.com/2016/04/26/pre-trained-convnets-and-object-localisation-in-keras/)\n- github: [https://github.com/heuritech/convnets-keras](https://github.com/heuritech/convnets-keras)\n\n**Deep Learning algorithms with TensorFlow: Ready to use implementations of various Deep Learning algorithms using TensorFlow**\n\n- homepage: [http://www.gabrieleangeletti.com/](http://www.gabrieleangeletti.com/)\n- github: [https://github.com/blackecho/Deep-Learning-TensorFlow](https://github.com/blackecho/Deep-Learning-TensorFlow)\n\n**Fast Multi-threaded VGG 19 Feature Extractor**\n\n- github: [https://github.com/coreylynch/vgg-19-feature-extractor](https://github.com/coreylynch/vgg-19-feature-extractor)\n\n**Live demo of neural network classifying images**\n\n![](/assets/cnn-materials/nn_classify_images_live_demo.jpg)\n\n[http://ml4a.github.io/dev/demos/cifar_confusion.html#](http://ml4a.github.io/dev/demos/cifar_confusion.html#)\n\n**mojo cnn: c++ convolutional neural network**\n\n- intro: the fast and easy header only c++ convolutional neural network package\n- github: [https://github.com/gnawice/mojo-cnn](https://github.com/gnawice/mojo-cnn)\n\n**DeepHeart: Neural networks for monitoring cardiac data**\n\n- github: [https://github.com/jisaacso/DeepHeart](https://github.com/jisaacso/DeepHeart)\n\n**Deep Water: Deep Learning in H2O using Native GPU Backends**\n\n- intro: Native implementation of Deep Learning models for GPU backends (mxnet, Caffe, TensorFlow, etc.)\n- github: [https://github.com/h2oai/deepwater](https://github.com/h2oai/deepwater)\n\n**Greentea LibDNN: Greentea LibDNN - a universal convolution implementation supporting CUDA and OpenCL**\n\n- github: [https://github.com/naibaf7/libdnn](https://github.com/naibaf7/libdnn)\n\n**Dracula: A spookily good Part of Speech Tagger optimized for Twitter**\n\n- intro: A deep, LSTM-based part of speech tagger and sentiment analyser using character embeddings instead of words. \nCompatible with Theano and TensorFlow. Optimized for Twitter.\n- homepage: [http://dracula.sentimentron.co.uk/](http://dracula.sentimentron.co.uk/)\n- speech tagging demo: [http://dracula.sentimentron.co.uk/pos-demo/](http://dracula.sentimentron.co.uk/pos-demo/)\n- sentiment demo: [http://dracula.sentimentron.co.uk/sentiment-demo/](http://dracula.sentimentron.co.uk/sentiment-demo/)\n- github: [https://github.com/Sentimentron/Dracula](https://github.com/Sentimentron/Dracula)\n\n**Trained image classification models for Keras**\n\n- intro: Keras code and weights files for popular deep learning models.\n- intro: VGG16, VGG19, ResNet50, Inception v3\n- github: [https://github.com/fchollet/deep-learning-models](https://github.com/fchollet/deep-learning-models)\n\n**PyCNN: Cellular Neural Networks Image Processing Python Library**\n\n![](https://camo.githubusercontent.com/0c5fd234a144b3d2145a133466766b2ecd9d3f3c/687474703a2f2f7777772e6973697765622e65652e6574687a2e63682f6861656e6767692f434e4e5f7765622f434e4e5f666967757265732f626c6f636b6469616772616d2e676966)\n\n- blog: [http://blog.ankitaggarwal.me/PyCNN/](http://blog.ankitaggarwal.me/PyCNN/)\n- github: [https://github.com/ankitaggarwal011/PyCNN](https://github.com/ankitaggarwal011/PyCNN)\n\n**regl-cnn: Digit recognition with Convolutional Neural Networks in WebGL**\n\n- intro: TensorFlow, WebGL, [regl](https://github.com/mikolalysenko/regl)\n- github: [https://github.com/Erkaman/regl-cnn/](https://github.com/Erkaman/regl-cnn/)\n- demo: [https://erkaman.github.io/regl-cnn/src/demo.html](https://erkaman.github.io/regl-cnn/src/demo.html)\n\n**dagstudio: Directed Acyclic Graph Studio with Javascript D3**\n\n![](https://raw.githubusercontent.com/TimZaman/dagstudio/master/misc/20160907_dagstudio_ex.gif)\n\n- github: [https://github.com/TimZaman/dagstudio](https://github.com/TimZaman/dagstudio)\n\n**NEUGO: Neural Networks in Go**\n\n- github: [https://github.com/wh1t3w01f/neugo](https://github.com/wh1t3w01f/neugo)\n\n**gvnn: Neural Network Library for Geometric Computer Vision**\n\n- arxiv: [http://arxiv.org/abs/1607.07405](http://arxiv.org/abs/1607.07405)\n- github: [https://github.com/ankurhanda/gvnn](https://github.com/ankurhanda/gvnn)\n\n**DeepForge: A development environment for deep learning**\n\n- github: [https://github.com/dfst/deepforge](https://github.com/dfst/deepforge)\n\n**Implementation of recent Deep Learning papers**\n\n- intro: DenseNet / DeconvNet / DenseRecNet\n- github: [https://github.com/tdeboissiere/DeepLearningImplementations](https://github.com/tdeboissiere/DeepLearningImplementations)\n\n**GPU-accelerated Theano & Keras on Windows 10 native**\n\n- github: [https://github.com/philferriere/dlwin](https://github.com/philferriere/dlwin)\n\n**Head Pose and Gaze Direction Estimation Using Convolutional Neural Networks**\n\n- github: [https://github.com/mpatacchiola/deepgaze](https://github.com/mpatacchiola/deepgaze)\n\n**Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)**\n\n- homepage: [https://01.org/mkl-dnn](https://01.org/mkl-dnn)\n- github: [https://github.com/01org/mkl-dnn](https://github.com/01org/mkl-dnn)\n\n**Deep CNN and RNN - Deep convolution/recurrent neural network project with TensorFlow**\n\n- github: [https://github.com/tobegit3hub/deep_cnn](https://github.com/tobegit3hub/deep_cnn)\n\n**Experimental implementation of novel neural network structures**\n\n- intro: binarynet / ternarynet / qrnn / vae / gcnn\n- github: [https://github.com/DingKe/nn_playground](https://github.com/DingKe/nn_playground)\n\n**WaterNet: A convolutional neural network that identifies water in satellite images**\n\n- github: [https://github.com/treigerm/WaterNet](https://github.com/treigerm/WaterNet)\n\n**Kur: Descriptive Deep Learning**\n\n- github: [https://github.com/deepgram/kur](https://github.com/deepgram/kur)\n- docs: [http://kur.deepgram.com/](http://kur.deepgram.com/)\n\n**Development of JavaScript-based deep learning platform and application to distributed training**\n\n- intro: Workshop paper for ICLR2017\n- arxiv: [https://arxiv.org/abs/1702.01846](https://arxiv.org/abs/1702.01846)\n- github: [https://github.com/mil-tokyo](https://github.com/mil-tokyo)\n\n**NewralNet**\n\n- intro: A lightweight, easy to use and open source Java library for experimenting with\nfeed-forward neural nets and deep learning.\n- gitlab: [https://gitlab.com/flimmerkiste/NewralNet](https://gitlab.com/flimmerkiste/NewralNet)\n\n**FeatherCNN**\n\n- intro: FeatherCNN is a high performance inference engine for convolutional neural networks\n- github: [https://github.com/Tencent/FeatherCNN](https://github.com/Tencent/FeatherCNN)\n\n# Readings and Questions\n\n**What you wanted to know about AI**\n\n[http://fastml.com/what-you-wanted-to-know-about-ai/](http://fastml.com/what-you-wanted-to-know-about-ai/)\n\n**Epoch vs iteration when training neural networks**\n\n- stackoverflow: [http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks](http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks)\n\n**Questions to Ask When Applying Deep Learning**\n\n[http://deeplearning4j.org/questions.html](http://deeplearning4j.org/questions.html)\n\n**How can I know if Deep Learning works better for a specific problem than SVM or random forest?**\n\n- github: [https://github.com/rasbt/python-machine-learning-book/blob/master/faq/deeplearn-vs-svm-randomforest.md](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/deeplearn-vs-svm-randomforest.md)\n\n**What is the difference between deep learning and usual machine learning?**\n\n- note: [https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md)\n\n# Resources\n\n**Awesome Deep Learning**\n\n- github: [https://github.com/ChristosChristofidis/awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning)\n\n**Awesome-deep-vision: A curated list of deep learning resources for computer vision**\n\n- website: [http://jiwonkim.org/awesome-deep-vision/](http://jiwonkim.org/awesome-deep-vision/)\n- github: [https://github.com/kjw0612/awesome-deep-vision](https://github.com/kjw0612/awesome-deep-vision)\n\n**Applied Deep Learning Resources: A collection of research articles, blog posts, slides and code snippets about deep learning in applied settings.**\n\n- github: [https://github.com/kristjankorjus/applied-deep-learning-resources](https://github.com/kristjankorjus/applied-deep-learning-resources)\n\n**Deep Learning Libraries by Language**\n\n- website: [http://www.teglor.com/b/deep-learning-libraries-language-cm569/](http://www.teglor.com/b/deep-learning-libraries-language-cm569/)\n\n**Deep Learning Resources**\n\n[http://yanirseroussi.com/deep-learning-resources/](http://yanirseroussi.com/deep-learning-resources/)\n\n**Deep Learning Resources**\n\n[https://omtcyfz.github.io/2016/08/29/Deep-Learning-Resources.html](https://omtcyfz.github.io/2016/08/29/Deep-Learning-Resources.html)\n\n**Turing Machine: musings on theory & code(DEEP LEARNING REVOLUTION, summer 2015, state of the art & topnotch links)**\n\n[https://vzn1.wordpress.com/2015/09/01/deep-learning-revolution-summer-2015-state-of-the-art-topnotch-links/](https://vzn1.wordpress.com/2015/09/01/deep-learning-revolution-summer-2015-state-of-the-art-topnotch-links/)\n\n**BICV Group: Biologically Inspired Computer Vision research group**\n\n[http://www.bicv.org/deep-learning/](http://www.bicv.org/deep-learning/)\n\n**Learning Deep Learning**\n\n[http://rt.dgyblog.com/ref/ref-learning-deep-learning.html](http://rt.dgyblog.com/ref/ref-learning-deep-learning.html)\n\n**Summaries and notes on Deep Learning research papers**\n\n- github: [https://github.com/dennybritz/deeplearning-papernotes](https://github.com/dennybritz/deeplearning-papernotes)\n\n**Deep Learning Glossary**\n\n- intro: \"Simple, opinionated explanations of various things encountered in Deep Learning / AI / ML.\"\n- author: Ryan Dahl, author of NodeJS. \n- github: [https://github.com/ry/deep_learning_glossary](https://github.com/ry/deep_learning_glossary)\n\n**The Deep Learning Playbook**\n\n[https://medium.com/@jiefeng/deep-learning-playbook-c5ebe34f8a1a#.eg9cdz5ak](https://medium.com/@jiefeng/deep-learning-playbook-c5ebe34f8a1a#.eg9cdz5ak)\n\n**Deep Learning Study: Study of HeXA@UNIST in Preparation for Submission**\n\n- github: [https://github.com/carpedm20/deep-learning-study](https://github.com/carpedm20/deep-learning-study)\n\n**Deep Learning Books**\n\n- blog: [http://machinelearningmastery.com/deep-learning-books/](http://machinelearningmastery.com/deep-learning-books/)\n\n**awesome-very-deep-learning: A curated list of papers and code about very deep neural networks (50+ layers)**\n\n- github: [https://github.com/daviddao/awesome-very-deep-learning](https://github.com/daviddao/awesome-very-deep-learning)\n\n**Deep Learning Resources and Tutorials using Keras and Lasagne**\n\n- github: [https://github.com/Vict0rSch/deep_learning](https://github.com/Vict0rSch/deep_learning)\n\n**Deep Learning: Definition, Resources, Comparison with Machine Learning**\n\n- blog: [http://www.datasciencecentral.com/profiles/blogs/deep-learning-definition-resources-comparison-with-machine-learni](http://www.datasciencecentral.com/profiles/blogs/deep-learning-definition-resources-comparison-with-machine-learni)\n\n**Awesome - Most Cited Deep Learning Papers**\n\n- github: [https://github.com/terryum/awesome-deep-learning-papers](https://github.com/terryum/awesome-deep-learning-papers)\n\n**The most cited papers in computer vision and deep learning**\n\n- blog: [https://computervisionblog.wordpress.com/2016/06/19/the-most-cited-papers-in-computer-vision-and-deep-learning/](https://computervisionblog.wordpress.com/2016/06/19/the-most-cited-papers-in-computer-vision-and-deep-learning/)\n\n**deep learning papers: A place to collect papers that are related to deep learning and computational biology**\n\n- github: [https://github.com/pimentel/deep_learning_papers](https://github.com/pimentel/deep_learning_papers)\n\n**papers-I-read**\n\n- intro: \"I am trying a new initiative - a-paper-a-week. This repository will hold all those papers and related summaries and notes.\"\n- github: [https://github.com/shagunsodhani/papers-I-read](https://github.com/shagunsodhani/papers-I-read)\n\n**LEARNING DEEP LEARNING - MY TOP-FIVE LIST**\n\n- blog: [http://thegrandjanitor.com/2016/08/15/learning-deep-learning-my-top-five-resource/](http://thegrandjanitor.com/2016/08/15/learning-deep-learning-my-top-five-resource/)\n\n**awesome-free-deep-learning-papers**\n\n- github: [https://github.com/HFTrader/awesome-free-deep-learning-papers](https://github.com/HFTrader/awesome-free-deep-learning-papers)\n\n**DeepLearningBibliography: Bibliography for Publications about Deep Learning using GPU**\n\n- homepage: [http://memkite.com/deep-learning-bibliography/](http://memkite.com/deep-learning-bibliography/)\n- github: [https://github.com/memkite/DeepLearningBibliography](https://github.com/memkite/DeepLearningBibliography)\n\n**Deep Learning Papers Reading Roadmap**\n\n- github: [https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)\n\n**deep-learning-papers**\n\n- intro: Papers about deep learning ordered by task, date. Current state-of-the-art papers are labelled.\n- github: [https://github.com/sbrugman/deep-learning-papers/blob/master/README.md](https://github.com/sbrugman/deep-learning-papers/blob/master/README.md)\n\n**Deep Learning and applications in Startups, CV, Text Mining, NLP**\n\n- github: [https://github.com/lipiji/app-dl](https://github.com/lipiji/app-dl)\n\n**ml4a-guides - a collection of practical resources for working with machine learning software, including code and tutorials**\n\n[http://ml4a.github.io/guides/](http://ml4a.github.io/guides/)\n\n**deep-learning-resources**\n\n- intro: A Collection of resources I have found useful on my journey finding my way through the world of Deep Learning.\n- github: [https://github.com/chasingbob/deep-learning-resources](https://github.com/chasingbob/deep-learning-resources)\n\n**21 Deep Learning Videos, Tutorials & Courses on Youtube from 2016**\n\n[https://www.analyticsvidhya.com/blog/2016/12/21-deep-learning-videos-tutorials-courses-on-youtube-from-2016/](https://www.analyticsvidhya.com/blog/2016/12/21-deep-learning-videos-tutorials-courses-on-youtube-from-2016/)\n\n**Awesome Deep learning papers and other resources**\n\n- github: [https://github.com/endymecy/awesome-deeplearning-resources](https://github.com/endymecy/awesome-deeplearning-resources)\n\n**awesome-deep-vision-web-demo**\n\n- intro: A curated list of awesome deep vision web demo\n- github: [https://github.com/hwalsuklee/awesome-deep-vision-web-demo](https://github.com/hwalsuklee/awesome-deep-vision-web-demo)\n\n**Summaries of machine learning papers**\n\n[https://github.com/aleju/papers](https://github.com/aleju/papers)\n\n**Awesome Deep Learning Resources**\n\n[https://github.com/guillaume-chevalier/awesome-deep-learning-resources](https://github.com/guillaume-chevalier/awesome-deep-learning-resources)\n\n**Virginia Tech Vision and Learning Reading Group**\n\n[https://github.com//vt-vl-lab/reading_group](https://github.com//vt-vl-lab/reading_group)\n\n**MEGALODON: ML/DL Resources At One Place**\n\n- intro: Various ML/DL Resources organised at a single place.\n- arxiv: [https://github.com//vyraun/Megalodon](https://github.com//vyraun/Megalodon)\n\n## Arxiv Pages\n\n**Neural and Evolutionary Computing**\n\n[https://arxiv.org/list/cs.NE/recent](https://arxiv.org/list/cs.NE/recent)\n\n**Learning**\n\n[https://arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent)\n\n**Computer Vision and Pattern Recognition**\n\n[https://arxiv.org/list/cs.CV/recent](https://arxiv.org/list/cs.CV/recent)\n\n## Arxiv Sanity Preserver\n\n- intro: Built by @karpathy to accelerate research.\n- page: [http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)\n\n**Today's Deep Learning**\n\n[http://todaysdeeplearning.com/](http://todaysdeeplearning.com/)\n\n**arXiv Analytics**\n\n[http://arxitics.com/](http://arxitics.com/)\n\n## Papers with Code\n\n**Papers with Code**\n\n[https://paperswithcode.com/](https://paperswithcode.com/)\n\n# Tools\n\n**DNNGraph - A deep neural network model generation DSL in Haskell**\n\n- homepage: [http://ajtulloch.github.io/dnngraph/](http://ajtulloch.github.io/dnngraph/)\n\n**Deep playground: an interactive visualization of neural networks, written in typescript using d3.js**\n\n- homepage: [http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.23990&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.23990&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification)\n- github: [https://github.com/tensorflow/playground](https://github.com/tensorflow/playground)\n\n**Neural Network Package**\n\n- intro: This package provides an easy and modular way to build and train simple or complex neural networks using Torch\n- github: [https://github.com/torch/nn](https://github.com/torch/nn)\n\n**deepdish: Deep learning and data science tools from the University of Chicago**\n**deepdish: Serving Up Chicago-Style Deep Learning**\n\n- homepage: [http://deepdish.io/](http://deepdish.io/)\n- github: [https://github.com/uchicago-cs/deepdish](https://github.com/uchicago-cs/deepdish)\n\n**AETROS CLI: Console application to manage deep neural network training in AETROS Trainer**\n\n- intro: Create, train and monitor deep neural networks using a model designer.\n- homepage: [http://aetros.com/](http://aetros.com/)\n- github: [https://github.com/aetros/aetros-cli](https://github.com/aetros/aetros-cli)\n\n**Deep Learning Studio: Cloud platform for designing Deep Learning AI without programming**\n\n[http://deepcognition.ai/](http://deepcognition.ai/)\n\n**cuda-on-cl: Build NVIDIA® CUDA™ code for OpenCL™ 1.2 devices**\n\n- github: [https://github.com/hughperkins/cuda-on-cl](https://github.com/hughperkins/cuda-on-cl)\n\n**Receptive Field Calculator**\n\n- homepage: [http://fomoro.com/tools/receptive-fields/](http://fomoro.com/tools/receptive-fields/)\n- example: [http://fomoro.com/tools/receptive-fields/#3,1,1,VALID;3,1,1,VALID;3,1,1,VALID](http://fomoro.com/tools/receptive-fields/#3,1,1,VALID;3,1,1,VALID;3,1,1,VALID)\n\n**receptivefield**\n\n- intro: (PyTorch/Keras/TensorFlow)Gradient based receptive field estimation for Convolutional Neural Networks\n- github: [https://github.com//fornaxai/receptivefield](https://github.com//fornaxai/receptivefield)\n\n# Challenges / Hackathons\n\n**Open Images Challenge 2018**\n\n[https://storage.googleapis.com/openimages/web/challenge.html](https://storage.googleapis.com/openimages/web/challenge.html)\n\n**VisionHack 2017**\n\n- intro: 10 - 14 Sep 2017, Moscow, Russia\n- intro: a full-fledged hackathon that will last three full days\n- homepage: [http://visionhack.misis.ru/](http://visionhack.misis.ru/)\n\n**NVIDIA AI City Challenge Workshop at CVPR 2018**\n\n[http://www.aicitychallenge.org/](http://www.aicitychallenge.org/)\n\n# Books\n\n**Deep Learning**\n\n- author: Ian Goodfellow, Aaron Courville and Yoshua Bengio\n- homepage: [http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)\n- website: [http://goodfeli.github.io/dlbook/](http://goodfeli.github.io/dlbook/)\n- github: [https://github.com/HFTrader/DeepLearningBook](https://github.com/HFTrader/DeepLearningBook)\n- notes(\"Deep Learning for Beginners\"): [http://randomekek.github.io/deep/deeplearning.html](http://randomekek.github.io/deep/deeplearning.html)\n\n**Fundamentals of Deep Learning: Designing Next-Generation Artificial Intelligence Algorithms**\n\n- author: Nikhil Buduma\n- book review: [http://www.opengardensblog.futuretext.com/archives/2015/08/book-review-fundamentals-of-deep-learning-designing-next-generation-artificial-intelligence-algorithms-by-nikhil-buduma.html](http://www.opengardensblog.futuretext.com/archives/2015/08/book-review-fundamentals-of-deep-learning-designing-next-generation-artificial-intelligence-algorithms-by-nikhil-buduma.html)\n- github: [https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book](https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book)\n\n**FIRST CONTACT WITH TENSORFLOW: Get started with with Deep Learning programming**\n\n- author: Jordi Torres\n- book: [http://www.jorditorres.org/first-contact-with-tensorflow/](http://www.jorditorres.org/first-contact-with-tensorflow/)\n\n**《解析卷积神经网络—深度学习实践手册》**\n\n- intro: by 魏秀参（Xiu-Shen WEI）\n- homepage: [http://lamda.nju.edu.cn/weixs/book/CNN_book.html](http://lamda.nju.edu.cn/weixs/book/CNN_book.html)\n\n**Make Your Own Neural Network: IPython Neural Networks on a Raspberry Pi Zero**\n\n- book: [http://makeyourownneuralnetwork.blogspot.jp/2016/03/ipython-neural-networks-on-raspberry-pi.html](http://makeyourownneuralnetwork.blogspot.jp/2016/03/ipython-neural-networks-on-raspberry-pi.html)\n- github: [https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork](https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork)\n\n# Blogs\n\n**Neural Networks and Deep Learning**\n\n[http://neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)\n\n**Deep Learning Reading List**\n\n[http://deeplearning.net/reading-list/](http://deeplearning.net/reading-list/)\n\n**WILDML: A BLOG ABOUT MACHINE LEARNING, DEEP LEARNING AND NLP.**\n\n[http://www.wildml.com/](http://www.wildml.com/)\n\n**Andrej Karpathy blog**\n\n[http://karpathy.github.io/](http://karpathy.github.io/)\n\n**Rodrigob's github page**\n\n[http://rodrigob.github.io/](http://rodrigob.github.io/)\n\n**colah's blog**\n\n[http://colah.github.io/](http://colah.github.io/)\n\n**What My Deep Model Doesn't Know...**\n\n[http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)\n\n**Christoph Feichtenhofer**\n\n- intro: PhD Student, Graz University of Technology\n- homepage: [http://feichtenhofer.github.io/](http://feichtenhofer.github.io/)\n\n**Image recognition is not enough: As with language, photos need contextual intelligence**\n\n[https://medium.com/@ken_getquik/image-recognition-is-not-enough-293cd7d58004#.dex817l2z](https://medium.com/@ken_getquik/image-recognition-is-not-enough-293cd7d58004#.dex817l2z)\n\n**ResNets, HighwayNets, and DenseNets, Oh My!**\n\n- blog: [https://medium.com/@awjuliani/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32#.pgltg8pro](https://medium.com/@awjuliani/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32#.pgltg8pro)\n- github: [https://github.com/awjuliani/TF-Tutorials/blob/master/Deep%20Network%20Comparison.ipynb](https://github.com/awjuliani/TF-Tutorials/blob/master/Deep%20Network%20Comparison.ipynb)\n\n**The Frontiers of Memory and Attention in Deep Learning**\n\n- sldies: [http://slides.com/smerity/quora-frontiers-of-memory-and-attention#/](http://slides.com/smerity/quora-frontiers-of-memory-and-attention#/)\n\n**Design Patterns for Deep Learning Architectures**\n\n[http://www.deeplearningpatterns.com/doku.php](http://www.deeplearningpatterns.com/doku.php)\n\n**Building a Deep Learning Powered GIF Search Engine**\n\n- blog: [https://medium.com/@zan2434/building-a-deep-learning-powered-gif-search-engine-a3eb309d7525](https://medium.com/@zan2434/building-a-deep-learning-powered-gif-search-engine-a3eb309d7525)\n\n**850k Images in 24 hours: Automating Deep Learning Dataset Creation**\n\n[https://gab41.lab41.org/850k-images-in-24-hours-automating-deep-learning-dataset-creation-60bdced04275#.xhq9feuxx](https://gab41.lab41.org/850k-images-in-24-hours-automating-deep-learning-dataset-creation-60bdced04275#.xhq9feuxx)\n\n**How six lines of code + SQL Server can bring Deep Learning to ANY App**\n\n- blog: [https://blogs.technet.microsoft.com/dataplatforminsider/2017/01/05/how-six-lines-of-code-sql-server-can-bring-deep-learning-to-any-app/](https://blogs.technet.microsoft.com/dataplatforminsider/2017/01/05/how-six-lines-of-code-sql-server-can-bring-deep-learning-to-any-app/)\n- github: [https://github.com/Microsoft/SQL-Server-R-Services-Samples/tree/master/Galaxies](https://github.com/Microsoft/SQL-Server-R-Services-Samples/tree/master/Galaxies)\n\n**Neural Network Architectures**\n\n![](https://culurciello.github.io/assets/nets/acc_vs_net_vs_ops.svg)\n\n- blog: [https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba#.m8y39oih6](https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba#.m8y39oih6)\n- blog: [https://culurciello.github.io/tech/2016/06/04/nets.html](https://culurciello.github.io/tech/2016/06/04/nets.html)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","title":"Object Detection"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Object Detection\ndate: 2015-10-09\n---\n\n| Method           | backbone      | test size | VOC2007 | VOC2010 | VOC2012 | ILSVRC 2013 | MSCOCO 2015                     | Speed                          |\n| :------------:   | :-----:       | :-----:   | :-----: | :-----: | :-----: | :---------: | :---------:                     | :---------:                    |\n| OverFeat         |               |           |         |         |         | 24.3%       |                                 |                                |\n| R-CNN            | AlexNet       |           | 58.5%   | 53.7%   | 53.3%   | 31.4%       |                                 |                                |\n| R-CNN            | VGG16         |           | 66.0%   |         |         |             |                                 |                                |\n| SPP_net          | ZF-5          |           | 54.2%   |         |         | 31.84%      |                                 |                                |\n| DeepID-Net       |               |           | 64.1%   |         |         | 50.3%       |                                 |                                |\n| NoC              | 73.3%         |           | 68.8%   |         |         |             |                                 |                                |\n| Fast-RCNN        | VGG16         |           | 70.0%   | 68.8%   | 68.4%   |             | 19.7%(@[0.5-0.95]), 35.9%(@0.5) |                                |\n| MR-CNN           | 78.2%         |           | 73.9%   |         |         |             |                                 |                                |\n| Faster-RCNN      | VGG16         |           | 78.8%   |         | 75.9%   |             | 21.9%(@[0.5-0.95]), 42.7%(@0.5) | 198ms                          |\n| Faster-RCNN      | ResNet101     |           | 85.6%   |         | 83.8%   |             | 37.4%(@[0.5-0.95]), 59.0%(@0.5) |                                |\n| YOLO             |               |           | 63.4%   |         | 57.9%   |             |                                 | 45 fps                         |\n| YOLO VGG-16      |               |           | 66.4%   |         |         |             |                                 | 21 fps                         |\n| YOLOv2           |               | 448x448   | 78.6%   |         | 73.4%   |             | 21.6%(@[0.5-0.95]), 44.0%(@0.5) | 40 fps                         |\n| SSD              | VGG16         | 300x300   | 77.2%   |         | 75.8%   |             | 25.1%(@[0.5-0.95]), 43.1%(@0.5) | 46 fps                         |\n| SSD              | VGG16         | 512x512   | 79.8%   |         | 78.5%   |             | 28.8%(@[0.5-0.95]), 48.5%(@0.5) | 19 fps                         |\n| SSD              | ResNet101     | 300x300   |         |         |         |             | 28.0%(@[0.5-0.95])              | 16 fps                         |\n| SSD              | ResNet101     | 512x512   |         |         |         |             | 31.2%(@[0.5-0.95])              | 8 fps                          |\n| DSSD             | ResNet101     | 300x300   |         |         |         |             | 28.0%(@[0.5-0.95])              | 8 fps                          |\n| DSSD             | ResNet101     | 500x500   |         |         |         |             | 33.2%(@[0.5-0.95])              | 6 fps                          |\n| ION              |               |           | 79.2%   |         | 76.4%   |             |                                 |                                |\n| CRAFT            |               |           | 75.7%   |         | 71.3%   | 48.5%       |                                 |                                |\n| OHEM             |               |           | 78.9%   |         | 76.3%   |             | 25.5%(@[0.5-0.95]), 45.9%(@0.5) |                                |\n| R-FCN            | ResNet50      |           | 77.4%   |         |         |             |                                 | 0.12sec(K40), 0.09sec(TitianX) |\n| R-FCN            | ResNet101     |           | 79.5%   |         |         |             |                                 | 0.17sec(K40), 0.12sec(TitianX) |\n| R-FCN(ms train)  | ResNet101     |           | 83.6%   |         | 82.0%   |             | 31.5%(@[0.5-0.95]), 53.2%(@0.5) |                                |\n| PVANet 9.0       |               |           | 84.9%   |         | 84.2%   |             |                                 | 750ms(CPU), 46ms(TitianX)      |\n| RetinaNet        | ResNet101-FPN |           |         |         |         |             |                                 |                                |\n| Light-Head R-CNN | Xception\\*    | 800/1200  |         |         |         |             | 31.5%@[0.5:0.95]                | 95 fps                         |\n| Light-Head R-CNN | Xception\\*    | 700/1100  |         |         |         |             | 30.7%@[0.5:0.95]                | 102 fps                        |\n\n# Papers\n\n**Deep Neural Networks for Object Detection**\n\n- paper: [http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf)\n\n**OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks**\n\n- arxiv: [http://arxiv.org/abs/1312.6229](http://arxiv.org/abs/1312.6229)\n- github: [https://github.com/sermanet/OverFeat](https://github.com/sermanet/OverFeat)\n- code: [http://cilvr.nyu.edu/doku.php?id=software:overfeat:start](http://cilvr.nyu.edu/doku.php?id=software:overfeat:start)\n\n**Scalable Object Detection using Deep Neural Networks**\n\n- intro: first MultiBox. Train a CNN to predict Region of Interest.\n- arxiv: [http://arxiv.org/abs/1312.2249](http://arxiv.org/abs/1312.2249)\n- github: [https://github.com/google/multibox](https://github.com/google/multibox)\n- blog: [https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html](https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html)\n\n**Scalable, High-Quality Object Detection**\n\n- intro: second MultiBox\n- arxiv: [http://arxiv.org/abs/1412.1441](http://arxiv.org/abs/1412.1441)\n- github: [https://github.com/google/multibox](https://github.com/google/multibox)\n\n**Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition**\n\n- intro: ECCV 2014 / TPAMI 2015\n- keywords: SPP-Net\n- arxiv: [http://arxiv.org/abs/1406.4729](http://arxiv.org/abs/1406.4729)\n- github: [https://github.com/ShaoqingRen/SPP_net](https://github.com/ShaoqingRen/SPP_net)\n- notes: [http://zhangliliang.com/2014/09/13/paper-note-sppnet/](http://zhangliliang.com/2014/09/13/paper-note-sppnet/)\n\n**DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection**\n\n- intro: PAMI 2016\n- intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations\n- project page: [http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html](http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html)\n- arxiv: [http://arxiv.org/abs/1412.5661](http://arxiv.org/abs/1412.5661)\n\n**Object Detectors Emerge in Deep Scene CNNs**\n\n- intro: ICLR 2015\n- arxiv: [http://arxiv.org/abs/1412.6856](http://arxiv.org/abs/1412.6856)\n- paper: [https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf](https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf)\n- paper: [https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf](https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf)\n- slides: [http://places.csail.mit.edu/slide_iclr2015.pdf](http://places.csail.mit.edu/slide_iclr2015.pdf)\n\n**segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection**\n\n- intro: CVPR 2015\n- project(code+data): [https://www.cs.toronto.edu/~yukun/segdeepm.html](https://www.cs.toronto.edu/~yukun/segdeepm.html)\n- arxiv: [https://arxiv.org/abs/1502.04275](https://arxiv.org/abs/1502.04275)\n- github: [https://github.com/YknZhu/segDeepM](https://github.com/YknZhu/segDeepM)\n\n**Object Detection Networks on Convolutional Feature Maps**\n\n- intro: TPAMI 2015\n- keywords: NoC\n- arxiv: [http://arxiv.org/abs/1504.06066](http://arxiv.org/abs/1504.06066)\n\n**Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction**\n\n- arxiv: [http://arxiv.org/abs/1504.03293](http://arxiv.org/abs/1504.03293)\n- slides: [http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf](http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf)\n- github: [https://github.com/YutingZhang/fgs-obj](https://github.com/YutingZhang/fgs-obj)\n\n**DeepBox: Learning Objectness with Convolutional Networks**\n\n- keywords: DeepBox\n- arxiv: [http://arxiv.org/abs/1505.02146](http://arxiv.org/abs/1505.02146)\n- github: [https://github.com/weichengkuo/DeepBox](https://github.com/weichengkuo/DeepBox)\n\n**Object detection via a multi-region & semantic segmentation-aware CNN model**\n\n- intro: ICCV 2015\n- keywords: MR-CNN\n- arxiv: [http://arxiv.org/abs/1505.01749](http://arxiv.org/abs/1505.01749)\n- github: [https://github.com/gidariss/mrcnn-object-detection](https://github.com/gidariss/mrcnn-object-detection)\n- notes: [http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/](http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/)\n- notes: [http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/](http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/)\n\n**AttentionNet: Aggregating Weak Directions for Accurate Object Detection**\n\n- intro: ICCV 2015\n- intro: state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 human detection task\n- arxiv: [http://arxiv.org/abs/1506.07704](http://arxiv.org/abs/1506.07704)\n- slides: [https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf](https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf)\n- slides: [http://image-net.org/challenges/talks/lunit-kaist-slide.pdf](http://image-net.org/challenges/talks/lunit-kaist-slide.pdf)\n\n## DenseBox\n\n**DenseBox: Unifying Landmark Localization with End to End Object Detection**\n\n- arxiv: [http://arxiv.org/abs/1509.04874](http://arxiv.org/abs/1509.04874)\n- demo: [http://pan.baidu.com/s/1mgoWWsS](http://pan.baidu.com/s/1mgoWWsS)\n- KITTI result: [http://www.cvlibs.net/datasets/kitti/eval_object.php](http://www.cvlibs.net/datasets/kitti/eval_object.php)\n\n**Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks**\n\n- intro: \"0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding-box regression\nand 1.15s per image with it\".\n- keywords: Inside-Outside Net (ION)\n- arxiv: [http://arxiv.org/abs/1512.04143](http://arxiv.org/abs/1512.04143)\n- slides: [http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf](http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf)\n- coco-leaderboard: [http://mscoco.org/dataset/#detections-leaderboard](http://mscoco.org/dataset/#detections-leaderboard)\n\n**Adaptive Object Detection Using Adjacency and Zoom Prediction**\n\n- intro: CVPR 2016. AZ-Net\n- arxiv: [http://arxiv.org/abs/1512.07711](http://arxiv.org/abs/1512.07711)\n- github: [https://github.com/luyongxi/az-net](https://github.com/luyongxi/az-net)\n- youtube: [https://www.youtube.com/watch?v=YmFtuNwxaNM](https://www.youtube.com/watch?v=YmFtuNwxaNM)\n\n**G-CNN: an Iterative Grid Based Object Detector**\n\n- arxiv: [http://arxiv.org/abs/1512.07729](http://arxiv.org/abs/1512.07729)\n\n**We don't need no bounding-boxes: Training object class detectors using only human verification**\n\n- arxiv: [http://arxiv.org/abs/1602.08405](http://arxiv.org/abs/1602.08405)\n\n**HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection**\n\n- arxiv: [http://arxiv.org/abs/1604.00600](http://arxiv.org/abs/1604.00600)\n\n**A MultiPath Network for Object Detection**\n\n- intro: BMVC 2016. Facebook AI Research (FAIR)\n- arxiv: [http://arxiv.org/abs/1604.02135](http://arxiv.org/abs/1604.02135)\n- github: [https://github.com/facebookresearch/multipathnet](https://github.com/facebookresearch/multipathnet)\n\n**CRAFT Objects from Images**\n\n- intro: CVPR 2016. Cascade Region-proposal-network And FasT-rcnn. an extension of Faster R-CNN\n- project page: [http://byangderek.github.io/projects/craft.html](http://byangderek.github.io/projects/craft.html)\n- arxiv: [https://arxiv.org/abs/1604.03239](https://arxiv.org/abs/1604.03239)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf)\n- github: [https://github.com/byangderek/CRAFT](https://github.com/byangderek/CRAFT)\n\n## OHEM\n\n**Training Region-based Object Detectors with Online Hard Example Mining**\n\n- intro: CVPR 2016 Oral. Online hard example mining (OHEM)\n- arxiv: [http://arxiv.org/abs/1604.03540](http://arxiv.org/abs/1604.03540)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf)\n- github(Official): [https://github.com/abhi2610/ohem](https://github.com/abhi2610/ohem)\n- author page: [http://abhinav-shrivastava.info/](http://abhinav-shrivastava.info/)\n\n**S-OHEM: Stratified Online Hard Example Mining for Object Detection**\n\n[https://arxiv.org/abs/1705.02233](https://arxiv.org/abs/1705.02233)\n\n- - -\n\n**Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers**\n\n- intro: CVPR 2016\n- keywords: scale-dependent pooling  (SDP), cascaded rejection classifiers (CRC)\n- paper: [http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf](http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf)\n\n## R-FCN\n\n**R-FCN: Object Detection via Region-based Fully Convolutional Networks**\n\n- arxiv: [http://arxiv.org/abs/1605.06409](http://arxiv.org/abs/1605.06409)\n- github: [https://github.com/daijifeng001/R-FCN](https://github.com/daijifeng001/R-FCN)\n- github(MXNet): [https://github.com/msracver/Deformable-ConvNets/tree/master/rfcn](https://github.com/msracver/Deformable-ConvNets/tree/master/rfcn)\n- github: [https://github.com/Orpine/py-R-FCN](https://github.com/Orpine/py-R-FCN)\n- github: [https://github.com/PureDiors/pytorch_RFCN](https://github.com/PureDiors/pytorch_RFCN)\n- github: [https://github.com/bharatsingh430/py-R-FCN-multiGPU](https://github.com/bharatsingh430/py-R-FCN-multiGPU)\n- github: [https://github.com/xdever/RFCN-tensorflow](https://github.com/xdever/RFCN-tensorflow)\n\n**R-FCN-3000 at 30fps: Decoupling Detection and Classification**\n\n[https://arxiv.org/abs/1712.01802](https://arxiv.org/abs/1712.01802)\n\n**Recycle deep features for better object detection**\n\n- arxiv: [http://arxiv.org/abs/1607.05066](http://arxiv.org/abs/1607.05066)\n\n**A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection**\n\n- intro: ECCV 2016\n- intro: 640×480: 15 fps, 960×720: 8 fps\n- keywords: MS-CNN\n- arxiv: [http://arxiv.org/abs/1607.07155](http://arxiv.org/abs/1607.07155)\n- github: [https://github.com/zhaoweicai/mscnn](https://github.com/zhaoweicai/mscnn)\n- poster: [http://www.eccv2016.org/files/posters/P-2B-38.pdf](http://www.eccv2016.org/files/posters/P-2B-38.pdf)\n\n**Multi-stage Object Detection with Group Recursive Learning**\n\n- intro: VOC2007: 78.6%, VOC2012: 74.9%\n- arxiv: [http://arxiv.org/abs/1608.05159](http://arxiv.org/abs/1608.05159)\n\n**Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection**\n\n- intro: WACV 2017. SubCNN\n- arxiv: [http://arxiv.org/abs/1604.04693](http://arxiv.org/abs/1604.04693)\n- github: [https://github.com/tanshen/SubCNN](https://github.com/tanshen/SubCNN)\n\n**PVANet: Lightweight Deep Neural Networks for Real-time Object Detection**\n\n- intro: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural Networks (EMDNN). \nContinuation of [arXiv:1608.08021](https://arxiv.org/abs/1608.08021)\n- arxiv: [https://arxiv.org/abs/1611.08588](https://arxiv.org/abs/1611.08588)\n- github: [https://github.com/sanghoon/pva-faster-rcnn](https://github.com/sanghoon/pva-faster-rcnn)\n- leaderboard(PVANet 9.0): [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=4](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=4)\n\n**Gated Bi-directional CNN for Object Detection**\n\n- intro: The Chinese University of Hong Kong & Sensetime Group Limited\n- keywords: GBD-Net\n- paper: [http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22](http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22)\n- mirror: [https://pan.baidu.com/s/1dFohO7v](https://pan.baidu.com/s/1dFohO7v)\n\n**Crafting GBD-Net for Object Detection**\n\n- intro: winner of the ImageNet object detection challenge of 2016. CUImage and CUVideo\n- intro: gated bi-directional CNN (GBD-Net)\n- arxiv: [https://arxiv.org/abs/1610.02579](https://arxiv.org/abs/1610.02579)\n- github: [https://github.com/craftGBD/craftGBD](https://github.com/craftGBD/craftGBD)\n\n**StuffNet: Using 'Stuff' to Improve Object Detection**\n\n- arxiv: [https://arxiv.org/abs/1610.05861](https://arxiv.org/abs/1610.05861)\n\n**Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene**\n\n- arxiv: [https://arxiv.org/abs/1610.09609](https://arxiv.org/abs/1610.09609)\n\n**Hierarchical Object Detection with Deep Reinforcement Learning**\n\n- intro: Deep Reinforcement Learning Workshop (NIPS 2016)\n- project page: [https://imatge-upc.github.io/detection-2016-nipsws/](https://imatge-upc.github.io/detection-2016-nipsws/)\n- arxiv: [https://arxiv.org/abs/1611.03718](https://arxiv.org/abs/1611.03718)\n- slides: [http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning](http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning)\n- github: [https://github.com/imatge-upc/detection-2016-nipsws](https://github.com/imatge-upc/detection-2016-nipsws)\n- blog: [http://jorditorres.org/nips/](http://jorditorres.org/nips/)\n\n**Learning to detect and localize many objects from few examples**\n\n- arxiv: [https://arxiv.org/abs/1611.05664](https://arxiv.org/abs/1611.05664)\n\n**Speed/accuracy trade-offs for modern convolutional object detectors**\n\n- intro: CVPR 2017. Google Research\n- arxiv: [https://arxiv.org/abs/1611.10012](https://arxiv.org/abs/1611.10012)\n\n**SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving**\n\n- arxiv: [https://arxiv.org/abs/1612.01051](https://arxiv.org/abs/1612.01051)\n- github: [https://github.com/BichenWuUCB/squeezeDet](https://github.com/BichenWuUCB/squeezeDet)\n- github: [https://github.com/fregu856/2D_detection](https://github.com/fregu856/2D_detection)\n\n## Feature Pyramid Network (FPN)\n\n**Feature Pyramid Networks for Object Detection**\n\n- intro: Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1612.03144](https://arxiv.org/abs/1612.03144)\n\n**Dynamic Feature Pyramid Networks for Object Detection**\n\n- intro: Zhejiang University & Noah’s Ark Lab & Westlake University\n- arxiv: [https://arxiv.org/abs/2012.00779](https://arxiv.org/abs/2012.00779)\n\n**Implicit Feature Pyramid Network for Object Detection**\n\n- intro: MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2012.13563](https://arxiv.org/abs/2012.13563)\n\n**You Should Look at All Objects**\n\n- intro: ECCV 2022\n- intro: The University of Hong Kong & Bytedance & University of Rochester\n- arxiv: [https://arxiv.org/abs/2207.07889](https://arxiv.org/abs/2207.07889)\n- github: [https://github.com/CharlesPikachu/YSLAO](https://github.com/CharlesPikachu/YSLAO)\n\n**Action-Driven Object Detection with Top-Down Visual Attentions**\n\n- arxiv: [https://arxiv.org/abs/1612.06704](https://arxiv.org/abs/1612.06704)\n\n**Beyond Skip Connections: Top-Down Modulation for Object Detection**\n\n- intro: CMU & UC Berkeley & Google Research\n- arxiv: [https://arxiv.org/abs/1612.06851](https://arxiv.org/abs/1612.06851)\n\n**Wide-Residual-Inception Networks for Real-time Object Detection**\n\n- intro: Inha University\n- arxiv: [https://arxiv.org/abs/1702.01243](https://arxiv.org/abs/1702.01243)\n\n**Attentional Network for Visual Object Detection**\n\n- intro: University of Maryland & Mitsubishi Electric Research Laboratories\n- arxiv: [https://arxiv.org/abs/1702.01478](https://arxiv.org/abs/1702.01478)\n\n**Learning Chained Deep Features and Classifiers for Cascade in Object Detection**\n\n- keykwords: CC-Net\n- intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007\n- arxiv: [https://arxiv.org/abs/1702.07054](https://arxiv.org/abs/1702.07054)\n\n**DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling**\n\n- intro: ICCV 2017 (poster)\n- arxiv: [https://arxiv.org/abs/1703.10295](https://arxiv.org/abs/1703.10295)\n\n**Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.03944](https://arxiv.org/abs/1704.03944)\n\n**Spatial Memory for Context Reasoning in Object Detection**\n\n- arxiv: [https://arxiv.org/abs/1704.04224](https://arxiv.org/abs/1704.04224)\n\n**Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection**\n\n[https://arxiv.org/abs/1704.05775](https://arxiv.org/abs/1704.05775)\n\n**LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems**\n\n- intro: Embedded Vision Workshop in CVPR. UC San Diego & Qualcomm Inc\n- arxiv: [https://arxiv.org/abs/1705.05922](https://arxiv.org/abs/1705.05922)\n\n**Point Linking Network for Object Detection**\n\n- intro: Point Linking Network (PLN)\n- arxiv: [https://arxiv.org/abs/1706.03646](https://arxiv.org/abs/1706.03646)\n\n**Perceptual Generative Adversarial Networks for Small Object Detection**\n\n[https://arxiv.org/abs/1706.05274](https://arxiv.org/abs/1706.05274)\n\n**Few-shot Object Detection**\n\n[https://arxiv.org/abs/1706.08249](https://arxiv.org/abs/1706.08249)\n\n**Yes-Net: An effective Detector Based on Global Information**\n\n[https://arxiv.org/abs/1706.09180](https://arxiv.org/abs/1706.09180)\n\n**Towards lightweight convolutional neural networks for object detection**\n\n[https://arxiv.org/abs/1707.01395](https://arxiv.org/abs/1707.01395)\n\n**RON: Reverse Connection with Objectness Prior Networks for Object Detection**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1707.01691](https://arxiv.org/abs/1707.01691)\n- github: [https://github.com/taokong/RON](https://github.com/taokong/RON)\n\n**Deformable Part-based Fully Convolutional Network for Object Detection**\n\n- intro: BMVC 2017 (oral). Sorbonne Universités & CEDRIC\n- arxiv: [https://arxiv.org/abs/1707.06175](https://arxiv.org/abs/1707.06175)\n\n**Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.06399](https://arxiv.org/abs/1707.06399)\n\n**Recurrent Scale Approximation for Object Detection in CNN**\n\n- intro: ICCV 2017\n- keywords: Recurrent Scale Approximation (RSA)\n- arxiv: [https://arxiv.org/abs/1707.09531](https://arxiv.org/abs/1707.09531)\n- github: [https://github.com/sciencefans/RSA-for-object-detection](https://github.com/sciencefans/RSA-for-object-detection)\n\n**DSOD: Learning Deeply Supervised Object Detectors from Scratch**\n\n![](https://user-images.githubusercontent.com/3794909/28934967-718c9302-78b5-11e7-89ee-8b514e53e23c.png)\n\n- intro: ICCV 2017. Fudan University & Tsinghua University & Intel Labs China\n- arxiv: [https://arxiv.org/abs/1708.01241](https://arxiv.org/abs/1708.01241)\n- github: [https://github.com/szq0214/DSOD](https://github.com/szq0214/DSOD)\n\n**Object Detection from Scratch with Deep Supervision**\n\n[https://arxiv.org/abs/1809.09294](https://arxiv.org/abs/1809.09294)\n\n**CoupleNet: Coupling Global Structure with Local Parts for Object Detection**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.02863](https://arxiv.org/abs/1708.02863)\n\n**Incremental Learning of Object Detectors without Catastrophic Forgetting**\n\n- intro: ICCV 2017. Inria\n- arxiv: [https://arxiv.org/abs/1708.06977](https://arxiv.org/abs/1708.06977)\n\n**Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection**\n\n[https://arxiv.org/abs/1709.04347](https://arxiv.org/abs/1709.04347)\n\n**StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection**\n\n[https://arxiv.org/abs/1709.05788](https://arxiv.org/abs/1709.05788)\n\n**Dynamic Zoom-in Network for Fast Object Detection in Large Images**\n\n[https://arxiv.org/abs/1711.05187](https://arxiv.org/abs/1711.05187)\n\n**Zero-Annotation Object Detection with Web Knowledge Transfer**\n\n- intro: NTU, Singapore & Amazon\n- keywords: multi-instance multi-label domain adaption learning framework\n- arxiv: [https://arxiv.org/abs/1711.05954](https://arxiv.org/abs/1711.05954)\n\n**MegDet: A Large Mini-Batch Object Detector**\n\n- intro: Peking University & Tsinghua University & Megvii Inc\n- arxiv: [https://arxiv.org/abs/1711.07240](https://arxiv.org/abs/1711.07240)\n\n**Receptive Field Block Net for Accurate and Fast Object Detection**\n\n- intro: RFBNet\n- arxiv: [https://arxiv.org/abs/1711.07767](https://arxiv.org/abs/1711.07767)\n- github: [https://github.com//ruinmessi/RFBNet](https://github.com//ruinmessi/RFBNet)\n\n**An Analysis of Scale Invariance in Object Detection - SNIP**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1711.08189](https://arxiv.org/abs/1711.08189)\n- github: [https://github.com/bharatsingh430/snip](https://github.com/bharatsingh430/snip)\n\n**Feature Selective Networks for Object Detection**\n\n[https://arxiv.org/abs/1711.08879](https://arxiv.org/abs/1711.08879)\n\n**Learning a Rotation Invariant Detector with Rotatable Bounding Box**\n\n- arxiv: [https://arxiv.org/abs/1711.09405](https://arxiv.org/abs/1711.09405)\n- github(official, Caffe): [https://github.com/liulei01/DRBox](https://github.com/liulei01/DRBox)\n\n**Scalable Object Detection for Stylized Objects**\n\n- intro: Microsoft AI & Research Munich\n- arxiv: [https://arxiv.org/abs/1711.09822](https://arxiv.org/abs/1711.09822)\n\n**Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids**\n\n- arxiv: [https://arxiv.org/abs/1712.00886](https://arxiv.org/abs/1712.00886)\n- github: [https://github.com/szq0214/GRP-DSOD](https://github.com/szq0214/GRP-DSOD)\n\n**Deep Regionlets for Object Detection**\n\n- keywords: region selection network, gating network\n- arxiv: [https://arxiv.org/abs/1712.02408](https://arxiv.org/abs/1712.02408)\n\n**Training and Testing Object Detectors with Virtual Images**\n\n- intro: IEEE/CAA Journal of Automatica Sinica\n- arxiv: [https://arxiv.org/abs/1712.08470](https://arxiv.org/abs/1712.08470)\n\n**Large-Scale Object Discovery and Detector Adaptation from Unlabeled Video**\n\n- keywords: object mining, object tracking, unsupervised object discovery by appearance-based clustering, self-supervised detector adaptation\n- arxiv: [https://arxiv.org/abs/1712.08832](https://arxiv.org/abs/1712.08832)\n\n**Spot the Difference by Object Detection**\n\n- intro: Tsinghua University & JD Group\n- arxiv: [https://arxiv.org/abs/1801.01051](https://arxiv.org/abs/1801.01051)\n\n**Localization-Aware Active Learning for Object Detection**\n\n- arxiv: [https://arxiv.org/abs/1801.05124](https://arxiv.org/abs/1801.05124)\n\n**Object Detection with Mask-based Feature Encoding**\n\n[https://arxiv.org/abs/1802.03934](https://arxiv.org/abs/1802.03934)\n\n**LSTD: A Low-Shot Transfer Detector for Object Detection**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1803.01529](https://arxiv.org/abs/1803.01529)\n\n**Pseudo Mask Augmented Object Detection**\n\n[https://arxiv.org/abs/1803.05858](https://arxiv.org/abs/1803.05858)\n\n**Revisiting RCNN: On Awakening the Classification Power of Faster RCNN**\n\n- intro: ECCV 2018\n- keywords: DCR V1\n- arxiv: [https://arxiv.org/abs/1803.06799](https://arxiv.org/abs/1803.06799)\n- github(official, MXNet): [https://github.com/bowenc0221/Decoupled-Classification-Refinement](https://github.com/bowenc0221/Decoupled-Classification-Refinement)\n\n**Decoupled Classification Refinement: Hard False Positive Suppression for Object Detection**\n\n- keywords: DCR V2\n- arxiv: [https://arxiv.org/abs/1810.04002](https://arxiv.org/abs/1810.04002)\n- github(official, MXNet): [https://github.com/bowenc0221/Decoupled-Classification-Refinement](https://github.com/bowenc0221/Decoupled-Classification-Refinement)\n\n**Learning Region Features for Object Detection**\n\n- intro: Peking University & MSRA\n- arxiv: [https://arxiv.org/abs/1803.07066](https://arxiv.org/abs/1803.07066)\n\n**Object Detection for Comics using Manga109 Annotations**\n\n- intro: University of Tokyo & National Institute of Informatics, Japan\n- arxiv: [https://arxiv.org/abs/1803.08670](https://arxiv.org/abs/1803.08670)\n\n**Task-Driven Super Resolution: Object Detection in Low-resolution Images**\n\n[https://arxiv.org/abs/1803.11316](https://arxiv.org/abs/1803.11316)\n\n**Transferring Common-Sense Knowledge for Object Detection**\n\n[https://arxiv.org/abs/1804.01077](https://arxiv.org/abs/1804.01077)\n\n**Multi-scale Location-aware Kernel Representation for Object Detection**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.00428](https://arxiv.org/abs/1804.00428)\n- github: [https://github.com/Hwang64/MLKP](https://github.com/Hwang64/MLKP)\n\n**Loss Rank Mining: A General Hard Example Mining Method for Real-time Detectors**\n\n- intro: National University of Defense Technology\n- arxiv: [https://arxiv.org/abs/1804.04606](https://arxiv.org/abs/1804.04606)\n\n**DetNet: A Backbone network for Object Detection**\n\n- intro: Tsinghua University & Megvii Inc\n- arxiv: [https://arxiv.org/abs/1804.06215](https://arxiv.org/abs/1804.06215)\n\n**AdvDetPatch: Attacking Object Detectors with Adversarial Patches**\n\n[https://arxiv.org/abs/1806.02299](https://arxiv.org/abs/1806.02299)\n\n**Attacking Object Detectors via Imperceptible Patches on Background**\n\n[https://arxiv.org/abs/1809.05966](https://arxiv.org/abs/1809.05966)\n\n**Physical Adversarial Examples for Object Detectors**\n\n- intro: WOOT 2018\n- arxiv: [https://arxiv.org/abs/1807.07769](https://arxiv.org/abs/1807.07769)\n\n**Object detection at 200 Frames Per Second**\n\n- intro: United Technologies Research Center-Ireland\n- arxiv: [https://arxiv.org/abs/1805.06361](https://arxiv.org/abs/1805.06361)\n\n**Object Detection using Domain Randomization and Generative Adversarial Refinement of Synthetic Images**\n\n- intro: CVPR 2018 Deep Vision Workshop\n- arxiv: [https://arxiv.org/abs/1805.11778](https://arxiv.org/abs/1805.11778)\n\n**SNIPER: Efficient Multi-Scale Training**\n\n- intro: University of Maryland\n- keywords: SNIPER (Scale Normalization for Image Pyramid with Efficient Resampling)\n- arxiv: [https://arxiv.org/abs/1805.09300](https://arxiv.org/abs/1805.09300)\n- github: [https://github.com/mahyarnajibi/SNIPER](https://github.com/mahyarnajibi/SNIPER)\n\n**Soft Sampling for Robust Object Detection**\n\n[https://arxiv.org/abs/1806.06986](https://arxiv.org/abs/1806.06986)\n\n**MetaAnchor: Learning to Detect Objects with Customized Anchors**\n\n- intro: Megvii Inc (Face++) & Fudan University\n- arxiv: [https://arxiv.org/abs/1807.00980](https://arxiv.org/abs/1807.00980)\n\n**Localization Recall Precision (LRP): A New Performance Metric for Object Detection**\n\n- intro: ECCV 2018. Middle East Technical University\n- arxiv: [https://arxiv.org/abs/1807.01696](https://arxiv.org/abs/1807.01696)\n- github: [https://github.com/cancam/LRP](https://github.com/cancam/LRP)\n\n**Pooling Pyramid Network for Object Detection**\n\n- intro: Google AI Perception\n- arxiv: [https://arxiv.org/abs/1807.03284](https://arxiv.org/abs/1807.03284)\n\n**Modeling Visual Context is Key to Augmenting Object Detection Datasets**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.07428](https://arxiv.org/abs/1807.07428)\n\n**Acquisition of Localization Confidence for Accurate Object Detection**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.11590](https://arxiv.org/abs/1807.11590)\n- gihtub: [https://github.com/vacancy/PreciseRoIPooling](https://github.com/vacancy/PreciseRoIPooling)\n\n**CornerNet: Detecting Objects as Paired Keypoints**\n\n- intro: ECCV 2018\n- keywords: IoU-Net, PreciseRoIPooling\n- arxiv: [https://arxiv.org/abs/1808.01244](https://arxiv.org/abs/1808.01244)\n- github: [https://github.com/umich-vl/CornerNet](https://github.com/umich-vl/CornerNet)\n\n**Unsupervised Hard Example Mining from Videos for Improved Object Detection**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.04285](https://arxiv.org/abs/1808.04285)\n\n**SAN: Learning Relationship between Convolutional Features for Multi-Scale Object Detection**\n\n[https://arxiv.org/abs/1808.04974](https://arxiv.org/abs/1808.04974)\n\n**A Survey of Modern Object Detection Literature using Deep Learning**\n\n[https://arxiv.org/abs/1808.07256](https://arxiv.org/abs/1808.07256)\n\n**Tiny-DSOD: Lightweight Object Detection for Resource-Restricted Usages**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.11013](https://arxiv.org/abs/1807.11013)\n- github: [https://github.com/lyxok1/Tiny-DSOD](https://github.com/lyxok1/Tiny-DSOD)\n\n**Deep Feature Pyramid Reconfiguration for Object Detection**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.07993](https://arxiv.org/abs/1808.07993)\n\n**MDCN: Multi-Scale, Deep Inception Convolutional Neural Networks for Efficient Object Detection**\n\n- intro: ICPR 2018\n- arxiv: [https://arxiv.org/abs/1809.01791](https://arxiv.org/abs/1809.01791)\n\n**Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1809.03193](https://arxiv.org/abs/1809.03193)\n\n**Deep Learning for Generic Object Detection: A Survey**\n\n[https://arxiv.org/abs/1809.02165](https://arxiv.org/abs/1809.02165)\n\n**Training Confidence-Calibrated Classifier for Detecting Out-of-Distribution Samples**\n\n- intro: ICLR 2018\n- arxiv: [https://github.com/alinlab/Confident_classifier](https://github.com/alinlab/Confident_classifier)\n\n**Fast and accurate object detection in high resolution 4K and 8K video using GPUs**\n\n- intro: Best Paper Finalist at IEEE High Performance Extreme Computing Conference (HPEC) 2018\n- intro: Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/1810.10551](https://arxiv.org/abs/1810.10551)\n\n**Hybrid Knowledge Routed Modules for Large-scale Object Detection**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.12681](https://arxiv.org/abs/1810.12681)\n- github(official, PyTorch): [https://github.com/chanyn/HKRM](https://github.com/chanyn/HKRM)\n\n**BAN: Focusing on Boundary Context for Object Detection**\n\n[https://arxiv.org/abs/1811.05243](https://arxiv.org/abs/1811.05243)\n\n**R2CNN++: Multi-Dimensional Attention Based Rotation Invariant Detector with Robust Anchor Strategy**\n\n- arxiv: [https://arxiv.org/abs/1811.07126](https://arxiv.org/abs/1811.07126)\n- github: [https://github.com/DetectionTeamUCAS/R2CNN-Plus-Plus_Tensorflow](https://github.com/DetectionTeamUCAS/R2CNN-Plus-Plus_Tensorflow)\n\n**DeRPN: Taking a further step toward more general object detection**\n\n- intro: AAAI 2019\n- intro: South China University of Technology\n- ariv: [https://arxiv.org/abs/1811.06700](https://arxiv.org/abs/1811.06700)\n- github: [https://github.com/HCIILAB/DeRPN](https://github.com/HCIILAB/DeRPN)\n\n**Fast Efficient Object Detection Using Selective Attention**\n\n[https://arxiv.org/abs/1811.07502](https://arxiv.org/abs/1811.07502)\n\n**Sampling Techniques for Large-Scale Object Detection from Sparsely Annotated Objects**\n\n[https://arxiv.org/abs/1811.10862](https://arxiv.org/abs/1811.10862)\n\n**Efficient Coarse-to-Fine Non-Local Module for the Detection of Small Objects**\n\n[https://arxiv.org/abs/1811.12152](https://arxiv.org/abs/1811.12152)\n\n**Deep Regionlets: Blended Representation and Deep Learning for Generic Object Detection**\n\n[https://arxiv.org/abs/1811.11318](https://arxiv.org/abs/1811.11318)\n\n**Transferable Adversarial Attacks for Image and Video Object Detection**\n\n[https://arxiv.org/abs/1811.12641](https://arxiv.org/abs/1811.12641)\n\n**Anchor Box Optimization for Object Detection**\n\n- intro: University of Illinois at Urbana-Champaign & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1812.00469](https://arxiv.org/abs/1812.00469)\n\n**AutoFocus: Efficient Multi-Scale Inference**\n\n- intro: University of Maryland\n- arxiv: [https://arxiv.org/abs/1812.01600](https://arxiv.org/abs/1812.01600)\n\n**Few-shot Object Detection via Feature Reweighting**\n\n[https://arxiv.org/abs/1812.01866](https://arxiv.org/abs/1812.01866)\n\n**Practical Adversarial Attack Against Object Detector**\n\n[https://arxiv.org/abs/1812.10217](https://arxiv.org/abs/1812.10217)\n\n**Scale-Aware Trident Networks for Object Detection**\n\n- intro: University of Chinese Academy of Sciences & TuSimple\n- arxiv: [https://arxiv.org/abs/1901.01892](https://arxiv.org/abs/1901.01892)\n- github: [https://github.com/TuSimple/simpledet](https://github.com/TuSimple/simpledet)\n\n**Region Proposal by Guided Anchoring**\n\n- intro: CVPR 2019\n- intro: CUHK - SenseTime Joint Lab & Amazon Rekognition & Nanyang Technological University\n- arxiv: [https://arxiv.org/abs/1901.03278](https://arxiv.org/abs/1901.03278)\n\n**Bottom-up Object Detection by Grouping Extreme and Center Points**\n\n- keywords: ExtremeNet\n- arxiv: [https://arxiv.org/abs/1901.08043](https://arxiv.org/abs/1901.08043)\n- github: [https://github.com/xingyizhou/ExtremeNet](https://github.com/xingyizhou/ExtremeNet)\n\n**Bag of Freebies for Training Object Detection Neural Networks**\n\n- intro: Amazon Web Services\n- arxiv: [https://arxiv.org/abs/1902.04103](https://arxiv.org/abs/1902.04103)\n\n**Augmentation for small object detection**\n\n[https://arxiv.org/abs/1902.07296](https://arxiv.org/abs/1902.07296)\n\n**Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1902.09630](https://arxiv.org/abs/1902.09630)\n\n**SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition**\n\n- intro: TuSimple\n- arxiv: [https://arxiv.org/abs/1903.05831](https://arxiv.org/abs/1903.05831)\n- github: [https://github.com/tusimple/simpledet](https://github.com/tusimple/simpledet)\n\n**BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors**\n\n- intro: University of Toronto\n- arxiv: [https://arxiv.org/abs/1903.03838](https://arxiv.org/abs/1903.03838)\n\n**DetNAS: Neural Architecture Search on Object Detection**\n\n- intro: Chinese Academy of Sciences & Megvii Inc\n- arxiv: [https://arxiv.org/abs/1903.10979](https://arxiv.org/abs/1903.10979)\n\n**ThunderNet: Towards Real-time Generic Object Detection**\n\n[https://arxiv.org/abs/1903.11752](https://arxiv.org/abs/1903.11752)\n\n**Feature Intertwiner for Object Detection**\n\n- intro: ICLR 2019\n- intro: CUHK & SenseTime & The University of Sydney\n- arxiv: [https://arxiv.org/abs/1903.11851](https://arxiv.org/abs/1903.11851)\n\n**Improving Object Detection with Inverted Attention**\n\n[https://arxiv.org/abs/1903.12255](https://arxiv.org/abs/1903.12255)\n\n**What Object Should I Use? - Task Driven Object Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.03000](https://arxiv.org/abs/1904.03000)\n\n**Towards Universal Object Detection by Domain Attention**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.04402](https://arxiv.org/abs/1904.04402)\n\n**Prime Sample Attention in Object Detection**\n\n[https://arxiv.org/abs/1904.04821](https://arxiv.org/abs/1904.04821)\n\n**BAOD: Budget-Aware Object Detection**\n\n[https://arxiv.org/abs/1904.05443](https://arxiv.org/abs/1904.05443)\n\n**An Analysis of Pre-Training on Object Detection**\n\n- intro: University of Maryland\n- arxiv: [https://arxiv.org/abs/1904.05871](https://arxiv.org/abs/1904.05871)\n\n**DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors**\n\n- intro: Baidu Inc.\n- arxiv: [https://arxiv.org/abs/1904.06883](https://arxiv.org/abs/1904.06883)\n\n**NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection**\n\n- intro: CVPR 2019\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1904.07392](https://arxiv.org/abs/1904.07392)\n\n**Objects as Points**\n\n![](https://raw.githubusercontent.com/xingyizhou/CenterNet/master/readme/fig2.png)\n\n- intro: Object detection, 3D detection, and pose estimation using center point detection\n- arxiv: [https://arxiv.org/abs/1904.07850](https://arxiv.org/abs/1904.07850)\n- github: [https://github.com/xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet)\n\n**MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach**\n\n- intro: ICCV 2021\n- intro: ZF Friedrichshafen AG, Artificial Intelligence Lab\n- arxiv: [https://arxiv.org/abs/2108.05060](https://arxiv.org/abs/2108.05060)\n\n**CenterNet: Object Detection with Keypoint Triplets**\n\n**CenterNet: Keypoint Triplets for Object Detection**\n\n- arxiv: [https://arxiv.org/abs/1904.08189](https://arxiv.org/abs/1904.08189)\n- github: [https://github.com/Duankaiwen/CenterNet](https://github.com/Duankaiwen/CenterNet)\n\n**CornerNet-Lite: Efficient Keypoint Based Object Detection**\n\n- intro: Princeton University\n- arxiv: [https://arxiv.org/abs/1904.08900](https://arxiv.org/abs/1904.08900)\n- github: [https://github.com/princeton-vl/CornerNet-Lite](https://github.com/princeton-vl/CornerNet-Lite)\n\n**CenterNet++ for Object Detection**\n\n- arxiv: [https://arxiv.org/abs/2204.08394](https://arxiv.org/abs/2204.08394)\n- github; [https://github.com/Duankaiwen/PyCenterNet](https://github.com/Duankaiwen/PyCenterNet)\n\n**Automated Focal Loss for Image based Object Detection**\n\n[https://arxiv.org/abs/1904.09048](https://arxiv.org/abs/1904.09048)\n\n**Exploring Object Relation in Mean Teacher for Cross-Domain Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.11245](https://arxiv.org/abs/1904.11245)\n\n**An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection**\n\n- intro: CVPR 2019 CEFRL Workshop\n- arxiv: [https://arxiv.org/abs/1904.09730](https://arxiv.org/abs/1904.09730)\n\n**RepPoints: Point Set Representation for Object Detection**\n\n- intro: ICCV 2019\n- intro: Peking University & Tsinghua University & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/1904.11490](https://arxiv.org/abs/1904.11490)\n- github: [https://github.com/microsoft/RepPoints](https://github.com/microsoft/RepPoints)\n\n**Dense RepPoints: Representing Visual Objects with Dense Point Sets**\n\n- intro: Peking University & CUHK & Zhejiang University & Shanghai Jiao Tong University & University of Toronto & MSRA\n- arxiv: [https://arxiv.org/abs/1912.11473](https://arxiv.org/abs/1912.11473)\n- github(official, mmdetection): [https://github.com/justimyhxu/Dense-RepPoints](https://github.com/justimyhxu/Dense-RepPoints)\n\n**RepPoints V2: Verification Meets Regression for Object Detection**\n\n- intro: Microsoft Research Asia & Peking University\n- arxiv: [https://arxiv.org/abs/2007.08508](https://arxiv.org/abs/2007.08508)\n- github(official, mmdetection): [https://github.com/Scalsol/RepPointsV2](https://github.com/Scalsol/RepPointsV2)\n\n**Object Detection in 20 Years: A Survey**\n\n[https://arxiv.org/abs/1905.05055](https://arxiv.org/abs/1905.05055)\n\n**Light-Weight RetinaNet for Object Detection**\n\n[https://arxiv.org/abs/1905.10011](https://arxiv.org/abs/1905.10011)\n\n**Learning Data Augmentation Strategies for Object Detection**\n\n- intro: Google Research, Brain Team\n- arxiv: [https://arxiv.org/abs/1906.11172](https://arxiv.org/abs/1906.11172)\n- github: [https://github.com/tensorflow/tpu/tree/master/models/official/detection](https://github.com/tensorflow/tpu/tree/master/models/official/detection)\n\n**Towards Adversarially Robust Object Detection**\n\n- intro: ICCV 2019\n- intro: Baidu Research, Sunnyvale USA\n- arxiv: [https://arxiv.org/abs/1907.10310](https://arxiv.org/abs/1907.10310)\n\n**Multi-adversarial Faster-RCNN for Unrestricted Object Detection**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1907.10343](https://arxiv.org/abs/1907.10343)\n\n**Object as Distribution**\n\n- intro: NeurIPS 2019\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1907.12929](https://arxiv.org/abs/1907.12929)\n\n**Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.05217](https://arxiv.org/abs/1908.05217)\n\n**R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object**\n\n- arxiv: [https://arxiv.org/abs/1908.05612](https://arxiv.org/abs/1908.05612)\n- github: [https://github.com/Thinklab-SJTU/R3Det_Tensorflow](https://github.com/Thinklab-SJTU/R3Det_Tensorflow)\n\n**SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing**\n\n- project page: [https://yangxue0827.github.io/SCRDet++.html](https://yangxue0827.github.io/SCRDet++.html)\n- arxiv: [https://arxiv.org/abs/2004.13316](https://arxiv.org/abs/2004.13316)\n\n**Relation Distillation Networks for Video Object Detection**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.09511](https://arxiv.org/abs/1908.09511)\n\n**Imbalance Problems in Object Detection: A Review**\n\n- arxiv: [https://arxiv.org/abs/1909.00169](https://arxiv.org/abs/1909.00169)\n- github: [https://github.com/kemaloksuz/ObjectDetectionImbalance](https://github.com/kemaloksuz/ObjectDetectionImbalance)\n\n**FreeAnchor: Learning to Match Anchors for Visual Object Detection**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1909.02466](https://arxiv.org/abs/1909.02466)\n\n**Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection**\n\n[https://arxiv.org/abs/1909.02293](https://arxiv.org/abs/1909.02293)\n\n**Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection**\n\n- intro: ICCV 2019 oral\n- arxiv: [https://arxiv.org/abs/1909.00597](https://arxiv.org/abs/1909.00597)\n\n**CBNet: A Novel Composite Backbone Network Architecture for Object Detection**\n\n- intro: AAAI 2020\n- keywords: Composite Backbone Network (CBNet)\n- arxiv: [https://arxiv.org/abs/1909.03625](https://arxiv.org/abs/1909.03625)\n- paper: [https://aaai.org/Papers/AAAI/2020GB/AAAI-LiuY.1833.pdf](https://aaai.org/Papers/AAAI/2020GB/AAAI-LiuY.1833.pdf)\n- github(Caffe2): [https://github.com/PKUbahuangliuhe/CBNet](https://github.com/PKUbahuangliuhe/CBNet)\n- github(mmdetection): [https://github.com/VDIGPKU/CBNet](https://github.com/VDIGPKU/CBNet)\n\n**CBNetV2: A Composite Backbone Network Architecture for Object Detection**\n\n- arxiv: [https://arxiv.org/abs/2107.00420](https://arxiv.org/abs/2107.00420)\n- github: [https://github.com/VDIGPKU/CBNetV2](https://github.com/VDIGPKU/CBNetV2)\n\n**A System-Level Solution for Low-Power Object Detection**\n\n- intro: ICCV 2019 Low-Power Computer Vision Workshop\n- arxiv: [https://arxiv.org/abs/1909.10964](https://arxiv.org/abs/1909.10964)\n\n**Anchor Loss: Modulating Loss Scale based on Prediction Difficulty**\n\n- intro: ICCV 2019 oral\n- arxiv: [https://arxiv.org/abs/1909.11155](https://arxiv.org/abs/1909.11155)\n- github(Pytorch): [https://github.com/slryou41/AnchorLoss](https://github.com/slryou41/AnchorLoss)\n\n**Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.08287](https://arxiv.org/abs/1911.08287)\n- github: [https://github.com/Zzh-tju/DIoU](https://github.com/Zzh-tju/DIoU)\n- github: [https://github.com/Zzh-tju/CIoU](https://github.com/Zzh-tju/CIoU)\n- github: [https://github.com/Zzh-tju/DIoU-darknet](https://github.com/Zzh-tju/DIoU-darknet)\n\n**Curriculum Self-Paced Learning for Cross-Domain Object Detection**\n\n[https://arxiv.org/abs/1911.06849](https://arxiv.org/abs/1911.06849)\n\n**Multiple Anchor Learning for Visual Object Detection**\n\n[https://arxiv.org/abs/1912.02252](https://arxiv.org/abs/1912.02252)\n\n**MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices**\n\n- intro: Google AI & Google Brain\n- arxiv: [https://arxiv.org/abs/1912.01106](https://arxiv.org/abs/1912.01106)\n\n**AugFPN: Improving Multi-scale Feature Learning for Object Detection**\n\n- intro: CVPR 2020\n- intro: CASIA & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1912.05384](https://arxiv.org/abs/1912.05384)\n- github(official, mmdetection): [https://github.com/Gus-Guo/AugFPN](https://github.com/Gus-Guo/AugFPN)\n\n**Object Detection as a Positive-Unlabeled Problem**\n\n[https://arxiv.org/abs/2002.04672](https://arxiv.org/abs/2002.04672)\n\n**Universal-RCNN: Universal Object Detector via Transferable Graph R-CNN**\n\n- intro: AAAI 2020\n- intro: Huawei Noah’s Ark Lab & South China University of Technology & Sun Yat-Sen University\n- arxiv: [https://arxiv.org/abs/2002.07417](https://arxiv.org/abs/2002.07417)\n\n**BiDet: An Efficient Binarized Object Detector**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.03961](https://arxiv.org/abs/2003.03961)\n- github: [https://github.com/ZiweiWangTHU/BiDet](https://github.com/ZiweiWangTHU/BiDet)\n\n**Revisiting the Sibling Head in Object Detector**\n\n- intro: CVPR 2020 & Method of Champion of OpenImage Challenge 2019, detection track\n- intro: SenseTime X-Lab & CUHK\n- keywords: task-aware spatial disentanglement (TSD)\n- arxiv: [https://arxiv.org/abs/2003.07540](https://arxiv.org/abs/2003.07540)\n- github: [https://github.com/Sense-X/TSD](https://github.com/Sense-X/TSD)\n\n**Extended Feature Pyramid Network for Small Object Detection**\n\n[https://arxiv.org/abs/2003.07021](https://arxiv.org/abs/2003.07021)\n\n**SaccadeNet: A Fast and Accurate Object Detector**\n\n- intro: University of Maryland & Wormpex AI Research\n- arxiv: [https://arxiv.org/abs/2003.12125](https://arxiv.org/abs/2003.12125)\n\n**Scale-Equalizing Pyramid Convolution for Object Detection**\n\n- intro: CVPR 2020\n- intro: SenseTime Research\n- arxiv: [https://arxiv.org/abs/2005.03101](https://arxiv.org/abs/2005.03101)\n- github: [https://github.com/jshilong/SEPC](https://github.com/jshilong/SEPC)\n\n**Dynamic Refinement Network for Oriented and Densely Packed Object Detection**\n\n- intro: CVPR 2020 oral\n- keywords: SKU110K-R\n- arxiv: [https://arxiv.org/abs/2005.09973](https://arxiv.org/abs/2005.09973)\n- github: [https://github.com/Anymake/DRN_CVPR2020](https://github.com/Anymake/DRN_CVPR2020)\n\n**Robust Object Detection under Occlusion with Context-Aware CompositionalNets**\n\n- intro: CVPR 2020\n- intro: Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/2005.11643](https://arxiv.org/abs/2005.11643)\n\n**DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution**\n\n- intro: Johns Hopkins University & Google Research\n- intro: COCO test-dev 54.7% box AP\n- arxiv: [https://arxiv.org/abs/2006.02334](https://arxiv.org/abs/2006.02334)\n- github(official, mmdetection): [https://github.com/joe-siyuan-qiao/DetectoRS](https://github.com/joe-siyuan-qiao/DetectoRS)\n\n**Learning a Unified Sample Weighting Network for Object Detection**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2006.06568](https://arxiv.org/abs/2006.06568)\n- github: [https://github.com/caiqi/sample-weighting-network](https://github.com/caiqi/sample-weighting-network)\n\n**2nd Place Solution for Waymo Open Dataset Challenge -- 2D Object Detection**\n\n- intro: Horizon Robotics Inc.\n- arxiv: [https://arxiv.org/abs/2006.15507](https://arxiv.org/abs/2006.15507)\n\n**Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.01571](https://arxiv.org/abs/2007.01571)\n\n**AQD: Towards Accurate Quantized Object Detection**\n\n- intro: South China University of Technology & University of Adelaide & Monash University\n- arxiv: [https://arxiv.org/abs/2007.06919](https://arxiv.org/abs/2007.06919)\n- github: [https://github.com/blueardour/model-quantization](https://github.com/blueardour/model-quantization)\n\n**Probabilistic Anchor Assignment with IoU Prediction for Object Detection**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.08103](https://arxiv.org/abs/2007.08103)\n- github: [https://github.com/kkhoot/PAA](https://github.com/kkhoot/PAA)\n\n**BorderDet: Border Feature for Dense Object Detection**\n\n- intro: ECCV 2020 oral\n- arxiv: [https://arxiv.org/abs/2007.11056](https://arxiv.org/abs/2007.11056)\n- github: [https://github.com/Megvii-BaseDetection/BorderDet](https://github.com/Megvii-BaseDetection/BorderDet)\n\n**Quantum-soft QUBO Suppression for Accurate Object Detection**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.13992](https://arxiv.org/abs/2007.13992)\n\n**VarifocalNet: An IoU-aware Dense Object Detector**\n\n- intro: Queensland University of Technology & University of Queensland\n- arxiv: [https://arxiv.org/abs/2008.13367](https://arxiv.org/abs/2008.13367)\n- github: [https://github.com/hyz-xmaster/VarifocalNet](https://github.com/hyz-xmaster/VarifocalNet)\n\n**The 1st Tiny Object Detection Challenge:Methods and Results**\n\n- intro: ECCV2020 Workshop on Real-world Computer Vision from Inputs with Limited Quality (RLQ) and Tiny Object Detection Challenge\n- arxiv: [https://arxiv.org/abs/2009.07506](https://arxiv.org/abs/2009.07506)\n\n**MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection**\n\n- intro: ECCV 2020\n- intro: SenseTime & CUHK\n- arxiv: [https://arxiv.org/abs/2009.11528](https://arxiv.org/abs/2009.11528)\n\n**SEA: Bridging the Gap Between One- and Two-stage Detector Distillation via SEmantic-aware Alignment**\n\n- intro: The Chinese University of Hong Kong & SmartMore\n- arxiv: [https://arxiv.org/abs/2203.00862](https://arxiv.org/abs/2203.00862)\n\n**A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection**\n\n- intro: NeurIPS 2020 spotlight\n- intro: Middle East Technical University\n- keywords: average Localization-Recall-Precision (aLRP)\n- arxiv: [https://arxiv.org/abs/2009.13592](https://arxiv.org/abs/2009.13592)\n- github(official, Pytorch): [https://github.com/kemaloksuz/aLRPLoss](https://github.com/kemaloksuz/aLRPLoss)\n\n**Effective Fusion Factor in FPN for Tiny Object Detection**\n\n- intro: WACV 2021\n- arxiv: [https://arxiv.org/abs/2011.02298](https://arxiv.org/abs/2011.02298)\n\n**Bi-Dimensional Feature Alignment for Cross-Domain Object Detection**\n\n- intro: ECCV 2020 TASK-CV Workshop\n- arxiv: [https://arxiv.org/abs/2011.07205](https://arxiv.org/abs/2011.07205)\n\n**Rethinking Transformer-based Set Prediction for Object Detection**\n\n- intro: Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/2011.10881](https://arxiv.org/abs/2011.10881)\n\n**Unsupervised Object Detection with LiDAR Clues**\n\n- intro: SenseTime & USTC & CASIA & CAS\n- arxiv: [https://arxiv.org/abs/2011.12953](https://arxiv.org/abs/2011.12953)\n\n**Self-EMD: Self-Supervised Object Detection without ImageNet**\n\n- intro: MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2011.13677](https://arxiv.org/abs/2011.13677)\n\n**End-to-End Object Detection with Fully Convolutional Network**\n\n- intro: Megvii Technology & Xi’an Jiaotong University\n- keywords: Prediction-aware One- To-One (POTO) label assignment, 3D Max Filtering (3DMF)\n- arxiv: [https://arxiv.org/abs/2012.03544](https://arxiv.org/abs/2012.03544)\n- github: [https://github.com/Megvii-BaseDetection/DeFCN](https://github.com/Megvii-BaseDetection/DeFCN)\n\n**Fine-Grained Dynamic Head for Object Detection**\n\n- intro: NeurIPS 2020\n- arxiv: [https://arxiv.org/abs/2012.03519](https://arxiv.org/abs/2012.03519)\n- github: [https://github.com/StevenGrove/DynamicHead](https://github.com/StevenGrove/DynamicHead)\n\n**Focal and Efficient IOU Loss for Accurate Bounding Box Regression**\n\n- intro: South China University of Technology & 2Horizon Robotics & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/2101.08158](https://arxiv.org/abs/2101.08158)\n\n**Scale Normalized Image Pyramids with AutoFocus for Object Detection**\n\n- intro: T-PAMI 2021\n- arxiv: [https://arxiv.org/abs/2102.05646](https://arxiv.org/abs/2102.05646)\n- github: [https://github.com/mahyarnajibi/SNIPER](https://github.com/mahyarnajibi/SNIPER)\n\n**DetCo: Unsupervised Contrastive Learning for Object Detection**\n\n- intro: The University of Hong Kong & Huawei Noah’s Ark Lab & Wuhan University & Nanjing University & Chinese University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2102.04803](https://arxiv.org/abs/2102.04803)\n- github: [https://github.com/xieenze/DetCo](https://github.com/xieenze/DetCo)\n- github: [https://github.com/open-mmlab/OpenSelfSup](https://github.com/open-mmlab/OpenSelfSup)\n\n**RMOPP: Robust Multi-Objective Post-Processing for Effective Object Detection**\n\n[https://arxiv.org/abs/2102.04582](https://arxiv.org/abs/2102.04582)\n\n**Instance Localization for Self-supervised Detection Pretraining**\n\n- intro: Chinese University of Hong Kong & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/2102.08318](https://arxiv.org/abs/2102.08318)\n\n**Localization Distillation for Object Detection**\n\n- arxiv: [https://arxiv.org/abs/2102.12252](https://arxiv.org/abs/2102.12252)\n- github: [https://github.com/HikariTJU/LD](https://github.com/HikariTJU/LD)\n\n**General Instance Distillation for Object Detection**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.02340](https://arxiv.org/abs/2103.02340)\n\n**Towards Open World Object Detection**\n\n- intro: CVPR 2021 oral\n- arxiv: [https://arxiv.org/abs/2103.02603](https://arxiv.org/abs/2103.02603)\n- github: [https://github.com/JosephKJ/OWOD](https://github.com/JosephKJ/OWOD)\n\n**Data Augmentation for Object Detection via Differentiable Neural Rendering**\n\n- arxiv: [https://arxiv.org/abs/2103.02852](https://arxiv.org/abs/2103.02852)\n- github: [https://github.com/Guanghan/DANR](https://github.com/Guanghan/DANR)\n\n**Revisiting the Loss Weight Adjustment in Object Detection**\n\n- intro: University of Science and Technology of China & University of Michigan\n- arxiv: [https://arxiv.org/abs/2103.09488](https://arxiv.org/abs/2103.09488)\n- github: [https://github.com/ywx-hub/ALWA](https://github.com/ywx-hub/ALWA)\n\n**You Only Look One-level Feature**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.09460](https://arxiv.org/abs/2103.09460)\n- github: [https://github.com/megvii-model/YOLOF](https://github.com/megvii-model/YOLOF)\n\n**Optimization for Oriented Object Detection via Representation Invariance Loss**\n\n- arxiv: [https://arxiv.org/abs/2103.11636](https://arxiv.org/abs/2103.11636)\n- github: [https://github.com/ming71/RIDet](https://github.com/ming71/RIDet)\n\n**Dynamic Anchor Learning for Arbitrary-Oriented Object Detection**\n\n- intro: AAAI 2021\n- arxiv: [https://arxiv.org/abs/2012.04150](https://arxiv.org/abs/2012.04150)\n- github: [https://github.com/ming71/DAL](https://github.com/ming71/DAL)\n\n**Control Distance IoU and Control Distance IoU Loss Function for Better Bounding Box Regression**\n\n[https://arxiv.org/abs/2103.11696](https://arxiv.org/abs/2103.11696)\n\n**OTA: Optimal Transport Assignment for Object Detection**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.14259](https://arxiv.org/abs/2103.14259)\n- github: [https://github.com/Megvii-BaseDetection/OTA](https://github.com/Megvii-BaseDetection/OTA)\n\n**Distilling Object Detectors via Decoupled Features**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.14475](https://arxiv.org/abs/2103.14475)\n- github: [https://github.com/ggjy/DeFeat.pytorch](https://github.com/ggjy/DeFeat.pytorch)\n\n**Distilling a Powerful Student Model via Online Knowledge Distillation**\n\n- arxiv: [https://arxiv.org/abs/2103.14473](https://arxiv.org/abs/2103.14473)\n- github: [https://github.com/SJLeo/FFSD](https://github.com/SJLeo/FFSD)\n\n**IQDet: Instance-wise Quality Distribution Sampling for Object Detection**\n\n- intro: CVPR 2021\n- intro: Megvii Technology\n- arxiv: [https://arxiv.org/abs/2104.06936](https://arxiv.org/abs/2104.06936)\n\n**You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection**\n\n- intro: Huazhong University of Science & Technology, Horizon Robotics\n- arxiv: [https://arxiv.org/abs/2106.00666](https://arxiv.org/abs/2106.00666)\n- github: [https://github.com/hustvl/YOLOS](https://github.com/hustvl/YOLOS)\n\n**Augmenting Anchors by the Detector Itself**\n\n[https://arxiv.org/abs/2105.14086](https://arxiv.org/abs/2105.14086)\n\n**Rethinking Training from Scratch for Object Detection**\n\n- intro: Zhejiang University\n- arxiv: [https://arxiv.org/abs/2106.03112](https://arxiv.org/abs/2106.03112)\n\n**Dynamic Head: Unifying Object Detection Heads with Attentions**\n\n- intro: CVPR 2021\n- intro: Microsoft\n- arxiv: [https://arxiv.org/abs/2106.08322](https://arxiv.org/abs/2106.08322)\n- github: [https://github.com/microsoft/DynamicHead](https://github.com/microsoft/DynamicHead)\n\n**Disentangle Your Dense Object Detector**\n\n- intro: ACM MM 2021\n- arxiv: [https://arxiv.org/abs/2107.02963](https://arxiv.org/abs/2107.02963)\n- github: [https://github.com/zehuichen123/DDOD](https://github.com/zehuichen123/DDOD)\n\n**Improving Object Detection by Label Assignment Distillation**\n\n- arxiv: [https://arxiv.org/abs/2108.10520](https://arxiv.org/abs/2108.10520)\n- github: [https://github.com/cybercore-co-ltd/CoLAD_paper](https://github.com/cybercore-co-ltd/CoLAD_paper)\n\n**Progressive Hard-case Mining across Pyramid Levels in Object Detection**\n\n- intro:  Baidu Inc.\n- arxiv: [https://arxiv.org/abs/2109.07217](https://arxiv.org/abs/2109.07217)\n- github: [https://github.com/zimoqingfeng/UMOP](https://github.com/zimoqingfeng/UMOP)\n\n**Multi-Scale Aligned Distillation for Low-Resolution Detection**\n\n- intro: CVPR 2021\n- intro: The Chinese University of Hong Kong & Adobe Research & SmartMore\n- arxiv: [https://arxiv.org/abs/2109.06875](https://arxiv.org/abs/2109.06875)\n- github: [https://github.com/dvlab-research/MSAD](https://github.com/dvlab-research/MSAD)\n\n**Pix2seq: A Language Modeling Framework for Object Detection**\n\n- intro: Google Research, Brain Team\n- arxiv: [https://arxiv.org/abs/2109.10852](https://arxiv.org/abs/2109.10852)\n\n**Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity**\n\n- intro: NeurIPS 2021\n- intro: Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/2110.14191](https://arxiv.org/abs/2110.14191)\n- github: [https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection](https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection)\n\n**Bootstrap Your Object Detector via Mixed Training**\n\n- intro: NeurIPS 2021 Spotlight\n- intro: Huazhong University of Science and Technology & Xi’an Jiaotong University & Microsoft Research Asia\n- keywords: MixTraining\n- arxiv: [https://arxiv.org/abs/2111.03056](https://arxiv.org/abs/2111.03056)\n- github: [https://github.com/MendelXu/MixTraining](https://github.com/MendelXu/MixTraining)\n\n**PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices**\n\n- intro: Baidu Inc.\n- arxiv: [https://arxiv.org/abs/2111.00902](https://arxiv.org/abs/2111.00902)\n- github: [https://github.com/PaddlePaddle/PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)\n\n**Toward Minimal Misalignment at Minimal Cost in One-Stage and Anchor-Free Object Detection**\n\n[https://arxiv.org/abs/2112.08902](https://arxiv.org/abs/2112.08902)\n\n**GiraffeDet: A Heavy-Neck Paradigm for Object Detection**\n\n[https://arxiv.org/abs/2202.04256](https://arxiv.org/abs/2202.04256)\n\n**A Dual Weighting Label Assignment Scheme for Object Detection**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.09730](https://arxiv.org/abs/2203.09730)\n- github: [https://github.com/strongwolf/DW](https://github.com/strongwolf/DW)\n\n**QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2103.09136](https://arxiv.org/abs/2103.09136)\n- github: [https://github.com/ChenhongyiYang/QueryDet-PyTorch](https://github.com/ChenhongyiYang/QueryDet-PyTorch)\n\n# Two-Stage Object Detection\n\n## R-CNN\n\n**Rich feature hierarchies for accurate object detection and semantic segmentation**\n\n- intro: R-CNN\n- arxiv: [http://arxiv.org/abs/1311.2524](http://arxiv.org/abs/1311.2524)\n- supp: [http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf](http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf)\n- slides: [http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf](http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf)\n- slides: [http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf](http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf)\n- github: [https://github.com/rbgirshick/rcnn](https://github.com/rbgirshick/rcnn)\n- notes: [http://zhangliliang.com/2014/07/23/paper-note-rcnn/](http://zhangliliang.com/2014/07/23/paper-note-rcnn/)\n- caffe-pr(\"Make R-CNN the Caffe detection example\"): [https://github.com/BVLC/caffe/pull/482](https://github.com/BVLC/caffe/pull/482) \n\n## Fast R-CNN\n\n**Fast R-CNN**\n\n- arxiv: [http://arxiv.org/abs/1504.08083](http://arxiv.org/abs/1504.08083)\n- slides: [http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf](http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf)\n- github: [https://github.com/rbgirshick/fast-rcnn](https://github.com/rbgirshick/fast-rcnn)\n- github(COCO-branch): [https://github.com/rbgirshick/fast-rcnn/tree/coco](https://github.com/rbgirshick/fast-rcnn/tree/coco)\n- webcam demo: [https://github.com/rbgirshick/fast-rcnn/pull/29](https://github.com/rbgirshick/fast-rcnn/pull/29)\n- notes: [http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/](http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/)\n- notes: [http://blog.csdn.net/linj_m/article/details/48930179](http://blog.csdn.net/linj_m/article/details/48930179)\n- github(\"Fast R-CNN in MXNet\"): [https://github.com/precedenceguo/mx-rcnn](https://github.com/precedenceguo/mx-rcnn)\n- github: [https://github.com/mahyarnajibi/fast-rcnn-torch](https://github.com/mahyarnajibi/fast-rcnn-torch)\n- github: [https://github.com/apple2373/chainer-simple-fast-rnn](https://github.com/apple2373/chainer-simple-fast-rnn)\n- github: [https://github.com/zplizzi/tensorflow-fast-rcnn](https://github.com/zplizzi/tensorflow-fast-rcnn)\n\n**A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.03414](https://arxiv.org/abs/1704.03414)\n- paper: [http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf](http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf)\n- github(Caffe): [https://github.com/xiaolonw/adversarial-frcnn](https://github.com/xiaolonw/adversarial-frcnn)\n\n## Faster R-CNN\n\n**Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks**\n\n- intro: NIPS 2015\n- arxiv: [http://arxiv.org/abs/1506.01497](http://arxiv.org/abs/1506.01497)\n- gitxiv: [http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region](http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region)\n- slides: [http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf](http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf)\n- github(official, Matlab): [https://github.com/ShaoqingRen/faster_rcnn](https://github.com/ShaoqingRen/faster_rcnn)\n- github: [https://github.com/rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn)\n- github(MXNet): [https://github.com/msracver/Deformable-ConvNets/tree/master/faster_rcnn](https://github.com/msracver/Deformable-ConvNets/tree/master/faster_rcnn)\n- github: [https://github.com//jwyang/faster-rcnn.pytorch](https://github.com//jwyang/faster-rcnn.pytorch)\n- github: [https://github.com/mitmul/chainer-faster-rcnn](https://github.com/mitmul/chainer-faster-rcnn)\n- github: [https://github.com/andreaskoepf/faster-rcnn.torch](https://github.com/andreaskoepf/faster-rcnn.torch)\n- github: [https://github.com/ruotianluo/Faster-RCNN-Densecap-torch](https://github.com/ruotianluo/Faster-RCNN-Densecap-torch)\n- github: [https://github.com/smallcorgi/Faster-RCNN_TF](https://github.com/smallcorgi/Faster-RCNN_TF)\n- github: [https://github.com/CharlesShang/TFFRCNN](https://github.com/CharlesShang/TFFRCNN)\n- github(C++ demo): [https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus](https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus)\n- github: [https://github.com/yhenon/keras-frcnn](https://github.com/yhenon/keras-frcnn)\n- github: [https://github.com/Eniac-Xie/faster-rcnn-resnet](https://github.com/Eniac-Xie/faster-rcnn-resnet)\n- github(C++): [https://github.com/D-X-Y/caffe-faster-rcnn/tree/dev](https://github.com/D-X-Y/caffe-faster-rcnn/tree/dev)\n\n**R-CNN minus R**\n\n- intro: BMVC 2015\n- arxiv: [http://arxiv.org/abs/1506.06981](http://arxiv.org/abs/1506.06981)\n\n**Faster R-CNN in MXNet with distributed implementation and data parallelization**\n\n- github: [https://github.com/dmlc/mxnet/tree/master/example/rcnn](https://github.com/dmlc/mxnet/tree/master/example/rcnn)\n\n**Contextual Priming and Feedback for Faster R-CNN**\n\n- intro: ECCV 2016. Carnegie Mellon University\n- paper: [http://abhinavsh.info/context_priming_feedback.pdf](http://abhinavsh.info/context_priming_feedback.pdf)\n- poster: [http://www.eccv2016.org/files/posters/P-1A-20.pdf](http://www.eccv2016.org/files/posters/P-1A-20.pdf)\n\n**An Implementation of Faster RCNN with Study for Region Sampling**\n\n- intro: Technical Report, 3 pages. CMU\n- arxiv: [https://arxiv.org/abs/1702.02138](https://arxiv.org/abs/1702.02138)\n- github: [https://github.com/endernewton/tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn)\n\n**Interpretable R-CNN**\n\n- intro: North Carolina State University & Alibaba\n- keywords: AND-OR Graph (AOG)\n- arxiv: [https://arxiv.org/abs/1711.05226](https://arxiv.org/abs/1711.05226)\n\n**Light-Head R-CNN: In Defense of Two-Stage Object Detector**\n\n- intro: Tsinghua University & Megvii Inc\n- arxiv: [https://arxiv.org/abs/1711.07264](https://arxiv.org/abs/1711.07264)\n- github(official, Tensorflow): [https://github.com/zengarden/light_head_rcnn](https://github.com/zengarden/light_head_rcnn)\n- github: [https://github.com/terrychenism/Deformable-ConvNets/blob/master/rfcn/symbols/resnet_v1_101_rfcn_light.py#L784](https://github.com/terrychenism/Deformable-ConvNets/blob/master/rfcn/symbols/resnet_v1_101_rfcn_light.py#L784)\n\n**Cascade R-CNN: Delving into High Quality Object Detection**\n\n- intro: CVPR 2018. UC San Diego\n- arxiv: [https://arxiv.org/abs/1712.00726](https://arxiv.org/abs/1712.00726)\n- github(Caffe, official): [https://github.com/zhaoweicai/cascade-rcnn](https://github.com/zhaoweicai/cascade-rcnn)\n\n**Cascade R-CNN: High Quality Object Detection and Instance Segmentation**\n\n -arxiv: [https://arxiv.org/abs/1906.09756](https://arxiv.org/abs/1906.09756)\n- github(Caffe, official): [https://github.com/zhaoweicai/cascade-rcnn](https://github.com/zhaoweicai/cascade-rcnn)\n- github(official): [https://github.com/zhaoweicai/Detectron-Cascade-RCNN](https://github.com/zhaoweicai/Detectron-Cascade-RCNN)\n\n**Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution**\n\n- intro: NeurIPS 2019 spotlight\n- arxiv: [https://arxiv.org/abs/1909.06720](https://arxiv.org/abs/1909.06720)\n- github: [https://github.com/thangvubk/Cascade-RPN](https://github.com/thangvubk/Cascade-RPN)\n\n**SMC Faster R-CNN: Toward a scene-specialized multi-object detector**\n\n[https://arxiv.org/abs/1706.10217](https://arxiv.org/abs/1706.10217)\n\n**Domain Adaptive Faster R-CNN for Object Detection in the Wild**\n\n- intro: CVPR 2018. ETH Zurich & ESAT/PSI\n- arxiv: [https://arxiv.org/abs/1803.03243](https://arxiv.org/abs/1803.03243)\n- github(official. Caffe): [https://github.com/yuhuayc/da-faster-rcnn](https://github.com/yuhuayc/da-faster-rcnn)\n\n**Robust Physical Adversarial Attack on Faster R-CNN Object Detector**\n\n[https://arxiv.org/abs/1804.05810](https://arxiv.org/abs/1804.05810)\n\n**Auto-Context R-CNN**\n\n- intro: Rejected by ECCV18\n- arxiv: [https://arxiv.org/abs/1807.02842](https://arxiv.org/abs/1807.02842)\n\n**Grid R-CNN**\n\n- intro: CVPR 2019\n- intro: SenseTime\n- arxiv: [https://arxiv.org/abs/1811.12030](https://arxiv.org/abs/1811.12030)\n\n**Grid R-CNN Plus: Faster and Better**\n\n- intro: SenseTime Research & CUHK & Beihang University\n- arxiv: [https://arxiv.org/abs/1906.05688](https://arxiv.org/abs/1906.05688)\n- github: [https://github.com/STVIR/Grid-R-CNN](https://github.com/STVIR/Grid-R-CNN)\n\n**Few-shot Adaptive Faster R-CNN**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.09372](https://arxiv.org/abs/1903.09372)\n\n**Libra R-CNN: Towards Balanced Learning for Object Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.02701](https://arxiv.org/abs/1904.02701)\n\n**Rethinking Classification and Localization in R-CNN**\n\n- intro: Northeastern University & Microsoft\n- arxiv: [https://arxiv.org/abs/1904.06493](https://arxiv.org/abs/1904.06493)\n\n**Reprojection R-CNN: A Fast and Accurate Object Detector for 360° Images**\n\n- intro: Peking University\n- arxiv: [https://arxiv.org/abs/1907.11830](https://arxiv.org/abs/1907.11830)\n\n**Rethinking Classification and Localization for Cascade R-CNN**\n\n- intro: BMVC 2019\n- arxiv: [https://arxiv.org/abs/1907.11914](https://arxiv.org/abs/1907.11914)\n\n**IoU-uniform R-CNN: Breaking Through the Limitations of RPN**\n\n- arxiv: [https://arxiv.org/abs/1912.05190](https://arxiv.org/abs/1912.05190)\n- github(mmdetection): [https://github.com/zl1994/IoU-Uniform-R-CNN](https://github.com/zl1994/IoU-Uniform-R-CNN)\n\n**Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training**\n\n- arxiv: [https://arxiv.org/abs/2004.06002](https://arxiv.org/abs/2004.06002)\n- github: [https://github.com/hkzhang95/DynamicRCNN](https://github.com/hkzhang95/DynamicRCNN)\n\n**Delving into the Imbalance of Positive Proposals in Two-stage Object Detection**\n\n- intro: Waseda University & Tencent AI Lab & Nanjing University of Science and Technology\n- arxiv: [https://arxiv.org/abs/2005.11472](https://arxiv.org/abs/2005.11472)\n\n**Hierarchical Context Embedding for Region-based Object Detection**\n\n- intro: ECCV 2020\n- intro: Nanjing University & Megvii Technology\n- arxiv: [https://arxiv.org/abs/2008.01338](https://arxiv.org/abs/2008.01338)\n\n**Sparse R-CNN: End-to-End Object Detection with Learnable Proposals**\n\n- intro: CVPR 2021\n- intro: The University of Hong Kong & Tongji University & ByteDance AI Lab 4University of California\n- arxiv: [https://arxiv.org/abs/2011.12450](https://arxiv.org/abs/2011.12450)\n- github: [https://github.com/PeizeSun/SparseR-CNN](https://github.com/PeizeSun/SparseR-CNN)\n\n**Dynamic Sparse R-CNN**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2205.02101](https://arxiv.org/abs/2205.02101)\n\n**Featurized Query R-CNN**\n\n- intro: Huazhong University of Science & Technology & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/2206.06258](https://arxiv.org/abs/2206.06258)\n- github: [https://github.com/hustvl/Featurized-QueryRCNN](https://github.com/hustvl/Featurized-QueryRCNN)\n\n**Augmenting Proposals by the Detector Itself**\n\n- intro: Tsinghua University & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2101.11789](https://arxiv.org/abs/2101.11789)\n\n**Probabilistic two-stage detection**\n\n- intro: UT Austin & Intel Labs\n- arxiv: [https://arxiv.org/abs/2103.07461](https://arxiv.org/abs/2103.07461)\n- github: [https://github.com/xingyizhou/CenterNet2](https://github.com/xingyizhou/CenterNet2)\n\n# Single-Shot Object Detection\n\n## YOLO\n\n**You Only Look Once: Unified, Real-Time Object Detection**\n\n![](https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67)\n\n- arxiv: [http://arxiv.org/abs/1506.02640](http://arxiv.org/abs/1506.02640)\n- code: [http://pjreddie.com/darknet/yolo/](http://pjreddie.com/darknet/yolo/)\n- github: [https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)\n- blog: [https://pjreddie.com/publications/yolo/](https://pjreddie.com/publications/yolo/)\n- slides: [https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p](https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/](https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/)\n- github: [https://github.com/gliese581gg/YOLO_tensorflow](https://github.com/gliese581gg/YOLO_tensorflow)\n- github: [https://github.com/xingwangsfu/caffe-yolo](https://github.com/xingwangsfu/caffe-yolo)\n- github: [https://github.com/frankzhangrui/Darknet-Yolo](https://github.com/frankzhangrui/Darknet-Yolo)\n- github: [https://github.com/BriSkyHekun/py-darknet-yolo](https://github.com/BriSkyHekun/py-darknet-yolo)\n- github: [https://github.com/tommy-qichang/yolo.torch](https://github.com/tommy-qichang/yolo.torch)\n- github: [https://github.com/frischzenger/yolo-windows](https://github.com/frischzenger/yolo-windows)\n- github: [https://github.com/AlexeyAB/yolo-windows](https://github.com/AlexeyAB/yolo-windows)\n- github: [https://github.com/nilboy/tensorflow-yolo](https://github.com/nilboy/tensorflow-yolo)\n\n**darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++**\n\n- blog: [https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp](https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp)\n- github: [https://github.com/thtrieu/darkflow](https://github.com/thtrieu/darkflow)\n\n**Start Training YOLO with Our Own Data**\n\n![](http://guanghan.info/blog/en/wp-content/uploads/2015/12/images-40.jpg)\n\n- intro: train with customized data and class numbers/labels. Linux / Windows version for darknet.\n- blog: [http://guanghan.info/blog/en/my-works/train-yolo/](http://guanghan.info/blog/en/my-works/train-yolo/)\n- github: [https://github.com/Guanghan/darknet](https://github.com/Guanghan/darknet)\n\n**YOLO: Core ML versus MPSNNGraph**\n\n- intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API.\n- blog: [http://machinethink.net/blog/yolo-coreml-versus-mps-graph/](http://machinethink.net/blog/yolo-coreml-versus-mps-graph/)\n- github: [https://github.com/hollance/YOLO-CoreML-MPSNNGraph](https://github.com/hollance/YOLO-CoreML-MPSNNGraph)\n\n**TensorFlow YOLO object detection on Android**\n\n- intro: Real-time object detection on Android using the YOLO network with TensorFlow\n- github: [https://github.com/natanielruiz/android-yolo](https://github.com/natanielruiz/android-yolo)\n\n**Computer Vision in iOS – Object Detection**\n\n- blog: [https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/](https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/)\n- github:[https://github.com/r4ghu/iOS-CoreML-Yolo](https://github.com/r4ghu/iOS-CoreML-Yolo)\n\n## YOLOv2\n\n**YOLO9000: Better, Faster, Stronger**\n\n- arxiv: [https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242)\n- code: [http://pjreddie.com/yolo9000/](http://pjreddie.com/yolo9000/)\n- github(Chainer): [https://github.com/leetenki/YOLOv2](https://github.com/leetenki/YOLOv2)\n- github(Keras): [https://github.com/allanzelener/YAD2K](https://github.com/allanzelener/YAD2K)\n- github(PyTorch): [https://github.com/longcw/yolo2-pytorch](https://github.com/longcw/yolo2-pytorch)\n- github(Tensorflow): [https://github.com/hizhangp/yolo_tensorflow](https://github.com/hizhangp/yolo_tensorflow)\n- github(Windows): [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)\n- github: [https://github.com/choasUp/caffe-yolo9000](https://github.com/choasUp/caffe-yolo9000)\n- github: [https://github.com/philipperemy/yolo-9000](https://github.com/philipperemy/yolo-9000)\n\n**darknet_scripts**\n\n- intro: Auxilary scripts to work with (YOLO) darknet deep learning famework. AKA -> How to generate YOLO anchors?\n- github: [https://github.com/Jumabek/darknet_scripts](https://github.com/Jumabek/darknet_scripts)\n\n**Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2**\n\n- github: [https://github.com/AlexeyAB/Yolo_mark](https://github.com/AlexeyAB/Yolo_mark)\n\n**LightNet: Bringing pjreddie's DarkNet out of the shadows**\n\n[https://github.com//explosion/lightnet](https://github.com//explosion/lightnet)\n\n**YOLO v2 Bounding Box Tool**\n\n- intro: Bounding box labeler tool to generate the training data in the format YOLO v2 requires.\n- github: [https://github.com/Cartucho/yolo-boundingbox-labeler-GUI](https://github.com/Cartucho/yolo-boundingbox-labeler-GUI)\n\n## YOLOv3\n\n**YOLOv3: An Incremental Improvement**\n\n- project page: [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)\n- paper: [https://pjreddie.com/media/files/papers/YOLOv3.pdf](https://pjreddie.com/media/files/papers/YOLOv3.pdf)\n- arxiv: [https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)\n- githb: [https://github.com/DeNA/PyTorch_YOLOv3](https://github.com/DeNA/PyTorch_YOLOv3)\n- github: [https://github.com/eriklindernoren/PyTorch-YOLOv3](https://github.com/eriklindernoren/PyTorch-YOLOv3)\n\n**Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving**\n\n[https://arxiv.org/abs/1904.04620](https://arxiv.org/abs/1904.04620)\n\n**YOLO-LITE: A Real-Time Object Detection Algorithm Optimized for Non-GPU Computers**\n\n[https://arxiv.org/abs/1811.05588](https://arxiv.org/abs/1811.05588)\n\n**Spiking-YOLO: Spiking Neural Network for Real-time Object Detection**\n\n[https://arxiv.org/abs/1903.06530](https://arxiv.org/abs/1903.06530)\n\n**YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection**\n\n[https://arxiv.org/abs/1910.01271](https://arxiv.org/abs/1910.01271)\n\n**REQ-YOLO: A Resource-Aware, Efficient Quantization Framework for Object Detection on FPGAs**\n\n[https://arxiv.org/abs/1909.13396](https://arxiv.org/abs/1909.13396)\n\n**Poly-YOLO: higher speed, more precise detection and instance segmentation for YOLOv3**\n\n- intro: TPAMI\n- arxiv: [https://arxiv.org/abs/2005.13243](https://arxiv.org/abs/2005.13243)\n- gitlab: [https://gitlab.com/irafm-ai/poly-yolo](https://gitlab.com/irafm-ai/poly-yolo)\n\n## YOLOv4\n\n**YOLOv4: Optimal Speed and Accuracy of Object Detection**\n\n- keywords: Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT), Mish-activation\n- arxiv: [https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934)\n- github: [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)\n- github: [https://github.com/WongKinYiu/PyTorch_YOLOv4](https://github.com/WongKinYiu/PyTorch_YOLOv4)\n\n**YOLOX: Exceeding YOLO Series in 2021**\n\n- intro: Megvii Technology\n- arxiv: [https://arxiv.org/abs/2107.08430](https://arxiv.org/abs/2107.08430)\n- github: [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n\n**PP-YOLO: An Effective and Efficient Implementation of Object Detector**\n\n- intro: Baidu Inc.\n- arxiv: [https://arxiv.org/abs/2007.12099](https://arxiv.org/abs/2007.12099)\n- github: [https://github.com/PaddlePaddle/PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)\n\n## YOLOv7\n\n**YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors**\n\n- arxiv: [https://arxiv.org/abs/2207.02696](https://arxiv.org/abs/2207.02696)\n- github: [https://github.com/WongKinYiu/yolov7](https://github.com/WongKinYiu/yolov7)\n\n**Real-time Object Detection for Streaming Perception**\n\n- intro: CVPR 2022 oral\n- arxiv: [https://arxiv.org/abs/2203.12338](https://arxiv.org/abs/2203.12338)\n- github: [https://github.com/yancie-yjr/StreamYOLO](https://github.com/yancie-yjr/StreamYOLO)\n\n## SSD\n\n**SSD: Single Shot MultiBox Detector**\n\n![](https://camo.githubusercontent.com/ad9b147ed3a5f48ffb7c3540711c15aa04ce49c6/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373642e706e67)\n\n- intro: ECCV 2016 Oral\n- arxiv: [http://arxiv.org/abs/1512.02325](http://arxiv.org/abs/1512.02325)\n- paper: [http://www.cs.unc.edu/~wliu/papers/ssd.pdf](http://www.cs.unc.edu/~wliu/papers/ssd.pdf)\n- slides: [http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf](http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf)\n- github(Official): [https://github.com/weiliu89/caffe/tree/ssd](https://github.com/weiliu89/caffe/tree/ssd)\n- video: [http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973](http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973)\n- github: [https://github.com/zhreshold/mxnet-ssd](https://github.com/zhreshold/mxnet-ssd)\n- github: [https://github.com/zhreshold/mxnet-ssd.cpp](https://github.com/zhreshold/mxnet-ssd.cpp)\n- github: [https://github.com/rykov8/ssd_keras](https://github.com/rykov8/ssd_keras)\n- github: [https://github.com/balancap/SSD-Tensorflow](https://github.com/balancap/SSD-Tensorflow)\n- github: [https://github.com/amdegroot/ssd.pytorch](https://github.com/amdegroot/ssd.pytorch)\n- github(Caffe): [https://github.com/chuanqi305/MobileNet-SSD](https://github.com/chuanqi305/MobileNet-SSD)\n\n**What's the diffience in performance between this new code you pushed and the previous code? #327**\n\n[https://github.com/weiliu89/caffe/issues/327](https://github.com/weiliu89/caffe/issues/327)\n\n**DSSD : Deconvolutional Single Shot Detector**\n\n- intro: UNC Chapel Hill & Amazon Inc\n- arxiv: [https://arxiv.org/abs/1701.06659](https://arxiv.org/abs/1701.06659)\n- github: [https://github.com/chengyangfu/caffe/tree/dssd](https://github.com/chengyangfu/caffe/tree/dssd)\n- github: [https://github.com/MTCloudVision/mxnet-dssd](https://github.com/MTCloudVision/mxnet-dssd)\n- demo: [http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4](http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4)\n\n**Enhancement of SSD by concatenating feature maps for object detection**\n\n- intro: rainbow SSD (R-SSD)\n- arxiv: [https://arxiv.org/abs/1705.09587](https://arxiv.org/abs/1705.09587)\n\n**Context-aware Single-Shot Detector**\n\n- keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs),  theoretical receptive fields (TRFs)\n- arxiv: [https://arxiv.org/abs/1707.08682](https://arxiv.org/abs/1707.08682)\n\n**Feature-Fused SSD: Fast Detection for Small Objects**\n\n[https://arxiv.org/abs/1709.05054](https://arxiv.org/abs/1709.05054)\n\n**FSSD: Feature Fusion Single Shot Multibox Detector**\n\n[https://arxiv.org/abs/1712.00960](https://arxiv.org/abs/1712.00960)\n\n**Weaving Multi-scale Context for Single Shot Detector**\n\n- intro: WeaveNet\n- keywords: fuse multi-scale information\n- arxiv: [https://arxiv.org/abs/1712.03149](https://arxiv.org/abs/1712.03149)\n\n**Extend the shallow part of Single Shot MultiBox Detector via Convolutional Neural Network**\n\n- keywords: ESSD\n- arxiv: [https://arxiv.org/abs/1801.05918](https://arxiv.org/abs/1801.05918)\n\n**Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection**\n\n[https://arxiv.org/abs/1802.06488](https://arxiv.org/abs/1802.06488)\n\n**MDSSD: Multi-scale Deconvolutional Single Shot Detector for small objects**\n\n- intro: Zhengzhou University\n- arxiv: [https://arxiv.org/abs/1805.07009](https://arxiv.org/abs/1805.07009)\n\n**Accurate Single Stage Detector Using Recurrent Rolling Convolution**\n\n- intro: CVPR 2017. SenseTime\n- keywords: Recurrent Rolling Convolution (RRC)\n- arxiv: [https://arxiv.org/abs/1704.05776](https://arxiv.org/abs/1704.05776)\n- github: [https://github.com/xiaohaoChen/rrc_detection](https://github.com/xiaohaoChen/rrc_detection)\n\n**Residual Features and Unified Prediction Network for Single Stage Detection**\n\n[https://arxiv.org/abs/1707.05031](https://arxiv.org/abs/1707.05031)\n\n## RetinaNet\n\n**Focal Loss for Dense Object Detection**\n\n- intro: ICCV 2017 Best student paper award. Facebook AI Research\n- keywords: RetinaNet\n- arxiv: [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002)\n\n**Cascade RetinaNet: Maintaining Consistency for Single-Stage Object Detection**\n\n- intro: BMVC 2019\n- keywords: Cas-RetinaNet, Feature Consistency Module\n- arxiv: [https://arxiv.org/abs/1907.06881](https://arxiv.org/abs/1907.06881)\n\n**Focal Loss Dense Detector for Vehicle Surveillance**\n\n[https://arxiv.org/abs/1803.01114](https://arxiv.org/abs/1803.01114)\n\n**Single-Shot Refinement Neural Network for Object Detection**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1711.06897](https://arxiv.org/abs/1711.06897)\n- github: [https://github.com/sfzhang15/RefineDet](https://github.com/sfzhang15/RefineDet)\n- github: [https://github.com/MTCloudVision/RefineDet-Mxnet](https://github.com/MTCloudVision/RefineDet-Mxnet)\n\n**Single-Shot Bidirectional Pyramid Networks for High-Quality Object Detection**\n\n- intro: Singapore Management University & Zhejiang University\n- arxiv: [https://arxiv.org/abs/1803.08208](https://arxiv.org/abs/1803.08208)\n\n**Dual Refinement Network for Single-Shot Object Detection**\n\n[https://arxiv.org/abs/1807.08638](https://arxiv.org/abs/1807.08638)\n\n**ScratchDet:Exploring to Train Single-Shot Object Detectors from Scratch**\n\n- arxiv: [https://arxiv.org/abs/1810.08425](https://arxiv.org/abs/1810.08425)\n- github: [https://github.com/KimSoybean/ScratchDet](https://github.com/KimSoybean/ScratchDethttps://github.com/KimSoybean/ScratchDet)\n\n**Gradient Harmonized Single-stage Detector**\n\n- intro: AAAI 2019 Oral\n- arxiv: [https://arxiv.org/abs/1811.05181](https://arxiv.org/abs/1811.05181)\n- gihtub(official): [https://github.com/libuyu/GHM_Detection](https://github.com/libuyu/GHM_Detection)\n\n**M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.04533](https://arxiv.org/abs/1811.04533)\n- github: [https://github.com/qijiezhao/M2Det](https://github.com/qijiezhao/M2Det)\n\n**Multi-layer Pruning Framework for Compressing Single Shot MultiBox Detector**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1811.08342](https://arxiv.org/abs/1811.08342)\n\n**Consistent Optimization for Single-Shot Object Detection**\n\n- arxiv: [https://arxiv.org/abs/1901.06563](https://arxiv.org/abs/1901.06563)\n- blog: [https://zhuanlan.zhihu.com/p/55416312](https://zhuanlan.zhihu.com/p/55416312)\n\n**A Single-shot Object Detector with Feature Aggragation and Enhancement**\n\n[https://arxiv.org/abs/1902.02923](https://arxiv.org/abs/1902.02923)\n\n**Towards Accurate One-Stage Object Detection with AP-Loss**\n\n- intro: CVPR 2019\n- intro: Shanghai Jiao Tong University & Intel Labs & Malaysia Multimedia University & Tencent YouTu Lab & Peking University\n- keywords: Average-Precision loss (AP-loss)\n- arxiv: {https://arxiv.org/abs/1904.06373}(https://arxiv.org/abs/1904.06373)\n\n**AP-Loss for Accurate One-Stage Object Detection**\n\n- intro: IEEE TPAMI\n- arxiv: [https://arxiv.org/abs/2008.07294](https://arxiv.org/abs/2008.07294)\n- github: [https://github.com/cccorn/AP-loss](https://github.com/cccorn/AP-loss)\n\n**Searching Parameterized AP Loss for Object Detection**\n\n- intro: NeurIPS 2021\n- intro: 1Tsinghua University & Zhejiang University & SenseTime Research & Shanghai Jiao Tong University & Beijing Academy of Artificial Intelligence\n- arxiv: [https://arxiv.org/abs/2112.05138](https://arxiv.org/abs/2112.05138)\n- github: [https://github.com/fundamentalvision/Parameterized-AP-Loss](https://github.com/fundamentalvision/Parameterized-AP-Loss)\n\n**Efficient Featurized Image Pyramid Network for Single Shot Detector**\n\n- intro: CVPR 2019\n- paper: [http://openaccess.thecvf.com/content_CVPR_2019/papers/Pang_Efficient_Featurized_Image_Pyramid_Network_for_Single_Shot_Detector_CVPR_2019_paper.pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Pang_Efficient_Featurized_Image_Pyramid_Network_for_Single_Shot_Detector_CVPR_2019_paper.pdf)\n- github: [https://github.com/vaesl/LFIP](https://github.com/vaesl/LFIP)\n\n**DR Loss: Improving Object Detection by Distributional Ranking**\n\n- intro: Alibaba Group\n- arxiv: [https://arxiv.org/abs/1907.10156](https://arxiv.org/abs/1907.10156)\n\n**HAR-Net: Joint Learning of Hybrid Attention for Single-stage Object Detection**\n\n[https://arxiv.org/abs/1904.11141](https://arxiv.org/abs/1904.11141)\n\n**Propose-and-Attend Single Shot Detector**\n\n[https://arxiv.org/abs/1907.12736](https://arxiv.org/abs/1907.12736)\n\n**Revisiting Feature Alignment for One-stage Object Detection**\n\n- intro: University of Chinese Academy of Sciences & TuSimple\n- keywords: AlignDet, RoIConv\n- arxiv: [https://arxiv.org/abs/1908.01570](https://arxiv.org/abs/1908.01570)\n\n**IoU-balanced Loss Functions for Single-stage Object Detection**\n\n- intro: HUST\n- arxiv: [https://arxiv.org/abs/1908.05641](https://arxiv.org/abs/1908.05641)\n\n**PosNeg-Balanced Anchors with Aligned Features for Single-Shot Object Detection**\n\n- intro: Chinese Academy of Sciences & University of Chinese Academy of Sciences\n- keywords: Anchor Promotion Module (APM), Feature Alignment Module (FAM)\n- arxiv: [https://arxiv.org/abs/1908.03295](https://arxiv.org/abs/1908.03295)\n\n**R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object**\n\n[https://arxiv.org/abs/1908.05612](https://arxiv.org/abs/1908.05612)\n\n**Hierarchical Shot Detector**\n\n- intro: ICCV 2019\n- keywords: reg-offset-cls (ROC) module\n- paper: [http://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.pdf)\n- github(official, Pytorch): [https://github.com/JialeCao001/HSD](https://github.com/JialeCao001/HSD)\n\n**Learning from Noisy Anchors for One-stage Object Detection**\n\n[https://arxiv.org/abs/1912.05086](https://arxiv.org/abs/1912.05086)\n\n**Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection**\n\n- intro: Nanjing University of Science and Technology & Momenta & Nanjing University & Microsoft Research & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2006.04388](https://arxiv.org/abs/2006.04388)\n- github: [https://github.com/implus/GFocal](https://github.com/implus/GFocal)\n\n**Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection**\n\n- intro: Nanjing University of Science and Technology & Momenta & Nanjing University & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2011.12885](https://arxiv.org/abs/2011.12885)\n- github: [https://github.com/implus/GFocalV2](https://github.com/implus/GFocalV2)\n\n**Single-Shot Two-Pronged Detector with Rectified IoU Loss**\n\n- intro: ACM MM 2020\n- arxiv: [https://arxiv.org/abs/2008.03511](https://arxiv.org/abs/2008.03511)\n\n**OneNet: Towards End-to-End One-Stage Object Detection**\n\n- intro: The University of Hong Kong & ByteDance AI Lab\n- arxiv: [https://arxiv.org/abs/2012.05780](https://arxiv.org/abs/2012.05780)\n- github: [https://github.com/PeizeSun/OneNet](https://github.com/PeizeSun/OneNet)\n\n**TOOD: Task-aligned One-stage Object Detection**\n\n- intro: ICCV 2021 Oral\n- intro: Intellifusion Inc. & Meituan Inc. & ByteDance Inc. & Malong LLC & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2108.07755](https://arxiv.org/abs/2108.07755)\n- github: [https://github.com/fcjian/TOOD](https://github.com/fcjian/TOOD)\n\n**Rethinking the Aligned and Misaligned Features in One-stage Object Detection**\n\n[https://arxiv.org/abs/2108.12176](https://arxiv.org/abs/2108.12176)\n\n# Anchor-free\n\n**Feature Selective Anchor-Free Module for Single-Shot Object Detection**\n\n- intro: CVPR 2019\n- keywords: feature selective anchor-free (FSAF) module\n- arxiv: [https://arxiv.org/abs/1903.00621](https://arxiv.org/abs/1903.00621)\n\n**FCOS: Fully Convolutional One-Stage Object Detection**\n\n- intro: The University of Adelaide\n- keywords: anchor-free\n- arxiv: [https://arxiv.org/abs/1904.01355](https://arxiv.org/abs/1904.01355)\n- github: [https://github.com/tianzhi0549/FCOS/](https://github.com/tianzhi0549/FCOS/)\n\n**FoveaBox: Beyond Anchor-based Object Detector**\n\n- intro: Tsinghua University & BNRist & ByteDance AI Lab & University of Pennsylvania\n- arxiv: [https://arxiv.org/abs/1904.03797](https://arxiv.org/abs/1904.03797)\n- github(official, mmdetection): [https://github.com/taokong/FoveaBox](https://github.com/taokong/FoveaBox)\n\n**IMMVP: An Efficient Daytime and Nighttime On-Road Object Detector**\n\n[https://arxiv.org/abs/1910.06573](https://arxiv.org/abs/1910.06573)\n\n**EfficientDet: Scalable and Efficient Object Detection**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/1911.09070](https://arxiv.org/abs/1911.09070)\n- github: [https://github.com/google/automl/tree/master/efficientdet](https://github.com/google/automl/tree/master/efficientdet)\n- github: [https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch)\n\n**Domain Adaptation for Object Detection via Style Consistency**\n\n- intro: BMVC 2019\n- arxiv: [https://arxiv.org/abs/1911.10033](https://arxiv.org/abs/1911.10033)\n\n**Soft Anchor-Point Object Detection**\n\n- intro: ECCV 2020\n- intro: Carnegie Mellon University\n- keywords: Soft Anchor-Point Detector (SAPD)\n- arxiv: [https://arxiv.org/abs/1911.12448](https://arxiv.org/abs/1911.12448)\n\n**IPG-Net: Image Pyramid Guidance Network for Object Detection**\n\n[https://arxiv.org/abs/1912.00632](https://arxiv.org/abs/1912.00632)\n\n**Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection**\n\n- arxiv: [https://arxiv.org/abs/1912.02424](https://arxiv.org/abs/1912.02424)\n- github: [https://github.com/sfzhang15/ATSS](https://github.com/sfzhang15/ATSS)\n\n**Localization Uncertainty Estimation for Anchor-Free Object Detection**\n\n- keywords: Gaussian-FCOS\n- arxiv: [https://arxiv.org/abs/2006.15607](https://arxiv.org/abs/2006.15607)\n\n**Corner Proposal Network for Anchor-free, Two-stage Object Detection**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.13816](https://arxiv.org/abs/2007.13816)\n- github: [https://github.com/Duankaiwen/CPNDet](https://github.com/Duankaiwen/CPNDet)\n\n**Dive Deeper Into Box for Object Detection**\n\n- intro: ECCV 2020\n- keywords: DDBNet, anchor free\n- arxiv: [https://arxiv.org/abs/2007.14350](https://arxiv.org/abs/2007.14350)\n\n**Corner Proposal Network for Anchor-free, Two-stage Object Detection**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.13816](https://arxiv.org/abs/2007.13816)\n- github: [https://github.com/Duankaiwen/CPNDet](https://github.com/Duankaiwen/CPNDet)\n\n**Reducing Label Noise in Anchor-Free Object Detection**\n\n- intro: BMVC 2020\n- arxiv: [https://arxiv.org/abs/2008.01167](https://arxiv.org/abs/2008.01167)\n- github: [https://github.com/nerminsamet/ppdet](https://github.com/nerminsamet/ppdet)\n\n**Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection**\n\n[https://arxiv.org/abs/2012.13763](https://arxiv.org/abs/2012.13763)\n\n**PAFNet: An Efficient Anchor-Free Object Detector Guidance**\n\n- intro: Baidu Inc.\n- github: [https://arxiv.org/abs/2104.13534](https://arxiv.org/abs/2104.13534)\n- arxiv: [https://github.com/PaddlePaddle/PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)\n\n**Pseudo-IoU: Improving Label Assignment in Anchor-Free Object Detection**\n\n- intro: CVPR 2021 Workshop\n- intro: UIUC & MIT-IBM Watson AI Lab & IBM T.J. Watson Research Center & NVIDIA & University of Oregon & Picsart AI Research (PAIR)\n- arxiv: [https://arxiv.org/abs/2104.14082](https://arxiv.org/abs/2104.14082)\n\n**ObjectBox: From Centers to Boxes for Anchor-Free Object Detection**\n\n- intro: ECCV 2022 Oral\n- intro: Ingenuity Labs Research Institute & Queen’s University\n- arxiv: [https://arxiv.org/abs/2207.06985](https://arxiv.org/abs/2207.06985)\n- github: [https://github.com/MohsenZand/ObjectBox](https://github.com/MohsenZand/ObjectBox)\n\n# Transformers\n\n**End-to-End Object Detection with Transformers**\n\n- intro: Facebook AI\n- keywords: DEtection TRansformer (DETR)\n- arxiv: [https://arxiv.org/abs/2005.12872](https://arxiv.org/abs/2005.12872)\n- github: [https://github.com/facebookresearch/detr](https://github.com/facebookresearch/detr)\n\n**Deformable DETR: Deformable Transformers for End-to-End Object Detection**\n\n- intro: SenseTime Research & USTC & CUHK\n- arxiv: [https://arxiv.org/abs/2010.04159](https://arxiv.org/abs/2010.04159)\n- github: [https://github.com/fundamentalvision/Deformable-DETR](https://github.com/fundamentalvision/Deformable-DETR)\n\n**RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder**\n\n- intro: NeurIPS2020 Spotlight\n- intro: CAS & MSRA\n- arxiv: [https://arxiv.org/abs/2010.15831](https://arxiv.org/abs/2010.15831)\n- github:[https://github.com/microsoft/RelationNet2](https://github.com/microsoft/RelationNet2)\n\n**UP-DETR: Unsupervised Pre-training for Object Detection with Transformers**\n\n- intro: South China University of Technology & Tencent Wechat AI\n- arxiv: [https://arxiv.org/abs/2011.09094](https://arxiv.org/abs/2011.09094)\n\n**Conditional DETR for Fast Training Convergence**\n\n- intro: ICCV 2021\n- intro: University of Science and Technology of China & Peking University & Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/2108.06152](https://arxiv.org/abs/2108.06152)\n- github: [https://github.com/Atten4Vis/ConditionalDETR](https://github.com/Atten4Vis/ConditionalDETR)\n\n**End-to-End Object Detection with Adaptive Clustering Transformer**\n\n- intro: Peking University & The Chinese University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2011.09315](https://arxiv.org/abs/2011.09315)\n\n**Toward Transformer-Based Object Detection**\n\n- intro: Pinterest\n- keywords: ViT-FRCNN\n- arxiv: [https://arxiv.org/abs/2012.09958](https://arxiv.org/abs/2012.09958)\n\n**Efficient DETR: Improving End-to-End Object Detector with Dense Prior**\n\n- intro: Megvii Technology\n- arxiv: [https://arxiv.org/abs/2104.01318](https://arxiv.org/abs/2104.01318)\n\n**Anchor DETR: Query Design for Transformer-Based Detector**\n\n- intro: MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2109.07107](https://arxiv.org/abs/2109.07107)\n- gihtub: [https://github.com/megvii-model/AnchorDETR](https://github.com/megvii-model/AnchorDETR)\n\n**DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR**\n\n- arxiv: [https://arxiv.org/abs/2201.12329](https://arxiv.org/abs/2201.12329)\n- github: [https://github.com/SlongLiu/DAB-DETR](https://github.com/SlongLiu/DAB-DETR)\n\n**DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection**\n\n- arxiv: [https://arxiv.org/abs/2203.03605](https://arxiv.org/abs/2203.03605)\n- github: [https://github.com/IDEACVR/DINO](https://github.com/IDEACVR/DINO)\n\n**Oriented Object Detection with Transformer**\n\n- intro: University at Buffalo & Beihang University & Baidu Inc\n- arxiv: [https://arxiv.org/abs/2106.03146](https://arxiv.org/abs/2106.03146)\n\n**ViDT: An Efficient and Effective Fully Transformer-based Object Detector**\n\n- intro: NAVER AI Lab & Google Research & University of California at Merced\n- arxiv: [https://arxiv.org/abs/2110.03921](https://arxiv.org/abs/2110.03921)\n- github: [https://github.com/naver-ai/vidt](https://github.com/naver-ai/vidt)\n\n**An Extendable, Efficient and Effective Transformer-based Object Detector**\n\n- arxiv: [https://arxiv.org/abs/2204.07962](https://arxiv.org/abs/2204.07962)\n- github: [https://github.com/naver-ai/vidt](https://github.com/naver-ai/vidt)\n\n**Omni-DETR: Omni-Supervised Object Detection with Transformers**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.16089](https://arxiv.org/abs/2203.16089)\n\n**Accelerating DETR Convergence via Semantic-Aligned Matching**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.06883](https://arxiv.org/abs/2203.06883)\n- github: [https://github.com/ZhangGongjie/SAM-DETR](https://github.com/ZhangGongjie/SAM-DETR)\n\n**AdaMixer: A Fast-Converging Query-Based Object Detector**\n\n- intro: CVPR 2022 oral\n- intro: Nanjing University, MYbank Ant Group\n- arxiv: [https://arxiv.org/abs/2203.16507](https://arxiv.org/abs/2203.16507)\n- github: [https://github.com/MCG-NJU/AdaMixer](https://github.com/MCG-NJU/AdaMixer)\n\n**Exploring Plain Vision Transformer Backbones for Object Detection**\n\n- intro: Facebook AI Research\n- arxiv: [https://arxiv.org/abs/2203.16527](https://arxiv.org/abs/2203.16527)\n\n**Efficient Decoder-free Object Detection with Transformers**\n\n- intro:  Tencent Youtu Lab & Zhejiang University\n- arxiv: [https://arxiv.org/abs/2206.06829](https://arxiv.org/abs/2206.06829)\n- github: [https://github.com/Pealing/DFFT](https://github.com/Pealing/DFFT)\n\n# Non-Maximum Suppression (NMS)\n\n**End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression**\n\n- intro: CVPR 2015\n- arxiv: [http://arxiv.org/abs/1411.5309](http://arxiv.org/abs/1411.5309)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf)\n\n**A convnet for non-maximum suppression**\n\n- arxiv: [http://arxiv.org/abs/1511.06437](http://arxiv.org/abs/1511.06437)\n\n**Improving Object Detection With One Line of Code**\n\n**Soft-NMS -- Improving Object Detection With One Line of Code**\n\n- intro: ICCV 2017. University of Maryland\n- keywords: Soft-NMS\n- arxiv: [https://arxiv.org/abs/1704.04503](https://arxiv.org/abs/1704.04503)\n- github: [https://github.com/bharatsingh430/soft-nms](https://github.com/bharatsingh430/soft-nms)\n\n**Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection**\n\n- intro: CMU & Megvii Inc. (Face++)\n- arxiv: [https://arxiv.org/abs/1809.08545](https://arxiv.org/abs/1809.08545)\n- github: [https://github.com/yihui-he/softer-NMS](https://github.com/yihui-he/softer-NMS)\n\n**Learning non-maximum suppression**\n\n- intro: CVPR 2017\n- project page: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/learning-nms/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/learning-nms/)\n- arxiv: [https://arxiv.org/abs/1705.02950](https://arxiv.org/abs/1705.02950)\n- github: [https://github.com/hosang/gossipnet](https://github.com/hosang/gossipnet)\n\n**Relation Networks for Object Detection**\n\n- intro: CVPR 2018 oral\n- arxiv: [https://arxiv.org/abs/1711.11575](https://arxiv.org/abs/1711.11575)\n- github(official, MXNet): [https://github.com/msracver/Relation-Networks-for-Object-Detection](https://github.com/msracver/Relation-Networks-for-Object-Detection)\n\n**Learning Pairwise Relationship for Multi-object Detection in Crowded Scenes**\n\n- keywords: Pairwise-NMS\n- arxiv: [https://arxiv.org/abs/1901.03796](https://arxiv.org/abs/1901.03796)\n\n**Daedalus: Breaking Non-Maximum Suppression in Object Detection via Adversarial Examples**\n\n[https://arxiv.org/abs/1902.02067](https://arxiv.org/abs/1902.02067)\n\n**NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing**\n\n- intro: CVPR 2020\n- intro: Waseda University & Tencent AI Lab\n- arxiv: [https://arxiv.org/abs/2003.12729](https://arxiv.org/abs/2003.12729)\n\n**Hashing-based Non-Maximum Suppression for Crowded Object Detection**\n\n- intro: Microsoft\n- arxiv: [https://arxiv.org/abs/2005.11426](https://arxiv.org/abs/2005.11426)\n- github: [https://github.com/microsoft/hnms](https://github.com/microsoft/hnms)\n\n**Visibility Guided NMS: Efficient Boosting of Amodal Object Detection in Crowded Traffic Scenes**\n\n- intro: NeurIPS 2019, Machine Learning for Autonomous Driving Workshop\n- intro: Mercedes-Benz AG, R&D & University of Jena\n- keywords: Visibility Guided NMS (vg-NMS)\n- arxiv: [https://arxiv.org/abs/2006.08547](https://arxiv.org/abs/2006.08547)\n\n**Determinantal Point Process as an alternative to NMS**\n\n[https://arxiv.org/abs/2008.11451](https://arxiv.org/abs/2008.11451)\n\n**Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding**\n\n- intro: Zhejiang University & Nanyang Technological University & Tencent AI Lab & Columbia University\n- arxiv: [https://arxiv.org/abs/2009.01449](https://arxiv.org/abs/2009.01449)\n\n# NMS-free\n\n**Object Detection Made Simpler by Eliminating Heuristic NMS**\n\n- intro: Alibaba Group & Monash University & The University of Adelaide\n- arxiv: [https://arxiv.org/abs/2101.11782](https://arxiv.org/abs/2101.11782)\n- github: [https://github.com/txdet/FCOSPss](https://github.com/txdet/FCOSPss)\n\n# Adversarial Examples\n\n**Adversarial Examples that Fool Detectors**\n\n- intro: University of Illinois\n- arxiv: [https://arxiv.org/abs/1712.02494](https://arxiv.org/abs/1712.02494)\n\n**Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods**\n\n- project page: [http://nicholas.carlini.com/code/nn_breaking_detection/](http://nicholas.carlini.com/code/nn_breaking_detection/)\n- arxiv: [https://arxiv.org/abs/1705.07263](https://arxiv.org/abs/1705.07263)\n- github: [https://github.com/carlini/nn_breaking_detection](https://github.com/carlini/nn_breaking_detection)\n\n# Knowledge Distillation\n\n**Mimicking Very Efficient Network for Object Detection**\n\n- intro: CVPR 2017. SenseTime & Beihang University\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf)\n\n**Quantization Mimic: Towards Very Tiny CNN for Object Detection**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1805.02152](https://arxiv.org/abs/1805.02152)\n\n**Learning Efficient Detector with Semi-supervised Adaptive Distillation**\n\n- intro: SenseTime Research\n- arxiv: [https://arxiv.org/abs/1901.00366](https://arxiv.org/abs/1901.00366)\n- github: [https://github.com/Tangshitao/Semi-supervised-Adaptive-Distillation](https://github.com/Tangshitao/Semi-supervised-Adaptive-Distillation)\n\n**Distilling Object Detectors with Fine-grained Feature Imitation**\n\n- intro: CVPR 2019\n- intro: National University of Singapore & Huawei Noah’s Ark Lab\n- keywords: mimic\n- arxiv: [https://arxiv.org/abs/1906.03609](https://arxiv.org/abs/1906.03609)\n- github: [https://github.com/twangnh/Distilling-Object-Detectors](https://github.com/twangnh/Distilling-Object-Detectors)\n\n**GAN-Knowledge Distillation for one-stage Object Detection**\n\n[https://arxiv.org/abs/1906.08467](https://arxiv.org/abs/1906.08467)\n\n**Learning Lightweight Pedestrian Detector with Hierarchical Knowledge Distillation**\n\n- intro: ICIP 2019 oral\n- arxiv: [https://arxiv.org/abs/1909.09325](https://arxiv.org/abs/1909.09325)\n\n**Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors**\n\n- intro: ICLR 2021 poster\n- openreview: [https://openreview.net/forum?id=uKhGRvM8QNH](https://openreview.net/forum?id=uKhGRvM8QNH)\n- paper: [https://openreview.net/pdf?id=uKhGRvM8QNH](https://openreview.net/pdf?id=uKhGRvM8QNH)\n- github: [https://github.com/ArchipLab-LinfengZhang/Object-Detection-Knowledge-Distillation-ICLR2021](https://github.com/ArchipLab-LinfengZhang/Object-Detection-Knowledge-Distillation-ICLR2021)\n\n**G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation**\n\n- intro: Hong Kong University of Science and Technology & Huawei Noah’s Ark Lab\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.07482](https://arxiv.org/abs/2108.07482)\n\n**LGD: Label-guided Self-distillation for Object Detection**\n\n- intro: MEGVII Technology & Xi’an Jiaotong University\n- arxiv: [https://arxiv.org/abs/2109.11496](https://arxiv.org/abs/2109.11496)\n\n**Deep Structured Instance Graph for Distilling Object Detectors**\n\n- intro: ICCV 2021\n- intro: The Chinese University of Hong Kong & SmartMore\n- arxiv: [https://arxiv.org/abs/2109.12862](https://arxiv.org/abs/2109.12862)\n- github: [https://github.com/dvlab-research/Dsig](https://github.com/dvlab-research/Dsig)\n\n**Instance-Conditional Knowledge Distillation for Object Detection**\n\n- intro: NeurIPS 2021 poster\n- intro: Xi’an Jiaotong University & MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2110.12724](https://arxiv.org/abs/2110.12724)\n\n**Distilling Object Detectors with Feature Richness**\n\n- intro: University of Science and Technology of China & CAS & Cambricon Technologies & University of Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/2111.00674](https://arxiv.org/abs/2111.00674)\n\n**Focal and Global Knowledge Distillation for Detectors**\n\n- intro: Tsinghua Shenzhen International Graduate School & ByteDance Inc & BeiHang University\n- arxiv: [https://arxiv.org/abs/2111.11837](https://arxiv.org/abs/2111.11837)\n- github: [https://github.com/yzd-v/FGD](https://github.com/yzd-v/FGD)\n\n**Prediction-Guided Distillation for Dense Object Detection**\n\n- intro: University of Edinburgh & Heriot-Watt University\n- arxiv: [https://arxiv.org/abs/2203.05469](https://arxiv.org/abs/2203.05469)\n- github: [https://github.com/ChenhongyiYang/PGD](https://github.com/ChenhongyiYang/PGD)\n\n# Rotated Object Detection\n\n**Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss**\n\n- intro: Shanghai Jiao Tong University & Huawei Inc. & Beijing Institute of Technology\n- arxiv: [https://arxiv.org/abs/2101.11952](https://arxiv.org/abs/2101.11952)\n- github: [https://github.com/yangxue0827/RotationDetection](https://github.com/yangxue0827/RotationDetection)\n\n# Long-Tailed Object Detection\n\n**Factors in Finetuning Deep Model for object detection**\n\n**Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution**\n\n- intro: CVPR 2016.rank 3rd for provided data and 2nd for external data on ILSVRC 2015 object detection\n- project page: [http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html](http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html)\n- arxiv: [http://arxiv.org/abs/1601.05150](http://arxiv.org/abs/1601.05150)\n\n**Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax**\n\n- intro: CVPR 2020 oral\n- arxiv: [https://arxiv.org/abs/2006.10408](https://arxiv.org/abs/2006.10408)\n- github: [https://github.com/FishYuLi/BalancedGroupSoftmax](https://github.com/FishYuLi/BalancedGroupSoftmax)\n\n**Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection**\n\n- intro: Tongji University & SenseTime Research & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2012.08548](https://arxiv.org/abs/2012.08548)\n\n**A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection**\n\n- intro: The Ohio State University & University of Central Florida & University of Southern California & Google Research\n- arxiv: [https://arxiv.org/abs/2102.08884](https://arxiv.org/abs/2102.08884)\n\n**Adaptive Class Suppression Loss for Long-Tail Object Detection**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.00885](https://arxiv.org/abs/2104.00885)\n- github: [https://github.com/CASIA-IVA-Lab/ACSL](https://github.com/CASIA-IVA-Lab/ACSL)\n\n# Weakly Supervised Object Detection\n\n**Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1604.05766](http://arxiv.org/abs/1604.05766)\n\n**Weakly supervised object detection using pseudo-strong labels**\n\n- arxiv: [http://arxiv.org/abs/1607.04731](http://arxiv.org/abs/1607.04731)\n\n**Saliency Guided End-to-End Learning for Weakly Supervised Object Detection**\n\n- intro: IJCAI 2017\n- arxiv: [https://arxiv.org/abs/1706.06768](https://arxiv.org/abs/1706.06768)\n\n**Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised Object Detection**\n\n- intro: TPAMI 2017. National Institutes of Health (NIH) Clinical Center\n- arxiv: [https://arxiv.org/abs/1801.03145](https://arxiv.org/abs/1801.03145)\n\n# Video Object Detection\n\n**Learning Object Class Detectors from Weakly Annotated Video**\n\n- intro: CVPR 2012\n- paper: [https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf](https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf)\n\n**Analysing domain shift factors between videos and images for object detection**\n\n- arxiv: [https://arxiv.org/abs/1501.01186](https://arxiv.org/abs/1501.01186)\n\n**Video Object Recognition**\n\n- slides: [http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx](http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx)\n\n**Deep Learning for Saliency Prediction in Natural Video**\n\n- intro: Submitted on 12 Jan 2016\n- keywords: Deep learning, saliency map, optical flow, convolution network, contrast features\n- paper: [https://hal.archives-ouvertes.fr/hal-01251614/document](https://hal.archives-ouvertes.fr/hal-01251614/document)\n\n**T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos**\n\n- intro: Winning solution in ILSVRC2015 Object Detection from Video(VID) Task\n- arxiv: [http://arxiv.org/abs/1604.02532](http://arxiv.org/abs/1604.02532)\n- github: [https://github.com/myfavouritekk/T-CNN](https://github.com/myfavouritekk/T-CNN)\n\n**Object Detection from Video Tubelets with Convolutional Neural Networks**\n\n- intro: CVPR 2016 Spotlight paper\n- arxiv: [https://arxiv.org/abs/1604.04053](https://arxiv.org/abs/1604.04053)\n- paper: [http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf](http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf)\n- gihtub: [https://github.com/myfavouritekk/vdetlib](https://github.com/myfavouritekk/vdetlib)\n\n**Object Detection in Videos with Tubelets and Multi-context Cues**\n\n- intro: SenseTime Group\n- slides: [http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf](http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf)\n- slides: [http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf](http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf)\n\n**Context Matters: Refining Object Detection in Video with Recurrent Neural Networks**\n\n- intro: BMVC 2016\n- keywords: pseudo-labeler\n- arxiv: [http://arxiv.org/abs/1607.04648](http://arxiv.org/abs/1607.04648)\n- paper: [http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf](http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf)\n\n**CNN Based Object Detection in Large Video Images**\n\n- intro: WangTao @ 爱奇艺\n- keywords: object retrieval, object detection, scene classification\n- slides: [http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf](http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf)\n\n**Object Detection in Videos with Tubelet Proposal Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.06355](https://arxiv.org/abs/1702.06355)\n\n**Flow-Guided Feature Aggregation for Video Object Detection**\n\n- intro: MSRA\n- arxiv: [https://arxiv.org/abs/1703.10025](https://arxiv.org/abs/1703.10025)\n\n**Video Object Detection using Faster R-CNN**\n\n- blog: [http://andrewliao11.github.io/object_detection/faster_rcnn/](http://andrewliao11.github.io/object_detection/faster_rcnn/)\n- github: [https://github.com/andrewliao11/py-faster-rcnn-imagenet](https://github.com/andrewliao11/py-faster-rcnn-imagenet)\n\n**Improving Context Modeling for Video Object Detection and Tracking**\n\n[http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf](http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf)\n\n**Temporal Dynamic Graph LSTM for Action-driven Video Object Detection**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.00666](https://arxiv.org/abs/1708.00666)\n\n**Mobile Video Object Detection with Temporally-Aware Feature Maps**\n\n[https://arxiv.org/abs/1711.06368](https://arxiv.org/abs/1711.06368)\n\n**Towards High Performance Video Object Detection**\n\n[https://arxiv.org/abs/1711.11577](https://arxiv.org/abs/1711.11577)\n\n**Impression Network for Video Object Detection**\n\n[https://arxiv.org/abs/1712.05896](https://arxiv.org/abs/1712.05896)\n\n**Spatial-Temporal Memory Networks for Video Object Detection**\n\n[https://arxiv.org/abs/1712.06317](https://arxiv.org/abs/1712.06317)\n\n**3D-DETNet: a Single Stage Video-Based Vehicle Detector**\n\n[https://arxiv.org/abs/1801.01769](https://arxiv.org/abs/1801.01769)\n\n**Object Detection in Videos by Short and Long Range Object Linking**\n\n[https://arxiv.org/abs/1801.09823](https://arxiv.org/abs/1801.09823)\n\n**Object Detection in Video with Spatiotemporal Sampling Networks**\n\n- intro: University of Pennsylvania, 2Dartmouth College\n- arxiv: [https://arxiv.org/abs/1803.05549](https://arxiv.org/abs/1803.05549)\n\n**Towards High Performance Video Object Detection for Mobiles**\n\n- intro: Microsoft Research Asia\n- arxiv: [https://arxiv.org/abs/1804.05830](https://arxiv.org/abs/1804.05830)\n\n**Optimizing Video Object Detection via a Scale-Time Lattice**\n\n- intro: CVPR 2018\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/](http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/)\n- arxiv: [https://arxiv.org/abs/1804.05472](https://arxiv.org/abs/1804.05472)\n- github: [https://github.com/hellock/scale-time-lattice](https://github.com/hellock/scale-time-lattice)\n\n**Pack and Detect: Fast Object Detection in Videos Using Region-of-Interest Packing**\n\n[https://arxiv.org/abs/1809.01701](https://arxiv.org/abs/1809.01701)\n\n**Fast Object Detection in Compressed Video**\n\n[https://arxiv.org/abs/1811.11057](https://arxiv.org/abs/1811.11057)\n\n**Tube-CNN: Modeling temporal evolution of appearance for object detection in video**\n\n- intro: INRIA/ENS\n- arxiv: [https://arxiv.org/abs/1812.02619](https://arxiv.org/abs/1812.02619)\n\n**AdaScale: Towards Real-time Video Object Detection Using Adaptive Scaling**\n\n- intro: SysML 2019 oral\n- arxiv: [https://arxiv.org/abs/1902.02910](https://arxiv.org/abs/1902.02910)\n\n**SCNN: A General Distribution based Statistical Convolutional Neural Network with Application to Video Object Detection**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1903.07663](https://arxiv.org/abs/1903.07663)\n\n**Looking Fast and Slow: Memory-Guided Mobile Video Object Detection**\n\n- intro: Cornell University & Google AI\n- arxiv: [https://arxiv.org/abs/1903.10172](https://arxiv.org/abs/1903.10172)\n\n**Progressive Sparse Local Attention for Video object detection**\n\n- intro: NLPR,CASIA & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1903.09126](https://arxiv.org/abs/1903.09126)\n\n**Sequence Level Semantics Aggregation for Video Object Detection**\n\n- intro: ICCV 2019 oral\n- arxiv: [https://arxiv.org/abs/1907.06390](https://arxiv.org/abs/1907.06390)\n- github(MXNet): [https://github.com/happywu/Sequence-Level-Semantics-Aggregation](https://github.com/happywu/Sequence-Level-Semantics-Aggregation)\n\n**Object Detection in Video with Spatial-temporal Context Aggregation**\n\n- intro: Huazhong University of Science and Technology & Horizon Robotics Inc.\n- arxiv: [https://arxiv.org/abs/1907.04988](https://arxiv.org/abs/1907.04988)\n\n**A Delay Metric for Video Object Detection: What Average Precision Fails to Tell**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.06368](https://arxiv.org/abs/1908.06368)\n\n**Minimum Delay Object Detection From Video**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.11092](https://arxiv.org/abs/1908.11092)\n\n**Learning Motion Priors for Efficient Video Object Detection**\n\n[https://arxiv.org/abs/1911.05253](https://arxiv.org/abs/1911.05253)\n\n**Object-aware Feature Aggregation for Video Object Detection**\n\n- intro: Beihang University & Capital Normal University & The University of Hong Kong & Baidu, Inc.\n- arxiv: [https://arxiv.org/abs/2010.12573](https://arxiv.org/abs/2010.12573)\n\n**End-to-End Video Object Detection with Spatial-Temporal Transformers**\n\n- arxiv: [https://arxiv.org/abs/2105.10920](https://arxiv.org/abs/2105.10920)\n- github: [https://github.com/SJTU-LuHe/TransVOD](https://github.com/SJTU-LuHe/TransVOD)\n\n# Object Detection on Mobile Devices\n\n**Pelee: A Real-Time Object Detection System on Mobile Devices**\n\n- intro: ICLR 2018 workshop track\n- intro: based on the SSD\n- arxiv: [https://arxiv.org/abs/1804.06882](https://arxiv.org/abs/1804.06882)\n- github: [https://github.com/Robert-JunWang/Pelee](https://github.com/Robert-JunWang/Pelee)\n\n# Object Detection on RGB-D\n\n**Learning Rich Features from RGB-D Images for Object Detection and Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1407.5736](http://arxiv.org/abs/1407.5736)\n\n**Differential Geometry Boosts Convolutional Neural Networks for Object Detection**\n\n- intro: CVPR 2016\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html](http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html)\n\n**A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation**\n\n[https://arxiv.org/abs/1703.03347](https://arxiv.org/abs/1703.03347)\n\n**Cross-Modal Attentional Context Learning for RGB-D Object Detection**\n\n- intro: IEEE Transactions on Image Processing\n- arxiv: [https://arxiv.org/abs/1810.12829](https://arxiv.org/abs/1810.12829)\n\n# Zero-Shot Object Detection\n\n**Zero-Shot Detection**\n\n- intro: Australian National University\n- keywords: YOLO\n- arxiv: [https://arxiv.org/abs/1803.07113](https://arxiv.org/abs/1803.07113)\n\n**Zero-Shot Object Detection**\n\n[https://arxiv.org/abs/1804.04340](https://arxiv.org/abs/1804.04340)\n\n**Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts**\n\n- intro: Australian National University\n- arxiv: [https://arxiv.org/abs/1803.06049](https://arxiv.org/abs/1803.06049)\n\n**Zero-Shot Object Detection by Hybrid Region Embedding**\n\n- intro: Middle East Technical University & Hacettepe University\n- arxiv: [https://arxiv.org/abs/1805.06157](https://arxiv.org/abs/1805.06157)\n\n# Visual Relationship Detection\n\n**Visual Relationship Detection with Language Priors**\n\n- intro: ECCV 2016 oral\n- paper: [https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf](https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf)\n- github: [https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection](https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection)\n\n**ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection**\n\n- intro: Visual Phrase reasoning Convolutional Neural Network (ViP-CNN), Visual Phrase Reasoning Structure (VPRS)\n- arxiv: [https://arxiv.org/abs/1702.07191](https://arxiv.org/abs/1702.07191)\n\n**Visual Translation Embedding Network for Visual Relation Detection**\n\n- arxiv: [https://www.arxiv.org/abs/1702.08319](https://www.arxiv.org/abs/1702.08319)\n\n**Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection**\n\n- intro: CVPR 2017 spotlight paper\n- arxiv: [https://arxiv.org/abs/1703.03054](https://arxiv.org/abs/1703.03054)\n\n**Detecting Visual Relationships with Deep Relational Networks**\n\n- intro: CVPR 2017 oral. The Chinese University of Hong Kong\n- arxiv: [https://arxiv.org/abs/1704.03114](https://arxiv.org/abs/1704.03114)\n\n**Identifying Spatial Relations in Images using Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1706.04215](https://arxiv.org/abs/1706.04215)\n\n**PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN**\n\n- intro: ICCV\n- arxiv: [https://arxiv.org/abs/1708.01956](https://arxiv.org/abs/1708.01956)\n\n**Natural Language Guided Visual Relationship Detection**\n\n[https://arxiv.org/abs/1711.06032](https://arxiv.org/abs/1711.06032)\n\n**Detecting Visual Relationships Using Box Attention**\n\n- intro: Google AI & IST Austria\n- arxiv: [https://arxiv.org/abs/1807.02136](https://arxiv.org/abs/1807.02136)\n\n**Google AI Open Images - Visual Relationship Track**\n\n- intro: Detect pairs of objects in particular relationships\n- kaggle: [https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track](https://www.kaggle.com/c/google-ai-open-images-visual-relationship-track)\n\n**Context-Dependent Diffusion Network for Visual Relationship Detection**\n\n- intro: 2018 ACM Multimedia Conference\n- arxiv: [https://arxiv.org/abs/1809.06213](https://arxiv.org/abs/1809.06213)\n\n**A Problem Reduction Approach for Visual Relationships Detection**\n\n- intro: ECCV 2018 Workshop\n- arxiv: [https://arxiv.org/abs/1809.09828](https://arxiv.org/abs/1809.09828)\n\n**Exploring the Semantics for Visual Relationship Detection**\n\n[https://arxiv.org/abs/1904.02104](https://arxiv.org/abs/1904.02104)\n\n# Face Detection\n\n**Multi-view Face Detection Using Deep Convolutional Neural Networks**\n\n- intro: Yahoo\n- arxiv: [http://arxiv.org/abs/1502.02766](http://arxiv.org/abs/1502.02766)\n- github: [https://github.com/guoyilin/FaceDetection_CNN](https://github.com/guoyilin/FaceDetection_CNN)\n\n**From Facial Parts Responses to Face Detection: A Deep Learning Approach**\n\n![](http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/support/index.png)\n\n- intro: ICCV 2015. CUHK\n- project page: [http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html](http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html)\n- arxiv: [https://arxiv.org/abs/1509.06451](https://arxiv.org/abs/1509.06451)\n- paper: [http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf)\n\n**Compact Convolutional Neural Network Cascade for Face Detection**\n\n- arxiv: [http://arxiv.org/abs/1508.01292](http://arxiv.org/abs/1508.01292)\n- github: [https://github.com/Bkmz21/FD-Evaluation](https://github.com/Bkmz21/FD-Evaluation)\n- github: [https://github.com/Bkmz21/CompactCNNCascade](https://github.com/Bkmz21/CompactCNNCascade)\n\n**Face Detection with End-to-End Integration of a ConvNet and a 3D Model**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1606.00850](https://arxiv.org/abs/1606.00850)\n- github(MXNet): [https://github.com/tfwu/FaceDetection-ConvNet-3D](https://github.com/tfwu/FaceDetection-ConvNet-3D)\n\n**CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1606.05413](https://arxiv.org/abs/1606.05413)\n\n**Towards a Deep Learning Framework for Unconstrained Face Detection**\n\n- intro: overlap with CMS-RCNN\n- arxiv: [https://arxiv.org/abs/1612.05322](https://arxiv.org/abs/1612.05322)\n\n**Supervised Transformer Network for Efficient Face Detection**\n\n- arxiv: [http://arxiv.org/abs/1607.05477](http://arxiv.org/abs/1607.05477)\n\n**UnitBox: An Advanced Object Detection Network**\n\n- intro: ACM MM 2016\n- intro: University of Illinois at Urbana−Champaign & Megvii Inc\n- keywords: IOULoss\n- arxiv: [http://arxiv.org/abs/1608.01471](http://arxiv.org/abs/1608.01471)\n\n**Bootstrapping Face Detection with Hard Negative Examples**\n\n- author: 万韶华 @ 小米.\n- intro: Faster R-CNN, hard negative mining. state-of-the-art on the FDDB dataset\n- arxiv: [http://arxiv.org/abs/1608.02236](http://arxiv.org/abs/1608.02236)\n\n**Grid Loss: Detecting Occluded Faces**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1609.00129](https://arxiv.org/abs/1609.00129)\n- paper: [http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf](http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf)\n- poster: [http://www.eccv2016.org/files/posters/P-2A-34.pdf](http://www.eccv2016.org/files/posters/P-2A-34.pdf)\n\n**A Multi-Scale Cascade Fully Convolutional Network Face Detector**\n\n- intro: ICPR 2016\n- arxiv: [http://arxiv.org/abs/1609.03536](http://arxiv.org/abs/1609.03536)\n\n## MTCNN\n\n**Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks**\n\n**Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks**\n\n![](https://kpzhang93.github.io/MTCNN_face_detection_alignment/support/index.png)\n\n- project page: [https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html)\n- arxiv: [https://arxiv.org/abs/1604.02878](https://arxiv.org/abs/1604.02878)\n- github(official, Matlab): [https://github.com/kpzhang93/MTCNN_face_detection_alignment](https://github.com/kpzhang93/MTCNN_face_detection_alignment)\n- github: [https://github.com/pangyupo/mxnet_mtcnn_face_detection](https://github.com/pangyupo/mxnet_mtcnn_face_detection)\n- github: [https://github.com/DaFuCoding/MTCNN_Caffe](https://github.com/DaFuCoding/MTCNN_Caffe)\n- github(MXNet): [https://github.com/Seanlinx/mtcnn](https://github.com/Seanlinx/mtcnn)\n- github: [https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion](https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion)\n- github(Caffe): [https://github.com/foreverYoungGitHub/MTCNN](https://github.com/foreverYoungGitHub/MTCNN)\n- github: [https://github.com/CongWeilin/mtcnn-caffe](https://github.com/CongWeilin/mtcnn-caffe)\n- github(OpenCV+OpenBlas): [https://github.com/AlphaQi/MTCNN-light](https://github.com/AlphaQi/MTCNN-light)\n- github(Tensorflow+golang): [https://github.com/jdeng/goface](https://github.com/jdeng/goface)\n\n**Face Detection using Deep Learning: An Improved Faster RCNN Approach**\n\n- intro: DeepIR Inc\n- arxiv: [https://arxiv.org/abs/1701.08289](https://arxiv.org/abs/1701.08289)\n\n**Faceness-Net: Face Detection through Deep Facial Part Responses**\n\n- intro: An extended version of ICCV 2015 paper\n- arxiv: [https://arxiv.org/abs/1701.08393](https://arxiv.org/abs/1701.08393)\n\n**Multi-Path Region-Based Convolutional Neural Network for Accurate Detection of Unconstrained \"Hard Faces\"**\n\n- intro: CVPR 2017. MP-RCNN, MP-RPN\n- arxiv: [https://arxiv.org/abs/1703.09145](https://arxiv.org/abs/1703.09145)\n\n**End-To-End Face Detection and Recognition**\n\n[https://arxiv.org/abs/1703.10818](https://arxiv.org/abs/1703.10818)\n\n**Face R-CNN**\n\n[https://arxiv.org/abs/1706.01061](https://arxiv.org/abs/1706.01061)\n\n**Face Detection through Scale-Friendly Deep Convolutional Networks**\n\n[https://arxiv.org/abs/1706.02863](https://arxiv.org/abs/1706.02863)\n\n**Scale-Aware Face Detection**\n\n- intro: CVPR 2017. SenseTime & Tsinghua University\n- arxiv: [https://arxiv.org/abs/1706.09876](https://arxiv.org/abs/1706.09876)\n\n**Detecting Faces Using Inside Cascaded Contextual CNN**\n\n- intro: CVPR 2017. Tencent AI Lab & SenseTime\n- paper: [http://ai.tencent.com/ailab/media/publications/Detecting_Faces_Using_Inside_Cascaded_Contextual_CNN.pdf](http://ai.tencent.com/ailab/media/publications/Detecting_Faces_Using_Inside_Cascaded_Contextual_CNN.pdf)\n\n**Multi-Branch Fully Convolutional Network for Face Detection**\n\n[https://arxiv.org/abs/1707.06330](https://arxiv.org/abs/1707.06330)\n\n**SSH: Single Stage Headless Face Detector**\n\n- intro: ICCV 2017. University of Maryland\n- arxiv: [https://arxiv.org/abs/1708.03979](https://arxiv.org/abs/1708.03979)\n- github(official, Caffe): [https://github.com/mahyarnajibi/SSH](https://github.com/mahyarnajibi/SSH)\n\n**Dockerface: an easy to install and use Faster R-CNN face detector in a Docker container**\n\n[https://arxiv.org/abs/1708.04370](https://arxiv.org/abs/1708.04370)\n\n**FaceBoxes: A CPU Real-time Face Detector with High Accuracy**\n\n- intro: IJCB 2017\n- keywords: Rapidly Digested Convolutional Layers (RDCL), Multiple Scale Convolutional Layers (MSCL)\n- intro: the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images\n- arxiv: [https://arxiv.org/abs/1708.05234](https://arxiv.org/abs/1708.05234)\n- github(official): [https://github.com/sfzhang15/FaceBoxes](https://github.com/sfzhang15/FaceBoxes)\n- github(Caffe): [https://github.com/zeusees/FaceBoxes](https://github.com/zeusees/FaceBoxes)\n\n**S3FD: Single Shot Scale-invariant Face Detector**\n\n- intro: ICCV 2017. Chinese Academy of Sciences\n- intro: can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images\n- arxiv: [https://arxiv.org/abs/1708.05237](https://arxiv.org/abs/1708.05237)\n- github(Caffe, official): [https://github.com/sfzhang15/SFD](https://github.com/sfzhang15/SFD)\n- github: [https://github.com//clcarwin/SFD_pytorch](https://github.com//clcarwin/SFD_pytorch)\n\n**Detecting Faces Using Region-based Fully Convolutional Networks**\n\n[https://arxiv.org/abs/1709.05256](https://arxiv.org/abs/1709.05256)\n\n**AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection**\n\n[https://arxiv.org/abs/1709.07326](https://arxiv.org/abs/1709.07326)\n\n**Face Attention Network: An effective Face Detector for the Occluded Faces**\n\n[https://arxiv.org/abs/1711.07246](https://arxiv.org/abs/1711.07246)\n\n**Feature Agglomeration Networks for Single Stage Face Detection**\n\n[https://arxiv.org/abs/1712.00721](https://arxiv.org/abs/1712.00721)\n\n**Face Detection Using Improved Faster RCNN**\n\n- intro: Huawei Cloud BU\n- arxiv: [https://arxiv.org/abs/1802.02142](https://arxiv.org/abs/1802.02142)\n\n**PyramidBox: A Context-assisted Single Shot Face Detector**\n\n- intro: Baidu, Inc\n- arxiv: [https://arxiv.org/abs/1803.07737](https://arxiv.org/abs/1803.07737)\n\n**PyramidBox++: High Performance Detector for Finding Tiny Face**\n\n- intro: Chinese Academy of Sciences & Baidu, Inc.\n- arxiv: [https://arxiv.org/abs/1904.00386](https://arxiv.org/abs/1904.00386)\n\n**A Fast Face Detection Method via Convolutional Neural Network**\n\n- intro: Neurocomputing\n- arxiv: [https://arxiv.org/abs/1803.10103](https://arxiv.org/abs/1803.10103)\n\n**Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy**\n\n- intro: CVPR 2018. Beihang University & CUHK & Sensetime\n- arxiv: [https://arxiv.org/abs/1804.05197](https://arxiv.org/abs/1804.05197)\n\n**Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.06039](https://arxiv.org/abs/1804.06039)\n- github(binary library): [https://github.com/Jack-CV/PCN](https://github.com/Jack-CV/PCN)\n\n**SFace: An Efficient Network for Face Detection in Large Scale Variations**\n\n- intro: Beihang University & Megvii Inc. (Face++)\n- arxiv: [https://arxiv.org/abs/1804.06559](https://arxiv.org/abs/1804.06559)\n\n**Survey of Face Detection on Low-quality Images**\n\n[https://arxiv.org/abs/1804.07362](https://arxiv.org/abs/1804.07362)\n\n**Anchor Cascade for Efficient Face Detection**\n\n- intro: The University of Sydney\n- arxiv: [https://arxiv.org/abs/1805.03363](https://arxiv.org/abs/1805.03363)\n\n**Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization**\n\n- intro: IEEE MMSP\n- arxiv: [https://arxiv.org/abs/1805.12302](https://arxiv.org/abs/1805.12302)\n\n**Selective Refinement Network for High Performance Face Detection**\n\n[https://arxiv.org/abs/1809.02693](https://arxiv.org/abs/1809.02693)\n\n**DSFD: Dual Shot Face Detector**\n\n[https://arxiv.org/abs/1810.10220](https://arxiv.org/abs/1810.10220)\n\n**Learning Better Features for Face Detection with Feature Fusion and Segmentation Supervision**\n\n[https://arxiv.org/abs/1811.08557](https://arxiv.org/abs/1811.08557)\n\n**FA-RPN: Floating Region Proposals for Face Detection**\n\n[https://arxiv.org/abs/1812.05586](https://arxiv.org/abs/1812.05586)\n\n**Robust and High Performance Face Detector**\n\n[https://arxiv.org/abs/1901.02350](https://arxiv.org/abs/1901.02350)\n\n**DAFE-FD: Density Aware Feature Enrichment for Face Detection**\n\n[https://arxiv.org/abs/1901.05375](https://arxiv.org/abs/1901.05375)\n\n**Improved Selective Refinement Network for Face Detection**\n\n- intro: Chinese Academy of Sciences & JD AI Research\n- arxiv: [https://arxiv.org/abs/1901.06651](https://arxiv.org/abs/1901.06651)\n\n**Revisiting a single-stage method for face detection**\n\n[https://arxiv.org/abs/1902.01559](https://arxiv.org/abs/1902.01559)\n\n**MSFD:Multi-Scale Receptive Field Face Detector**\n\n- intro: ICPR 2018\n- arxiv: [https://arxiv.org/abs/1903.04147](https://arxiv.org/abs/1903.04147)\n\n**LFFD: A Light and Fast Face Detector for Edge Devices**\n\n- arxiv: [https://arxiv.org/abs/1904.10633](https://arxiv.org/abs/1904.10633)\n- github: [https://github.com/YonghaoHe/A-Light-and-Fast-Face-Detector-for-Edge-Devices](https://github.com/YonghaoHe/A-Light-and-Fast-Face-Detector-for-Edge-Devices)\n\n**RetinaFace: Single-stage Dense Face Localisation in the Wild**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/1905.00641](https://arxiv.org/abs/1905.00641)\n- gihtub: [https://github.com/deepinsight/insightface/tree/master/RetinaFace](https://github.com/deepinsight/insightface/tree/master/RetinaFace)\n\n**BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs**\n\n- intro: CVPR Workshop on Computer Vision for Augmented and Virtual Reality, 2019\n- arxiv: [https://arxiv.org/abs/1907.05047](https://arxiv.org/abs/1907.05047)\n\n**HAMBox: Delving into Online High-quality Anchors Mining for Detecting Outer Faces**\n\n- intro: Baidu Inc. &  Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1912.09231](https://arxiv.org/abs/1912.09231)\n\n**KPNet: Towards Minimal Face Detector**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/2003.07543](https://arxiv.org/abs/2003.07543)\n\n**ASFD: Automatic and Scalable Face Detector**\n\n- intro: Youtu Lab, Tencent & Southeast University & Xiamen University\n- arxiv: [https://arxiv.org/abs/2003.11228](https://arxiv.org/abs/2003.11228)\n\n**TinaFace: Strong but Simple Baseline for Face Detection**\n\n- intro: Media Intelligence Technology Co.,Ltd\n- arxiv: [https://arxiv.org/abs/2011.13183](https://arxiv.org/abs/2011.13183)\n- github(PyTorch): [https://github.com/Media-Smart/vedadet](https://github.com/Media-Smart/vedadet)\n\n**MogFace: Rethinking Scale Augmentation on the Face Detector**\n\n- intro: Alibaba Group & Imperial College\n- arxiv: [https://arxiv.org/abs/2103.11139](https://arxiv.org/abs/2103.11139)\n\n**HLA-Face: Joint High-Low Adaptation for Low Light Face Detection**\n\n- intro: CVPR 2021\n- intro: Peking University\n- project page: [https://daooshee.github.io/HLA-Face-Website/](https://daooshee.github.io/HLA-Face-Website/)\n- arxiv: [https://arxiv.org/abs/2104.01984](https://arxiv.org/abs/2104.01984)\n- github: [https://github.com/daooshee/HLA-Face-Code](https://github.com/daooshee/HLA-Face-Code)\n\n**1st Place Solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition**\n\n- intro: Tomorrow Advancing Life (TAL) Education Group\n- arxiv: [https://arxiv.org/abs/2107.00818](https://arxiv.org/abs/2107.00818)\n\n**MOS: A Low Latency and Lightweight Framework for Face Detection, Landmark Localization, and Head Pose Estimation**\n\n- intro: BMVC 2021\n- arxiv: [https://arxiv.org/abs/2110.10953](https://arxiv.org/abs/2110.10953)\n- github: [https://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect](https://github.com/lyp-deeplearning/MOS-Multi-Task-Face-Detect)\n\n## Detect Small Faces\n\n**Finding Tiny Faces**\n\n- intro: CVPR 2017. CMU\n- project page: [http://www.cs.cmu.edu/~peiyunh/tiny/index.html](http://www.cs.cmu.edu/~peiyunh/tiny/index.html)\n- arxiv: [https://arxiv.org/abs/1612.04402](https://arxiv.org/abs/1612.04402)\n- github(official, Matlab): [https://github.com/peiyunh/tiny](https://github.com/peiyunh/tiny)\n- github(inference-only): [https://github.com/chinakook/hr101_mxnet](https://github.com/chinakook/hr101_mxnet)\n- github: [https://github.com/cydonia999/Tiny_Faces_in_Tensorflow](https://github.com/cydonia999/Tiny_Faces_in_Tensorflow)\n\n**Detecting and counting tiny faces**\n\n- intro: ENS Paris-Saclay. ExtendedTinyFaces\n- intro: Detecting and counting small objects - Analysis, review and application to counting\n- arxiv: [https://arxiv.org/abs/1801.06504](https://arxiv.org/abs/1801.06504)\n- github: [https://github.com/alexattia/ExtendedTinyFaces](https://github.com/alexattia/ExtendedTinyFaces)\n\n**Seeing Small Faces from Robust Anchor's Perspective**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1802.09058](https://arxiv.org/abs/1802.09058)\n\n**Face-MagNet: Magnifying Feature Maps to Detect Small Faces**\n\n- intro: WACV 2018\n- keywords: Face Magnifier Network (Face-MageNet)\n- arxiv: [https://arxiv.org/abs/1803.05258](https://arxiv.org/abs/1803.05258)\n- github: [https://github.com/po0ya/face-magnet](https://github.com/po0ya/face-magnet)\n\n**Robust Face Detection via Learning Small Faces on Hard Images**\n\n- intro: Johns Hopkins University & Stanford University\n- arxiv: [https://arxiv.org/abs/1811.11662](https://arxiv.org/abs/1811.11662)\n- github: [https://github.com/bairdzhang/smallhardface](https://github.com/bairdzhang/smallhardface)\n\n**SFA: Small Faces Attention Face Detector**\n\n- intro: Jilin University\n- arxiv: [https://arxiv.org/abs/1812.08402](https://arxiv.org/abs/1812.08402)\n\n# Person Head Detection\n\n**Context-aware CNNs for person head detection**\n\n- intro: ICCV 2015\n- project page: [http://www.di.ens.fr/willow/research/headdetection/](http://www.di.ens.fr/willow/research/headdetection/)\n- arxiv: [http://arxiv.org/abs/1511.07917](http://arxiv.org/abs/1511.07917)\n- github: [https://github.com/aosokin/cnn_head_detection](https://github.com/aosokin/cnn_head_detection)\n\n**Detecting Heads using Feature Refine Net and Cascaded Multi-scale Architecture**\n\n[https://arxiv.org/abs/1803.09256](https://arxiv.org/abs/1803.09256)\n\n**A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications**\n\n[https://arxiv.org/abs/1809.03336](https://arxiv.org/abs/1809.03336)\n\n**FCHD: A fast and accurate head detector**\n\n- arxiv: [https://arxiv.org/abs/1809.08766](https://arxiv.org/abs/1809.08766)\n- github(PyTorch, official): [https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector](https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector)\n\n**Relational Learning for Joint Head and Human Detection**\n\n- keywords: JointDet, head-body Relationship Discriminating Module (RDM)\n- arxiv: [https://arxiv.org/abs/1909.10674](https://arxiv.org/abs/1909.10674)\n\n**Body-Face Joint Detection via Embedding and Head Hook**\n\n- intro: ICCV 2021\n- paper: [https://openaccess.thecvf.com/content/ICCV2021/papers/Wan_Body-Face_Joint_Detection_via_Embedding_and_Head_Hook_ICCV_2021_paper.pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Wan_Body-Face_Joint_Detection_via_Embedding_and_Head_Hook_ICCV_2021_paper.pdf)\n- gihtub: [https://github.com/AibeeDetect/BFJDet](https://github.com/AibeeDetect/BFJDet)\n\n# Pedestrian Detection / People Detection\n\n**Pedestrian Detection aided by Deep Learning Semantic Tasks**\n\n- intro: CVPR 2015\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/](http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/)\n- arxiv: [http://arxiv.org/abs/1412.0069](http://arxiv.org/abs/1412.0069)\n\n**Deep Learning Strong Parts for Pedestrian Detection**\n\n- intro: ICCV 2015. CUHK. DeepParts\n- intro: Achieving 11.89% average miss rate on Caltech Pedestrian Dataset\n- paper: [http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf](http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf)\n\n**Taking a Deeper Look at Pedestrians**\n\n- intro: CVPR 2015\n- arxiv: [https://arxiv.org/abs/1501.05790](https://arxiv.org/abs/1501.05790)\n\n**Convolutional Channel Features**\n\n- intro: ICCV 2015\n- arxiv: [https://arxiv.org/abs/1504.07339](https://arxiv.org/abs/1504.07339)\n- github: [https://github.com/byangderek/CCF](https://github.com/byangderek/CCF)\n\n**End-to-end people detection in crowded scenes**\n\n- arxiv: [http://arxiv.org/abs/1506.04878](http://arxiv.org/abs/1506.04878)\n- github: [https://github.com/Russell91/reinspect](https://github.com/Russell91/reinspect)\n- ipn: [http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb](http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb)\n- youtube: [https://www.youtube.com/watch?v=QeWl0h3kQ24](https://www.youtube.com/watch?v=QeWl0h3kQ24)\n\n**Learning Complexity-Aware Cascades for Deep Pedestrian Detection**\n\n- intro: ICCV 2015\n- arxiv: [https://arxiv.org/abs/1507.05348](https://arxiv.org/abs/1507.05348)\n\n**Deep convolutional neural networks for pedestrian detection**\n\n- arxiv: [http://arxiv.org/abs/1510.03608](http://arxiv.org/abs/1510.03608)\n- github: [https://github.com/DenisTome/DeepPed](https://github.com/DenisTome/DeepPed)\n\n**Scale-aware Fast R-CNN for Pedestrian Detection**\n\n- arxiv: [https://arxiv.org/abs/1510.08160](https://arxiv.org/abs/1510.08160)\n\n**New algorithm improves speed and accuracy of pedestrian detection**\n\n- blog: [http://www.eurekalert.org/pub_releases/2016-02/uoc--nai020516.php](http://www.eurekalert.org/pub_releases/2016-02/uoc--nai020516.php)\n\n**Pushing the Limits of Deep CNNs for Pedestrian Detection**\n\n- intro: \"set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from 11.7% to 8.9%\"\n- arxiv: [http://arxiv.org/abs/1603.04525](http://arxiv.org/abs/1603.04525)\n\n**A Real-Time Deep Learning Pedestrian Detector for Robot Navigation**\n\n- arxiv: [http://arxiv.org/abs/1607.04436](http://arxiv.org/abs/1607.04436)\n\n**A Real-Time Pedestrian Detector using Deep Learning for Human-Aware Navigation**\n\n- arxiv: [http://arxiv.org/abs/1607.04441](http://arxiv.org/abs/1607.04441)\n\n**Is Faster R-CNN Doing Well for Pedestrian Detection?**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1607.07032](http://arxiv.org/abs/1607.07032)\n- github: [https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian](https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian)\n\n**Unsupervised Deep Domain Adaptation for Pedestrian Detection**\n\n- intro: ECCV Workshop 2016\n- arxiv: [https://arxiv.org/abs/1802.03269](https://arxiv.org/abs/1802.03269)\n\n**Reduced Memory Region Based Deep Convolutional Neural Network Detection**\n\n- intro: IEEE 2016 ICCE-Berlin\n- arxiv: [http://arxiv.org/abs/1609.02500](http://arxiv.org/abs/1609.02500)\n\n**Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection**\n\n- arxiv: [https://arxiv.org/abs/1610.03466](https://arxiv.org/abs/1610.03466)\n\n**Detecting People in Artwork with CNNs**\n\n- intro: ECCV 2016 Workshops\n- arxiv: [https://arxiv.org/abs/1610.08871](https://arxiv.org/abs/1610.08871)\n\n**Deep Multi-camera People Detection**\n\n- arxiv: [https://arxiv.org/abs/1702.04593](https://arxiv.org/abs/1702.04593)\n\n**Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters**\n\n- intro: CVPR 2017\n- project page: [http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/](http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/)\n- arxiv: [https://arxiv.org/abs/1703.06283](https://arxiv.org/abs/1703.06283)\n- github(Tensorflow): [https://github.com/huangshiyu13/RPNplus](https://github.com/huangshiyu13/RPNplus)\n\n**What Can Help Pedestrian Detection?**\n\n- intro: CVPR 2017. Tsinghua University & Peking University & Megvii Inc.\n- keywords: Faster R-CNN, HyperLearner\n- arxiv: [https://arxiv.org/abs/1705.02757](https://arxiv.org/abs/1705.02757)\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Mao_What_Can_Help_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Mao_What_Can_Help_CVPR_2017_paper.pdf)\n\n**Illuminating Pedestrians via Simultaneous Detection & Segmentation**\n\n[https://arxiv.org/abs/1706.08564](https://arxiv.org/abs/1706.08564)\n\n**Rotational Rectification Network for Robust Pedestrian Detection**\n\n- intro: CMU & Volvo Construction\n- arxiv: [https://arxiv.org/abs/1706.08917](https://arxiv.org/abs/1706.08917)\n\n**STD-PD: Generating Synthetic Training Data for Pedestrian Detection in Unannotated Videos**\n\n- intro: The University of North Carolina at Chapel Hill\n- arxiv: [https://arxiv.org/abs/1707.09100](https://arxiv.org/abs/1707.09100)\n\n**Too Far to See? Not Really! --- Pedestrian Detection with Scale-aware Localization Policy**\n\n[https://arxiv.org/abs/1709.00235](https://arxiv.org/abs/1709.00235)\n\n**Aggregated Channels Network for Real-Time Pedestrian Detection**\n\n[https://arxiv.org/abs/1801.00476](https://arxiv.org/abs/1801.00476)\n\n**Exploring Multi-Branch and High-Level Semantic Networks for Improving Pedestrian Detection**\n\n[https://arxiv.org/abs/1804.00872](https://arxiv.org/abs/1804.00872)\n\n**Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond**\n\n[https://arxiv.org/abs/1804.02047](https://arxiv.org/abs/1804.02047)\n\n**PCN: Part and Context Information for Pedestrian Detection with CNNs**\n\n- intro: British Machine Vision Conference(BMVC) 2017\n- arxiv: [https://arxiv.org/abs/1804.04483](https://arxiv.org/abs/1804.04483)\n\n**Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf)\n\n**Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation**\n\n- intro: ECCV 2018\n- intro: Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1807.01438](https://arxiv.org/abs/1807.01438)\n\n**Bi-box Regression for Pedestrian Detection and Occlusion Estimation**\n\n- intro: ECCV 2018\n- paper: [http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf](http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf)\n- github(Pytorch): [https://github.com/rainofmine/Bi-box_Regression](https://github.com/rainofmine/Bi-box_Regression)\n\n**Pedestrian Detection with Autoregressive Network Phases**\n\n- intro: Michigan State University\n- arxiv: [https://arxiv.org/abs/1812.00440](https://arxiv.org/abs/1812.00440)\n\n**SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection**\n\n[https://arxiv.org/abs/1902.09080](https://arxiv.org/abs/1902.09080)\n\n**High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection**\n\n**Center and Scale Prediction: A Box-free Approach for Object Detection**\n\n- intro: CVPR 2019\n- intro: National University of Defense Technology & Chinese Academy of Sciences & Inception Institute of Artificial Intelligence (IIAI) & Horizon Robotics Inc.\n- arxiv: [https://arxiv.org/abs/1904.02948](https://arxiv.org/abs/1904.02948)\n- github(official, Keras): [https://github.com/liuwei16/CSP](https://github.com/liuwei16/CSP)\n\n**Evading Real-Time Person Detectors by Adversarial T-shirt**\n\n[https://arxiv.org/abs/1910.11099](https://arxiv.org/abs/1910.11099)\n\n**Coupled Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling**\n\n[https://arxiv.org/abs/1912.08661](https://arxiv.org/abs/1912.08661)\n\n**Scale Match for Tiny Person Detection**\n\n- intro: WACV 2020\n- arxiv: [https://arxiv.org/abs/1912.10664](https://arxiv.org/abs/1912.10664)\n- github: [https://github.com/ucas-vg/TinyBenchmark](https://github.com/ucas-vg/TinyBenchmark)\n\n**SM+: Refined Scale Match for Tiny Person Detection**\n\n[https://arxiv.org/abs/2102.03558](https://arxiv.org/abs/2102.03558)\n\n**Resisting the Distracting-factors in Pedestrian Detection**\n\n- intro: Beihang University & Arizona State University\n- arxiv: [https://arxiv.org/abs/2005.07344](https://arxiv.org/abs/2005.07344)\n\n**SADet: Learning An Efficient and Accurate Pedestrian Detector**\n\n[https://arxiv.org/abs/2007.13119](https://arxiv.org/abs/2007.13119)\n\n**NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination**\n\n- intro: ACM MM 2020\n- intro: Tencent Youtu Lab\n- arxiv: [https://arxiv.org/abs/2007.13376](https://arxiv.org/abs/2007.13376)\n\n**Anchor-free Small-scale Multispectral Pedestrian Detection**\n\n- intro: BMVC 2020\n- arxiv: [https://arxiv.org/abs/2008.08418](https://arxiv.org/abs/2008.08418)\n- github: [https://github.com/HensoldtOptronicsCV/MultispectralPedestrianDetection](https://github.com/HensoldtOptronicsCV/MultispectralPedestrianDetection)\n\n**LLA: Loss-aware Label Assignment for Dense Pedestrian Detection**\n\n- arxiv: [https://arxiv.org/abs/2101.04307](https://arxiv.org/abs/2101.04307)\n- github: [https://github.com/Megvii-BaseDetection/LLA](https://github.com/Megvii-BaseDetection/LLA)\n\n**DETR for Pedestrian Detection**\n\n[https://arxiv.org/abs/2012.06785](https://arxiv.org/abs/2012.06785)\n\n**V2F-Net: Explicit Decomposition of Occluded Pedestrian Detection**\n\n- intro: MEGVII Technology & Texas A&M University\n- arxiv: [https://arxiv.org/abs/2104.03106](https://arxiv.org/abs/2104.03106)\n\n## Pedestrian Detection in a Crowd\n\n**Repulsion Loss: Detecting Pedestrians in a Crowd**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1711.07752](https://arxiv.org/abs/1711.07752)\n\n**Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.08407](https://arxiv.org/abs/1807.08407)\n\n**Adaptive NMS: Refining Pedestrian Detection in a Crowd**\n\n- intro: CVPR 2019 oral\n- arxiv: [https://arxiv.org/abs/1904.03629](https://arxiv.org/abs/1904.03629)\n\n**PedHunter: Occlusion Robust Pedestrian Detector in Crowded Scenes**\n\n- keywords: SUR-PED\n- arxiv: [https://arxiv.org/abs/1909.06826](https://arxiv.org/abs/1909.06826)\n\n**Double Anchor R-CNN for Human Detection in a Crowd**\n\n- intro: Megvii Inc. (Face++) & Tsinghua University & Xi’an Jiaotong University & Zhejiang University\n- arxiv: [https://arxiv.org/abs/1909.09998](https://arxiv.org/abs/1909.09998)\n\n**CSID: Center, Scale, Identity and Density-aware Pedestrian Detection in a Crowd**\n\n[https://arxiv.org/abs/1910.09188](https://arxiv.org/abs/1910.09188)\n\n**Semantic Head Enhanced Pedestrian Detection in a Crowd**\n\n[https://arxiv.org/abs/1911.11985](https://arxiv.org/abs/1911.11985)\n\n**Detection in Crowded Scenes: One Proposal, Multiple Predictions**\n\n- intro: CVPR 2020 Oral\n- arxiv: [https://arxiv.org/abs/2003.09163](https://arxiv.org/abs/2003.09163)\n- github: [https://github.com/Purkialo/CrowdDet](https://github.com/Purkialo/CrowdDet)\n\n**Visible Feature Guidance for Crowd Pedestrian Detection**\n\n- intro: ECCV 2020 RLQ Workshop\n- arxiv: [https://arxiv.org/abs/2008.09993](https://arxiv.org/abs/2008.09993)\n\n# Occluded Pedestrian Detection\n\n**Mask-Guided Attention Network for Occluded Pedestrian Detection**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.06160](https://arxiv.org/abs/1910.06160)\n- github: [https://github.com/Leotju/MGAN](https://github.com/Leotju/MGAN)\n\n## Multispectral Pedestrian Detection\n\n**Multispectral Deep Neural Networks for Pedestrian Detection**\n\n- intro: BMVC 2016 oral\n- arxiv: [https://arxiv.org/abs/1611.02644](https://arxiv.org/abs/1611.02644)\n\n**Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian Detection**\n\n- intro: State Key Lab of CAD&CG, Zhejiang University\n- arxiv: [https://arxiv.org/abs/1803.05347](https://arxiv.org/abs/1803.05347)\n\n**Multispectral Pedestrian Detection via Simultaneous Detection and Segmentation**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1808.04818](https://arxiv.org/abs/1808.04818)\n\n**The Cross-Modality Disparity Problem in Multispectral Pedestrian Detection**\n\n[https://arxiv.org/abs/1901.02645](https://arxiv.org/abs/1901.02645)\n\n**Box-level Segmentation Supervised Deep Neural Networks for Accurate and Real-time Multispectral Pedestrian Detection**\n\n[https://arxiv.org/abs/1902.05291](https://arxiv.org/abs/1902.05291)\n\n**GFD-SSD: Gated Fusion Double SSD for Multispectral Pedestrian Detection**\n\n[https://arxiv.org/abs/1903.06999](https://arxiv.org/abs/1903.06999)\n\n**Unsupervised Domain Adaptation for Multispectral Pedestrian Detection**\n\n[https://arxiv.org/abs/1904.03692](https://arxiv.org/abs/1904.03692)\n\n# Vehicle Detection\n\n**DAVE: A Unified Framework for Fast Vehicle Detection and Annotation**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1607.04564](http://arxiv.org/abs/1607.04564)\n\n**Evolving Boxes for fast Vehicle Detection**\n\n- arxiv: [https://arxiv.org/abs/1702.00254](https://arxiv.org/abs/1702.00254)\n\n**Fine-Grained Car Detection for Visual Census Estimation**\n\n- intro: AAAI 2016\n- arxiv: [https://arxiv.org/abs/1709.02480](https://arxiv.org/abs/1709.02480)\n\n**SINet: A Scale-insensitive Convolutional Neural Network for Fast Vehicle Detection**\n\n- intro: IEEE Transactions on Intelligent Transportation Systems (T-ITS)\n- arxiv: [https://arxiv.org/abs/1804.00433](https://arxiv.org/abs/1804.00433)\n\n**Label and Sample: Efficient Training of Vehicle Object Detector from Sparsely Labeled Data**\n\n- intro: UC Berkeley\n- arxiv: [https://arxiv.org/abs/1808.08603](https://arxiv.org/abs/1808.08603)\n\n**Domain Randomization for Scene-Specific Car Detection and Pose Estimation**\n\n[https://arxiv.org/abs/1811.05939](https://arxiv.org/abs/1811.05939)\n\n**ShuffleDet: Real-Time Vehicle Detection Network in On-board Embedded UAV Imagery**\n\n- intro: ECCV 2018, UAVision 2018\n- arxiv: [https://arxiv.org/abs/1811.06318](https://arxiv.org/abs/1811.06318)\n\n# Traffic-Sign Detection\n\n**Traffic-Sign Detection and Classification in the Wild**\n\n- intro: CVPR 2016\n- project page(code+dataset): [http://cg.cs.tsinghua.edu.cn/traffic-sign/](http://cg.cs.tsinghua.edu.cn/traffic-sign/)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf)\n- code & model: [http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip](http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip)\n\n**Evaluating State-of-the-art Object Detector on Challenging Traffic Light Data**\n\n- intro: CVPR 2017 workshop\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017_workshops/w9/papers/Jensen_Evaluating_State-Of-The-Art_Object_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w9/papers/Jensen_Evaluating_State-Of-The-Art_Object_CVPR_2017_paper.pdf)\n\n**Detecting Small Signs from Large Images**\n\n- intro: IEEE Conference on Information Reuse and Integration (IRI) 2017 oral\n- arxiv: [https://arxiv.org/abs/1706.08574](https://arxiv.org/abs/1706.08574)\n\n**Localized Traffic Sign Detection with Multi-scale Deconvolution Networks**\n\n[https://arxiv.org/abs/1804.10428](https://arxiv.org/abs/1804.10428)\n\n**Detecting Traffic Lights by Single Shot Detection**\n\n- intro: ITSC 2018\n- arxiv: [https://arxiv.org/abs/1805.02523](https://arxiv.org/abs/1805.02523)\n\n**A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection**\n\n- intro: IEEE 15th Conference on Computer and Robot Vision\n- arxiv: [https://arxiv.org/abs/1806.07987](https://arxiv.org/abs/1806.07987)\n- demo: [https://www.youtube.com/watch?v=_YmogPzBXOw&feature=youtu.be](https://www.youtube.com/watch?v=_YmogPzBXOw&feature=youtu.be)\n\n# Skeleton Detection\n\n**Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs**\n\n![](https://camo.githubusercontent.com/88a65f132aa4ae4b0477e3ad02c13cdc498377d9/687474703a2f2f37786e37777a2e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f44656570536b656c65746f6e2e706e673f696d61676556696577322f322f772f353030)\n\n- arxiv: [http://arxiv.org/abs/1603.09446](http://arxiv.org/abs/1603.09446)\n- github: [https://github.com/zeakey/DeepSkeleton](https://github.com/zeakey/DeepSkeleton)\n\n**DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images**\n\n- arxiv: [http://arxiv.org/abs/1609.03659](http://arxiv.org/abs/1609.03659)\n\n**SRN: Side-output Residual Network for Object Symmetry Detection in the Wild**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1703.02243](https://arxiv.org/abs/1703.02243)\n- github: [https://github.com/KevinKecc/SRN](https://github.com/KevinKecc/SRN)\n\n**Hi-Fi: Hierarchical Feature Integration for Skeleton Detection**\n\n[https://arxiv.org/abs/1801.01849](https://arxiv.org/abs/1801.01849)\n\n# Fruit Detection\n\n**Deep Fruit Detection in Orchards**\n\n- arxiv: [https://arxiv.org/abs/1610.03677](https://arxiv.org/abs/1610.03677)\n\n**Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards**\n\n- intro: The Journal of Field Robotics in May 2016\n- project page: [http://confluence.acfr.usyd.edu.au/display/AGPub/](http://confluence.acfr.usyd.edu.au/display/AGPub/)\n- arxiv: [https://arxiv.org/abs/1610.08120](https://arxiv.org/abs/1610.08120)\n\n## Shadow Detection\n\n**Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network**\n\n[https://arxiv.org/abs/1709.09283](https://arxiv.org/abs/1709.09283)\n\n**A+D-Net: Shadow Detection with Adversarial Shadow Attenuation**\n\n[https://arxiv.org/abs/1712.01361](https://arxiv.org/abs/1712.01361)\n\n**Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal**\n\n[https://arxiv.org/abs/1712.02478](https://arxiv.org/abs/1712.02478)\n\n**Direction-aware Spatial Context Features for Shadow Detection**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1712.04142](https://arxiv.org/abs/1712.04142)\n\n**Direction-aware Spatial Context Features for Shadow Detection and Removal**\n\n- intro: The Chinese University of Hong Kong & The Hong Kong Polytechnic University\n- arxiv:  [https://arxiv.org/abs/1805.04635](https://arxiv.org/abs/1805.04635)\n\n# Others Detection\n\n**Deep Deformation Network for Object Landmark Localization**\n\n- arxiv: [http://arxiv.org/abs/1605.01014](http://arxiv.org/abs/1605.01014)\n\n**Fashion Landmark Detection in the Wild**\n\n- intro: ECCV 2016\n- project page: [http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html](http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html)\n- arxiv: [http://arxiv.org/abs/1608.03049](http://arxiv.org/abs/1608.03049)\n- github(Caffe): [https://github.com/liuziwei7/fashion-landmarks](https://github.com/liuziwei7/fashion-landmarks)\n\n**Deep Learning for Fast and Accurate Fashion Item Detection**\n\n- intro: Kuznech Inc.\n- intro: MultiBox and Fast R-CNN\n- paper: [https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf](https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf)\n\n**OSMDeepOD - OSM and Deep Learning based Object Detection from Aerial Imagery (formerly known as \"OSM-Crosswalk-Detection\")**\n\n![](https://raw.githubusercontent.com/geometalab/OSMDeepOD/master/imgs/process.png)\n\n- github: [https://github.com/geometalab/OSMDeepOD](https://github.com/geometalab/OSMDeepOD)\n\n**Selfie Detection by Synergy-Constraint Based Convolutional Neural Network**\n\n- intro:  IEEE SITIS 2016\n- arxiv: [https://arxiv.org/abs/1611.04357](https://arxiv.org/abs/1611.04357)\n\n**Associative Embedding:End-to-End Learning for Joint Detection and Grouping**\n\n- arxiv: [https://arxiv.org/abs/1611.05424](https://arxiv.org/abs/1611.05424)\n\n**Deep Cuboid Detection: Beyond 2D Bounding Boxes**\n\n- intro: CMU & Magic Leap\n- arxiv: [https://arxiv.org/abs/1611.10010](https://arxiv.org/abs/1611.10010)\n\n**Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection**\n\n- arxiv: [https://arxiv.org/abs/1612.03019](https://arxiv.org/abs/1612.03019)\n\n**Deep Learning Logo Detection with Data Expansion by Synthesising Context**\n\n- arxiv: [https://arxiv.org/abs/1612.09322](https://arxiv.org/abs/1612.09322)\n\n**Scalable Deep Learning Logo Detection**\n\n[https://arxiv.org/abs/1803.11417](https://arxiv.org/abs/1803.11417)\n\n**Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.00307](https://arxiv.org/abs/1702.00307)\n\n**Automatic Handgun Detection Alarm in Videos Using Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1702.05147](https://arxiv.org/abs/1702.05147)\n- results: [https://github.com/SihamTabik/Pistol-Detection-in-Videos](https://github.com/SihamTabik/Pistol-Detection-in-Videos)\n\n**Objects as context for part detection**\n\n[https://arxiv.org/abs/1703.09529](https://arxiv.org/abs/1703.09529)\n\n**Using Deep Networks for Drone Detection**\n\n- intro: AVSS 2017\n- arxiv: [https://arxiv.org/abs/1706.05726](https://arxiv.org/abs/1706.05726)\n\n**Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.01642](https://arxiv.org/abs/1708.01642)\n\n**Target Driven Instance Detection**\n\n[https://arxiv.org/abs/1803.04610](https://arxiv.org/abs/1803.04610)\n\n**DeepVoting: An Explainable Framework for Semantic Part Detection under Partial Occlusion**\n\n[https://arxiv.org/abs/1709.04577](https://arxiv.org/abs/1709.04577)\n\n**VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1710.06288](https://arxiv.org/abs/1710.06288)\n- github: [https://github.com/SeokjuLee/VPGNet](https://github.com/SeokjuLee/VPGNet)\n\n**Grab, Pay and Eat: Semantic Food Detection for Smart Restaurants**\n\n[https://arxiv.org/abs/1711.05128](https://arxiv.org/abs/1711.05128)\n\n**ReMotENet: Efficient Relevant Motion Event Detection for Large-scale Home Surveillance Videos**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.02031](https://arxiv.org/abs/1801.02031)\n\n**Deep Learning Object Detection Methods for Ecological Camera Trap Data**\n\n- intro: Conference of Computer and Robot Vision. University of Guelph\n- arxiv: [https://arxiv.org/abs/1803.10842](https://arxiv.org/abs/1803.10842)\n\n**EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection**\n\n[https://arxiv.org/abs/1806.05525](https://arxiv.org/abs/1806.05525)\n\n**Towards End-to-End Lane Detection: an Instance Segmentation Approach**\n\n- arxiv: [https://arxiv.org/abs/1802.05591](https://arxiv.org/abs/1802.05591)\n- github: [https://github.com/MaybeShewill-CV/lanenet-lane-detection](https://github.com/MaybeShewill-CV/lanenet-lane-detection)\n\n**Densely Supervised Grasp Detector (DSGD)**\n\n[https://arxiv.org/abs/1810.03962](https://arxiv.org/abs/1810.03962)\n\n# Object Proposal\n\n**DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers**\n\n- arxiv: [http://arxiv.org/abs/1510.04445](http://arxiv.org/abs/1510.04445)\n- github: [https://github.com/aghodrati/deepproposal](https://github.com/aghodrati/deepproposal)\n\n**Scale-aware Pixel-wise Object Proposal Networks**\n\n- intro: IEEE Transactions on Image Processing\n- arxiv: [http://arxiv.org/abs/1601.04798](http://arxiv.org/abs/1601.04798)\n\n**Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization**\n\n- intro: BMVC 2016. AttractioNet\n- arxiv: [https://arxiv.org/abs/1606.04446](https://arxiv.org/abs/1606.04446)\n- github: [https://github.com/gidariss/AttractioNet](https://github.com/gidariss/AttractioNet)\n\n**Learning to Segment Object Proposals via Recursive Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1612.01057](https://arxiv.org/abs/1612.01057)\n\n**Learning Detection with Diverse Proposals**\n\n- intro: CVPR 2017\n- keywords: differentiable Determinantal Point Process (DPP) layer, Learning Detection with Diverse Proposals (LDDP)\n- arxiv: [https://arxiv.org/abs/1704.03533](https://arxiv.org/abs/1704.03533)\n\n**ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond**\n\n- keywords: product detection\n- arxiv: [https://arxiv.org/abs/1704.06752](https://arxiv.org/abs/1704.06752)\n\n**Improving Small Object Proposals for Company Logo Detection**\n\n- intro: ICMR 2017\n- arxiv: [https://arxiv.org/abs/1704.08881](https://arxiv.org/abs/1704.08881)\n\n**Open Logo Detection Challenge**\n\n- intro: BMVC 2018\n- keywords: QMUL-OpenLogo\n- project page: [https://qmul-openlogo.github.io/](https://qmul-openlogo.github.io/)\n- arxiv: [https://arxiv.org/abs/1807.01964](https://arxiv.org/abs/1807.01964)\n\n**AttentionMask: Attentive, Efficient Object Proposal Generation Focusing on Small Objects**\n\n- intro: ACCV 2018 oral\n- arxiv: [https://arxiv.org/abs/1811.08728](https://arxiv.org/abs/1811.08728)\n- github: [https://github.com/chwilms/AttentionMask](https://github.com/chwilms/AttentionMask)\n\n# Localization\n\n**Beyond Bounding Boxes: Precise Localization of Objects in Images**\n\n- intro: PhD Thesis\n- homepage: [http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html](http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html)\n- phd-thesis: [http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf](http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf)\n- github(\"SDS using hypercolumns\"): [https://github.com/bharath272/sds](https://github.com/bharath272/sds)\n\n**Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning**\n\n- arxiv: [http://arxiv.org/abs/1503.00949](http://arxiv.org/abs/1503.00949)\n\n**Weakly Supervised Object Localization Using Size Estimates**\n\n- arxiv: [http://arxiv.org/abs/1608.04314](http://arxiv.org/abs/1608.04314)\n\n**Active Object Localization with Deep Reinforcement Learning**\n\n- intro: ICCV 2015\n- keywords: Markov Decision Process\n- arxiv: [https://arxiv.org/abs/1511.06015](https://arxiv.org/abs/1511.06015)\n\n**Localizing objects using referring expressions**\n\n- intro: ECCV 2016\n- keywords: LSTM, multiple instance learning (MIL)\n- paper: [http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf](http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf)\n- github: [https://github.com/varun-nagaraja/referring-expressions](https://github.com/varun-nagaraja/referring-expressions)\n\n**LocNet: Improving Localization Accuracy for Object Detection**\n\n- intro: CVPR 2016 oral\n- arxiv: [http://arxiv.org/abs/1511.07763](http://arxiv.org/abs/1511.07763)\n- github: [https://github.com/gidariss/LocNet](https://github.com/gidariss/LocNet)\n\n**Learning Deep Features for Discriminative Localization**\n\n![](http://cnnlocalization.csail.mit.edu/framework.jpg)\n\n- homepage: [http://cnnlocalization.csail.mit.edu/](http://cnnlocalization.csail.mit.edu/)\n- arxiv: [http://arxiv.org/abs/1512.04150](http://arxiv.org/abs/1512.04150)\n- github(Tensorflow): [https://github.com/jazzsaxmafia/Weakly_detector](https://github.com/jazzsaxmafia/Weakly_detector)\n- github: [https://github.com/metalbubble/CAM](https://github.com/metalbubble/CAM)\n- github: [https://github.com/tdeboissiere/VGG16CAM-keras](https://github.com/tdeboissiere/VGG16CAM-keras)\n\n**ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization**\n\n![](http://www.di.ens.fr/willow/research/contextlocnet/model.png)\n\n- intro: ECCV 2016\n- project page: [http://www.di.ens.fr/willow/research/contextlocnet/](http://www.di.ens.fr/willow/research/contextlocnet/)\n- arxiv: [http://arxiv.org/abs/1609.04331](http://arxiv.org/abs/1609.04331)\n- github: [https://github.com/vadimkantorov/contextlocnet](https://github.com/vadimkantorov/contextlocnet)\n\n**Ensemble of Part Detectors for Simultaneous Classification and Localization**\n\n[https://arxiv.org/abs/1705.10034](https://arxiv.org/abs/1705.10034)\n\n**STNet: Selective Tuning of Convolutional Networks for Object Localization**\n\n[https://arxiv.org/abs/1708.06418](https://arxiv.org/abs/1708.06418)\n\n**Soft Proposal Networks for Weakly Supervised Object Localization**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.01829](https://arxiv.org/abs/1709.01829)\n\n**Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN**\n\n- intro: ACM MM 2017\n- arxiv: [https://arxiv.org/abs/1709.08295](https://arxiv.org/abs/1709.08295)\n\n# Tutorials / Talks\n\n**Convolutional Feature Maps: Elements of efficient (and accurate) CNN-based object detection**\n\n- slides: [http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf](http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf)\n\n**Towards Good Practices for Recognition & Detection**\n\n- intro: Hikvision Research Institute. Supervised Data Augmentation (SDA)\n- slides: [http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf](http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf)\n\n**Work in progress: Improving object detection and instance segmentation for small objects**\n\n[https://docs.google.com/presentation/d/1OTfGn6mLe1VWE8D0q6Tu_WwFTSoLGd4OF8WCYnOWcVo/edit#slide=id.g37418adc7a_0_229](https://docs.google.com/presentation/d/1OTfGn6mLe1VWE8D0q6Tu_WwFTSoLGd4OF8WCYnOWcVo/edit#slide=id.g37418adc7a_0_229)\n\n**Object Detection with Deep Learning: A Review**\n\n[https://arxiv.org/abs/1807.05511](https://arxiv.org/abs/1807.05511)\n\n# Projects\n\n**Detectron**\n\n- intro: FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.\n- github: [https://github.com/facebookresearch/Detectron](https://github.com/facebookresearch/Detectron)\n\n**Detectron2**\n\n- intro: Detectron2 is FAIR's next-generation platform for object detection and segmentation.\n- github: [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2)\n\n**MMDetection**\n\n- intro: MMDetection: Open MMLab Detection Toolbox and Benchmark\n- arxiv: [https://arxiv.org/abs/1906.07155](https://arxiv.org/abs/1906.07155)\n- github: [https://github.com/open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection)\n- docs: [https://mmdetection.readthedocs.io/en/latest/](https://mmdetection.readthedocs.io/en/latest/)\n\n**SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition**\n\n- intro: A Simple and Versatile Framework for Object Detection and Instance Recognition\n- github: [https://github.com/TuSimple/simpledet](https://github.com/TuSimple/simpledet)\n\n**AdelaiDet**\n\n- intro: AdelaiDet is an open source toolbox for multiple instance-level detection and recognition tasks.\n- github: [https://github.com/aim-uofa/AdelaiDet/](https://github.com/aim-uofa/AdelaiDet/)\n\n**TensorBox: a simple framework for training neural networks to detect objects in images**\n\n- intro: \"The basic model implements the simple and robust GoogLeNet-OverFeat algorithm. \nWe additionally provide an implementation of the [ReInspect](https://github.com/Russell91/ReInspect/) algorithm\"\n- github: [https://github.com/Russell91/TensorBox](https://github.com/Russell91/TensorBox)\n\n**NanoDet**\n\n- intro: Super fast and lightweight anchor-free object detection model. Real-time on mobile devices.\n- arxiv: [https://github.com/RangiLyu/nanodet](https://github.com/RangiLyu/nanodet)\n\n**Object detection in torch: Implementation of some object detection frameworks in torch**\n\n- github: [https://github.com/fmassa/object-detection.torch](https://github.com/fmassa/object-detection.torch)\n\n**Using DIGITS to train an Object Detection network**\n\n- github: [https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md](https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md)\n\n**FCN-MultiBox Detector**\n\n- intro: Full convolution MultiBox Detector (like SSD) implemented in Torch.\n- github: [https://github.com/teaonly/FMD.torch](https://github.com/teaonly/FMD.torch)\n\n**KittiBox: A car detection model implemented in Tensorflow.**\n\n- keywords: MultiNet\n- intro: KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset\n- github: [https://github.com/MarvinTeichmann/KittiBox](https://github.com/MarvinTeichmann/KittiBox)\n\n**Deformable Convolutional Networks + MST + Soft-NMS**\n\n- github: [https://github.com/bharatsingh430/Deformable-ConvNets](https://github.com/bharatsingh430/Deformable-ConvNets)\n\n**How to Build a Real-time Hand-Detector using Neural Networks (SSD) on Tensorflow**\n\n- blog: [https://towardsdatascience.com/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce](https://towardsdatascience.com/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce)\n- github: [https://github.com//victordibia/handtracking](https://github.com//victordibia/handtracking)\n\n**Metrics for object detection**\n\n- intro: Most popular metrics used to evaluate object detection algorithms\n- github: [https://github.com/rafaelpadilla/Object-Detection-Metrics](https://github.com/rafaelpadilla/Object-Detection-Metrics)\n\n**MobileNetv2-SSDLite**\n\n- intro: Caffe implementation of SSD and SSDLite detection on MobileNetv2, converted from tensorflow.\n- github: [https://github.com/chuanqi305/MobileNetv2-SSDLite](https://github.com/chuanqi305/MobileNetv2-SSDLite)\n\n# Leaderboard\n\n**Detection Results: VOC2012**\n\n- intro: Competition \"comp4\" (train on additional data)\n- homepage: [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=4](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=4)\n\n# Tools\n\n**BeaverDam: Video annotation tool for deep learning training labels**\n\n[https://github.com/antingshen/BeaverDam](https://github.com/antingshen/BeaverDam)\n\n# Blogs\n\n**Convolutional Neural Networks for Object Detection**\n\n[http://rnd.azoft.com/convolutional-neural-networks-object-detection/](http://rnd.azoft.com/convolutional-neural-networks-object-detection/)\n\n**Introducing automatic object detection to visual search (Pinterest)**\n\n- keywords: Faster R-CNN\n- blog: [https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search](https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search)\n- demo: [https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4](https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4)\n- review: [https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D](https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D)\n\n**Deep Learning for Object Detection with DIGITS**\n\n- blog: [https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/](https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/)\n\n**Analyzing The Papers Behind Facebook's Computer Vision Approach**\n\n- keywords: DeepMask, SharpMask, MultiPathNet\n- blog: [https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook's-Computer-Vision-Approach/](https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook's-Computer-Vision-Approach/)\n\n**Easily Create High Quality Object Detectors with Deep Learning**\n\n- intro: dlib v19.2\n- blog: [http://blog.dlib.net/2016/10/easily-create-high-quality-object.html](http://blog.dlib.net/2016/10/easily-create-high-quality-object.html)\n\n**How to Train a Deep-Learned Object Detection Model in the Microsoft Cognitive Toolkit**\n\n- blog: [https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/](https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/)\n- github: [https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN](https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN)\n\n**Object Detection in Satellite Imagery, a Low Overhead Approach**\n\n- part 1: [https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9](https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9)\n- part 2: [https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64](https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64)\n\n**You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks**\n\n- part 1: [https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of](https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of)\n- part 2: [https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t](https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t)\n\n**Faster R-CNN Pedestrian and Car Detection**\n\n- blog: [https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/](https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/)\n- ipn: [https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb](https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb)\n- github: [https://github.com/bigsnarfdude/Faster-RCNN_TF](https://github.com/bigsnarfdude/Faster-RCNN_TF)\n\n**Small U-Net for vehicle detection**\n\n- blog: [https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad](https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad)\n\n**Region of interest pooling explained**\n\n- blog: [https://deepsense.io/region-of-interest-pooling-explained/](https://deepsense.io/region-of-interest-pooling-explained/)\n- github: [https://github.com/deepsense-io/roi-pooling](https://github.com/deepsense-io/roi-pooling)\n\n**Supercharge your Computer Vision models with the TensorFlow Object Detection API**\n\n- blog: [https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html](https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html)\n- github: [https://github.com/tensorflow/models/tree/master/object_detection](https://github.com/tensorflow/models/tree/master/object_detection)\n\n**Understanding SSD MultiBox — Real-Time Object Detection In Deep Learning**\n\n[https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab)\n\n**One-shot object detection**\n\n[http://machinethink.net/blog/object-detection/](http://machinethink.net/blog/object-detection/)\n\n**An overview of object detection: one-stage methods**\n\n[https://www.jeremyjordan.me/object-detection-one-stage/](https://www.jeremyjordan.me/object-detection-one-stage/)\n\n**deep learning object detection**\n\n- intro: A paper list of object detection using deep learning.\n- arxiv: [https://github.com/hoya012/deep_learning_object_detection](https://github.com/hoya012/deep_learning_object_detection)\n"},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","title":"Segmentation"},"frontmatter":{"draft":false},"rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Segmentation\ndate: 2015-10-09\n---\n\n# Papers\n\n**Deep Joint Task Learning for Generic Object Extraction**\n\n- intro: NIPS 2014\n- homepage: [http://vision.sysu.edu.cn/projects/deep-joint-task-learning/](http://vision.sysu.edu.cn/projects/deep-joint-task-learning/)\n- paper: [http://ss.sysu.edu.cn/~ll/files/NIPS2014_JointTask.pdf](http://ss.sysu.edu.cn/~ll/files/NIPS2014_JointTask.pdf)\n- github: [https://github.com/xiaolonw/nips14_loc_seg_testonly](https://github.com/xiaolonw/nips14_loc_seg_testonly)\n- dataset: [http://objectextraction.github.io/](http://objectextraction.github.io/)\n\n**Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification**\n\n- arxiv: [https://arxiv.org/abs/1412.4526](https://arxiv.org/abs/1412.4526)\n- code(Caffe): [https://dl.dropboxusercontent.com/u/6448899/caffe.zip](https://dl.dropboxusercontent.com/u/6448899/caffe.zip)\n- author page: [http://www.ee.cuhk.edu.hk/~hsli/](http://www.ee.cuhk.edu.hk/~hsli/)\n\n**Segmentation from Natural Language Expressions**\n\n- intro: ECCV 2016\n- project page: [http://ronghanghu.com/text_objseg/](http://ronghanghu.com/text_objseg/)\n- arxiv: [http://arxiv.org/abs/1603.06180](http://arxiv.org/abs/1603.06180)\n- github(TensorFlow): [https://github.com/ronghanghu/text_objseg](https://github.com/ronghanghu/text_objseg)\n- gtihub(Caffe): [https://github.com/Seth-Park/text_objseg_caffe](https://github.com/Seth-Park/text_objseg_caffe)\n\n**Semantic Object Parsing with Graph LSTM**\n\n- arxiv: [http://arxiv.org/abs/1603.07063](http://arxiv.org/abs/1603.07063)\n\n**Fine Hand Segmentation using Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1608.07454](http://arxiv.org/abs/1608.07454)\n\n**Feedback Neural Network for Weakly Supervised Geo-Semantic Segmentation**\n\n- intro: Facebook Connectivity Lab & Facebook Core Data Science & University of Illinois\n- arxiv: [https://arxiv.org/abs/1612.02766](https://arxiv.org/abs/1612.02766)\n\n**FusionNet: A deep fully residual convolutional neural network for image segmentation in connectomics**\n\n- arxiv: [https://arxiv.org/abs/1612.05360](https://arxiv.org/abs/1612.05360)\n\n**A deep learning model integrating FCNNs and CRFs for brain tumor segmentation**\n\n- arxiv: [https://arxiv.org/abs/1702.04528](https://arxiv.org/abs/1702.04528)\n\n**Texture segmentation with Fully Convolutional Networks**\n\n- intro: Dublin City University\n- arxiv: [https://arxiv.org/abs/1703.05230](https://arxiv.org/abs/1703.05230)\n\n**Fast LIDAR-based Road Detection Using Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1703.03613](https://arxiv.org/abs/1703.03613)\n\n**Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs**\n\n- arxiv: [https://arxiv.org/abs/1703.04363](https://arxiv.org/abs/1703.04363)\n- demo: [https://gyglim.github.io/deep-value-net/](https://gyglim.github.io/deep-value-net/)\n\n**Annotating Object Instances with a Polygon-RNN**\n\n- intro: CVPR 2017. CVPR Best Paper Honorable Mention Award\n- intro: University of Toronto\n- keywords: PolygonRNN\n- project page: [http://www.cs.toronto.edu/polyrnn/](http://www.cs.toronto.edu/polyrnn/)\n- arxiv: [https://arxiv.org/abs/1704.05548](https://arxiv.org/abs/1704.05548)\n\n**Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++**\n\n- intro: CVPR 2018\n- keywords: PolygonRNN++\n- project page: [http://www.cs.toronto.edu/polyrnn/](http://www.cs.toronto.edu/polyrnn/)\n- arxiv: [https://arxiv.org/abs/1803.09693](https://arxiv.org/abs/1803.09693)\n- github: [https://github.com/davidjesusacu/polyrnn-pp](https://github.com/davidjesusacu/polyrnn-pp)\n\n**Semantic Segmentation via Structured Patch Prediction, Context CRF and Guidance CRF**\n\n- intro: CVPR 2017\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Semantic_Segmentation_via_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Semantic_Segmentation_via_CVPR_2017_paper.pdf)\n- github(Caffe): [https://github.com//FalongShen/SegModel](https://github.com//FalongShen/SegModel)\n\n**Distantly Supervised Road Segmentation**\n\n- intro: ICCV workshop CVRSUAD2017. Indiana University & Preferred Networks\n- arxiv: [https://arxiv.org/abs/1708.06118](https://arxiv.org/abs/1708.06118)\n\n**Ω-Net: Fully Automatic, Multi-View Cardiac MR Detection, Orientation, and Segmentation with Deep Neural Networks**\n\n**Ω-Net (Omega-Net): Fully Automatic, Multi-View Cardiac MR Detection, Orientation, and Segmentation with Deep Neural Networks**\n\n[https://arxiv.org/abs/1711.01094](https://arxiv.org/abs/1711.01094)\n\n**Superpixel clustering with deep features for unsupervised road segmentation**\n\n- intro: Preferred Networks, Inc & Indiana University\n- arxiv: [https://arxiv.org/abs/1711.05998](https://arxiv.org/abs/1711.05998)\n\n**Learning to Segment Human by Watching YouTube**\n\n- intro: TPAMI 2017\n- arxiv: [https://arxiv.org/abs/1710.01457](https://arxiv.org/abs/1710.01457)\n\n**W-Net: A Deep Model for Fully Unsupervised Image Segmentation**\n\n[https://arxiv.org/abs/1711.08506](https://arxiv.org/abs/1711.08506)\n\n**End-to-end detection-segmentation network with ROI convolution**\n\n- intro: ISBI 2018\n- arxiv: [https://arxiv.org/abs/1801.02722](https://arxiv.org/abs/1801.02722)\n\n**A Foreground Inference Network for Video Surveillance Using Multi-View Receptive Field**\n\n[https://arxiv.org/abs/1801.06593](https://arxiv.org/abs/1801.06593)\n\n**Piecewise Flat Embedding for Image Segmentation**\n\n[https://arxiv.org/abs/1802.03248](https://arxiv.org/abs/1802.03248)\n\n**A Pyramid CNN for Dense-Leaves Segmentation**\n\n- intro: Computer and Robot Vision, Toronto, May 2018\n- arxiv: [https://arxiv.org/abs/1804.01646](https://arxiv.org/abs/1804.01646)\n\n**Capsules for Object Segmentation**\n\n- keywords: convolutional-deconvolutional capsule network, SegCaps, U-Net\n- arxiv: [https://arxiv.org/abs/1804.04241](https://arxiv.org/abs/1804.04241)\n\n**Deep Object Co-Segmentation**\n\n[https://arxiv.org/abs/1804.06423](https://arxiv.org/abs/1804.06423)\n\n**Semantic Aware Attention Based Deep Object Co-segmentation**\n\n[https://arxiv.org/abs/1810.06859](https://arxiv.org/abs/1810.06859)\n\n**Contextual Hourglass Networks for Segmentation and Density Estimation**\n\n[https://arxiv.org/abs/1806.04009](https://arxiv.org/abs/1806.04009)\n\n## U-Net\n\n**U-Net: Convolutional Networks for Biomedical Image Segmentation**\n\n- intro: conditionally accepted at MICCAI 2015\n- project page: [http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n- arxiv: [http://arxiv.org/abs/1505.04597](http://arxiv.org/abs/1505.04597)\n- code+data: [http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz)\n- github: [https://github.com/orobix/retina-unet](https://github.com/orobix/retina-unet)\n- github: [https://github.com/jakeret/tf_unet](https://github.com/jakeret/tf_unet)\n- notes: [http://zongwei.leanote.com/post/Pa](http://zongwei.leanote.com/post/Pa)\n\n**UNet++: A Nested U-Net Architecture for Medical Image Segmentation**\n\n- intro: 4th Deep Learning in Medical Image Analysis (DLMIA) Workshop\n- arxiv: [https://arxiv.org/abs/1807.10165](https://arxiv.org/abs/1807.10165)\n\n**UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation**\n\n- intro: ICASSP 2020\n- arxiv: [https://arxiv.org/abs/2004.08790](https://arxiv.org/abs/2004.08790)\n- github: [https://github.com/ZJUGiveLab/UNet-Version](https://github.com/ZJUGiveLab/UNet-Version)\n\n**DeepUNet: A Deep Fully Convolutional Network for Pixel-level Sea-Land Segmentation**\n\n[https://arxiv.org/abs/1709.00201](https://arxiv.org/abs/1709.00201)\n\n**TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation**\n\n- intro: Lyft Inc. & MIT\n- intro: part of the winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge\n- arxiv: [https://arxiv.org/abs/1801.05746](https://arxiv.org/abs/1801.05746)\n- github: [https://github.com/ternaus/TernausNet](https://github.com/ternaus/TernausNet)\n\n**A Probabilistic U-Net for Segmentation of Ambiguous Images**\n\n- intro: DeepMind & German Cancer Research Center\n- arxiv: [https://arxiv.org/abs/1806.05034](https://arxiv.org/abs/1806.05034)\n\n**Deep Dual Pyramid Network for Barcode Segmentation using Barcode-30k Database**\n\n[https://arxiv.org/abs/1807.11886](https://arxiv.org/abs/1807.11886)\n\n**Deep Smoke Segmentation**\n\n[https://arxiv.org/abs/1809.00774](https://arxiv.org/abs/1809.00774)\n\n**Smoothed Dilated Convolutions for Improved Dense Prediction**\n\n- intro: KDD 2018\n- arxiv: [https://arxiv.org/abs/1808.08931](https://arxiv.org/abs/1808.08931)\n- github: [https://github.com/divelab/dilated](https://github.com/divelab/dilated)\n\n**DASNet: Reducing Pixel-level Annotations for Instance and Semantic Segmentation**\n\n[https://arxiv.org/abs/1809.06013](https://arxiv.org/abs/1809.06013)\n\n**Improving Fast Segmentation With Teacher-student Learning**\n\n[https://arxiv.org/abs/1810.08476](https://arxiv.org/abs/1810.08476)\n\n**DSNet: An Efficient CNN for Road Scene Segmentation**\n\n[https://arxiv.org/abs/1904.05022](https://arxiv.org/abs/1904.05022)\n\n**Line Segment Detection Using Transformers without Edges**\n\n[https://arxiv.org/abs/2101.01909](https://arxiv.org/abs/2101.01909)\n\n# Unified Image Segmentation\n\n**K-Net: Towards Unified Image Segmentation**\n\n- intro: NeurIPS 2021\n- intro:  Nanyang Technological University &  Chinese University of Hong Kon & SenseTime Research & Shanghai AI Laborator\n- project page: [https://www.mmlab-ntu.com/project/knet/index.html](https://www.mmlab-ntu.com/project/knet/index.html)\n- arxiv: [https://arxiv.org/abs/2106.14855](https://arxiv.org/abs/2106.14855)\n- github: [https://github.com/ZwwWayne/K-Net/](https://github.com/ZwwWayne/K-Net/)\n\n**Masked-attention Mask Transformer for Universal Image Segmentation**\n\n- project page: [https://bowenc0221.github.io/mask2former/](https://bowenc0221.github.io/mask2former/)\n- arxiv: [https://arxiv.org/abs/2112.01527](https://arxiv.org/abs/2112.01527)\n- github: [https://github.com/facebookresearch/Mask2Former](https://github.com/facebookresearch/Mask2Former)\n\n**Mask2Former for Video Instance Segmentation**\n\n- intro: University of Illinois at Urbana-Champaign (UIUC) & Facebook AI Research (FAIR\n- arxiv: [https://arxiv.org/abs/2112.10764](https://arxiv.org/abs/2112.10764)\n- github: [https://github.com/facebookresearch/Mask2Former](https://github.com/facebookresearch/Mask2Former)\n\n# Foreground Object Segmentation\n\n**Pixel Objectness**\n\n- project page: [http://vision.cs.utexas.edu/projects/pixelobjectness/](http://vision.cs.utexas.edu/projects/pixelobjectness/)\n- arxiv: [https://arxiv.org/abs/1701.05349](https://arxiv.org/abs/1701.05349)\n- github: [https://github.com/suyogduttjain/pixelobjectness](https://github.com/suyogduttjain/pixelobjectness)\n\n**A Deep Convolutional Neural Network for Background Subtraction**\n\n- arxiv: [https://arxiv.org/abs/1702.01731](https://arxiv.org/abs/1702.01731)\n\n**Learning Multi-scale Features for Foreground Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1808.01477](https://arxiv.org/abs/1808.01477)\n- github: [https://github.com/lim-anggun/FgSegNet_v2](https://github.com/lim-anggun/FgSegNet_v2)\n\n**Learning Deep Representations for Semantic Image Parsing: a Comprehensive Overview**\n\n[https://arxiv.org/abs/1810.04377](https://arxiv.org/abs/1810.04377)\n\n# Semantic Segmentation\n\n**Fully Convolutional Networks for Semantic Segmentation**\n\n- intro: CVPR 2015, PAMI 2016\n- keywords: deconvolutional layer, crop layer\n- arxiv: [http://arxiv.org/abs/1411.4038](http://arxiv.org/abs/1411.4038)\n- arxiv(PAMI 2016): [http://arxiv.org/abs/1605.06211](http://arxiv.org/abs/1605.06211)\n- slides: [https://docs.google.com/presentation/d/1VeWFMpZ8XN7OC3URZP4WdXvOGYckoFWGVN7hApoXVnc](https://docs.google.com/presentation/d/1VeWFMpZ8XN7OC3URZP4WdXvOGYckoFWGVN7hApoXVnc)\n- slides: [http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-pixels.pdf](http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-pixels.pdf)\n- talk: [http://techtalks.tv/talks/fully-convolutional-networks-for-semantic-segmentation/61606/](http://techtalks.tv/talks/fully-convolutional-networks-for-semantic-segmentation/61606/)\n- github(official): [https://github.com/shelhamer/fcn.berkeleyvision.org](https://github.com/shelhamer/fcn.berkeleyvision.org)\n- github: [https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn](https://github.com/BVLC/caffe/wiki/Model-Zoo#fcn)\n- github: [https://github.com/MarvinTeichmann/tensorflow-fcn](https://github.com/MarvinTeichmann/tensorflow-fcn)\n- github(Chainer): [https://github.com/wkentaro/fcn](https://github.com/wkentaro/fcn)\n- github: [https://github.com/wkentaro/pytorch-fcn](https://github.com/wkentaro/pytorch-fcn)\n- github: [https://github.com/shekkizh/FCN.tensorflow](https://github.com/shekkizh/FCN.tensorflow)\n- notes: [http://zhangliliang.com/2014/11/28/paper-note-fcn-segment/](http://zhangliliang.com/2014/11/28/paper-note-fcn-segment/)\n\n**From Image-level to Pixel-level Labeling with Convolutional Networks**\n\n- intro: CVPR 2015\n- intro: \"Weakly Supervised Semantic Segmentation with Convolutional Networks\"\n- intro: performs semantic segmentation based only on image-level annotations in a multiple instance learning framework\n- arxiv: [http://arxiv.org/abs/1411.6228](http://arxiv.org/abs/1411.6228)\n- paper: [http://ronan.collobert.com/pub/matos/2015_semisupsemseg_cvpr.pdf](http://ronan.collobert.com/pub/matos/2015_semisupsemseg_cvpr.pdf)\n\n**Feedforward semantic segmentation with zoom-out features**\n\n- intro: CVPR 2015. Toyota Technological Institute at Chicago\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf)\n- bitbuckt: [https://bitbucket.org/m_mostajabi/zoom-out-release](https://bitbucket.org/m_mostajabi/zoom-out-release)\n- video: [https://www.youtube.com/watch?v=HvgvX1LXQa8](https://www.youtube.com/watch?v=HvgvX1LXQa8)\n\n## DeepLab\n\n**Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs**\n\n- intro: ICLR 2015. DeepLab\n- arxiv: [http://arxiv.org/abs/1412.7062](http://arxiv.org/abs/1412.7062)\n- bitbucket: [https://bitbucket.org/deeplab/deeplab-public/](https://bitbucket.org/deeplab/deeplab-public/)\n- github: [https://github.com/TheLegendAli/DeepLab-Context](https://github.com/TheLegendAli/DeepLab-Context)\n\n**Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation**\n\n- intro: DeepLab\n- arxiv: [http://arxiv.org/abs/1502.02734](http://arxiv.org/abs/1502.02734)\n- bitbucket: [https://bitbucket.org/deeplab/deeplab-public/](https://bitbucket.org/deeplab/deeplab-public/)\n- github: [https://github.com/TheLegendAli/DeepLab-Context](https://github.com/TheLegendAli/DeepLab-Context)\n\n## DeepLab v2\n\n**DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs**\n\n- intro: TPAMI\n- intro: 79.7% mIOU in the test set, PASCAL VOC-2012 semantic image segmentation task\n- intro: Updated version of our previous ICLR 2015 paper\n- project page: [http://liangchiehchen.com/projects/DeepLab.html](http://liangchiehchen.com/projects/DeepLab.html)\n- arxiv: [https://arxiv.org/abs/1606.00915](https://arxiv.org/abs/1606.00915)\n- bitbucket: [https://bitbucket.org/aquariusjay/deeplab-public-ver2](https://bitbucket.org/aquariusjay/deeplab-public-ver2)\n- github: [https://github.com/DrSleep/tensorflow-deeplab-resnet](https://github.com/DrSleep/tensorflow-deeplab-resnet)\n- github: [https://github.com/isht7/pytorch-deeplab-resnet](https://github.com/isht7/pytorch-deeplab-resnet)\n\n**DeepLabv2 (ResNet-101)**\n\n[http://liangchiehchen.com/projects/DeepLabv2_resnet.html](http://liangchiehchen.com/projects/DeepLabv2_resnet.html)\n\n## DeepLab v3\n\n**Rethinking Atrous Convolution for Semantic Image Segmentation**\n\n- intro: Google. DeepLabv3\n- arxiv: [https://arxiv.org/abs/1706.05587](https://arxiv.org/abs/1706.05587)\n\n## DeepLabv3+\n\n**Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation**\n\n- intro: Google Inc.\n- arxiv: [https://arxiv.org/abs/1802.02611](https://arxiv.org/abs/1802.02611)\n- github: [https://github.com/tensorflow/models/tree/master/research/deeplab](https://github.com/tensorflow/models/tree/master/research/deeplab)\n- blog: [https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html](https://research.googleblog.com/2018/03/semantic-image-segmentation-with.html)\n- github: [https://github.com/hualin95/Deeplab-v3plus](https://github.com/hualin95/Deeplab-v3plus)\n\n## DeeperLab\n\n**DeeperLab: Single-Shot Image Parser**\n\n- intro: MIT & Google Inc. & UC Berkeley\n- arxiv: [https://arxiv.org/abs/1902.05093](https://arxiv.org/abs/1902.05093)\n\n## Auto-DeepLab\n\n**Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation**\n\n- intro: CVPR 2019 oral\n- intro: Johns Hopkins University & Google & Stanford University\n- arxiv: [https://arxiv.org/abs/1901.02985](https://arxiv.org/abs/1901.02985)\n- github: [https://github.com/tensorflow/models/tree/master/research/deeplab](https://github.com/tensorflow/models/tree/master/research/deeplab)\n\n- - -\n\n**Conditional Random Fields as Recurrent Neural Networks**\n\n- intro: ICCV 2015\n- intro: Oxford / Stanford / Baidu\n- keywords: CRF-RNN\n- project page: [http://www.robots.ox.ac.uk/~szheng/CRFasRNN.html](http://www.robots.ox.ac.uk/~szheng/CRFasRNN.html)\n- arxiv: [http://arxiv.org/abs/1502.03240](http://arxiv.org/abs/1502.03240)\n- github: [https://github.com/torrvision/crfasrnn](https://github.com/torrvision/crfasrnn)\n- demo: [http://www.robots.ox.ac.uk/~szheng/crfasrnndemo](http://www.robots.ox.ac.uk/~szheng/crfasrnndemo)\n- github: [https://github.com/martinkersner/train-CRF-RNN](https://github.com/martinkersner/train-CRF-RNN)\n\n**BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1503.01640](http://arxiv.org/abs/1503.01640)\n\n**Efficient piecewise training of deep structured models for semantic segmentation**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1504.01013](http://arxiv.org/abs/1504.01013)\n\n**Learning Deconvolution Network for Semantic Segmentation**\n\n![](http://cvlab.postech.ac.kr/research/deconvnet/images/overall.png)\n\n- intro: ICCV 2015\n- intro: two-stage training: train the network with easy examples first and \nfine-tune the trained network with more challenging examples later\n- keywords: DeconvNet\n- project page: [http://cvlab.postech.ac.kr/research/deconvnet/](http://cvlab.postech.ac.kr/research/deconvnet/)\n- arxiv: [http://arxiv.org/abs/1505.04366](http://arxiv.org/abs/1505.04366)\n- slides: [http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w06-deconvnet.pdf](http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w06-deconvnet.pdf)\n- gitxiv: [http://gitxiv.com/posts/9tpJKNTYksN5eWcHz/learning-deconvolution-network-for-semantic-segmentation](http://gitxiv.com/posts/9tpJKNTYksN5eWcHz/learning-deconvolution-network-for-semantic-segmentation)\n- github: [https://github.com/HyeonwooNoh/DeconvNet](https://github.com/HyeonwooNoh/DeconvNet)\n- github: [https://github.com/HyeonwooNoh/caffe](https://github.com/HyeonwooNoh/caffe)\n\n## SegNet\n\n**SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling**\n\n- arxiv: [http://arxiv.org/abs/1505.07293](http://arxiv.org/abs/1505.07293)\n- github: [https://github.com/alexgkendall/caffe-segnet](https://github.com/alexgkendall/caffe-segnet)\n- github: [https://github.com/pfnet-research/chainer-segnet](https://github.com/pfnet-research/chainer-segnet)\n\n**SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation**\n\n![](http://mi.eng.cam.ac.uk/projects/segnet/images/segnet.png)\n\n- homepage: [http://mi.eng.cam.ac.uk/projects/segnet/](http://mi.eng.cam.ac.uk/projects/segnet/)\n- arxiv: [http://arxiv.org/abs/1511.00561](http://arxiv.org/abs/1511.00561)\n- github: [https://github.com/alexgkendall/caffe-segnet](https://github.com/alexgkendall/caffe-segnet)\n- tutorial: [http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html](http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html)\n\n**SegNet: Pixel-Wise Semantic Labelling Using a Deep Networks**\n\n- youtube: [https://www.youtube.com/watch?v=xfNYAly1iXo](https://www.youtube.com/watch?v=xfNYAly1iXo)\n- mirror: [http://pan.baidu.com/s/1gdUzDlD](http://pan.baidu.com/s/1gdUzDlD)\n\n**Getting Started with SegNet**\n\n- blog: [http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html](http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html)\n- github: [https://github.com/alexgkendall/SegNet-Tutorial](https://github.com/alexgkendall/SegNet-Tutorial)\n\n**ParseNet: Looking Wider to See Better**\n\n- intro:ICLR 2016\n- arxiv: [http://arxiv.org/abs/1506.04579](http://arxiv.org/abs/1506.04579)\n- github: [https://github.com/weiliu89/caffe/tree/fcn](https://github.com/weiliu89/caffe/tree/fcn)\n- caffe model zoo: [https://github.com/BVLC/caffe/wiki/Model-Zoo#parsenet-looking-wider-to-see-better](https://github.com/BVLC/caffe/wiki/Model-Zoo#parsenet-looking-wider-to-see-better)\n\n**Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation**\n\n- intro: ICLR 2016\n- keywords: DecoupledNet\n- project(paper+code): [http://cvlab.postech.ac.kr/research/decouplednet/](http://cvlab.postech.ac.kr/research/decouplednet/)\n- arxiv: [http://arxiv.org/abs/1506.04924](http://arxiv.org/abs/1506.04924)\n- github: [https://github.com/HyeonwooNoh/DecoupledNet](https://github.com/HyeonwooNoh/DecoupledNet)\n\n**Semantic Image Segmentation via Deep Parsing Network**\n\n- intro: ICCV 2015. CUHK\n- keywords: Deep Parsing Network (DPN), Markov Random Field (MRF)\n- homepage: [http://personal.ie.cuhk.edu.hk/~lz013/projects/DPN.html](http://personal.ie.cuhk.edu.hk/~lz013/projects/DPN.html)\n- arxiv.org: [http://arxiv.org/abs/1509.02634](http://arxiv.org/abs/1509.02634)\n- paper: [http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liu_Semantic_Image_Segmentation_ICCV_2015_paper.pdf](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liu_Semantic_Image_Segmentation_ICCV_2015_paper.pdf)\n- slides: [http://personal.ie.cuhk.edu.hk/~pluo/pdf/presentation_dpn.pdf](http://personal.ie.cuhk.edu.hk/~pluo/pdf/presentation_dpn.pdf)\n\n**Multi-Scale Context Aggregation by Dilated Convolutions**\n\n- intro: ICLR 2016.\n- intro: Dilated Convolution for Semantic Image Segmentation\n- homepage: [http://vladlen.info/publications/multi-scale-context-aggregation-by-dilated-convolutions/](http://vladlen.info/publications/multi-scale-context-aggregation-by-dilated-convolutions/)\n- arxiv: [http://arxiv.org/abs/1511.07122](http://arxiv.org/abs/1511.07122)\n- github: [https://github.com/fyu/dilation](https://github.com/fyu/dilation)\n- github: [https://github.com/nicolov/segmentation_keras](https://github.com/nicolov/segmentation_keras)\n- notes: [http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/](http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/)\n\n**Instance-aware Semantic Segmentation via Multi-task Network Cascades**\n\n- intro: CVPR 2016 oral. 1st-place winner of MS COCO 2015 segmentation competition\n- keywords: RoI warping layer, Multi-task Network Cascades (MNC)\n- arxiv: [http://arxiv.org/abs/1512.04412](http://arxiv.org/abs/1512.04412)\n- github: [https://github.com/daijifeng001/MNC](https://github.com/daijifeng001/MNC)\n\n**Object Segmentation on SpaceNet via Multi-task Network Cascades (MNC)**\n\n- blog: [https://medium.com/the-downlinq/object-segmentation-on-spacenet-via-multi-task-network-cascades-mnc-f1c89d790b42](https://medium.com/the-downlinq/object-segmentation-on-spacenet-via-multi-task-network-cascades-mnc-f1c89d790b42)\n- github: [https://github.com/lncohn/pascal_to_spacenet](https://github.com/lncohn/pascal_to_spacenet)\n\n**Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network**\n\n![](http://cvlab.postech.ac.kr/research/transfernet/images/architecture.png)\n\n- intro: TransferNet\n- project page: [http://cvlab.postech.ac.kr/research/transfernet/](http://cvlab.postech.ac.kr/research/transfernet/)\n- arxiv: [http://arxiv.org/abs/1512.07928](http://arxiv.org/abs/1512.07928)\n- github: [https://github.com/maga33/TransferNet](https://github.com/maga33/TransferNet)\n\n**Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1603.04871](http://arxiv.org/abs/1603.04871)\n\n**Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1603.06098](https://arxiv.org/abs/1603.06098)\n- github: [https://github.com/kolesman/SEC](https://github.com/kolesman/SEC)\n\n**ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation**\n\n- project page: [http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup/](http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup/)\n- arxiv: [http://arxiv.org/abs/1604.05144](http://arxiv.org/abs/1604.05144)\n\n**Laplacian Reconstruction and Refinement for Semantic Segmentation**\n\n**Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1605.02264](https://arxiv.org/abs/1605.02264)\n- paper: [https://www.ics.uci.edu/~fowlkes/papers/gf-eccv16.pdf](https://www.ics.uci.edu/~fowlkes/papers/gf-eccv16.pdf)\n- github(MatConvNet): [https://github.com/golnazghiasi/LRR](https://github.com/golnazghiasi/LRR)\n\n**Natural Scene Image Segmentation Based on Multi-Layer Feature Extraction**\n\n- arxiv: [http://arxiv.org/abs/1605.07586](http://arxiv.org/abs/1605.07586)\n\n**Convolutional Random Walk Networks for Semantic Image Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1605.07681](http://arxiv.org/abs/1605.07681)\n\n**ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1606.02147](http://arxiv.org/abs/1606.02147)\n- github: [https://github.com/e-lab/ENet-training](https://github.com/e-lab/ENet-training)\n- github(Caffe): [https://github.com/TimoSaemann/ENet](https://github.com/TimoSaemann/ENet)\n- github: [https://github.com/PavlosMelissinos/enet-keras](https://github.com/PavlosMelissinos/enet-keras)\n- github: [https://github.com/kwotsin/TensorFlow-ENet](https://github.com/kwotsin/TensorFlow-ENet)\n- blog: [http://culurciello.github.io/tech/2016/06/20/training-enet.html](http://culurciello.github.io/tech/2016/06/20/training-enet.html)\n\n**Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery**\n\n- arxiv: [http://arxiv.org/abs/1606.02585](http://arxiv.org/abs/1606.02585)\n\n**Deep Learning Markov Random Field for Semantic Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1606.07230](http://arxiv.org/abs/1606.07230)\n\n**Region-based semantic segmentation with end-to-end training**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1607.07671](http://arxiv.org/abs/1607.07671)\n- githun: [https://github.com/nightrome/matconvnet-calvin](https://github.com/nightrome/matconvnet-calvin)\n\n**Built-in Foreground/Background Prior for Weakly-Supervised Semantic Segmentation**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1609.00446](http://arxiv.org/abs/1609.00446)\n\n**PixelNet: Towards a General Pixel-level Architecture**\n\n- intro: semantic segmentation, edge detection\n- arxiv: [http://arxiv.org/abs/1609.06694](http://arxiv.org/abs/1609.06694)\n\n**Exploiting Depth from Single Monocular Images for Object Detection and Semantic Segmentation**\n\n- intro: IEEE T. Image Processing\n- intro: propose an RGB-D semantic segmentation method which applies a multi-task training scheme: semantic label prediction and depth value regression\n- arxiv: [https://arxiv.org/abs/1610.01706](https://arxiv.org/abs/1610.01706)\n\n**PixelNet: Representation of the pixels, by the pixels, and for the pixels**\n\n- intro: CMU & Adobe Research\n- project page: [http://www.cs.cmu.edu/~aayushb/pixelNet/](http://www.cs.cmu.edu/~aayushb/pixelNet/)\n- arxiv: [https://arxiv.org/abs/1702.06506](https://arxiv.org/abs/1702.06506)\n- github(Caffe): [https://github.com/aayushbansal/PixelNet](https://github.com/aayushbansal/PixelNet)\n\n**Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks**\n\n- arxiv: [http://arxiv.org/abs/1609.06846](http://arxiv.org/abs/1609.06846)\n\n**Deep Structured Features for Semantic Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1609.07916](http://arxiv.org/abs/1609.07916)\n\n**CNN-aware Binary Map for General Semantic Segmentation**\n\n- intro: ICIP 2016 Best Paper / Student Paper Finalist\n- arxiv: [https://arxiv.org/abs/1609.09220](https://arxiv.org/abs/1609.09220)\n\n**Efficient Convolutional Neural Network with Binary Quantization Layer**\n\n- arxiv: [https://arxiv.org/abs/1611.06764](https://arxiv.org/abs/1611.06764)\n\n**Mixed context networks for semantic segmentation**\n\n- intro: Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1610.05854](https://arxiv.org/abs/1610.05854)\n\n**High-Resolution Semantic Labeling with Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.01962](https://arxiv.org/abs/1611.01962)\n\n**Gated Feedback Refinement Network for Dense Image Labeling**\n\n- intro: CVPR 2017\n- paper: [http://www.cs.umanitoba.ca/~ywang/papers/cvpr17.pdf](http://www.cs.umanitoba.ca/~ywang/papers/cvpr17.pdf)\n\n**RefineNet: Multi-Path Refinement Networks with Identity Mappings for High-Resolution Semantic Segmentation**\n\n**RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation**\n\n- intro: CVPR 2017. IoU 83.4% on PASCAL VOC 2012\n- arxiv: [https://arxiv.org/abs/1611.06612](https://arxiv.org/abs/1611.06612)\n- github: [https://github.com/guosheng/refinenet](https://github.com/guosheng/refinenet)\n- leaderboard: [http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6#KEY_Multipath-RefineNet-Res152](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6#KEY_Multipath-RefineNet-Res152)\n\n**Light-Weight RefineNet for Real-Time Semantic Segmentation**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1810.03272](https://arxiv.org/abs/1810.03272)\n- github: [https://github.com/drsleep/light-weight-refinenet](https://github.com/drsleep/light-weight-refinenet)\n\n**Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes**\n\n- keywords: Full-Resolution Residual Units (FRRU), Full-Resolution Residual Networks (FRRNs)\n- arxiv: [https://arxiv.org/abs/1611.08323](https://arxiv.org/abs/1611.08323)\n- github(Theano/Lasagne): [https://github.com/TobyPDE/FRRN](https://github.com/TobyPDE/FRRN)\n- youtube: [https://www.youtube.com/watch?v=PNzQ4PNZSzc](https://www.youtube.com/watch?v=PNzQ4PNZSzc)\n\n**Semantic Segmentation using Adversarial Networks**\n\n- intro: Facebook AI Research & INRIA. NIPS Workshop on Adversarial Training, Dec 2016, Barcelona, Spain\n- arxiv: [https://arxiv.org/abs/1611.08408](https://arxiv.org/abs/1611.08408)\n- github(Chainer): [https://github.com/oyam/Semantic-Segmentation-using-Adversarial-Networks](https://github.com/oyam/Semantic-Segmentation-using-Adversarial-Networks)\n\n**Improving Fully Convolution Network for Semantic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1611.08986](https://arxiv.org/abs/1611.08986)\n\n**The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation**\n\n- intro: Montreal Institute for Learning Algorithms & Ecole Polytechnique de Montreal\n- arxiv: [https://arxiv.org/abs/1611.09326](https://arxiv.org/abs/1611.09326)\n- github: [https://github.com/SimJeg/FC-DenseNet](https://github.com/SimJeg/FC-DenseNet)\n- github: [https://github.com/titu1994/Fully-Connected-DenseNets-Semantic-Segmentation](https://github.com/titu1994/Fully-Connected-DenseNets-Semantic-Segmentation)\n- github(Keras): [https://github.com/0bserver07/One-Hundred-Layers-Tiramisu](https://github.com/0bserver07/One-Hundred-Layers-Tiramisu)\n\n**Training Bit Fully Convolutional Network for Fast Semantic Segmentation**\n\n- intro: Megvii\n- arxiv: [https://arxiv.org/abs/1612.00212](https://arxiv.org/abs/1612.00212)\n\n**Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection**\n\n- intro: \"an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation \nwith built-in awareness of semantically meaningful boundaries. \"\n- arxiv: [https://arxiv.org/abs/1612.01337](https://arxiv.org/abs/1612.01337)\n\n**Diverse Sampling for Self-Supervised Learning of Semantic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1612.01991](https://arxiv.org/abs/1612.01991)\n\n**Mining Pixels: Weakly Supervised Semantic Segmentation Using Image Labels**\n\n- intro: Nankai University & University of Oxford & NUS\n- arxiv: [https://arxiv.org/abs/1612.02101](https://arxiv.org/abs/1612.02101)\n\n**FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation**\n\n- arxiv: [https://arxiv.org/abs/1612.02649](https://arxiv.org/abs/1612.02649)\n\n**Understanding Convolution for Semantic Segmentation**\n\n- intro: UCSD & CMU & UIUC & TuSimple\n- arxiv: [https://arxiv.org/abs/1702.08502](https://arxiv.org/abs/1702.08502)\n- github(MXNet): [https://github.com/TuSimple/TuSimple-DUC]https://github.com/TuSimple/TuSimple-DUC\n- pretrained-models: [https://drive.google.com/drive/folders/0B72xLTlRb0SoREhISlhibFZTRmM](https://drive.google.com/drive/folders/0B72xLTlRb0SoREhISlhibFZTRmM)\n\n**Label Refinement Network for Coarse-to-Fine Semantic Segmentation**\n\n[https://www.arxiv.org/abs/1703.00551](https://www.arxiv.org/abs/1703.00551)\n\n**Predicting Deeper into the Future of Semantic Segmentation**\n\n- intro: Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1703.07684](https://arxiv.org/abs/1703.07684)\n\n**Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach**\n\n- intro: CVPR 2017 (oral)\n- keywords: Adversarial Erasing (AE)\n- arxiv: [https://arxiv.org/abs/1703.08448](https://arxiv.org/abs/1703.08448)\n\n**Guided Perturbations: Self Corrective Behavior in Convolutional Neural Networks**\n\n- intro: University of Maryland & GE Global Research Center\n- arxiv: [https://arxiv.org/abs/1703.07928](https://arxiv.org/abs/1703.07928)\n\n**Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via Deep Layer Cascade**\n\n- intro: CVPR 2017 spotlight paper\n- arxxiv: [https://arxiv.org/abs/1704.01344](https://arxiv.org/abs/1704.01344)\n\n**Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network**\n\n[https://arxiv.org/abs/1703.02719](https://arxiv.org/abs/1703.02719)\n\n**Loss Max-Pooling for Semantic Image Segmentation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.02966](https://arxiv.org/abs/1704.02966)\n\n**Reformulating Level Sets as Deep Recurrent Neural Network Approach to Semantic Segmentation**\n\n[https://arxiv.org/abs/1704.03593](https://arxiv.org/abs/1704.03593)\n\n**A Review on Deep Learning Techniques Applied to Semantic Segmentation**\n\n[https://arxiv.org/abs/1704.06857](https://arxiv.org/abs/1704.06857)\n\n**Joint Semantic and Motion Segmentation for dynamic scenes using Deep Convolutional Networks**\n\n- intro: [International Institute of Information Technology & Max Planck Institute For Intelligent Systems\n- arxiv: [https://arxiv.org/abs/1704.08331](https://arxiv.org/abs/1704.08331)\n\n**ICNet for Real-Time Semantic Segmentation on High-Resolution Images**\n\n- intro: CUHK & Sensetime\n- project page: [https://hszhao.github.io/projects/icnet/](https://hszhao.github.io/projects/icnet/)\n- arxiv: [https://arxiv.org/abs/1704.08545](https://arxiv.org/abs/1704.08545)\n- github: [https://github.com/hszhao/ICNet](https://github.com/hszhao/ICNet)\n- video: [https://www.youtube.com/watch?v=qWl9idsCuLQ](https://www.youtube.com/watch?v=qWl9idsCuLQ)\n\n**Feature Forwarding: Exploiting Encoder Representations for Efficient Semantic Segmentation**\n\n**LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation**\n\n- project page: [https://codeac29.github.io/projects/linknet/](https://codeac29.github.io/projects/linknet/)\n- arxiv: [https://arxiv.org/abs/1707.03718](https://arxiv.org/abs/1707.03718)\n- github: [https://github.com/e-lab/LinkNet](https://github.com/e-lab/LinkNet)\n\n**Pixel Deconvolutional Networks**\n\n- intro: Washington State University\n- arxiv: [https://arxiv.org/abs/1705.06820](https://arxiv.org/abs/1705.06820)\n\n**Incorporating Network Built-in Priors in Weakly-supervised Semantic Segmentation**\n\n- intro: IEEE TPAMI\n- arxiv: [https://arxiv.org/abs/1706.02189](https://arxiv.org/abs/1706.02189)\n\n**Deep Semantic Segmentation for Automated Driving: Taxonomy, Roadmap and Challenges**\n\n- intro: IEEE ITSC 2017\n- arxiv: [https://arxiv.org/abs/1707.02432](https://arxiv.org/abs/1707.02432)\n\n**Semantic Segmentation with Reverse Attention**\n\n- intro: BMVC 2017 oral. University of Southern California\n- arxiv: [https://arxiv.org/abs/1707.06426](https://arxiv.org/abs/1707.06426)\n\n**Stacked Deconvolutional Network for Semantic Segmentation**\n\n[https://arxiv.org/abs/1708.04943](https://arxiv.org/abs/1708.04943)\n\n**Learning Dilation Factors for Semantic Segmentation of Street Scenes**\n\n- intro: GCPR 2017\n- arxiv: [https://arxiv.org/abs/1709.01956](https://arxiv.org/abs/1709.01956)\n\n**A Self-aware Sampling Scheme to Efficiently Train Fully Convolutional Networks for Semantic Segmentation**\n\n[https://arxiv.org/abs/1709.02764](https://arxiv.org/abs/1709.02764)\n\n**One-Shot Learning for Semantic Segmentation**\n\n- intro: BMWC 2017\n- arcxiv: [https://arxiv.org/abs/1709.03410](https://arxiv.org/abs/1709.03410)\n- github: [https://github.com/lzzcd001/OSLSM](https://github.com/lzzcd001/OSLSM)\n\n**An Adaptive Sampling Scheme to Efficiently Train Fully Convolutional Networks for Semantic Segmentation**\n\n[https://arxiv.org/abs/1709.02764](https://arxiv.org/abs/1709.02764)\n\n**Semantic Segmentation from Limited Training Data**\n\n[https://arxiv.org/abs/1709.07665](https://arxiv.org/abs/1709.07665)\n\n**Unsupervised Domain Adaptation for Semantic Segmentation with GANs**\n\n[https://arxiv.org/abs/1711.06969](https://arxiv.org/abs/1711.06969)\n\n**Neuron-level Selective Context Aggregation for Scene Segmentation**\n\n[https://arxiv.org/abs/1711.08278](https://arxiv.org/abs/1711.08278)\n\n**Road Extraction by Deep Residual U-Net**\n\n[https://arxiv.org/abs/1711.10684](https://arxiv.org/abs/1711.10684)\n\n**Mix-and-Match Tuning for Self-Supervised Semantic Segmentation**\n\n- intro: AAAI 2018\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/M&M/](http://mmlab.ie.cuhk.edu.hk/projects/M&M/)\n- arxiv: [https://arxiv.org/abs/1712.00661](https://arxiv.org/abs/1712.00661)\n- github: [https://github.com/XiaohangZhan/mix-and-match/](https://github.com/XiaohangZhan/mix-and-match/)\n- github: [https://github.com//liuziwei7/mix-and-match](https://github.com//liuziwei7/mix-and-match)\n\n**Error Correction for Dense Semantic Image Labeling**\n\n[https://arxiv.org/abs/1712.03812](https://arxiv.org/abs/1712.03812)\n\n**Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions**\n\n[https://arxiv.org/abs/1801.01317](https://arxiv.org/abs/1801.01317)\n\n**RTSeg: Real-time Semantic Segmentation Comparative Study**\n\n- arxiv: [https://arxiv.org/abs/1803.02758](https://arxiv.org/abs/1803.02758)\n- github: [https://github.com/MSiam/TFSegmentation](https://github.com/MSiam/TFSegmentation)\n\n**ShuffleSeg: Real-time Semantic Segmentation Network**\n\n- intro: Cairo University\n- arxiv: [https://arxiv.org/abs/1803.03816](https://arxiv.org/abs/1803.03816)\n\n**Dynamic-structured Semantic Propagation Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.06067](https://arxiv.org/abs/1803.06067)\n\n**ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation**\n\n- project page: [https://sacmehta.github.io/ESPNet/](https://sacmehta.github.io/ESPNet/)\n- arxiv: [https://arxiv.org/abs/1803.06815](https://arxiv.org/abs/1803.06815)\n- github: [https://github.com/sacmehta/ESPNet](https://github.com/sacmehta/ESPNet)\n\n**Context Encoding for Semantic Segmentation**\n\n- intro: CVPR 2018\n- keywords: Synchronized Cross-GPU Batch Normalization\n- arxiv: [https://arxiv.org/abs/1803.08904](https://arxiv.org/abs/1803.08904)\n- github: [https://github.com/zhanghang1989/PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding)\n\n**Adaptive Affinity Field for Semantic Segmentation**\n\n- intro: UC Berkeley / ICSI\n- arxiv: [https://arxiv.org/abs/1803.10335](https://arxiv.org/abs/1803.10335)\n\n**Predicting Future Instance Segmentations by Forecasting Convolutional Features**\n\n- intro: Facebook AI Research & Univ. Grenoble Alpes\n- arxiv: [https://arxiv.org/abs/1803.11496](https://arxiv.org/abs/1803.11496)\n\n**Fully Convolutional Adaptation Networks for Semantic Segmentation**\n\n- intro: CVPR 2018, Rank 1 in Segmentation Track of Visual Domain Adaptation Challenge 2017\n- keywords: Fully Convolutional Adaptation Networks (FCAN), Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN)\n- arxiv: [https://arxiv.org/abs/1804.08286](https://arxiv.org/abs/1804.08286)\n\n**Learning a Discriminative Feature Network for Semantic Segmentation**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.09337](https://arxiv.org/abs/1804.09337)\n\n**Deep Representation Learning for Domain Adaptation of Semantic Image Segmentation**\n\n[https://arxiv.org/abs/1805.04141](https://arxiv.org/abs/1805.04141)\n\n**Convolutional CRFs for Semantic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1805.04777](https://arxiv.org/abs/1805.04777)\n- github: [https://github.com/MarvinTeichmann/ConvCRF](https://github.com/MarvinTeichmann/ConvCRF)\n\n**ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time**\n\n- intro: Toshiba Research\n- arxiv: [https://arxiv.org/abs/1805.04554](https://arxiv.org/abs/1805.04554)\n\n**DifNet: Semantic Segmentation by DiffusionNetworks**\n\n[https://arxiv.org/abs/1805.08015](https://arxiv.org/abs/1805.08015)\n\n**Pyramid Attention Network for Semantic Segmentation**\n\n[https://arxiv.org/abs/1805.10180](https://arxiv.org/abs/1805.10180)\n\n**Semantic Segmentation with Scarce Data**\n\n- intro: ICML 2018 Workshop\n- arxiv: [https://arxiv.org/abs/1807.00911](https://arxiv.org/abs/1807.00911)\n\n**Attention to Refine through Multi-Scales for Semantic Segmentation**\n\n[https://arxiv.org/abs/1807.02917](https://arxiv.org/abs/1807.02917)\n\n**Guided Upsampling Network for Real-Time Semantic Segmentation**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.07466](https://arxiv.org/abs/1807.07466)\n\n**Deep Learning for Semantic Segmentation on Minimal Hardware**\n\n- intro: RoboCup International Symposium 2018. University of Hertfordshire\n- arxiv: [https://arxiv.org/abs/1807.05597](https://arxiv.org/abs/1807.05597)\n\n**Future Semantic Segmentation with Convolutional LSTM**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.07946](https://arxiv.org/abs/1807.07946)\n\n**BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.00897](https://arxiv.org/abs/1808.00897)\n\n**Dual Attention Network for Scene Segmentation**\n\n[https://arxiv.org/abs/1809.02983](https://arxiv.org/abs/1809.02983)\n\n**Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations**\n\n[https://arxiv.org/abs/1809.04766](https://arxiv.org/abs/1809.04766)\n\n**Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic Segmentation**\n\n[https://arxiv.org/abs/1809.06323](https://arxiv.org/abs/1809.06323)\n\n**Semantic Image Segmentation by Scale-Adaptive Networks**\n\n- github(Caffe): [https://github.com/speedinghzl/Scale-Adaptive-Network](https://github.com/speedinghzl/Scale-Adaptive-Network)\n\n**Recurrent Iterative Gating Networks for Semantic Segmentation**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1811.08043](https://arxiv.org/abs/1811.08043)\n\n**CGNet: A Light-weight Context Guided Network for Semantic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1811.08201](https://arxiv.org/abs/1811.08201)\n- github: [https://github.com/wutianyiRosun/CGNet](https://github.com/wutianyiRosun/CGNet)\n\n**CCNet: Criss-Cross Attention for Semantic Segmentation**\n\n- intro: Huazhong University of Science and Technology & Horizon Robotics & University of Illinois at Urbana-Champaign\n- arxiv: [https://arxiv.org/abs/1811.11721](https://arxiv.org/abs/1811.11721)\n- github: [https://github.com/speedinghzl/CCNet](https://github.com/speedinghzl/CCNet)\n\n**ShelfNet for Real-time Semantic Segmentation**\n\n- intro: Yale University\n- arxiv: [https://arxiv.org/abs/1811.11254](https://arxiv.org/abs/1811.11254)\n- github: [https://github.com/juntang-zhuang/ShelfNet](https://github.com/juntang-zhuang/ShelfNet)\n\n**Improving Semantic Segmentation via Video Propagation and Label Relaxation**\n\n- intro: CVPR 2019 oral\n- arxiv: [https://arxiv.org/abs/1812.01593](https://arxiv.org/abs/1812.01593)\n- github: [https://github.com/NVIDIA/semantic-segmentation](https://github.com/NVIDIA/semantic-segmentation)\n\n**RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free**\n\n- arxiv: [https://arxiv.org/abs/1901.03353](https://arxiv.org/abs/1901.03353)\n- github: [https://github.com/chengyangfu/retinamask](https://github.com/chengyangfu/retinamask)\n\n**Fast-SCNN: Fast Semantic Segmentation Network**\n\n[https://arxiv.org/abs/1902.04502](https://arxiv.org/abs/1902.04502)\n\n**Structured Knowledge Distillation for Semantic Segmentation**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.04197](https://arxiv.org/abs/1903.04197)\n\n**In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images**\n\n- intro: CVPR 2019\n- intro: University of Zagreb\n- keywords: SwiftNet\n- arxiv: [https://arxiv.org/abs/1903.08469](https://arxiv.org/abs/1903.08469)\n- github: [https://github.com/orsic/swiftnet](https://github.com/orsic/swiftnet)\n\n**FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation**\n\n- intro: Chinese Academy of Sciences & Deepwise AI Lab\n- keywords: Joint Pyramid Upsampling (JPU)\n- project page: [http://wuhuikai.me/FastFCNProject/](http://wuhuikai.me/FastFCNProject/)\n- arxiv: [https://arxiv.org/abs/1903.11816](https://arxiv.org/abs/1903.11816)\n- github: [https://github.com/wuhuikai/FastFCN](https://github.com/wuhuikai/FastFCN)\n\n**Significance-aware Information Bottleneck for Domain Adaptive Semantic Segmentation**\n\n- intro: HUST & UTS\n- arxiv: [https://arxiv.org/abs/1904.00876](https://arxiv.org/abs/1904.00876)\n\n**GFF: Gated Fully Fusion for Semantic Segmentation**\n\n[https://arxiv.org/abs/1904.01803](https://arxiv.org/abs/1904.01803)\n\n**DADA: Depth-aware Domain Adaptation in Semantic Segmentation**\n\n[https://arxiv.org/abs/1904.01886](https://arxiv.org/abs/1904.01886)\n\n**DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation**\n\n- intro: Megvii Technology\n- arxiv: [https://arxiv.org/abs/1904.02216](https://arxiv.org/abs/1904.02216)\n\n**ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1906.09826](https://arxiv.org/abs/1906.09826)\n- github(official): [https://github.com/xiaoyufenfei/ESNet](https://github.com/xiaoyufenfei/ESNet)\n\n**Gated-SCNN: Gated Shape CNNs for Semantic Segmentation**\n\n- intro: NVIDIA & University of Waterloo & University of Toronto & Vector Institute\n- project page: [https://nv-tlabs.github.io/GSCNN/](https://nv-tlabs.github.io/GSCNN/)\n- arxiv: [https://arxiv.org/abs/1907.05740](https://arxiv.org/abs/1907.05740)\n\n**DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic Segmentation**\n\n- intro: BMVC 2019\n- arxiv: [https://arxiv.org/abs/1907.11830](https://arxiv.org/abs/1907.11830)\n\n**Dynamic Graph Message Passing Networks**\n\n- intro: CVPR 2020 oral\n- arxiv: [https://arxiv.org/abs/1908.06955](https://arxiv.org/abs/1908.06955)\n\n**Squeeze-and-Attention Networks for Semantic Segmentation**\n\n[https://arxiv.org/abs/1909.03402](https://arxiv.org/abs/1909.03402)\n\n**Global Aggregation then Local Distribution in Fully Convolutional Networks**\n\n- intro: BMVC 2019\n- arxiv: [https://arxiv.org/abs/1909.07229](https://arxiv.org/abs/1909.07229)\n- github: [https://github.com/lxtGH/GALD-Net](https://github.com/lxtGH/GALD-Net)\n\n**Graph-guided Architecture Search for Real-time Semantic Segmentation**\n\n[https://arxiv.org/abs/1909.06793](https://arxiv.org/abs/1909.06793)\n\n**Feature Pyramid Encoding Network for Real-time Semantic Segmentation**\n\n- intro: BMVC 2019\n- arxiv: [https://arxiv.org/abs/1909.08599](https://arxiv.org/abs/1909.08599)\n\n**ACFNet: Attentional Class Feature Network for Semantic Segmentation**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.09408](https://arxiv.org/abs/1909.09408)\n\n**Region Mutual Information Loss for Semantic Segmentation**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1910.12037](https://arxiv.org/abs/1910.12037)\n- github: [https://github.com/ZJULearning/RMI](https://github.com/ZJULearning/RMI)\n\n**Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1910.13049](https://arxiv.org/abs/1910.13049)\n- github: [https://github.com/RogerZhangzz/CAG_UDA](https://github.com/RogerZhangzz/CAG_UDA)\n\n**Efficacy of Pixel-Level OOD Detection for Semantic Segmentation**\n\n[https://arxiv.org/abs/1911.02897](https://arxiv.org/abs/1911.02897)\n\n**Location-aware Upsampling for Semantic Segmentation**\n\n- keywords: LaU\n- arxiv: [https://arxiv.org/abs/1911.05250](https://arxiv.org/abs/1911.05250)\n- github: [https://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation](https://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation)\n\n**FasterSeg: Searching for Faster Real-time Semantic Segmentation**\n\n- intro: ICLR 2020\n- intro: Texas A&M University & Horizon Robotics Inc.\n- arxiv: [https://arxiv.org/abs/1912.10917](https://arxiv.org/abs/1912.10917)\n\n**AlignSeg: Feature-Aligned Segmentation Networks**\n\n[https://arxiv.org/abs/2003.00872](https://arxiv.org/abs/2003.00872)\n\n**Deep Grouping Model for Unified Perceptual Parsing**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.11647](https://arxiv.org/abs/2003.11647)\n\n**Spatial Pyramid Based Graph Reasoning for Semantic Segmentation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.10211](https://arxiv.org/abs/2003.10211)\n\n**Learning Dynamic Routing for Semantic Segmentation**\n\n- intro: CVPR 2020 oral\n- arxiv: [https://arxiv.org/abs/2003.10401](https://arxiv.org/abs/2003.10401)\n- giihub(official): [https://github.com/yanwei-li/DynamicRouting](https://github.com/yanwei-li/DynamicRouting)\n\n**Learning to Predict Context-adaptive Convolution for Semantic Segmentation**\n\n[https://arxiv.org/abs/2004.08222](https://arxiv.org/abs/2004.08222)\n\n**Transferring and Regularizing Prediction for Semantic Segmentation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2006.06570](https://arxiv.org/abs/2006.06570)\n\n**Tensor Low-Rank Reconstruction for Semantic Segmentation**\n\n- intro: ECCV 2020\n- intro: Top-1 performance on PASCAL-VOC12\n- arxiv: [https://arxiv.org/abs/2008.00490](https://arxiv.org/abs/2008.00490)\n- github: [https://github.com/CWanli/RecoNet](https://github.com/CWanli/RecoNet)\n\n**Representative Graph Neural Network**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2008.05202](https://arxiv.org/abs/2008.05202)\n\n**EfficientFCN: Holistically-guided Decoding for Semantic Segmentation**\n\n[https://arxiv.org/abs/2008.10487](https://arxiv.org/abs/2008.10487)\n\n**Improving Semantic Segmentation via Decoupled Body and Edge Supervision**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.10035](https://arxiv.org/abs/2007.10035)\n- github: [https://github.com/lxtGH/DecoupleSegNets](https://github.com/lxtGH/DecoupleSegNets)\n\n**Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation**\n\n[https://arxiv.org/abs/2010.07930](https://arxiv.org/abs/2010.07930)\n\n**PseudoSeg: Designing Pseudo Labels for Semantic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/2010.09713](https://arxiv.org/abs/2010.09713)\n- github: [https://github.com/googleinterns/wss](https://github.com/googleinterns/wss)\n\n**Importance-Aware Semantic Segmentation in Self-Driving with Discrete Wasserstein Training**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/2010.12440](https://arxiv.org/abs/2010.12440)\n\n**Pixel-Level Cycle Association: A New Perspective for Domain Adaptive Semantic Segmentation**\n\n- intro: NeurIPS 2020 oral\n- arxiv: [https://arxiv.org/abs/2011.00147](https://arxiv.org/abs/2011.00147)\n- github: [https://github.com/kgl-prml/Pixel-Level-Cycle-Association](https://github.com/kgl-prml/Pixel-Level-Cycle-Association)\n\n**CABiNet: Efficient Context Aggregation Network for Low-Latency Semantic Segmentation**\n\n- intro: University of Twente\n- arxiv: [https://arxiv.org/abs/2011.00993](https://arxiv.org/abs/2011.00993)\n\n**SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation**\n\n[https://arxiv.org/abs/2011.12025](https://arxiv.org/abs/2011.12025)\n\n**Channel-wise Distillation for Semantic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/2011.13256](https://arxiv.org/abs/2011.13256)\n- github: [https://github.com/drilistbox](https://github.com/drilistbox)\n\n**BoxInst: High-Performance Instance Segmentation with Box Annotations**\n\n- intro: University of Adelaide\n- arxiv: [https://arxiv.org/abs/2012.02310](https://arxiv.org/abs/2012.02310)\n- github: [https://github.com/aim-uofa/AdelaiDet/](https://github.com/aim-uofa/AdelaiDet/)\n\n**Scaling Semantic Segmentation Beyond 1K Classes on a Single GPU**\n\n- arxiv: [https://arxiv.org/abs/2012.07489](https://arxiv.org/abs/2012.07489)\n- github: [https://github.com/shipra25jain/ESSNet](https://github.com/shipra25jain/ESSNet)\n\n**Cross-Domain Grouping and Alignment for Domain Adaptive Semantic Segmentation**\n\n- intro: AAAI 2021\n- arxiv: [https://arxiv.org/abs/2012.08226](https://arxiv.org/abs/2012.08226)\n\n**HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation**\n\n- intro: Facebook AI & Tel Aviv University\n- arxiv: [https://arxiv.org/abs/2012.11582](https://arxiv.org/abs/2012.11582)\n\n## SETR\n\n**Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers**\n\n- intro: CVPR 2021\n- intro: Fudan University & University of Oxford & University of Surrey & Tencent Youtu Lab & Facebook AI\n- project page: [https://fudan-zvg.github.io/SETR/](https://fudan-zvg.github.io/SETR/)\n- arxiv: [https://arxiv.org/abs/2012.15840](https://arxiv.org/abs/2012.15840)\n- github: [https://github.com/fudan-zvg/SETR](https://github.com/fudan-zvg/SETR)\n\n**Exploring Cross-Image Pixel Contrast for Semantic Segmentation**\n\n- intro: ICCV 2021 oral\n- intro: Computer Vision Lab, ETH Zurich & SenseTime Research\n- arxiv: [https://arxiv.org/abs/2101.11939](https://arxiv.org/abs/2101.11939)\n- github: [https://github.com/tfzhou/ContrastiveSeg](https://github.com/tfzhou/ContrastiveSeg)\n\n**Active Boundary Loss for Semantic Segmentation**\n\n[https://arxiv.org/abs/2102.02696](https://arxiv.org/abs/2102.02696)\n\n**Learning Statistical Texture for Semantic Segmentation**\n\n- intro: CVPR 2021\n- intro: Beihang University & SenseTime Research\n- arxiv: [https://arxiv.org/abs/2103.04133](https://arxiv.org/abs/2103.04133)\n\n**Cross-Dataset Collaborative Learning for Semantic Segmentation**\n\n- intro: CVPR 2021\n- intro: Xilinx Inc. & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/2103.11351](https://arxiv.org/abs/2103.11351)\n\n**Vision Transformers for Dense Prediction**\n\n- intro: Intel Labs\n- arxiv: [https://arxiv.org/abs/2103.13413](https://arxiv.org/abs/2103.13413)\n- github: [https://github.com/intel-isl/DPT](https://github.com/intel-isl/DPT)\n\n**InverseForm: A Loss Function for Structured Boundary-Aware Segmentation**\n\n- intro: CVPR 2021 oral\n- intro: Qualcomm AI Research\n- arxiv: [https://arxiv.org/abs/2104.02745](https://arxiv.org/abs/2104.02745)\n\n**Rethinking BiSeNet For Real-time Semantic Segmentation**\n\n- intro: Meituan\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.13188](https://arxiv.org/abs/2104.13188)\n- github: [https://github.com/MichaelFan01/STDC-Seg](https://github.com/MichaelFan01/STDC-Seg)\n\n**Segmenter: Transformer for Semantic Segmentation**\n\n- intro: Inria\n- arxiv: [https://arxiv.org/abs/2105.05633](https://arxiv.org/abs/2105.05633)\n- github: [https://github.com/rstrudel/segmenter](https://github.com/rstrudel/segmenter)\n\n**SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers**\n\n[https://arxiv.org/abs/2105.15203](https://arxiv.org/abs/2105.15203)\n\n**Per-Pixel Classification is Not All You Need for Semantic Segmentation**\n\n- keywords: UIUC & FAIR\n- project page: [https://bowenc0221.github.io/maskformer/](https://bowenc0221.github.io/maskformer/)\n- arxiv: [https://arxiv.org/abs/2107.06278](https://arxiv.org/abs/2107.06278)\n\n**A Unified Efficient Pyramid Transformer for Semantic Segmentation**\n\n- intro: School of Data Science, Fudan University & Amazon Web Services & University of California, Davis\n- arxiv: [https://arxiv.org/abs/2107.14209](https://arxiv.org/abs/2107.14209)\n\n**Deep Metric Learning for Open World Semantic Segmentation**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.04562](https://arxiv.org/abs/2108.04562)\n\n**Multi-Anchor Active Domain Adaptation for Semantic Segmentation**\n\n- intro: ICCV 2021 Oral\n- arxiv: [https://arxiv.org/abs/2108.08012](https://arxiv.org/abs/2108.08012)\n\n**Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation**\n\n- intro: ICCV 2021\n- intro: Indian Institute of Science & Google Research\n- project page: [https://sites.google.com/view/sfdaseg](https://sites.google.com/view/sfdaseg)\n- arxiv: [https://arxiv.org/abs/2108.11249](https://arxiv.org/abs/2108.11249)\n\n**HRFormer: High-Resolution Transformer for Dense Prediction**\n\n- intro: NeurIPS 2021\n- intro: University of Chinese Academy of Sciences & Institute of Computing Technology, CAS & Peking University & Microsoft Research Asia & Baidu\n- arxiv: [https://arxiv.org/abs/2110.09408](https://arxiv.org/abs/2110.09408)\n- github: [https://github.com/HRNet/HRFormer](https://github.com/HRNet/HRFormer)\n\n**Deep Hierarchical Semantic Segmentation**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.14335](https://arxiv.org/abs/2203.14335)\n- github: [https://github.com/0liliulei/HieraSeg](https://github.com/0liliulei/HieraSeg)\n\n**TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.05525](https://arxiv.org/abs/2204.05525)\n- github: [https://github.com/hustvl/TopFormer](https://github.com/hustvl/TopFormer)\n\n**Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation**\n\n- intro: The Hong Kong University of Science and Technology & Tsinghua University & International Digital Economy Academy (IDEA) & The Hong Kong University of Science and Technology (Guangzhou)\n- arxiv: [https://arxiv.org/abs/2206.02777](https://arxiv.org/abs/2206.02777)\n- github: [https://github.com/IDEACVR/MaskDINO](https://github.com/IDEACVR/MaskDINO)\n\n# Instance Segmentation\n\n**Simultaneous Detection and Segmentation**\n\n- intro: ECCV 2014\n- author: Bharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik\n- arxiv: [http://arxiv.org/abs/1407.1808](http://arxiv.org/abs/1407.1808)\n- github(Matlab): [https://github.com/bharath272/sds_eccv2014](https://github.com/bharath272/sds_eccv2014)\n\n**Convolutional Feature Masking for Joint Object and Stuff Segmentation**\n\n- intro: CVPR 2015\n- keywords: masking layers\n- arxiv: [https://arxiv.org/abs/1412.1283](https://arxiv.org/abs/1412.1283)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf)\n\n**Proposal-free Network for Instance-level Object Segmentation**\n\n- paper: [http://arxiv.org/abs/1509.02636](http://arxiv.org/abs/1509.02636)\n\n**Hypercolumns for object segmentation and fine-grained localization**\n\n- intro: CVPR 2015\n- arxiv: [https://arxiv.org/abs/1411.5752](https://arxiv.org/abs/1411.5752)\n- paper: [http://www.cs.berkeley.edu/~bharath2/pubs/pdfs/BharathCVPR2015.pdf](http://www.cs.berkeley.edu/~bharath2/pubs/pdfs/BharathCVPR2015.pdf)\n\n**SDS using hypercolumns**\n\n- github: [https://github.com/bharath272/sds](https://github.com/bharath272/sds)\n\n**Learning to decompose for object detection and instance segmentation**\n\n- intro: ICLR 2016 Workshop\n- keyword: CNN / RNN, MNIST, KITTI\n- arxiv: [http://arxiv.org/abs/1511.06449](http://arxiv.org/abs/1511.06449)\n\n**Recurrent Instance Segmentation**\n\n- intro: ECCV 2016\n- porject page: [http://romera-paredes.com/ris](http://romera-paredes.com/ris)\n- arxiv: [http://arxiv.org/abs/1511.08250](http://arxiv.org/abs/1511.08250)\n- github(Torch): [https://github.com/bernard24/ris](https://github.com/bernard24/ris)\n- poster: [http://www.eccv2016.org/files/posters/P-4B-46.pdf](http://www.eccv2016.org/files/posters/P-4B-46.pdf)\n- youtube: [https://www.youtube.com/watch?v=l_WD2OWOqBk](https://www.youtube.com/watch?v=l_WD2OWOqBk)\n\n**Instance-sensitive Fully Convolutional Networks**\n\n- intro: ECCV 2016. instance segment proposal\n- arxiv: [http://arxiv.org/abs/1603.08678](http://arxiv.org/abs/1603.08678)\n\n**Amodal Instance Segmentation**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1604.08202](http://arxiv.org/abs/1604.08202)\n\n**Bridging Category-level and Instance-level Semantic Image Segmentation**\n\n- keywords: online bootstrapping\n- arxiv: [http://arxiv.org/abs/1605.06885](http://arxiv.org/abs/1605.06885)\n\n**Bottom-up Instance Segmentation using Deep Higher-Order CRFs**\n\n- intro: BMVC 2016\n- arxiv: [http://arxiv.org/abs/1609.02583](http://arxiv.org/abs/1609.02583)\n\n**DeepCut: Object Segmentation from Bounding Box Annotations using Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1605.07866](http://arxiv.org/abs/1605.07866)\n\n**End-to-End Instance Segmentation and Counting with Recurrent Attention**\n\n- intro: ReInspect\n- arxiv: [http://arxiv.org/abs/1605.09410](http://arxiv.org/abs/1605.09410)\n\n**Translation-aware Fully Convolutional Instance Segmentation**\n\n**Fully Convolutional Instance-aware Semantic Segmentation**\n\n- intro:  CVPR 2017 Spotlight paper. winning entry of COCO segmentation challenge 2016\n- keywords:  TA-FCN / FCIS\n- arxiv: [https://arxiv.org/abs/1611.07709](https://arxiv.org/abs/1611.07709)\n- github: [https://github.com/msracver/FCIS](https://github.com/msracver/FCIS)\n- slides: [https://onedrive.live.com/?cid=f371d9563727b96f&id=F371D9563727B96F%2197213&authkey=%21AEYOyOirjIutSVk](https://onedrive.live.com/?cid=f371d9563727b96f&id=F371D9563727B96F%2197213&authkey=%21AEYOyOirjIutSVk)\n\n**InstanceCut: from Edges to Instances with MultiCut**\n\n- arxiv: [https://arxiv.org/abs/1611.08272](https://arxiv.org/abs/1611.08272)\n\n**Deep Watershed Transform for Instance Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1611.08303](https://arxiv.org/abs/1611.08303)\n\n**Object Detection Free Instance Segmentation With Labeling Transformations**\n\n- arxiv: [https://arxiv.org/abs/1611.08991](https://arxiv.org/abs/1611.08991)\n\n**Shape-aware Instance Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1612.03129](https://arxiv.org/abs/1612.03129)\n\n**Interpretable Structure-Evolving LSTM**\n\n- intro: CMU & Sun Yat-sen University & National University of Singapore & Adobe Research\n- intro: CVPR 2017 spotlight paper\n- arxiv: [https://arxiv.org/abs/1703.03055](https://arxiv.org/abs/1703.03055)\n\n**Mask R-CNN**\n\n- intro: ICCV 2017 Best paper award. Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)\n- slides: [http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf](http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf)\n- github(official, Caffe2): [https://github.com/facebookresearch/Detectron](https://github.com/facebookresearch/Detectron)\n- github: [https://github.com/facebookresearch/maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)\n- github: [https://github.com/TuSimple/mx-maskrcnn](https://github.com/TuSimple/mx-maskrcnn)\n- slides: [https://lmb.informatik.uni-freiburg.de/lectures/seminar_brox/seminar_ss17/maskrcnn_slides.pdf](https://lmb.informatik.uni-freiburg.de/lectures/seminar_brox/seminar_ss17/maskrcnn_slides.pdf)\n- github(Keras+TensorFlow): [https://github.com/matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN)\n\n**Faster Training of Mask R-CNN by Focusing on Instance Boundaries**\n\n- intro: BMW Car IT GmbH\n- arxiv: [https://arxiv.org/abs/1809.07069](https://arxiv.org/abs/1809.07069)\n\n**Boundary-preserving Mask R-CNN**\n\n- intro: ECCV 2020\n- intro: Huazhong University of Science and Technology & Horizon Robotics Inc.\n- arxiv: [https://arxiv.org/abs/2007.08921](https://arxiv.org/abs/2007.08921)\n- github: [https://github.com/hustvl/BMaskR-CNN](https://github.com/hustvl/BMaskR-CNN)\n\n**Semantic Instance Segmentation via Deep Metric Learning**\n\n[https://arxiv.org/abs/1703.10277](https://arxiv.org/abs/1703.10277)\n\n**Pose2Instance: Harnessing Keypoints for Person Instance Segmentation**\n\n[https://arxiv.org/abs/1704.01152](https://arxiv.org/abs/1704.01152)\n\n**Pixelwise Instance Segmentation with a Dynamically Instantiated Network**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.02386](https://arxiv.org/abs/1704.02386)\n\n**Instance-Level Salient Object Segmentation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.03604](https://arxiv.org/abs/1704.03604)\n\n**MEnet: A Metric Expression Network for Salient Object Segmentation**\n\n- intro: IJCAI\n- arxiv: [https://arxiv.org/abs/1805.05638](https://arxiv.org/abs/1805.05638)\n\n**Semantic Instance Segmentation with a Discriminative Loss Function**\n\n- intro: Published at \"Deep Learning for Robotic Vision\", workshop at CVPR 2017\n- arxiv: [https://arxiv.org/abs/1708.02551](https://arxiv.org/abs/1708.02551)\n- github: [https://github.com/Wizaron/instance-segmentation-pytorch](https://github.com/Wizaron/instance-segmentation-pytorch)\n\n**SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes**\n\n[https://arxiv.org/abs/1709.07158](https://arxiv.org/abs/1709.07158)\n\n**S4 Net: Single Stage Salient-Instance Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1711.07618](https://arxiv.org/abs/1711.07618)\n- github: [https://github.com/RuochenFan/S4Net](https://github.com/RuochenFan/S4Net)\n\n**Deep Extreme Cut: From Extreme Points to Object Segmentation**\n\n[https://arxiv.org/abs/1711.09081](https://arxiv.org/abs/1711.09081)\n\n**Learning to Segment Every Thing**\n\n- intro: CVPR 2018. UC Berkeley & Facebook AI Research\n- keywords: MaskX R-CNN\n- project page: [http://ronghanghu.com/seg_every_thing/](http://ronghanghu.com/seg_every_thing/)\n- arxiv: [https://arxiv.org/abs/1711.10370](https://arxiv.org/abs/1711.10370)\n- gihtub(official, Caffe2): [https://github.com/ronghanghu/seg_every_thing](https://github.com/ronghanghu/seg_every_thing)\n\n**Recurrent Neural Networks for Semantic Instance Segmentation**\n\n- project page: [https://imatge-upc.github.io/rsis/](https://imatge-upc.github.io/rsis/)\n- arxiv: [https://arxiv.org/abs/1712.00617](https://arxiv.org/abs/1712.00617)\n- github: [https://github.com/imatge-upc/rsis](https://github.com/imatge-upc/rsis)\n\n**MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features**\n\n- intro: Google Inc. & RWTH Aachen University & UCLA\n- arxiv: [https://arxiv.org/abs/1712.04837](https://arxiv.org/abs/1712.04837)\n\n**Recurrent Pixel Embedding for Instance Grouping**\n\n- intro: learning to embed pixels and group them into boundaries, object proposals, semantic segments and instances.\n- project page: [http://www.ics.uci.edu/~skong2/SMMMSG.html](http://www.ics.uci.edu/~skong2/SMMMSG.html)\n- arxiv: [https://arxiv.org/abs/1712.08273](https://arxiv.org/abs/1712.08273)\n- github: [https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping](https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping)\n- slides: [http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_public_version.pdf](http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_public_version.pdf)\n- poster: [http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_poster.pdf](http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_poster.pdf)\n\n**Annotation-Free and One-Shot Learning for Instance Segmentation of Homogeneous Object Clusters**\n\n[https://arxiv.org/abs/1802.00383](https://arxiv.org/abs/1802.00383)\n\n**Path Aggregation Network for Instance Segmentation**\n\n- intro: CVPR 2018 Spotlight\n- intro: CUHK & Peking University & SenseTime Research & YouTu Lab\n- keywords: PANet\n- arxiv: [https://arxiv.org/abs/1803.01534](https://arxiv.org/abs/1803.01534)\n- github: [https://github.com/ShuLiu1993/PANet](https://github.com/ShuLiu1993/PANet)\n\n**Learning to Segment via Cut-and-Paste**\n\n- intro: Google\n- keywords: weakly-supervised, adversarial learning setup\n- arxiv: [https://arxiv.org/abs/1803.06414](https://arxiv.org/abs/1803.06414)\n\n**Learning to Cluster for Proposal-Free Instance Segmentation**\n\n[https://arxiv.org/abs/1803.06459](https://arxiv.org/abs/1803.06459)\n\n**Bayesian Semantic Instance Segmentation in Open Set World**\n\n[https://arxiv.org/abs/1806.00911](https://arxiv.org/abs/1806.00911)\n\n**TernausNetV2: Fully Convolutional Network for Instance Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1806.00844](https://arxiv.org/abs/1806.00844)\n- github: [https://github.com/ternaus/TernausNetV2](https://github.com/ternaus/TernausNetV2)\n\n**Dynamic Multimodal Instance Segmentation guided by natural language queries**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.02257](https://arxiv.org/abs/1807.02257)\n- github: [https://github.com/andfoy/query-objseg](https://github.com/andfoy/query-objseg)\n\n**Traits & Transferability of Adversarial Examples against Instance Segmentation & Object Detection**\n\n[https://arxiv.org/abs/1808.01452](https://arxiv.org/abs/1808.01452)\n\n**Affinity Derivation and Graph Merge for Instance Segmentation**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1811.10870](https://arxiv.org/abs/1811.10870)\n\n**One-Shot Instance Segmentation**\n\n- intro: University of Tubingen\n- arxiv: [https://arxiv.org/abs/1811.11507](https://arxiv.org/abs/1811.11507)\n\n**Hybrid Task Cascade for Instance Segmentation**\n\n- intro: CVPR 2019\n- intro: The Chinese University of Hong Kong & SenseTime Research & Zhejiang University & The University of Sydney & Nanyang Technological University\n- intro: Winning entry of COCO 2018 Challenge (object detection task)\n- arxiv: [https://arxiv.org/abs/1901.07518](https://arxiv.org/abs/1901.07518)\n- github(mmdetection): [https://github.com/open-mmlab/mmdetection/tree/master/configs/htc](https://github.com/open-mmlab/mmdetection/tree/master/configs/htc)\n\n**Mask Scoring R-CNN**\n\n- intro: CVPR 2019\n- intro: Huazhong University of Science and Technology & Horizon Robotics Inc.\n- arxiv: [https://arxiv.org/abs/1903.00241](https://arxiv.org/abs/1903.00241)\n- github: [https://github.com/zjhuang22/maskscoring_rcnn](https://github.com/zjhuang22/maskscoring_rcnn)\n\n**TensorMask: A Foundation for Dense Object Segmentation**\n\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/1903.12174](https://arxiv.org/abs/1903.12174)\n\n**Actor-Critic Instance Segmentation**\n\n- intro: CVPR 2019\n- keywords: reinforcement learning\n- arxiv: [https://arxiv.org/abs/1904.05126](https://arxiv.org/abs/1904.05126)\n\n**Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth**\n\n- arxiv: [https://arxiv.org/abs/1906.11109](https://arxiv.org/abs/1906.11109)\n- github: [https://github.com/davyneven/SpatialEmbeddings](https://github.com/davyneven/SpatialEmbeddings)\n\n**InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.07801](https://arxiv.org/abs/1908.07801)\n- github: [https://github.com/GothicAi/Instaboost](https://github.com/GothicAi/Instaboost)\n\n**SSAP: Single-Shot Instance Segmentation With Affinity Pyramid**\n\n- intro: ICCV 2019\n- intro: Chinese Academy of Sciences & Horizon Robotics, Inc\n- arxiv: [https://arxiv.org/abs/1909.01616](https://arxiv.org/abs/1909.01616)\n\n**YOLACT: Real-time Instance Segmentation**\n\n- intro: You Only Look At CoefficienTs\n- intro: University of California, Davis\n- keywords: one-stage, Fast NMS\n- arxiv: [https://arxiv.org/abs/1904.02689](https://arxiv.org/abs/1904.02689)\n- github(official, Pytorch): [https://github.com/dbolya/yolact](https://github.com/dbolya/yolact)\n\n**YOLACT++: Better Real-time Instance Segmentation**\n\n[https://arxiv.org/abs/1912.06218](https://arxiv.org/abs/1912.06218)\n\n**YolactEdge: Real-time Instance Segmentation on the Edge**\n\n- arxiv: [https://arxiv.org/abs/2012.12259](https://arxiv.org/abs/2012.12259)\n- github: [https://github.com/haotian-liu/yolact_edge](https://github.com/haotian-liu/yolact_edge)\n\n**PolarMask: Single Shot Instance Segmentation with Polar Representation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/1909.13226](https://arxiv.org/abs/1909.13226)\n- github: [https://github.com/xieenze/PolarMask](https://github.com/xieenze/PolarMask)\n\n**PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond**\n\n- intro: TPAMI 2021\n- arxiv: [https://arxiv.org/abs/2105.02184](https://arxiv.org/abs/2105.02184)\n- github: [https://github.com/xieenze/PolarMask](https://github.com/xieenze/PolarMask)\n\n**CenterMask : Real-Time Anchor-Free Instance Segmentation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/1911.06667](https://arxiv.org/abs/1911.06667)\n- github: [https://github.com/youngwanLEE/CenterMask](https://github.com/youngwanLEE/CenterMask)\n- github: [https://github.com/youngwanLEE/centermask2](https://github.com/youngwanLEE/centermask2)\n\n**CenterMask: single shot instance segmentation with point representation**\n\n- intro: CVPR 2020\n- intro: Meituan Dianping Group\n- arxiv: [https://arxiv.org/abs/2004.04446](https://arxiv.org/abs/2004.04446)\n\n**Shape-aware Feature Extraction for Instance Segmentation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/1911.11263](https://arxiv.org/abs/1911.11263)\n\n**PolyTransform: Deep Polygon Transformer for Instance Segmentation**\n\n[https://arxiv.org/abs/1912.02801](https://arxiv.org/abs/1912.02801)\n\n**EmbedMask: Embedding Coupling for One-stage Instance Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1912.01954](https://arxiv.org/abs/1912.01954)\n- gitub: [https://github.com/yinghdb/EmbedMask](https://github.com/yinghdb/EmbedMask)\n\n**SAIS: Single-stage Anchor-free Instance Segmentation**\n\n[https://arxiv.org/abs/1912.01176](https://arxiv.org/abs/1912.01176)\n\n**SOLO: Segmenting Objects by Locations**\n\n- arxiv: [https://arxiv.org/abs/1912.04488](https://arxiv.org/abs/1912.04488)\n -github: [https://github.com/WXinlong/SOLO](https://github.com/WXinlong/SOLO)\n\n**SOLOv2: Dynamic, Faster and Stronger**\n\n- arxiv: [https://arxiv.org/abs/2003.10152](https://arxiv.org/abs/2003.10152)\n- github: [https://github.com/aim-uofa/AdelaiDet/](https://github.com/aim-uofa/AdelaiDet/)\n\n**SOLO: A Simple Framework for Instance Segmentation**\n\n- arxiv: [https://arxiv.org/abs/2106.15947](https://arxiv.org/abs/2106.15947)\n- github: [https://github.com/aim-uofa/AdelaiDet/](https://github.com/aim-uofa/AdelaiDet/)\n\n**RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation**\n\n- intro: AAAI 2020\n- intro: Chinese Academy of Sciences & 2Horizon Robotics Inc.\n- arxiv: [https://arxiv.org/abs/1912.05070](https://arxiv.org/abs/1912.05070)\n- github: [https://github.com/wangsr126/RDSNet](https://github.com/wangsr126/RDSNet)\n\n**BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation**\n\n[https://arxiv.org/abs/2001.00309](https://arxiv.org/abs/2001.00309)\n\n**Conditional Convolutions for Instance Segmentation**\n\n- intro: ECCV 2020 oral\n- intro: The University of Adelaide\n- arxiv: [https://arxiv.org/abs/2003.05664](https://arxiv.org/abs/2003.05664)\n- github: [https://github.com/aim-uofa/adet](https://github.com/aim-uofa/adet)\n\n**PointINS: Point-based Instance Segmentation**\n\n- intro: CUHK & MEGVII & Chinese Academy of Sciences & SmartMore\n- arxiv: [https://arxiv.org/abs/2003.06148](https://arxiv.org/abs/2003.06148)\n\n**1st Place Solutions for OpenImage2019 -- Object Detection and Instance Segmentation**\n\n[https://arxiv.org/abs/2003.07557](https://arxiv.org/abs/2003.07557)\n\n**Mask Encoding for Single Shot Instance Segmentation**\n\n- intro: CVPR 2020\n- intro:  Tongji University & University of Adelaide & Huawei Noah’s Ark Lab\n- arxiv: [https://arxiv.org/abs/2003.11712](https://arxiv.org/abs/2003.11712)\n\n**The Devil is in Classification: A Simple Framework for Long-tail Instance Segmentation**\n\n- arxiv: [https://arxiv.org/abs/2007.11978](https://arxiv.org/abs/2007.11978)\n- github: [https://github.com/twangnh/SimCal](https://github.com/twangnh/SimCal)\n\n**Deep Variational Instance Segmentation**\n\n[https://arxiv.org/abs/2007.11576](https://arxiv.org/abs/2007.11576)\n\n**Mask Point R-CNN**\n\n[https://arxiv.org/abs/2008.00460](https://arxiv.org/abs/2008.00460)\n\n**Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance Segmentation**\n\n- intro: ACM MM 2020\n- arxiv: [https://arxiv.org/abs/2008.05676](https://arxiv.org/abs/2008.05676)\n- github: [https://github.com/JialianW/Forest_RCNN](https://github.com/JialianW/Forest_RCNN)\n\n**Seesaw Loss for Long-Tailed Instance Segmentation**\n\n[https://arxiv.org/abs/2008.10032](https://arxiv.org/abs/2008.10032)\n\n**Joint COCO and Mapillary Workshop at ICCV 2019: COCO Instance Segmentation Challenge Track**\n\n- intro: 1st Place Technical Report in ICCV2019/ ECCV2020: MegDetV2\n- arxiv: [https://arxiv.org/abs/2010.02475](https://arxiv.org/abs/2010.02475)\n\n**DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation**\n\n- intro: Zhejiang University & Alibaba Group\n- arxiv: [https://arxiv.org/abs/2011.09876](https://arxiv.org/abs/2011.09876)\n\n**The Devil is in the Boundary: Exploiting Boundary Representation for Basis-based Instance Segmentation**\n\n[https://arxiv.org/abs/2011.13241](https://arxiv.org/abs/2011.13241)\n\n**Robust Instance Segmentation through Reasoning about Multi-Object Occlusion**\n\n[https://arxiv.org/abs/2012.02107](https://arxiv.org/abs/2012.02107)\n\n**Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation**\n\n- intro: Google Research & UC Berkeley & Cornell University\n- arxiv: [https://arxiv.org/abs/2012.07177](https://arxiv.org/abs/2012.07177)\n\n**How Shift Equivariance Impacts Metric Learning for Instance Segmentation**\n\n[https://arxiv.org/abs/2101.05846](https://arxiv.org/abs/2101.05846)\n\n**FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation**\n\n- intro: Nanyang Technological University & Carnegie Mellon Universit\n- arxiv: [https://arxiv.org/abs/2102.12867](https://arxiv.org/abs/2102.12867)\n\n**Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2103.12340](https://arxiv.org/abs/2103.12340)\n- github: [https://github.com/lkeab/BCNet](https://github.com/lkeab/BCNet)\n- youtube: [https://www.youtube.com/watch?v=iHlGJppJGiQ](https://www.youtube.com/watch?v=iHlGJppJGiQ)\n- zhihu: [https://zhuanlan.zhihu.com/p/378269087](https://zhuanlan.zhihu.com/p/378269087)\n\n**Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings**\n\n- arxiv: [https://arxiv.org/abs/2103.14572](https://arxiv.org/abs/2103.14572)\n- github: [https://github.com/kreshuklab/spoco](https://github.com/kreshuklab/spoco)\n\n**FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.00073](https://arxiv.org/abs/2104.00073)\n\n**ISTR: End-to-End Instance Segmentation with Transformers**\n\n- arxiv: [https://arxiv.org/abs/2105.00637](https://arxiv.org/abs/2105.00637)\n- github: [https://github.com/hujiecpp/ISTR](https://github.com/hujiecpp/ISTR)\n\n**Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers**\n\n- intro: CVPR 2021\n- intro: The Hong Kong University of Science and Technology & Kuaishou Technology\n- keywords: BCNet\n- arxiv: [https://arxiv.org/abs/2103.12340](https://arxiv.org/abs/2103.12340)\n- github: [https://github.com/lkeab/BCNet](https://github.com/lkeab/BCNet)\n\n**SOLQ: Segmenting Objects by Learning Queries**\n\n- intro: MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2106.02351](https://arxiv.org/abs/2106.02351)\n- github: [https://github.com/megvii-research/SOLQ](https://github.com/megvii-research/SOLQ)\n\n**1st Place Solution for YouTubeVOS Challenge 2021:Video Instance Segmentation**\n\n- intro: CPVR 2021 Workshop\n- arxiv: [https://arxiv.org/abs/2106.06649](https://arxiv.org/abs/2106.06649)\n\n**Rank & Sort Loss for Object Detection and Instance Segmentation**\n\n- intro: ICCV 2021 Oral\n- arxiv: [https://arxiv.org/abs/2107.11669](https://arxiv.org/abs/2107.11669)\n- github: [https://github.com/kemaloksuz/RankSortLoss](https://github.com/kemaloksuz/RankSortLoss)\n\n**SOTR: Segmenting Objects with Transformers**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.06747](https://arxiv.org/abs/2108.06747)\n- github: [https://github.com/easton-cau/SOTR](https://github.com/easton-cau/SOTR)\n\n**FaPN: Feature-aligned Pyramid Network for Dense Image Prediction**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.07058](https://arxiv.org/abs/2108.07058)\n- github: [https://github.com/EMI-Group/FaPN](https://github.com/EMI-Group/FaPN)\n\n**Instances as Queries**\n\n- intro: ICCV 2021\n- intro: HUST & Tencent\n- arxiv: [https://arxiv.org/abs/2105.01928](https://arxiv.org/abs/2105.01928)\n- github: [https://github.com/hustvl/QueryInst](https://github.com/hustvl/QueryInst)\n\n**Mask Transfiner for High-Quality Instance Segmentation**\n\n- intro: ETH Zurich & HKUST & Kuaishou Technology\n- arixv: [https://arxiv.org/abs/2111.13673](https://arxiv.org/abs/2111.13673)\n\n**SOIT: Segmenting Objects with Instance-Aware Transformers**\n\n- intro: AAAI 2022\n- arxiv: [https://arxiv.org/abs/2112.11037](https://arxiv.org/abs/2112.11037)\n- github: [https://github.com/yuxiaodongHRI/SOIT](https://github.com/yuxiaodongHRI/SOIT)\n\n**ContrastMask: Contrastive Learning to Segment Every Thing**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.09775](https://arxiv.org/abs/2203.09775)\n\n**Sparse Instance Activation for Real-Time Instance Segmentation**\n\n- intro CVPR 2022\n- intro: Huazhong University of Science & Technology & Horizon Robotics & CASIA\n- arxiv: [https://arxiv.org/abs/2203.12827](https://arxiv.org/abs/2203.12827)\n- github: [https://github.com/hustvl/SparseInst](https://github.com/hustvl/SparseInst)\n\n## Human Instance Segmentation\n\n**PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model**\n\n- intro: Google, Inc.\n- keywords: Person detection and pose estimation, segmentation and grouping\n- arxiv: [https://arxiv.org/abs/1803.08225](https://arxiv.org/abs/1803.08225)\n\n**Pose2Seg: Detection Free Human Instance Segmentation**\n\n- intro: CVPR 2019\n- intro: Tsinghua Unviersity & BNRist & Tencent AI Lab & Cardiff University\n- keywords: Occluded Human (OCHuman)\n- project page: [http://www.liruilong.cn/Pose2Seg/index.html](http://www.liruilong.cn/Pose2Seg/index.html)\n- arxiv: [https://arxiv.org/abs/1803.10683](https://arxiv.org/abs/1803.10683)\n- github: [https://github.com/liruilong940607/Pose2Seg](https://github.com/liruilong940607/Pose2Seg)\n- dataset: [https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman](https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman)\n\n**Bounding Box Embedding for Single Shot Person Instance Segmentation**\n\n[https://arxiv.org/abs/1807.07674](https://arxiv.org/abs/1807.07674)\n\n**Parsing R-CNN for Instance-Level Human Analysis**\n\n- intro: COCO 2018 DensePose Challenge Winner\n- arxiv: [https://arxiv.org/abs/1811.12596](https://arxiv.org/abs/1811.12596)\n- github: [https://github.com/soeaver/Parsing-R-CNN](https://github.com/soeaver/Parsing-R-CNN)\n\n**Graphonomy: Universal Human Parsing via Graph Transfer Learning**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.04536](https://arxiv.org/abs/1904.04536)\n- github: [https://github.com/Gaoyiminggithub/Graphonomy](https://github.com/Gaoyiminggithub/Graphonomy)\n\n## Video Instance Segmentation\n\n**SipMask: Spatial Information Preservation for Fast Image and Video Instance Segmentation**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.14772](https://arxiv.org/abs/2007.14772)\n- github: [https://github.com/JialeCao001/SipMask](https://github.com/JialeCao001/SipMask)\n\n**End-to-End Video Instance Segmentation with Transformers**\n\n- intro: Meituan & The University of Adelaide\n- arxiv: [https://arxiv.org/abs/2011.14503](https://arxiv.org/abs/2011.14503)\n\n**Spatial Feature Calibration and Temporal Fusion for Effective One-stage Video Instance Segmentation**\n\n- intro: CVPR 2021\n- intro: The HongKong Polytechnic University & DAMO Academy, Alibaba Group\n- arxiv: [https://arxiv.org/abs/2104.05606](https://arxiv.org/abs/2104.05606)\n- github: [https://github.com/MinghanLi/STMask](https://github.com/MinghanLi/STMask)\n\n**Tracking Instances as Queries**\n\n- intro: HUST & Tencent PCG\n- arxiv: [https://arxiv.org/abs/2106.11963](https://arxiv.org/abs/2106.11963)\n\n**Video Mask Transfiner for High-Quality Video Instance Segmentation**\n\n- intro: ECCV 2022\n- intro: ETH Z¨urich & The Hong Kong University of Science and Technology & Kuaishou Technology\n- arxiv: [https://arxiv.org/abs/2207.14012](https://arxiv.org/abs/2207.14012)\n\n# Panoptic Segmentation\n\n**Panoptic Segmentation**\n\n- intro: Facebook AI Research (FAIR) & Heidelberg University\n- arxiv: [https://arxiv.org/abs/1801.00868](https://arxiv.org/abs/1801.00868)\n- slides: [http://presentations.cocodataset.org/COCO17-Invited-PanopticAlexKirillov.pdf](http://presentations.cocodataset.org/COCO17-Invited-PanopticAlexKirillov.pdf)\n\n**Panoptic Segmentation with a Joint Semantic and Instance Segmentation Network**\n\n[https://arxiv.org/abs/1809.02110](https://arxiv.org/abs/1809.02110)\n\n**Learning to Fuse Things and Stuff**\n\n- intro: Toyota Research Institute (TRI)\n- keywords: TASCNet\n- arxiv: [https://arxiv.org/abs/1812.01192](https://arxiv.org/abs/1812.01192)\n\n**Attention-guided Unified Network for Panoptic Segmentation**\n\n- intro: CVPR 2019\n- intro: University of Chinese Academy of Sciences & Horizon Robotics, Inc. & The Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/1812.03904](https://arxiv.org/abs/1812.03904)\n\n**Panoptic Feature Pyramid Networks**\n\n- intro: FAIR\n- arxiv: [https://arxiv.org/abs/1901.02446](https://arxiv.org/abs/1901.02446)\n\n**UPSNet: A Unified Panoptic Segmentation Network**\n\n- intro: Uber ATG & University of Toronto & The Chinese University of Hong Kong\n- arxiv: [https://arxiv.org/abs/1901.03784](https://arxiv.org/abs/1901.03784)\n\n**Single Network Panoptic Segmentation for Street Scene Understanding**\n\n[https://arxiv.org/abs/1902.02678](https://arxiv.org/abs/1902.02678)\n\n**An End-to-End Network for Panoptic Segmentation**\n\n[https://arxiv.org/abs/1903.05027](https://arxiv.org/abs/1903.05027)\n\n**Learning Instance Occlusion for Panoptic Segmentation**\n\n[https://arxiv.org/abs/1906.05896](https://arxiv.org/abs/1906.05896)\n\n**SpatialFlow: Bridging All Tasks for Panoptic Segmentation**\n\n[https://arxiv.org/abs/1910.08787](https://arxiv.org/abs/1910.08787)\n\n**Single-Shot Panoptic Segmentation**\n\n[https://arxiv.org/abs/1911.00764](https://arxiv.org/abs/1911.00764)\n\n**SOGNet: Scene Overlap Graph Network for Panoptic Segmentation**\n\n- intro: AAAI 2020. Innovation Award in COCO 2019 challenge\n- arxiv: [https://arxiv.org/abs/1911.07527](https://arxiv.org/abs/1911.07527)\n\n**Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation**\n\n- intro: UIUC & Google Research\n- arxiv: [https://arxiv.org/abs/1911.10194](https://arxiv.org/abs/1911.10194)\n\n**PanDA: Panoptic Data Augmentation**\n\n[https://arxiv.org/abs/1911.12317](https://arxiv.org/abs/1911.12317)\n\n**Real-Time Panoptic Segmentation from Dense Detections**\n\n- intro: CVPR 2020 oral\n- arxiv: [https://arxiv.org/abs/1912.01202](https://arxiv.org/abs/1912.01202)\n- github: [https://github.com/TRI-ML/realtime_panoptic](https://github.com/TRI-ML/realtime_panoptic)\n\n**Bipartite Conditional Random Fields for Panoptic Segmentation**\n\n[https://arxiv.org/abs/1912.05307](https://arxiv.org/abs/1912.05307)\n\n**Unifying Training and Inference for Panoptic Segmentation**\n\n[https://arxiv.org/abs/2001.04982](https://arxiv.org/abs/2001.04982)\n\n**Towards Bounding-Box Free Panoptic Segmentation**\n\n- intro: SLAMcore Ltd. & Imperial College London\n- arxiv: [https://arxiv.org/abs/2002.07705](https://arxiv.org/abs/2002.07705)\n\n**A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI**\n\n- project page: [http://semantic-kitti.org/](http://semantic-kitti.org/)\n- arxiv: [https://arxiv.org/abs/2003.02371](https://arxiv.org/abs/2003.02371)\n\n**Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation**\n\n- intro: Johns Hopkins University & Google Research\n- arxiv: [https://arxiv.org/abs/2003.07853](https://arxiv.org/abs/2003.07853)\n\n**EPSNet: Efficient Panoptic Segmentation Network with Cross-layer Attention Fusion**\n\n[https://arxiv.org/abs/2003.10142](https://arxiv.org/abs/2003.10142)\n\n**Pixel Consensus Voting for Panoptic Segmentation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2004.01849](https://arxiv.org/abs/2004.01849)\n\n**EfficientPS: Efficient Panoptic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/2004.02307](https://arxiv.org/abs/2004.02307)\n- github: [https://github.com/DeepSceneSeg/EfficientPS](https://github.com/DeepSceneSeg/EfficientPS)\n\n**Video Panoptic Segmentation**\n\n- intro: CVPR 2020 Oral\n- intro: KAIST & Adobe Research\n- arxiv: [https://arxiv.org/abs/2006.11339](https://arxiv.org/abs/2006.11339)\n- github: [https://github.com/mcahny/vps](https://github.com/mcahny/vps)\n\n**PanoNet: Real-time Panoptic Segmentation through Position-Sensitive Feature Embedding**\n\n[https://arxiv.org/abs/2008.00192](https://arxiv.org/abs/2008.00192)\n\n**Robust Vision Challenge 2020 -- 1st Place Report for Panoptic Segmentation**\n\n[https://arxiv.org/abs/2008.10112](https://arxiv.org/abs/2008.10112)\n\n**Learning Category- and Instance-Aware Pixel Embedding for Fast Panoptic Segmentation**\n\n- intro: Chinese Academy of Sciences & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/2009.13342](https://arxiv.org/abs/2009.13342)\n\n**Auto-Panoptic: Cooperative Multi-Component Architecture Search for Panoptic Segmentation**\n\n- intro: NeurIPS 2020\n- intro: Sun Yat-sen University & Huawei Noah’s Ark Lab & DarkMatter AI Research\n- arxiv: [https://arxiv.org/abs/2010.16119](https://arxiv.org/abs/2010.16119)\n- github: [https://github.com/Jacobew/AutoPanoptic](https://github.com/Jacobew/AutoPanoptic)\n\n**Scaling Wide Residual Networks for Panoptic Segmentation**\n\n- intro: Google Research & Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/2011.11675](https://arxiv.org/abs/2011.11675)\n\n**Fully Convolutional Networks for Panoptic Segmentation**\n\n- intro: Chinese University of Hong Kong & University of Oxford & University of Hong Kong & MEGVII Technology4\n- arxiv: [https://arxiv.org/abs/2012.00720](https://arxiv.org/abs/2012.00720)\n- github: [https://github.com/yanwei-li/PanopticFCN](https://github.com/yanwei-li/PanopticFCN)\n\n**MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers**\n\n- intro: Johns Hopkins University & Google Research\n- arxiv: [https://arxiv.org/abs/2012.00759](https://arxiv.org/abs/2012.00759)\n\n**Ada-Segment: Automated Multi-loss Adaptation for Panoptic Segmentation**\n\n- intro: AAAI 2021\n- intro: Sun Yat-Sen University & Huawei Noah’s Ark Lab & Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/2012.03603](https://arxiv.org/abs/2012.03603)\n\n**ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation**\n\n- intro: Johns Hopkins University & Google Research\n- arxiv: [https://arxiv.org/abs/2012.05258](https://arxiv.org/abs/2012.05258)\n- github: [https://github.com/joe-siyuan-qiao/ViP-DeepLab](https://github.com/joe-siyuan-qiao/ViP-DeepLab)\n\n**STEP: Segmenting and Tracking Every Pixel**\n\n- intro: Technical University Munich & Google Research & RWTH Aachen University & MPI-IS and University of Tubingen\n- arxiv: [https://arxiv.org/abs/2102.11859](https://arxiv.org/abs/2102.11859)\n\n**Cross-View Regularization for Domain Adaptive Panoptic Segmentation**\n\n- intro: CVPR 2021 oral\n- arxiv: [https://arxiv.org/abs/2103.02584](https://arxiv.org/abs/2103.02584)\n\n**MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers**\n\n- intro: Johns Hopkins University & Google Research\n- arixv: [https://arxiv.org/abs/2012.00759](https://arxiv.org/abs/2012.00759)\n\n**Panoptic Segmentation Forecasting**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.03962](https://arxiv.org/abs/2104.03962)\n\n**Exemplar-Based Open-Set Panoptic Segmentation Network**\n\n- intro: CVPR 2021\n- intro: Seoul National University & Adobe Research\n- project page: [https://cv.snu.ac.kr/research/EOPSN/](https://cv.snu.ac.kr/research/EOPSN/)\n- arxiv: [https://arxiv.org/abs/2105.08336](https://arxiv.org/abs/2105.08336)\n- github: [https://github.com/jd730/EOPSN](https://github.com/jd730/EOPSN)\n\n**Hierarchical Lovász Embeddings for Proposal-free Panoptic Segmentation**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2106.04555](https://arxiv.org/abs/2106.04555)\n\nP**art-aware Panoptic Segmentation**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2106.06351](https://arxiv.org/abs/2106.06351)\n- github: [https://github.com/tue-mps/panoptic_parts](https://github.com/tue-mps/panoptic_parts)\n\n**Panoptic SegFormer**\n\n- intro: Nanjing University & The University of Hong Kong & NVIDIA & Caltech\n- arxiv: [https://arxiv.org/abs/2109.03814](https://arxiv.org/abs/2109.03814)\n\n**Slot-VPS: Object-centric Representation Learning for Video Panoptic Segmentation**\n\n- intro: Samsung Research China - Beijing (SRC-B) & 2Samsung Advanced Institute of Technology (SAIT) & University of Oxford & The University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2112.08949](https://arxiv.org/abs/2112.08949)\n\n**CFNet: Learning Correlation Functions for One-Stage Panoptic Segmentation**\n\n- intro: Zhejiang University & Tencent Youtu Lab & Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/2201.04796](https://arxiv.org/abs/2201.04796)\n\n**Panoptic, Instance and Semantic Relations: A Relational Context Encoder to Enhance Panoptic Segmentation**\n\n- intro: CVPR 2022\n- intro: Qualcomm AI Research\n- arxiv: [https://arxiv.org/abs/2204.05370](https://arxiv.org/abs/2204.05370)\n\n**PanopticDepth: A Unified Framework for Depth-aware Panoptic Segmentation**\n\n- intro: CVPR 2022\n- intro: Chinese Academy of Sciences & University of Chinese Academy of Sciences & Horizon Robotics, Inc.\n- arxiv: [https://arxiv.org/abs/2206.00468](https://arxiv.org/abs/2206.00468)\n\n**CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation**\n\n- intro: CVPR 2022 Oral\n- intro: Johns Hopkins University & KAIST & Google Research\n- arxiv: [https://arxiv.org/abs/2206.08948](https://arxiv.org/abs/2206.08948)\n\n**Uncertainty-aware Panoptic Segmentation**\n\n- intro: Technical University Nurnberg\n- arxiv: [https://arxiv.org/abs/2206.14554](https://arxiv.org/abs/2206.14554)\n\n**k-means Mask Transformer**\n\n- intro: ECCV 2022\n- intro: Johns Hopkins University & Google Research\n- arxiv: [https://arxiv.org/abs/2207.04044](https://arxiv.org/abs/2207.04044)\n- github: [https://github.com/google-research/deeplab2](https://github.com/google-research/deeplab2)\n\n# Nightime Segmentation\n\n**Nighttime sky/cloud image segmentation**\n\n- intro: ICIP 2017\n- arxiv: [https://arxiv.org/abs/1705.10583](https://arxiv.org/abs/1705.10583)\n\n**Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime**\n\n- intro: International Conference on Intelligent Transportation Systems (ITSC 2018)\n- arxiv: [https://arxiv.org/abs/1810.02575](https://arxiv.org/abs/1810.02575)\n\n**Semantic Nighttime Image Segmentation with Synthetic Stylized Data, Gradual Adaptation and Uncertainty-Aware Evaluation**\n\n**Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation**\n\n- intro: ICCV 2019\n- intro: ETH Zurich & KU Leuven\n- arxiv: [https://arxiv.org/abs/1901.05946](https://arxiv.org/abs/1901.05946)\n\n**Bi-Mix: Bidirectional Mixing for Domain Adaptive Nighttime Semantic Segmentation**\n\n- arxiv: [https://arxiv.org/abs/2111.10339](https://arxiv.org/abs/2111.10339)\n- github: [https://github.com/ygjwd12345/BiMix](https://github.com/ygjwd12345/BiMix)\n\n**DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation**\n\n- intro: CVPR 2021 oral\n- intro: University of South Carolina & Farsee2 Technology Ltd\n- arxiv: [https://arxiv.org/abs/2104.10834](https://arxiv.org/abs/2104.10834)\n- github: [https://github.com/W-zx-Y/DANNet](https://github.com/W-zx-Y/DANNet)\n\n**NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.05538](https://arxiv.org/abs/2204.05538)\n- github: [https://github.com/xdeng7/NightLab](https://github.com/xdeng7/NightLab)\n\n# Face Parsing\n\n**Face Parsing via Recurrent Propagation**\n\n- intro: BMVC 2017\n- arxiv: [https://arxiv.org/abs/1708.01936](https://arxiv.org/abs/1708.01936)\n\n**Face Parsing via a Fully-Convolutional Continuous CRF Neural Network**\n\n[https://arxiv.org/abs/1708.03736](https://arxiv.org/abs/1708.03736)\n\n**Face Parsing with RoI Tanh-Warping**\n\n- intro: Software School of Xiamen University & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1906.01342](https://arxiv.org/abs/1906.01342)\n\n**End-to-End Face Parsing via Interlinked Convolutional Neural Networks**\n\n[https://arxiv.org/abs/2002.04831](https://arxiv.org/abs/2002.04831)\n\n**RoI Tanh-polar Transformer Network for Face Parsing in the Wild**\n\n- arxiv: [https://arxiv.org/abs/2102.02717](https://arxiv.org/abs/2102.02717)\n- code: [https://ibug.doc.ic.ac.uk/resources/ibugmask/](https://ibug.doc.ic.ac.uk/resources/ibugmask/)\n\n**Decoupled Multi-task Learning with Cyclical Self-Regulation for Face Parsing**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.14448](https://arxiv.org/abs/2203.14448)\n- github: [https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr](https://github.com/deepinsight/insightface/tree/master/parsing/dml_csr)\n\n# Specific Segmentation\n\n**A CNN Cascade for Landmark Guided Semantic Part Segmentation**\n\n- project page: [http://aaronsplace.co.uk/](http://aaronsplace.co.uk/)\n- paper: [https://aaronsplace.co.uk/papers/jackson2016guided/jackson2016guided.pdf](https://aaronsplace.co.uk/papers/jackson2016guided/jackson2016guided.pdf)\n\n**End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks**\n\n- arxiv: [https://arxiv.org/abs/1703.03305](https://arxiv.org/abs/1703.03305)\n\n**Boundary-sensitive Network for Portrait Segmentation**\n\n[https://arxiv.org/abs/1712.08675](https://arxiv.org/abs/1712.08675)\n\n**Boundary-Aware Network for Fast and High-Accuracy Portrait Segmentation**\n\n- intro: Zhejiang University\n- arxiv: [https://arxiv.org/abs/1901.03814](https://arxiv.org/abs/1901.03814)\n\n**Beef Cattle Instance Segmentation Using Fully Convolutional Neural Network**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.01972](https://arxiv.org/abs/1807.01972)\n\n**Face Mask Extraction in Video Sequence**\n\n- keywords: ConvLSTM & FCN\n- arxiv: [https://arxiv.org/abs/1807.09207](https://arxiv.org/abs/1807.09207)\n\n# Segment Proposal\n\n**Learning to Segment Object Candidates**\n\n- intro: Facebook AI Research (FAIR)\n- intro: DeepMask. learning segmentation proposals\n- arxiv: [http://arxiv.org/abs/1506.06204](http://arxiv.org/abs/1506.06204)\n- github: [https://github.com/facebookresearch/deepmask](https://github.com/facebookresearch/deepmask)\n- github: [https://github.com/abbypa/NNProject_DeepMask](https://github.com/abbypa/NNProject_DeepMask)\n\n**Learning to Refine Object Segments**\n\n- intro: ECCV 2016. Facebook AI Research (FAIR)\n- intro: SharpMask. an extension of DeepMask which generates higher-fidelity masks using an additional top-down refinement step.\n- arxiv: [http://arxiv.org/abs/1603.08695](http://arxiv.org/abs/1603.08695)\n- github: [https://github.com/facebookresearch/deepmask](https://github.com/facebookresearch/deepmask)\n\n**FastMask: Segment Object Multi-scale Candidates in One Shot**\n\n- intro: CVPR 2017. University of California & Fudan University & Megvii Inc.\n- arxiv: [https://arxiv.org/abs/1612.08843](https://arxiv.org/abs/1612.08843)\n- github: [https://github.com/voidrank/FastMask](https://github.com/voidrank/FastMask)\n\n# Scene Labeling / Scene Parsing\n\n**Indoor Semantic Segmentation using depth information**\n\n- arxiv: [http://arxiv.org/abs/1301.3572](http://arxiv.org/abs/1301.3572)\n\n**Recurrent Convolutional Neural Networks for Scene Parsing**\n\n- arxiv: [http://arxiv.org/abs/1306.2795](http://arxiv.org/abs/1306.2795)\n- slides: [http://people.ee.duke.edu/~lcarin/Yizhe8.14.2015.pdf](http://people.ee.duke.edu/~lcarin/Yizhe8.14.2015.pdf)\n- github: [https://github.com/NP-coder/CLPS1520Project](https://github.com/NP-coder/CLPS1520Project)\n- github: [https://github.com/rkargon/Scene-Labeling](https://github.com/rkargon/Scene-Labeling)\n\n**Learning hierarchical features for scene labeling**\n\n- paper: [http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf](http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf)\n\n**Multi-modal unsupervised feature learning for rgb-d scene labeling**\n\n- intro: ECCV 2014\n- paper: [http://www3.ntu.edu.sg/home/wanggang/WangECCV2014.pdf](http://www3.ntu.edu.sg/home/wanggang/WangECCV2014.pdf)\n\n**Scene Labeling with LSTM Recurrent Neural Networks**\n\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Byeon_Scene_Labeling_With_2015_CVPR_paper.pdf)\n\n**Attend, Infer, Repeat: Fast Scene Understanding with Generative Models**\n\n- arxiv: [http://arxiv.org/abs/1603.08575](http://arxiv.org/abs/1603.08575)\n- notes: [http://www.shortscience.org/paper?bibtexKey=journals/corr/EslamiHWTKH16](http://www.shortscience.org/paper?bibtexKey=journals/corr/EslamiHWTKH16)\n\n**\"Semantic Segmentation for Scene Understanding: Algorithms and Implementations\" tutorial**\n\n- intro: 2016 Embedded Vision Summit\n- youtube: [https://www.youtube.com/watch?v=pQ318oCGJGY](https://www.youtube.com/watch?v=pQ318oCGJGY)\n\n**Semantic Understanding of Scenes through the ADE20K Dataset**\n\n- arxiv: [https://arxiv.org/abs/1608.05442](https://arxiv.org/abs/1608.05442)\n\n**Learning Deep Representations for Scene Labeling with Guided Supervision**\n\n**Learning Deep Representations for Scene Labeling with Semantic Context Guided Supervision**\n\n- intro: CUHK\n- arxiv: [https://arxiv.org/abs/1706.02493](https://arxiv.org/abs/1706.02493)\n\n**Spatial As Deep: Spatial CNN for Traffic Scene Understanding**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1712.06080](https://arxiv.org/abs/1712.06080)\n\n**Multi-Path Feedback Recurrent Neural Network for Scene Parsing**\n\n- arxiv: [http://arxiv.org/abs/1608.07706](http://arxiv.org/abs/1608.07706)\n\n**Scene Labeling using Recurrent Neural Networks with Explicit Long Range Contextual Dependency**\n\n- arxiv: [https://arxiv.org/abs/1611.07485](https://arxiv.org/abs/1611.07485)\n\n**FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.01587](https://arxiv.org/abs/2204.01587)\n\n## PSPNet\n\n**Pyramid Scene Parsing Network**\n\n- intro: CVPR 2017\n- intro: mIoU score as 85.4% on PASCAL VOC 2012 and 80.2% on Cityscapes, \nranked 1st place in ImageNet Scene Parsing Challenge 2016\n- project page: [http://appsrv.cse.cuhk.edu.hk/~hszhao/projects/pspnet/index.html](http://appsrv.cse.cuhk.edu.hk/~hszhao/projects/pspnet/index.html)\n- arxiv: [https://arxiv.org/abs/1612.01105](https://arxiv.org/abs/1612.01105)\n- slides: [http://image-net.org/challenges/talks/2016/SenseCUSceneParsing.pdf](http://image-net.org/challenges/talks/2016/SenseCUSceneParsing.pdf)\n- github: [https://github.com/hszhao/PSPNet](https://github.com/hszhao/PSPNet)\n- github: [https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow](https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow)\n\n**Open Vocabulary Scene Parsing**\n\n[https://arxiv.org/abs/1703.08769](https://arxiv.org/abs/1703.08769)\n\n**Deep Contextual Recurrent Residual Networks for Scene Labeling**\n\n[https://arxiv.org/abs/1704.03594](https://arxiv.org/abs/1704.03594)\n\n**Fast Scene Understanding for Autonomous Driving**\n\n- intro: Published at \"Deep Learning for Vehicle Perception\", workshop at the IEEE Symposium on Intelligent Vehicles 2017\n- arxiv: [https://arxiv.org/abs/1708.02550](https://arxiv.org/abs/1708.02550)\n\n**FoveaNet: Perspective-aware Urban Scene Parsing**\n\n[https://arxiv.org/abs/1708.02421](https://arxiv.org/abs/1708.02421)\n\n**BlitzNet: A Real-Time Deep Network for Scene Understanding**\n\n- intro: INRIA\n- arxiv: [https://arxiv.org/abs/1708.02813](https://arxiv.org/abs/1708.02813)\n\n**Semantic Foggy Scene Understanding with Synthetic Data**\n\n[https://arxiv.org/abs/1708.07819](https://arxiv.org/abs/1708.07819)\n\n**Scale-adaptive Convolutions for Scene Parsing**\n\n- intro: ICCV 2017\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Scale-Adaptive_Convolutions_for_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Scale-Adaptive_Convolutions_for_ICCV_2017_paper.pdf)\n\n**Restricted Deformable Convolution based Road Scene Semantic Segmentation Using Surround View Cameras**\n\n[https://arxiv.org/abs/1801.00708](https://arxiv.org/abs/1801.00708)\n\n**Dense Recurrent Neural Networks for Scene Labeling**\n\n[https://arxiv.org/abs/1801.06831](https://arxiv.org/abs/1801.06831)\n\n**DenseASPP for Semantic Segmentation in Street Scenes**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf)\n- github: [https://github.com/DeepMotionAIResearch/DenseASPP](https://github.com/DeepMotionAIResearch/DenseASPP)\n\n**OCNet: Object Context Network for Scene Parsing**\n\n- intro: Microsoft Research\n- arxiv: [https://arxiv.org/abs/1809.00916](https://arxiv.org/abs/1809.00916)\n- github: [https://github.com/PkuRainBow/OCNet](https://github.com/PkuRainBow/OCNet)\n\n**PSANet: Point-wise Spatial Attention Network for Scene Parsing**\n\n- intro: ECCV 2018\n- project page: [https://hszhao.github.io/projects/psanet/](https://hszhao.github.io/projects/psanet/)\n- paper: [https://hszhao.github.io/papers/eccv18_psanet.pdf](https://hszhao.github.io/papers/eccv18_psanet.pdf)\n- slides: [https://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit#slide=id.p](https://docs.google.com/presentation/d/1_brKNBtv8nVu_jOwFRGwVkEPAq8B8hEngBSQuZCWaZA/edit#slide=id.p)\n- github: [https://github.com/hszhao/PSANet](https://github.com/hszhao/PSANet)\n\n**Adaptive Context Network for Scene Parsing**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1911.01664](https://arxiv.org/abs/1911.01664)\n\n**Semantic Flow for Fast and Accurate Scene Parsing**\n\n- intro: ECCV 2020 oral\n- arxiv: [https://arxiv.org/abs/2002.10120](https://arxiv.org/abs/2002.10120)\n- github: [https://github.com/donnyyou/torchcv](https://github.com/donnyyou/torchcv)\n\n**Strip Pooling: Rethinking Spatial Pooling for Scene Parsing**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.13328](https://arxiv.org/abs/2003.13328)\n- github: [https://github.com/Andrew-Qibin/SPNet](https://github.com/Andrew-Qibin/SPNet)\n\n**S3-Net: A Fast and Lightweight Video Scene Understanding Network by Single-shot Segmentation**\n\n- intro: WACV 2021\n- arxiv: [https://arxiv.org/abs/2011.02265](https://arxiv.org/abs/2011.02265)\n\n## Benchmarks\n\n**MIT Scene Parsing Benchmark**\n\n- homepage: [http://sceneparsing.csail.mit.edu/](http://sceneparsing.csail.mit.edu/)\n- github(devkit): [https://github.com/CSAILVision/sceneparsing](https://github.com/CSAILVision/sceneparsing)\n\n**Semantic Understanding of Urban Street Scenes: Benchmark Suite**\n\n[https://www.cityscapes-dataset.com/benchmarks/](https://www.cityscapes-dataset.com/benchmarks/)\n\n## Challenges\n\n**Large-scale Scene Understanding Challenge**\n\n![](http://lsun.cs.princeton.edu/img/overview_4crop.jpg)\n\n- homepage: [http://lsun.cs.princeton.edu/](http://lsun.cs.princeton.edu/)\n\n**Places2 Challenge**\n\n[http://places2.csail.mit.edu/challenge.html](http://places2.csail.mit.edu/challenge.html)\n\n# Human Parsing\n\n**Human Parsing with Contextualized Convolutional Neural Network**\n\n- intro: ICCV 2015\n- paper: [http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Liang_Human_Parsing_With_ICCV_2015_paper.html](http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Liang_Human_Parsing_With_ICCV_2015_paper.html)\n\n**Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing**\n\n- intro: CVPR 2017. SYSU & CMU\n- keywords: Look Into Person (LIP)\n- project page: [http://hcp.sysu.edu.cn/lip/](http://hcp.sysu.edu.cn/lip/)\n- arxiv: [https://arxiv.org/abs/1703.05446](https://arxiv.org/abs/1703.05446)\n- github: [https://github.com/Engineering-Course/LIP_SSL](https://github.com/Engineering-Course/LIP_SSL)\n\n**Multiple-Human Parsing in the Wild**\n\n[https://arxiv.org/abs/1705.07206](https://arxiv.org/abs/1705.07206)\n\n**Look into Person: Joint Body Parsing & Pose Estimation Network and A New Benchmark**\n\n- intro: T-PAMI 2018\n- keywords: Joint Body Parsing & Pose Estimation Network (JPPNet)\n- arxiv: [https://arxiv.org/abs/1804.01984](https://arxiv.org/abs/1804.01984)\n- github: [https://github.com/Engineering-Course/LIP_JPPNet](https://github.com/Engineering-Course/LIP_JPPNet)\n\n**Cross-domain Human Parsing via Adversarial Feature and Label Adaptation**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1801.01260](https://arxiv.org/abs/1801.01260)\n\n**Fusing Hierarchical Convolutional Features for Human Body Segmentation and Clothing Fashion Classification**\n\n- intro: Wuhan University\n- arxiv: [https://arxiv.org/abs/1803.03415](https://arxiv.org/abs/1803.03415)\n\n**Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing**\n\n- arxiv: [https://arxiv.org/abs/1804.03287](https://arxiv.org/abs/1804.03287)\n- github: [https://github.com/ZhaoJ9014/Multi-Human-Parsing](https://github.com/ZhaoJ9014/Multi-Human-Parsing)\n\n**Macro-Micro Adversarial Network for Human Parsing**\n\n- intro: ECCV 2018\n- keywords: Macro-Micro Adversarial Net (MMAN)\n- arxiv: [https://arxiv.org/abs/1807.08260](https://arxiv.org/abs/1807.08260)\n- github: [https://github.com/RoyalVane/MMAN](https://github.com/RoyalVane/MMAN)\n\n**Instance-level Human Parsing via Part Grouping Network**\n\n- intro: ECCV 2018 Oral\n- arxiv: [https://arxiv.org/abs/1808.00157](https://arxiv.org/abs/1808.00157)\n\n**Adaptive Temporal Encoding Network for Video Instance-level Human Parsing**\n\n- intro: ACM MM 2018\n= arixv: [https://arxiv.org/abs/1808.00661](https://arxiv.org/abs/1808.00661)\n- github(official, TensorFlow): [https://github.com/HCPLab-SYSU/ATEN](https://github.com/HCPLab-SYSU/ATEN)\n\n**Devil in the Details: Towards Accurate Single and Multiple Human Parsing**\n\n- keywords: Context Embedding with Edge Perceiving (CE2P)\n- arxiv: [https://arxiv.org/abs/1809.05996](https://arxiv.org/abs/1809.05996)\n- github: [https://github.com/liutinglt/CE2P](https://github.com/liutinglt/CE2P)\n\n**Cross-Domain Complementary Learning with Synthetic Data for Multi-Person Part Segmentation**\n\n- intro: University of Washington & Microsof\n- arxiv: [https://arxiv.org/abs/1907.05193](https://arxiv.org/abs/1907.05193)\n\n**Self-Correction for Human Parsing**\n\n- arxiv: [https://arxiv.org/abs/1910.09777](https://arxiv.org/abs/1910.09777)\n- github: [https://github.com/PeikeLi/Self-Correction-Human-Parsing](https://github.com/PeikeLi/Self-Correction-Human-Parsing)\n\n**Grapy-ML: Graph Pyramid Mutual Learning for Cross-dataset Human Parsing**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.12053](https://arxiv.org/abs/1911.12053)\n- github: [https://github.com/Charleshhy/Grapy-ML](https://github.com/Charleshhy/Grapy-ML)\n\n**Learning Semantic Neural Tree for Human Parsing**\n\n- intro: Institute of Software Chinese Academy of Sciences & State University of New York & JD Finance America Corporation & Tencent Youtu Lab\n- arxiv: [https://arxiv.org/abs/1912.09622](https://arxiv.org/abs/1912.09622)\n- code: [https://isrc.iscas.ac.cn/gitlab/research/sematree](https://isrc.iscas.ac.cn/gitlab/research/sematree)\n\n**Self-Learning with Rectification Strategy for Human Parsing**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2004.08055](https://arxiv.org/abs/2004.08055)\n\n**Correlating Edge, Pose with Parsing**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2005.01431](https://arxiv.org/abs/2005.01431)\n- github: [https://github.com/ziwei-zh/CorrPM](https://github.com/ziwei-zh/CorrPM)\n\n**Affinity-aware Compression and Expansion Network for Human Parsing**\n\n[https://arxiv.org/abs/2008.10191](https://arxiv.org/abs/2008.10191)\n\n**Renovating Parsing R-CNN for Accurate Multiple Human Parsing**\n\n- intro: ECCV 2020\n- intro: BUPT & Noah’s Ark Lab, Huawei Technologies\n- arxiv: [https://arxiv.org/abs/2009.09447](https://arxiv.org/abs/2009.09447)\n- github: [https://github.com/soeaver/RP-R-CNN](https://github.com/soeaver/RP-R-CNN)\n\n**Progressive One-shot Human Parsing**\n\n- intro: AAAI 2021\n- arxiv: [https://arxiv.org/abs/2012.11810](https://arxiv.org/abs/2012.11810)\n- github: [https://github.com/Charleshhy/One-shot-Human-Parsing](https://github.com/Charleshhy/One-shot-Human-Parsing)\n\n**Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing**\n\n- intro: CVPR 2021 oral\n- arxiv: [https://arxiv.org/abs/2103.04570](https://arxiv.org/abs/2103.04570)\n- github: [https://github.com/tfzhou/MG-HumanParsing](https://github.com/tfzhou/MG-HumanParsing)\n\n**Quality-Aware Network for Human Parsing**\n\n- intro: BUPT & Institute of Automation Chinese Academy of Sciences & 3Noah’s Ark Lab\n- arxiv: [https://arxiv.org/abs/2103.05997](https://arxiv.org/abs/2103.05997)\n- github(Pytorch): [https://github.com/soeaver/QANet](https://github.com/soeaver/QANet)\n\n**End-to-end One-shot Human Parsing**\n\n[https://arxiv.org/abs/2105.01241](https://arxiv.org/abs/2105.01241)\n\n**CDGNet: Class Distribution Guided Network for Human Parsing**\n\n- intro: Ajou University & Tiangong University & Incheon National University\n- arxiv: [https://arxiv.org/abs/2111.14173](https://arxiv.org/abs/2111.14173)\n\n**AIParsing: Anchor-free Instance-level Human Parsing**\n\n- intro: IEEE Transactions on Image Processing (TIP)\n- arxiv: [https://arxiv.org/abs/2207.06854](https://arxiv.org/abs/2207.06854)\n\n# Joint Detection and Segmentation\n\n**Triply Supervised Decoder Networks for Joint Detection and Segmentation**\n\n[https://arxiv.org/abs/1809.09299](https://arxiv.org/abs/1809.09299)\n\n**D2Det: Towards High Quality Object Detection and Instance Segmentation**\n\n- intro: CVPR 2020\n- paper: [https://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_D2Det_Towards_High_Quality_Object_Detection_and_Instance_Segmentation_CVPR_2020_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_D2Det_Towards_High_Quality_Object_Detection_and_Instance_Segmentation_CVPR_2020_paper.pdf)\n- github: [https://github.com/JialeCao001/D2Det](https://github.com/JialeCao001/D2Det)\n\n# Video Object Segmentation\n\n**Fast object segmentation in unconstrained video**\n\n- project page: [http://calvin.inf.ed.ac.uk/software/fast-video-segmentation/](http://calvin.inf.ed.ac.uk/software/fast-video-segmentation/)\n- paper: [http://calvin.inf.ed.ac.uk/wp-content/uploads/Publications/papazoglouICCV2013-camera-ready.pdf](http://calvin.inf.ed.ac.uk/wp-content/uploads/Publications/papazoglouICCV2013-camera-ready.pdf)\n\n**Recurrent Fully Convolutional Networks for Video Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1606.00487](https://arxiv.org/abs/1606.00487)\n\n**Object Detection, Tracking, and Motion Segmentation for Object-level Video Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1608.03066](http://arxiv.org/abs/1608.03066)\n\n**Clockwork Convnets for Video Semantic Segmentation**\n\n- intro: ECCV 2016 Workshops\n- intro: evaluated on the Youtube-Objects, NYUD, and Cityscapes video datasets\n- arxiv: [http://arxiv.org/abs/1608.03609](http://arxiv.org/abs/1608.03609)\n- github: [https://github.com/shelhamer/clockwork-fcn](https://github.com/shelhamer/clockwork-fcn)\n\n**STFCN: Spatio-Temporal FCN for Semantic Video Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1608.05971](http://arxiv.org/abs/1608.05971)\n\n**One-Shot Video Object Segmentation**\n\n- intro: OSVOS\n- project: [http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/](http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/)\n- arxiv: [https://arxiv.org/abs/1611.05198](https://arxiv.org/abs/1611.05198)\n- github(official): [https://github.com/kmaninis/OSVOS-caffe](https://github.com/kmaninis/OSVOS-caffe)\n- github(official): [https://github.com/scaelles/OSVOS-TensorFlow](https://github.com/scaelles/OSVOS-TensorFlow)\n- github(official): [https://github.com/kmaninis/OSVOS-PyTorch](https://github.com/kmaninis/OSVOS-PyTorch)\n\n**DAVIS: Densely Annotated VIdeo Segmentation**\n\n- homepage: [http://davischallenge.org/](http://davischallenge.org/)\n- arxiv: [https://arxiv.org/abs/1704.00675](https://arxiv.org/abs/1704.00675)\n\n**Video Object Segmentation Without Temporal Information**\n\n[https://arxiv.org/abs/1709.06031](https://arxiv.org/abs/1709.06031)\n\n**Convolutional Gated Recurrent Networks for Video Segmentation**\n\n- arxiv: [https://arxiv.org/abs/1611.05435](https://arxiv.org/abs/1611.05435)\n\n**Learning Video Object Segmentation from Static Images**\n\n- arxiv: [https://arxiv.org/abs/1612.02646](https://arxiv.org/abs/1612.02646)\n\n**Semantic Video Segmentation by Gated Recurrent Flow Propagation**\n\n- arxiv: [https://arxiv.org/abs/1612.08871](https://arxiv.org/abs/1612.08871)\n\n**FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos**\n\n- project page: [http://vision.cs.utexas.edu/projects/fusionseg/](http://vision.cs.utexas.edu/projects/fusionseg/)\n- arxiv: [https://arxiv.org/abs/1701.05384](https://arxiv.org/abs/1701.05384)\n- github: [https://github.com/suyogduttjain/fusionseg](https://github.com/suyogduttjain/fusionseg)\n\n**Unsupervised learning from video to detect foreground objects in single images**\n\n[https://arxiv.org/abs/1703.10901](https://arxiv.org/abs/1703.10901)\n\n**Semantically-Guided Video Object Segmentation**\n\n[https://arxiv.org/abs/1704.01926](https://arxiv.org/abs/1704.01926)\n\n**Learning Video Object Segmentation with Visual Memory**\n\n[https://arxiv.org/abs/1704.05737](https://arxiv.org/abs/1704.05737)\n\n**Flow-free Video Object Segmentation**\n\n[https://arxiv.org/abs/1706.09544](https://arxiv.org/abs/1706.09544)\n\n**Online Adaptation of Convolutional Neural Networks for Video Object Segmentation**\n\n[https://arxiv.org/abs/1706.09364](https://arxiv.org/abs/1706.09364)\n\n**Video Object Segmentation using Tracked Object Proposals**\n\n- intro: CVPR-2017 workshop, DAVIS-2017 Challenge\n- arxiv: [https://arxiv.org/abs/1707.06545](https://arxiv.org/abs/1707.06545)\n\n**Video Object Segmentation with Re-identification**\n\n- intro: CVPR 2017 Workshop, DAVIS Challenge on Video Object Segmentation 2017 (Winning Entry)\n- arxiv: [https://arxiv.org/abs/1708.00197](https://arxiv.org/abs/1708.00197)\n- github(official, PyTorch): [https://github.com/lxx1991/VS-ReID](https://github.com/lxx1991/VS-ReID)\n\n**Pixel-Level Matching for Video Object Segmentation using Convolutional Neural Networks**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.05137](https://arxiv.org/abs/1708.05137)\n\n**MaskRNN: Instance Level Video Object Segmentation**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1803.11187](https://arxiv.org/abs/1803.11187)\n\n**SegFlow: Joint Learning for Video Object Segmentation and Optical Flow**\n\n- project page: [https://sites.google.com/site/yihsuantsai/research/iccv17-segflow](https://sites.google.com/site/yihsuantsai/research/iccv17-segflow)\n- arxiv: [https://arxiv.org/abs/1709.06750](https://arxiv.org/abs/1709.06750)\n- github: [https://github.com/JingchunCheng/SegFlow](https://github.com/JingchunCheng/SegFlow)\n\n**Video Semantic Object Segmentation by Self-Adaptation of DCNN**\n\n[https://arxiv.org/abs/1711.08180](https://arxiv.org/abs/1711.08180)\n\n**Learning to Segment Moving Objects**\n\n[https://arxiv.org/abs/1712.01127](https://arxiv.org/abs/1712.01127)\n\n**Instance Embedding Transfer to Unsupervised Video Object Segmentation**\n\n- intro: University of Southern California & Google Inc\n- arxiv: [https://arxiv.org/abs/1801.00908](https://arxiv.org/abs/1801.00908)\n- blog: [https://medium.com/@barvinograd1/instance-embedding-instance-segmentation-without-proposals-31946a7c53e1](https://medium.com/@barvinograd1/instance-embedding-instance-segmentation-without-proposals-31946a7c53e1)\n\n**Efficient Video Object Segmentation via Network Modulation**\n\n- intro: Snap Inc. & Northwestern University & Google Inc.\n- arxiv: [https://arxiv.org/abs/1802.01218](https://arxiv.org/abs/1802.01218)\n\n**Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation**\n\n- intro: ECCV 2018\n- intro: CUHK\n- keywords: DyeNet\n- arxiv: [https://arxiv.org/abs/1803.04242](https://arxiv.org/abs/1803.04242)\n\n**Video Object Segmentation with Language Referring Expressions**\n\n[https://arxiv.org/abs/1803.08006](https://arxiv.org/abs/1803.08006)\n\n**Dynamic Video Segmentation Network**\n\n- intro: CVPR 2018\n- keywords: DVSNet\n- arxiv: [https://arxiv.org/abs/1804.00931](https://arxiv.org/abs/1804.00931)\n- github: [https://github.com/XUSean0118/DVSNet](https://github.com/XUSean0118/DVSNet)\n\n**Low-Latency Video Semantic Segmentation**\n\n- intro: CVPR 2018 Spotlight\n- arxiv: [https://arxiv.org/abs/1804.00389](https://arxiv.org/abs/1804.00389)\n\n**Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.03131](https://arxiv.org/abs/1804.03131)\n\n**Unsupervised Video Object Segmentation for Deep Reinforcement Learning**\n\n- intro: University of Waterloo\n- arxiv: [https://arxiv.org/abs/1805.07780](https://arxiv.org/abs/1805.07780)\n\n**Fast and Accurate Online Video Object Segmentation via Tracking Parts**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1806.02323](https://arxiv.org/abs/1806.02323)\n- github: [https://github.com/JingchunCheng/FAVOS](https://github.com/JingchunCheng/FAVOS)\n\n**ReConvNet: Video Object Segmentation with Spatio-Temporal Features Modulation**\n\n- intro: CVPR Workshop - DAVIS Challenge 2018\n- arxiv: [https://arxiv.org/abs/1806.05510](https://arxiv.org/abs/1806.05510)\n\n**Deep Spatio-Temporal Random Fields for Efficient Video Segmentation**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1807.03148](https://arxiv.org/abs/1807.03148)\n\n**Fast Video Object Segmentation by Reference-Guided Mask Propagation**\n\n- intro: CVPR 2018\n- paper: [http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1029.pdf](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1029.pdf)\n- github: [https://github.com/seoungwugoh/RGMP](https://github.com/seoungwugoh/RGMP)\n\n**PReMVOS: Proposal-generation, Refinement and Merging for Video Object Segmentation**\n\n[https://arxiv.org/abs/1807.09190](https://arxiv.org/abs/1807.09190)\n\n**YouTube-VOS: Sequence-to-Sequence Video Object Segmentation**\n\n- intro: ECCV 2018. Adobe Research & Snapchat Research & UIUC\n- project page:[https://youtube-vos.org/](https://youtube-vos.org/)\n- arxiv: [https://arxiv.org/abs/1809.00461](https://arxiv.org/abs/1809.00461)\n\n**VideoMatch: Matching based Video Object Segmentation**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1809.01123](https://arxiv.org/abs/1809.01123)\n\n**Mask Propagation Network for Video Object Segmentation**\n\n- intro: ByteDance AI Lab\n- arxiv: [https://arxiv.org/abs/1810.10289](https://arxiv.org/abs/1810.10289)\n\n**Tukey-Inspired Video Object Segmentation**\n\n[https://arxiv.org/abs/1811.07958](https://arxiv.org/abs/1811.07958)\n\n**A Generative Appearance Model for End-to-end Video Object Segmentation**\n\n[https://arxiv.org/abs/1811.11611](https://arxiv.org/abs/1811.11611)\n\n**Unseen Object Segmentation in Videos via Transferable Representations**\n\n- intro: ACCV 2018 oral\n- arxiv: [https://arxiv.org/abs/1901.02444](https://arxiv.org/abs/1901.02444)\n- github: [https://github.com/wenz116/TransferSeg](https://github.com/wenz116/TransferSeg)\n\n**FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation**\n\n- intro: CVPR 2019\n- intro: RWTH Aachen University & Google Inc.\n- arxiv: [https://arxiv.org/abs/1902.09513](https://arxiv.org/abs/1902.09513)\n\n**RVOS: End-to-End Recurrent Network for Video Object Segmentation**\n\n- intro: CVPR 2019\n- project page: [https://imatge-upc.github.io/rvos/](https://imatge-upc.github.io/rvos/)\n- arxiv: [https://arxiv.org/abs/1903.05612](https://arxiv.org/abs/1903.05612)\n\n**BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames**\n\n- intro: CVPR 2019\n- intro: University of Michigan\n- arxiv: [https://arxiv.org/abs/1903.11779](https://arxiv.org/abs/1903.11779)\n- github: [https://github.com/griffbr/BubbleNets](https://github.com/griffbr/BubbleNets)\n- video: [https://www.youtube.com/watch?v=0kNmm8SBnnU&feature=youtu.be](https://www.youtube.com/watch?v=0kNmm8SBnnU&feature=youtu.be)\n\n**Fast video object segmentation with Spatio-Temporal GANs**\n\n[https://arxiv.org/abs/1903.12161](https://arxiv.org/abs/1903.12161)\n\n**Video Object Segmentation using Space-Time Memory Networks**\n\n- intro: ICCV 2019\n- intro: Yonsei University & Adobe Research\n- arxiv: [https://arxiv.org/abs/1904.00607](https://arxiv.org/abs/1904.00607)\n- github: [https://github.com/seoungwugoh/STM](https://github.com/seoungwugoh/STM)\n\n**Spatiotemporal CNN for Video Object Segmentation**\n\n[https://arxiv.org/abs/1904.02363]\n\n**Architecture Search of Dynamic Cells for Semantic Video Segmentation**\n\n[https://arxiv.org/abs/1904.02371](https://arxiv.org/abs/1904.02371)\n\n**BoLTVOS: Box-Level Tracking for Video Object Segmentation**\n\n[https://arxiv.org/abs/1904.04552](https://arxiv.org/abs/1904.04552)\n\n**MAIN: Multi-Attention Instance Network for Video Segmentation**\n\n[https://arxiv.org/abs/1904.05847](https://arxiv.org/abs/1904.05847)\n\n**MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation**\n\n- intro: CVPR 2019 oral\n- arxiv: [https://arxiv.org/abs/1904.08141](https://arxiv.org/abs/1904.08141)\n\n**Video Instance Segmentation**\n\n- intro: ICCV 2019\n- intro: ByteDance AI Lab & UIUC & Adobe Research\n- keywords: MaskTrack R-CNN\n- arxiv: [https://arxiv.org/abs/1905.04804](https://arxiv.org/abs/1905.04804)\n- github: [https://github.com/youtubevos/MaskTrackRCNN](https://github.com/youtubevos/MaskTrackRCNN)\n\n**OVSNet : Towards One-Pass Real-Time Video Object Segmentation**\n\n- intro: Zhejiang University & SenseTime Research & Tianjin University]\n- arxiv: [https://arxiv.org/abs/1905.10064](https://arxiv.org/abs/1905.10064)\n\n**Proposal, Tracking and Segmentation (PTS): A Cascaded Network for Video Object Segmentation**\n\n- intro: Huazhong University of Science and Technology & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1907.01203](https://arxiv.org/abs/1907.01203)\n- github: [https://github.com/sydney0zq/PTSNet](https://github.com/sydney0zq/PTSNet)\n\n**RANet: Ranking Attention Network for Fast Video Object Segmentation**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.06647](https://arxiv.org/abs/1908.06647)\n- github: [https://github.com/Storife/RANet](https://github.com/Storife/RANet)\n\n**DMM-Net: Differentiable Mask-Matching Network for Video Object Segmentation**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.12471](https://arxiv.org/abs/1909.12471)\n\n**CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.00132](https://arxiv.org/abs/1910.00132)\n\n**Towards Good Practices for Video Object Segmentation**\n\n- intro: ByteDance AI Lab\n- arxiv: [https://arxiv.org/abs/1909.13583](https://arxiv.org/abs/1909.13583)\n\n**Anchor Diffusion for Unsupervised Video Object Segmentation**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.10895](https://arxiv.org/abs/1910.10895)\n\n**Learning a Spatio-Temporal Embedding for Video Instance Segmentation**\n\n- intro: University of Cambridge\n- arxiv: [https://arxiv.org/abs/1912.08969](https://arxiv.org/abs/1912.08969)\n\n**Efficient Semantic Video Segmentation with Per-frame Inference**\n\n- intro: ECCV 2020\n- intro: The University of Adelaide & Huazhong University of Science and Technology & Microsoft Research\n- arxiv: [https://arxiv.org/abs/2002.11433](https://arxiv.org/abs/2002.11433)\n- github: [https://github.com/irfanICMLL/ETC-Real-time-Per-frame-Semantic-video-segmentation](https://github.com/irfanICMLL/ETC-Real-time-Per-frame-Semantic-video-segmentation)\n\n**State-Aware Tracker for Real-Time Video Object Segmentation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.00482](https://arxiv.org/abs/2003.00482)\n- github: [https://github.com/MegviiDetection/video_analyst](https://github.com/MegviiDetection/video_analyst)\n\n**Video Object Segmentation with Adaptive Feature Bank and Uncertain-Region Refinement**\n\n- intro: NeurIPS 2020\n- arxiv: [https://arxiv.org/abs/2010.07958](https://arxiv.org/abs/2010.07958)\n\n**SwiftNet: Real-time Video Object Segmentation**\n\n[https://arxiv.org/abs/2102.04604](https://arxiv.org/abs/2102.04604)\n\n**SG-Net: Spatial Granularity Network for One-Stage Video Instance Segmentation**\n\n[https://arxiv.org/abs/2103.10284](https://arxiv.org/abs/2103.10284)\n\n## Challenge\n\n**DAVIS Challenge on Video Object Segmentation 2017**\n\n[http://davischallenge.org/challenge2017/publications.html](http://davischallenge.org/challenge2017/publications.html)\n\n# Matting\n\n**Deep Image Matting**\n\n- intro: CVPR 2017\n- intro: Beckman Institute for Advanced Science and Technology & Adobe Research\n- project page: [https://sites.google.com/view/deepimagematting](https://sites.google.com/view/deepimagematting)\n- arxiv: [https://arxiv.org/abs/1703.03872](https://arxiv.org/abs/1703.03872)\n- github(unofficial): [https://github.com/open-mmlab/mmediting/tree/master/configs/mattors/dim](https://github.com/open-mmlab/mmediting/tree/master/configs/mattors/dim)\n- github(unofficial): [https://github.com/foamliu/Deep-Image-Matting](https://github.com/foamliu/Deep-Image-Matting)\n- github(unofficial): [https://github.com/foamliu/Deep-Image-Matting-PyTorch](https://github.com/foamliu/Deep-Image-Matting-PyTorch)\n- github(unofficial): [https://github.com/huochaitiantang/pytorch-deep-image-matting](https://github.com/huochaitiantang/pytorch-deep-image-matting)\n\n**Fast Deep Matting for Portrait Animation on Mobile Phone**\n\n- intro: ACM Multimedia Conference (MM) 2017\n- intro: does not need any interaction and can realize real-time matting with 15 fps\n- arxiv: [https://arxiv.org/abs/1707.08289](https://arxiv.org/abs/1707.08289)\n\n**Real-time deep hair matting on mobile devices**\n\n- intro: ModiFace Inc, University of Toronto\n- arxiv: [https://arxiv.org/abs/1712.07168](https://arxiv.org/abs/1712.07168)\n\n**TOM-Net: Learning Transparent Object Matting from a Single Image**\n\n- intro: CVPR 2018\n- project page: [http://gychen.org/TOM-Net/](http://gychen.org/TOM-Net/)\n- arxiv: [https://arxiv.org/abs/1803.04636](https://arxiv.org/abs/1803.04636)\n- github: [https://github.com/guanyingc/TOM-Net](https://github.com/guanyingc/TOM-Net)\n\n**Deep Video Portraits**\n\n- intro: SIGGRAPH 2018\n- arxiv: [https://arxiv.org/abs/1805.11714](https://arxiv.org/abs/1805.11714)\n- youtube: [https://www.youtube.com/watch?v=qc5P2bvfl44](https://www.youtube.com/watch?v=qc5P2bvfl44)\n\n**Inductive Guided Filter: Real-time Deep Image Matting with Weakly Annotated Masks on Mobile Devices**\n\n- intro: Shanghai Jiao Tong University & Versa\n- arxiv: [https://arxiv.org/abs/1905.06747](https://arxiv.org/abs/1905.06747)\n\n**Indices Matter: Learning to Index for Deep Image Matting**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.00672](https://arxiv.org/abs/1908.00672)\n- github(official): [https://github.com/poppinace/indexnet_matting](https://github.com/poppinace/indexnet_matting)\n- github: [https://github.com/open-mmlab/mmediting/tree/master/configs/mattors/indexnet](https://github.com/open-mmlab/mmediting/tree/master/configs/mattors/indexnet)\n\n**Disentangled Image Matting**\n\n[https://arxiv.org/abs/1909.04686](https://arxiv.org/abs/1909.04686)\n\n**Natural Image Matting via Guided Contextual Attention**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/2001.04069](https://arxiv.org/abs/2001.04069)\n- github: [https://github.com/Yaoyi-Li/GCA-Matting](https://github.com/Yaoyi-Li/GCA-Matting)\n\n**F, B, Alpha Matting**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2003.07711](https://arxiv.org/abs/2003.07711)\n- github: [https://github.com/MarcoForte/FBA_Matting](https://github.com/MarcoForte/FBA_Matting)\n\n**Background Matting: The World is Your Green Screen**\n\n- intro: CVPR 2020\n- intro: University of Washington\n- project page: [https://grail.cs.washington.edu/projects/background-matting/](https://grail.cs.washington.edu/projects/background-matting/)\n- arxiv: [https://arxiv.org/abs/2004.00626](https://arxiv.org/abs/2004.00626)\n- github: [https://github.com/senguptaumd/Background-Matting](https://github.com/senguptaumd/Background-Matting)\n- blog: [https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635](https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635)\n\n**Hierarchical Opacity Propagation for Image Matting**\n\n- intro: Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/2004.03249](https://arxiv.org/abs/2004.03249)\n- github: [https://github.com/Yaoyi-Li/HOP-Matting](https://github.com/Yaoyi-Li/HOP-Matting)\n\n**High-Resolution Deep Image Matting**\n\n- intro: UIUC & Adobe Research & University of Oregon\n- arxiv: [https://arxiv.org/abs/2009.06613](https://arxiv.org/abs/2009.06613)\n\n**Learning Affinity-Aware Upsampling for Deep Image Matting**\n\n- intro: The University of Adelaide & Huazhong University of Science and Technology\n- arxiv: [https://arxiv.org/abs/2011.14288](https://arxiv.org/abs/2011.14288)\n\n**Real-Time High-Resolution Background Matting**\n\n- project page: [https://grail.cs.washington.edu/projects/background-matting-v2/](https://grail.cs.washington.edu/projects/background-matting-v2/)\n- arxiv: [https://arxiv.org/abs/2012.07810](https://arxiv.org/abs/2012.07810)\n- github: [https://github.com/PeterL1n/BackgroundMattingV2](https://github.com/PeterL1n/BackgroundMattingV2)\n\n**Deep Video Matting via Spatio-Temporal Alignment and Aggregation**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.11208](https://arxiv.org/abs/2104.11208)\n- github: [https://github.com/nowsyn/DVM](https://github.com/nowsyn/DVM)\n\n**Trimap-guided Feature Mining and Fusion Network for Natural Image Matting**\n\n- intro: Shanghai Jiao Tong University & ByteDance Inc.\n- arxiv: [https://arxiv.org/abs/2112.00510](https://arxiv.org/abs/2112.00510)\n\n**Boosting Robustness of Image Matting with Context Assembling and Strong Data Augmentation**\n\n- intro: The University of Adelaide & Adobe Inc. & Zhejiang University\n- arxiv: [https://arxiv.org/abs/2201.06889](https://arxiv.org/abs/2201.06889)\n\n**MatteFormer: Transformer-Based Image Matting via Prior-Tokens**\n\n- intro: Seoul National University & NAVER WEBTOON AI\n- arxiv: [https://arxiv.org/abs/2203.15662](https://arxiv.org/abs/2203.15662)\n\n**Referring Image Matting**\n\n- intro: The University of Sydney & JD Explore Academy\n- arxiv: [https://arxiv.org/abs/2206.05149](https://arxiv.org/abs/2206.05149)\n- github: [https://github.com/JizhiziLi/RIM](https://github.com/JizhiziLi/RIM)\n\n**One-Trimap Video Matting**\n\n- intro: ECCV 2022\n- arxiv: [https://arxiv.org/abs/2207.13353](https://arxiv.org/abs/2207.13353)\n- github: [https://github.com/Hongje/OTVM](https://github.com/Hongje/OTVM)\n\n## trimap-free matting\n\n**Semantic Human Matting**\n\n- intro: ACM Multimedia 2018\n- arxiv: [https://arxiv.org/abs/1809.01354](https://arxiv.org/abs/1809.01354)\n- github(unofficial): [https://github.com/lizhengwei1992/Semantic_Human_Matting](https://github.com/lizhengwei1992/Semantic_Human_Matting)\n\n**Instance Segmentation based Semantic Matting for Compositing Applications**\n\n- intro: CRV 2019\n- arxiv: [https://arxiv.org/abs/1904.05457](https://arxiv.org/abs/1904.05457)\n\n**A Late Fusion CNN for Digital Matting**\n\n- intro: CVPR 2019\n- intro: Zhejiang University & Alibaba Group & University of Texas at Austin\n- paper: [https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_A_Late_Fusion_CNN_for_Digital_Matting_CVPR_2019_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_A_Late_Fusion_CNN_for_Digital_Matting_CVPR_2019_paper.pdf)\n- github(official, Keras): [https://github.com/yunkezhang/FusionMatting](https://github.com/yunkezhang/FusionMatting)\n\n**Attention-Guided Hierarchical Structure Aggregation for Image Matting**\n\n- intro: CVPR 2020\n- project page: [https://wukaoliu.github.io/HAttMatting/](https://wukaoliu.github.io/HAttMatting/)\n- paper: [https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Attention-Guided_Hierarchical_Structure_Aggregation_for_Image_Matting_CVPR_2020_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Attention-Guided_Hierarchical_Structure_Aggregation_for_Image_Matting_CVPR_2020_paper.pdf)\n- github: [https://github.com/wukaoliu/CVPR2020-HAttMatting](https://github.com/wukaoliu/CVPR2020-HAttMatting)\n\n**Boosting Semantic Human Matting with Coarse Annotations**\n\n- intro: Alibaba Group & Tsinghua University\n- arxiv: [https://arxiv.org/abs/2004.04955](https://arxiv.org/abs/2004.04955)\n\n**End-to-end Animal Image Matting**\n\n- keywords: Glance and Focus Matting network (GFM), AM-2k dataset, BG-20k dataset\n- arxiv: [https://arxiv.org/abs/2010.16188](https://arxiv.org/abs/2010.16188)\n- github: [https://github.com/JizhiziLi/animal-matting/](https://github.com/JizhiziLi/animal-matting/)\n\n**Is a Green Screen Really Necessary for Real-Time Human Matting?**\n\n- intro: City University of Hong Kong & SenseTime Research\n- arxiv: [https://arxiv.org/abs/2011.11961](https://arxiv.org/abs/2011.11961)\n- github: [https://github.com/ZHKKKe/MODNet](https://github.com/ZHKKKe/MODNet)\n\n**Multi-scale Information Assembly for Image Matting**\n\n[https://arxiv.org/abs/2101.02391](https://arxiv.org/abs/2101.02391)\n\n**Salient Image Matting**\n\n- intro: Fynd & University of Michigan\n- arxiv: [https://arxiv.org/abs/2103.12337](https://arxiv.org/abs/2103.12337)\n\n**Mask Guided Matting via Progressive Refinement Network**\n\n- intro: CVPR 2021\n- intro: The Johns Hopkins University & Adobe\n- arxiv: [https://arxiv.org/abs/2012.06722](https://arxiv.org/abs/2012.06722)\n- github: [https://github.com/yucornetto/MGMatting](https://github.com/yucornetto/MGMatting)\n\n**Privacy-Preserving Portrait Matting**\n\n- intro: The University of Sydney & JD Explore Academy\n- arxiv: [https://arxiv.org/abs/2104.14222](https://arxiv.org/abs/2104.14222)\n- github: [https://github.com/SHI-Labs/Pseudo-IoU-for-Anchor-Free-Object-Detection](https://github.com/SHI-Labs/Pseudo-IoU-for-Anchor-Free-Object-Detection)\n\n**Highly Efficient Natural Image Matting**\n\n- intro: BMVC 2021\n- arxiv: [https://arxiv.org/abs/2110.12748](https://arxiv.org/abs/2110.12748)\n\n**PP-HumanSeg: Connectivity-Aware Portrait Segmentation with a Large-Scale Teleconferencing Video Dataset**\n\n- intro: WACV 2021 workshop\n- intro: Baidu, Inc.\n- arxiv: [https://arxiv.org/abs/2112.07146](https://arxiv.org/abs/2112.07146)\n- github: [https://github.com/PaddlePaddle/PaddleSeg](https://github.com/PaddlePaddle/PaddleSeg)\n\n**Situational Perception Guided Image Matting**\n\n- intro: OPPO Research Institute & PicUp.AI & Xmotors\n- arxiv: [https://arxiv.org/abs/2204.09276](https://arxiv.org/abs/2204.09276)\n\n**PP-Matting: High-Accuracy Natural Image Matting**\n\n- intro: Baidu Inc.\n- arixv: [https://arxiv.org/abs/2204.09433](https://arxiv.org/abs/2204.09433)\n- github: [https://github.com/PaddlePaddle/PaddleSeg](https://github.com/PaddlePaddle/PaddleSeg)\n\n# 3D Segmentation\n\n**PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation**\n\n- intro: Stanford University\n- project page: [http://stanford.edu/~rqi/pointnet/](http://stanford.edu/~rqi/pointnet/)\n- arxiv: [https://arxiv.org/abs/1612.00593](https://arxiv.org/abs/1612.00593)\n- github: [https://github.com/charlesq34/pointnet](https://github.com/charlesq34/pointnet)\n\n**DA-RNN: Semantic Mapping with Data Associated Recurrent Neural Networks**\n\n[https://arxiv.org/abs/1703.03098](https://arxiv.org/abs/1703.03098)\n\n**SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud**\n\n- intro: UC Berkeley\n- arxiv: [https://arxiv.org/abs/1710.07368](https://arxiv.org/abs/1710.07368)\n\n**SEGCloud: Semantic Segmentation of 3D Point Clouds**\n\n- intro: International Conference of 3D Vision (3DV) 2017 (Spotlight). Stanford University\n- homepage: [http://segcloud.stanford.edu/](http://segcloud.stanford.edu/)\n- arxiv: [https://arxiv.org/abs/1710.07563](https://arxiv.org/abs/1710.07563)\n\n**3D Instance Segmentation via Multi-task Metric Learning**\n\n- intro: KAUST & ETH Zurich\n- arxiv: [https://arxiv.org/abs/1906.08650](https://arxiv.org/abs/1906.08650)\n\n**3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation**\n\n- intro: RWTH Aachen University & Google & Technical University Munich\n- project page: [https://www.vision.rwth-aachen.de/publication/00199/](https://www.vision.rwth-aachen.de/publication/00199/)\n- arxiv: [https://arxiv.org/abs/2003.13867](https://arxiv.org/abs/2003.13867)\n\n**PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2004.01658](https://arxiv.org/abs/2004.01658)\n\n# Line Parsing\n\n**Fully Convolutional Line Parsing**\n\n- intro: ICCV 2021\n- intro: UESTC & UC Berkeley\n- arxiv: [https://arxiv.org/abs/2104.11207](https://arxiv.org/abs/2104.11207)\n- github(PyTorch): [https://github.com/Delay-Xili/F-Clip](https://github.com/Delay-Xili/F-Clip)\n\n# Projects\n\n**TF Image Segmentation: Image Segmentation framework**\n\n- intro: Image Segmentation framework based on Tensorflow and TF-Slim library\n- github: [https://github.com/warmspringwinds/tf-image-segmentation](https://github.com/warmspringwinds/tf-image-segmentation)\n\n**KittiSeg: A Kitti Road Segmentation model implemented in tensorflow.**\n\n- keywords: MultiNet\n- intro: KittiSeg performs segmentation of roads by utilizing an FCN based model.\n- github: [https://github.com/MarvinTeichmann/KittiBox](https://github.com/MarvinTeichmann/KittiBox)\n\n**Semantic Segmentation Architectures Implemented in PyTorch**\n\n- intro: Segnet/FCN/U-Net/Link-Net\n- github: [https://github.com/meetshah1995/pytorch-semseg](https://github.com/meetshah1995/pytorch-semseg)\n\n**PyTorch for Semantic Segmentation**\n\n[https://github.com/ZijunDeng/pytorch-semantic-segmentation](https://github.com/ZijunDeng/pytorch-semantic-segmentation)\n\n**LightNet: Light-weight Networks for Semantic Image Segmentation**\n\n- project page: [https://ansleliu.github.io/LightNet.html](https://ansleliu.github.io/LightNet.html)\n- github: [https://github.com/ansleliu/LightNet](https://github.com/ansleliu/LightNet)\n\n**LightNet++: Boosted Light-weighted Networks for Real-time Semantic Segmentation**\n\n- project page: [https://ansleliu.github.io/LightNet.html](https://ansleliu.github.io/LightNet.html)\n- github: [https://github.com/ansleliu/LightNetPlusPlus](https://github.com/ansleliu/LightNetPlusPlus)\n\n# Leaderboard\n\n**Segmentation Results: VOC2012 BETA: Competition \"comp6\" (train on own data)**\n\n[http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=6](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&challengeid=11&compid=6)\n\n# Blogs\n\n**Mobile Real-time Video Segmentation**\n\n[https://research.googleblog.com/2018/03/mobile-real-time-video-segmentation.html](https://research.googleblog.com/2018/03/mobile-real-time-video-segmentation.html)\n\n**Deep Learning for Natural Image Segmentation Priors**\n\n[http://cs.brown.edu/courses/csci2951-t/finals/ghope/](http://cs.brown.edu/courses/csci2951-t/finals/ghope/)\n\n**Image Segmentation Using DIGITS 5**\n\n[https://devblogs.nvidia.com/parallelforall/image-segmentation-using-digits-5/](https://devblogs.nvidia.com/parallelforall/image-segmentation-using-digits-5/)\n\n**Image Segmentation with Tensorflow using CNNs and Conditional Random Fields**\n[http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/18/image-segmentation-with-tensorflow-using-cnns-and-conditional-random-fields/](http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/18/image-segmentation-with-tensorflow-using-cnns-and-conditional-random-fields/)\n\n**Fully Convolutional Networks (FCNs) for Image Segmentation**\n\n- blog: [http://warmspringwinds.github.io/tensorflow/tf-slim/2017/01/23/fully-convolutional-networks-(fcns)-for-image-segmentation/](http://warmspringwinds.github.io/tensorflow/tf-slim/2017/01/23/fully-convolutional-networks-(fcns)-for-image-segmentation/)\n- ipn: [https://github.com/warmspringwinds/tensorflow_notes/blob/master/fully_convolutional_networks.ipynb](https://github.com/warmspringwinds/tensorflow_notes/blob/master/fully_convolutional_networks.ipynb)\n\n**Image segmentation with Neural Net**\n\n- blog: [https://medium.com/@m.zaradzki/image-segmentation-with-neural-net-d5094d571b1e#.s5f711g1q](https://medium.com/@m.zaradzki/image-segmentation-with-neural-net-d5094d571b1e#.s5f711g1q)\n- github: [https://github.com/mzaradzki/neuralnets/tree/master/vgg_segmentation_keras](https://github.com/mzaradzki/neuralnets/tree/master/vgg_segmentation_keras)\n\n**A 2017 Guide to Semantic Segmentation with Deep Learning**\n\n[http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review)\n\n# Tutorails / Talks\n\n**A Unified Architecture for Instance and Semantic Segmentation**\n\n- intro: FPN\n- slides: [http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf](http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf)\n\n**Deep learning for image segmentation**\n\n- intro: PyData Warsaw - Mateusz Opala & Michał Jamroż\n- youtube: [https://www.youtube.com/watch?v=W6r_a5crqGI](https://www.youtube.com/watch?v=W6r_a5crqGI)\n"},{"fields":{"slug":"/placeholder/","title":"This Is a Placeholder File for Mdx"},"frontmatter":{"draft":true},"rawBody":"---\ntitle: This Is a Placeholder File for Mdx\ndraft: true\ntags:\n  - gatsby-theme-primer-wiki-placeholder\n---\n"}]}}}