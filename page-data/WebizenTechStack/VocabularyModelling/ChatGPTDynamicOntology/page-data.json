{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/WebizenTechStack/VocabularyModelling/ChatGPTDynamicOntology/",
    "result": {"data":{"mdx":{"id":"aef21189-e9c4-5d7c-8d6b-a9c941a3f5ad","tableOfContents":{"items":[{"url":"#dynamic-ontology","title":"Dynamic Ontology","items":[{"url":"#chatgpt-interactive-notes","title":"ChatGPT Interactive Notes"}]}]},"fields":{"title":"Dynamic Ontology","slug":"/WebizenTechStack/VocabularyModelling/ChatGPTDynamicOntology/","url":"https://devdocs.webizen.org/WebizenTechStack/VocabularyModelling/ChatGPTDynamicOntology/","editUrl":"https://github.com/webizenai/devdocs/tree/main/WebizenTechStack/VocabularyModelling/ChatGPTDynamicOntology.md","lastUpdatedAt":"2023-01-20T22:14:02.000Z","lastUpdated":"1/20/2023","gitCreatedAt":"2023-01-20T22:14:02.000Z","shouldShowTitle":false},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":[],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"dynamic-ontology\"\n  }, \"Dynamic Ontology\"), mdx(\"p\", null, \"The below text is sourced via the Q&A with ChatGPT to figure out how to create a dynamic ontology solution.  What i'm then doing, is going through and making notes in interleave. \"), mdx(\"p\", null, \"The solution is not intended to be operated by ChatGPT  - rather, this is intended to be a root resource employed locally on systems. \"), mdx(\"h2\", {\n    \"id\": \"chatgpt-interactive-notes\"\n  }, \"ChatGPT Interactive Notes\"), mdx(\"p\", null, \"I am working on a project that seeks to create innovative semantic web related technology. I don't like OWL being used for human beings and as a means to describe parts of our biosphere that is not property of a company, but rather something that is of earth. To address this problem, i am trying to figure out how to create a machine-learning training program, that would be provided an english language dictionary, including information about the use of words in the context of them being used as a NOUN, PRONOUN, VERB, ADJECTIVE, ADVERB, PREPOSITION, CONJUNCTION or INTERJECTION and related descriptions of the definition provided using the english language. The program would process this information and provide a model that can be used for the purpose of defining a top-level ontology that can be used with RDF and RDFS for the creation of RDF ontologies. How can this be achieved?\"), mdx(\"p\", null, \"ChatGPT:\"), mdx(\"p\", null, \"There are several ways to approach this problem of creating a machine-learning training program that can process an English language dictionary and provide a model for defining a top-level ontology that can be used with RDF and RDFS. One approach would be to use natural language processing (NLP) techniques to automatically extract information about the usage and meaning of words in the dictionary.\"), mdx(\"p\", null, \"One specific method to achieve this would be to use a supervised machine learning algorithm, such as a neural network. This would involve training the algorithm on a large dataset of labeled examples of text, where the label is the part of speech (e.g. noun, verb, adjective) and its definition. The algorithm would learn to predict the part of speech and definition based on the input text.\"), mdx(\"p\", null, \"Another approach would be to use unsupervised learning methods such as topic modeling, where the algorithm would be able to identify the latent topics represented by the words and its definition.\"), mdx(\"p\", null, \"Additionally, it's important to note that this is a big task and it would require a lot of computational power and time to process the data, and might require a team of experts in the field of NLP and machine learning to implement it.\"), mdx(\"p\", null, \"Once you have the model, it can be used to automatically classify new words and phrases and generate the top-level ontology for RDF and RDFS. This would allow you to automatically generate RDF ontologies that are more accurate and comprehensive than those created by manual annotation.\"), mdx(\"p\", null, \"Another approach to achieve this would be to use a combination of rule-based and machine learning methods. In this approach, a set of rules would be manually defined to extract information about the usage and meaning of words in the dictionary. These rules would be based on the grammar and syntax of the English language, and would be used to identify and classify words based on their part of speech and definition.\"), mdx(\"p\", null, \"The rule-based system would be used as a pre-processing step to extract the information, and then a machine learning model would be trained on this pre-processed data to improve the accuracy of the classification.\"), mdx(\"p\", null, \"It's also worth mentioning that it's important to have a large, diverse and representative dataset to train the model on, so that the model can generalize well to unseen data.\"), mdx(\"p\", null, \"Additionally, you may want to consider using pre-trained language models like BERT, RoBERTa, GPT-3, etc, that have been trained on large amounts of text data and can be fine-tuned to perform the specific task of part-of-speech tagging and definition extraction.\"), mdx(\"p\", null, \"Once you have the model, it can be used to automatically classify new words and phrases and generate the top-level ontology for RDF and RDFS. This would allow you to automatically generate RDF ontologies that are more accurate and comprehensive than those created by manual annotation.\"), mdx(\"p\", null, \"Another approach to achieve this would be to use a combination of rule-based and machine learning methods. In this approach, a set of rules would be manually defined to extract information about the usage and meaning of words in the dictionary. These rules would be based on the grammar and syntax of the English language, and would be used to identify and classify words based on their part of speech and definition.\"), mdx(\"p\", null, \"The rule-based system would be used as a pre-processing step to extract the information, and then a machine learning model would be trained on this pre-processed data to improve the accuracy of the classification.\"), mdx(\"p\", null, \"It's also worth mentioning that it's important to have a large, diverse and representative dataset to train the model on, so that the model can generalize well to unseen data.\"), mdx(\"p\", null, \"Additionally, you may want to consider using pre-trained language models like BERT, RoBERTa, GPT-3, etc, that have been trained on large amounts of text data and can be fine-tuned to perform the specific task of part-of-speech tagging and definition extraction.\"), mdx(\"p\", null, \"Once you have the model, it can be used to automatically classify new words and phrases and generate the top-level ontology for RDF and RDFS. This would allow you to automatically generate RDF ontologies that are more accurate and comprehensive than those created by manual annotation.\"), mdx(\"p\", null, \"It's also important to note that once you have the top level ontology, It's important to validate the ontology by testing it on a sample data set, and check for any errors, inconsistencies or missing information, and make adjustments accordingly.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"my Question: Can the English Vocabulary be expressed mathematically?\"))), mdx(\"p\", null, \"The English vocabulary can be expressed mathematically to some extent, but it's important to note that natural language is a complex phenomenon and not all aspects of it can be fully captured by mathematical representation.\"), mdx(\"p\", null, \"One way to express the English vocabulary mathematically is by using mathematical models such as probability and statistics. For example, it's possible to use probability distributions to model the likelihood of different words appearing in a given context, and to use statistical methods to analyze the relationships between different words and their meanings.\"), mdx(\"p\", null, \"Another way to express the English vocabulary mathematically is by using formal language theory, which is a branch of theoretical computer science. Formal languages are a mathematical abstraction of natural languages and can be described using mathematical notation, such as regular expressions, context-free grammars, and formal languages. These formal languages provide a mathematical framework for describing the syntax and structure of natural languages.\"), mdx(\"p\", null, \"Additionally, it's possible to use mathematical models such as vector spaces and tensor representations to represent words and phrases, and to use mathematical operations such as matrix multiplication and dot product to perform operations such as similarity, analogy, and relationship between words.\"), mdx(\"p\", null, \"In summary, while it's possible to express the English vocabulary mathematically to some extent, natural language is a complex phenomenon and not all aspects of it can be fully captured by mathematical representation, and it's important to use a combination of mathematical and computational models to represent natural languages.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My Question: What are the ontological rules for the english language?\"))), mdx(\"p\", null, \"Ontological rules are guidelines for organizing and structuring knowledge in a specific domain. In the context of the English language, ontological rules would be guidelines for how to organize and structure the vocabulary and grammar of the language in a consistent and logical way.\"), mdx(\"p\", null, \"There are several different ontological frameworks that have been proposed for the English language. Some examples include:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://wordnet-rdf.princeton.edu/\"\n  }, \"WordNet\"), \": This is a lexical database that organizes English words into sets of synonyms (synsets) and describes the relationships between them. WordNet provides a hierarchical structure for the English vocabulary, with nouns, verbs, adjectives, and adverbs arranged in a tree-like structure.\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://framenet.icsi.berkeley.edu/fndrupal/\"\n  }, \"FrameNet\"), \": This is a lexical database that organizes English words into frames, which are sets of words that are related to a specific concept or event. FrameNet provides a way to organize the English vocabulary based on the roles that words play in different situations.\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Cyc\"\n  }, \"Cyc\"), \": This is a large-scale ontology that organizes knowledge about the world in a hierarchical and logical way. Cyc includes a large number of concepts and relations, and provides a framework for organizing the English vocabulary in a consistent and logical way.\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.ontologyportal.org/\"\n  }, \"SUMO\"), \": This is a large-scale ontology that organizes knowledge in a hierarchical and logical way. SUMO provides a framework for organizing the English vocabulary in a consistent and logical way, and includes a large number of concepts and relations.\"))), mdx(\"p\", null, \"\", mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"/95680cbb7bd85236ea24bc6f677c79a6/SUMOMILO.gif\",\n    \"title\": \"SUMOMILO.gif\"\n  }), \"\\nImageSource: \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.ontologyportal.org/\"\n  }, \"Ontology Portal\")), mdx(\"ol\", {\n    \"start\": 5\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"OntoWordNet: This is an ontology that is based on WordNet, but it provides a more formal and logical structure for the English vocabulary. It organizes words in a hierarchical structure and provides formal definitions and axioms for the concepts represented by the words.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"The General English Ontology (GEO): This is an ontology that provides a general structure for organizing the English vocabulary in a consistent and logical way. The GEO provides a high-level structure for organizing concepts and relationships, and it has been used in a variety of NLP and AI applications.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"The OpenCyc: An open-source version of the Cyc ontology, it's a large-scale ontology that organizes knowledge about the world in a hierarchical and logical way. It includes a large number of concepts and relations, and provides a framework for organizing the English vocabulary in a consistent and logical way.\")), mdx(\"p\", null, \"It's important to note that these ontologies and frameworks are not mutually exclusive and they can work together in a complementary way. Additionally, these ontologies are continuously being updated and improved by the community of researchers and practitioners.\"), mdx(\"p\", null, \"In summary, ontological rules for the English language are guidelines for organizing and structuring the vocabulary and grammar of the language in a consistent and logical way. There are different ontological frameworks that have been proposed for the English language, such as WordNet, FrameNet, Cyc, SUMO, OntoWordNet, GEO, and OpenCyc, each with its own strengths and weaknesses and they are continuously being updated and improved by the community.\"), mdx(\"p\", null, \"It's worth noting that these ontologies were created by experts in the field, and they are widely used in NLP and AI applications.\"), mdx(\"p\", null, \"In summary, ontological rules for the English language are guidelines for organizing and structuring the vocabulary and grammar of the language in a consistent and logical way, and there are different ontological frameworks that have been proposed for the English language such as WordNet, FrameNet, Cyc, SUMO, etc.\"), mdx(\"p\", null, \"NOTES:\\nOntoWordNet LINKS\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lists.w3.org/Archives/Public/public-swbp-wg/2005Feb/0066.html\"\n  }, \"https://lists.w3.org/Archives/Public/public-swbp-wg/2005Feb/0066.html\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.w3.org/2001/sw/BestPractices/WNET/\"\n  }, \"https://www.w3.org/2001/sw/BestPractices/WNET/\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://wordnet-rdf.princeton.edu/\"\n  }, \"http://wordnet-rdf.princeton.edu/\"), \"\\nOther links\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://babelnet.org/\"\n  }, \"https://babelnet.org/\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/ontologyportal/sumo\"\n  }, \"https://github.com/ontologyportal/sumo\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://wordnet-rdf.princeton.edu/about\"\n  }, \"http://wordnet-rdf.princeton.edu/about\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://wordnet-rdf.princeton.edu/\"\n  }, \"http://wordnet-rdf.princeton.edu/\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://old.datahub.io/dataset/opencyc\"\n  }, \"https://old.datahub.io/dataset/opencyc\"), \" is unavailable;  a version of it has been found: \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/asanchez75/opencyc/blob/master/opencyc-latest.owl.gz\"\n  }, \"https://github.com/asanchez75/opencyc/blob/master/opencyc-latest.owl.gz\"), \" \"), mdx(\"p\", null, \"noteAlso: \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.wordsapi.com/\"\n  }, \"https://www.wordsapi.com/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My Question:\\nnote; my objective is to specifically seek to maintain the proper use of terms defined using english words. This is distinct to the creation of software that might seek to support NLP related processes for sentences and broader documents. Now Therefore; When defining a high-level schema to describe the logical use of an english term or a group of english terms that are used in camelCase to define an ontological class, subclass, property, relation or datatype; what terms should be used to define the logical use of english terms for the purpose of defining an RDFS ontology document?\"))), mdx(\"p\", null, \"ChatGPT Answer:\"), mdx(\"p\", null, \"When defining a high-level schema to describe the logical use of English terms for the purpose of defining an RDFS ontology document, it's important to use terms that are clear, consistent, and aligned with existing ontologies and standards.\"), mdx(\"p\", null, \"Here are some terms that you may consider using when defining the schema:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Class: A class is a category of things or concepts that share common characteristics. In the context of an RDFS ontology, classes are used to group together resources that have similar properties and relationships.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Subclass: A subclass is a category of things or concepts that are more specific than a general class. In the context of an RDFS ontology, subclasses are used to group together resources that have more specific properties and relationships than a general class.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Property: A property is a characteristic or attribute of a class or individual resource. In the context of an RDFS ontology, properties are used to describe the relationships between classes and individuals.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Relation: A relation is a connection or association between classes or individuals. In the context of an RDFS ontology, relations are used to describe the relationships between classes and individuals.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Datatype: A datatype is a specific type of data, such as a string, integer, or date. In the context of an RDFS ontology, datatypes are used to specify the type of data that a property can hold.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"URI: A URI (Uniform Resource Identifier) is a string of characters that identify a name or a resource on the Internet. In the context of an RDFS ontology, URIs are used to identify and reference classes, properties, and individuals in the ontology.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"RDFS: RDF Schema is a set of classes and properties that provide a vocabulary for expressing constraints on RDF statements.\")), mdx(\"p\", null, \"It's also important to note that when defining the schema, you should use camelCase to define the ontological class, subclass, property, relation, or datatype, as it's one of the conventions followed in RDFS ontologies.\"), mdx(\"p\", null, \"In summary, when defining a high-level schema to describe the logical use of English terms for the purpose of defining an RDFS ontology document, it's important to use terms such as Class, Subclass, Property, Relation, Datatype, URI, RDFS and to use camelCase to define the ontological class, subclass, property, relation, or datatype.\"), mdx(\"p\", null, \"Another important aspect to consider when defining the high-level schema is to align it with existing ontologies and standards. This will make it easier to integrate the ontology with other systems and to ensure that it is consistent with the existing knowledge representation practices.\"), mdx(\"p\", null, \"Some examples of existing ontologies and standards that you may consider aligning your schema with include:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"OWL (Web Ontology Language): OWL is a standard for representing ontologies on the web. It provides a rich set of constructs for describing classes, properties, individuals, and relationships.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"RDF (Resource Description Framework): RDF is a standard for representing information on the web. It provides a way to describe resources and their relationships in a machine-readable format.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"RDFS (RDF Schema): RDFS is a standard for defining vocabularies and constraints on RDF statements. It provides a set of classes and properties that can be used to define the structure of an ontology.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"SKOS (Simple Knowledge Organization System): SKOS is a standard for representing knowledge organization systems, such as thesauri, taxonomies, and other types of controlled vocabularies.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"DC (Dublin Core): DC is a set of 15 properties that can be used to describe resources in a simple, standard way.\")), mdx(\"p\", null, \"By aligning the schema with existing ontologies and standards, you can ensure that the ontology is consistent with the existing knowledge representation practices and it can be easily integrated with other systems.\"), mdx(\"p\", null, \"In summary, when defining a high-level schema to describe the logical use of English terms for the purpose of defining an RDFS ontology document, it's important to use terms such as Class, Subclass, Property, Relation, Datatype, URI, RDFS and to use camelCase to define the ontological class, subclass, property, relation, or datatype. Additionally, aligning the schema with existing ontologies and standards can ensure that the ontology is consistent with the existing knowledge representation practices and it can be easily integrated with other systems.\"), mdx(\"p\", null, \"The only existing ontologies and standards that i want to align the method with is RDF and RDFS. In this case, describe an ontology that provides the constructs for describing the logic of an english dictionary in N3\"), mdx(\"p\", null, \"An ontology that provides the constructs for describing the logic of an English dictionary in N3 (Notation3) using RDF and RDFS could be organized as follows:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Classes: The ontology would define classes for different types of words, such as nouns, verbs, adjectives, and adverbs. These classes would be used to group together resources that have similar properties and relationships.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Properties: The ontology would define properties for describing the characteristics of words, such as their part of speech, definition, and examples of usage. These properties would be used to describe the relationships between classes and individuals.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Relations: The ontology would define relations for describing the connections or associations between words, such as synonyms and antonyms. These relations would be used to describe the relationships between classes and individuals.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Individuals: The ontology would define individuals for each word in the dictionary, with each individual having properties such as part of speech, definition, and examples of usage.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"RDFS: The ontology would use RDFS to define the class hierarchy, properties and restrictions.\")), mdx(\"p\", null, \"Here is an example of how a word \\\"Apple\\\" could be represented in N3 using RDF and RDFS:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"@prefix rdf: [http://www.w3.org/1999/02/22-rdf-syntax-ns#](http://www.w3.org/1999/02/22-rdf-syntax-ns#) . @prefix rdfs: [http://www.w3.org/2000/01/rdf-schema#](http://www.w3.org/2000/01/rdf-schema#) . @prefix dic: [http://example.com/dictionary#](http://example.com/dictionary#) .\\n\\n\")), mdx(\"p\", null, \"dic:Apple a dic:Noun ; rdfs:label \\\"Apple\\\" ; dic:definition \\\"a round fruit with a red, green or yellow skin and a core of small seeds in the middle\\\" ; dic:example \\\"An apple a day keeps the doctor away.\\\" .\"), mdx(\"p\", null, \"In this example, the class \\\"Noun\\\" is defined as a subclass of rdfs:Class, the individual \\\"Apple\\\" is defined as an instance of the class \\\"Noun\\\", and it has properties such as definition and example of usage. The prefix dic: is used to identify the namespace of the ontology, and the properties and classes are defined using the dic: namespace.\"), mdx(\"p\", null, \"Additionally, the ontology could include additional classes and properties to represent more specific information about the words, such as the tense of verbs, singular and plural forms of nouns, comparative and superlative forms of adjectives, etc.\"), mdx(\"p\", null, \"It's also worth noting that, in order to make the ontology more expressive, it can use some additional RDFS constructs such as RDFS:subClassOf, RDFS:domain, RDFS:range, RDFS:subPropertyOf, etc.\"), mdx(\"p\", null, \"Additionally, it's important to note that RDF and RDFS are quite flexible and can be used to represent a wide range of different ontologies and vocabularies. Therefore, you may want to consider consulting existing ontologies and vocabularies, such as WordNet, FrameNet, and others, to identify the best practices and common patterns for representing the logic of an English dictionary.\"), mdx(\"p\", null, \"In summary, an ontology that provides the constructs for describing the logic of an English dictionary in N3 using RDF and RDFS could be organized by defining classes for different types of words, properties for describing the characteristics of words, relations for describing the connections between words, individuals for each word in the dictionary, and using RDFS to define the class hierarchy, properties, and restrictions. Additionally, it can use additional RDFS constructs and also aligning with existing ontologies and standards can help to make the ontology more expressive and consistent.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My Question: provide a code example in turtle\"))), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-turtle\"\n  }, \"@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n@prefix dic: <http://example.com/dictionary#> .\\n\\ndic:Word rdfs:subClassOf rdfs:Resource .\\ndic:Noun rdfs:subClassOf dic:Word .\\ndic:Verb rdfs:subClassOf dic:Word .\\ndic:Adjective rdfs:subClassOf dic:Word .\\ndic:Adverb rdfs:subClassOf dic:Word .\\n\\ndic:partOfSpeech rdfs:domain dic:Word ;\\n                  rdfs:range rdfs:Literal .\\ndic:definition rdfs:domain dic:Word ;\\n                rdfs:range rdfs:Literal .\\ndic:example rdfs:domain dic:Word ;\\n                rdfs:range rdfs:Literal .\\n\\ndic:synonym rdfs:domain dic:Word ;\\n                rdfs:range dic:Word .\\n\\ndic:Apple a dic:Noun ;\\n\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My Question:\\nhow can multiple prefixes for the same ontological asset be defined so that the ontology document can be provided over multiple protocols (ie: IPFS, WebTorrent, etc) whilst providing the semantic RDF RDFS requirements to define that the document URIs are referring to the same document?\"))), mdx(\"p\", null, \"ChatGPT Response:\"), mdx(\"p\", null, \"When defining an ontology that will be provided over multiple protocols, such as IPFS and WebTorrent, it's important to ensure that the URIs used to reference the ontology document are consistent across all protocols. One way to achieve this is to use multiple prefixes for the same ontological asset, and to define them in such a way that they all refer to the same document.\"), mdx(\"p\", null, \"Here are a few ways to achieve this:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Use a single prefix and different URI schemes: You can use a single prefix and define different URI schemes for the same ontological asset. For example, you can use the prefix \\\"dic:\\\" to refer to the ontology, and then define the URIs as \\\"dic:Apple\\\" for the web, \\\"ipfs:Qm1234567890\\\" for IPFS, and \\\"webtorrent:abcdefgh\\\" for WebTorrent.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Use multiple prefixes and same URI scheme: You can use multiple prefixes for the same ontological asset, and define them to use the same URI scheme. For example, you can use the prefix \\\"dic:\\\" for the web, \\\"ipfs_dic:\\\" for IPFS, and \\\"webtorrent_dic:\\\" for WebTorrent and then define the URIs as \\\"dic:Apple\\\" for the web, \\\"ipfs_dic:Apple\\\" for IPFS, and \\\"webtorrent_dic:Apple\\\" for WebTorrent.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Linked Data Fragments (LDF) : It's a set of conventions that allows to define multiple URIs for the same resource, which can be identified by different access methods. With LDF, you can use the same URI as the representation of the resource, and it will redirect to the appropriate access method.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Use 303 redirection: Use the HTTP 303 status code to redirect requests for the ontology to the appropriate location, depending on the protocol being used. This allows you to use a single URI for the ontology, but redirect requests to the appropriate location based on the protocol being used.\")), mdx(\"p\", null, \"In summary, when defining an ontology that will be provided over multiple protocols, it's important to ensure that the URIs used to reference the ontology document are consistent across all protocols. This can be achieved by using a single prefix and different URI schemes, using multiple prefixes and same URI scheme, using Linked Data Fragments, or using HTTP 303 redirection.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My question: Provide an example in turtle\"))), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-turtle\"\n  }, \"\\n@prefix dic: <http://example.com/dictionary#> .\\n@prefix ipfs_dic: <ipfs:Qm1234567890/dictionary#> .\\n@prefix webtorrent_dic: <webtorrent:abcdefgh/dictionary#> .\\n\\ndic:Apple a dic:Noun ;\\n    rdfs:label \\\"Apple\\\" ;\\n    dic:definition \\\"a round fruit with a red, green or yellow skin and a core of small seeds in the middle\\\" ;\\n    dic:example \\\"An apple a day keeps the doctor away.\\\" .\\n\\nipfs_dic:Apple owl:sameAs dic:Apple .\\nwebtorrent_dic:Apple owl:sameAs dic:Apple .\\n\\n\")), mdx(\"p\", null, \"In this example, the same class \\\"Apple\\\" is defined using three different prefixes \\\"dic:\\\", \\\"ipfs_dic:\\\", and \\\"webtorrent_dic:\\\", but they refer to the same resource. Then we use the owl:sameAs property to indicate that those resources are the same.\"), mdx(\"p\", null, \"This way, when the ontology is accessed over IPFS, the URI \\\"ipfs_dic:Apple\\\" can be used to reference the class, and when accessed over WebTorrent, the URI \\\"webtorrent_dic:Apple\\\" can be used instead. And, when accessed over the web, the URI \\\"dic:Apple\\\" can be used.\"), mdx(\"p\", null, \"It's also worth noting that, when using the owl:sameAs property, the ontology will be consistent with OWL 2 standard and it will be able to indicate that resources are identical.\"), mdx(\"p\", null, \"Keep in mind that this is an example, and the URIs used in the example are not valid and it's just for illustration purposes.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My Question:\\nwhat is a solution that provides the ability to define an ontology document as a version that relates to a collection of ontology documents about the same topic. For example; when seeking to describe a law or legislation, the law may change and the ontology document may be only relavent within a particular period of time. So, in turtle, i want to describe a topic collection (ie: a legislative instrument) and relate versions as described in RDF to specific periods of time and provide an ontological reference to the latest version of that document. How could this be achieved? Provide a solution in turtle\"))), mdx(\"p\", null, \"ChatGPT Answer:\\nIn order to define an ontology document as a version that relates to a collection of ontology documents about the same topic, such as a legislative instrument, you can use the RDF and RDFS constructs to describe the topic collection and relate versions to specific periods of time. Here's an example of how this could be achieved in Turtle:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-Turtle\"\n  }, \"@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n@prefix law: <http://example.com/legislation#> .\\n@prefix time: <http://www.w3.org/2006/time#> .\\n\\nlaw:LegislativeInstrument rdfs:subClassOf rdfs:Resource .\\nlaw:Version rdfs:subClassOf rdfs:Resource .\\n\\nlaw:hasVersion rdfs:domain law:LegislativeInstrument ;\\n               rdfs:range law:Version .\\n\\nlaw:validFrom rdfs:domain law:Version ;\\n               rdfs:range time:Instant .\\n\\nlaw:validTo rdfs:domain law:Version ;\\n               rdfs:range time:Instant .\\n\\nlaw:latestVersion rdfs:domain law:LegislativeInstrument ;\\n               rdfs:range law:Version .\\n\\nlaw:LegislativeInstrument1 a law:LegislativeInstrument ;\\n    rdfs:label \\\"Legislative Instrument 1\\\" ;\\n    law:hasVersion law:Version1, law:Version2, law:Version3 .\\n\\nlaw:Version1 a law:Version ;\\n    rdfs:label \\\"Version 1\\\" ;\\n    law:validFrom \\\"2022-01-01T00:00:00Z\\\"^^time:Instant ;\\n    law:validTo \\\"2022-12-31T23:59:59Z\\\"^^time:Instant .\\n\\nlaw:Version2 a law:Version ;\\n    rdfs:label \\\"Version 2\\\" ;\\n    law:validFrom \\\"2023-01-01T00:00:00Z\\\"^^time:Instant ;\\n    law:validTo \\\"2023-12-31T23:59:59Z\\\"^^time:Instant .\\n\\nlaw:Version3 a law:Version ;\\n    rdfs:label \\\"Version 3\\\" ;\\n    law:validFrom \\\"2024-01-01T00:00:00Z\\\"^^time:Instant ;\\n    law:validTo \\\"2024-12-31T23:59:59Z\\\"^^time:Instant .\\n\\nlaw:LegislativeInstrument1 law:latestVersion law:Version3 .\\n\\n\")), mdx(\"p\", null, \"In this example, the class \\\"LegislativeInstrument\\\" is defined as a subclass of \\\"Resource\\\", and the class \\\"Version\\\" is defined as a subclass of \\\"Resource\\\" as well. The property \\\"hasVersion\\\" is defined to relate the \\\"LegislativeInstrument\\\" class with the \\\"Version\\\" class. The property \\\"validFrom\\\" and \\\"validTo\\\" are defined to indicate the time range when the version is valid. Finally, the property \\\"latestVersion\\\" is used to indicate the latest version of the legislative instrument.\"), mdx(\"p\", null, \"Here, the specific legislative instrument \\\"LegislativeInstrument1\\\" is defined with three versions \\\"Version1\\\", \\\"Version2\\\" and \\\"Version3\\\". Each version is defined with properties \\\"validFrom\\\" and \\\"validTo\\\" indicating the time range when the version is valid. The latest version \\\"Version3\\\" is indicated using the property \\\"latestVersion\\\".\"), mdx(\"p\", null, \"This way, by using the RDF and RDFS constructs, you can describe the topic collection of a legislative instrument and relate versions to specific periods of time. And also, you can specify the latest version of that document and it will be easy to identify it.\"), mdx(\"p\", null, \"It's worth noting that in this example, I've used the time ontology (time:) to define the time range but you can use other ways to represent time, such as using xsd:dateTime.\"), mdx(\"p\", null, \"It's also worth noting that this is an example, and the URIs and values used in the example are not valid and it's just for illustration purposes.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My Question:\\nhow can a cryptographic signature be generated on an RDF document to enable a parser to validate whether or not the document has been altered?\"))), mdx(\"p\", null, \"A cryptographic signature can be generated on an RDF document to enable a parser to validate whether or not the document has been altered, by using a digital signature algorithm. A digital signature algorithm, such as RSA or ECDSA, can be used to create a digital signature of the RDF document.\"), mdx(\"p\", null, \"Here's an example of how this could be achieved in Turtle:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-turtle\"\n  }, \"\\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n@prefix dc: <http://purl.org/dc/elements/1.1/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n@prefix sig: <http://example.com/signature#> .\\n\\nsig:Signature rdfs:subClassOf rdfs:Resource .\\nsig:signatureValue rdfs:domain sig:Signature ;\\n                  rdfs:range xsd:base64Binary .\\n\\ndc:creator sig:signatureValue \\\"base64 encoded signature value\\\" .\\ndc:date \\\"2022-09-20T19:20:30-05:00\\\"^^xsd:dateTime .\\n\\n\")), mdx(\"p\", null, \"In this example, we defined a new class \\\"Signature\\\" as a subclass of \\\"Resource\\\" and the property \\\"signatureValue\\\" which is defined to relate the \\\"Signature\\\" class with the value of the signature, which is represented as xsd:base64Binary. Additionally, we use dc:creator and dc:date properties to indicate the signature creator and the date when the signature was generated.\"), mdx(\"p\", null, \"This way, the parser can use the public key of the signature creator to verify the signature value, and check if the document has been altered.\"), mdx(\"p\", null, \"It's important to keep in mind that this is just an example, and in real-world scenarios, the signature should be generated using proper libraries and tools that implement the desired digital signature algorithm. And also it's important to use secure key management techniques to protect the private key.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"my Question;\\nHow do i define a cuDNN training model to process an english dictionary for the purpose of using the vocabulary to define RDF / RDFS ontologies, that do not use OWL\")), mdx(\"p\", null, \"ChatGPT Answer:\"), mdx(\"p\", null, \"The cuDNN library is a GPU-accelerated library for deep neural networks, and it can be used to train models for various tasks such as image classification, natural language processing, and speech recognition.\"), mdx(\"p\", null, \"To define a cuDNN training model to process an English dictionary for the purpose of using the vocabulary to define RDF/RDFS ontologies, you would need to:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Prepare the data: This would involve preprocessing the English dictionary to extract the vocabulary and related information such as part of speech, definition, etc. and format it into a format suitable for training a neural network.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Define the model architecture: This would involve choosing a suitable model architecture that can handle the task of vocabulary extraction, such as a recurrent neural network (RNN) or a transformer-based model.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train the model: This would involve using cuDNN to train the model on the preprocessed data, using techniques such as backpropagation and stochastic gradient descent.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Use the trained model: Once the model is trained, you can use it to extract vocabulary from new texts and use it to define RDF/RDFS ontologies.\")), mdx(\"p\", null, \"It's worth noting that creating an ontology from text is a complex task, and the final ontology will depend on the specific use case and the ontology model that you choose to represent the vocabulary. Additionally, it's also important to note that using cuDNN for this task is not the most common way, as it's a powerful library that is mostly used for deep learning tasks. There are other libraries and models that might be more suitable for this kind of task, like word2vec, GloVe, or BERT.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"My Question:\\n\", mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"provide an example of preparing hte data in golang and include the syntactic field to define \\\"parts of speech\\\" to distinguish whether a word definition relates to the use of that word in the form of a NOUN, PRONOUN, VERB, ADJECTIVE, ADVERB, PREPOSITION, CONJUNCTION or INTERJECTION.\"))), mdx(\"p\", null, \"Here's an example of how you could prepare the data in Go for a model that aims to distinguish whether a word definition relates to the use of that word in the form of a NOUN, PRONOUN, VERB, ADJECTIVE, ADVERB, PREPOSITION, CONJUNCTION or INTERJECTION:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-go\"\n  }, \"package main\\n\\nimport (\\n    \\\"encoding/json\\\"\\n    \\\"os\\\"\\n)\\n\\ntype Vocab struct {\\n    Word       string `json:\\\"word\\\"`\\n    POS        string `json:\\\"pos\\\"`\\n    Definition string `json:\\\"definition\\\"`\\n}\\n\\nfunc main() {\\n    // load the english dictionary\\n    file, _ := os.Open(\\\"english_dictionary.json\\\")\\n    defer file.Close()\\n\\n    var dictionary map[string]interface{}\\n    json.NewDecoder(file).Decode(&dictionary)\\n\\n    // create a slice to store the vocabulary and related information\\n    var vocab []Vocab\\n\\n    // iterate over the dictionary entries\\n    for word, entry := range dictionary {\\n        pos := entry.(map[string]interface{})[\\\"pos\\\"].(string)\\n        definition := entry.(map[string]interface{})[\\\"definition\\\"].(string)\\n\\n        // add the vocabulary and related information to the vocab slice\\n        vocab = append(vocab, Vocab{Word: word, POS: pos, Definition: definition})\\n    }\\n\\n    // save the vocab slice to a file for training\\n    vocabFile, _ := os.Create(\\\"vocab.json\\\")\\n    defer vocabFile.Close()\\n\\n    json.NewEncoder(vocabFile).Encode(voc\\n\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ChatGPT Question:\\nProvide me an example of how to do step 2, using that model in golang using cuDNN\")), mdx(\"p\", null, \"ChatGPT Answer\\nStep 2 of defining a cuDNN training model to process an English dictionary for the purpose of using the vocabulary to define RDF/RDFS ontologies, would involve defining the model architecture.\"), mdx(\"p\", null, \"Here's an example of how you could define a model architecture in Go using cuDNN:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-go\"\n  }, \"package main\\n\\nimport (\\n    \\\"github.com/dereklstinson/GoCUDA\\\"\\n    \\\"github.com/dereklstinson/GoCUDA/cudnn\\\"\\n)\\n\\nfunc main() {\\n    // create a cudnn handle\\n    handle, _ := cudnn.Create()\\n    defer handle.Destroy()\\n\\n    // define the model architecture\\n    // here we will use a simple feedforward neural network with one hidden layer\\n    inputTensor, _ := cudnn.CreateTensorDescriptor()\\n    defer inputTensor.Destroy()\\n    inputTensor.Set(cudnn.DataTypeFloat, []int32{batchSize, inputSize}, []int32{inputSize, 1})\\n\\n    hiddenTensor, _ := cudnn.CreateTensorDescriptor()\\n    defer hiddenTensor.Destroy()\\n    hiddenTensor.Set(cudnn.DataTypeFloat, []int32{batchSize, hiddenSize}, []int32{hiddenSize, 1})\\n\\n    outputTensor, _ := cudnn.CreateTensorDescriptor()\\n    defer outputTensor.Destroy()\\n    outputTensor.Set(cudnn.DataTypeFloat, []int32{batchSize, outputSize}, []int32{outputSize, 1})\\n\\n    filterDesc, _ := cudnn.CreateFilterDescriptor()\\n    defer\\n    filterDesc.Set(cudnn.DataTypeFloat, []int32{hiddenSize, inputSize})\\n\\n    filterData, _ := GoCUDA.Malloc(filterDesc.Size())\\n    defer filterData.Free()\\n\\n    biasDesc, _ := cudnn.CreateTensorDescriptor()\\n    defer biasDesc.Destroy()\\n    biasDesc.Set(cudnn.DataTypeFloat, []int32{hiddenSize}, []int32{hiddenSize, 1})\\n\\n    biasData, _ := GoCUDA.Malloc(biasDesc.Size())\\n    defer biasData.Free()\\n\\n    // initialize the filter and bias data with random values\\n    rand.Read(filterData.Pointer())\\n    rand.Read(biasData.Pointer())\\n\\n    // create the feedforward neural network\\n    _, _ = handle.CreateFeedForward(inputTensor, filterDesc, filterData, biasDesc, biasData, hiddenTensor, outputTensor)\\n}\\n\\n\")), mdx(\"p\", null, \"This code snippet defines a simple feedforward neural network with one hidden layer, and creates a handle to interact with cuDNN library. It creates the tensor descriptor, filter descriptor, and bias descriptor for the input, hidden, and output layers respectively. It also initializes the filter and bias data with random values using the Go's math/rand package.\"), mdx(\"p\", null, \"It's worth noting that this is just an example, and the actual model architecture will depend on the specific requirements of your use case, and it might require more complex architectures. Also, it's important to note that this example uses a library called GoCUDA which is a Go\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"My question: Provide an example of how to do step 3\")), mdx(\"p\", null, \"Step 3 of defining a cuDNN training model to process an English dictionary for the purpose of using the vocabulary to define RDF/RDFS ontologies, would involve training the model using the preprocessed data.\"), mdx(\"p\", null, \"Here's an example of how you could train the model in Go using cuDNN:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-go\"\n  }, \"package main\\n\\nimport (\\n    \\\"github.com/dereklstinson/GoCUDA\\\"\\n    \\\"github.com/dereklstinson/GoCUDA/cudnn\\\"\\n)\\n\\nfunc main() {\\n    // create a cudnn handle\\n    handle, _ := cudnn.Create()\\n    defer handle.Destroy()\\n\\n    // load the preprocessed data\\n    data, _ := GoCUDA.Malloc(dataSize)\\n    defer data.Free()\\n\\n    labels, _ := GoCUDA.Malloc(labelSize)\\n    defer labels.Free()\\n\\n    // set the data and labels\\n    // ...\\n\\n    // set the training parameters\\n    learningRate := 0.01\\n    weightDecay := 0.0005\\n    momentum := 0.9\\n\\n    // train the model\\n    _, _ = handle.Train(data, labels, learningRate, weightDecay, momentum)\\n}\\n\\n\")), mdx(\"p\", null, \"This code snippet loads the preprocessed data, which is the vocabulary and related information extracted from the English dictionary, and the corresponding labels (the part of speech) into memory, and sets the training parameters such as learning rate, weight decay, and momentum. It then uses the cuDNN library to train the model on the data.\"), mdx(\"p\", null, \"It's important to note that this is a simplified example, and the actual training process will depend on the specific requirements of your use case and the model architecture. Also, it's important to note that this example uses a library called GoCUDA which is a Go library that provides bindings for CUDA, but it doesn't provide bindings for cuDNN library.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"My question; provide an example of step 4\")), mdx(\"p\", null, \"Step 4 of defining a cuDNN training model to process an English dictionary for the purpose of using the vocabulary to define RDF/RDFS ontologies, would involve evaluating the model's performance and fine-tuning it as necessary.\"), mdx(\"p\", null, \"Here's an example of how you could evaluate the model's performance and fine-tune it in Go using cuDNN:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-go\"\n  }, \"\\npackage main\\n\\nimport (\\n    \\\"github.com/dereklstinson/GoCUDA\\\"\\n    \\\"github.com/dereklstinson/GoCUDA/cudnn\\\"\\n)\\n\\nfunc main() {\\n    // create a cudnn handle\\n    handle, _ := cudnn.Create()\\n    defer handle.Destroy()\\n\\n    // load the test data\\n    testData, _ := GoCUDA.Malloc(testDataSize)\\n    defer testData.Free()\\n\\n    testLabels, _ := GoCUDA.Malloc(testLabelSize)\\n    defer testLabels.Free()\\n\\n    // set the test data and labels\\n    // ...\\n\\n    // evaluate the model's performance on the test data\\n    accuracy, _ := handle.Evaluate(testData, testLabels)\\n\\n    // print the accuracy\\n    println(\\\"Accuracy:\\\", accuracy)\\n\\n    // fine-tune the model if necessary\\n    if accuracy < desiredAccuracy {\\n        // adjust the training parameters\\n        learningRate *= 0.1\\n        weightDecay *= 0.1\\n\\n        // retrain the model\\n        _, _ = handle.Train(data, labels, learningRate, weightDecay, momentum)\\n    }\\n}\\n\\n\")), mdx(\"p\", null, \"This code snippet loads test data and labels, and uses the cuDNN library to evaluate the model's performance on this test data. It then prints the accuracy, and if the accuracy is below a certain threshold, it adjusts the training parameters and retrains the model.\"), mdx(\"p\", null, \"It's important to note that this is a simplified example, and the actual evaluation and fine-tuning process will depend on the specific requirements of your use case and the model architecture. Also, it's important to note that this example uses a library called GoCUDA which is a Go library that provides bindings for CUDA, but it doesn't provide bindings for cuDNN library.\"), mdx(\"p\", null, \"As I mentioned earlier, there are no Go libraries that provide bindings for cuDNN, which means that using cuDNN in Go is not a straightforward task, and it would require to use CGo to interact with the cuDNN C library, and this will require a deep understanding of both Go and C programming languages.\"), mdx(\"h3\", {\n    \"id\": \"review-and-considerations\"\n  }, \"Review and considerations\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Its seemingly defined using tensorcores.  my laptop doesn't have tensor cores, only cudacores\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"I didn't know much about the existing solutions like wordnet.\")), mdx(\"h3\", {\n    \"id\": \"following-up\"\n  }, \"Following-up\"), mdx(\"p\", null, \"Overnight, i found the various existing NLP libraries that were highlighted earlier, alongside a relatively easy to use example - \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.wordsapi.com/\"\n  }, \"WordsAPI\"), \" that might be the best choice for producing a Proof of Concept to illustrate the general idea.   Yet, any overly simplified solution (as required to materialise something that demonstrates the general idea); will need an upgrade path.  Furthermore, the method of how to form a longer-term strategy is required.  \"), mdx(\"p\", null, \"It is important to note (as i've started to illustrate \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/WebizenTechStack/Webizen2.5/FunctionalObjectives/RootConcepts/\",\n    \"title\": \"RootConcepts\"\n  }, \"RootConcepts\"), \" ); that this underlying service MUST be able to be operated privately and in a stand-alone manner.  To achieve this solution, the outcome may be that the first time a topic or word is sought to be processed; an API call might be made to go and get the information that's required to be added to the database.  Presently, i'm unsure of the technical implications relating to any methods that are reasonably available to achieve that;  such as - can it run on a mobile phone or laptop?  etc.\"), mdx(\"p\", null, \"As more work continues on the topic (the next day); the below 'work' was produced,\"), mdx(\"h4\", {\n    \"id\": \"chatgpt\"\n  }, \"ChatGPT\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"My Question:\"), \" what are the root languages for the english vocabulary?\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ChatGPT Answer\"), \":\\nThe English vocabulary is made up of words borrowed from many different languages over the centuries. The main root languages of the English vocabulary are:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Germanic: The Germanic languages, including Old English, are the most important source of words in English. They make up around 50% of the words in the English language. Words from Germanic languages include common words such as \\\"house,\\\" \\\"father,\\\" \\\"mother,\\\" \\\"man,\\\" \\\"woman,\\\" \\\"child,\\\" and \\\"dog.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Latin: Latin is the second most important source of words in English. Latin words make up around 30% of the English vocabulary. Many of these words are technical terms in fields such as science, medicine, and law. Words from Latin include \\\"hospital,\\\" \\\"doctor,\\\" \\\"university,\\\" \\\"species,\\\" \\\"algebra,\\\" \\\"formula,\\\" and \\\"jurisdiction.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"French: French is the third most important source of words in English. French words make up around 10% of the English vocabulary. Many French words were borrowed during the Norman Conquest of England in 1066 and the following centuries of Norman rule. Words from French include \\\"money,\\\" \\\"art,\\\" \\\"government,\\\" \\\"justice,\\\" \\\"royalty,\\\" \\\"diamond,\\\" and \\\"restaurant.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Greek: Greek is the fourth most important source of words in English. Greek words make up around 5% of the English vocabulary. Many Greek words were borrowed through Latin, but some were borrowed directly into English. Words from Greek include \\\"philosophy,\\\" \\\"theology,\\\" \\\"anatomy,\\\" \\\"geometry,\\\" \\\"telephone,\\\" \\\"photograph,\\\" and \\\"encyclopedia.\\\"\")), mdx(\"p\", null, \"Other languages that have contributed to the English vocabulary include Arabic, Italian, Spanish, and many others.\"), mdx(\"p\", null, \"In addition to the root languages mentioned above, there are also many other languages that have contributed to the English vocabulary over time. These include:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Arabic: Many Arabic words were borrowed into English through Latin and Greek. Words from Arabic include \\\"algebra,\\\" \\\"alcohol,\\\" \\\"candy,\\\" \\\"jar,\\\" \\\"lemon,\\\" \\\"magazine,\\\" \\\"sofa,\\\" \\\"sugar,\\\" and \\\"zero.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Italian: Words from Italian include \\\"balcony,\\\" \\\"broccoli,\\\" \\\"cappuccino,\\\" \\\"fiasco,\\\" \\\"gazebo,\\\" \\\"grotto,\\\" \\\"paparazzo,\\\" \\\"piano,\\\" \\\"sonata,\\\" and \\\"terrace.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Spanish: Words from Spanish include \\\"alligator,\\\" \\\"barbecue,\\\" \\\"bronco,\\\" \\\"chocolate,\\\" \\\"coyote,\\\" \\\"guerrilla,\\\" \\\"jaguar,\\\" \\\"mosquito,\\\" \\\"mustang,\\\" \\\"patio,\\\" \\\"siesta,\\\" \\\"sombrero,\\\" \\\"taco,\\\" and \\\"tornado.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Native American languages: Words from Native American languages include \\\"chocolate,\\\" \\\"chipmunk,\\\" \\\"moose,\\\" \\\"opossum,\\\" \\\"pecan,\\\" \\\"racoon,\\\" \\\"skunk,\\\" \\\"squash,\\\" \\\"tobacco,\\\" and \\\"tomato.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Asian languages: Words from Asian languages include \\\"bamboo,\\\" \\\"guru,\\\" \\\"karma,\\\" \\\"pajama,\\\" \\\"tycoon,\\\" \\\"yoga,\\\" and \\\"zen.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"African languages: Words from African languages include \\\"banjo,\\\" \\\"jazz,\\\" \\\"okra,\\\" \\\"yam,\\\" and \\\"Zulu.\\\"\")), mdx(\"p\", null, \"It's worth noting that these are just a few examples of the many languages that have contributed to the English vocabulary over time. The English language is known for its rich history of borrowing words from other languages, and this is one of the reasons why it has such a diverse and dynamic vocabulary.\"), mdx(\"p\", null, \"My question: provide an example relating to norse and old norse \"), mdx(\"p\", null, \"Sure, here are a few examples of words that have been borrowed from Old Norse and Norse into the English language:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Anguish\\\" comes from the Old Norse word \\\"angr\\\" meaning \\\"grief\\\" or \\\"sorrow.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Berserk\\\" comes from the Old Norse word \\\"berserkr\\\" meaning \\\"a wild warrior.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Bliss\\\" comes from the Old Norse word \\\"bliss\\\" meaning \\\"happiness.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Eke\\\" means \\\"also\\\" or \\\"additionally\\\" comes from the Old Norse word \\\"eigi\\\" meaning \\\"also\\\" or \\\"in addition.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Eg\\\" meaning \\\"I\\\" from Old Norse \\\"eg\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Fjord\\\" comes from the Old Norse word \\\"fj\\xF6r\\xF0r\\\" meaning \\\"a narrow inlet of the sea.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Knurl\\\" comes from the Old Norse word \\\"knurr\\\" meaning \\\"a small knot or knob.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Mist\\\" comes from the Old Norse word \\\"mist\\\" meaning \\\"moisture.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Ransack\\\" comes from the Old Norse word \\\"rannsaka\\\" meaning \\\"to search through.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Scowl\\\" comes from the Old Norse word \\\"skul\\\" meaning \\\"a face with a frown.\\\"\")), mdx(\"p\", null, \"It is worth noting that the Norse and Old Norse languages are closely related, and the vast majority of words borrowed into English from Old Norse came directly or indirectly from Norse. Norse is also known as Old Icelandic, it is a Germanic language that was spoken by the Vikings and other inhabitants of the Norse regions of Scandinavia and the North Atlantic islands in the Middle Ages.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My question: what benefit might the use of a neuromorphic processer provide for processing a natural language model?\")), \"  (note: that was part of the question)\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ChatGPT:\"), \"\\nA neuromorphic processor is a type of computer processor that is designed to mimic the way the human brain works. These processors use artificial neural networks, which are modeled after the structure and function of the human brain, to perform tasks such as pattern recognition and decision-making.\"), mdx(\"p\", null, \"The use of a neuromorphic processor for processing a natural language model could provide several benefits.\"), mdx(\"p\", null, \"-Neuromorphic processors are highly energy-efficient, which makes them well-suited for running large-scale natural language processing models on mobile or embedded devices.\"), mdx(\"p\", null, \"-Neuromorphic processors are also highly parallel, which means they can perform multiple computations simultaneously. This can lead to faster and more efficient processing of natural language models, as well as the ability to handle larger models.\"), mdx(\"p\", null, \"-Neuromorphic processors are also highly adaptable and can learn from new data, which means they can improve their performance over time. This can be particularly useful for natural language processing, as the meaning and usage of words and phrases can change over time, and a model that can adapt to these changes will be more accurate and useful.\"), mdx(\"p\", null, \"-Neuromorphic processors can also be more robust to noise, errors and variations in the data, which can be common in natural language data.\"), mdx(\"p\", null, \"It's worth mentioning that the field of neuromorphic computing and Neuromorphic processors are relatively new and still in development, and more research is needed to fully understand their potential benefits and limitations.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"My Question: what is the earliest known time the english language was first used?\"))), mdx(\"p\", null, \"ChatGPT: \"), mdx(\"p\", null, \"The earliest known use of the English language dates back to the 5th century AD, when Germanic tribes, such as the Angles, Saxons, and Jutes, began to migrate to the island of Great Britain and settled there. These tribes spoke a Germanic language, which over time evolved into Old English.\"), mdx(\"p\", null, \"Old English, also known as Anglo-Saxon, was the language spoken in England from the 5th century to the Norman conquest in 1066 AD. Old English is considered to be the earliest form of the English language, and it was written in the runic alphabet, which was later replaced by the Latin alphabet.\"), mdx(\"p\", null, \"Old English was a complex and inflected language, with a rich vocabulary and many grammatical features. However, it was quite different from the English spoken today and would be difficult for modern speakers to understand without study or translation.\"), mdx(\"p\", null, \"It's worth mentioning that the Angles, Saxons, and Jutes were not the only people that were in Great Britain during this time, there were also the Celts, and others, and they were speaking different languages, and the Germanic language of the invaders slowly replaced the Celtic languages and others, it took several centuries to form the language we know today as English.\"));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"# Dynamic Ontology\n\nThe below text is sourced via the Q&A with ChatGPT to figure out how to create a dynamic ontology solution.  What i'm then doing, is going through and making notes in interleave. \n\nThe solution is not intended to be operated by ChatGPT  - rather, this is intended to be a root resource employed locally on systems. \n\n\n## ChatGPT Interactive Notes\n\nI am working on a project that seeks to create innovative semantic web related technology. I don't like OWL being used for human beings and as a means to describe parts of our biosphere that is not property of a company, but rather something that is of earth. To address this problem, i am trying to figure out how to create a machine-learning training program, that would be provided an english language dictionary, including information about the use of words in the context of them being used as a NOUN, PRONOUN, VERB, ADJECTIVE, ADVERB, PREPOSITION, CONJUNCTION or INTERJECTION and related descriptions of the definition provided using the english language. The program would process this information and provide a model that can be used for the purpose of defining a top-level ontology that can be used with RDF and RDFS for the creation of RDF ontologies. How can this be achieved?\n\nChatGPT:\n\nThere are several ways to approach this problem of creating a machine-learning training program that can process an English language dictionary and provide a model for defining a top-level ontology that can be used with RDF and RDFS. One approach would be to use natural language processing (NLP) techniques to automatically extract information about the usage and meaning of words in the dictionary.\n\nOne specific method to achieve this would be to use a supervised machine learning algorithm, such as a neural network. This would involve training the algorithm on a large dataset of labeled examples of text, where the label is the part of speech (e.g. noun, verb, adjective) and its definition. The algorithm would learn to predict the part of speech and definition based on the input text.\n\nAnother approach would be to use unsupervised learning methods such as topic modeling, where the algorithm would be able to identify the latent topics represented by the words and its definition.\n\nAdditionally, it's important to note that this is a big task and it would require a lot of computational power and time to process the data, and might require a team of experts in the field of NLP and machine learning to implement it.\n\nOnce you have the model, it can be used to automatically classify new words and phrases and generate the top-level ontology for RDF and RDFS. This would allow you to automatically generate RDF ontologies that are more accurate and comprehensive than those created by manual annotation.\n\nAnother approach to achieve this would be to use a combination of rule-based and machine learning methods. In this approach, a set of rules would be manually defined to extract information about the usage and meaning of words in the dictionary. These rules would be based on the grammar and syntax of the English language, and would be used to identify and classify words based on their part of speech and definition.\n\nThe rule-based system would be used as a pre-processing step to extract the information, and then a machine learning model would be trained on this pre-processed data to improve the accuracy of the classification.\n\nIt's also worth mentioning that it's important to have a large, diverse and representative dataset to train the model on, so that the model can generalize well to unseen data.\n\nAdditionally, you may want to consider using pre-trained language models like BERT, RoBERTa, GPT-3, etc, that have been trained on large amounts of text data and can be fine-tuned to perform the specific task of part-of-speech tagging and definition extraction.\n\nOnce you have the model, it can be used to automatically classify new words and phrases and generate the top-level ontology for RDF and RDFS. This would allow you to automatically generate RDF ontologies that are more accurate and comprehensive than those created by manual annotation.\n\nAnother approach to achieve this would be to use a combination of rule-based and machine learning methods. In this approach, a set of rules would be manually defined to extract information about the usage and meaning of words in the dictionary. These rules would be based on the grammar and syntax of the English language, and would be used to identify and classify words based on their part of speech and definition.\n\nThe rule-based system would be used as a pre-processing step to extract the information, and then a machine learning model would be trained on this pre-processed data to improve the accuracy of the classification.\n\nIt's also worth mentioning that it's important to have a large, diverse and representative dataset to train the model on, so that the model can generalize well to unseen data.\n\nAdditionally, you may want to consider using pre-trained language models like BERT, RoBERTa, GPT-3, etc, that have been trained on large amounts of text data and can be fine-tuned to perform the specific task of part-of-speech tagging and definition extraction.\n\nOnce you have the model, it can be used to automatically classify new words and phrases and generate the top-level ontology for RDF and RDFS. This would allow you to automatically generate RDF ontologies that are more accurate and comprehensive than those created by manual annotation.\n\nIt's also important to note that once you have the top level ontology, It's important to validate the ontology by testing it on a sample data set, and check for any errors, inconsistencies or missing information, and make adjustments accordingly.\n\n***my Question: Can the English Vocabulary be expressed mathematically?***\n\nThe English vocabulary can be expressed mathematically to some extent, but it's important to note that natural language is a complex phenomenon and not all aspects of it can be fully captured by mathematical representation.\n\nOne way to express the English vocabulary mathematically is by using mathematical models such as probability and statistics. For example, it's possible to use probability distributions to model the likelihood of different words appearing in a given context, and to use statistical methods to analyze the relationships between different words and their meanings.\n\nAnother way to express the English vocabulary mathematically is by using formal language theory, which is a branch of theoretical computer science. Formal languages are a mathematical abstraction of natural languages and can be described using mathematical notation, such as regular expressions, context-free grammars, and formal languages. These formal languages provide a mathematical framework for describing the syntax and structure of natural languages.\n\nAdditionally, it's possible to use mathematical models such as vector spaces and tensor representations to represent words and phrases, and to use mathematical operations such as matrix multiplication and dot product to perform operations such as similarity, analogy, and relationship between words.\n\nIn summary, while it's possible to express the English vocabulary mathematically to some extent, natural language is a complex phenomenon and not all aspects of it can be fully captured by mathematical representation, and it's important to use a combination of mathematical and computational models to represent natural languages.\n\n***My Question: What are the ontological rules for the english language?***\n\nOntological rules are guidelines for organizing and structuring knowledge in a specific domain. In the context of the English language, ontological rules would be guidelines for how to organize and structure the vocabulary and grammar of the language in a consistent and logical way.\n\nThere are several different ontological frameworks that have been proposed for the English language. Some examples include:\n\n1.  [WordNet](http://wordnet-rdf.princeton.edu/): This is a lexical database that organizes English words into sets of synonyms (synsets) and describes the relationships between them. WordNet provides a hierarchical structure for the English vocabulary, with nouns, verbs, adjectives, and adverbs arranged in a tree-like structure.\n    \n2.  [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/): This is a lexical database that organizes English words into frames, which are sets of words that are related to a specific concept or event. FrameNet provides a way to organize the English vocabulary based on the roles that words play in different situations.\n    \n3.  [Cyc](https://en.wikipedia.org/wiki/Cyc): This is a large-scale ontology that organizes knowledge about the world in a hierarchical and logical way. Cyc includes a large number of concepts and relations, and provides a framework for organizing the English vocabulary in a consistent and logical way.\n\n4.  [SUMO](https://www.ontologyportal.org/): This is a large-scale ontology that organizes knowledge in a hierarchical and logical way. SUMO provides a framework for organizing the English vocabulary in a consistent and logical way, and includes a large number of concepts and relations.\n\n![[SUMOMILO.gif]]\nImageSource: [Ontology Portal](https://www.ontologyportal.org/)\n\n\n5.  OntoWordNet: This is an ontology that is based on WordNet, but it provides a more formal and logical structure for the English vocabulary. It organizes words in a hierarchical structure and provides formal definitions and axioms for the concepts represented by the words.\n    \n6.  The General English Ontology (GEO): This is an ontology that provides a general structure for organizing the English vocabulary in a consistent and logical way. The GEO provides a high-level structure for organizing concepts and relationships, and it has been used in a variety of NLP and AI applications.\n    \n7.  The OpenCyc: An open-source version of the Cyc ontology, it's a large-scale ontology that organizes knowledge about the world in a hierarchical and logical way. It includes a large number of concepts and relations, and provides a framework for organizing the English vocabulary in a consistent and logical way.\n    \n\nIt's important to note that these ontologies and frameworks are not mutually exclusive and they can work together in a complementary way. Additionally, these ontologies are continuously being updated and improved by the community of researchers and practitioners.\n\nIn summary, ontological rules for the English language are guidelines for organizing and structuring the vocabulary and grammar of the language in a consistent and logical way. There are different ontological frameworks that have been proposed for the English language, such as WordNet, FrameNet, Cyc, SUMO, OntoWordNet, GEO, and OpenCyc, each with its own strengths and weaknesses and they are continuously being updated and improved by the community.\n\nIt's worth noting that these ontologies were created by experts in the field, and they are widely used in NLP and AI applications.\n\nIn summary, ontological rules for the English language are guidelines for organizing and structuring the vocabulary and grammar of the language in a consistent and logical way, and there are different ontological frameworks that have been proposed for the English language such as WordNet, FrameNet, Cyc, SUMO, etc.\n\n\nNOTES:\nOntoWordNet LINKS\nhttps://lists.w3.org/Archives/Public/public-swbp-wg/2005Feb/0066.html\nhttps://www.w3.org/2001/sw/BestPractices/WNET/\nhttp://wordnet-rdf.princeton.edu/\nOther links\nhttps://babelnet.org/\n\nhttps://github.com/ontologyportal/sumo\n\nhttp://wordnet-rdf.princeton.edu/about \nhttp://wordnet-rdf.princeton.edu/\n\nhttps://old.datahub.io/dataset/opencyc is unavailable;  a version of it has been found: https://github.com/asanchez75/opencyc/blob/master/opencyc-latest.owl.gz \n\nnoteAlso: https://www.wordsapi.com/\n\n\n\n***My Question: \nnote; my objective is to specifically seek to maintain the proper use of terms defined using english words. This is distinct to the creation of software that might seek to support NLP related processes for sentences and broader documents. Now Therefore; When defining a high-level schema to describe the logical use of an english term or a group of english terms that are used in camelCase to define an ontological class, subclass, property, relation or datatype; what terms should be used to define the logical use of english terms for the purpose of defining an RDFS ontology document?***\n\nChatGPT Answer:\n\nWhen defining a high-level schema to describe the logical use of English terms for the purpose of defining an RDFS ontology document, it's important to use terms that are clear, consistent, and aligned with existing ontologies and standards.\n\nHere are some terms that you may consider using when defining the schema:\n\n1.  Class: A class is a category of things or concepts that share common characteristics. In the context of an RDFS ontology, classes are used to group together resources that have similar properties and relationships.\n    \n2.  Subclass: A subclass is a category of things or concepts that are more specific than a general class. In the context of an RDFS ontology, subclasses are used to group together resources that have more specific properties and relationships than a general class.\n    \n3.  Property: A property is a characteristic or attribute of a class or individual resource. In the context of an RDFS ontology, properties are used to describe the relationships between classes and individuals.\n    \n4.  Relation: A relation is a connection or association between classes or individuals. In the context of an RDFS ontology, relations are used to describe the relationships between classes and individuals.\n    \n5.  Datatype: A datatype is a specific type of data, such as a string, integer, or date. In the context of an RDFS ontology, datatypes are used to specify the type of data that a property can hold.\n    \n6.  URI: A URI (Uniform Resource Identifier) is a string of characters that identify a name or a resource on the Internet. In the context of an RDFS ontology, URIs are used to identify and reference classes, properties, and individuals in the ontology.\n    \n7.  RDFS: RDF Schema is a set of classes and properties that provide a vocabulary for expressing constraints on RDF statements.\n    \n\nIt's also important to note that when defining the schema, you should use camelCase to define the ontological class, subclass, property, relation, or datatype, as it's one of the conventions followed in RDFS ontologies.\n\nIn summary, when defining a high-level schema to describe the logical use of English terms for the purpose of defining an RDFS ontology document, it's important to use terms such as Class, Subclass, Property, Relation, Datatype, URI, RDFS and to use camelCase to define the ontological class, subclass, property, relation, or datatype.\n\nAnother important aspect to consider when defining the high-level schema is to align it with existing ontologies and standards. This will make it easier to integrate the ontology with other systems and to ensure that it is consistent with the existing knowledge representation practices.\n\nSome examples of existing ontologies and standards that you may consider aligning your schema with include:\n\n-   OWL (Web Ontology Language): OWL is a standard for representing ontologies on the web. It provides a rich set of constructs for describing classes, properties, individuals, and relationships.\n    \n-   RDF (Resource Description Framework): RDF is a standard for representing information on the web. It provides a way to describe resources and their relationships in a machine-readable format.\n    \n-   RDFS (RDF Schema): RDFS is a standard for defining vocabularies and constraints on RDF statements. It provides a set of classes and properties that can be used to define the structure of an ontology.\n    \n-   SKOS (Simple Knowledge Organization System): SKOS is a standard for representing knowledge organization systems, such as thesauri, taxonomies, and other types of controlled vocabularies.\n    \n-   DC (Dublin Core): DC is a set of 15 properties that can be used to describe resources in a simple, standard way.\n    \n\nBy aligning the schema with existing ontologies and standards, you can ensure that the ontology is consistent with the existing knowledge representation practices and it can be easily integrated with other systems.\n\nIn summary, when defining a high-level schema to describe the logical use of English terms for the purpose of defining an RDFS ontology document, it's important to use terms such as Class, Subclass, Property, Relation, Datatype, URI, RDFS and to use camelCase to define the ontological class, subclass, property, relation, or datatype. Additionally, aligning the schema with existing ontologies and standards can ensure that the ontology is consistent with the existing knowledge representation practices and it can be easily integrated with other systems.\n\nThe only existing ontologies and standards that i want to align the method with is RDF and RDFS. In this case, describe an ontology that provides the constructs for describing the logic of an english dictionary in N3\n\n\nAn ontology that provides the constructs for describing the logic of an English dictionary in N3 (Notation3) using RDF and RDFS could be organized as follows:\n\n1.  Classes: The ontology would define classes for different types of words, such as nouns, verbs, adjectives, and adverbs. These classes would be used to group together resources that have similar properties and relationships.\n    \n2.  Properties: The ontology would define properties for describing the characteristics of words, such as their part of speech, definition, and examples of usage. These properties would be used to describe the relationships between classes and individuals.\n    \n3.  Relations: The ontology would define relations for describing the connections or associations between words, such as synonyms and antonyms. These relations would be used to describe the relationships between classes and individuals.\n    \n4.  Individuals: The ontology would define individuals for each word in the dictionary, with each individual having properties such as part of speech, definition, and examples of usage.\n    \n5.  RDFS: The ontology would use RDFS to define the class hierarchy, properties and restrictions.\n    \n\nHere is an example of how a word \"Apple\" could be represented in N3 using RDF and RDFS:\n\n```\n@prefix rdf: [http://www.w3.org/1999/02/22-rdf-syntax-ns#](http://www.w3.org/1999/02/22-rdf-syntax-ns#) . @prefix rdfs: [http://www.w3.org/2000/01/rdf-schema#](http://www.w3.org/2000/01/rdf-schema#) . @prefix dic: [http://example.com/dictionary#](http://example.com/dictionary#) .\n\n```\n\ndic:Apple a dic:Noun ; rdfs:label \"Apple\" ; dic:definition \"a round fruit with a red, green or yellow skin and a core of small seeds in the middle\" ; dic:example \"An apple a day keeps the doctor away.\" .\n\nIn this example, the class \"Noun\" is defined as a subclass of rdfs:Class, the individual \"Apple\" is defined as an instance of the class \"Noun\", and it has properties such as definition and example of usage. The prefix dic: is used to identify the namespace of the ontology, and the properties and classes are defined using the dic: namespace.\n\nAdditionally, the ontology could include additional classes and properties to represent more specific information about the words, such as the tense of verbs, singular and plural forms of nouns, comparative and superlative forms of adjectives, etc.\n\nIt's also worth noting that, in order to make the ontology more expressive, it can use some additional RDFS constructs such as RDFS:subClassOf, RDFS:domain, RDFS:range, RDFS:subPropertyOf, etc.\n\nAdditionally, it's important to note that RDF and RDFS are quite flexible and can be used to represent a wide range of different ontologies and vocabularies. Therefore, you may want to consider consulting existing ontologies and vocabularies, such as WordNet, FrameNet, and others, to identify the best practices and common patterns for representing the logic of an English dictionary.\n\nIn summary, an ontology that provides the constructs for describing the logic of an English dictionary in N3 using RDF and RDFS could be organized by defining classes for different types of words, properties for describing the characteristics of words, relations for describing the connections between words, individuals for each word in the dictionary, and using RDFS to define the class hierarchy, properties, and restrictions. Additionally, it can use additional RDFS constructs and also aligning with existing ontologies and standards can help to make the ontology more expressive and consistent.\n\n***My Question: provide a code example in turtle***\n\n```turtle\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix dic: <http://example.com/dictionary#> .\n\ndic:Word rdfs:subClassOf rdfs:Resource .\ndic:Noun rdfs:subClassOf dic:Word .\ndic:Verb rdfs:subClassOf dic:Word .\ndic:Adjective rdfs:subClassOf dic:Word .\ndic:Adverb rdfs:subClassOf dic:Word .\n\ndic:partOfSpeech rdfs:domain dic:Word ;\n                  rdfs:range rdfs:Literal .\ndic:definition rdfs:domain dic:Word ;\n                rdfs:range rdfs:Literal .\ndic:example rdfs:domain dic:Word ;\n                rdfs:range rdfs:Literal .\n\ndic:synonym rdfs:domain dic:Word ;\n                rdfs:range dic:Word .\n\ndic:Apple a dic:Noun ;\n\n```\n\n***My Question:\nhow can multiple prefixes for the same ontological asset be defined so that the ontology document can be provided over multiple protocols (ie: IPFS, WebTorrent, etc) whilst providing the semantic RDF RDFS requirements to define that the document URIs are referring to the same document?***\n\nChatGPT Response:\n\nWhen defining an ontology that will be provided over multiple protocols, such as IPFS and WebTorrent, it's important to ensure that the URIs used to reference the ontology document are consistent across all protocols. One way to achieve this is to use multiple prefixes for the same ontological asset, and to define them in such a way that they all refer to the same document.\n\nHere are a few ways to achieve this:\n\n1.  Use a single prefix and different URI schemes: You can use a single prefix and define different URI schemes for the same ontological asset. For example, you can use the prefix \"dic:\" to refer to the ontology, and then define the URIs as \"dic:Apple\" for the web, \"ipfs:Qm1234567890\" for IPFS, and \"webtorrent:abcdefgh\" for WebTorrent.\n    \n2.  Use multiple prefixes and same URI scheme: You can use multiple prefixes for the same ontological asset, and define them to use the same URI scheme. For example, you can use the prefix \"dic:\" for the web, \"ipfs_dic:\" for IPFS, and \"webtorrent_dic:\" for WebTorrent and then define the URIs as \"dic:Apple\" for the web, \"ipfs_dic:Apple\" for IPFS, and \"webtorrent_dic:Apple\" for WebTorrent.\n    \n3.  Linked Data Fragments (LDF) : It's a set of conventions that allows to define multiple URIs for the same resource, which can be identified by different access methods. With LDF, you can use the same URI as the representation of the resource, and it will redirect to the appropriate access method.\n    \n4.  Use 303 redirection: Use the HTTP 303 status code to redirect requests for the ontology to the appropriate location, depending on the protocol being used. This allows you to use a single URI for the ontology, but redirect requests to the appropriate location based on the protocol being used.\n    \n\nIn summary, when defining an ontology that will be provided over multiple protocols, it's important to ensure that the URIs used to reference the ontology document are consistent across all protocols. This can be achieved by using a single prefix and different URI schemes, using multiple prefixes and same URI scheme, using Linked Data Fragments, or using HTTP 303 redirection.\n\n***My question: Provide an example in turtle***\n\n```turtle\n\n@prefix dic: <http://example.com/dictionary#> .\n@prefix ipfs_dic: <ipfs:Qm1234567890/dictionary#> .\n@prefix webtorrent_dic: <webtorrent:abcdefgh/dictionary#> .\n\ndic:Apple a dic:Noun ;\n    rdfs:label \"Apple\" ;\n    dic:definition \"a round fruit with a red, green or yellow skin and a core of small seeds in the middle\" ;\n    dic:example \"An apple a day keeps the doctor away.\" .\n\nipfs_dic:Apple owl:sameAs dic:Apple .\nwebtorrent_dic:Apple owl:sameAs dic:Apple .\n\n```\n\nIn this example, the same class \"Apple\" is defined using three different prefixes \"dic:\", \"ipfs_dic:\", and \"webtorrent_dic:\", but they refer to the same resource. Then we use the owl:sameAs property to indicate that those resources are the same.\n\nThis way, when the ontology is accessed over IPFS, the URI \"ipfs_dic:Apple\" can be used to reference the class, and when accessed over WebTorrent, the URI \"webtorrent_dic:Apple\" can be used instead. And, when accessed over the web, the URI \"dic:Apple\" can be used.\n\nIt's also worth noting that, when using the owl:sameAs property, the ontology will be consistent with OWL 2 standard and it will be able to indicate that resources are identical.\n\nKeep in mind that this is an example, and the URIs used in the example are not valid and it's just for illustration purposes.\n\n***My Question:\nwhat is a solution that provides the ability to define an ontology document as a version that relates to a collection of ontology documents about the same topic. For example; when seeking to describe a law or legislation, the law may change and the ontology document may be only relavent within a particular period of time. So, in turtle, i want to describe a topic collection (ie: a legislative instrument) and relate versions as described in RDF to specific periods of time and provide an ontological reference to the latest version of that document. How could this be achieved? Provide a solution in turtle***\n\nChatGPT Answer:\nIn order to define an ontology document as a version that relates to a collection of ontology documents about the same topic, such as a legislative instrument, you can use the RDF and RDFS constructs to describe the topic collection and relate versions to specific periods of time. Here's an example of how this could be achieved in Turtle:\n\n```Turtle\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix law: <http://example.com/legislation#> .\n@prefix time: <http://www.w3.org/2006/time#> .\n\nlaw:LegislativeInstrument rdfs:subClassOf rdfs:Resource .\nlaw:Version rdfs:subClassOf rdfs:Resource .\n\nlaw:hasVersion rdfs:domain law:LegislativeInstrument ;\n               rdfs:range law:Version .\n\nlaw:validFrom rdfs:domain law:Version ;\n               rdfs:range time:Instant .\n\nlaw:validTo rdfs:domain law:Version ;\n               rdfs:range time:Instant .\n\nlaw:latestVersion rdfs:domain law:LegislativeInstrument ;\n               rdfs:range law:Version .\n\nlaw:LegislativeInstrument1 a law:LegislativeInstrument ;\n    rdfs:label \"Legislative Instrument 1\" ;\n    law:hasVersion law:Version1, law:Version2, law:Version3 .\n\nlaw:Version1 a law:Version ;\n    rdfs:label \"Version 1\" ;\n    law:validFrom \"2022-01-01T00:00:00Z\"^^time:Instant ;\n    law:validTo \"2022-12-31T23:59:59Z\"^^time:Instant .\n\nlaw:Version2 a law:Version ;\n    rdfs:label \"Version 2\" ;\n    law:validFrom \"2023-01-01T00:00:00Z\"^^time:Instant ;\n    law:validTo \"2023-12-31T23:59:59Z\"^^time:Instant .\n\nlaw:Version3 a law:Version ;\n    rdfs:label \"Version 3\" ;\n    law:validFrom \"2024-01-01T00:00:00Z\"^^time:Instant ;\n    law:validTo \"2024-12-31T23:59:59Z\"^^time:Instant .\n\nlaw:LegislativeInstrument1 law:latestVersion law:Version3 .\n\n```\n\nIn this example, the class \"LegislativeInstrument\" is defined as a subclass of \"Resource\", and the class \"Version\" is defined as a subclass of \"Resource\" as well. The property \"hasVersion\" is defined to relate the \"LegislativeInstrument\" class with the \"Version\" class. The property \"validFrom\" and \"validTo\" are defined to indicate the time range when the version is valid. Finally, the property \"latestVersion\" is used to indicate the latest version of the legislative instrument.\n\nHere, the specific legislative instrument \"LegislativeInstrument1\" is defined with three versions \"Version1\", \"Version2\" and \"Version3\". Each version is defined with properties \"validFrom\" and \"validTo\" indicating the time range when the version is valid. The latest version \"Version3\" is indicated using the property \"latestVersion\".\n\nThis way, by using the RDF and RDFS constructs, you can describe the topic collection of a legislative instrument and relate versions to specific periods of time. And also, you can specify the latest version of that document and it will be easy to identify it.\n\nIt's worth noting that in this example, I've used the time ontology (time:) to define the time range but you can use other ways to represent time, such as using xsd:dateTime.\n\nIt's also worth noting that this is an example, and the URIs and values used in the example are not valid and it's just for illustration purposes.\n\n***My Question:\nhow can a cryptographic signature be generated on an RDF document to enable a parser to validate whether or not the document has been altered?***\n\nA cryptographic signature can be generated on an RDF document to enable a parser to validate whether or not the document has been altered, by using a digital signature algorithm. A digital signature algorithm, such as RSA or ECDSA, can be used to create a digital signature of the RDF document.\n\nHere's an example of how this could be achieved in Turtle:\n\n```turtle\n\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix dc: <http://purl.org/dc/elements/1.1/> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n@prefix sig: <http://example.com/signature#> .\n\nsig:Signature rdfs:subClassOf rdfs:Resource .\nsig:signatureValue rdfs:domain sig:Signature ;\n                  rdfs:range xsd:base64Binary .\n\ndc:creator sig:signatureValue \"base64 encoded signature value\" .\ndc:date \"2022-09-20T19:20:30-05:00\"^^xsd:dateTime .\n\n```\n\nIn this example, we defined a new class \"Signature\" as a subclass of \"Resource\" and the property \"signatureValue\" which is defined to relate the \"Signature\" class with the value of the signature, which is represented as xsd:base64Binary. Additionally, we use dc:creator and dc:date properties to indicate the signature creator and the date when the signature was generated.\n\nThis way, the parser can use the public key of the signature creator to verify the signature value, and check if the document has been altered.\n\nIt's important to keep in mind that this is just an example, and in real-world scenarios, the signature should be generated using proper libraries and tools that implement the desired digital signature algorithm. And also it's important to use secure key management techniques to protect the private key.\n\n**my Question;\nHow do i define a cuDNN training model to process an english dictionary for the purpose of using the vocabulary to define RDF / RDFS ontologies, that do not use OWL**\n\nChatGPT Answer:\n\nThe cuDNN library is a GPU-accelerated library for deep neural networks, and it can be used to train models for various tasks such as image classification, natural language processing, and speech recognition.\n\nTo define a cuDNN training model to process an English dictionary for the purpose of using the vocabulary to define RDF/RDFS ontologies, you would need to:\n\n1.  Prepare the data: This would involve preprocessing the English dictionary to extract the vocabulary and related information such as part of speech, definition, etc. and format it into a format suitable for training a neural network.\n    \n2.  Define the model architecture: This would involve choosing a suitable model architecture that can handle the task of vocabulary extraction, such as a recurrent neural network (RNN) or a transformer-based model.\n    \n3.  Train the model: This would involve using cuDNN to train the model on the preprocessed data, using techniques such as backpropagation and stochastic gradient descent.\n    \n4.  Use the trained model: Once the model is trained, you can use it to extract vocabulary from new texts and use it to define RDF/RDFS ontologies.\n    \n\nIt's worth noting that creating an ontology from text is a complex task, and the final ontology will depend on the specific use case and the ontology model that you choose to represent the vocabulary. Additionally, it's also important to note that using cuDNN for this task is not the most common way, as it's a powerful library that is mostly used for deep learning tasks. There are other libraries and models that might be more suitable for this kind of task, like word2vec, GloVe, or BERT.\n\n\n\n**My Question:\n*provide an example of preparing hte data in golang and include the syntactic field to define \"parts of speech\" to distinguish whether a word definition relates to the use of that word in the form of a NOUN, PRONOUN, VERB, ADJECTIVE, ADVERB, PREPOSITION, CONJUNCTION or INTERJECTION.***\n\nHere's an example of how you could prepare the data in Go for a model that aims to distinguish whether a word definition relates to the use of that word in the form of a NOUN, PRONOUN, VERB, ADJECTIVE, ADVERB, PREPOSITION, CONJUNCTION or INTERJECTION:\n\n```go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"os\"\n)\n\ntype Vocab struct {\n    Word       string `json:\"word\"`\n    POS        string `json:\"pos\"`\n    Definition string `json:\"definition\"`\n}\n\nfunc main() {\n    // load the english dictionary\n    file, _ := os.Open(\"english_dictionary.json\")\n    defer file.Close()\n\n    var dictionary map[string]interface{}\n    json.NewDecoder(file).Decode(&dictionary)\n\n    // create a slice to store the vocabulary and related information\n    var vocab []Vocab\n\n    // iterate over the dictionary entries\n    for word, entry := range dictionary {\n        pos := entry.(map[string]interface{})[\"pos\"].(string)\n        definition := entry.(map[string]interface{})[\"definition\"].(string)\n\n        // add the vocabulary and related information to the vocab slice\n        vocab = append(vocab, Vocab{Word: word, POS: pos, Definition: definition})\n    }\n\n    // save the vocab slice to a file for training\n    vocabFile, _ := os.Create(\"vocab.json\")\n    defer vocabFile.Close()\n\n    json.NewEncoder(vocabFile).Encode(voc\n\n```\n\n**ChatGPT Question:\nProvide me an example of how to do step 2, using that model in golang using cuDNN**\n\nChatGPT Answer\nStep 2 of defining a cuDNN training model to process an English dictionary for the purpose of using the vocabulary to define RDF/RDFS ontologies, would involve defining the model architecture.\n\nHere's an example of how you could define a model architecture in Go using cuDNN:\n\n```go\npackage main\n\nimport (\n    \"github.com/dereklstinson/GoCUDA\"\n    \"github.com/dereklstinson/GoCUDA/cudnn\"\n)\n\nfunc main() {\n    // create a cudnn handle\n    handle, _ := cudnn.Create()\n    defer handle.Destroy()\n\n    // define the model architecture\n    // here we will use a simple feedforward neural network with one hidden layer\n    inputTensor, _ := cudnn.CreateTensorDescriptor()\n    defer inputTensor.Destroy()\n    inputTensor.Set(cudnn.DataTypeFloat, []int32{batchSize, inputSize}, []int32{inputSize, 1})\n\n    hiddenTensor, _ := cudnn.CreateTensorDescriptor()\n    defer hiddenTensor.Destroy()\n    hiddenTensor.Set(cudnn.DataTypeFloat, []int32{batchSize, hiddenSize}, []int32{hiddenSize, 1})\n\n    outputTensor, _ := cudnn.CreateTensorDescriptor()\n    defer outputTensor.Destroy()\n    outputTensor.Set(cudnn.DataTypeFloat, []int32{batchSize, outputSize}, []int32{outputSize, 1})\n\n    filterDesc, _ := cudnn.CreateFilterDescriptor()\n    defer\n    filterDesc.Set(cudnn.DataTypeFloat, []int32{hiddenSize, inputSize})\n\n    filterData, _ := GoCUDA.Malloc(filterDesc.Size())\n    defer filterData.Free()\n\n    biasDesc, _ := cudnn.CreateTensorDescriptor()\n    defer biasDesc.Destroy()\n    biasDesc.Set(cudnn.DataTypeFloat, []int32{hiddenSize}, []int32{hiddenSize, 1})\n\n    biasData, _ := GoCUDA.Malloc(biasDesc.Size())\n    defer biasData.Free()\n\n    // initialize the filter and bias data with random values\n    rand.Read(filterData.Pointer())\n    rand.Read(biasData.Pointer())\n\n    // create the feedforward neural network\n    _, _ = handle.CreateFeedForward(inputTensor, filterDesc, filterData, biasDesc, biasData, hiddenTensor, outputTensor)\n}\n\n```\n\nThis code snippet defines a simple feedforward neural network with one hidden layer, and creates a handle to interact with cuDNN library. It creates the tensor descriptor, filter descriptor, and bias descriptor for the input, hidden, and output layers respectively. It also initializes the filter and bias data with random values using the Go's math/rand package.\n\nIt's worth noting that this is just an example, and the actual model architecture will depend on the specific requirements of your use case, and it might require more complex architectures. Also, it's important to note that this example uses a library called GoCUDA which is a Go\n\n**My question: Provide an example of how to do step 3**\n\nStep 3 of defining a cuDNN training model to process an English dictionary for the purpose of using the vocabulary to define RDF/RDFS ontologies, would involve training the model using the preprocessed data.\n\nHere's an example of how you could train the model in Go using cuDNN:\n\n```go\npackage main\n\nimport (\n    \"github.com/dereklstinson/GoCUDA\"\n    \"github.com/dereklstinson/GoCUDA/cudnn\"\n)\n\nfunc main() {\n    // create a cudnn handle\n    handle, _ := cudnn.Create()\n    defer handle.Destroy()\n\n    // load the preprocessed data\n    data, _ := GoCUDA.Malloc(dataSize)\n    defer data.Free()\n\n    labels, _ := GoCUDA.Malloc(labelSize)\n    defer labels.Free()\n\n    // set the data and labels\n    // ...\n\n    // set the training parameters\n    learningRate := 0.01\n    weightDecay := 0.0005\n    momentum := 0.9\n\n    // train the model\n    _, _ = handle.Train(data, labels, learningRate, weightDecay, momentum)\n}\n\n```\n\nThis code snippet loads the preprocessed data, which is the vocabulary and related information extracted from the English dictionary, and the corresponding labels (the part of speech) into memory, and sets the training parameters such as learning rate, weight decay, and momentum. It then uses the cuDNN library to train the model on the data.\n\nIt's important to note that this is a simplified example, and the actual training process will depend on the specific requirements of your use case and the model architecture. Also, it's important to note that this example uses a library called GoCUDA which is a Go library that provides bindings for CUDA, but it doesn't provide bindings for cuDNN library.\n\n**My question; provide an example of step 4**\n\nStep 4 of defining a cuDNN training model to process an English dictionary for the purpose of using the vocabulary to define RDF/RDFS ontologies, would involve evaluating the model's performance and fine-tuning it as necessary.\n\nHere's an example of how you could evaluate the model's performance and fine-tune it in Go using cuDNN:\n\n```go\n\npackage main\n\nimport (\n    \"github.com/dereklstinson/GoCUDA\"\n    \"github.com/dereklstinson/GoCUDA/cudnn\"\n)\n\nfunc main() {\n    // create a cudnn handle\n    handle, _ := cudnn.Create()\n    defer handle.Destroy()\n\n    // load the test data\n    testData, _ := GoCUDA.Malloc(testDataSize)\n    defer testData.Free()\n\n    testLabels, _ := GoCUDA.Malloc(testLabelSize)\n    defer testLabels.Free()\n\n    // set the test data and labels\n    // ...\n\n    // evaluate the model's performance on the test data\n    accuracy, _ := handle.Evaluate(testData, testLabels)\n\n    // print the accuracy\n    println(\"Accuracy:\", accuracy)\n\n    // fine-tune the model if necessary\n    if accuracy < desiredAccuracy {\n        // adjust the training parameters\n        learningRate *= 0.1\n        weightDecay *= 0.1\n\n        // retrain the model\n        _, _ = handle.Train(data, labels, learningRate, weightDecay, momentum)\n    }\n}\n\n```\n\nThis code snippet loads test data and labels, and uses the cuDNN library to evaluate the model's performance on this test data. It then prints the accuracy, and if the accuracy is below a certain threshold, it adjusts the training parameters and retrains the model.\n\nIt's important to note that this is a simplified example, and the actual evaluation and fine-tuning process will depend on the specific requirements of your use case and the model architecture. Also, it's important to note that this example uses a library called GoCUDA which is a Go library that provides bindings for CUDA, but it doesn't provide bindings for cuDNN library.\n\nAs I mentioned earlier, there are no Go libraries that provide bindings for cuDNN, which means that using cuDNN in Go is not a straightforward task, and it would require to use CGo to interact with the cuDNN C library, and this will require a deep understanding of both Go and C programming languages.\n\n\n### Review and considerations\n\n1. Its seemingly defined using tensorcores.  my laptop doesn't have tensor cores, only cudacores\n2.  I didn't know much about the existing solutions like wordnet.\n\n\n### Following-up\n\nOvernight, i found the various existing NLP libraries that were highlighted earlier, alongside a relatively easy to use example - [WordsAPI](https://www.wordsapi.com/) that might be the best choice for producing a Proof of Concept to illustrate the general idea.   Yet, any overly simplified solution (as required to materialise something that demonstrates the general idea); will need an upgrade path.  Furthermore, the method of how to form a longer-term strategy is required.  \n\nIt is important to note (as i've started to illustrate [[RootConcepts]] ); that this underlying service MUST be able to be operated privately and in a stand-alone manner.  To achieve this solution, the outcome may be that the first time a topic or word is sought to be processed; an API call might be made to go and get the information that's required to be added to the database.  Presently, i'm unsure of the technical implications relating to any methods that are reasonably available to achieve that;  such as - can it run on a mobile phone or laptop?  etc.\n\nAs more work continues on the topic (the next day); the below 'work' was produced,\n\n#### ChatGPT\n\n**My Question:** what are the root languages for the english vocabulary?\n\n**ChatGPT Answer**:\nThe English vocabulary is made up of words borrowed from many different languages over the centuries. The main root languages of the English vocabulary are:\n\n-   Germanic: The Germanic languages, including Old English, are the most important source of words in English. They make up around 50% of the words in the English language. Words from Germanic languages include common words such as \"house,\" \"father,\" \"mother,\" \"man,\" \"woman,\" \"child,\" and \"dog.\"\n    \n-   Latin: Latin is the second most important source of words in English. Latin words make up around 30% of the English vocabulary. Many of these words are technical terms in fields such as science, medicine, and law. Words from Latin include \"hospital,\" \"doctor,\" \"university,\" \"species,\" \"algebra,\" \"formula,\" and \"jurisdiction.\"\n    \n-   French: French is the third most important source of words in English. French words make up around 10% of the English vocabulary. Many French words were borrowed during the Norman Conquest of England in 1066 and the following centuries of Norman rule. Words from French include \"money,\" \"art,\" \"government,\" \"justice,\" \"royalty,\" \"diamond,\" and \"restaurant.\"\n    \n-   Greek: Greek is the fourth most important source of words in English. Greek words make up around 5% of the English vocabulary. Many Greek words were borrowed through Latin, but some were borrowed directly into English. Words from Greek include \"philosophy,\" \"theology,\" \"anatomy,\" \"geometry,\" \"telephone,\" \"photograph,\" and \"encyclopedia.\"\n    \n\nOther languages that have contributed to the English vocabulary include Arabic, Italian, Spanish, and many others.\n\nIn addition to the root languages mentioned above, there are also many other languages that have contributed to the English vocabulary over time. These include:\n\n-   Arabic: Many Arabic words were borrowed into English through Latin and Greek. Words from Arabic include \"algebra,\" \"alcohol,\" \"candy,\" \"jar,\" \"lemon,\" \"magazine,\" \"sofa,\" \"sugar,\" and \"zero.\"\n    \n-   Italian: Words from Italian include \"balcony,\" \"broccoli,\" \"cappuccino,\" \"fiasco,\" \"gazebo,\" \"grotto,\" \"paparazzo,\" \"piano,\" \"sonata,\" and \"terrace.\"\n    \n-   Spanish: Words from Spanish include \"alligator,\" \"barbecue,\" \"bronco,\" \"chocolate,\" \"coyote,\" \"guerrilla,\" \"jaguar,\" \"mosquito,\" \"mustang,\" \"patio,\" \"siesta,\" \"sombrero,\" \"taco,\" and \"tornado.\"\n    \n-   Native American languages: Words from Native American languages include \"chocolate,\" \"chipmunk,\" \"moose,\" \"opossum,\" \"pecan,\" \"racoon,\" \"skunk,\" \"squash,\" \"tobacco,\" and \"tomato.\"\n    \n-   Asian languages: Words from Asian languages include \"bamboo,\" \"guru,\" \"karma,\" \"pajama,\" \"tycoon,\" \"yoga,\" and \"zen.\"\n    \n-   African languages: Words from African languages include \"banjo,\" \"jazz,\" \"okra,\" \"yam,\" and \"Zulu.\"\n    \n\nIt's worth noting that these are just a few examples of the many languages that have contributed to the English vocabulary over time. The English language is known for its rich history of borrowing words from other languages, and this is one of the reasons why it has such a diverse and dynamic vocabulary.\n\nMy question: provide an example relating to norse and old norse \n\nSure, here are a few examples of words that have been borrowed from Old Norse and Norse into the English language:\n\n-   \"Anguish\" comes from the Old Norse word \"angr\" meaning \"grief\" or \"sorrow.\"\n-   \"Berserk\" comes from the Old Norse word \"berserkr\" meaning \"a wild warrior.\"\n-   \"Bliss\" comes from the Old Norse word \"bliss\" meaning \"happiness.\"\n-   \"Eke\" means \"also\" or \"additionally\" comes from the Old Norse word \"eigi\" meaning \"also\" or \"in addition.\"\n-   \"Eg\" meaning \"I\" from Old Norse \"eg\"\n-   \"Fjord\" comes from the Old Norse word \"fjörðr\" meaning \"a narrow inlet of the sea.\"\n-   \"Knurl\" comes from the Old Norse word \"knurr\" meaning \"a small knot or knob.\"\n-   \"Mist\" comes from the Old Norse word \"mist\" meaning \"moisture.\"\n-   \"Ransack\" comes from the Old Norse word \"rannsaka\" meaning \"to search through.\"\n-   \"Scowl\" comes from the Old Norse word \"skul\" meaning \"a face with a frown.\"\n\nIt is worth noting that the Norse and Old Norse languages are closely related, and the vast majority of words borrowed into English from Old Norse came directly or indirectly from Norse. Norse is also known as Old Icelandic, it is a Germanic language that was spoken by the Vikings and other inhabitants of the Norse regions of Scandinavia and the North Atlantic islands in the Middle Ages.\n\n\n***My question: what benefit might the use of a neuromorphic processer provide for processing a natural language model?***  (note: that was part of the question)\n\n**ChatGPT:**\nA neuromorphic processor is a type of computer processor that is designed to mimic the way the human brain works. These processors use artificial neural networks, which are modeled after the structure and function of the human brain, to perform tasks such as pattern recognition and decision-making.\n\nThe use of a neuromorphic processor for processing a natural language model could provide several benefits.\n\n-Neuromorphic processors are highly energy-efficient, which makes them well-suited for running large-scale natural language processing models on mobile or embedded devices.\n\n-Neuromorphic processors are also highly parallel, which means they can perform multiple computations simultaneously. This can lead to faster and more efficient processing of natural language models, as well as the ability to handle larger models.\n\n-Neuromorphic processors are also highly adaptable and can learn from new data, which means they can improve their performance over time. This can be particularly useful for natural language processing, as the meaning and usage of words and phrases can change over time, and a model that can adapt to these changes will be more accurate and useful.\n\n-Neuromorphic processors can also be more robust to noise, errors and variations in the data, which can be common in natural language data.\n\nIt's worth mentioning that the field of neuromorphic computing and Neuromorphic processors are relatively new and still in development, and more research is needed to fully understand their potential benefits and limitations.\n\n***My Question: what is the earliest known time the english language was first used?***\n\nChatGPT: \n\nThe earliest known use of the English language dates back to the 5th century AD, when Germanic tribes, such as the Angles, Saxons, and Jutes, began to migrate to the island of Great Britain and settled there. These tribes spoke a Germanic language, which over time evolved into Old English.\n\nOld English, also known as Anglo-Saxon, was the language spoken in England from the 5th century to the Norman conquest in 1066 AD. Old English is considered to be the earliest form of the English language, and it was written in the runic alphabet, which was later replaced by the Latin alphabet.\n\nOld English was a complex and inflected language, with a rich vocabulary and many grammatical features. However, it was quite different from the English spoken today and would be difficult for modern speakers to understand without study or translation.\n\nIt's worth mentioning that the Angles, Saxons, and Jutes were not the only people that were in Great Britain during this time, there were also the Celts, and others, and they were speaking different languages, and the Germanic language of the invaders slowly replaced the Celtic languages and others, it took several centuries to form the language we know today as English.","excerpt":"Dynamic Ontology The below text is sourced via the Q&A with ChatGPT to figure out how to create a dynamic ontology solution.  What i'm then…","outboundReferences":[{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"webizen---root-concepts\"\n  }, \"Webizen - Root Concepts\"), mdx(\"h2\", {\n    \"id\": \"software-development-concepts\"\n  }, \"Software Development Concepts\"), mdx(\"h3\", {\n    \"id\": \"entity-relations\"\n  }, \"Entity Relations\"), mdx(\"p\", null, \"This is a more technical expansion of the concepts noted in the document about \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/SocialFactors/WebScience/Centricity/\",\n    \"title\": \"Centricity\"\n  }, \"Centricity\"), \".\"), mdx(\"p\", null, \"Primary Inferred Entity:  Human\"), mdx(\"p\", null, \"The concept of Human Centric - is that the informatics environment is augmented to primarily serve the human user.  Informatics might relate to groups of humans, in various ways; but fundamentally, the 'operating (natural) agents' in our 'biosphere' are human beings. \"), mdx(\"h5\", {\n    \"id\": \"primary-interface--binary-webizen\"\n  }, \"Primary Interface:  Binary (Webizen)\"), mdx(\"p\", null, \"The primary interface is a computing device that reduces instruction-sets to binary commands that then process through computational 'gates', etc. \"), mdx(\"p\", null, \"The 'webizen' may use various forms of abstration, but at the end of the day; it ends-up binary. \"), mdx(\"h5\", {\n    \"id\": \"secondary-interface---semantic-web\"\n  }, \"Secondary Interface - Semantic Web\"), mdx(\"p\", null, \"The secondary interface target is semantic web; although, if the models are not considered to be the best way of going about addressing a particular problem, then semantic web methods are not the primary (native webizen) method of doing something.\"), mdx(\"h3\", {\n    \"id\": \"duality\"\n  }, \"Duality\"), mdx(\"iframe\", {\n    width: \"560\",\n    height: \"315\",\n    src: \"https://www.youtube.com/embed/sUad0ZL-nBU\",\n    title: \"YouTube video player\",\n    frameBorder: \"0\",\n    allow: \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\",\n    allowFullScreen: true\n  }), mdx(\"p\", null, \"There are various considerations relating to the duality of requiring a machine to process inputs from a human being, in-order for the effect of a human being to be made tangible, or able to be 'sensed'.  In-order to support \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/SocialFactors/WebScience/SafetyProtocols/FreedomOfThought/\",\n    \"title\": \"FreedomOfThought\"\n  }, \"FreedomOfThought\"), \" this human-machine interface needs to be made private and entirely confidential; whereby the trigger for any external rules is a sharing event with any other agent, other than those that represent the individual themselves.  \"), mdx(\"p\", null, \"In otherwords - a person becomes subject to scrutiny when there is an action of sharing something with someone else - not their 'internal' process of thinking about something, ie; a problem, which should not be unduely limited - the concern being, that human beings should be entitled to safely think about anything and the process of doing so is a requirement for enabling the human mind to consider the concepts / topics / issues.  Yet, when 'thought' turns into some sort of 'action' that relates to other people, then an issue that has some sort of implication upon other (ie: socially) becomes relavent in some way; without getting into the semantics of how that might be investigated and/or considered more later.\"), mdx(\"p\", null, \"The machine side of the equation, doesn't 'understand' in a human like way.  This consideration may lead to various sorts of philosphical discussions about consciousness, sould and spirit - which is not something that is sought to be considered herein.\"), mdx(\"p\", null, \"Rather; that there needs to be a computational model for categorising basic natural language, and performing computational logic processes, that end-up being processed as complex binary computations - effectively using mathematics.\"), mdx(\"h3\", {\n    \"id\": \"investigative-hypothesis\"\n  }, \"Investigative Hypothesis\"), mdx(\"p\", null, \"I have a theory about a causal relationship between the evolution of computing, the hierarchy of 'values' (ie: matters / issues / use-cases of importance) that have been historically associated with these works; and, repercussive 'ripple effects' that are exhibited through the use of these systems; rendering, subtle but significant implications upon humanity at large. \"), mdx(\"p\", null, \"In the webizen designs, many factors relating to areas of consideration associated with the structure of informatics are being reconsidered; rightly or wrongly, hypothesis are defined in some way - that may be correct or incorrect or have importance nuances that are attributable, and thereafter processed - as part of a development process that seeks to revise, support a means to reconsider structures - from a present point of view (ie: it is now the year 2023); and then scaffold 'fit for purpose' methodologies, without necessarily requiring 1st class support for functional requirements that relate to systems that may have been designed in the 1960s or earlier (indeed also, later also - noting the practical process of scaffolding upon existing works, etc).  \"), mdx(\"h3\", {\n    \"id\": \"socialisation-structures\"\n  }, \"Socialisation Structures\"), mdx(\"p\", null, \"There are 'socialisation' structures in both the computational environment; as does in-turn relate to the human environment, and in-turn implications with respect to our biosphere.\"), mdx(\"p\", null, \"Importantly also; there are \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/SocialFactors/SocioEconomicTechReqs/GuardianshipRelations/\",\n    \"title\": \"GuardianshipRelations\"\n  }, \"GuardianshipRelations\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/SocialFactors/SocioEconomicTechReqs/GuardianshipSemantics/\",\n    \"title\": \"GuardianshipSemantics\"\n  }, \"GuardianshipSemantics\"), \" requirements. \"));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/WebizenTechStack/Webizen2.5/FunctionalObjectives/RootConcepts/","title":"Webizen - Root Concepts","shouldShowTitle":false}}],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/WebizenTechStack/VocabularyModelling/ChatGPTDynamicOntology/","sidebarItems":[{"title":"Categories","items":[{"title":"Agent-Labelling-WIP","url":"/Agent-Labelling-WIP/","items":[]},{"title":"Documentation Method","url":"/DocumentationMethod/","items":[]},{"title":"DRAFT: Guide For Developers","url":"/GuideForDevelopers/","items":[{"title":"Webizen on Discord","url":"/GuideForDevelopers/WebizenDiscord/","items":[]},{"title":"Webizen Terms","url":"/GuideForDevelopers/WebizenTerms/","items":[]}]},{"title":"Old-Work-Archives","url":"/old-work-archives/","items":[{"title":"2005 Lots of ideas - some big & some small","url":"/old-work-archives/2005LotsOfIdeas-SomeBig&SomeSmall/","items":[]},{"title":"2018-Webizen-Net-Au","url":"/old-work-archives/2018-webizen-net-au/","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations → Principles → The Institution of Socio & Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]},{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between ‘privacy’ and ‘dignity’.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles & Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/posts/About W3C/","items":[]},{"title":"Advanced Functions & Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/posts/Advanced Functions & Facebook Pages/","items":[]},{"title":"Advanced Search & Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/posts/Advanced Search & Discovery Tips/","items":[]},{"title":"An Introduction to Virtual Machines","url":"/old-work-archives/2018-webizen-net-au/posts/An introduction to Virtual Machines/","items":[]},{"title":"Basic Media Analysis & Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/posts/Basic Media Analysis & Part 1 (Audio)/","items":[]},{"title":"Basic Media Analysis & Part 2 (Visual)","url":"/old-work-archives/2018-webizen-net-au/posts/Basic Media Analysis & Part 2 (visual)/","items":[]},{"title":"Basic Media Analysis & Part 3 (Text & Metadata)","url":"/old-work-archives/2018-webizen-net-au/posts/Basic Media Analysis & Part 3 (Text & Metadata)/","items":[]},{"title":"Building an Economy Based upon Knowledge Equity","url":"/old-work-archives/2018-webizen-net-au/posts/Building an Economy based upon Knowledge Equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/posts/Choice of Law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/posts/Contemplation of the ITU Dubai Meeting and the Future of the Internet/","items":[]},{"title":"Creating a Presence & Online","url":"/old-work-archives/2018-webizen-net-au/posts/Creating a Presence & Online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/posts/Credentials and Payments by Manu Sporny/","items":[]},{"title":"Data Recovery - Laptop & Computers","url":"/old-work-archives/2018-webizen-net-au/posts/Data Recovery - Laptop & Computers/","items":[]},{"title":"Data Recovery & Collection Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/posts/Data Recovery & Collection Mobile Devices/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/posts/Decentralized Web Conference 2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/posts/Decentralized Web Summit 2018/","items":[]},{"title":"Does Anonymity Exist","url":"/old-work-archives/2018-webizen-net-au/posts/Does Anonymity exist/","items":[]},{"title":"Downloading My Data From Social Networks","url":"/old-work-archives/2018-webizen-net-au/posts/Downloading My Data from Social Networks/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/posts/Facebook Pages/","items":[]},{"title":"Google Tracking Data (Geolocation)","url":"/old-work-archives/2018-webizen-net-au/posts/Google Tracking Data (geolocation)/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/posts/Human Consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/posts/Image Recgonition Video Playlist/","items":[]},{"title":"Inferencing (Introduction)","url":"/old-work-archives/2018-webizen-net-au/posts/Inferencing (introduction)/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/posts/Introduction to AI/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/posts/Introduction to Maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/posts/Introduction to Ontologies/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/posts/Introduction to Semantic Web/","items":[]},{"title":"Knowledge-Capital","url":"/old-work-archives/2018-webizen-net-au/posts/knowledge-capital/","items":[]},{"title":"Logos-Style-Guides-and-Artwork","url":"/old-work-archives/2018-webizen-net-au/posts/logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping & Setting-up a Business & Identity","url":"/old-work-archives/2018-webizen-net-au/posts/MindMapping & Setting-up a business & Identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/posts/Openlink Virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/posts/OpenRefine/","items":[]},{"title":"Projects-Customers-and-Invoicing-Web-Services-for-Startups","url":"/old-work-archives/2018-webizen-net-au/posts/projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW & some Solid History","url":"/old-work-archives/2018-webizen-net-au/posts/RWW & some Solid history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/posts/Semantic Web (An Intro)/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/posts/Setting-up Twitter/","items":[]},{"title":"Social Encryption An Introduction","url":"/old-work-archives/2018-webizen-net-au/posts/Social Encryption An Introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/posts/Stock Content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/posts/The WayBack Machine/","items":[]},{"title":"Tim Berners Lee & Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/posts/Tim Berners Lee & Turing Lecture/","items":[]},{"title":"Tools-of-Trade","url":"/old-work-archives/2018-webizen-net-au/posts/tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/posts/Trust Factory 2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/posts/Verifiable Claims (An Introduction)/","items":[]},{"title":"Web of Things & an Introduction","url":"/old-work-archives/2018-webizen-net-au/posts/Web of Things & an Introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/posts/Web-Persistence/","items":[]},{"title":"Web-Services & Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/posts/Web-Services & Marketing Tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/posts/Website Templates/","items":[]},{"title":"What is <span class=\"textannotation disambiguated wl-thing\" id=\"urn:local-text-annotation-16v2koc9fcuj5lwn7helj9zpo9t4iqv4\" itemid=\"http://data.wordlift.io/wl0293/entity/linked_data\">Linked Data</span>?","url":"/old-work-archives/2018-webizen-net-au/posts/Introduction to Linked Data/","items":[]},{"title":"What Is Linked Data","url":"/old-work-archives/2018-webizen-net-au/posts/What is Linked Data/","items":[]},{"title":"What Is Open Source Intelligence","url":"/old-work-archives/2018-webizen-net-au/posts/What is Open Source Intelligence/","items":[]}]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[]}]},{"title":"Basedrive","url":"/old-work-archives/Basedrive/","items":[]},{"title":"Human Centric","url":"/old-work-archives/Human-Centric/","items":[]},{"title":"iBANK - Concept overview","url":"/old-work-archives/iBank/","items":[]},{"title":"LinkedIn Articles","url":"/old-work-archives/LinkedInArticles/","items":[{"title":"AI & Software Agents: Social-Engineering Considerations","url":"/old-work-archives/LinkedInArticles/AI&SoftwareAgentsSocial-Engineering Considerations/","items":[]},{"title":"AI, Digital Identity & Guardianship Systems?","url":"/old-work-archives/LinkedInArticles/linkedinNoteAIDigitalIdentity&Guardianship Systems/","items":[]},{"title":"Knowledge & Agent Labelling: Clear Labelling of AI Interactions & some ideas about how to test students, by AI.","url":"/old-work-archives/LinkedInArticles/LinkedInLabellingAiAgents/","items":[]},{"title":"Reinventing the Kindness Equation","url":"/old-work-archives/LinkedInArticles/ReinventingTheKindnessEquation/","items":[]},{"title":"What is the Definition of an 'AI Weapon'","url":"/old-work-archives/LinkedInArticles/WhatIsTheDefinitionOfAnAIWeapon/","items":[]}]},{"title":"WebCivics BizPlan 2018","url":"/old-work-archives/WebCivicsBizPlan2018/","items":[]},{"title":"WebCivics Medium Posts","url":"/old-work-archives/WebCivicsMediumPosts/","items":[{"title":"2019","url":"","items":[{"title":"An Open Letter - What is your Digital Identity Mandate for Humanity","url":"/old-work-archives/WebCivicsMediumPosts/2019/AnOpenLetter-WhatisyourDigitalIdentity MandateforHumanity/","items":[]},{"title":"Building a #NewStart for the Australian Creative Sector.","url":"/old-work-archives/WebCivicsMediumPosts/2019/BuildingaNewStartfortheAustralianCreative Sector./","items":[]},{"title":"Building a Safe Environment for Kids to Thrive with Their ‘Digital Twin’","url":"/old-work-archives/WebCivicsMediumPosts/2019/Building a safe environment for Kids to thrive with their ‘Digital Twin’/","items":[]},{"title":"Causal Relationships between Social informatics design and Quantum Mechanical Phenomenon.","url":"/old-work-archives/WebCivicsMediumPosts/2019/CausalRelationshipsbetweenSocial informaticsdesignandQuantumMechanical Phenomenon./","items":[]},{"title":"Comms & Security Privacy vs. Dignity","url":"/old-work-archives/WebCivicsMediumPosts/2019/Comms&SecurityPrivacyVsDignity/","items":[]},{"title":"Cyber Security Infrastructure, AI & Semantics","url":"/old-work-archives/WebCivicsMediumPosts/2019/CyberSecurityInfrastructureAI&Semantics/","items":[]},{"title":"Dependents The Emergence of Cyber Nation. Estates of ‘Virtual IDentity’","url":"/old-work-archives/WebCivicsMediumPosts/2019/DependentsTheEmergenceOfCyberNationEstatesOfVirtualIDentity/","items":[]},{"title":"Engineering Einstein","url":"/old-work-archives/WebCivicsMediumPosts/2019/EngineeringEinstein/","items":[]},{"title":"Health Informatics History, Challenges & Opportunities","url":"/old-work-archives/WebCivicsMediumPosts/2019/HealthInformaticsHistoryChallenges& Opportunities/","items":[]},{"title":"History Community, Civics & GLAM. Means to Bootstrap Digital Economy Growth & Knowledge Banking Tools.","url":"/old-work-archives/WebCivicsMediumPosts/2019/HistoryCommunityCivics&GLAMMeanstoBootstrapDigitalEconomyGrowth&KnowledgeBankingTools./","items":[]},{"title":"History of Banking, Knowledge Economy and Moral Poverty","url":"/old-work-archives/WebCivicsMediumPosts/2019/History of Banking, Knowledge Economy and Moral Poverty/","items":[]},{"title":"HyperMedia Exhibition & Events Spaces","url":"/old-work-archives/WebCivicsMediumPosts/2019/HyperMediaExhibition&EventsSpaces/","items":[]},{"title":"Inforgs & The Collective Infosphere","url":"/old-work-archives/WebCivicsMediumPosts/2019/Inforgs&TheCollectiveInfosphere/","items":[]},{"title":"Knowledge Age - A conceptual evolution of the Cyber-Physical from our Present Information Age","url":"/old-work-archives/WebCivicsMediumPosts/2019/KnowledgeAge-AConceptualEvolutionoftheCyber-PhysicalfromourPresentInformationAge/","items":[]},{"title":"Knowledge Banking Infrastructure","url":"/old-work-archives/WebCivicsMediumPosts/2019/Knowledge Banking Infrastructure/","items":[]},{"title":"Knowledge Banking Infrastructure - Cyber Socio-Economic Foundations for Our Shared World","url":"/old-work-archives/WebCivicsMediumPosts/2019/Knowledge Banking Infrastructure - Cyber Socio-Economic Foundations for our shared world/","items":[]},{"title":"Knowledge Clouds","url":"/old-work-archives/WebCivicsMediumPosts/2019/Knowledge Clouds/","items":[]},{"title":"Media Analytics & Hyper-Media Services","url":"/old-work-archives/WebCivicsMediumPosts/2019/MediaAnalytics&Hyper-MediaServices/","items":[]},{"title":"Micropayments Standards - An Economic Imperative for the Knowledge Age","url":"/old-work-archives/WebCivicsMediumPosts/2019/MicropaymentsStandards-AnEconomicImperativefortheKnowledgeAge/","items":[]},{"title":"Open Data v3.0 Permissive Commons","url":"/old-work-archives/WebCivicsMediumPosts/2019/OpenDatav3.0PermissiveCommons/","items":[]},{"title":"Skills & Social Activities","url":"/old-work-archives/WebCivicsMediumPosts/2019/Skills&SocialActivities/","items":[]},{"title":"Small to Medium Business, WebPayments & Knowledge Banking","url":"/old-work-archives/WebCivicsMediumPosts/2019/SmallToMediumBusinessWebPayments&KnowledgeBanking/","items":[]},{"title":"Tech for Permissive Commons","url":"/old-work-archives/WebCivicsMediumPosts/2019/TechforPermissiveCommons/","items":[]},{"title":"The Case for a ‘Human Centric’ Global Cyber Infrastructure Project","url":"/old-work-archives/WebCivicsMediumPosts/2019/TheCaseforaHumanCentricGlobalCyberInfrastructureProject/","items":[]},{"title":"The Semantic Inforg & The “Human Centric Web” — Reality Check, Tech.","url":"/old-work-archives/WebCivicsMediumPosts/2019/The Semantic Inforg & The “Human Centric Web” — Reality Check, Tech./","items":[]},{"title":"Tooling for Democracies","url":"/old-work-archives/WebCivicsMediumPosts/2019/ToolingforDemocracies/","items":[]},{"title":"Universitas Doctrina et Sapientiae","url":"/old-work-archives/WebCivicsMediumPosts/2019/UniversitasDoctrinaetSapientiae/","items":[]},{"title":"Why a Trust Factory (“think tank”)","url":"/old-work-archives/WebCivicsMediumPosts/2019/WhyaTrustFactorythinktank/","items":[]},{"title":"WoT Smart Cities Engineering Digital Twin Things & Ecosystems.","url":"/old-work-archives/WebCivicsMediumPosts/2019/WoTSmartCitiesEngineeringDigitalTwinThings&Ecosystems/","items":[]}]},{"title":"During-Covid","url":"","items":[{"title":"A Future For Australians","url":"/old-work-archives/WebCivicsMediumPosts/during-covid/AFutureForAustralians/","items":[]},{"title":"A Future to Support Informed Decisions S.T.E.A.M & Stars","url":"/old-work-archives/WebCivicsMediumPosts/during-covid/AFuturetoSupportInformedDecisionsSTEAM&Stars/","items":[]},{"title":"A Major Manifestation Problem","url":"/old-work-archives/WebCivicsMediumPosts/during-covid/AMajorManifestationProblem/","items":[]},{"title":"Choices about the future of Knowledge Governance & alternatives Solutions to what appears to be how VaccinePassports are now being made","url":"/old-work-archives/WebCivicsMediumPosts/during-covid/ChoicesaboutthefutureofKnowledgeGovernanceAlternativesSolutionstoVaccinePassports/","items":[]},{"title":"Digital Covid19 “Vaccine Passport”","url":"/old-work-archives/WebCivicsMediumPosts/during-covid/DigitalCovid19VaccinePassports/","items":[]},{"title":"Do We Need a Space Time Navigator","url":"/old-work-archives/WebCivicsMediumPosts/during-covid/DoWeNeedaSpaceTimeNavigator/","items":[]},{"title":"Is an International Cyber Migration Even Possible","url":"/old-work-archives/WebCivicsMediumPosts/during-covid/IsanInternationalCyberMigrationEvenPossible/","items":[]}]}]},{"title":"Webizen Group 2015","url":"/old-work-archives/WebizenGroup2015/","items":[]}]},{"title":"Permissive Commons","url":"/PermissiveCommons/","items":[{"title":"PCT Ontology Modelling","url":"/PermissiveCommons/PCTOntologyModelling/","items":[{"title":"Biosphere Ontologies","url":"/PermissiveCommons/PCTOntologyModelling/BiosphereOntologies/","items":[]},{"title":"English Language Modelling","url":"/PermissiveCommons/PCTOntologyModelling/EnglishLanguageModelling/","items":[]},{"title":"NLP","url":"/PermissiveCommons/PCTOntologyModelling/NLP/","items":[]},{"title":"Ontology Modelling","url":"/PermissiveCommons/PCTOntologyModelling/OntologyModelling/","items":[]},{"title":"OntologyWork","url":"","items":[{"title":"OWL","url":"/PermissiveCommons/PCTOntologyModelling/OntologyWork/OWL/","items":[]}]},{"title":"PCTWebizenUseOfOntology","url":"/PermissiveCommons/PCTOntologyModelling/PCTWebizenUseOfOntology/","items":[]},{"title":"SocioSphere Ontologies","url":"/PermissiveCommons/PCTOntologyModelling/SocioSphereOntologies/","items":[]},{"title":"Understanding Ontologies","url":"/PermissiveCommons/PCTOntologyModelling/UnderstandingOntologies/","items":[]}]},{"title":"Permissive Commons Tech","url":"/PermissiveCommons/PermissiveCommonsTech/","items":[{"title":"PCTRequirements","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/","items":[{"title":"Engineering Considerations","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/","items":[{"title":"Cryptographic Signatures","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/CryptographicSignatures/","items":[]},{"title":"Energy Consumption","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/EnergyConsumption/","items":[]},{"title":"ForTheGoodOfHumanity","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/ForTheGoodOfHumanity/","items":[]},{"title":"Object Ontology Capabilities.","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/ObjectOntologyCapabilities/","items":[]},{"title":"PCT-DNS Considerations","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/PCT-DNS-Considerations/","items":[]},{"title":"Protocol Independence","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/Protocolindependence/","items":[]},{"title":"PublishingWorkflow","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/PublishingWorkflow/","items":[]},{"title":"SocioEconomic Relations","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/SocioEconomicRelations/","items":[]},{"title":"Temporal Semantics","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/PCTTemporalSemantics/","items":[]},{"title":"Version Control","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTEngineeringConsiderations/VersionControl/","items":[]}]},{"title":"PCT_PeaceInfrastructureProjectRequirements","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCT_PeaceInfrastructureProjectRequirements/","items":[]},{"title":"PCT-Core-Services","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCT-Core-Services/","items":[{"title":"Decentralised Ontologies","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCT-Core-Services/DecentralisedOntologies/","items":[]},{"title":"Human Centric Ontology","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCT-Core-Services/HumanCentricOntology/","items":[]}]},{"title":"PCTSpecifications","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTSpecifications/","items":[{"title":"Permissive Commons Categories","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTSpecifications/PermissiveCommonsCategories/","items":[]}]},{"title":"Permissive Commons Technology Docs","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTREADME/","items":[]},{"title":"Use Cases","url":"/PermissiveCommons/PermissiveCommonsTech/PCTRequirements/PCTuse-cases/","items":[]}]},{"title":"Problem Statement","url":"/PermissiveCommons/PermissiveCommonsTech/PCTProblemStatement/","items":[]},{"title":"What are 'The Commons'","url":"/PermissiveCommons/PermissiveCommonsTech/WhatAreTheCommons/","items":[]},{"title":"What is 'Open Data'","url":"/PermissiveCommons/PermissiveCommonsTech/WhatIsOpenData/","items":[]},{"title":"Work in Progress","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/","items":[{"title":"Collect Existing Ontologies","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/CollectExistingOntologies/","items":[]},{"title":"DIDsEval","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/DIDsEval/","items":[]},{"title":"Knowledge Modelling","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/KnowledgeModelling/","items":[{"title":"Agent Ontology Modelling","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/KnowledgeModelling/AgentOntologyModelling/","items":[]},{"title":"GeoTemporalSemantics","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/KnowledgeModelling/GeoTemporalSemantics/","items":[]}]},{"title":"Minimum Wages","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/MinimumWages/","items":[]},{"title":"Ontology in Markdown Example","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/RDF2MD-Example/","items":[]},{"title":"Permissive Commons Technology (PCT) Requirements.","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/PermissiveCommonsRequirements/","items":[]},{"title":"Safety Considerations","url":"/PermissiveCommons/PermissiveCommonsTech/PCTWorkInProgress/SafetyConsiderations/","items":[]}]}]},{"title":"Sustainable Development Goals (ESG)","url":"/PermissiveCommons/SDGs/","items":[]}]},{"title":"Social Factors","url":"/SocialFactors/","items":[{"title":"Background","url":"/SocialFactors/Background/","items":[]},{"title":"Commercial Models","url":"/SocialFactors/CommercialModels/","items":[{"title":"Best Efforts","url":"/SocialFactors/CommercialModels/BestEfforts/","items":[]},{"title":"Currencies","url":"/SocialFactors/CommercialModels/Currencies/","items":[{"title":"Financial Payment Types","url":"/SocialFactors/CommercialModels/Currencies/FinancialPaymentTypes/","items":[]},{"title":"Gifts and Donations","url":"/SocialFactors/CommercialModels/Currencies/GiftsAndDonations/","items":[]},{"title":"Micro Payments","url":"/SocialFactors/CommercialModels/Currencies/Micropayments/","items":[]},{"title":"Non-Financial Currencies","url":"/SocialFactors/CommercialModels/Currencies/Non-FinancialCurrencies/","items":[]}]},{"title":"Economic Systems","url":"/SocialFactors/CommercialModels/EconomicSystems/","items":[]},{"title":"Environmental, Social and Governance (ESG)","url":"/SocialFactors/CommercialModels/ESG/","items":[{"title":"Biosphere Calcs","url":"/SocialFactors/CommercialModels/ESG/BiosphereCalcs/","items":[]},{"title":"Energy Calcs","url":"/SocialFactors/CommercialModels/ESG/EnergyCalcs/","items":[]},{"title":"Sociosphere Calcs","url":"/SocialFactors/CommercialModels/ESG/SocioshereCalcs/","items":[]}]},{"title":"Fit For Purpose","url":"/SocialFactors/CommercialModels/FitForPurpose/","items":[]},{"title":"Hardware Strategy","url":"/SocialFactors/CommercialModels/HardwareStrategy/","items":[]},{"title":"Historical Information","url":"/SocialFactors/CommercialModels/Historical Information/","items":[{"title":"Domains","url":"/SocialFactors/CommercialModels/Historical Information/Domains/","items":[]},{"title":"Value Accounting Initiatives","url":"/SocialFactors/CommercialModels/Historical Information/ValueAccountingInitiatives/","items":[]},{"title":"Web Civics","url":"/SocialFactors/CommercialModels/Historical Information/WebCivics/","items":[]}]},{"title":"The Work Project","url":"/SocialFactors/CommercialModels/WorkProject/","items":[{"title":"Contribution Classifications","url":"/SocialFactors/CommercialModels/WorkProject/ContributionClassifications/","items":[]},{"title":"Equipment and Supplies","url":"/SocialFactors/CommercialModels/WorkProject/EquipmentAndSupplies/","items":[]},{"title":"Intellectual Property","url":"/SocialFactors/CommercialModels/WorkProject/IntellectualProperty/","items":[]},{"title":"Knowledge Work","url":"/SocialFactors/CommercialModels/WorkProject/KnowledgeWork/","items":[]},{"title":"Labour Work","url":"/SocialFactors/CommercialModels/WorkProject/LabourWork/","items":[]},{"title":"Licensing Frameworks","url":"/SocialFactors/CommercialModels/WorkProject/LicensingFrameworks/","items":[]},{"title":"Project Financing","url":"/SocialFactors/CommercialModels/WorkProject/ProjectFinancing/","items":[]},{"title":"Resources","url":"/SocialFactors/CommercialModels/WorkProject/Resources/","items":[]},{"title":"Work Valuation Methods","url":"/SocialFactors/CommercialModels/WorkProject/WorkValuationMethods/","items":[]}]},{"title":"Trust Factory","url":"/SocialFactors/CommercialModels/TrustFactory/","items":[]},{"title":"Webizen","url":"/SocialFactors/CommercialModels/Webizen/","items":[]},{"title":"Webizen Alliance","url":"/SocialFactors/CommercialModels/WebizenAlliance/","items":[{"title":"Community of Practice","url":"/SocialFactors/CommercialModels/WebizenAlliance/CommunityOfPractice/","items":[]},{"title":"Early Stage Income Strategies","url":"/SocialFactors/CommercialModels/WebizenAlliance/EarlyStageIncomeStrategies/","items":[]},{"title":"The Webizen Charter","url":"/SocialFactors/CommercialModels/WebizenAlliance/TheWebizenCharter/","items":[]},{"title":"Webizen Host Providers","url":"/SocialFactors/CommercialModels/WebizenAlliance/WebizenAllianceHostProviders/","items":[]}]},{"title":"Webizen Pro VAR Solutions","url":"/SocialFactors/CommercialModels/WebizenProVARSolutions/","items":[]},{"title":"WebizenPro Usecases","url":"/SocialFactors/CommercialModels/WebizenProUsecases/","items":[{"title":"Webizen Pro Ecosystem Usecases","url":"/SocialFactors/CommercialModels/WebizenProUsecases/WebizenProEcosystemUsecases/","items":[]},{"title":"Webizen Pro Edge UseCases","url":"/SocialFactors/CommercialModels/WebizenProUsecases/WebizenProEdgeUseCases/","items":[]},{"title":"Webizen Pro Use-Cases","url":"/SocialFactors/CommercialModels/WebizenProUsecases/WebizenProUse-Cases/","items":[]}]}]},{"title":"Commercial-Technical Requirements","url":"/SocialFactors/SocioEconomicTechReqs/","items":[{"title":"Account Creation","url":"/SocialFactors/SocioEconomicTechReqs/AccountCreation/","items":[]},{"title":"Guardianship Relations","url":"/SocialFactors/SocioEconomicTechReqs/GuardianshipRelations/","items":[]},{"title":"Guardianship Semantics","url":"/SocialFactors/SocioEconomicTechReqs/GuardianshipSemantics/","items":[]},{"title":"Support for Human Agency","url":"/SocialFactors/SocioEconomicTechReqs/SupportForHumanAgency/","items":[]}]},{"title":"Peace Infrastructure Project","url":"/SocialFactors/PeaceInfrastructureProject/","items":[{"title":"About the Peace Infrastructure Project","url":"/SocialFactors/PeaceInfrastructureProject/AboutThePeaceInfrastructureProject/","items":[]}]},{"title":"Social Attack Vectors","url":"/SocialFactors/SocialAttackVectors/","items":[{"title":"Criminal Activity","url":"/SocialFactors/SocialAttackVectors/CriminalActivity/","items":[{"title":"Corporate Crime","url":"/SocialFactors/SocialAttackVectors/CriminalActivity/CorporateCrime/","items":[]},{"title":"Environmental Crime","url":"/SocialFactors/SocialAttackVectors/CriminalActivity/EnvironmentalCrime/","items":[]},{"title":"Financial Crimes","url":"/SocialFactors/SocialAttackVectors/CriminalActivity/FinancialCrimes/","items":[]},{"title":"Fraud","url":"/SocialFactors/SocialAttackVectors/CriminalActivity/Fraud/","items":[]},{"title":"Proxy Attacks","url":"/SocialFactors/SocialAttackVectors/CriminalActivity/ProxyAttacks/","items":[]}]},{"title":"Cyber Attack Vectors","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/","items":[{"title":"Attacks Online","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/AttacksOnline/","items":[]},{"title":"Cyber Crime","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/CyberCrime/","items":[]},{"title":"Database Attacks","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/DataBaseAttacks/","items":[]},{"title":"DeepFakes","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/DeepFakes/","items":[]},{"title":"Digital Slavery","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/DigitalSlavery/","items":[]},{"title":"MisRepresentation","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/MisRepresentation/","items":[]},{"title":"NobodyAI","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/NobodyAI/","items":[]},{"title":"Permanent Identifiers","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/permanentIdentifiers/","items":[]},{"title":"Tech Debt","url":"/SocialFactors/SocialAttackVectors/CyberAttackVectors/TechDebt/","items":[]}]},{"title":"Discrimination based attacks.","url":"/SocialFactors/SocialAttackVectors/discriminationBasedAttacks/","items":[]},{"title":"Dishonesty","url":"/SocialFactors/SocialAttackVectors/Dishonesty/","items":[]},{"title":"Strategic Harms & Exploitation","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/","items":[{"title":"\"The Secret\"","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/TheSecret/","items":[]},{"title":"Academic / Institutional Funnelling","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/AcademicInstitutionalFunnelling/","items":[]},{"title":"Authorised Thinkers","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/AuthorisedThinkers/","items":[]},{"title":"Brain Rape","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/BrainRape/","items":[]},{"title":"Commercial Exploitation","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/CommercialExploitation/","items":[]},{"title":"Compromise","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/Compromise/","items":[]},{"title":"Dehumanisation","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/Dehumanisation/","items":[]},{"title":"Fake News","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/FakeNews/","items":[]},{"title":"False Attribution","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/FalseAttribution/","items":[]},{"title":"Impairment","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/Impairment/","items":[]},{"title":"Manipulation","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/Manipulation/","items":[]},{"title":"Psychological Attack & PsyOps","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/PsychologicalAttack/","items":[]},{"title":"Public Sector Wrong Doings","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/PublicSectorWrongDoings/","items":[]},{"title":"Reversal","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/Reversal/","items":[]},{"title":"Social Contract Gamification","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/SocialContractGamification/","items":[]},{"title":"TemporalAttacks","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/TemporalAttacks/","items":[]},{"title":"The Harvester","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/TheHarvester/","items":[]},{"title":"You Must Be Famous!!!","url":"/SocialFactors/SocialAttackVectors/StrategicHarms/MustBeFamous/","items":[]}]},{"title":"Torts","url":"/SocialFactors/SocialAttackVectors/Torts/","items":[{"title":"Economic Torts","url":"/SocialFactors/SocialAttackVectors/Torts/EconomicTorts/","items":[]},{"title":"Intentional Torts","url":"/SocialFactors/SocialAttackVectors/Torts/IntentionalTorts/","items":[]},{"title":"Negligence","url":"/SocialFactors/SocialAttackVectors/Torts/Negligence/","items":[]},{"title":"Nuisance","url":"/SocialFactors/SocialAttackVectors/Torts/Nuisance/","items":[]}]}]},{"title":"Web Science","url":"/SocialFactors/WebScience/","items":[{"title":"Artificial Minds","url":"/SocialFactors/WebScience/ArtificialMinds/","items":[{"title":"A ChatGPT Experiment: Holborn","url":"/SocialFactors/WebScience/ArtificialMinds/AChatGPTExperimentHolborn/","items":[]},{"title":"BotScience","url":"/SocialFactors/WebScience/ArtificialMinds/BotScience/","items":[]},{"title":"Cognitive AI","url":"/SocialFactors/WebScience/ArtificialMinds/CognitiveAI/","items":[{"title":"Cognitive AI","url":"/SocialFactors/WebScience/ArtificialMinds/CognitiveAI/CognitiveAI-WIP/","items":[]},{"title":"Defeasible Logic","url":"/SocialFactors/WebScience/ArtificialMinds/CognitiveAI/DefeasibleLogic/","items":[]},{"title":"Plausible Reasoning","url":"/SocialFactors/WebScience/ArtificialMinds/CognitiveAI/PlausibleReasoning/","items":[]}]},{"title":"The Artificial Minds List","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/","items":[{"title":"Baymax","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/Baymax/","items":[]},{"title":"Bishop - Aliens","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/Bishop - Aliens/","items":[]},{"title":"Borg - Star Trek","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/Borg - Star Trek/","items":[]},{"title":"ChatGPT","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/ChatGPT/","items":[]},{"title":"Edwin Jarvis","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/Edwin Jarvis/","items":[]},{"title":"IBM Watson","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/IBM Watson/","items":[]},{"title":"Max - Flight of the Navigator","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/Max - Flight of the Navigator/","items":[]},{"title":"Rosie - The Jetsons","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/Rosie - The Jetsons/","items":[]},{"title":"Sarah - Eureka","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/Sarah - Eureka/","items":[]},{"title":"SkyNet","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/SkyNet/","items":[]},{"title":"The Doctor - Star Trek Voyager","url":"/SocialFactors/WebScience/ArtificialMinds/TheArtificialMindsIndex/The Doctor - Star Trek Voyager/","items":[]}]},{"title":"The Webizen Agent","url":"/SocialFactors/WebScience/ArtificialMinds/TheWebizenAgent/","items":[]}]},{"title":"Banishment & Exile.","url":"/SocialFactors/WebScience/banishmentExile/","items":[]},{"title":"Centricity","url":"/SocialFactors/WebScience/Centricity/","items":[]},{"title":"Human-Centric AI Ethics","url":"/SocialFactors/WebScience/HumanCentricAIEthics/","items":[]},{"title":"Identity Modelling","url":"/SocialFactors/WebScience/IdentityModelling/","items":[]},{"title":"identityModelling","url":"","items":[{"title":"NISTIDOntology","url":"/SocialFactors/WebScience/identityModelling/NISTIDOntology/","items":[]}]},{"title":"Learning Modals","url":"/SocialFactors/WebScience/LearningModals/","items":[]},{"title":"Safety Protocols","url":"/SocialFactors/WebScience/SafetyProtocols/","items":[{"title":"Agent Labelling","url":"/SocialFactors/WebScience/SafetyProtocols/AgentLabelling/","items":[]},{"title":"Data Portability","url":"/SocialFactors/WebScience/SafetyProtocols/DataPortability/","items":[]},{"title":"Ending Digital Slavery","url":"/SocialFactors/WebScience/SafetyProtocols/EndingDigitalSlavery/","items":[]},{"title":"Fair Work Protocols","url":"/SocialFactors/WebScience/SafetyProtocols/FairWorkProtocols/","items":[]},{"title":"Freedom of Thought","url":"/SocialFactors/WebScience/SafetyProtocols/FreedomOfThought/","items":[]},{"title":"Human Centric AI","url":"/SocialFactors/WebScience/SafetyProtocols/HumanCentricAI/","items":[]},{"title":"HumanCentric Digital Identity","url":"/SocialFactors/WebScience/SafetyProtocols/HumanCentricDigitalIdentity/","items":[]},{"title":"Knowledge Qualification Methods","url":"/SocialFactors/WebScience/SafetyProtocols/KnowledgeQualificationMethods/","items":[]},{"title":"No Lock-ins","url":"/SocialFactors/WebScience/SafetyProtocols/NoLock-ins/","items":[]},{"title":"Protecting Children","url":"/SocialFactors/WebScience/SafetyProtocols/ProtectingChildren/","items":[]},{"title":"Relationships (Social)","url":"/SocialFactors/WebScience/SafetyProtocols/Relationships(Social)/","items":[]},{"title":"Values Credentials","url":"/SocialFactors/WebScience/SafetyProtocols/ValuesCredentials/","items":[]}]},{"title":"Science of Consciousness","url":"/SocialFactors/WebScience/ScienceOfConsciousness/","items":[]},{"title":"The Values Project","url":"/SocialFactors/WebScience/TheValuesProject/","items":[]}]},{"title":"Webizen 3.0 Setup User Experience","url":"/SocialFactors/Webizen3.0SetupUserExperience/","items":[]},{"title":"Webizen: For Humans","url":"/SocialFactors/WebizenForHumans/","items":[]}]},{"title":"Webizen Docs Introduction","url":"/","items":[]},{"title":"Webizen Tech Stack","url":"/WebizenTechStack/","items":[{"title":"Authentication Fabric","url":"/WebizenTechStack/AuthenticationFabric/","items":[{"title":"Verifiable Credentials","url":"/WebizenTechStack/AuthenticationFabric/VerifiableCredentials/","items":[]},{"title":"WebAuthn","url":"/WebizenTechStack/AuthenticationFabric/WebAuthn/","items":[]},{"title":"WebID","url":"/WebizenTechStack/AuthenticationFabric/WebID/","items":[]}]},{"title":"Core Services","url":"/WebizenTechStack/CoreServices/","items":[{"title":"Non-HTTP(s) Protocols","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/","items":[{"title":"Chia","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/Chia/","items":[]},{"title":"DAT","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/DAT/","items":[]},{"title":"GIT","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/GUNECO/","items":[]},{"title":"Hedera","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/Hedera/","items":[]},{"title":"Holochain","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/Holochain/","items":[]},{"title":"IOTA","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/IOTA/","items":[]},{"title":"IPFS","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/IPFS/","items":[]},{"title":"IPLD","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/IPLD/","items":[]},{"title":"Lightning Network","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/LightningNetwork/","items":[]},{"title":"NYM","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/NYM/","items":[]},{"title":"Obyte","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/obyte/","items":[]},{"title":"WebCash","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/WebCash/","items":[]},{"title":"WebRTC","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/WebTorrent/","items":[]},{"title":"WireGuard","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/WireGuard/","items":[]},{"title":"XRP Ledger","url":"/WebizenTechStack/CoreServices/Non-HTTP(s)Protocols/XRPLedger/","items":[]}]},{"title":"Verifiable Claims & Credentials","url":"/WebizenTechStack/CoreServices/VerifiableClaims&Credentials/","items":[]}]},{"title":"Core Technologies","url":"/WebizenTechStack/CoreTechnologies/","items":[{"title":"GoLang Libraries","url":"/WebizenTechStack/CoreTechnologies/GoLangLibraries/","items":[]},{"title":"Open Badges","url":"/WebizenTechStack/CoreTechnologies/OpenBadges/","items":[]},{"title":"Semantic Web","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/","items":[{"title":"GRDDL","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/GRDDL/","items":[]},{"title":"Semantic Web Authentication Tech","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWeb-AUTH/","items":[{"title":"WebID-OIDC","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"SemWebOntologies","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/","items":[{"title":"DAML+OIL","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/DAML+OIL/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/FOAF/","items":[]},{"title":"HumanRightsOntologies","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/HumanRightsOntologies/","items":[{"title":"UDHR","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/HumanRightsOntologies/UDHR/","items":[]}]},{"title":"LKIF-Core","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/LKIF-Core/","items":[]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]}]},{"title":"OWL","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/OWL/","items":[]},{"title":"RDFS","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/RDFS/","items":[]},{"title":"Sitemap","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/Sitemap/","items":[]},{"title":"SKOS","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/SKOS/","items":[]},{"title":"SOIC","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SemWebOntologies/SOIC/","items":[]}]},{"title":"Sparql Family","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/SparqlFamily/","items":[]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID Specifications","url":"/WebizenTechStack/CoreTechnologies/SemanticWeb/W3C Specifications/WebID Specifications/","items":[]}]}]},{"title":"Temporal Semantics","url":"/WebizenTechStack/CoreTechnologies/TemporalSemantics/","items":[]}]},{"title":"Engineering Considerations","url":"/WebizenTechStack/EngineeringConsiderations/","items":[{"title":"Authentication Logic","url":"/WebizenTechStack/EngineeringConsiderations/AUTHLogic/","items":[]},{"title":"Containerisation","url":"/WebizenTechStack/EngineeringConsiderations/Containerisation/","items":[]},{"title":"Cryptography","url":"/WebizenTechStack/EngineeringConsiderations/Cryptography/","items":[]},{"title":"Database Requirements","url":"/WebizenTechStack/EngineeringConsiderations/DatabaseRequirements/","items":[{"title":"DatabaseAlternatives","url":"","items":[{"title":"Akutan","url":"/WebizenTechStack/EngineeringConsiderations/DatabaseRequirements/DatabaseAlternatives/akutan/","items":[]},{"title":"Apache Kafka","url":"/WebizenTechStack/EngineeringConsiderations/DatabaseRequirements/DatabaseAlternatives/ApacheKafka/","items":[]},{"title":"Bad Wolf","url":"/WebizenTechStack/EngineeringConsiderations/DatabaseRequirements/DatabaseAlternatives/BadWolf/","items":[]},{"title":"CayleyGraph","url":"/WebizenTechStack/EngineeringConsiderations/DatabaseRequirements/DatabaseAlternatives/CayleyGraph/","items":[]}]},{"title":"DatabaseMethods","url":"","items":[{"title":"BQL","url":"/WebizenTechStack/EngineeringConsiderations/DatabaseRequirements/DatabaseMethods/BQL/","items":[]},{"title":"GraphQL","url":"/WebizenTechStack/EngineeringConsiderations/DatabaseRequirements/DatabaseMethods/GraphQL/","items":[]},{"title":"Sparql","url":"/WebizenTechStack/EngineeringConsiderations/DatabaseRequirements/DatabaseMethods/Sparql/","items":[]}]}]},{"title":"Email Server Logic","url":"/WebizenTechStack/EngineeringConsiderations/MailServerLogic/","items":[]},{"title":"InternetStrategy","url":"/WebizenTechStack/EngineeringConsiderations/InternetStrategy/","items":[{"title":"DNS Considerations","url":"/WebizenTechStack/EngineeringConsiderations/InternetStrategy/DNSConsiderations/","items":[]},{"title":"General Internet Info","url":"/WebizenTechStack/EngineeringConsiderations/InternetStrategy/InternetSummaryInfo/","items":[]},{"title":"List of Protocols ISO model","url":"/WebizenTechStack/EngineeringConsiderations/InternetStrategy/ListOfProtocolsISOmodel/","items":[]}]},{"title":"Prolog","url":"/WebizenTechStack/EngineeringConsiderations/Prolog/","items":[]}]},{"title":"Private DataStore (Vault)","url":"/WebizenTechStack/DataStoreVaultNotes/","items":[]},{"title":"Vocabulary Modelling","url":"/WebizenTechStack/VocabularyModelling/","items":[{"title":"Dynamic Ontology","url":"/WebizenTechStack/VocabularyModelling/ChatGPTDynamicOntology/","items":[]},{"title":"Ontology App Development","url":"/WebizenTechStack/VocabularyModelling/OntologyAppDev/","items":[]},{"title":"TopLevelOntologyNLPResearch","url":"/WebizenTechStack/VocabularyModelling/TopLevelOntologyNLPResearch/","items":[]}]},{"title":"Webize","url":"/WebizenTechStack/Webize/","items":[{"title":"Appy","url":"/WebizenTechStack/Webize/appy/","items":[]},{"title":"Beaker Browser (Fork)","url":"/WebizenTechStack/Webize/BeakerBrowser/","items":[]},{"title":"Solid","url":"/WebizenTechStack/Webize/Solid/","items":[]},{"title":"Syncthing","url":"/WebizenTechStack/Webize/Syncthing/","items":[]}]},{"title":"Webizen 2.5 Dev","url":"/WebizenTechStack/Webizen2.5/","items":[{"title":"Chat GPT Supported Notes","url":"/WebizenTechStack/Webizen2.5/ChatGPTSupported/","items":[{"title":"ChatGPTDistributedDBGoNotes","url":"/WebizenTechStack/Webizen2.5/ChatGPTSupported/ChatGPTDistributedDBGoNotes/","items":[]},{"title":"Go WebID-OIDC & ChatGPT","url":"/WebizenTechStack/Webizen2.5/ChatGPTSupported/GoLangWebIDOIDC/","items":[]}]},{"title":"DevNotesWebizen2.5","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/","items":[{"title":"AgentAPI","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/AgentAPI/","items":[]},{"title":"AppRequirements","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/AppRequirements/","items":[]},{"title":"Cryptography Notes","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/CryptographyNotes/","items":[]},{"title":"ImplementationNotes-2.5","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/","items":[{"title":"Golang Packages","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/GoLangPackages/","items":[]},{"title":"Webizen 2.5 Host Components","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/","items":[{"title":"Caddy Server Implementation","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/CaddyServerImplementation/","items":[]},{"title":"Implementation Progress","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/ImplementationProgress2.5/","items":[{"title":"Auth UseCases","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/ImplementationProgress2.5/AuthUseCases/","items":[]},{"title":"jsonInterfaces","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/ImplementationProgress2.5/jsonInterfaces/","items":[]},{"title":"Modding Solid","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/ImplementationProgress2.5/ModdingSolid/","items":[]},{"title":"VerifiableCredentialLibrary","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/ImplementationProgress2.5/VerifiableCredentialLibrary/","items":[]},{"title":"WebizenDB","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/ImplementationProgress2.5/WebizenDB/","items":[]}]},{"title":"Solid / RWW / CrossCloud - Fork","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/SolidBasedSystemsFork/","items":[]},{"title":"Webizen Networking 2.5","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/BasicWebizenNetworking2.5/","items":[{"title":"HeadScale Implementation","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/BasicWebizenNetworking2.5/HeadScaleImplementation/","items":[]},{"title":"Headscale UI","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/BasicWebizenNetworking2.5/Headscale-UI/","items":[]},{"title":"TailNets","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/BasicWebizenNetworking2.5/TailNets/","items":[]},{"title":"WebizenDNSNotes","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/BasicWebizenNetworking2.5/WebizenDNSNotes/","items":[]}]},{"title":"WebizenServiceManagement","url":"","items":[{"title":"Agent Logic","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/WebizenServiceManagement/AgentLogic/","items":[]},{"title":"Curator","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/WebizenServiceManagement/Curator/","items":[]},{"title":"Curator Logic Engine","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/WebizenServiceManagement/CuratorLogic/","items":[]},{"title":"EmailMethodsModification","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/WebizenServiceManagement/EmailMethodsModification/","items":[]},{"title":"Json App Interface","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/WebizenServiceManagement/JsonAppInterface/","items":[]},{"title":"WebizenHostWebPlatform V2.5","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/Webizen2.5HostComponents/WebizenServiceManagement/WebizenHostWebPlatform/","items":[]}]}]},{"title":"Webizen Identity Fabric","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/WebizenID/","items":[{"title":"WebizenID-DNS","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/WebizenID/WebizenID-DNS/","items":[]},{"title":"WebizenID-TLS","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/ImplementationNotes-2.5/WebizenID/WebizenID-TLS/","items":[]}]}]},{"title":"PCT-SocialAPI","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/PCT-SocialAPI/","items":[]},{"title":"Proof Of Concept App Reqs","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/POCAppRequirements/","items":[]},{"title":"WebizenEngine","url":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/WebizenEngine/","items":[]}]},{"title":"FunctionalObjectives","url":"/WebizenTechStack/Webizen2.5/FunctionalObjectives/","items":[{"title":"FUNCTIONS","url":"/WebizenTechStack/Webizen2.5/FunctionalObjectives/FUNCTIONS/","items":[]},{"title":"HelperAppObjectives","url":"/WebizenTechStack/Webizen2.5/FunctionalObjectives/HelperAppObjectives/","items":[]},{"title":"PCT-NOTES","url":"/WebizenTechStack/Webizen2.5/FunctionalObjectives/PCT-NOTES/","items":[]},{"title":"Webizen - Root Concepts","url":"/WebizenTechStack/Webizen2.5/FunctionalObjectives/RootConcepts/","items":[]}]},{"title":"POC of Core Concepts","url":"/WebizenTechStack/Webizen2.5/POCOfCoreConcepts/","items":[]},{"title":"Sacrifices-Hard Problems","url":"/WebizenTechStack/Webizen2.5/Sacrifices-hardproblems/","items":[]}]},{"title":"Webizen 3.0","url":"/WebizenTechStack/Webizen3.0/","items":[{"title":"Client Software Requirements","url":"/WebizenTechStack/Webizen3.0/ClientSoftwareRequirements/","items":[]},{"title":"Host Software Requirements","url":"/WebizenTechStack/Webizen3.0/HostServiceRequirements/","items":[{"title":"Domain Hosting","url":"/WebizenTechStack/Webizen3.0/HostServiceRequirements/DomainHosting/","items":[]},{"title":"Media Processing","url":"/WebizenTechStack/Webizen3.0/HostServiceRequirements/MediaProcessing/","items":[{"title":"Ffmpeg","url":"/WebizenTechStack/Webizen3.0/HostServiceRequirements/MediaProcessing/ffmpeg/","items":[]},{"title":"Opencv","url":"/WebizenTechStack/Webizen3.0/HostServiceRequirements/MediaProcessing/opencv/","items":[]}]},{"title":"Networking Considerations","url":"/WebizenTechStack/Webizen3.0/HostServiceRequirements/NetworkingConsiderations/","items":[]}]},{"title":"HyperMedia Containers","url":"/WebizenTechStack/Webizen3.0/HyperMediaContainers/","items":[{"title":"Knowledge Documents","url":"/WebizenTechStack/Webizen3.0/HyperMediaContainers/KnowledgeDocuments/","items":[]},{"title":"webizen.n3","url":"/WebizenTechStack/Webizen3.0/HyperMediaContainers/webizen.n3/","items":[]}]}]},{"title":"Webizen 3.5","url":"/WebizenTechStack/Webizen3.5/","items":[{"title":"AI Capabilities","url":"/WebizenTechStack/Webizen3.5/AICapabilities/","items":[{"title":"AI Capabilities Objectives","url":"/WebizenTechStack/Webizen3.5/AICapabilities/AICapabilitiesObjectives/","items":[]},{"title":"Audio & Video Analysis","url":"/WebizenTechStack/Webizen3.5/AICapabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/WebizenTechStack/Webizen3.5/AICapabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/WebizenTechStack/Webizen3.5/AICapabilities/Text Analysis/","items":[]}]},{"title":"Web Of Things (IoT)","url":"/WebizenTechStack/Webizen3.5/WebOfThings(IoT)/","items":[]},{"title":"Webizen AI OS Platform","url":"/WebizenTechStack/Webizen3.5/WebizenAiOS-Platform/","items":[]},{"title":"Webizen Pro Series","url":"/WebizenTechStack/Webizen3.5/WebizenProSeries/","items":[{"title":"Webizen Pro","url":"/WebizenTechStack/Webizen3.5/WebizenProSeries/WebizenPro/","items":[]},{"title":"Webizen Pro - Edge","url":"/WebizenTechStack/Webizen3.5/WebizenProSeries/WebizenPro-Edge/","items":[]},{"title":"WebizenProHardware","url":"/WebizenTechStack/Webizen3.5/WebizenProSeries/WebizenProHardware/","items":[]},{"title":"WebizenProSoftware","url":"/WebizenTechStack/Webizen3.5/WebizenProSeries/WebizenProSoftware/","items":[]}]},{"title":"Webizen Vision","url":"/WebizenTechStack/Webizen3.5/WebizenVision/","items":[]}]},{"title":"Webizen 4.0","url":"/WebizenTechStack/Webizen4.0/","items":[{"title":"MyWebizen","url":"/WebizenTechStack/Webizen4.0/MyWebizen/","items":[]}]},{"title":"Webizen Apps","url":"/WebizenTechStack/WebizenApps/","items":[{"title":"Historical RWW Apps","url":"/WebizenTechStack/WebizenApps/Historical RWW Apps/","items":[]},{"title":"Webizen 3.5 Apps","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/","items":[{"title":"Media Production","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/MediaProduction/","items":[]},{"title":"Mobile Apps","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/MobileApps/","items":[{"title":"Android","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/MobileApps/Android/","items":[]},{"title":"iOS","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/MobileApps/iOS/","items":[]},{"title":"The Webizen (mobile) Agent","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/MobileApps/TheWebizen(mobile)Agent/","items":[]}]},{"title":"Search","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/Search/","items":[]},{"title":"Webizen Host Apps","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/HostApps/","items":[{"title":"Email Services","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/HostApps/EmailServices/","items":[]},{"title":"Webizen Server - WebApps","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/HostApps/WebizenServer-WebApps/","items":[]},{"title":"Website Hosting","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/HostApps/WebsiteHost/","items":[]}]},{"title":"Webizen Manager","url":"/WebizenTechStack/WebizenApps/Webizen3.5Apps/WebizenManager/","items":[]}]},{"title":"Webizen Interfaces","url":"/WebizenTechStack/WebizenApps/WebizenInterfaces/","items":[{"title":"API","url":"/WebizenTechStack/WebizenApps/WebizenInterfaces/API/","items":[]},{"title":"The Attendant","url":"/WebizenTechStack/WebizenApps/WebizenInterfaces/TheAttendant/","items":[]},{"title":"TheWebizen","url":"/WebizenTechStack/WebizenApps/WebizenInterfaces/TheWebizen/","items":[]}]},{"title":"Webizen3.0Apps","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/","items":[{"title":"Design Goals","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/DesignGoals/","items":[]},{"title":"MobileBasics","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/MobileBasics/","items":[]},{"title":"WADE No-Code - Low-Code","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/NoCodeAppDev/","items":[]},{"title":"Webizen App Spec","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/WebizenAppSpec/","items":[{"title":"DOAP Form Example","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/WebizenAppSpec/DOAP form Example/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/WebizenAppSpec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL Query Specs","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/WebizenAppSpec/WebSpec/Query Interfaces/GraphQL Query Specs/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/WebizenAppSpec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/WebizenAppSpec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]},{"title":"Webizen Core Apps","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/","items":[{"title":"Agent Directory","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/AgentDirectory/","items":[]},{"title":"Cookie Monster","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/CookieMonster/","items":[]},{"title":"Credentials & Contracts Manager","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/Credentials & Contracts Manager/","items":[]},{"title":"Knowledge Clouds","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/KnowledgeClouds/","items":[]},{"title":"Library","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/Library/","items":[{"title":"Form Builder","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/Library/Form Builder/","items":[]},{"title":"HyperMedia Library","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/Library/HyperMediaLibrary/","items":[]}]},{"title":"SocialMedia","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/SocialMedia/","items":[]},{"title":"Temporal Apps","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/TemporalApps/","items":[{"title":"Calendar","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/TemporalApps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/TemporalApps/TimelineInterface/","items":[]}]},{"title":"The Work Platform","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/TheWorkPlatform/","items":[]},{"title":"Webizen Social","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/WebizenSocial/","items":[]}]},{"title":"Webizen-Connect","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/Webizen-Connect/","items":[{"title":"Data Applications","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/Webizen-Connect/Data Applications/","items":[]},{"title":"Social Media APIs","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/Webizen-Connect/Social Media APIs/","items":[]},{"title":"UI Toolkits","url":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/Webizen-Connect/UIToolKits/","items":[]}]}]}]}]},{"title":"Work in Progress","url":"/WorkInProgress/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/GuideForDevelopers/","title":"DRAFT: Guide For Developers","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/GuideForDevelopers/WebizenTerms/","title":"Webizen Terms","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/Human-Centric/","title":"Human Centric","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/WebizenTechStack/Webizen2.5/DevNotesWebizen2.5/","title":"DevNotesWebizen2.5","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/WebizenTechStack/WebizenApps/WebizenInterfaces/API/","title":"API","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/WebizenTechStack/WebizenApps/WebizenInterfaces/TheAttendant/","title":"The Attendant","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/CookieMonster/","title":"Cookie Monster","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/SocialMedia/","title":"SocialMedia","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/CoreApps/TemporalApps/","title":"Temporal Apps","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/WebizenTechStack/WebizenApps/Webizen3.0Apps/Webizen-Connect/UIToolKits/","title":"UI Toolkits","lastUpdatedAt":"2023-02-04T16:41:41.000Z","lastUpdated":"2/4/2023"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}