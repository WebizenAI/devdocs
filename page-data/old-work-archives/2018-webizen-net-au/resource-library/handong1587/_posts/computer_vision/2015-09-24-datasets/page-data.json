{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/",
    "result": {"data":{"mdx":{"id":"0bcc799c-3186-5041-9db5-36b7ea40dc27","tableOfContents":{"items":[{"url":"#classification--recognition","title":"Classification / Recognition","items":[{"url":"#face","title":"Face"},{"url":"#vehicle","title":"Vehicle"},{"url":"#scene-recognition","title":"Scene Recognition"},{"url":"#mnist","title":"MNIST"}]},{"url":"#food","title":"Food"},{"url":"#detection","title":"Detection","items":[{"url":"#face-detection","title":"Face Detection"},{"url":"#pedestrian-detection","title":"Pedestrian Detection"}]},{"url":"#full-body-annotations","title":"Full-Body Annotations","items":[{"url":"#vehicle-detection","title":"Vehicle Detection"}]},{"url":"#vehicle-re-id","title":"Vehicle Re-ID","items":[{"url":"#logo-detection","title":"Logo Detection"}]},{"url":"#head-detection","title":"Head Detection","items":[{"url":"#detection-from-video","title":"Detection From Video"}]},{"url":"#segmentation","title":"Segmentation","items":[{"url":"#mapillary-vistas-dataset","title":"Mapillary Vistas Dataset"}]},{"url":"#pascal-voc","title":"PASCAL VOC","items":[{"url":"#augmented-pascal-voc","title":"Augmented Pascal VOC"}]},{"url":"#supervisely-person","title":"Supervisely Person"},{"url":"#microsoft-coco","title":"Microsoft COCO"},{"url":"#the-oxford-iiit-pet-dataset","title":"The Oxford-IIIT Pet Dataset","items":[{"url":"#coco-stuff","title":"COCO-Stuff"}]},{"url":"#scene-parsing","title":"Scene Parsing"},{"url":"#imagenet","title":"ImageNet"},{"url":"#captioning--description","title":"Captioning / Description"},{"url":"#video","title":"Video"},{"url":"#scene","title":"Scene"},{"url":"#autonomous-driving","title":"Autonomous Driving"},{"url":"#ocr","title":"OCR"},{"url":"#retrieval","title":"Retrieval"},{"url":"#person-re-id","title":"Person Re-ID"},{"url":"#fashion","title":"Fashion"},{"url":"#attribute-datasets","title":"Attribute Datasets","items":[{"url":"#pedestrian-attribute-recognition","title":"Pedestrian Attribute Recognition"}]},{"url":"#tracking","title":"Tracking"},{"url":"#color-classification","title":"Color Classification"},{"url":"#license-plate-detection-and-recognition","title":"License Plate Detection and Recognition"},{"url":"#face-anti-spoofing","title":"Face Anti-Spoofing"},{"url":"#tools","title":"Tools"},{"url":"#artist","title":"Artist"},{"url":"#resources","title":"Resources"}]},"fields":{"title":"Computer Vision Datasets","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets.md","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"Computer Vision Datasets","description":null,"imageAlt":null,"tags":[],"date":"2015-09-24T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"computer_vision\",\n  \"title\": \"Computer Vision Datasets\",\n  \"date\": \"2015-09-24T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Datasets who is the best at X ?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rodrigob.github.io/are_we_there_yet/build/#datasets\"\n  }, \"http://rodrigob.github.io/are_we_there_yet/build/#datasets\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Computer Vision Datasets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"website: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://clickdamage.com/sourcecode/index.html\"\n  }, \"http://clickdamage.com/sourcecode/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://clickdamage.com/sourcecode/cv_datasets.php\"\n  }, \"http://clickdamage.com/sourcecode/cv_datasets.php\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1pJmqD4n\"\n  }, \"http://pan.baidu.com/s/1pJmqD4n\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Introducing the Open Images Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://research.googleblog.com/2016/09/introducing-open-images-dataset.html\"\n  }, \"https://research.googleblog.com/2016/09/introducing-open-images-dataset.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/openimages/dataset\"\n  }, \"https://github.com/openimages/dataset\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Academic Torrents: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b\"\n  }, \"http://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A parallel download util for Google's open image dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ejlb/google-open-image-download\"\n  }, \"https://github.com/ejlb/google-open-image-download\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image & Vision Group - Datasets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Image & Vision , Clothing & Fashion, Computer Graphics, Video Sequences\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://caiivg.weebly.com/dataset.html\"\n  }, \"http://caiivg.weebly.com/dataset.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Huizhong Chen - Datasets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google I/O Dataset, Names 100 Dataset, Clothing Attributes Dataset,\\nStanford Mobile Visual Search Dataset, CNN 2-Hours Videos Dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://huizhongchen.github.io/datasets.html#clothingattributedataset\"\n  }, \"http://huizhongchen.github.io/datasets.html#clothingattributedataset\"))), mdx(\"h1\", {\n    \"id\": \"classification--recognition\"\n  }, \"Classification / Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Large-Scale Car Dataset for Fine-Grained Categorization and Verification\")), mdx(\"img\", {\n    \"src\": \"http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/illustration.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html\"\n  }, \"http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1506.08959\"\n  }, \"http://arxiv.org/abs/1506.08959\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CIFAR-10 / CIFAR100\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\\nThere are 50000 training images and 10000 test images.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.toronto.edu/~kriz/cifar.html\"\n  }, \"http://www.cs.toronto.edu/~kriz/cifar.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tencent ML-Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Largest multi-label image database; ResNet-101 model; 80.73% top-1 acc on ImageNet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Tencent/tencent-ml-images\"\n  }, \"https://github.com/Tencent/tencent-ml-images\"))), mdx(\"h2\", {\n    \"id\": \"face\"\n  }, \"Face\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The MegaFace Benchmark: 1 Million Faces for Recognition at Scale\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://megaface.cs.washington.edu/\"\n  }, \"http://megaface.cs.washington.edu/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.00596\"\n  }, \"http://arxiv.org/abs/1512.00596\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.08221\"\n  }, \"http://arxiv.org/abs/1607.08221\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MSR Image Recognition Challenge (IRC)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/\"\n  }, \"https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"UMDFaces: An Annotated Face Dataset for Training Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01484\"\n  }, \"https://arxiv.org/abs/1611.01484\"))), mdx(\"h2\", {\n    \"id\": \"vehicle\"\n  }, \"Vehicle\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Comprehensive Cars (CompCars) dataset\")), mdx(\"img\", {\n    \"src\": \"http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/illustration.png\",\n    \"alt\": null\n  }), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/\"\n  }, \"http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BoxCars: Improving Fine-Grained Recognition of Vehicles Using 3-D Bounding Boxes in Traffic Surveillance \", \"[IEEE T-ITS]\")), mdx(\"img\", {\n    \"src\": \"https://medusa.fit.vutbr.cz/traffic/wp-content/uploads/2017/03/boxcars_bb_estimation_pipeline.png\",\n    \"alt\": null\n  }), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/\"\n  }, \"https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Vehicle Make and Model Recognition Dataset (VMMRdb)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: containing 9,170 classes consisting of 291,752 images, covering models manufactured between 1950 to 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vmmrdb.cecsresearch.org/\"\n  }, \"http://vmmrdb.cecsresearch.org/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cars Dataset\")), mdx(\"img\", {\n    \"src\": \"http://ai.stanford.edu/~jkrause/cars/class_montage_flop.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: contains 16,185 images of 196 classes of cars.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\"\n  }, \"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\"))), mdx(\"h2\", {\n    \"id\": \"scene-recognition\"\n  }, \"Scene Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Places: An Image Database for Deep Scene Understanding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://places.csail.mit.edu/index.html\"\n  }, \"http://places.csail.mit.edu/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.02055\"\n  }, \"https://arxiv.org/abs/1610.02055\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Places2\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Places2 contains more than 10 million images comprising 400+ unique scene categories\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://places2.csail.mit.edu/\"\n  }, \"http://places2.csail.mit.edu/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Places365-CNNs for Scene Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/CSAILVision/places365\"\n  }, \"https://github.com/CSAILVision/places365\"))), mdx(\"h2\", {\n    \"id\": \"mnist\"\n  }, \"MNIST\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EMNIST: an extension of MNIST to handwritten letters\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.05373\"\n  }, \"https://arxiv.org/abs/1702.05373\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fashion-MNIST\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.07747\"\n  }, \"https://arxiv.org/abs/1708.07747\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zalandoresearch/fashion-mnist\"\n  }, \"https://github.com/zalandoresearch/fashion-mnist\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"benchmark: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n  }, \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"))), mdx(\"h1\", {\n    \"id\": \"food\"\n  }, \"Food\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"3 Million Instacart Orders, Open Sourced\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2\"\n  }, \"https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2\")), mdx(\"h1\", {\n    \"id\": \"detection\"\n  }, \"Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: YouTube-BoundingBoxes (YT-BB)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://research.google.com/youtubebb/\"\n  }, \"https://research.google.com/youtubebb/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.00824\"\n  }, \"https://arxiv.org/abs/1702.00824\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.00525\"\n  }, \"https://arxiv.org/abs/1804.00525\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Exclusively Dark (ExDark) Image Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Exclusively Dark (ExDARK) dataset which to the best of our knowledge,\\nis the largest collection of low-light images taken in very low-light environments to twilight (i.e 10 different conditions)\\nto-date with image class and object level annotations.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cs-chan/Exclusively-Dark-Image-Dataset\"\n  }, \"https://github.com/cs-chan/Exclusively-Dark-Image-Dataset\"))), mdx(\"h2\", {\n    \"id\": \"face-detection\"\n  }, \"Face Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FDDB: Face Detection Data Set and Benchmark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vis-www.cs.umass.edu/fddb/index.html\"\n  }, \"http://vis-www.cs.umass.edu/fddb/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"results: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vis-www.cs.umass.edu/fddb/results.html\"\n  }, \"http://vis-www.cs.umass.edu/fddb/results.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"WIDER FACE: A Face Detection Benchmark\")), mdx(\"img\", {\n    \"src\": \"http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/intro.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/\"\n  }, \"http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06523\"\n  }, \"http://arxiv.org/abs/1511.06523\"))), mdx(\"h2\", {\n    \"id\": \"pedestrian-detection\"\n  }, \"Pedestrian Detection\"), mdx(\"img\", {\n    \"src\": \"https://sshao0516.github.io/CrowdHuman/images/fig1.png\",\n    \"alt\": null\n  }), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Caltech Pedestrian Detection Benchmark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\"\n  }, \"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Caltech Pedestrian Dataset Converter\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/mitmul/caltech-pedestrian-dataset-converter\"\n  }, \"https://github.com/mitmul/caltech-pedestrian-dataset-converter\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CityPersons: A Diverse Dataset for Pedestrian Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.05693\"\n  }, \"https://arxiv.org/abs/1702.05693\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"bitbucket: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://bitbucket.org/shanshanzhang/citypersons\"\n  }, \"https://bitbucket.org/shanshanzhang/citypersons\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"supplemental: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf\"\n  }, \"http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CrowdHuman: A Benchmark for Detecting Human in a Crowd\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CrowdHuman contains 15000, 4370 and 5000 images for training, validation, and testing, respectively.\\na total of 470K human instances from train and validation subsets and 23 persons per image, with various kinds of occlusions in the dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sshao0516.github.io/CrowdHuman/\"\n  }, \"https://sshao0516.github.io/CrowdHuman/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EuroCity Persons Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: collected on-board a moving vehicle in 31 cities of 12 European countries,\\nover 238200 person instances manually labeled in over 47300 images,\\ncontains a large number of person orientation annotations (over 211200)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://eurocity-dataset.tudelft.nl/\"\n  }, \"https://eurocity-dataset.tudelft.nl/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.07193\"\n  }, \"https://arxiv.org/abs/1805.07193\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild\")), mdx(\"img\", {\n    \"src\": \"http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/files/intro.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/\"\n  }, \"http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/\"))), mdx(\"h1\", {\n    \"id\": \"full-body-annotations\"\n  }, \"Full-Body Annotations\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"COCO-WholeBody\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/jin-s13/COCO-WholeBody\"\n  }, \"https://github.com/jin-s13/COCO-WholeBody\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Halpe Full-Body Human Keypoints and HOI-Det dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Halpe: full body human pose estimation and human-object interaction detection dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github:\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Fang-Haoshu/Halpe-FullBody\"\n  }, \"https://github.com/Fang-Haoshu/Halpe-FullBody\"))), mdx(\"h2\", {\n    \"id\": \"vehicle-detection\"\n  }, \"Vehicle Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Toyota Motor Europe (TME) Motorway Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: composed by 28 clips for a total of approximately 27 minutes (30000+ frames) with vehicle annotation\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cmp.felk.cvut.cz/data/motorway/\"\n  }, \"http://cmp.felk.cvut.cz/data/motorway/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Welcome to BIT-Vehicle Dataset\")), mdx(\"img\", {\n    \"src\": \"http://iitlab.bit.edu.cn/mcislab/vehicledb/dataset.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 9,850 vehicle images, sizes of 1600\", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"1200 and 1920\"), \"1080 captured from two cameras at different time and places in the dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://iitlab.bit.edu.cn/mcislab/vehicledb/\"\n  }, \"http://iitlab.bit.edu.cn/mcislab/vehicledb/\"))), mdx(\"h1\", {\n    \"id\": \"vehicle-re-id\"\n  }, \"Vehicle Re-ID\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Large-Scale Dataset for Vehicle Re-Identification in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/PKU-IMRE/VERI-Wild\"\n  }, \"https://github.com/PKU-IMRE/VERI-Wild\"))), mdx(\"h2\", {\n    \"id\": \"logo-detection\"\n  }, \"Logo Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"QMUL-OpenLogo: Open Logo Detection Challenge\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: QMUL-OpenLogo contains 27,083 images from 352 logo classes,\\nbuilt by aggregating and refining 7 existing datasets and establishing an open logo detection evaluation protocol\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://qmul-openlogo.github.io/\"\n  }, \"https://qmul-openlogo.github.io/\"))), mdx(\"h1\", {\n    \"id\": \"head-detection\"\n  }, \"Head Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SCUT-HEAD\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: SCUT HEAD is a large-scale head detection dataset, including 4405 images labeld with 111251 heads.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release\"\n  }, \"https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"HollywoodHeads dataset\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.di.ens.fr/willow/research/headdetection/\"\n  }, \"http://www.di.ens.fr/willow/research/headdetection/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Brainwash dataset.\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://exhibits.stanford.edu/data/catalog/sx925dc9385\"\n  }, \"https://exhibits.stanford.edu/data/catalog/sx925dc9385\")), mdx(\"h2\", {\n    \"id\": \"detection-from-video\"\n  }, \"Detection From Video\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"YouTube-Objects dataset v2.2\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://calvin.inf.ed.ac.uk/datasets/youtube-objects-dataset/\"\n  }, \"http://calvin.inf.ed.ac.uk/datasets/youtube-objects-dataset/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ILSVRC2015: Object detection from video (VID)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vision.cs.unc.edu/ilsvrc2015/download-videos-3j16.php#vid\"\n  }, \"http://vision.cs.unc.edu/ilsvrc2015/download-videos-3j16.php#vid\"))), mdx(\"h1\", {\n    \"id\": \"segmentation\"\n  }, \"Segmentation\"), mdx(\"h2\", {\n    \"id\": \"mapillary-vistas-dataset\"\n  }, \"Mapillary Vistas Dataset\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Mapillary Vistas Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 25,000 high-resolution images, 100 object categories, 60 of those instance-specific\\n\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.mapillary.com/dataset/\"\n  }, \"https://www.mapillary.com/dataset/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Releasing the World\\u2019s Largest Street-level Imagery Dataset for Teaching Machines to See\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html\"\n  }, \"http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-Human Parsing\")), mdx(\"img\", {\n    \"src\": \"https://lv-mhp.github.io/static/images/3.png\",\n    \"alt\": null\n  }), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lv-mhp.github.io/\"\n  }, \"https://lv-mhp.github.io/\")), mdx(\"h1\", {\n    \"id\": \"pascal-voc\"\n  }, \"PASCAL VOC\"), mdx(\"h2\", {\n    \"id\": \"augmented-pascal-voc\"\n  }, \"Augmented Pascal VOC\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://home.bharathh.info/pubs/codes/SBD/download.html\"\n  }, \"http://home.bharathh.info/pubs/codes/SBD/download.html\")), mdx(\"h1\", {\n    \"id\": \"supervisely-person\"\n  }, \"Supervisely Person\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://supervise.ly/\"\n  }, \"https://supervise.ly/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469\"\n  }, \"https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469\"))), mdx(\"h1\", {\n    \"id\": \"microsoft-coco\"\n  }, \"Microsoft COCO\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mscoco.org/\"\n  }, \"http://mscoco.org/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pdollar/coco\"\n  }, \"https://github.com/pdollar/coco\"))), mdx(\"h1\", {\n    \"id\": \"the-oxford-iiit-pet-dataset\"\n  }, \"The Oxford-IIIT Pet Dataset\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: a 37 category pet dataset with roughly 200 images for each class.\\nAll images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.robots.ox.ac.uk/~vgg/data/pets/\"\n  }, \"http://www.robots.ox.ac.uk/~vgg/data/pets/\"))), mdx(\"h2\", {\n    \"id\": \"coco-stuff\"\n  }, \"COCO-Stuff\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"COCO-Stuff: Thing and Stuff Classes in Context\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"COCO-Stuff 10K dataset v1.1\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1612.03716\"\n  }, \"https://arxiv.org/abs/1612.03716\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/nightrome/cocostuff\"\n  }, \"https://github.com/nightrome/cocostuff\")), mdx(\"h1\", {\n    \"id\": \"scene-parsing\"\n  }, \"Scene Parsing\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MIT Scene Parsing Benchmark\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://sceneparsing.csail.mit.edu/\"\n  }, \"http://sceneparsing.csail.mit.edu/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ADE20K\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: train: 20,120 images, val: 2000 images. contains 150 stuff/object category labels (e.g., wall, sky, and tree) and 1,038 imagelevel scene descriptors (e.g., airport terminal, bedroom, and street).\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://groups.csail.mit.edu/vision/datasets/ADE20K/\"\n  }, \"http://groups.csail.mit.edu/vision/datasets/ADE20K/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Understanding of Scenes through the ADE20K Dataset\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1608.05442\"\n  }, \"https://arxiv.org/abs/1608.05442\")), mdx(\"h1\", {\n    \"id\": \"imagenet\"\n  }, \"ImageNet\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"synsets: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://image-net.org/challenges/LSVRC/2014/browse-det-synsets\"\n  }, \"http://image-net.org/challenges/LSVRC/2014/browse-det-synsets\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ImageNet-Utils\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Utils to help download images by id, crop bounding box, label images, etc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tzutalin/ImageNet_Utils\"\n  }, \"https://github.com/tzutalin/ImageNet_Utils\"))), mdx(\"h1\", {\n    \"id\": \"captioning--description\"\n  }, \"Captioning / Description\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TGIF: A New Dataset and Benchmark on Animated GIF Description\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.02748\"\n  }, \"http://arxiv.org/abs/1604.02748\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/raingo/TGIF-Release\"\n  }, \"https://github.com/raingo/TGIF-Release\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Collecting Multilingual Parallel Video Descriptions Using Mechanical Turk\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 1970 YouTube video snippets: 1200 training, 100 validation, 670 test\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.utexas.edu/users/ml/clamp/videoDescription/\"\n  }, \"http://www.cs.utexas.edu/users/ml/clamp/videoDescription/\"))), mdx(\"h1\", {\n    \"id\": \"video\"\n  }, \"Video\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"Dataset\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"# Videos\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"# Classes\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"Year\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"Manually Labeled ?\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"Kodak\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"1,358\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"25\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2007\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2713\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"HMDB51\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"7000\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"51\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  })), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"Charades\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"9848\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"157\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  })), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"MCG-WEBV\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"234,414\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"15\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2009\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2713\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"CCV\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"9,317\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"20\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2011\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2713\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"UCF-101\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"13,320\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"101\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2012\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2713\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"THUMOS-2\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"18,394\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"101\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2014\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2713\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"MED-2014\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u224828,000\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"20\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2014\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2713\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"Sports-1M\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"1M\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"487\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2014\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2717\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"ActivityNet\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"27,801\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"203\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2015\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2713\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"FCVID\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"91,223\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"239\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"2015\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"\\u2713\")))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"UCF101 - Action Recognition Data Set\")), mdx(\"img\", {\n    \"src\": \"http://crcv.ucf.edu/data/UCF101/UCF101.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://crcv.ucf.edu/data/UCF101.php\"\n  }, \"http://crcv.ucf.edu/data/UCF101.php\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"HMDB51: A Large Video Database for Human Motion Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"\n  }, \"http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://activity-net.org/\"\n  }, \"http://activity-net.org/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"download: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://activity-net.org/download.html\"\n  }, \"http://activity-net.org/download.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/activitynet\"\n  }, \"https://github.com/activitynet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sports-1M\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md\"\n  }, \"https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gtoderici/sports-1m-dataset/\"\n  }, \"https://github.com/gtoderici/sports-1m-dataset/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"thumbnails: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/people/karpathy/deepvideo/classes.html\"\n  }, \"http://cs.stanford.edu/people/karpathy/deepvideo/classes.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Charades Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: This dataset guides our research into unstructured video activity recogntion and commonsense reasoning for daily human activities.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The dataset contains 66,500 temporal annotations for 157 action classes,\\n41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://allenai.org/plato/charades/\"\n  }, \"http://allenai.org/plato/charades/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FCVID: Fudan-Columbia Video Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://bigvid.fudan.edu.cn/FCVID/\"\n  }, \"http://bigvid.fudan.edu.cn/FCVID/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"YouTube-8M: A Large-Scale Video Classification Benchmark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://research.google.com/youtube8m/\"\n  }, \"http://research.google.com/youtube8m/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.08675\"\n  }, \"http://arxiv.org/abs/1609.08675\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"stabilized video frames\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 9 TB, 35,000,000 clips, 32 frames\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Generating Videos with Scene Dynamics\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.mit.edu/vondrick/tinyvideo/#data\"\n  }, \"http://web.mit.edu/vondrick/tinyvideo/#data\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Kinetics Human Action Video Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://deepmind.com/research/open-source/open-source-datasets/kinetics/\"\n  }, \"https://deepmind.com/research/open-source/open-source-datasets/kinetics/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.06950\"\n  }, \"https://arxiv.org/abs/1705.06950\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"e-Lab Video Data Set(s)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"Currently, e-VDS35 has 35 classes and a total of 2050 videos of roughly 10 seconds each (see histogram below). We are aiming to collect overall 1750 (50 \\xD7 35) videos with your help.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://engineering.purdue.edu/elab/eVDS\"\n  }, \"https://engineering.purdue.edu/elab/eVDS\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Dataset Overview\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Sortable and searchable compilation of video dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.di.ens.fr/~miech/datasetviz/\"\n  }, \"https://www.di.ens.fr/~miech/datasetviz/\"))), mdx(\"h1\", {\n    \"id\": \"scene\"\n  }, \"Scene\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Imperial College London\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://robotvault.bitbucket.org/scenenet-rgbd.html\"\n  }, \"https://robotvault.bitbucket.org/scenenet-rgbd.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.05079\"\n  }, \"https://arxiv.org/abs/1612.05079\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jmccormac/pySceneNetRGBD\"\n  }, \"https://github.com/jmccormac/pySceneNetRGBD\"))), mdx(\"h1\", {\n    \"id\": \"autonomous-driving\"\n  }, \"Autonomous Driving\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BDD: Berkely Deep Drive\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 100,000 HD video sequences of over 1,100-hour driving experience across many different times in the day,\\nweather conditions, and driving scenarios\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://bdd-data.berkeley.edu/\"\n  }, \"http://bdd-data.berkeley.edu/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ucbdrive/bdd-data\"\n  }, \"https://github.com/ucbdrive/bdd-data\"))), mdx(\"h1\", {\n    \"id\": \"ocr\"\n  }, \"OCR\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vision.cornell.edu/se3/coco-text/\"\n  }, \"http://vision.cornell.edu/se3/coco-text/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1601.07140\"\n  }, \"http://arxiv.org/abs/1601.07140\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Chinese Text in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 32,285 high resolution images, 1,018,402 character instances, 3,850 character categories, 6 kinds of attributes\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://ctwdataset.github.io/\"\n  }, \"https://ctwdataset.github.io/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.00085\"\n  }, \"https://arxiv.org/abs/1803.00085\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.10412\"\n  }, \"https://arxiv.org/abs/1903.10412\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chongshengzhang/shopsign\"\n  }, \"https://github.com/chongshengzhang/shopsign\"))), mdx(\"h1\", {\n    \"id\": \"retrieval\"\n  }, \"Retrieval\"), mdx(\"p\", null, \"Oxford5k\"), mdx(\"p\", null, \"Paris6k\"), mdx(\"p\", null, \"Oxford105k\"), mdx(\"p\", null, \"UKB\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"NUS-WIDE\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ImageNet-YahooQA\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"University-1652\"), \": \"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/Data.jpg\",\n    \"alt\": null\n  }), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2002.12186\"\n  }, \"[Paper]\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_drone.jpg?raw=true\"\n  }, \"[Explore Drone-view Data]\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_satellite.jpg?raw=true\"\n  }, \"[Explore Satellite-view Data]\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_street.jpg?raw=true\"\n  }, \"[Explore Street-view Data]\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.youtube.com/embed/dzxXPp8tVn4?vq=hd1080\"\n  }, \"[Video Sample]\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://zhuanlan.zhihu.com/p/110987552\"\n  }, \"[\\u4E2D\\u6587\\u4ECB\\u7ECD]\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Dataset and Baseline Code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/layumi/University1652-Baseline\"\n  }, \"https://github.com/layumi/University1652-Baseline\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepFashion: In-shop Clothes Retrieval\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 7,982 number of clothing items; 52,712 number of in-shop clothes images, and ~200,000 cross-pose/scale pairs; Each image is annotated by bounding box, clothing type and pose type.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html\"\n  }, \"http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html\"))), mdx(\"h1\", {\n    \"id\": \"person-re-id\"\n  }, \"Person Re-ID\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"Dataset\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"Description\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"CUHK01\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"971 identities, 3884 images, manually cropped\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"CUHK02\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"1816 identities, 7264 images, manually cropped\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"CUHK03\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": \"center\"\n  }, \"1360 identities, 13164 images, manually cropped + automatically detected\")))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Person Re-identification Datasets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html\"\n  }, \"http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/RSL-NEU/person-reid-benchmark\"\n  }, \"https://github.com/RSL-NEU/person-reid-benchmark\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CUHK Person Re-identification Datasets\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html\"\n  }, \"http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PRW (Person Re-identification in the Wild) Dataset\")), mdx(\"img\", {\n    \"src\": \"http://www.liangzheng.com.cn/Project/pipeline_prw.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.liangzheng.com.cn/Project/project_prw.html\"\n  }, \"http://www.liangzheng.com.cn/Project/project_prw.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/liangzheng06/PRW-baseline\"\n  }, \"https://github.com/liangzheng06/PRW-baseline\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Person Re-identification in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017 spotlight\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1604.02531\"\n  }, \"https://arxiv.org/abs/1604.02531\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DukeMTMC-reID\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DukeMTMC-reID is a subset of the DukeMTMC for image-based re-identification, in the format of the Market-1501 dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/layumi/DukeMTMC-reID_evaluation\"\n  }, \"https://github.com/layumi/DukeMTMC-reID_evaluation\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DukeMTMC4ReID\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DukeMTMC4ReID dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/NEU-Gou/DukeReID\"\n  }, \"https://github.com/NEU-Gou/DukeReID\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Person Re-ID (PRID) Dataset 2011\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/\"\n  }, \"https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MARS (Motion Analysis and Re-identification Set) Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: an extension of the Market-1501 dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.liangzheng.com.cn/Project/project_mars.html\"\n  }, \"http://www.liangzheng.com.cn/Project/project_mars.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/liangzheng06/MARS-evaluation\"\n  }, \"https://github.com/liangzheng06/MARS-evaluation\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"X-MARS Reordering of the MARS Dataset for Image to Video Evaluation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: This repository provides the X-MARS dataset splits for image to video/tracklet evaluation\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/andreas-eberle/x-mars\"\n  }, \"https://github.com/andreas-eberle/x-mars\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MSMT17\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 15-camera (12 outdoor cameras, 3 indoor cameras), 4,101 Identities, 126,441 BBoxes\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.pkuvmc.com/publications/longhui.html\"\n  }, \"http://www.pkuvmc.com/publications/longhui.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"soa: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.pkuvmc.com/publications/state_of_the_art.html\"\n  }, \"http://www.pkuvmc.com/publications/state_of_the_art.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Labeled Pedestrian in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: train/test identities: 1,975/756\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://liuyu.us/dataset/lpw/\"\n  }, \"http://liuyu.us/dataset/lpw/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SenseReID\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://drive.google.com/file/d/0B56OfSrVI8hubVJLTzkwV2VaOWM/view\"\n  }, \"https://drive.google.com/file/d/0B56OfSrVI8hubVJLTzkwV2VaOWM/view\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"3DPeS\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.openvisor.org/3dpes.asp\"\n  }, \"http://www.openvisor.org/3dpes.asp\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"iQIYI-VID: A Large Dataset for Multi-modal Person Identification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.07548\"\n  }, \"https://arxiv.org/abs/1811.07548\")), mdx(\"h1\", {\n    \"id\": \"fashion\"\n  }, \"Fashion\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large-scale Fashion (DeepFashion) Database\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Attribute Prediction, Consumer-to-shop Clothes Retrieval, In-shop Clothes Retrieval, and Landmark Detection\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html\"\n  }, \"http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Apparel classification with Style\")), mdx(\"img\", {\n    \"src\": \"http://people.ee.ethz.ch/~lbossard/projects/accv12/img/motivation.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 15 clothing classes, 88951 images\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://people.ee.ethz.ch/~lbossard/projects/accv12/index.html\"\n  }, \"http://people.ee.ethz.ch/~lbossard/projects/accv12/index.html\"))), mdx(\"h1\", {\n    \"id\": \"attribute-datasets\"\n  }, \"Attribute Datasets\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attribute Datasets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: in total 41,585 pedestrian samples, each of which is annotated with 72 attributes\\nas well as viewpoints, occlusions, body parts information\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.ecse.rpi.edu/homepages/cvrl/database/AttributeDataset.htm\"\n  }, \"https://www.ecse.rpi.edu/homepages/cvrl/database/AttributeDataset.htm\"))), mdx(\"h2\", {\n    \"id\": \"pedestrian-attribute-recognition\"\n  }, \"Pedestrian Attribute Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Richly Annotated Dataset for Pedestrian Attribute Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rap.idealtest.org/\"\n  }, \"http://rap.idealtest.org/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1603.07054\"\n  }, \"https://arxiv.org/abs/1603.07054\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pedestrian Attribute Recognition At Far Distance\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: PEdesTrian Attribute (PETA)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mmlab.ie.cuhk.edu.hk/projects/PETA.html\"\n  }, \"http://mmlab.ie.cuhk.edu.hk/projects/PETA.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf\"\n  }, \"http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Market-1501_Attribute\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/vana77/Market-1501_Attribute\"\n  }, \"https://github.com/vana77/Market-1501_Attribute\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://vana77.github.io\"\n  }, \"https://vana77.github.io\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DukeMTMC-attribute\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/vana77/DukeMTMC-attribute\"\n  }, \"https://github.com/vana77/DukeMTMC-attribute\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://vana77.github.io\"\n  }, \"https://vana77.github.io\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Parse27k\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Pedestrian Attribute Recognition in Sequences\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: >27,000 annotated pedestrians, 10 attributes\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.vision.rwth-aachen.de/page/parse27k\"\n  }, \"https://www.vision.rwth-aachen.de/page/parse27k\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"tools: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/psudowe/parse27k_tools\"\n  }, \"https://github.com/psudowe/parse27k_tools\"))), mdx(\"h1\", {\n    \"id\": \"tracking\"\n  }, \"Tracking\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://detrac-db.rit.albany.edu/\"\n  }, \"http://detrac-db.rit.albany.edu/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1511.04136\"\n  }, \"https://arxiv.org/abs/1511.04136\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DukeMTMC: Duke Multi-Target, Multi-Camera Tracking Project\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DukeMTMC aims to accelerate advances in multi-target multi-camera tracking. It provides a tracking system that works within and across cameras, a new large scale HD video data set recorded by 8 synchronized cameras with more than 7,000 single camera trajectories and over 2,000 unique identities\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vision.cs.duke.edu/DukeMTMC/\"\n  }, \"http://vision.cs.duke.edu/DukeMTMC/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The WILDTRACK Seven-Camera HD Dataset\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://cvlab.epfl.ch/data/wildtrack\"\n  }, \"https://cvlab.epfl.ch/data/wildtrack\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GOT-10k: Generic Object Tracking Benchmark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A large, high-diversity, one-shot database for generic object tracking in the wild\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://got-10k.aitestunion.com/\"\n  }, \"http://got-10k.aitestunion.com/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/got-10k/toolkit\"\n  }, \"https://github.com/got-10k/toolkit\"))), mdx(\"h1\", {\n    \"id\": \"color-classification\"\n  }, \"Color Classification\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Vehicle Color Recognition on an Urban Road by Feature Context\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://mclab.eic.hust.edu.cn/~pchen/project.html\"\n  }, \"http://mclab.eic.hust.edu.cn/~pchen/project.html\")), mdx(\"h1\", {\n    \"id\": \"license-plate-detection-and-recognition\"\n  }, \"License Plate Detection and Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Application-Oriented License Plate (AVOP) Database\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://aolpr.ntust.edu.tw/lab/download.html\"\n  }, \"http://aolpr.ntust.edu.tw/lab/download.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CCPD: Chinese City Parking Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf\"\n  }, \"http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/detectRecog/CCPD\"\n  }, \"https://github.com/detectRecog/CCPD\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"dataset: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view\"\n  }, \"https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view\"))), mdx(\"h1\", {\n    \"id\": \"face-anti-spoofing\"\n  }, \"Face Anti-Spoofing\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2007.12342\"\n  }, \"https://arxiv.org/abs/2007.12342\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Davidzhangyuanhan/CelebA-Spoof\"\n  }, \"https://github.com/Davidzhangyuanhan/CelebA-Spoof\"))), mdx(\"h1\", {\n    \"id\": \"tools\"\n  }, \"Tools\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VoTT: Visual Object Tagging Tool 1.5\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Microsoft/VoTT\"\n  }, \"https://github.com/Microsoft/VoTT\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LabelImg: a graphical image annotation tool and label object bounding boxes in images\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo2.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tzutalin/labelImg\"\n  }, \"https://github.com/tzutalin/labelImg\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pychet Labeller\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A python based annotation/labelling toolbox for images.\\nThe program allows the user to annotate individual objects in images.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/sbargoti/pychetlabeller\"\n  }, \"https://github.com/sbargoti/pychetlabeller\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ml-pyxis: Tool for reading and writing datasets of tensors (numpy.ndarray) with MessagePack and Lightning Memory-Mapped Database (LMDB).\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tool for reading and writing datasets of tensors in a Lightning Memory-Mapped Database (LMDB).\\nDesigned to manage machine learning datasets with fast reading speeds.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/vicolab/ml-pyxis\"\n  }, \"https://github.com/vicolab/ml-pyxis\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Open Image Dataset downloader\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/e-lab/crawl-dataset\"\n  }, \"https://github.com/e-lab/crawl-dataset\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BBox-Label-Tool\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A simple tool for labeling object bounding boxes in images\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/puzzledqs/BBox-Label-Tool\"\n  }, \"https://github.com/puzzledqs/BBox-Label-Tool\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Labeler for Video\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A GUI tool for conveniently label the objects in video, using the powerful object tracking.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//hahnyuan/video_labeler\"\n  }, \"https://github.com//hahnyuan/video_labeler\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Computer Vision Annotation Tool (CVAT)\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/opencv/cvat/master/cvat/apps/documentation/static/documentation/images/gif003.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Computer Vision Annotation Tool (CVAT) is a web-based tool which helps to annotate video and images for Computer Vision algorithms\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/opencv/cvat\"\n  }, \"https://github.com/opencv/cvat\"))), mdx(\"h1\", {\n    \"id\": \"artist\"\n  }, \"Artist\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BAM! The Behance Artistic Media Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 2.5M artwork urls, 393K attribute labels, 74K short image descriptions/captions\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://bam-dataset.org/\"\n  }, \"https://bam-dataset.org/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.08614\"\n  }, \"https://arxiv.org/abs/1704.08614\"))), mdx(\"h1\", {\n    \"id\": \"resources\"\n  }, \"Resources\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CV Datasets on the web\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.cvpapers.com/datasets.html\"\n  }, \"http://www.cvpapers.com/datasets.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Awesome Public Datasets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: An awesome list of high-quality open datasets in public domains (on-going). By everyone, for everyone!\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/caesar0301/awesome-public-datasets\"\n  }, \"https://github.com/caesar0301/awesome-public-datasets\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Machine Learning Repository\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://archive.ics.uci.edu/ml/datasets.html\"\n  }, \"https://archive.ics.uci.edu/ml/datasets.html\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\nlayout: post\ncategory: computer_vision\ntitle: Computer Vision Datasets\ndate: 2015-09-24\n---\n\n**Datasets who is the best at X ?**\n\n- blog: [http://rodrigob.github.io/are_we_there_yet/build/#datasets](http://rodrigob.github.io/are_we_there_yet/build/#datasets)\n\n**Computer Vision Datasets**\n\n- website: [http://clickdamage.com/sourcecode/index.html](http://clickdamage.com/sourcecode/index.html)\n- code: [http://clickdamage.com/sourcecode/cv_datasets.php](http://clickdamage.com/sourcecode/cv_datasets.php)\n- mirror: [http://pan.baidu.com/s/1pJmqD4n](http://pan.baidu.com/s/1pJmqD4n)\n\n**Introducing the Open Images Dataset**\n\n- blog: [https://research.googleblog.com/2016/09/introducing-open-images-dataset.html](https://research.googleblog.com/2016/09/introducing-open-images-dataset.html)\n- github: [https://github.com/openimages/dataset](https://github.com/openimages/dataset)\n- Academic Torrents: [http://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b](http://academictorrents.com/details/9e9194e21ce045deee8d811481b4cd676b20b06b)\n\n**A parallel download util for Google's open image dataset**\n\n- github: [https://github.com/ejlb/google-open-image-download](https://github.com/ejlb/google-open-image-download)\n\n**Image & Vision Group - Datasets**\n\n- intro: Image & Vision , Clothing & Fashion, Computer Graphics, Video Sequences\n- homepage: [http://caiivg.weebly.com/dataset.html](http://caiivg.weebly.com/dataset.html)\n\n**Huizhong Chen - Datasets**\n\n- intro: Google I/O Dataset, Names 100 Dataset, Clothing Attributes Dataset,\nStanford Mobile Visual Search Dataset, CNN 2-Hours Videos Dataset\n- homepage: [http://huizhongchen.github.io/datasets.html#clothingattributedataset](http://huizhongchen.github.io/datasets.html#clothingattributedataset)\n\n# Classification / Recognition\n\n**A Large-Scale Car Dataset for Fine-Grained Categorization and Verification**\n\n![](http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/illustration.png)\n\n- project page: [http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html](http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html)\n- arxiv: [http://arxiv.org/abs/1506.08959](http://arxiv.org/abs/1506.08959)\n\n**CIFAR-10 / CIFAR100**\n\n- intro: The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. \nThere are 50000 training images and 10000 test images.\n- homepage: [http://www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)\n\n**Tencent ML-Images**\n\n- intro: Largest multi-label image database; ResNet-101 model; 80.73% top-1 acc on ImageNet\n- github: [https://github.com/Tencent/tencent-ml-images](https://github.com/Tencent/tencent-ml-images)\n\n## Face\n\n**The MegaFace Benchmark: 1 Million Faces for Recognition at Scale**\n\n- homepage: [http://megaface.cs.washington.edu/](http://megaface.cs.washington.edu/)\n- arxiv: [http://arxiv.org/abs/1512.00596](http://arxiv.org/abs/1512.00596)\n\n**MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.08221](http://arxiv.org/abs/1607.08221)\n\n**MSR Image Recognition Challenge (IRC)**\n\n- homepage: [https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/](https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/)\n\n**UMDFaces: An Annotated Face Dataset for Training Deep Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.01484](https://arxiv.org/abs/1611.01484)\n\n## Vehicle\n\n**The Comprehensive Cars (CompCars) dataset**\n\n![](http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/illustration.png)\n\n[http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/](http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/)\n\n**BoxCars: Improving Fine-Grained Recognition of Vehicles Using 3-D Bounding Boxes in Traffic Surveillance [IEEE T-ITS]**\n\n![](https://medusa.fit.vutbr.cz/traffic/wp-content/uploads/2017/03/boxcars_bb_estimation_pipeline.png)\n\n[https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/](https://medusa.fit.vutbr.cz/traffic/research-topics/fine-grained-vehicle-recognition/boxcars-improving-vehicle-fine-grained-recognition-using-3d-bounding-boxes-in-traffic-surveillance/)\n\n**Vehicle Make and Model Recognition Dataset (VMMRdb)**\n\n- intro: containing 9,170 classes consisting of 291,752 images, covering models manufactured between 1950 to 2016\n- homepage: [http://vmmrdb.cecsresearch.org/](http://vmmrdb.cecsresearch.org/)\n\n**Cars Dataset**\n\n![](http://ai.stanford.edu/~jkrause/cars/class_montage_flop.jpg)\n\n- intro: contains 16,185 images of 196 classes of cars.\n- homepage: [http://ai.stanford.edu/~jkrause/cars/car_dataset.html](http://ai.stanford.edu/~jkrause/cars/car_dataset.html)\n\n## Scene Recognition\n\n**Places: An Image Database for Deep Scene Understanding**\n\n- project page: [http://places.csail.mit.edu/index.html](http://places.csail.mit.edu/index.html)\n- arxiv: [https://arxiv.org/abs/1610.02055](https://arxiv.org/abs/1610.02055)\n\n**Places2**\n\n- intro: Places2 contains more than 10 million images comprising 400+ unique scene categories\n- homepage: [http://places2.csail.mit.edu/](http://places2.csail.mit.edu/)\n\n**The Places365-CNNs for Scene Classification**\n\n- github: [https://github.com/CSAILVision/places365](https://github.com/CSAILVision/places365)\n\n## MNIST\n\n**EMNIST: an extension of MNIST to handwritten letters**\n\n- arxiv: [https://arxiv.org/abs/1702.05373](https://arxiv.org/abs/1702.05373)\n\n**Fashion-MNIST**\n\n- arxiv: [https://arxiv.org/abs/1708.07747](https://arxiv.org/abs/1708.07747)\n- github: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)\n- benchmark: [http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/)\n\n# Food\n\n**3 Million Instacart Orders, Open Sourced**\n\n[https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)\n\n# Detection\n\n**YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video**\n\n- intro: YouTube-BoundingBoxes (YT-BB)\n- homepage: [https://research.google.com/youtubebb/](https://research.google.com/youtubebb/)\n- arxiv: [https://arxiv.org/abs/1702.00824](https://arxiv.org/abs/1702.00824)\n\n**DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects**\n\n[https://arxiv.org/abs/1804.00525](https://arxiv.org/abs/1804.00525)\n\n**Exclusively Dark (ExDark) Image Dataset**\n\n- intro: Exclusively Dark (ExDARK) dataset which to the best of our knowledge, \nis the largest collection of low-light images taken in very low-light environments to twilight (i.e 10 different conditions) \nto-date with image class and object level annotations.\n- github: [https://github.com/cs-chan/Exclusively-Dark-Image-Dataset](https://github.com/cs-chan/Exclusively-Dark-Image-Dataset)\n\n## Face Detection\n\n**FDDB: Face Detection Data Set and Benchmark**\n\n- homepage: [http://vis-www.cs.umass.edu/fddb/index.html](http://vis-www.cs.umass.edu/fddb/index.html)\n- results: [http://vis-www.cs.umass.edu/fddb/results.html](http://vis-www.cs.umass.edu/fddb/results.html)\n\n**WIDER FACE: A Face Detection Benchmark**\n\n![](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/intro.jpg)\n\n- homepage: [http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)\n- arxiv: [http://arxiv.org/abs/1511.06523](http://arxiv.org/abs/1511.06523)\n\n## Pedestrian Detection\n\n![](https://sshao0516.github.io/CrowdHuman/images/fig1.png)\n\n**Caltech Pedestrian Detection Benchmark**\n\n- homepage: [http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)\n\n**Caltech Pedestrian Dataset Converter**\n\n[https://github.com/mitmul/caltech-pedestrian-dataset-converter](https://github.com/mitmul/caltech-pedestrian-dataset-converter)\n\n**CityPersons: A Diverse Dataset for Pedestrian Detection**\n\n- arxiv: [https://arxiv.org/abs/1702.05693](https://arxiv.org/abs/1702.05693)\n- bitbucket: [https://bitbucket.org/shanshanzhang/citypersons](https://bitbucket.org/shanshanzhang/citypersons)\n- supplemental: [http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf)\n\n**CrowdHuman: A Benchmark for Detecting Human in a Crowd**\n\n- intro: CrowdHuman contains 15000, 4370 and 5000 images for training, validation, and testing, respectively.\na total of 470K human instances from train and validation subsets and 23 persons per image, with various kinds of occlusions in the dataset\n- homepage: [https://sshao0516.github.io/CrowdHuman/](https://sshao0516.github.io/CrowdHuman/)\n\n**EuroCity Persons Dataset**\n\n- intro: collected on-board a moving vehicle in 31 cities of 12 European countries, \nover 238200 person instances manually labeled in over 47300 images, \ncontains a large number of person orientation annotations (over 211200)\n- homepage: [https://eurocity-dataset.tudelft.nl/](https://eurocity-dataset.tudelft.nl/)\n- arxiv: [https://arxiv.org/abs/1805.07193](https://arxiv.org/abs/1805.07193)\n\n**WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild**\n\n![](http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/files/intro.jpg)\n\n- project page: [http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/](http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/)\n\n# Full-Body Annotations\n\n**COCO-WholeBody**\n\n[https://github.com/jin-s13/COCO-WholeBody](https://github.com/jin-s13/COCO-WholeBody)\n\n**Halpe Full-Body Human Keypoints and HOI-Det dataset**\n\n- intro: Halpe: full body human pose estimation and human-object interaction detection dataset\n- github:[https://github.com/Fang-Haoshu/Halpe-FullBody](https://github.com/Fang-Haoshu/Halpe-FullBody)\n\n## Vehicle Detection\n\n**Toyota Motor Europe (TME) Motorway Dataset**\n\n- intro: composed by 28 clips for a total of approximately 27 minutes (30000+ frames) with vehicle annotation\n- homepage: [http://cmp.felk.cvut.cz/data/motorway/](http://cmp.felk.cvut.cz/data/motorway/)\n\n**Welcome to BIT-Vehicle Dataset**\n\n![](http://iitlab.bit.edu.cn/mcislab/vehicledb/dataset.jpg)\n\n- intro: 9,850 vehicle images, sizes of 1600*1200 and 1920*1080 captured from two cameras at different time and places in the dataset\n- homepage: [http://iitlab.bit.edu.cn/mcislab/vehicledb/](http://iitlab.bit.edu.cn/mcislab/vehicledb/)\n\n# Vehicle Re-ID\n\n**A Large-Scale Dataset for Vehicle Re-Identification in the Wild**\n\n- github: [https://github.com/PKU-IMRE/VERI-Wild](https://github.com/PKU-IMRE/VERI-Wild)\n\n## Logo Detection\n\n**QMUL-OpenLogo: Open Logo Detection Challenge**\n\n- intro: QMUL-OpenLogo contains 27,083 images from 352 logo classes, \nbuilt by aggregating and refining 7 existing datasets and establishing an open logo detection evaluation protocol\n- homepage: [https://qmul-openlogo.github.io/](https://qmul-openlogo.github.io/)\n\n# Head Detection\n\n**SCUT-HEAD**\n\n- intro: SCUT HEAD is a large-scale head detection dataset, including 4405 images labeld with 111251 heads.\n- github: [https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release](https://github.com/HCIILAB/SCUT-HEAD-Dataset-Release)\n\n**HollywoodHeads dataset**\n\n[http://www.di.ens.fr/willow/research/headdetection/](http://www.di.ens.fr/willow/research/headdetection/)\n\n**Brainwash dataset.**\n\n[https://exhibits.stanford.edu/data/catalog/sx925dc9385](https://exhibits.stanford.edu/data/catalog/sx925dc9385)\n\n## Detection From Video\n\n**YouTube-Objects dataset v2.2**\n\n- homepage: [http://calvin.inf.ed.ac.uk/datasets/youtube-objects-dataset/](http://calvin.inf.ed.ac.uk/datasets/youtube-objects-dataset/)\n\n**ILSVRC2015: Object detection from video (VID)**\n\n- homepage: [http://vision.cs.unc.edu/ilsvrc2015/download-videos-3j16.php#vid](http://vision.cs.unc.edu/ilsvrc2015/download-videos-3j16.php#vid)\n\n# Segmentation\n\n## Mapillary Vistas Dataset\n\n**Mapillary Vistas Dataset**\n\n- intro: 25,000 high-resolution images, 100 object categories, 60 of those instance-specific\n[https://www.mapillary.com/dataset/](https://www.mapillary.com/dataset/)\n\n**Releasing the World’s Largest Street-level Imagery Dataset for Teaching Machines to See**\n\n[http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html](http://blog.mapillary.com/product/2017/05/03/mapillary-vistas-dataset.html)\n\n**Multi-Human Parsing**\n\n![](https://lv-mhp.github.io/static/images/3.png)\n\n[https://lv-mhp.github.io/](https://lv-mhp.github.io/)\n\n# PASCAL VOC\n\n## Augmented Pascal VOC\n\n[http://home.bharathh.info/pubs/codes/SBD/download.html](http://home.bharathh.info/pubs/codes/SBD/download.html)\n\n# Supervisely Person\n\n- homepage: [https://supervise.ly/](https://supervise.ly/)\n- blog: [https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469](https://hackernoon.com/releasing-supervisely-person-dataset-for-teaching-machines-to-segment-humans-1f1fc1f28469)\n\n# Microsoft COCO\n\n- homepage: [http://mscoco.org/](http://mscoco.org/)\n- github: [https://github.com/pdollar/coco](https://github.com/pdollar/coco)\n\n# The Oxford-IIIT Pet Dataset\n\n- intro: a 37 category pet dataset with roughly 200 images for each class.\nAll images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation\n- homepage: [http://www.robots.ox.ac.uk/~vgg/data/pets/](http://www.robots.ox.ac.uk/~vgg/data/pets/)\n\n## COCO-Stuff\n\n**COCO-Stuff: Thing and Stuff Classes in Context**\n\n**COCO-Stuff 10K dataset v1.1**\n\n[https://arxiv.org/abs/1612.03716](https://arxiv.org/abs/1612.03716)\n[https://github.com/nightrome/cocostuff](https://github.com/nightrome/cocostuff)\n\n# Scene Parsing\n\n**MIT Scene Parsing Benchmark**\n\n[http://sceneparsing.csail.mit.edu/](http://sceneparsing.csail.mit.edu/)\n\n**ADE20K**\n\n- intro: train: 20,120 images, val: 2000 images. contains 150 stuff/object category labels (e.g., wall, sky, and tree) and 1,038 imagelevel scene descriptors (e.g., airport terminal, bedroom, and street).\n- homepage: [http://groups.csail.mit.edu/vision/datasets/ADE20K/](http://groups.csail.mit.edu/vision/datasets/ADE20K/)\n\n**Semantic Understanding of Scenes through the ADE20K Dataset**\n\n[https://arxiv.org/abs/1608.05442](https://arxiv.org/abs/1608.05442)\n\n# ImageNet\n\n- synsets: [http://image-net.org/challenges/LSVRC/2014/browse-det-synsets](http://image-net.org/challenges/LSVRC/2014/browse-det-synsets)\n\n**ImageNet-Utils**\n\n- intro: Utils to help download images by id, crop bounding box, label images, etc.\n- github: [https://github.com/tzutalin/ImageNet_Utils](https://github.com/tzutalin/ImageNet_Utils)\n\n# Captioning / Description\n\n**TGIF: A New Dataset and Benchmark on Animated GIF Description**\n\n- arxiv: [http://arxiv.org/abs/1604.02748](http://arxiv.org/abs/1604.02748)\n- github: [https://github.com/raingo/TGIF-Release](https://github.com/raingo/TGIF-Release)\n\n**Collecting Multilingual Parallel Video Descriptions Using Mechanical Turk**\n\n- intro: 1970 YouTube video snippets: 1200 training, 100 validation, 670 test\n- homepage: [http://www.cs.utexas.edu/users/ml/clamp/videoDescription/](http://www.cs.utexas.edu/users/ml/clamp/videoDescription/)\n\n# Video\n\n| Dataset          | # Videos     | # Classes   | Year     | Manually Labeled ? |\n|:----------------:|:----------- :|:-----------:|:--------:|:------------------:|\n| Kodak            |   1,358      |    25       |  2007    | ✓                 |\n| HMDB51           |   7000       |    51       |          |                    |\n| Charades         |   9848       |    157      |          |                    |\n| MCG-WEBV         |   234,414    |    15       |  2009    | ✓                 |\n| CCV              |   9,317      |    20       |  2011    | ✓                 |\n| UCF-101          |   13,320     |    101      |  2012    | ✓                 |\n| THUMOS-2         |   18,394     |    101      |  2014    | ✓                 |\n| MED-2014         |   ≈28,000    |    20       |  2014    | ✓                 |\n| Sports-1M        |   1M         |    487      |  2014    | ✗                 |\n| ActivityNet      |   27,801     |    203      |  2015    | ✓                 |\n| FCVID            |   91,223     |    239      |  2015    | ✓                 |\n\n**UCF101 - Action Recognition Data Set**\n\n![](http://crcv.ucf.edu/data/UCF101/UCF101.jpg)\n\n- homepage: [http://crcv.ucf.edu/data/UCF101.php](http://crcv.ucf.edu/data/UCF101.php)\n\n**HMDB51: A Large Video Database for Human Motion Recognition**\n\n- homepage: [http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)\n\n**ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding**\n\n- homepage: [http://activity-net.org/](http://activity-net.org/)\n- download: [http://activity-net.org/download.html](http://activity-net.org/download.html)\n- github: [https://github.com/activitynet](https://github.com/activitynet)\n\n**Sports-1M**\n\n- homepage: [https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md](https://github.com/gtoderici/sports-1m-dataset/blob/wiki/ProjectHome.md)\n- github: [https://github.com/gtoderici/sports-1m-dataset/](https://github.com/gtoderici/sports-1m-dataset/)\n- thumbnails: [http://cs.stanford.edu/people/karpathy/deepvideo/classes.html](http://cs.stanford.edu/people/karpathy/deepvideo/classes.html)\n\n**Charades Dataset**\n\n- intro: This dataset guides our research into unstructured video activity recogntion and commonsense reasoning for daily human activities.\n- intro: The dataset contains 66,500 temporal annotations for 157 action classes, \n41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos.\n- homepage: [http://allenai.org/plato/charades/](http://allenai.org/plato/charades/)\n\n**FCVID: Fudan-Columbia Video Dataset**\n\n- homepage: [http://bigvid.fudan.edu.cn/FCVID/](http://bigvid.fudan.edu.cn/FCVID/)\n\n**YouTube-8M: A Large-Scale Video Classification Benchmark**\n\n- homepage: [http://research.google.com/youtube8m/](http://research.google.com/youtube8m/)\n- arxiv: [http://arxiv.org/abs/1609.08675](http://arxiv.org/abs/1609.08675)\n\n**stabilized video frames**\n\n- intro: 9 TB, 35,000,000 clips, 32 frames\n- intro: Generating Videos with Scene Dynamics\n- homepage: [http://web.mit.edu/vondrick/tinyvideo/#data](http://web.mit.edu/vondrick/tinyvideo/#data)\n\n**The Kinetics Human Action Video Dataset**\n\n- intro: Google\n- homepage: [https://deepmind.com/research/open-source/open-source-datasets/kinetics/](https://deepmind.com/research/open-source/open-source-datasets/kinetics/)\n- arxiv: [https://arxiv.org/abs/1705.06950](https://arxiv.org/abs/1705.06950)\n\n**e-Lab Video Data Set(s)**\n\n- intro: \"Currently, e-VDS35 has 35 classes and a total of 2050 videos of roughly 10 seconds each (see histogram below). We are aiming to collect overall 1750 (50 × 35) videos with your help.\"\n- homepage: [https://engineering.purdue.edu/elab/eVDS](https://engineering.purdue.edu/elab/eVDS)\n\n**Video Dataset Overview**\n\n- intro: Sortable and searchable compilation of video dataset\n- arxiv: [https://www.di.ens.fr/~miech/datasetviz/](https://www.di.ens.fr/~miech/datasetviz/)\n\n# Scene\n\n**SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth**\n\n- intro: Imperial College London\n- project page: [https://robotvault.bitbucket.org/scenenet-rgbd.html](https://robotvault.bitbucket.org/scenenet-rgbd.html)\n- github: [https://arxiv.org/abs/1612.05079](https://arxiv.org/abs/1612.05079)\n- github: [https://github.com/jmccormac/pySceneNetRGBD](https://github.com/jmccormac/pySceneNetRGBD)\n\n# Autonomous Driving\n\n**BDD: Berkely Deep Drive**\n\n- intro: 100,000 HD video sequences of over 1,100-hour driving experience across many different times in the day, \nweather conditions, and driving scenarios\n- homepage: [http://bdd-data.berkeley.edu/](http://bdd-data.berkeley.edu/)\n- github: [https://github.com/ucbdrive/bdd-data](https://github.com/ucbdrive/bdd-data)\n\n# OCR\n\n**COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images**\n\n- homepage: [http://vision.cornell.edu/se3/coco-text/](http://vision.cornell.edu/se3/coco-text/)\n- arxiv: [http://arxiv.org/abs/1601.07140](http://arxiv.org/abs/1601.07140)\n\n**Chinese Text in the Wild**\n\n- intro: 32,285 high resolution images, 1,018,402 character instances, 3,850 character categories, 6 kinds of attributes\n- homepage: [https://ctwdataset.github.io/](https://ctwdataset.github.io/)\n- arxiv: [https://arxiv.org/abs/1803.00085](https://arxiv.org/abs/1803.00085)\n\n**ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views**\n\n- arxiv: [https://arxiv.org/abs/1903.10412](https://arxiv.org/abs/1903.10412)\n- github: [https://github.com/chongshengzhang/shopsign](https://github.com/chongshengzhang/shopsign)\n\n# Retrieval\n\nOxford5k\n\nParis6k\n\nOxford105k\n\nUKB\n\n**NUS-WIDE**\n\n**ImageNet-YahooQA**\n\n**University-1652**: \n\n![](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/Data.jpg)\n[[Paper]](https://arxiv.org/abs/2002.12186) \n[[Explore Drone-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_drone.jpg?raw=true)\n[[Explore Satellite-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_satellite.jpg?raw=true)\n[[Explore Street-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_street.jpg?raw=true)\n[[Video Sample]](https://www.youtube.com/embed/dzxXPp8tVn4?vq=hd1080)\n[[中文介绍]](https://zhuanlan.zhihu.com/p/110987552)\n- Dataset and Baseline Code: https://github.com/layumi/University1652-Baseline\n\n**DeepFashion: In-shop Clothes Retrieval**\n\n- intro: 7,982 number of clothing items; 52,712 number of in-shop clothes images, and ~200,000 cross-pose/scale pairs; Each image is annotated by bounding box, clothing type and pose type.\n- homepage: [http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html)\n\n# Person Re-ID\n\n| Dataset            | Description                                                              |\n| :----------------: | :---------------------------------------------------------------------:  |\n| CUHK01             | 971 identities, 3884 images, manually cropped                            |\n| CUHK02             | 1816 identities, 7264 images, manually cropped                           |\n| CUHK03             | 1360 identities, 13164 images, manually cropped + automatically detected |\n\n**Person Re-identification Datasets**\n\n- homepage: [http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html](http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html)\n- github: [https://github.com/RSL-NEU/person-reid-benchmark](https://github.com/RSL-NEU/person-reid-benchmark)\n\n**CUHK Person Re-identification Datasets**\n\n[http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html](http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html)\n\n**PRW (Person Re-identification in the Wild) Dataset**\n\n![](http://www.liangzheng.com.cn/Project/pipeline_prw.png)\n\n- homepage: [http://www.liangzheng.com.cn/Project/project_prw.html](http://www.liangzheng.com.cn/Project/project_prw.html)\n- github: [https://github.com/liangzheng06/PRW-baseline](https://github.com/liangzheng06/PRW-baseline)\n\n**Person Re-identification in the Wild**\n\n- intro: CVPR 2017 spotlight\n- arxiv: [https://arxiv.org/abs/1604.02531](https://arxiv.org/abs/1604.02531)\n\n**DukeMTMC-reID**\n\n- intro: DukeMTMC-reID is a subset of the DukeMTMC for image-based re-identification, in the format of the Market-1501 dataset\n- intro: 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images\n- github: [https://github.com/layumi/DukeMTMC-reID_evaluation](https://github.com/layumi/DukeMTMC-reID_evaluation)\n\n**DukeMTMC4ReID**\n\n- intro: DukeMTMC4ReID dataset\n- github: [https://github.com/NEU-Gou/DukeReID](https://github.com/NEU-Gou/DukeReID)\n\n**Person Re-ID (PRID) Dataset 2011**\n\n[https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/)\n\n**MARS (Motion Analysis and Re-identification Set) Dataset**\n\n- intro: an extension of the Market-1501 dataset\n- homepage: [http://www.liangzheng.com.cn/Project/project_mars.html](http://www.liangzheng.com.cn/Project/project_mars.html)\n- github: [https://github.com/liangzheng06/MARS-evaluation](https://github.com/liangzheng06/MARS-evaluation)\n\n**X-MARS Reordering of the MARS Dataset for Image to Video Evaluation**\n\n- intro: This repository provides the X-MARS dataset splits for image to video/tracklet evaluation\n- github: [https://github.com/andreas-eberle/x-mars](https://github.com/andreas-eberle/x-mars)\n\n**MSMT17**\n\n- intro: 15-camera (12 outdoor cameras, 3 indoor cameras), 4,101 Identities, 126,441 BBoxes\n- homepage: [http://www.pkuvmc.com/publications/longhui.html](http://www.pkuvmc.com/publications/longhui.html)\n- soa: [http://www.pkuvmc.com/publications/state_of_the_art.html](http://www.pkuvmc.com/publications/state_of_the_art.html)\n\n**Labeled Pedestrian in the Wild**\n\n- intro: train/test identities: 1,975/756\n- homepage: [http://liuyu.us/dataset/lpw/](http://liuyu.us/dataset/lpw/)\n\n**SenseReID**\n\n[https://drive.google.com/file/d/0B56OfSrVI8hubVJLTzkwV2VaOWM/view](https://drive.google.com/file/d/0B56OfSrVI8hubVJLTzkwV2VaOWM/view)\n\n**3DPeS**\n\n[http://www.openvisor.org/3dpes.asp](http://www.openvisor.org/3dpes.asp)\n\n**iQIYI-VID: A Large Dataset for Multi-modal Person Identification**\n\n[https://arxiv.org/abs/1811.07548](https://arxiv.org/abs/1811.07548)\n\n# Fashion\n\n**Large-scale Fashion (DeepFashion) Database**\n\n- intro: Attribute Prediction, Consumer-to-shop Clothes Retrieval, In-shop Clothes Retrieval, and Landmark Detection\n- homepage: [http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html)\n\n**Apparel classification with Style**\n\n![](http://people.ee.ethz.ch/~lbossard/projects/accv12/img/motivation.jpg)\n\n- intro: 15 clothing classes, 88951 images\n- homepage: [http://people.ee.ethz.ch/~lbossard/projects/accv12/index.html](http://people.ee.ethz.ch/~lbossard/projects/accv12/index.html)\n\n# Attribute Datasets\n\n**Attribute Datasets**\n\n- intro: in total 41,585 pedestrian samples, each of which is annotated with 72 attributes \nas well as viewpoints, occlusions, body parts information\n- homepage: [https://www.ecse.rpi.edu/homepages/cvrl/database/AttributeDataset.htm](https://www.ecse.rpi.edu/homepages/cvrl/database/AttributeDataset.htm)\n\n## Pedestrian Attribute Recognition\n\n**A Richly Annotated Dataset for Pedestrian Attribute Recognition**\n\n- homepage: [http://rap.idealtest.org/](http://rap.idealtest.org/)\n- arxiv: [https://arxiv.org/abs/1603.07054](https://arxiv.org/abs/1603.07054)\n\n**Pedestrian Attribute Recognition At Far Distance**\n\n- intro: PEdesTrian Attribute (PETA)\n- homepage: [http://mmlab.ie.cuhk.edu.hk/projects/PETA.html](http://mmlab.ie.cuhk.edu.hk/projects/PETA.html)\n- paper: [http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf](http://personal.ie.cuhk.edu.hk/~pluo/pdf/mm14.pdf)\n\n**Market-1501_Attribute**\n\n- github: [https://github.com/vana77/Market-1501_Attribute](https://github.com/vana77/Market-1501_Attribute)\n- blog: [https://vana77.github.io](https://vana77.github.io)\n\n**DukeMTMC-attribute**\n\n- github: [https://github.com/vana77/DukeMTMC-attribute](https://github.com/vana77/DukeMTMC-attribute)\n- blog: [https://vana77.github.io](https://vana77.github.io)\n\n**Parse27k**\n\n- intro: Pedestrian Attribute Recognition in Sequences\n- intro: >27,000 annotated pedestrians, 10 attributes\n- homepage: [https://www.vision.rwth-aachen.de/page/parse27k](https://www.vision.rwth-aachen.de/page/parse27k)\n- tools: [https://github.com/psudowe/parse27k_tools](https://github.com/psudowe/parse27k_tools)\n\n# Tracking\n\n**UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking**\n\n- homepage: [http://detrac-db.rit.albany.edu/](http://detrac-db.rit.albany.edu/)\n- arxiv: [https://arxiv.org/abs/1511.04136](https://arxiv.org/abs/1511.04136)\n\n**DukeMTMC: Duke Multi-Target, Multi-Camera Tracking Project**\n\n- intro: DukeMTMC aims to accelerate advances in multi-target multi-camera tracking. It provides a tracking system that works within and across cameras, a new large scale HD video data set recorded by 8 synchronized cameras with more than 7,000 single camera trajectories and over 2,000 unique identities\n- homepage: [http://vision.cs.duke.edu/DukeMTMC/](http://vision.cs.duke.edu/DukeMTMC/)\n\n**The WILDTRACK Seven-Camera HD Dataset**\n\n[https://cvlab.epfl.ch/data/wildtrack](https://cvlab.epfl.ch/data/wildtrack)\n\n**GOT-10k: Generic Object Tracking Benchmark**\n\n- intro: A large, high-diversity, one-shot database for generic object tracking in the wild\n- project page: [http://got-10k.aitestunion.com/](http://got-10k.aitestunion.com/)\n- github: [https://github.com/got-10k/toolkit](https://github.com/got-10k/toolkit)\n\n# Color Classification\n\n**Vehicle Color Recognition on an Urban Road by Feature Context**\n\n[http://mclab.eic.hust.edu.cn/~pchen/project.html](http://mclab.eic.hust.edu.cn/~pchen/project.html)\n\n# License Plate Detection and Recognition\n\n**Application-Oriented License Plate (AVOP) Database**\n\n[http://aolpr.ntust.edu.tw/lab/download.html](http://aolpr.ntust.edu.tw/lab/download.html)\n\n**CCPD: Chinese City Parking Dataset**\n\n- paper: [http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf](http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf)\n- github: [https://github.com/detectRecog/CCPD](https://github.com/detectRecog/CCPD)\n- dataset: [https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view](https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view)\n\n# Face Anti-Spoofing\n\n**CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations**\n\n- intro: ECCV 2020\n- arxiv: [https://arxiv.org/abs/2007.12342](https://arxiv.org/abs/2007.12342)\n- github: [https://github.com/Davidzhangyuanhan/CelebA-Spoof](https://github.com/Davidzhangyuanhan/CelebA-Spoof)\n\n# Tools\n\n**VoTT: Visual Object Tagging Tool 1.5**\n\n- intro: Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos\n- github: [https://github.com/Microsoft/VoTT](https://github.com/Microsoft/VoTT)\n\n**LabelImg: a graphical image annotation tool and label object bounding boxes in images**\n\n![](https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo2.png)\n\n- github: [https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg)\n\n**Pychet Labeller**\n\n- intro: A python based annotation/labelling toolbox for images. \nThe program allows the user to annotate individual objects in images.\n- github: [https://github.com/sbargoti/pychetlabeller](https://github.com/sbargoti/pychetlabeller)\n\n**ml-pyxis: Tool for reading and writing datasets of tensors (numpy.ndarray) with MessagePack and Lightning Memory-Mapped Database (LMDB).**\n\n- intro: Tool for reading and writing datasets of tensors in a Lightning Memory-Mapped Database (LMDB). \nDesigned to manage machine learning datasets with fast reading speeds.\n- github: [https://github.com/vicolab/ml-pyxis](https://github.com/vicolab/ml-pyxis)\n\n**Open Image Dataset downloader**\n\n- github: [https://github.com/e-lab/crawl-dataset](https://github.com/e-lab/crawl-dataset)\n\n**BBox-Label-Tool**\n\n- intro: A simple tool for labeling object bounding boxes in images\n- github: [https://github.com/puzzledqs/BBox-Label-Tool](https://github.com/puzzledqs/BBox-Label-Tool)\n\n**Data Labeler for Video**\n\n- intro: A GUI tool for conveniently label the objects in video, using the powerful object tracking.\n- github: [https://github.com//hahnyuan/video_labeler](https://github.com//hahnyuan/video_labeler)\n\n**Computer Vision Annotation Tool (CVAT)**\n\n![](https://raw.githubusercontent.com/opencv/cvat/master/cvat/apps/documentation/static/documentation/images/gif003.gif)\n\n- intro: Computer Vision Annotation Tool (CVAT) is a web-based tool which helps to annotate video and images for Computer Vision algorithms\n- github: [https://github.com/opencv/cvat](https://github.com/opencv/cvat)\n\n# Artist\n\n**BAM! The Behance Artistic Media Dataset**\n\n- intro: 2.5M artwork urls, 393K attribute labels, 74K short image descriptions/captions\n- project page: [https://bam-dataset.org/](https://bam-dataset.org/)\n- arxiv: [https://arxiv.org/abs/1704.08614](https://arxiv.org/abs/1704.08614)\n\n# Resources\n\n**CV Datasets on the web**\n\n[http://www.cvpapers.com/datasets.html](http://www.cvpapers.com/datasets.html)\n\n**Awesome Public Datasets**\n\n- intro: An awesome list of high-quality open datasets in public domains (on-going). By everyone, for everyone!\n- github: [https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets)\n\n**Machine Learning Repository**\n\n[https://archive.ics.uci.edu/ml/datasets.html](https://archive.ics.uci.edu/ml/datasets.html)\n","excerpt":"Datasets who is the best at X ? blog:  http://rodrigob.github.io/are_we_there_yet/build/#datasets Computer Vision Datasets website:  http:/…","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Content","url":"/Commercial/Value Accounting Initiatives/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Background","url":"/Core Services/Webizen Socio-Economics/The Values Project/","items":[]},{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"HyperMedia Library","url":"/HyperMedia Library/","items":[]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"OVERVIEW","url":"/Implementation V1/Networking Considerations/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations → Principles → The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]},{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between ‘privacy’ and ‘dignity’.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"awesomeLists","url":"","items":[{"title":"Awesome Computer Vision: Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-computer-vision/","items":[]},{"title":"Awesome Natural Language Generation Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awsome-nl-gen/","items":[]},{"title":"Awesome Semantic Web Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-semweb/","items":[]},{"title":"Awesome-General","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-general/","items":[]}]},{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]},{"title":"EXECUTIVE SUMMARY","url":"/old-work-archives/2018 - Web Civics BizPlan/","items":[]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Overview","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Vision","url":"/Webizen 2.0/Webizen Vision/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/Webizen 2.0/Webizen Pro Summary/","title":"Overview","lastUpdatedAt":"2022-12-28T22:16:43.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Core Services/Webizen Socio-Economics/The Values Project/","title":"Background","lastUpdatedAt":"2022-12-28T22:01:50.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018 - Web Civics BizPlan/","title":"EXECUTIVE SUMMARY","lastUpdatedAt":"2022-12-28T22:01:50.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Implementation V1/Networking Considerations/","title":"OVERVIEW","lastUpdatedAt":"2022-12-28T21:49:13.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/HyperMedia Library/","title":"HyperMedia Library","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/","title":"Webizen V1 Project Documentation","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Commercial/Value Accounting Initiatives/","title":"Content","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Core Services/Permissive Commons/","title":"Permissive Commons","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Webizen 2.0/Webizen Vision/","title":"Webizen Vision","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","title":"Knowledge Banking: A Technical Architecture Summary","lastUpdatedAt":"2022-12-28T20:36:06.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}