{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/",
    "result": {"data":{"mdx":{"id":"625a738d-43b0-58b6-8ad3-b3626f22a32e","tableOfContents":{"items":[{"url":"#papers","title":"Papers"},{"url":"#text-detection","title":"Text Detection"},{"url":"#text-recognition","title":"Text Recognition"},{"url":"#text-spotting--text-detection--recognition","title":"Text Spotting & Text Detection + Recognition"},{"url":"#breaking-captcha","title":"Breaking Captcha"},{"url":"#handwritten-recognition","title":"Handwritten Recognition"},{"url":"#plate-recognition","title":"Plate Recognition"},{"url":"#blogs","title":"Blogs"},{"url":"#projects","title":"Projects"},{"url":"#videos","title":"Videos"},{"url":"#resources","title":"Resources"}]},"fields":{"title":"OCR","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr.md","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"OCR","description":null,"imageAlt":null,"tags":[],"date":"2015-10-09T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"deep_learning\",\n  \"title\": \"OCR\",\n  \"date\": \"2015-10-09T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"papers\"\n  }, \"Papers\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google. Ian J. Goodfellow\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1312.6082\"\n  }, \"https://arxiv.org/abs/1312.6082\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Text Recognition with Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.stanford.edu/~acoates/papers/wangwucoatesng_icpr2012.pdf\"\n  }, \"http://www.cs.stanford.edu/~acoates/papers/wangwucoatesng_icpr2012.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"PhD thesis: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/people/dwu4/HonorThesis.pdf\"\n  }, \"http://cs.stanford.edu/people/dwu4/HonorThesis.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Word Spotting and Recognition with Embedded Attributes\")), mdx(\"img\", {\n    \"src\": \"/assets/ocr-materials/Word_Spotting_and_Recognition_with_Embedded_Attributes.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ieeexplore.ieee.org.sci-hub.org/xpl/articleDetails.jsp?arnumber=6857995&filter%3DAND%28p_IS_Number%3A6940341%29\"\n  }, \"http://ieeexplore.ieee.org.sci-hub.org/xpl/articleDetails.jsp?arnumber=6857995&filter%3DAND%28p_IS_Number%3A6940341%29\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reading Text in the Wild with Convolutional Neural Networks\")), mdx(\"img\", {\n    \"src\": \"http://www.robots.ox.ac.uk/~vgg/research/text/pipeline.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.1842\"\n  }, \"http://arxiv.org/abs/1412.1842\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.robots.ox.ac.uk/~vgg/publications/2016/Jaderberg16/\"\n  }, \"http://www.robots.ox.ac.uk/~vgg/publications/2016/Jaderberg16/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://zeus.robots.ox.ac.uk/textsearch/#/search/\"\n  }, \"http://zeus.robots.ox.ac.uk/textsearch/#/search/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.robots.ox.ac.uk/~vgg/research/text/\"\n  }, \"http://www.robots.ox.ac.uk/~vgg/research/text/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep structured output learning for unconstrained text recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"propose an architecture consisting of a character sequence CNN and\\nan N-gram encoding CNN which act on an input image in parallel and whose outputs are utilized\\nalong with a CRF model to recognize the text content present within the image.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.5903\"\n  }, \"http://arxiv.org/abs/1412.5903\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Features for Text Spotting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14/jaderberg14.pdf\"\n  }, \"http://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14/jaderberg14.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"bitbucket: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://bitbucket.org/jaderberg/eccv2014_textspotting\"\n  }, \"https://bitbucket.org/jaderberg/eccv2014_textspotting\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gitxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://gitxiv.com/posts/uB4y7QdD5XquEJ69c/deep-features-for-text-spotting\"\n  }, \"http://gitxiv.com/posts/uB4y7QdD5XquEJ69c/deep-features-for-text-spotting\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reading Scene Text in Deep Convolutional Sequences\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1506.04395\"\n  }, \"http://arxiv.org/abs/1506.04395\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepFont: Identify Your Font from An Image\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1507.03196\"\n  }, \"http://arxiv.org/abs/1507.03196\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Convolutional Recurrent Neural Network (CRNN)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1507.05717\"\n  }, \"http://arxiv.org/abs/1507.05717\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bgshih/crnn\"\n  }, \"https://github.com/bgshih/crnn\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/meijieru/crnn.pytorch\"\n  }, \"https://github.com/meijieru/crnn.pytorch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recursive Recurrent Nets with Attention Modeling for OCR in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.03101\"\n  }, \"http://arxiv.org/abs/1603.03101\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Writer-independent Feature Learning for Offline Signature Verification using Deep Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.00974\"\n  }, \"http://arxiv.org/abs/1604.00974\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepText: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.07314\"\n  }, \"http://arxiv.org/abs/1605.07314\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Interpretation of the French Street Name Signs Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://link.springer.com/chapter/10.1007%2F978-3-319-46604-0_30\"\n  }, \"http://link.springer.com/chapter/10.1007%2F978-3-319-46604-0_30\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tensorflow/models/tree/master/street\"\n  }, \"https://github.com/tensorflow/models/tree/master/street\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Subtitle Detection and Recognition for Videos in East Asian Languages via CNN Ensemble with Near-Human-Level Performance\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06159\"\n  }, \"https://arxiv.org/abs/1611.06159\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Smart Library: Identifying Books in a Library using Richly Supervised Deep Scene Text Reading\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.07385\"\n  }, \"https://arxiv.org/abs/1611.07385\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Text Proposals for Scene Images with Fully Convolutional Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Universitat Autonoma de Barcelona (UAB) & University of Florence\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: International Conference on Pattern Recognition (ICPR) - DLPR (Deep Learning for Pattern Recognition) workshop\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.05089\"\n  }, \"https://arxiv.org/abs/1702.05089\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scene Text Eraser\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.02772\"\n  }, \"https://arxiv.org/abs/1705.02772\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attention-based Extraction of Structured Information from Street View Imagery\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University College London & Google Inc\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.03549\"\n  }, \"https://arxiv.org/abs/1704.03549\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tensorflow/models/tree/master/attention_ocr\"\n  }, \"https://github.com/tensorflow/models/tree/master/attention_ocr\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Implicit Language Model in LSTM for OCR\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.09441\"\n  }, \"https://arxiv.org/abs/1805.09441\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scene Text Magnifier\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICDAR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1907.00693\"\n  }, \"https://arxiv.org/abs/1907.00693\"))), mdx(\"h1\", {\n    \"id\": \"text-detection\"\n  }, \"Text Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Object Proposals for Text Extraction in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICDAR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.02317\"\n  }, \"http://arxiv.org/abs/1509.02317\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/lluisgomez/TextProposals\"\n  }, \"https://github.com/lluisgomez/TextProposals\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Text-Attentional Convolutional Neural Networks for Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.03283\"\n  }, \"http://arxiv.org/abs/1510.03283\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.09423\"\n  }, \"http://arxiv.org/abs/1603.09423\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Synthetic Data for Text Localisation in Natural Images\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/ankush-me/SynthText/master/samples.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.robots.ox.ac.uk/~vgg/data/scenetext/\"\n  }, \"http://www.robots.ox.ac.uk/~vgg/data/scenetext/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.06646\"\n  }, \"http://arxiv.org/abs/1604.06646\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.robots.ox.ac.uk/~vgg/data/scenetext/gupta16.pdf\"\n  }, \"http://www.robots.ox.ac.uk/~vgg/data/scenetext/gupta16.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ankush-me/SynthText\"\n  }, \"https://github.com/ankush-me/SynthText\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scene Text Detection via Holistic, Multi-Channel Prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.09002\"\n  }, \"http://arxiv.org/abs/1606.09002\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Detecting Text in Natural Image with Connectionist Text Proposal Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.03605\"\n  }, \"http://arxiv.org/abs/1609.03605\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tianzhi0549/CTPN\"\n  }, \"https://github.com/tianzhi0549/CTPN\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(CUDA8.0 support): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/qingswu/CTPN\"\n  }, \"https://github.com/qingswu/CTPN\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://textdet.com/\"\n  }, \"http://textdet.com/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/eragonruan/text-detection-ctpn\"\n  }, \"https://github.com/eragonruan/text-detection-ctpn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TextBoxes: A Fast Text Detector with a Single Deep Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06779\"\n  }, \"https://arxiv.org/abs/1611.06779\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MhLiao/TextBoxes\"\n  }, \"https://github.com/MhLiao/TextBoxes\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/xiaodiu2010/TextBoxes-TensorFlow\"\n  }, \"https://github.com/xiaodiu2010/TextBoxes-TensorFlow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TextBoxes++: A Single-Shot Oriented Scene Text Detector\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: TIP 2018. University of Science and Technology(HUST)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.02765\"\n  }, \"https://arxiv.org/abs/1801.02765\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MhLiao/TextBoxes_plusplus\"\n  }, \"https://github.com/MhLiao/TextBoxes_plusplus\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Arbitrary-Oriented Scene Text Detection via Rotation Proposals\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IEEE Transactions on Multimedia\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: RRPN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.01086\"\n  }, \"https://arxiv.org/abs/1703.01086\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mjq11302010044/RRPN\"\n  }, \"https://github.com/mjq11302010044/RRPN\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/DetectionTeamUCAS/RRPN_Faster-RCNN_Tensorflow\"\n  }, \"https://github.com/DetectionTeamUCAS/RRPN_Faster-RCNN_Tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: F-measure 70.64%, outperforming the existing state-of-the-art method with F-measure 63.76%\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.01425\"\n  }, \"https://arxiv.org/abs/1703.01425\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Detecting Oriented Text in Natural Images by Linking Segments\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.06520\"\n  }, \"https://arxiv.org/abs/1703.06520\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/dengdan/seglink\"\n  }, \"https://github.com/dengdan/seglink\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Direct Regression for Multi-Oriented Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.08289\"\n  }, \"https://arxiv.org/abs/1703.08289\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cascaded Segmentation-Detection Networks for Word-Level Text Spotting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1704.00834\"\n  }, \"https://arxiv.org/abs/1704.00834\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Text-Detection-using-py-faster-rcnn-framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jugg1024/Text-Detection-with-FRCN\"\n  }, \"https://github.com/jugg1024/Text-Detection-with-FRCN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"WordFence: Text Detection in Natural Images with Border Awareness\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICIP 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arcxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.05483\"\n  }, \"https://arxiv.org/abs/1705.05483\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SSD-text detection: Text Detector\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A modified SSD model for text detection\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/oyxhust/ssd-text_detection\"\n  }, \"https://github.com/oyxhust/ssd-text_detection\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Samsung R&D Institute China\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.09579\"\n  }, \"https://arxiv.org/abs/1706.09579\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"R-PHOC: Segmentation-Free Word Spotting using CNN\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICDAR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.01294\"\n  }, \"https://arxiv.org/abs/1707.01294\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.03985\"\n  }, \"https://arxiv.org/abs/1707.03985\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EAST: An Efficient and Accurate Scene Text Detector\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. Megvii\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.03155\"\n  }, \"https://arxiv.org/abs/1704.03155\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf\"\n  }, \"http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/argman/EAST\"\n  }, \"https://github.com/argman/EAST\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Scene Text Detection with Connected Component Proposals\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Amap Vision Lab, Alibaba Group\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.05133\"\n  }, \"https://arxiv.org/abs/1708.05133\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single Shot Text Detector with Regional Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.00138\"\n  }, \"https://arxiv.org/abs/1709.00138\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/BestSonny/SSTD\"\n  }, \"https://github.com/BestSonny/SSTD\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://sstd.whuang.org\"\n  }, \"http://sstd.whuang.org\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fused Text Segmentation Networks for Multi-oriented Scene Text Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.03272\"\n  }, \"https://arxiv.org/abs/1709.03272\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Residual Text Detection Network for Scene Text\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IAPR International Conference on Document Analysis and Recognition (ICDAR) 2017. Samsung R&D Institute of China, Beijing\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04147\"\n  }, \"https://arxiv.org/abs/1711.04147\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Feature Enhancement Network: A Refined Scene Text Detector\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04249\"\n  }, \"https://arxiv.org/abs/1711.04249\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ArbiText: Arbitrary-Oriented Text Detection in Unconstrained Scene\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.11249\"\n  }, \"https://arxiv.org/abs/1711.11249\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Detecting Curve Text in the Wild: New Dataset and New Solution\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.02170\"\n  }, \"https://arxiv.org/abs/1712.02170\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Yuliang-Liu/Curve-Text-Detector\"\n  }, \"https://github.com/Yuliang-Liu/Curve-Text-Detector\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FOTS: Fast Oriented Text Spotting with a Unified Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.01671\"\n  }, \"https://arxiv.org/abs/1801.01671\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PixelLink: Detecting Scene Text via Instance Segmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.01315\"\n  }, \"https://arxiv.org/abs/1801.01315\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PixelLink: Detecting Scene Text via Instance Segmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018. Zhejiang University & Chinese Academy of Sciences\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.01315\"\n  }, \"https://arxiv.org/abs/1801.01315\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sliding Line Point Regression for Shape Robust Scene Text Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.09969\"\n  }, \"https://arxiv.org/abs/1801.09969\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.08948\"\n  }, \"https://arxiv.org/abs/1802.08948\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single Shot TextSpotter with Explicit Alignment and Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.03474\"\n  }, \"https://arxiv.org/abs/1803.03474\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rotation-Sensitive Regression for Oriented Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.05265\"\n  }, \"https://arxiv.org/abs/1803.05265\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Detecting Multi-Oriented Text with Corner-based Region Proposals\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.02690\"\n  }, \"https://arxiv.org/abs/1804.02690\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/xhzdeng/crpn\"\n  }, \"https://github.com/xhzdeng/crpn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Anchor-Free Region Proposal Network for Faster R-CNN based Text Detection Approaches\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.09003\"\n  }, \"https://arxiv.org/abs/1804.09003\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2018. Alibaba Group\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.01167\"\n  }, \"https://arxiv.org/abs/1805.01167\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Boosting up Scene Text Detectors with Guided CNN\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.04132\"\n  }, \"https://arxiv.org/abs/1805.04132\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Shape Robust Text Detection with Progressive Scale Expansion Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.02559\"\n  }, \"https://arxiv.org/abs/1806.02559\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/whai362/PSENet\"\n  }, \"https://github.com/whai362/PSENet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Single Shot Text Detector with Scale-adaptive Anchors\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1807.01884\"\n  }, \"https://arxiv.org/abs/1807.01884\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.01544\"\n  }, \"https://arxiv.org/abs/1807.01544\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018. Huazhong University of Science and Technology & Megvii (Face++) Technology\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.02242\"\n  }, \"https://arxiv.org/abs/1807.02242\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MhLiao/MaskTextSpotter\"\n  }, \"https://github.com/MhLiao/MaskTextSpotter\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.03547\"\n  }, \"https://arxiv.org/abs/1807.03547\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.03050\"\n  }, \"https://arxiv.org/abs/1809.03050\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Correlation Propagation Networks for Scene Text Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.00304\"\n  }, \"https://arxiv.org/abs/1810.00304\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scene Text Detection with Supervised Pyramid Context Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.08605\"\n  }, \"https://arxiv.org/abs/1811.08605\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Rotated Text Detection with Rotation Region Proposal Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.07031\"\n  }, \"https://arxiv.org/abs/1811.07031\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pixel-Anchor: A Fast Oriented Scene Text Detector with Combined Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.07432\"\n  }, \"https://arxiv.org/abs/1811.07432\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Mask R-CNN with Pyramid Attention Network for Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.09058\"\n  }, \"https://arxiv.org/abs/1811.09058\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TextField: Learning A Deep Direction Field for Irregular Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Huazhong University of Science and Technology (HUST) &  Alibaba Group\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.01393\"\n  }, \"https://arxiv.org/abs/1812.01393\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Detecting Text in the Wild with Deep Character Embedding Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Baidu\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1901.00363\"\n  }, \"https://arxiv.org/abs/1901.00363\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MSR: Multi-Scale Shape Regression for Scene Text Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1901.02596\"\n  }, \"https://arxiv.org/abs/1901.02596\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pyramid Mask Text Detector\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: SenseTime & Beihang University & CUHK\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.11800\"\n  }, \"https://arxiv.org/abs/1903.11800\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Shape Robust Text Detection with Progressive Scale Expansion Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.12473\"\n  }, \"https://arxiv.org/abs/1903.12473\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tightness-aware Evaluation Protocol for Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.00813\"\n  }, \"https://arxiv.org/abs/1904.00813\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Yuliang-Liu/TIoU-metric\"\n  }, \"https://github.com/Yuliang-Liu/TIoU-metric\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Character Region Awareness for Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: CRAFT: Character-Region Awareness For Text detection\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.01941\"\n  }, \"https://arxiv.org/abs/1904.01941\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/clovaai/CRAFT-pytorch\"\n  }, \"https://github.com/clovaai/CRAFT-pytorch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards End-to-End Text Spotting in Natural Scenes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: An extension of the work \\\"Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks\\\", Proc. Int. Conf. Comp. Vision 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1906.06013\"\n  }, \"https://arxiv.org/abs/1906.06013\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACM MM 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1908.05498\"\n  }, \"https://arxiv.org/abs/1908.05498\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Geometry Normalization Networks for Accurate Scene Text Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.00794\"\n  }, \"https://arxiv.org/abs/1909.00794\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-time Scene Text Detection with Differentiable Binarization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1911.08947\"\n  }, \"https://arxiv.org/abs/1911.08947\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MhLiao/DB\"\n  }, \"https://github.com/MhLiao/DB\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TextTubes for Detecting Curved Text in the Wild\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1912.08990\"\n  }, \"https://arxiv.org/abs/1912.08990\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2002.06820\"\n  }, \"https://arxiv.org/abs/2002.06820\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2002.10200\"\n  }, \"https://arxiv.org/abs/2002.10200\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DGST : Discriminator Guided Scene Text detector\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2002.12509\"\n  }, \"https://arxiv.org/abs/2002.12509\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MANGO: A Mask Attention Guided One-Stage Scene Text Spotter\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2012.04350\"\n  }, \"https://arxiv.org/abs/2012.04350\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Vision-Language Pre-Training for Boosting Scene Text Detectors\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2022\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2204.13867\"\n  }, \"https://arxiv.org/abs/2204.13867\"))), mdx(\"h1\", {\n    \"id\": \"text-recognition\"\n  }, \"Text Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sequence to sequence learning for unconstrained scene text recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: master thesis\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.06125\"\n  }, \"http://arxiv.org/abs/1607.06125\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Drawing and Recognizing Chinese Characters with Recurrent Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.06539\"\n  }, \"https://arxiv.org/abs/1606.06539\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: correct rates: Dataset-CASIA 97.10% and Dataset-ICDAR 97.15%\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.02616\"\n  }, \"https://arxiv.org/abs/1610.02616\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stroke Sequence-Dependent Deep Convolutional Neural Network for Online Handwritten Chinese Character Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.04057\"\n  }, \"https://arxiv.org/abs/1610.04057\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual attention models for scene text recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.01487\"\n  }, \"https://arxiv.org/abs/1706.01487\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Focusing Attention: Towards Accurate Text Recognition in Natural Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.02054\"\n  }, \"https://arxiv.org/abs/1709.02054\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scene Text Recognition with Sliding Convolutional Character Models\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.01727\"\n  }, \"https://arxiv.org/abs/1709.01727\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AdaDNNs: Adaptive Ensemble of Deep Neural Networks for Scene Text Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1710.03425\"\n  }, \"https://arxiv.org/abs/1710.03425\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A New Hybrid-parameter Recurrent Neural Networks for Online Handwritten Chinese Character Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.02809\"\n  }, \"https://arxiv.org/abs/1711.02809\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AON: Towards Arbitrarily-Oriented Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04226\"\n  }, \"https://arxiv.org/abs/1711.04226\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/huizhang0110/AON\"\n  }, \"https://github.com/huizhang0110/AON\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Arbitrarily-Oriented Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A method used in ICDAR 2017 word recognition competitions\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04226\"\n  }, \"https://arxiv.org/abs/1711.04226\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SEE: Towards Semi-Supervised End-to-End Scene Text Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.05404\"\n  }, \"https://arxiv.org/abs/1712.05404\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Edit Probability for Scene Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Fudan University & Hikvision Research Institute\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.03384\"\n  }, \"https://arxiv.org/abs/1805.03384\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SCAN: Sliding Convolutional Attention Network for Scene Text Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.00578\"\n  }, \"https://arxiv.org/abs/1806.00578\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adaptive Adversarial Attack on Scene Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Florida\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.03326\"\n  }, \"https://arxiv.org/abs/1807.03326\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1812.05824\"\n  }, \"https://arxiv.org/abs/1812.05824\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Multi-Object Rectified Attention Network for Scene Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Pattern Recognition 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: MORAN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1901.03003\"\n  }, \"https://arxiv.org/abs/1901.03003\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SAFE: Scale Aware Feature Encoder for Scene Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1901.05770\"\n  }, \"https://arxiv.org/abs/1901.05770\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Simple and Robust Convolutional-Attention Network for Irregular Text Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1904.01375\"\n  }, \"https://arxiv.org/abs/1904.01375\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FACLSTM: ConvLSTM with Focused Attention for Scene Text Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1904.09405\"\n  }, \"https://arxiv.org/abs/1904.09405\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards Accurate Scene Text Recognition with Semantic Reasoning Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2003.12294\"\n  }, \"https://arxiv.org/abs/2003.12294\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FedOCR: Communication-Efficient Federated Learning for Scene Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Huazhong University of Science and Technology & Meituan-Dianping Group\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2007.11462\"\n  }, \"https://arxiv.org/abs/2007.11462\"))), mdx(\"h1\", {\n    \"id\": \"text-spotting--text-detection--recognition\"\n  }, \"Text Spotting & Text Detection + Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"STN-OCR: A single Neural Network for Text Detection and Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.08831\"\n  }, \"https://arxiv.org/abs/1707.08831\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(MXNet): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Bartzi/stn-ocr\"\n  }, \"https://github.com/Bartzi/stn-ocr\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf\"\n  }, \"http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FOTS: Fast Oriented Text Spotting with a Unified Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.01671\"\n  }, \"https://arxiv.org/abs/1801.01671\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single Shot TextSpotter with Explicit Alignment and Attention\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An end-to-end TextSpotter with Explicit Alignment and Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.03474\"\n  }, \"https://arxiv.org/abs/1803.03474\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tonghe90/textspotter\"\n  }, \"https://github.com/tonghe90/textspotter\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.03021\"\n  }, \"https://arxiv.org/abs/1807.03021\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scenes\"\n  }, \"https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scenes\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scene Text Detection and Recognition: The Deep Learning Era\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.04256\"\n  }, \"https://arxiv.org/abs/1811.04256\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gihtub: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Jyouhou/SceneTextPapers\"\n  }, \"https://github.com/Jyouhou/SceneTextPapers\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Novel Integrated Framework for Learning both Text Detection and Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Alibaba\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.08611\"\n  }, \"https://arxiv.org/abs/1811.08611\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Efficient Video Scene Text Spotting: Unifying Detection, Tracking, and Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Zhejiang University & Hikvision Research Institute\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.03299\"\n  }, \"https://arxiv.org/abs/1903.03299\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Multitask Network for Localization and Recognition of Text in Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICDAR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1906.09266\"\n  }, \"https://arxiv.org/abs/1906.09266\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1907.09653\"\n  }, \"https://arxiv.org/abs/1907.09653\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Character Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1910.07954\"\n  }, \"https://arxiv.org/abs/1910.07954\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MalongTech/research-charnet\"\n  }, \"https://github.com/MalongTech/research-charnet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RoadText-1K: Text Detection & Recognition Dataset for Driving Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICRA 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k\"\n  }, \"http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2005.09496\"\n  }, \"https://arxiv.org/abs/2005.09496\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SPTS: Single-Point Text Spotting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Chinese University of Hong Kong & South China University of Technology & University of Adelaide & ByteDance Inc. & Huawei Technologies & Zhejiang University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2112.07917\"\n  }, \"https://arxiv.org/abs/2112.07917\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AWS AI Labs\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2202.05508\"\n  }, \"https://arxiv.org/abs/2202.05508\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2022\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: South China University of Technology & Chinese University of Hong Kong 3Huawei Cloud AI & IntSig Information Co., Ltd & Peng Cheng Laboratory\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2203.10209\"\n  }, \"https://arxiv.org/abs/2203.10209\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mxin262/SwinTextSpotter\"\n  }, \"https://github.com/mxin262/SwinTextSpotter\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Video Text Spotting with Transformer\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Zhejiang University & Kuaishou Technology & Beijing Institute of Technology & Beijing University of Posts and Telecommunications & The University of Hong Kong\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2203.10539\"\n  }, \"https://arxiv.org/abs/2203.10539\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/weijiawu/TransDETR\"\n  }, \"https://github.com/weijiawu/TransDETR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Text Spotting Transformers\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2022\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2204.01918\"\n  }, \"https://arxiv.org/abs/2204.01918\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2022\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Zhejiang University & Hikvision Research Institute\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2207.06694\"\n  }, \"https://arxiv.org/abs/2207.06694\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hikopensource/DAVAR-Lab-OCR/\"\n  }, \"https://github.com/hikopensource/DAVAR-Lab-OCR/\"))), mdx(\"h1\", {\n    \"id\": \"breaking-captcha\"\n  }, \"Breaking Captcha\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Using deep learning to break a Captcha system\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"Using Torch code to break simplecaptcha with 92% accuracy\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://deepmlblog.wordpress.com/2016/01/03/how-to-break-a-captcha-system/\"\n  }, \"https://deepmlblog.wordpress.com/2016/01/03/how-to-break-a-captcha-system/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/arunpatala/captcha\"\n  }, \"https://github.com/arunpatala/captcha\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Breaking reddit captcha with 96% accuracy\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://deepmlblog.wordpress.com/2016/01/05/breaking-reddit-captcha-with-96-accuracy/\"\n  }, \"https://deepmlblog.wordpress.com/2016/01/05/breaking-reddit-captcha-with-96-accuracy/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/arunpatala/reddit.captcha\"\n  }, \"https://github.com/arunpatala/reddit.captcha\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"I\\u2019m not a human: Breaking the Google reCAPTCHA\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.blackhat.com/docs/asia-16/materials/asia-16-Sivakorn-Im-Not-a-Human-Breaking-the-Google-reCAPTCHA-wp.pdf\"\n  }, \"https://www.blackhat.com/docs/asia-16/materials/asia-16-Sivakorn-Im-Not-a-Human-Breaking-the-Google-reCAPTCHA-wp.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Net CAPTCHA Cracker\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring15/geetika/CS298%20Slides%20-%20PDF\"\n  }, \"http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring15/geetika/CS298%20Slides%20-%20PDF\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bgeetika/Captcha-Decoder\"\n  }, \"https://github.com/bgeetika/Captcha-Decoder\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cp-training.appspot.com/\"\n  }, \"http://cp-training.appspot.com/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent neural networks for decoding CAPTCHAS\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://deepmlblog.wordpress.com/2016/01/12/recurrent-neural-networks-for-decoding-captchas/\"\n  }, \"https://deepmlblog.wordpress.com/2016/01/12/recurrent-neural-networks-for-decoding-captchas/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://simplecaptcha.sourceforge.net/\"\n  }, \"http://simplecaptcha.sourceforge.net/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://sourceforge.net/projects/simplecaptcha/\"\n  }, \"http://sourceforge.net/projects/simplecaptcha/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reading irctc captchas with 95% accuracy using deep learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/arunpatala/captcha.irctc\"\n  }, \"https://github.com/arunpatala/captcha.irctc\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"\\u7AEF\\u5230\\u7AEF\\u7684OCR\\uFF1A\\u57FA\\u4E8ECNN\\u7684\\u5B9E\\u73B0\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.xlvector.net/2016-05/mxnet-ocr-cnn/\"\n  }, \"http://blog.xlvector.net/2016-05/mxnet-ocr-cnn/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"I Am Robot: (Deep) Learning to Break Semantic Image CAPTCHAs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: automatically solving 70.78% of the image reCaptchachallenges,\\nwhile requiring only 19 seconds per challenge.\\napply to the Facebook image captcha and achieve an accuracy of 83.5%\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.columbia.edu/~polakis/papers/sivakorn_eurosp16.pdf\"\n  }, \"http://www.cs.columbia.edu/~polakis/papers/sivakorn_eurosp16.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SimGAN-Captcha\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Solve captcha without manually labeling a training set \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/rickyhan/SimGAN-Captcha\"\n  }, \"https://github.com/rickyhan/SimGAN-Captcha\"))), mdx(\"h1\", {\n    \"id\": \"handwritten-recognition\"\n  }, \"Handwritten Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.04925\"\n  }, \"http://arxiv.org/abs/1505.04925\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhongzhuoyao/HCCR-GoogLeNet\"\n  }, \"https://github.com/zhongzhuoyao/HCCR-GoogLeNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recognize your handwritten numbers\")), mdx(\"img\", {\n    \"src\": \"https://cdn-images-1.medium.com/max/800/1*YGT2w66tVNIgdiK7hEPSTQ.png\",\n    \"alt\": null\n  }), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@o.kroeger/recognize-your-handwritten-numbers-3f007cbe46ff#.jllz62xgu\"\n  }, \"https://medium.com/@o.kroeger/recognize-your-handwritten-numbers-3f007cbe46ff#.jllz62xgu\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Handwritten Digit Recognition using Convolutional Neural Networks in Python with Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\"\n  }, \"http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MNIST Handwritten Digit Classifier\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/karandesai-96/digit-classifier\"\n  }, \"https://github.com/karandesai-96/digit-classifier\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"\\u5982\\u4F55\\u7528\\u5377\\u79EF\\u795E\\u7ECF\\u7F51\\u7EDCCNN\\u8BC6\\u522B\\u624B\\u5199\\u6570\\u5B57\\u96C6\\uFF1F\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cnblogs.com/charlotte77/p/5671136.html\"\n  }, \"http://www.cnblogs.com/charlotte77/p/5671136.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LeNet \\u2013 Convolutional Neural Network in Python\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/\"\n  }, \"http://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.03286\"\n  }, \"http://arxiv.org/abs/1604.03286\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MLPaint: the Real-Time Handwritten Digit Recognizer\")), mdx(\"img\", {\n    \"src\": \"http://blog.mldb.ai/img/mlpaint_digits_thumb.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.mldb.ai/blog/posts/2016/09/mlpaint/\"\n  }, \"http://blog.mldb.ai/blog/posts/2016/09/mlpaint/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mldbai/mlpaint\"\n  }, \"https://github.com/mldbai/mlpaint\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.mldb.ai/ipy/notebooks/_demos/_latest/Image%20Processing%20with%20Convolutions.html\"\n  }, \"https://docs.mldb.ai/ipy/notebooks/_demos/_latest/Image%20Processing%20with%20Convolutions.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training a Computer to Recognize Your Handwriting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@annalyzin/training-a-computer-to-recognize-your-handwriting-24b808fb584#.gd4pb9jk2\"\n  }, \"https://medium.com/@annalyzin/training-a-computer-to-recognize-your-handwriting-24b808fb584#.gd4pb9jk2\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Using TensorFlow to create your own handwriting recognition engine\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://niektemme.com/2016/02/21/tensorflow-handwriting/\"\n  }, \"https://niektemme.com/2016/02/21/tensorflow-handwriting/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/niektemme/tensorflow-mnist-predict/\"\n  }, \"https://github.com/niektemme/tensorflow-mnist-predict/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Building a Deep Handwritten Digits Classifier using Microsoft Cognitive Toolkit\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69#.c3h6o7oxf\"\n  }, \"https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69#.c3h6o7oxf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tuzzer/ai-gym/blob/a97936619cf56b5ed43329c6fa13f7e26b1d46b8/MNIST/minist_softmax_cntk.py\"\n  }, \"https://github.com/tuzzer/ai-gym/blob/a97936619cf56b5ed43329c6fa13f7e26b1d46b8/MNIST/minist_softmax_cntk.py\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hand Writing Recognition Using Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: This CNN-based model for recognition of hand written digits attains a validation accuracy of 99.2% after training for 12 epochs.\\nIts trained on the MNIST dataset on Kaggle.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ayushoriginal/HandWritingRecognition-CNN\"\n  }, \"https://github.com/ayushoriginal/HandWritingRecognition-CNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Design of a Very Compact CNN Classifier for Online Handwritten Chinese Character Recognition Using DropWeight and Global Pooling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 0.57 MB, performance is decreased only by 0.91%.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.05207\"\n  }, \"https://arxiv.org/abs/1705.05207\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Handwritten digit string recognition by combination of residual network and RNN-CTC\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1710.03112\"\n  }, \"https://arxiv.org/abs/1710.03112\")), mdx(\"h1\", {\n    \"id\": \"plate-recognition\"\n  }, \"Plate Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1601.05610\"\n  }, \"http://arxiv.org/abs/1601.05610\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Number plate recognition with Tensorflow\")), mdx(\"img\", {\n    \"src\": \"http://matthewearl.github.io/assets/cnn-anpr/topology.svg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://matthewearl.github.io/2016/05/06/cnn-anpr/\"\n  }, \"http://matthewearl.github.io/2016/05/06/cnn-anpr/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Deep ANPR): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/matthewearl/deep-anpr\"\n  }, \"https://github.com/matthewearl/deep-anpr\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"end-to-end-for-plate-recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/szad670401/end-to-end-for-chinese-plate-recognition\"\n  }, \"https://github.com/szad670401/end-to-end-for-chinese-plate-recognition\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Segmentation-free Vehicle License Plate Recognition using ConvNet-RNN\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: International Workshop on Advanced Image Technology, January, 8-10, 2017. Penang, Malaysia. Proceeding IWAIT2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.06439\"\n  }, \"https://arxiv.org/abs/1701.06439\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"License Plate Detection and Recognition Using Deeply Learned Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.07330\"\n  }, \"https://arxiv.org/abs/1703.07330\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"api: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.sighthound.com/products/cloud\"\n  }, \"https://www.sighthound.com/products/cloud\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adversarial Generation of Training Examples for Vehicle License Plate Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1707.03124\"\n  }, \"https://arxiv.org/abs/1707.03124\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards End-to-End Car License Plates Detection and Recognition with Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.08828\"\n  }, \"https://arxiv.org/abs/1709.08828\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf\"\n  }, \"http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/detectRecog/CCPD\"\n  }, \"https://github.com/detectRecog/CCPD\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"dataset: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view\"\n  }, \"https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"High Accuracy Chinese Plate Recognition Framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\u57FA\\u4E8E\\u6DF1\\u5EA6\\u5B66\\u4E60\\u9AD8\\u6027\\u80FD\\u4E2D\\u6587\\u8F66\\u724C\\u8BC6\\u522B High Performance Chinese License Plate Recognition Framework.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gihtub: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zeusees/HyperLPR\"\n  }, \"https://github.com/zeusees/HyperLPR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LPRNet: License Plate Recognition via Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intrp=o: Intel IOTG Computer Vision Group\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: works in real-time with recognition accuracy up to 95% for Chinese license plates:\\n3 ms/plate on nVIDIAR GeForceTMGTX 1080 and 1.3 ms/plate on IntelR CoreTMi7-6700K CPU.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.10447\"\n  }, \"https://arxiv.org/abs/1806.10447\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How many labeled license plates are needed?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Chinese Conference on Pattern Recognition and Computer Vision\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.08410\"\n  }, \"https://arxiv.org/abs/1808.08410\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An End-to-End Neural Network for Multi-line License Plate Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sci-hub.se/10.1109/ICPR.2018.8546200#\"\n  }, \"https://sci-hub.se/10.1109/ICPR.2018.8546200#\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deeplearningshare/multi-line-plate-recognition\"\n  }, \"https://github.com/deeplearningshare/multi-line-plate-recognition\"))), mdx(\"h1\", {\n    \"id\": \"blogs\"\n  }, \"Blogs\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Applying OCR Technology for Receipt Recognition\")), mdx(\"img\", {\n    \"src\": \"http://rnd.azoft.com/wp-content/uploads/2016/04/applying-ocr-technology-for-receipt-recognition.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rnd.azoft.com/applying-ocr-technology-receipt-recognition/\"\n  }, \"http://rnd.azoft.com/applying-ocr-technology-receipt-recognition/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1qXQBQiC\"\n  }, \"http://pan.baidu.com/s/1qXQBQiC\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hacking MNIST in 30 lines of Python\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jrusev.github.io/post/hacking-mnist/\"\n  }, \"http://jrusev.github.io/post/hacking-mnist/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jrusev/simple-neural-networks\"\n  }, \"https://github.com/jrusev/simple-neural-networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Optical Character Recognition Using One-Shot Learning, RNN, and TensorFlow\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://blog.altoros.com/optical-character-recognition-using-one-shot-learning-rnn-and-tensorflow.html\"\n  }, \"https://blog.altoros.com/optical-character-recognition-using-one-shot-learning-rnn-and-tensorflow.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/\"\n  }, \"https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/\")), mdx(\"h1\", {\n    \"id\": \"projects\"\n  }, \"Projects\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ocropy: Python-based tools for document analysis and OCR\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tmbdev/ocropy\"\n  }, \"https://github.com/tmbdev/ocropy\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Extracting text from an image using Ocropus\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html\"\n  }, \"http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CLSTM : A small C++ implementation of LSTM networks, focused on OCR\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tmbdev/clstm\"\n  }, \"https://github.com/tmbdev/clstm\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"OCR text recognition using tensorflow with attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pannous/caffe-ocr\"\n  }, \"https://github.com/pannous/caffe-ocr\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pannous/tensorflow-ocr\"\n  }, \"https://github.com/pannous/tensorflow-ocr\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Digit Recognition via CNN: digital meter numbers detection\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/bbdd1924ed88618f11cfe2f302a624591122d666/687474703a2f2f37786e37777a2e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f64696769742e6a7067\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/SHUCV/digit\"\n  }, \"https://github.com/SHUCV/digit\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attention-OCR: Visual Attention based OCR\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/70902af6f701308b84854eb5f2f46f0729288a84/687474703a2f2f63732e636d752e6564752f25374579756e7469616e642f4f43522d322e6a7067\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/da03/Attention-OCR\"\n  }, \"https://github.com/da03/Attention-OCR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"umaru: An OCR-system based on torch using the technique of LSTM/GRU-RNN, CTC and referred to the works of rnnlib and clstm\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/edward-zhu/umaru\"\n  }, \"https://github.com/edward-zhu/umaru\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tesseract.js: Pure Javascript OCR for 62 Languages\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/naptha/tesseract.js/master/demo.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://tesseract.projectnaptha.com/\"\n  }, \"http://tesseract.projectnaptha.com/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/naptha/tesseract.js\"\n  }, \"https://github.com/naptha/tesseract.js\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepHCCR: Offline Handwritten Chinese Character Recognition based on GoogLeNet and AlexNet (With CaffeModel)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chongyangtao/DeepHCCR\"\n  }, \"https://github.com/chongyangtao/DeepHCCR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"deep ocr: make a better chinese character recognition OCR than tesseract\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/JinpengLI/deep_ocr\"\n  }, \"https://github.com/JinpengLI/deep_ocr\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Practical Deep OCR for scene text using CTPN + CRNN\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/blob/master/notebooks/OCR/readme.md\"\n  }, \"https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/blob/master/notebooks/OCR/readme.md\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tensorflow-based CNN+LSTM trained with CTC-loss for OCR\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com//weinman/cnn_lstm_ctc_ocr\"\n  }, \"https://github.com//weinman/cnn_lstm_ctc_ocr\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SSD_scene-text-detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//chenxinpeng/SSD_scene_text_detection\"\n  }, \"https://github.com//chenxinpeng/SSD_scene_text_detection\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.csdn.net/u010167269/article/details/52563573\"\n  }, \"http://blog.csdn.net/u010167269/article/details/52563573\"))), mdx(\"h1\", {\n    \"id\": \"videos\"\n  }, \"Videos\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LSTMs for OCR\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=5vW8faXvnrc\"\n  }, \"https://www.youtube.com/watch?v=5vW8faXvnrc\"))), mdx(\"h1\", {\n    \"id\": \"resources\"\n  }, \"Resources\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for OCR\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/hs105/Deep-Learning-for-OCR\"\n  }, \"https://github.com/hs105/Deep-Learning-for-OCR\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scene Text Localization & Recognition Resources\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A curated list of resources dedicated to scene text localization and recognition\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chongyangtao/Awesome-Scene-Text-Recognition\"\n  }, \"https://github.com/chongyangtao/Awesome-Scene-Text-Recognition\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scene Text Localization & Recognition Resources\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\u56FE\\u50CF\\u6587\\u672C\\u4F4D\\u7F6E\\u611F\\u77E5\\u4E0E\\u8BC6\\u522B\\u7684\\u8BBA\\u6587\\u8D44\\u6E90\\u6C47\\u603B\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/whitelok/image-text-localization-recognition/blob/master/README.zh-cn.md\"\n  }, \"https://github.com/whitelok/image-text-localization-recognition/blob/master/README.zh-cn.md\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"awesome-ocr: A curated list of promising OCR resources\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/wanghaisheng/awesome-ocr\"\n  }, \"https://github.com/wanghaisheng/awesome-ocr\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: OCR\ndate: 2015-10-09\n---\n\n# Papers\n\n**Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks**\n\n- intro: Google. Ian J. Goodfellow\n- arxiv: [https://arxiv.org/abs/1312.6082](https://arxiv.org/abs/1312.6082)\n\n**End-to-End Text Recognition with Convolutional Neural Networks**\n\n- paper: [http://www.cs.stanford.edu/~acoates/papers/wangwucoatesng_icpr2012.pdf](http://www.cs.stanford.edu/~acoates/papers/wangwucoatesng_icpr2012.pdf)\n- PhD thesis: [http://cs.stanford.edu/people/dwu4/HonorThesis.pdf](http://cs.stanford.edu/people/dwu4/HonorThesis.pdf)\n\n**Word Spotting and Recognition with Embedded Attributes**\n\n![](/assets/ocr-materials/Word_Spotting_and_Recognition_with_Embedded_Attributes.jpg)\n\n- paper: [http://ieeexplore.ieee.org.sci-hub.org/xpl/articleDetails.jsp?arnumber=6857995&filter%3DAND%28p_IS_Number%3A6940341%29](http://ieeexplore.ieee.org.sci-hub.org/xpl/articleDetails.jsp?arnumber=6857995&filter%3DAND%28p_IS_Number%3A6940341%29)\n\n**Reading Text in the Wild with Convolutional Neural Networks**\n\n![](http://www.robots.ox.ac.uk/~vgg/research/text/pipeline.png)\n\n- arxiv: [http://arxiv.org/abs/1412.1842](http://arxiv.org/abs/1412.1842)\n- homepage: [http://www.robots.ox.ac.uk/~vgg/publications/2016/Jaderberg16/](http://www.robots.ox.ac.uk/~vgg/publications/2016/Jaderberg16/)\n- demo: [http://zeus.robots.ox.ac.uk/textsearch/#/search/](http://zeus.robots.ox.ac.uk/textsearch/#/search/)\n- code: [http://www.robots.ox.ac.uk/~vgg/research/text/](http://www.robots.ox.ac.uk/~vgg/research/text/)\n\n**Deep structured output learning for unconstrained text recognition**\n\n- intro: \"propose an architecture consisting of a character sequence CNN and \nan N-gram encoding CNN which act on an input image in parallel and whose outputs are utilized\nalong with a CRF model to recognize the text content present within the image.\"\n- arxiv: [http://arxiv.org/abs/1412.5903](http://arxiv.org/abs/1412.5903)\n\n**Deep Features for Text Spotting**\n\n- paper: [http://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14/jaderberg14.pdf](http://www.robots.ox.ac.uk/~vgg/publications/2014/Jaderberg14/jaderberg14.pdf)\n- bitbucket: [https://bitbucket.org/jaderberg/eccv2014_textspotting](https://bitbucket.org/jaderberg/eccv2014_textspotting)\n- gitxiv: [http://gitxiv.com/posts/uB4y7QdD5XquEJ69c/deep-features-for-text-spotting](http://gitxiv.com/posts/uB4y7QdD5XquEJ69c/deep-features-for-text-spotting)\n\n**Reading Scene Text in Deep Convolutional Sequences**\n\n- intro: AAAI 2016\n- arxiv: [http://arxiv.org/abs/1506.04395](http://arxiv.org/abs/1506.04395)\n\n**DeepFont: Identify Your Font from An Image**\n\n- arxiv: [http://arxiv.org/abs/1507.03196](http://arxiv.org/abs/1507.03196)\n\n**An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition**\n\n- intro: Convolutional Recurrent Neural Network (CRNN)\n- arxiv: [http://arxiv.org/abs/1507.05717](http://arxiv.org/abs/1507.05717)\n- github: [https://github.com/bgshih/crnn](https://github.com/bgshih/crnn)\n- github: [https://github.com/meijieru/crnn.pytorch](https://github.com/meijieru/crnn.pytorch)\n\n**Recursive Recurrent Nets with Attention Modeling for OCR in the Wild**\n\n- arxiv: [http://arxiv.org/abs/1603.03101](http://arxiv.org/abs/1603.03101)\n\n**Writer-independent Feature Learning for Offline Signature Verification using Deep Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.00974](http://arxiv.org/abs/1604.00974)\n\n**DeepText: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images**\n\n- arxiv: [http://arxiv.org/abs/1605.07314](http://arxiv.org/abs/1605.07314)\n\n**End-to-End Interpretation of the French Street Name Signs Dataset**\n\n- paper: [http://link.springer.com/chapter/10.1007%2F978-3-319-46604-0_30](http://link.springer.com/chapter/10.1007%2F978-3-319-46604-0_30)\n- github: [https://github.com/tensorflow/models/tree/master/street](https://github.com/tensorflow/models/tree/master/street)\n\n**End-to-End Subtitle Detection and Recognition for Videos in East Asian Languages via CNN Ensemble with Near-Human-Level Performance**\n\n- arxiv: [https://arxiv.org/abs/1611.06159](https://arxiv.org/abs/1611.06159)\n\n**Smart Library: Identifying Books in a Library using Richly Supervised Deep Scene Text Reading**\n\n- arxiv: [https://arxiv.org/abs/1611.07385](https://arxiv.org/abs/1611.07385)\n\n**Improving Text Proposals for Scene Images with Fully Convolutional Networks**\n\n- intro: Universitat Autonoma de Barcelona (UAB) & University of Florence\n- intro: International Conference on Pattern Recognition (ICPR) - DLPR (Deep Learning for Pattern Recognition) workshop\n- arxiv: [https://arxiv.org/abs/1702.05089](https://arxiv.org/abs/1702.05089)\n\n**Scene Text Eraser**\n\n[https://arxiv.org/abs/1705.02772](https://arxiv.org/abs/1705.02772)\n\n**Attention-based Extraction of Structured Information from Street View Imagery**\n\n- intro: University College London & Google Inc\n- arxiv: [https://arxiv.org/abs/1704.03549](https://arxiv.org/abs/1704.03549)\n- github: [https://github.com/tensorflow/models/tree/master/attention_ocr](https://github.com/tensorflow/models/tree/master/attention_ocr)\n\n**Implicit Language Model in LSTM for OCR**\n\n[https://arxiv.org/abs/1805.09441](https://arxiv.org/abs/1805.09441)\n\n**Scene Text Magnifier**\n\n- intro: ICDAR 2019\n- arxiv: [https://arxiv.org/abs/1907.00693](https://arxiv.org/abs/1907.00693)\n\n# Text Detection\n\n**Object Proposals for Text Extraction in the Wild**\n\n- intro: ICDAR 2015\n- arxiv: [http://arxiv.org/abs/1509.02317](http://arxiv.org/abs/1509.02317)\n- github: [https://github.com/lluisgomez/TextProposals](https://github.com/lluisgomez/TextProposals)\n\n**Text-Attentional Convolutional Neural Networks for Scene Text Detection**\n\n- arxiv: [http://arxiv.org/abs/1510.03283](http://arxiv.org/abs/1510.03283)\n\n**Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network**\n\n- arxiv: [http://arxiv.org/abs/1603.09423](http://arxiv.org/abs/1603.09423)\n\n**Synthetic Data for Text Localisation in Natural Images**\n\n![](https://raw.githubusercontent.com/ankush-me/SynthText/master/samples.png)\n\n- intro: CVPR 2016\n- project page: [http://www.robots.ox.ac.uk/~vgg/data/scenetext/](http://www.robots.ox.ac.uk/~vgg/data/scenetext/)\n- arxiv: [http://arxiv.org/abs/1604.06646](http://arxiv.org/abs/1604.06646)\n- paper: [http://www.robots.ox.ac.uk/~vgg/data/scenetext/gupta16.pdf](http://www.robots.ox.ac.uk/~vgg/data/scenetext/gupta16.pdf)\n- github: [https://github.com/ankush-me/SynthText](https://github.com/ankush-me/SynthText)\n\n**Scene Text Detection via Holistic, Multi-Channel Prediction**\n\n- arxiv: [http://arxiv.org/abs/1606.09002](http://arxiv.org/abs/1606.09002)\n\n**Detecting Text in Natural Image with Connectionist Text Proposal Network**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1609.03605](http://arxiv.org/abs/1609.03605)\n- github(Caffe): [https://github.com/tianzhi0549/CTPN](https://github.com/tianzhi0549/CTPN)\n- github(CUDA8.0 support): [https://github.com/qingswu/CTPN](https://github.com/qingswu/CTPN)\n- demo: [http://textdet.com/](http://textdet.com/)\n- github(Tensorflow): [https://github.com/eragonruan/text-detection-ctpn](https://github.com/eragonruan/text-detection-ctpn)\n\n**TextBoxes: A Fast Text Detector with a Single Deep Neural Network**\n\n- intro: AAAI 2017\n- arxiv: [https://arxiv.org/abs/1611.06779](https://arxiv.org/abs/1611.06779)\n- github(Caffe): [https://github.com/MhLiao/TextBoxes](https://github.com/MhLiao/TextBoxes)\n- github: [https://github.com/xiaodiu2010/TextBoxes-TensorFlow](https://github.com/xiaodiu2010/TextBoxes-TensorFlow)\n\n**TextBoxes++: A Single-Shot Oriented Scene Text Detector**\n\n- intro: TIP 2018. University of Science and Technology(HUST)\n- arxiv: [https://arxiv.org/abs/1801.02765](https://arxiv.org/abs/1801.02765)\n- github(official, Caffe): [https://github.com/MhLiao/TextBoxes_plusplus](https://github.com/MhLiao/TextBoxes_plusplus)\n\n**Arbitrary-Oriented Scene Text Detection via Rotation Proposals**\n\n- intro: IEEE Transactions on Multimedia\n- keywords: RRPN\n- arxiv: [https://arxiv.org/abs/1703.01086](https://arxiv.org/abs/1703.01086)\n- github: [https://github.com/mjq11302010044/RRPN](https://github.com/mjq11302010044/RRPN)\n- github: [https://github.com/DetectionTeamUCAS/RRPN_Faster-RCNN_Tensorflow](https://github.com/DetectionTeamUCAS/RRPN_Faster-RCNN_Tensorflow)\n\n**Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection**\n\n- intro: CVPR 2017\n- intro: F-measure 70.64%, outperforming the existing state-of-the-art method with F-measure 63.76%\n- arxiv: [https://arxiv.org/abs/1703.01425](https://arxiv.org/abs/1703.01425)\n\n**Detecting Oriented Text in Natural Images by Linking Segments**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1703.06520](https://arxiv.org/abs/1703.06520)\n- github(Tensorflow): [https://github.com/dengdan/seglink](https://github.com/dengdan/seglink)\n\n**Deep Direct Regression for Multi-Oriented Scene Text Detection**\n\n- arxiv: [https://arxiv.org/abs/1703.08289](https://arxiv.org/abs/1703.08289)\n\n**Cascaded Segmentation-Detection Networks for Word-Level Text Spotting**\n\n[https://arxiv.org/abs/1704.00834](https://arxiv.org/abs/1704.00834)\n\n**Text-Detection-using-py-faster-rcnn-framework**\n\n- github: [https://github.com/jugg1024/Text-Detection-with-FRCN](https://github.com/jugg1024/Text-Detection-with-FRCN)\n\n**WordFence: Text Detection in Natural Images with Border Awareness**\n\n- intro: ICIP 2017\n- arcxiv: [https://arxiv.org/abs/1705.05483](https://arxiv.org/abs/1705.05483)\n\n**SSD-text detection: Text Detector**\n\n- intro: A modified SSD model for text detection\n- github: [https://github.com/oyxhust/ssd-text_detection](https://github.com/oyxhust/ssd-text_detection)\n\n**R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection**\n\n- intro: Samsung R&D Institute China\n- arxiv: [https://arxiv.org/abs/1706.09579](https://arxiv.org/abs/1706.09579)\n\n**R-PHOC: Segmentation-Free Word Spotting using CNN**\n\n- intro: ICDAR 2017\n- arxiv: [https://arxiv.org/abs/1707.01294](https://arxiv.org/abs/1707.01294)\n\n**Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.03985](https://arxiv.org/abs/1707.03985)\n\n**EAST: An Efficient and Accurate Scene Text Detector**\n\n- intro: CVPR 2017. Megvii\n- arxiv: [https://arxiv.org/abs/1704.03155](https://arxiv.org/abs/1704.03155)\n- paper: [http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf)\n- github(Tensorflow): [https://github.com/argman/EAST](https://github.com/argman/EAST)\n\n**Deep Scene Text Detection with Connected Component Proposals**\n\n- intro: Amap Vision Lab, Alibaba Group\n- arxiv: [https://arxiv.org/abs/1708.05133](https://arxiv.org/abs/1708.05133)\n\n**Single Shot Text Detector with Regional Attention**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.00138](https://arxiv.org/abs/1709.00138)\n- github: [https://github.com/BestSonny/SSTD](https://github.com/BestSonny/SSTD)\n- code: [http://sstd.whuang.org](http://sstd.whuang.org)\n\n**Fused Text Segmentation Networks for Multi-oriented Scene Text Detection**\n\n[https://arxiv.org/abs/1709.03272](https://arxiv.org/abs/1709.03272)\n\n**Deep Residual Text Detection Network for Scene Text**\n\n- intro: IAPR International Conference on Document Analysis and Recognition (ICDAR) 2017. Samsung R&D Institute of China, Beijing\n- arxiv: [https://arxiv.org/abs/1711.04147](https://arxiv.org/abs/1711.04147)\n\n**Feature Enhancement Network: A Refined Scene Text Detector**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1711.04249](https://arxiv.org/abs/1711.04249)\n\n**ArbiText: Arbitrary-Oriented Text Detection in Unconstrained Scene**\n\n[https://arxiv.org/abs/1711.11249](https://arxiv.org/abs/1711.11249)\n\n**Detecting Curve Text in the Wild: New Dataset and New Solution**\n\n- arxiv: [https://arxiv.org/abs/1712.02170](https://arxiv.org/abs/1712.02170)\n- github: [https://github.com/Yuliang-Liu/Curve-Text-Detector](https://github.com/Yuliang-Liu/Curve-Text-Detector)\n\n**FOTS: Fast Oriented Text Spotting with a Unified Network**\n\n[https://arxiv.org/abs/1801.01671](https://arxiv.org/abs/1801.01671)\n\n**PixelLink: Detecting Scene Text via Instance Segmentation**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1801.01315](https://arxiv.org/abs/1801.01315)\n\n**PixelLink: Detecting Scene Text via Instance Segmentation**\n\n- intro: AAAI 2018. Zhejiang University & Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1801.01315](https://arxiv.org/abs/1801.01315)\n\n**Sliding Line Point Regression for Shape Robust Scene Text Detection**\n\n[https://arxiv.org/abs/1801.09969](https://arxiv.org/abs/1801.09969)\n\n**Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1802.08948](https://arxiv.org/abs/1802.08948)\n\n**Single Shot TextSpotter with Explicit Alignment and Attention**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.03474](https://arxiv.org/abs/1803.03474)\n\n**Rotation-Sensitive Regression for Oriented Scene Text Detection**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.05265](https://arxiv.org/abs/1803.05265)\n\n**Detecting Multi-Oriented Text with Corner-based Region Proposals**\n\n- arxiv: [https://arxiv.org/abs/1804.02690](https://arxiv.org/abs/1804.02690)\n- github: [https://github.com/xhzdeng/crpn](https://github.com/xhzdeng/crpn)\n\n**An Anchor-Free Region Proposal Network for Faster R-CNN based Text Detection Approaches**\n\n[https://arxiv.org/abs/1804.09003](https://arxiv.org/abs/1804.09003)\n\n**IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection**\n\n- intro: IJCAI 2018. Alibaba Group\n- arxiv: [https://arxiv.org/abs/1805.01167](https://arxiv.org/abs/1805.01167)\n\n**Boosting up Scene Text Detectors with Guided CNN**\n\n[https://arxiv.org/abs/1805.04132](https://arxiv.org/abs/1805.04132)\n\n**Shape Robust Text Detection with Progressive Scale Expansion Network**\n\n- arxiv: [https://arxiv.org/abs/1806.02559](https://arxiv.org/abs/1806.02559)\n- github: [https://github.com/whai362/PSENet](https://github.com/whai362/PSENet)\n\n**A Single Shot Text Detector with Scale-adaptive Anchors**\n\n[https://arxiv.org/abs/1807.01884](https://arxiv.org/abs/1807.01884)\n\n**TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.01544](https://arxiv.org/abs/1807.01544)\n\n**Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes**\n\n- intro: ECCV 2018. Huazhong University of Science and Technology & Megvii (Face++) Technology\n- arxiv: [https://arxiv.org/abs/1807.02242](https://arxiv.org/abs/1807.02242)\n- github: [https://github.com/MhLiao/MaskTextSpotter](https://github.com/MhLiao/MaskTextSpotter)\n\n**Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.03547](https://arxiv.org/abs/1807.03547)\n\n**TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade**\n\n[https://arxiv.org/abs/1809.03050](https://arxiv.org/abs/1809.03050)\n\n**Correlation Propagation Networks for Scene Text Detection**\n\n[https://arxiv.org/abs/1810.00304](https://arxiv.org/abs/1810.00304)\n\n**Scene Text Detection with Supervised Pyramid Context Network**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.08605](https://arxiv.org/abs/1811.08605)\n\n**Improving Rotated Text Detection with Rotation Region Proposal Networks**\n\n[https://arxiv.org/abs/1811.07031](https://arxiv.org/abs/1811.07031)\n\n**Pixel-Anchor: A Fast Oriented Scene Text Detector with Combined Networks**\n\n[https://arxiv.org/abs/1811.07432](https://arxiv.org/abs/1811.07432)\n\n**Mask R-CNN with Pyramid Attention Network for Scene Text Detection**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1811.09058](https://arxiv.org/abs/1811.09058)\n\n**TextField: Learning A Deep Direction Field for Irregular Scene Text Detection**\n\n- intro: Huazhong University of Science and Technology (HUST) &  Alibaba Group\n- arxiv: [https://arxiv.org/abs/1812.01393](https://arxiv.org/abs/1812.01393)\n\n**Detecting Text in the Wild with Deep Character Embedding Network**\n\n- intro: ACCV 2018\n- intro: Baidu\n- arxiv: [https://arxiv.org/abs/1901.00363](https://arxiv.org/abs/1901.00363)\n\n**MSR: Multi-Scale Shape Regression for Scene Text Detection**\n\n[https://arxiv.org/abs/1901.02596](https://arxiv.org/abs/1901.02596)\n\n**Pyramid Mask Text Detector**\n\n- intro: SenseTime & Beihang University & CUHK\n- arxiv: [https://arxiv.org/abs/1903.11800](https://arxiv.org/abs/1903.11800)\n\n**Shape Robust Text Detection with Progressive Scale Expansion Network**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.12473](https://arxiv.org/abs/1903.12473)\n\n**Tightness-aware Evaluation Protocol for Scene Text Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.00813](https://arxiv.org/abs/1904.00813)\n- github: [https://github.com/Yuliang-Liu/TIoU-metric](https://github.com/Yuliang-Liu/TIoU-metric)\n\n**Character Region Awareness for Text Detection**\n\n- intro: CVPR 2019\n- keywords: CRAFT: Character-Region Awareness For Text detection\n- arxiv: [https://arxiv.org/abs/1904.01941](https://arxiv.org/abs/1904.01941)\n- github(official): [https://github.com/clovaai/CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch)\n\n**Towards End-to-End Text Spotting in Natural Scenes**\n\n- intro: An extension of the work \"Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks\", Proc. Int. Conf. Comp. Vision 2017\n- arxiv: [https://arxiv.org/abs/1906.06013](https://arxiv.org/abs/1906.06013)\n\n**A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning**\n\n- intro: ACM MM 2019\n- arxiv: [https://arxiv.org/abs/1908.05498](https://arxiv.org/abs/1908.05498)\n\n**Geometry Normalization Networks for Accurate Scene Text Detection**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.00794](https://arxiv.org/abs/1909.00794)\n\n**Real-time Scene Text Detection with Differentiable Binarization**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.08947](https://arxiv.org/abs/1911.08947)\n- github: [https://github.com/MhLiao/DB](https://github.com/MhLiao/DB)\n\n**TextTubes for Detecting Curved Text in the Wild**\n\n[https://arxiv.org/abs/1912.08990](https://arxiv.org/abs/1912.08990)\n\n**Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/2002.06820](https://arxiv.org/abs/2002.06820)\n\n**ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2002.10200](https://arxiv.org/abs/2002.10200)\n\n**DGST : Discriminator Guided Scene Text detector**\n\n[https://arxiv.org/abs/2002.12509](https://arxiv.org/abs/2002.12509)\n\n**MANGO: A Mask Attention Guided One-Stage Scene Text Spotter**\n\n- intro: AAAI 2021\n- arxiv: [https://arxiv.org/abs/2012.04350](https://arxiv.org/abs/2012.04350)\n\n**Vision-Language Pre-Training for Boosting Scene Text Detectors**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.13867](https://arxiv.org/abs/2204.13867)\n\n# Text Recognition\n\n**Sequence to sequence learning for unconstrained scene text recognition**\n\n- intro: master thesis\n- arxiv: [http://arxiv.org/abs/1607.06125](http://arxiv.org/abs/1607.06125)\n\n**Drawing and Recognizing Chinese Characters with Recurrent Neural Network**\n\n- arxiv: [https://arxiv.org/abs/1606.06539](https://arxiv.org/abs/1606.06539)\n\n**Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition**\n\n- intro: correct rates: Dataset-CASIA 97.10% and Dataset-ICDAR 97.15%\n- arxiv: [https://arxiv.org/abs/1610.02616](https://arxiv.org/abs/1610.02616)\n\n**Stroke Sequence-Dependent Deep Convolutional Neural Network for Online Handwritten Chinese Character Recognition**\n\n- arxiv: [https://arxiv.org/abs/1610.04057](https://arxiv.org/abs/1610.04057)\n\n**Visual attention models for scene text recognition**\n\n[https://arxiv.org/abs/1706.01487](https://arxiv.org/abs/1706.01487)\n\n**Focusing Attention: Towards Accurate Text Recognition in Natural Images**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.02054](https://arxiv.org/abs/1709.02054)\n\n**Scene Text Recognition with Sliding Convolutional Character Models**\n\n[https://arxiv.org/abs/1709.01727](https://arxiv.org/abs/1709.01727)\n\n**AdaDNNs: Adaptive Ensemble of Deep Neural Networks for Scene Text Recognition**\n\n[https://arxiv.org/abs/1710.03425](https://arxiv.org/abs/1710.03425)\n\n**A New Hybrid-parameter Recurrent Neural Networks for Online Handwritten Chinese Character Recognition**\n\n[https://arxiv.org/abs/1711.02809](https://arxiv.org/abs/1711.02809)\n\n**AON: Towards Arbitrarily-Oriented Text Recognition**\n\n- arxiv: [https://arxiv.org/abs/1711.04226](https://arxiv.org/abs/1711.04226)\n- github: [https://github.com/huizhang0110/AON](https://github.com/huizhang0110/AON)\n\n**Arbitrarily-Oriented Text Recognition**\n\n- intro: A method used in ICDAR 2017 word recognition competitions\n- arxiv: [https://arxiv.org/abs/1711.04226](https://arxiv.org/abs/1711.04226)\n\n**SEE: Towards Semi-Supervised End-to-End Scene Text Recognition**\n\n[https://arxiv.org/abs/1712.05404](https://arxiv.org/abs/1712.05404)\n\n**Edit Probability for Scene Text Recognition**\n\n- intro: Fudan University & Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1805.03384](https://arxiv.org/abs/1805.03384)\n\n**SCAN: Sliding Convolutional Attention Network for Scene Text Recognition**\n\n[https://arxiv.org/abs/1806.00578](https://arxiv.org/abs/1806.00578)\n\n**Adaptive Adversarial Attack on Scene Text Recognition**\n\n- intro: University of Florida\n- arxiv: [https://arxiv.org/abs/1807.03326](https://arxiv.org/abs/1807.03326)\n\n**ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification**\n\n[https://arxiv.org/abs/1812.05824](https://arxiv.org/abs/1812.05824)\n\n**A Multi-Object Rectified Attention Network for Scene Text Recognition**\n\n- intro: Pattern Recognition 2019\n- keywords: MORAN\n- arxiv: [https://arxiv.org/abs/1901.03003](https://arxiv.org/abs/1901.03003)\n\n**SAFE: Scale Aware Feature Encoder for Scene Text Recognition**\n\n- intro: ACCV 2018\n- arxiv: [https://arxiv.org/abs/1901.05770](https://arxiv.org/abs/1901.05770)\n\n**A Simple and Robust Convolutional-Attention Network for Irregular Text Recognition**\n\n[https://arxiv.org/abs/1904.01375](https://arxiv.org/abs/1904.01375)\n\n**FACLSTM: ConvLSTM with Focused Attention for Scene Text Recognition**\n\n[https://arxiv.org/abs/1904.09405](https://arxiv.org/abs/1904.09405)\n\n**Towards Accurate Scene Text Recognition with Semantic Reasoning Networks**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2003.12294](https://arxiv.org/abs/2003.12294)\n\n**FedOCR: Communication-Efficient Federated Learning for Scene Text Recognition**\n\n- intro: Huazhong University of Science and Technology & Meituan-Dianping Group\n- arxiv: [https://arxiv.org/abs/2007.11462](https://arxiv.org/abs/2007.11462)\n\n# Text Spotting & Text Detection + Recognition\n\n**STN-OCR: A single Neural Network for Text Detection and Text Recognition**\n\n- arxiv: [https://arxiv.org/abs/1707.08831](https://arxiv.org/abs/1707.08831)\n- github(MXNet): [https://github.com/Bartzi/stn-ocr](https://github.com/Bartzi/stn-ocr)\n\n**Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework**\n\n- intro: ICCV 2017\n- arxiv: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf)\n\n**FOTS: Fast Oriented Text Spotting with a Unified Network**\n\n[https://arxiv.org/abs/1801.01671](https://arxiv.org/abs/1801.01671)\n\n**Single Shot TextSpotter with Explicit Alignment and Attention**\n\n**An end-to-end TextSpotter with Explicit Alignment and Attention**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.03474](https://arxiv.org/abs/1803.03474)\n- github(official, Caffe): [https://github.com/tonghe90/textspotter](https://github.com/tonghe90/textspotter)\n\n**Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.03021](https://arxiv.org/abs/1807.03021)\n- github: [https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scenes](https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scenes)\n\n**Scene Text Detection and Recognition: The Deep Learning Era**\n\n- arxiv: [https://arxiv.org/abs/1811.04256](https://arxiv.org/abs/1811.04256)\n- gihtub: [https://github.com/Jyouhou/SceneTextPapers](https://github.com/Jyouhou/SceneTextPapers)\n\n**A Novel Integrated Framework for Learning both Text Detection and Recognition**\n\n- intro: Alibaba\n- arxiv: [https://arxiv.org/abs/1811.08611](https://arxiv.org/abs/1811.08611)\n\n**Efficient Video Scene Text Spotting: Unifying Detection, Tracking, and Recognition**\n\n- intro: Zhejiang University & Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1903.03299](https://arxiv.org/abs/1903.03299)\n\n**A Multitask Network for Localization and Recognition of Text in Images**\n\n- intro: ICDAR 2019\n- arxiv: [https://arxiv.org/abs/1906.09266](https://arxiv.org/abs/1906.09266)\n\n**GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1907.09653](https://arxiv.org/abs/1907.09653)\n\n**Convolutional Character Networks**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1910.07954](https://arxiv.org/abs/1910.07954)\n- github: [https://github.com/MalongTech/research-charnet](https://github.com/MalongTech/research-charnet)\n\n**RoadText-1K: Text Detection & Recognition Dataset for Driving Videos**\n\n- intro: ICRA 2020\n- project page: [http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k](http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k)\n- arxiv: [https://arxiv.org/abs/2005.09496](https://arxiv.org/abs/2005.09496)\n\n**SPTS: Single-Point Text Spotting**\n\n- intro: Chinese University of Hong Kong & South China University of Technology & University of Adelaide & ByteDance Inc. & Huawei Technologies & Zhejiang University\n- arxiv: [https://arxiv.org/abs/2112.07917](https://arxiv.org/abs/2112.07917)\n\n**Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer**\n\n- intro: AWS AI Labs\n- arxiv: [https://arxiv.org/abs/2202.05508](https://arxiv.org/abs/2202.05508)\n\n**SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition**\n\n- intro: CVPR 2022\n- intro: South China University of Technology & Chinese University of Hong Kong 3Huawei Cloud AI & IntSig Information Co., Ltd & Peng Cheng Laboratory\n- arxiv: [https://arxiv.org/abs/2203.10209](https://arxiv.org/abs/2203.10209)\n- github: [https://github.com/mxin262/SwinTextSpotter](https://github.com/mxin262/SwinTextSpotter)\n\n**End-to-End Video Text Spotting with Transformer**\n\n- intro: Zhejiang University & Kuaishou Technology & Beijing Institute of Technology & Beijing University of Posts and Telecommunications & The University of Hong Kong\n- arxiv: [https://arxiv.org/abs/2203.10539](https://arxiv.org/abs/2203.10539)\n- github: [https://github.com/weijiawu/TransDETR](https://github.com/weijiawu/TransDETR)\n\n**Text Spotting Transformers**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2204.01918](https://arxiv.org/abs/2204.01918)\n\n**Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting**\n\n- intro: ECCV 2022\n- intro: Zhejiang University & Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/2207.06694](https://arxiv.org/abs/2207.06694)\n- github: [https://github.com/hikopensource/DAVAR-Lab-OCR/](https://github.com/hikopensource/DAVAR-Lab-OCR/)\n\n# Breaking Captcha\n\n**Using deep learning to break a Captcha system**\n\n- intro: \"Using Torch code to break simplecaptcha with 92% accuracy\"\n- blog: [https://deepmlblog.wordpress.com/2016/01/03/how-to-break-a-captcha-system/](https://deepmlblog.wordpress.com/2016/01/03/how-to-break-a-captcha-system/)\n- github: [https://github.com/arunpatala/captcha](https://github.com/arunpatala/captcha)\n\n**Breaking reddit captcha with 96% accuracy**\n\n- blog: [https://deepmlblog.wordpress.com/2016/01/05/breaking-reddit-captcha-with-96-accuracy/](https://deepmlblog.wordpress.com/2016/01/05/breaking-reddit-captcha-with-96-accuracy/)\n- github: [https://github.com/arunpatala/reddit.captcha](https://github.com/arunpatala/reddit.captcha)\n\n**I’m not a human: Breaking the Google reCAPTCHA**\n\n- paper: [https://www.blackhat.com/docs/asia-16/materials/asia-16-Sivakorn-Im-Not-a-Human-Breaking-the-Google-reCAPTCHA-wp.pdf](https://www.blackhat.com/docs/asia-16/materials/asia-16-Sivakorn-Im-Not-a-Human-Breaking-the-Google-reCAPTCHA-wp.pdf)\n\n**Neural Net CAPTCHA Cracker**\n\n- slides: [http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring15/geetika/CS298%20Slides%20-%20PDF](http://www.cs.sjsu.edu/faculty/pollett/masters/Semesters/Spring15/geetika/CS298%20Slides%20-%20PDF)\n- github: [https://github.com/bgeetika/Captcha-Decoder](https://github.com/bgeetika/Captcha-Decoder)\n- demo: [http://cp-training.appspot.com/](http://cp-training.appspot.com/)\n\n**Recurrent neural networks for decoding CAPTCHAS**\n\n- blog: [https://deepmlblog.wordpress.com/2016/01/12/recurrent-neural-networks-for-decoding-captchas/](https://deepmlblog.wordpress.com/2016/01/12/recurrent-neural-networks-for-decoding-captchas/)\n- demo: [http://simplecaptcha.sourceforge.net/](http://simplecaptcha.sourceforge.net/)\n- code: [http://sourceforge.net/projects/simplecaptcha/](http://sourceforge.net/projects/simplecaptcha/)\n\n**Reading irctc captchas with 95% accuracy using deep learning**\n\n- github: [https://github.com/arunpatala/captcha.irctc](https://github.com/arunpatala/captcha.irctc)\n\n**端到端的OCR：基于CNN的实现**\n\n- blog: [http://blog.xlvector.net/2016-05/mxnet-ocr-cnn/](http://blog.xlvector.net/2016-05/mxnet-ocr-cnn/)\n\n**I Am Robot: (Deep) Learning to Break Semantic Image CAPTCHAs**\n\n- intro: automatically solving 70.78% of the image reCaptchachallenges, \nwhile requiring only 19 seconds per challenge. \napply to the Facebook image captcha and achieve an accuracy of 83.5%\n- paper: [http://www.cs.columbia.edu/~polakis/papers/sivakorn_eurosp16.pdf](http://www.cs.columbia.edu/~polakis/papers/sivakorn_eurosp16.pdf)\n\n**SimGAN-Captcha**\n\n- intro: Solve captcha without manually labeling a training set \n- github: [https://github.com/rickyhan/SimGAN-Captcha](https://github.com/rickyhan/SimGAN-Captcha)\n\n# Handwritten Recognition\n\n**High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps**\n\n- arxiv: [http://arxiv.org/abs/1505.04925](http://arxiv.org/abs/1505.04925)\n- github: [https://github.com/zhongzhuoyao/HCCR-GoogLeNet](https://github.com/zhongzhuoyao/HCCR-GoogLeNet)\n\n**Recognize your handwritten numbers**\n\n![](https://cdn-images-1.medium.com/max/800/1*YGT2w66tVNIgdiK7hEPSTQ.png)\n\n[https://medium.com/@o.kroeger/recognize-your-handwritten-numbers-3f007cbe46ff#.jllz62xgu](https://medium.com/@o.kroeger/recognize-your-handwritten-numbers-3f007cbe46ff#.jllz62xgu)\n\n**Handwritten Digit Recognition using Convolutional Neural Networks in Python with Keras**\n\n- blog: [http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/](http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/)\n\n**MNIST Handwritten Digit Classifier**\n\n- github: [https://github.com/karandesai-96/digit-classifier](https://github.com/karandesai-96/digit-classifier)\n\n**如何用卷积神经网络CNN识别手写数字集？**\n\n- blog: [http://www.cnblogs.com/charlotte77/p/5671136.html](http://www.cnblogs.com/charlotte77/p/5671136.html)\n\n**LeNet – Convolutional Neural Network in Python**\n\n- blog: [http://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/](http://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/)\n\n**Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention**\n\n- arxiv: [http://arxiv.org/abs/1604.03286](http://arxiv.org/abs/1604.03286)\n\n**MLPaint: the Real-Time Handwritten Digit Recognizer**\n\n![](http://blog.mldb.ai/img/mlpaint_digits_thumb.gif)\n\n- blog: [http://blog.mldb.ai/blog/posts/2016/09/mlpaint/](http://blog.mldb.ai/blog/posts/2016/09/mlpaint/)\n- github: [https://github.com/mldbai/mlpaint](https://github.com/mldbai/mlpaint)\n- demo: [https://docs.mldb.ai/ipy/notebooks/_demos/_latest/Image%20Processing%20with%20Convolutions.html](https://docs.mldb.ai/ipy/notebooks/_demos/_latest/Image%20Processing%20with%20Convolutions.html)\n\n**Training a Computer to Recognize Your Handwriting**\n\n[https://medium.com/@annalyzin/training-a-computer-to-recognize-your-handwriting-24b808fb584#.gd4pb9jk2](https://medium.com/@annalyzin/training-a-computer-to-recognize-your-handwriting-24b808fb584#.gd4pb9jk2)\n\n**Using TensorFlow to create your own handwriting recognition engine**\n\n- blog: [https://niektemme.com/2016/02/21/tensorflow-handwriting/](https://niektemme.com/2016/02/21/tensorflow-handwriting/)\n- github: [https://github.com/niektemme/tensorflow-mnist-predict/](https://github.com/niektemme/tensorflow-mnist-predict/)\n\n**Building a Deep Handwritten Digits Classifier using Microsoft Cognitive Toolkit**\n\n- blog: [https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69#.c3h6o7oxf](https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69#.c3h6o7oxf)\n- github: [https://github.com/tuzzer/ai-gym/blob/a97936619cf56b5ed43329c6fa13f7e26b1d46b8/MNIST/minist_softmax_cntk.py](https://github.com/tuzzer/ai-gym/blob/a97936619cf56b5ed43329c6fa13f7e26b1d46b8/MNIST/minist_softmax_cntk.py)\n\n**Hand Writing Recognition Using Convolutional Neural Networks**\n\n- intro: This CNN-based model for recognition of hand written digits attains a validation accuracy of 99.2% after training for 12 epochs. \nIts trained on the MNIST dataset on Kaggle.\n- github: [https://github.com/ayushoriginal/HandWritingRecognition-CNN](https://github.com/ayushoriginal/HandWritingRecognition-CNN)\n\n**Design of a Very Compact CNN Classifier for Online Handwritten Chinese Character Recognition Using DropWeight and Global Pooling**\n\n- intro: 0.57 MB, performance is decreased only by 0.91%.\n- arxiv: [https://arxiv.org/abs/1705.05207](https://arxiv.org/abs/1705.05207)\n\n**Handwritten digit string recognition by combination of residual network and RNN-CTC**\n\n[https://arxiv.org/abs/1710.03112](https://arxiv.org/abs/1710.03112)\n\n# Plate Recognition\n\n**Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs**\n\n- arxiv: [http://arxiv.org/abs/1601.05610](http://arxiv.org/abs/1601.05610)\n\n**Number plate recognition with Tensorflow**\n\n![](http://matthewearl.github.io/assets/cnn-anpr/topology.svg)\n\n- blog: [http://matthewearl.github.io/2016/05/06/cnn-anpr/](http://matthewearl.github.io/2016/05/06/cnn-anpr/)\n- github(Deep ANPR): [https://github.com/matthewearl/deep-anpr](https://github.com/matthewearl/deep-anpr)\n\n**end-to-end-for-plate-recognition**\n\n- github: [https://github.com/szad670401/end-to-end-for-chinese-plate-recognition](https://github.com/szad670401/end-to-end-for-chinese-plate-recognition)\n\n**Segmentation-free Vehicle License Plate Recognition using ConvNet-RNN**\n\n- intro: International Workshop on Advanced Image Technology, January, 8-10, 2017. Penang, Malaysia. Proceeding IWAIT2017\n- arxiv: [https://arxiv.org/abs/1701.06439](https://arxiv.org/abs/1701.06439)\n\n**License Plate Detection and Recognition Using Deeply Learned Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1703.07330](https://arxiv.org/abs/1703.07330)\n- api: [https://www.sighthound.com/products/cloud](https://www.sighthound.com/products/cloud)\n\n**Adversarial Generation of Training Examples for Vehicle License Plate Recognition**\n\n[https://arxiv.org/abs/1707.03124](https://arxiv.org/abs/1707.03124)\n\n**Towards End-to-End Car License Plates Detection and Recognition with Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1709.08828](https://arxiv.org/abs/1709.08828)\n\n**Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline**\n\n- paper: [http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf](http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenbo_Xu_Towards_End-to-End_License_ECCV_2018_paper.pdf)\n- github: [https://github.com/detectRecog/CCPD](https://github.com/detectRecog/CCPD)\n- dataset: [https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view](https://drive.google.com/file/d/1fFqCXjhk7vE9yLklpJurEwP9vdLZmrJd/view)\n\n**High Accuracy Chinese Plate Recognition Framework**\n\n- intro: 基于深度学习高性能中文车牌识别 High Performance Chinese License Plate Recognition Framework.\n- gihtub: [https://github.com/zeusees/HyperLPR](https://github.com/zeusees/HyperLPR)\n\n**LPRNet: License Plate Recognition via Deep Neural Networks**\n\n- intrp=o: Intel IOTG Computer Vision Group\n- intro: works in real-time with recognition accuracy up to 95% for Chinese license plates: \n3 ms/plate on nVIDIAR GeForceTMGTX 1080 and 1.3 ms/plate on IntelR CoreTMi7-6700K CPU.\n- arxiv: [https://arxiv.org/abs/1806.10447](https://arxiv.org/abs/1806.10447)\n\n**How many labeled license plates are needed?**\n\n- intro: Chinese Conference on Pattern Recognition and Computer Vision\n- arxiv: [https://arxiv.org/abs/1808.08410](https://arxiv.org/abs/1808.08410)\n\n**An End-to-End Neural Network for Multi-line License Plate Recognition**\n\n- intro: ICPR 2018\n- paper: [https://sci-hub.se/10.1109/ICPR.2018.8546200#](https://sci-hub.se/10.1109/ICPR.2018.8546200#)\n- github: [https://github.com/deeplearningshare/multi-line-plate-recognition](https://github.com/deeplearningshare/multi-line-plate-recognition)\n\n# Blogs\n\n**Applying OCR Technology for Receipt Recognition**\n\n![](http://rnd.azoft.com/wp-content/uploads/2016/04/applying-ocr-technology-for-receipt-recognition.png)\n\n- blog: [http://rnd.azoft.com/applying-ocr-technology-receipt-recognition/](http://rnd.azoft.com/applying-ocr-technology-receipt-recognition/)\n- mirror: [http://pan.baidu.com/s/1qXQBQiC](http://pan.baidu.com/s/1qXQBQiC)\n\n**Hacking MNIST in 30 lines of Python**\n\n- blog: [http://jrusev.github.io/post/hacking-mnist/](http://jrusev.github.io/post/hacking-mnist/)\n- github: [https://github.com/jrusev/simple-neural-networks](https://github.com/jrusev/simple-neural-networks)\n\n**Optical Character Recognition Using One-Shot Learning, RNN, and TensorFlow**\n\n[https://blog.altoros.com/optical-character-recognition-using-one-shot-learning-rnn-and-tensorflow.html](https://blog.altoros.com/optical-character-recognition-using-one-shot-learning-rnn-and-tensorflow.html)\n\n**Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning**\n\n[https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/](https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/)\n\n# Projects\n\n**ocropy: Python-based tools for document analysis and OCR**\n\n- github: [https://github.com/tmbdev/ocropy](https://github.com/tmbdev/ocropy)\n\n**Extracting text from an image using Ocropus**\n\n- blog: [http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html](http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html)\n\n**CLSTM : A small C++ implementation of LSTM networks, focused on OCR**\n\n- github: [https://github.com/tmbdev/clstm](https://github.com/tmbdev/clstm)\n\n**OCR text recognition using tensorflow with attention**\n\n- github: [https://github.com/pannous/caffe-ocr](https://github.com/pannous/caffe-ocr)\n- github: [https://github.com/pannous/tensorflow-ocr](https://github.com/pannous/tensorflow-ocr)\n\n**Digit Recognition via CNN: digital meter numbers detection**\n\n![](https://camo.githubusercontent.com/bbdd1924ed88618f11cfe2f302a624591122d666/687474703a2f2f37786e37777a2e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f64696769742e6a7067)\n\n- github(caffe): [https://github.com/SHUCV/digit](https://github.com/SHUCV/digit)\n\n**Attention-OCR: Visual Attention based OCR**\n\n![](https://camo.githubusercontent.com/70902af6f701308b84854eb5f2f46f0729288a84/687474703a2f2f63732e636d752e6564752f25374579756e7469616e642f4f43522d322e6a7067)\n\n- github: [https://github.com/da03/Attention-OCR](https://github.com/da03/Attention-OCR)\n\n**umaru: An OCR-system based on torch using the technique of LSTM/GRU-RNN, CTC and referred to the works of rnnlib and clstm**\n\n- github: [https://github.com/edward-zhu/umaru](https://github.com/edward-zhu/umaru)\n\n**Tesseract.js: Pure Javascript OCR for 62 Languages**\n\n![](https://raw.githubusercontent.com/naptha/tesseract.js/master/demo.gif)\n\n- homepage: [http://tesseract.projectnaptha.com/](http://tesseract.projectnaptha.com/)\n- github: [https://github.com/naptha/tesseract.js](https://github.com/naptha/tesseract.js)\n\n**DeepHCCR: Offline Handwritten Chinese Character Recognition based on GoogLeNet and AlexNet (With CaffeModel)**\n\n- github: [https://github.com/chongyangtao/DeepHCCR](https://github.com/chongyangtao/DeepHCCR)\n\n**deep ocr: make a better chinese character recognition OCR than tesseract**\n\n[https://github.com/JinpengLI/deep_ocr](https://github.com/JinpengLI/deep_ocr)\n\n**Practical Deep OCR for scene text using CTPN + CRNN**\n\n[https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/blob/master/notebooks/OCR/readme.md](https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/blob/master/notebooks/OCR/readme.md)\n\n**Tensorflow-based CNN+LSTM trained with CTC-loss for OCR**\n\n[https://github.com//weinman/cnn_lstm_ctc_ocr](https://github.com//weinman/cnn_lstm_ctc_ocr)\n\n**SSD_scene-text-detection**\n\n- github: [https://github.com//chenxinpeng/SSD_scene_text_detection](https://github.com//chenxinpeng/SSD_scene_text_detection)\n- blog: [http://blog.csdn.net/u010167269/article/details/52563573](http://blog.csdn.net/u010167269/article/details/52563573)\n\n# Videos\n\n**LSTMs for OCR**\n\n- youtube: [https://www.youtube.com/watch?v=5vW8faXvnrc](https://www.youtube.com/watch?v=5vW8faXvnrc)\n\n# Resources\n\n**Deep Learning for OCR**\n\n[https://github.com/hs105/Deep-Learning-for-OCR](https://github.com/hs105/Deep-Learning-for-OCR)\n\n**Scene Text Localization & Recognition Resources**\n\n- intro: A curated list of resources dedicated to scene text localization and recognition\n- github: [https://github.com/chongyangtao/Awesome-Scene-Text-Recognition](https://github.com/chongyangtao/Awesome-Scene-Text-Recognition)\n\n**Scene Text Localization & Recognition Resources**\n\n- intro: 图像文本位置感知与识别的论文资源汇总\n- github: [https://github.com/whitelok/image-text-localization-recognition/blob/master/README.zh-cn.md](https://github.com/whitelok/image-text-localization-recognition/blob/master/README.zh-cn.md)\n\n**awesome-ocr: A curated list of promising OCR resources**\n\n[https://github.com/wanghaisheng/awesome-ocr](https://github.com/wanghaisheng/awesome-ocr)\n","excerpt":"Papers Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks intro: Google. Ian J. Goodfellow ar…","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Content","url":"/Commercial/Value Accounting Initiatives/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Background","url":"/Core Services/Webizen Socio-Economics/The Values Project/","items":[]},{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"HyperMedia Library","url":"/HyperMedia Library/","items":[]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"OVERVIEW","url":"/Implementation V1/Networking Considerations/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations → Principles → The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]},{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between ‘privacy’ and ‘dignity’.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"awesomeLists","url":"","items":[{"title":"Awesome Computer Vision: Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-computer-vision/","items":[]},{"title":"Awesome Natural Language Generation Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awsome-nl-gen/","items":[]},{"title":"Awesome Semantic Web Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-semweb/","items":[]},{"title":"Awesome-General","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-general/","items":[]}]},{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]},{"title":"EXECUTIVE SUMMARY","url":"/old-work-archives/2018 - Web Civics BizPlan/","items":[]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Overview","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Vision","url":"/Webizen 2.0/Webizen Vision/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/Webizen 2.0/Webizen Pro Summary/","title":"Overview","lastUpdatedAt":"2022-12-28T22:16:43.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Core Services/Webizen Socio-Economics/The Values Project/","title":"Background","lastUpdatedAt":"2022-12-28T22:01:50.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018 - Web Civics BizPlan/","title":"EXECUTIVE SUMMARY","lastUpdatedAt":"2022-12-28T22:01:50.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Implementation V1/Networking Considerations/","title":"OVERVIEW","lastUpdatedAt":"2022-12-28T21:49:13.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/HyperMedia Library/","title":"HyperMedia Library","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/","title":"Webizen V1 Project Documentation","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Commercial/Value Accounting Initiatives/","title":"Content","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Core Services/Permissive Commons/","title":"Permissive Commons","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Webizen 2.0/Webizen Vision/","title":"Webizen Vision","lastUpdatedAt":"2022-12-28T21:45:12.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","title":"Knowledge Banking: A Technical Architecture Summary","lastUpdatedAt":"2022-12-28T20:36:06.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}