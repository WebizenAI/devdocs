{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/",
    "result": {"data":{"mdx":{"id":"c700e624-8a8b-5860-8c87-a928bcaf5c69","tableOfContents":{"items":[{"url":"#applications","title":"Applications"},{"url":"#boundary--edge--contour-detection","title":"Boundary / Edge / Contour Detection"},{"url":"#image-processing","title":"Image Processing"},{"url":"#image-text","title":"Image-Text"},{"url":"#age-estimation","title":"Age Estimation"},{"url":"#face-aging","title":"Face Aging"},{"url":"#emotion-recognition--expression-recognition","title":"Emotion Recognition / Expression Recognition"},{"url":"#attribution-prediction","title":"Attribution Prediction"},{"url":"#place-recognition","title":"Place Recognition","items":[{"url":"#camera-relocalization","title":"Camera Relocalization"}]},{"url":"#activity-recognition","title":"Activity Recognition"},{"url":"#music-classification--sound-classification","title":"Music Classification / Sound Classification"},{"url":"#nsfw-detection--classification","title":"NSFW Detection / Classification"},{"url":"#image-reconstruction--inpainting","title":"Image Reconstruction / Inpainting"},{"url":"#image-restoration","title":"Image Restoration","items":[{"url":"#face-completion","title":"Face Completion"}]},{"url":"#image-denoising","title":"Image Denoising"},{"url":"#image-dehazing--image-haze-removal","title":"Image Dehazing / Image Haze Removal"},{"url":"#image-rain-removal--de-raining","title":"Image Rain Removal / De-raining"},{"url":"#fence-removal","title":"Fence Removal"},{"url":"#snow-removal","title":"Snow Removal"},{"url":"#blur-detection-and-removal","title":"Blur Detection and Removal"},{"url":"#image-compression","title":"Image Compression"},{"url":"#image-quality-assessment","title":"Image Quality Assessment"},{"url":"#image-blending","title":"Image Blending"},{"url":"#image-enhancement","title":"Image Enhancement"},{"url":"#abnormality-detection--anomaly-detection","title":"Abnormality Detection / Anomaly Detection"},{"url":"#depth-prediction--depth-estimation","title":"Depth Prediction / Depth Estimation"},{"url":"#texture-synthesis","title":"Texture Synthesis"},{"url":"#image-cropping","title":"Image Cropping"},{"url":"#image-synthesis","title":"Image Synthesis"},{"url":"#image-tagging","title":"Image Tagging"},{"url":"#image-matching","title":"Image Matching"},{"url":"#image-editing","title":"Image Editing","items":[{"url":"#face-swap--face-editing","title":"Face Swap & Face Editing"}]},{"url":"#stereo","title":"Stereo"},{"url":"#3d","title":"3D"},{"url":"#deep-learning-for-makeup","title":"Deep Learning for Makeup"},{"url":"#music-tagging","title":"Music Tagging"},{"url":"#action-recognition","title":"Action Recognition"},{"url":"#ctr-prediction","title":"CTR Prediction"},{"url":"#cryptography","title":"Cryptography"},{"url":"#cyber-security","title":"Cyber Security"},{"url":"#lip-reading","title":"Lip Reading"},{"url":"#event-recognition","title":"Event Recognition"},{"url":"#trajectory-prediction","title":"Trajectory Prediction"},{"url":"#human-object-interaction","title":"Human-Object Interaction"},{"url":"#deep-learning-in-finance","title":"Deep Learning in Finance"},{"url":"#deep-learning-in-speech","title":"Deep Learning in Speech","items":[{"url":"#wavenet","title":"WaveNet"}]},{"url":"#deep-learning-for-sound--music","title":"Deep Learning for Sound / Music","items":[{"url":"#sound","title":"Sound"},{"url":"#music","title":"Music"}]},{"url":"#deep-learning-in-medicine-and-biology","title":"Deep Learning in Medicine and Biology"},{"url":"#deep-learning-for-fashion","title":"Deep Learning for Fashion"},{"url":"#others","title":"Others"},{"url":"#blogs","title":"Blogs"},{"url":"#resources","title":"Resources"}]},"fields":{"title":"Deep Learning Applications","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications.md","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"Deep Learning Applications","description":null,"imageAlt":null,"tags":[],"date":"2015-10-09T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"deep_learning\",\n  \"title\": \"Deep Learning Applications\",\n  \"date\": \"2015-10-09T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"applications\"\n  }, \"Applications\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.02927\"\n  }, \"http://arxiv.org/abs/1510.02927\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Some like it hot - visual guidance for preference prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.07867\"\n  }, \"http://arxiv.org/abs/1510.07867\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://howhot.io/\"\n  }, \"http://howhot.io/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.03131\"\n  }, \"http://arxiv.org/abs/1512.03131\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Relative Attributes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.04103\"\n  }, \"http://arxiv.org/abs/1512.04103\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yassersouri/ghiaseddin\"\n  }, \"https://github.com/yassersouri/ghiaseddin\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep-Spying: Spying using Smartwatch and Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.05616\"\n  }, \"http://arxiv.org/abs/1512.05616\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tonybeltramelli/Deep-Spying\"\n  }, \"https://github.com/tonybeltramelli/Deep-Spying\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Camera identification with deep convolutional networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"key word: copyright infringement cases, ownership attribution\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.01068\"\n  }, \"http://arxiv.org/abs/1603.01068\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Analysis of Deep Neural Network Models for Practical Applications\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.07678\"\n  }, \"http://arxiv.org/abs/1605.07678\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"8 Inspirational Applications of Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Colorization of Black and White Images, Adding Sounds To Silent Movies, Automatic Machine Translation\\nObject Classification in Photographs, Automatic Handwriting Generation, Character Text Generation,\\nImage Caption Generation, Automatic Game Playing\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://machinelearningmastery.com/inspirational-applications-deep-learning/\"\n  }, \"http://machinelearningmastery.com/inspirational-applications-deep-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"16 Open Source Deep Learning Models Running as Microservices\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Places 365 Classifier, Deep Face Recognition, Real Estate Classifier, Colorful Image Colorization,\\nIllustration Tagger, InceptionNet, Parsey McParseface, ArtsyNetworks\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.algorithmia.com/2016/07/open-source-deep-learning-algorithm-roundup/\"\n  }, \"http://blog.algorithmia.com/2016/07/open-source-deep-learning-algorithm-roundup/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Cascaded Bi-Network for Face Hallucination\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mmlab.ie.cuhk.edu.hk/projects/CBN.html\"\n  }, \"http://mmlab.ie.cuhk.edu.hk/projects/CBN.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.05046\"\n  }, \"http://arxiv.org/abs/1607.05046\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation\")), mdx(\"img\", {\n    \"src\": \"http://sites.skoltech.ru/compvision/projects/deepwarp/images/pipeline.svg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://yaroslav.ganin.net/static/deepwarp/\"\n  }, \"http://yaroslav.ganin.net/static/deepwarp/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.07215\"\n  }, \"http://arxiv.org/abs/1607.07215\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Autoencoding Blade Runner\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.9kckqg7cq\"\n  }, \"https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.9kckqg7cq\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/terrybroad/Learned-Sim-Autoencoder-For-Video-Frames\"\n  }, \"https://github.com/terrybroad/Learned-Sim-Autoencoder-For-Video-Frames\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A guy trained a machine to \\\"watch\\\" Blade Runner. Then things got seriously sci-fi.\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.vox.com/2016/6/1/11787262/blade-runner-neural-network-encoding\"\n  }, \"http://www.vox.com/2016/6/1/11787262/blade-runner-neural-network-encoding\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Convolution Networks for Compression Artifacts Reduction\")), mdx(\"img\", {\n    \"src\": \"http://mmlab.ie.cuhk.edu.hk/projects/ARCNN/img/fig1.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page(code): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html\"\n  }, \"http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.02778\"\n  }, \"http://arxiv.org/abs/1608.02778\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep GDashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Deep Genomic Dashboard (Deep GDashboard)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.03644\"\n  }, \"http://arxiv.org/abs/1608.03644\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Instagram photos reveal predictive markers of depression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.03282\"\n  }, \"http://arxiv.org/abs/1608.03282\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How an Algorithm Learned to Identify Depressed Individuals by Studying Their Instagram Photos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"review: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/\"\n  }, \"https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"IM2CAD\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.05137\"\n  }, \"http://arxiv.org/abs/1608.05137\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/melicher\"\n  }, \"https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/melicher\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cupslab/neural_network_cracking\"\n  }, \"https://github.com/cupslab/neural_network_cracking\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Defeating Image Obfuscation with Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.00408\"\n  }, \"http://arxiv.org/abs/1609.00408\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Detecting Music BPM using Neural Networks\")), mdx(\"img\", {\n    \"src\": \"https://nlml.github.io/images/convnet_diagram.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: BPM (Beats Per Minutes)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/\"\n  }, \"https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/nlml/bpm\"\n  }, \"https://github.com/nlml/bpm\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generative Visual Manipulation on the Natural Image Manifold\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/junyanz/iGAN/master/pics/demo_teaser.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://people.eecs.berkeley.edu/~junyanz/projects/gvm/\"\n  }, \"https://people.eecs.berkeley.edu/~junyanz/projects/gvm/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.03552\"\n  }, \"http://arxiv.org/abs/1609.03552\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/junyanz/iGAN\"\n  }, \"https://github.com/junyanz/iGAN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Impression: Audiovisual Deep Residual Networks for Multimodal Apparent Personality Trait Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.05119\"\n  }, \"http://arxiv.org/abs/1609.05119\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Gold: Using Convolution Networks to Find Minerals\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df#.lgh95ub4a\"\n  }, \"https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df#.lgh95ub4a\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/scottvallance/DeepGold\"\n  }, \"https://github.com/scottvallance/DeepGold\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Predicting First Impressions with Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.08119\"\n  }, \"https://arxiv.org/abs/1610.08119\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Judging a Book By its Cover\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.09204\"\n  }, \"https://arxiv.org/abs/1610.09204\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"review: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.technologyreview.com/s/602807/deep-neural-network-learns-to-judge-books-by-their-covers/\"\n  }, \"https://www.technologyreview.com/s/602807/deep-neural-network-learns-to-judge-books-by-their-covers/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Credibility Analysis with Effective Domain Transferred Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05328\"\n  }, \"https://arxiv.org/abs/1611.05328\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A novel image tag completion method based on convolutional neural network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.arxiv.org/abs/1703.00586\"\n  }, \"https://www.arxiv.org/abs/1703.00586\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image operator learning coupled with CNN classification and its application to staff line removal\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICDAR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.06476\"\n  }, \"https://arxiv.org/abs/1709.06476\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joint Image Filtering with Deep Convolutional Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of California, Merced & Virginia Tech & University of Illinois\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vllab1.ucmerced.edu/~yli62/DJF_residual/\"\n  }, \"http://vllab1.ucmerced.edu/~yli62/DJF_residual/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.04200\"\n  }, \"https://arxiv.org/abs/1710.04200\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Yijunmaverick/DeepJointFilter\"\n  }, \"https://github.com/Yijunmaverick/DeepJointFilter\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.02470\"\n  }, \"https://arxiv.org/abs/1704.02470\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/aiff22/DPED\"\n  }, \"https://github.com/aiff22/DPED\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Scene De-rendering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://nsd.csail.mit.edu/\"\n  }, \"http://nsd.csail.mit.edu/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf\"\n  }, \"http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gihtub: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jiajunwu/nsd\"\n  }, \"https://github.com/jiajunwu/nsd\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image2GIF: Generating Cinemagraphs using Recurrent Deep Q-Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://bvision11.cs.unc.edu/bigpen/yipin/WACV2018/\"\n  }, \"http://bvision11.cs.unc.edu/bigpen/yipin/WACV2018/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.09042\"\n  }, \"https://arxiv.org/abs/1801.09042\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Neural Networks In Fully Connected CRF For Image Labeling With Social Network Metadata\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.09108\"\n  }, \"https://arxiv.org/abs/1801.09108\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single Image Reflection Removal Using Deep Encoder-Decoder Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.00094\"\n  }, \"https://arxiv.org/abs/1802.00094\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CRRN: Multi-Scale Guided Concurrent Reflection Removal Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.11802\"\n  }, \"https://arxiv.org/abs/1805.11802\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Deep Convolutional Networks for Demosaicing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.03769\"\n  }, \"https://arxiv.org/abs/1802.03769\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fully convolutional watermark removal attack\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/marcbelmont/cnn-watermark-removal\"\n  }, \"https://github.com/marcbelmont/cnn-watermark-removal\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.10562\"\n  }, \"https://arxiv.org/abs/1803.10562\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Prinsphield/ELEGANT\"\n  }, \"https://github.com/Prinsphield/ELEGANT\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to See in the Dark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.engr.illinois.edu/~cchen156/SID.html\"\n  }, \"http://web.engr.illinois.edu/~cchen156/SID.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.01934\"\n  }, \"https://arxiv.org/abs/1805.01934\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cchen156/Learning-to-See-in-the-Dark\"\n  }, \"https://github.com/cchen156/Learning-to-See-in-the-Dark\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=qWKUFK7MWvg&feature=youtu.be\"\n  }, \"https://www.youtube.com/watch?v=qWKUFK7MWvg&feature=youtu.be\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.bilibili.com/video/av23195280/\"\n  }, \"https://www.bilibili.com/video/av23195280/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generative Smoke Removal\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1902.00311\"\n  }, \"https://arxiv.org/abs/1902.00311\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1903.10683\"\n  }, \"https://arxiv.org/abs/1903.10683\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Blind Visual Motif Removal from a Single Image\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.02756\"\n  }, \"https://arxiv.org/abs/1904.02756\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Camera Simulators\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2104.05237\"\n  }, \"https://arxiv.org/abs/2104.05237\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Lighting the Darkness in the Deep Learning Era\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2104.10729\"\n  }, \"https://arxiv.org/abs/2104.10729\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open\"\n  }, \"https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open\"))), mdx(\"h1\", {\n    \"id\": \"boundary--edge--contour-detection\"\n  }, \"Boundary / Edge / Contour Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Holistically-Nested Edge Detection\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/da32e7e3275c2a9693dd2a6925b03a1151e2b098/687474703a2f2f70616765732e756373642e6564752f7e7a74752f6865642e6a7067\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2015, Marr Prize\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1504.06375\"\n  }, \"http://arxiv.org/abs/1504.06375\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/s9xie/hed\"\n  }, \"https://github.com/s9xie/hed\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/moabitcoin/holy-edge\"\n  }, \"https://github.com/moabitcoin/holy-edge\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Learning of Edges\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016. Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.04166\"\n  }, \"http://arxiv.org/abs/1511.04166\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"zn-blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html\"\n  }, \"http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pushing the Boundaries of Boundary Detection using Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.07386\"\n  }, \"http://arxiv.org/abs/1511.07386\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Oriented Boundaries\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.02755\"\n  }, \"http://arxiv.org/abs/1608.02755\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.vision.ee.ethz.ch/~cvlsegmentation/\"\n  }, \"http://www.vision.ee.ethz.ch/~cvlsegmentation/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.04658\"\n  }, \"https://arxiv.org/abs/1701.04658\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kmaninis/COB\"\n  }, \"https://github.com/kmaninis/COB\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Richer Convolutional Features for Edge Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: richer convolutional features (RCF)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.02103\"\n  }, \"https://arxiv.org/abs/1612.02103\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yun-liu/rcf\"\n  }, \"https://github.com/yun-liu/rcf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Contour Detection from Deep Patch-level Boundary Prediction\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.03159\"\n  }, \"https://arxiv.org/abs/1705.03159\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CASENet: Deep Category-Aware Semantic Edge Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. CMU & Mitsubishi Electric Research Laboratories (MERL)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.09759\"\n  }, \"https://arxiv.org/abs/1705.09759\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.merl.com/research/license#CASENet\"\n  }, \"http://www.merl.com/research/license#CASENet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=BNE1hAP6Qho\"\n  }, \"https://www.youtube.com/watch?v=BNE1hAP6Qho\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.00524\"\n  }, \"https://arxiv.org/abs/1801.00524\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Crisp Boundaries: From Boundaries to Higher-level Tasks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.02439\"\n  }, \"https://arxiv.org/abs/1801.02439\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DOOBNet: Deep Object Occlusion Boundary Detection from an Image\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.03772\"\n  }, \"https://arxiv.org/abs/1806.03772\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamic Feature Fusion for Semantic Edge Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1902.09104\"\n  }, \"https://arxiv.org/abs/1902.09104\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EDTER: Edge Detection with Transformer\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2022\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2203.08566\"\n  }, \"https://arxiv.org/abs/2203.08566\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MengyangPu/EDTER\"\n  }, \"https://github.com/MengyangPu/EDTER\"))), mdx(\"h1\", {\n    \"id\": \"image-processing\"\n  }, \"Image Processing\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Image Processing with Fully-Convolutional Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017. Qifeng Chen (\\u9648\\u542F\\u5CF0)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cqf.io/ImageProcessing/\"\n  }, \"http://www.cqf.io/ImageProcessing/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.00643\"\n  }, \"https://arxiv.org/abs/1709.00643\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"supp: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://youtu.be/eQyfHgLx8Dc\"\n  }, \"https://youtu.be/eQyfHgLx8Dc\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/CQFIO/FastImageProcessing\"\n  }, \"https://github.com/CQFIO/FastImageProcessing\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepISP: Learning End-to-End Image Processing Pipeline\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.06724\"\n  }, \"https://arxiv.org/abs/1801.06724\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.04323\"\n  }, \"https://arxiv.org/abs/1811.04323\"))), mdx(\"h1\", {\n    \"id\": \"image-text\"\n  }, \"Image-Text\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Two-Branch Neural Networks for Image-Text Matching Tasks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1704.03470\"\n  }, \"https://arxiv.org/abs/1704.03470\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dual-Path Convolutional Image-Text Embedding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.05535\"\n  }, \"https://arxiv.org/abs/1711.05535\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//layumi/Image-Text-Embedding\"\n  }, \"https://github.com//layumi/Image-Text-Embedding\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Conditional Image-Text Embedding Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.08389\"\n  }, \"https://arxiv.org/abs/1711.08389\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.10485\"\n  }, \"https://arxiv.org/abs/1711.10485\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stacked Cross Attention for Image-Text Matching\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.08024\"\n  }, \"https://arxiv.org/abs/1803.08024\")), mdx(\"h1\", {\n    \"id\": \"age-estimation\"\n  }, \"Age Estimation\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deeply-Learned Feature for Age Estimation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7045931&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7045931\"\n  }, \"http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7045931&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7045931\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Age and Gender Classification using Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf\"\n  }, \"http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.openu.ac.il/home/hassner/projects/cnn_agegender/\"\n  }, \"http://www.openu.ac.il/home/hassner/projects/cnn_agegender/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/GilLevi/AgeGenderDeepLearning\"\n  }, \"https://github.com/GilLevi/AgeGenderDeepLearning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Group-Aware Deep Feature Learning For Facial Age Estimation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.sciencedirect.com/science/article/pii/S0031320316303417\"\n  }, \"http://www.sciencedirect.com/science/article/pii/S0031320316303417\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Local Deep Neural Networks for Age and Gender Classification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.08497\"\n  }, \"https://arxiv.org/abs/1703.08497\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Understanding and Comparing Deep Neural Networks for Age and Gender Classification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.07689\"\n  }, \"https://arxiv.org/abs/1708.07689\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Age Group and Gender Estimation in the Wild with Deep RoR Architecture\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IEEE ACCESS\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.02985\"\n  }, \"https://arxiv.org/abs/1710.02985\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Age and gender estimation based on Convolutional Neural Network and TensorFlow\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/BoyuanJiang/Age-Gender-Estimate-TF\"\n  }, \"https://github.com/BoyuanJiang/Age-Gender-Estimate-TF\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Regression Forests for Age Estimation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Shanghai University & Johns Hopkins University & Nankai University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.07195\"\n  }, \"https://arxiv.org/abs/1712.07195\"))), mdx(\"h1\", {\n    \"id\": \"face-aging\"\n  }, \"Face Aging\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent Face Aging\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf/\"\n  }, \"www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Face Aging With Conditional Generative Adversarial Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.01983\"\n  }, \"https://arxiv.org/abs/1702.01983\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Face Age Progression: A Pyramid Architecture of GANs\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.10352\"\n  }, \"https://arxiv.org/abs/1711.10352\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Face Aging with Contextual Generative Adversarial Nets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACM Multimedia 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.00237\"\n  }, \"https://arxiv.org/abs/1802.00237\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recursive Chaining of Reversible Image-to-image Translators For Face Aging\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.05023\"\n  }, \"https://arxiv.org/abs/1802.05023\")), mdx(\"h1\", {\n    \"id\": \"emotion-recognition--expression-recognition\"\n  }, \"Emotion Recognition / Expression Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-time emotion recognition for gaming using deep convolutional network features\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1408.3750v1\"\n  }, \"http://arxiv.org/abs/1408.3750v1\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Zebreu/ConvolutionalEmotion\"\n  }, \"https://github.com/Zebreu/ConvolutionalEmotion\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.openu.ac.il/home/hassner/projects/cnn_emotions/\"\n  }, \"http://www.openu.ac.il/home/hassner/projects/cnn_emotions/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf\"\n  }, \"http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/GilLevi/54aee1b8b0397721aa4b\"\n  }, \"https://gist.github.com/GilLevi/54aee1b8b0397721aa4b\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gilscvblog.com/2017/01/31/emotion-recognition-in-the-wild-via-convolutional-neural-networks-and-mapped-binary-patterns/\"\n  }, \"https://gilscvblog.com/2017/01/31/emotion-recognition-in-the-wild-via-convolutional-neural-networks-and-mapped-binary-patterns/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeXpression: Deep Convolutional Neural Network for Expression Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.05371\"\n  }, \"http://arxiv.org/abs/1509.05371\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DEX: Deep EXpectation of apparent age from a single image\")), mdx(\"img\", {\n    \"src\": \"https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/img/pipeline.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.vision.ee.ethz.ch/en/publications/papers/proceedings/eth_biwi_01229.pdf\"\n  }, \"https://www.vision.ee.ethz.ch/en/publications/papers/proceedings/eth_biwi_01229.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/\"\n  }, \"https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EmotioNet: EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cbcsl.ece.ohio-state.edu/cvpr16.pdf\"\n  }, \"http://cbcsl.ece.ohio-state.edu/cvpr16.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"database: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html\"\n  }, \"http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How Deep Neural Networks Can Improve Emotion Recognition on Video Data\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICIP 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.07377\"\n  }, \"http://arxiv.org/abs/1602.07377\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Peak-Piloted Deep Network for Facial Expression Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.06997\"\n  }, \"http://arxiv.org/abs/1607.06997\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.01041\"\n  }, \"http://arxiv.org/abs/1608.01041\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Recursive Framework for Expression Recognition: From Web Images to Deep Models to Game Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.01647\"\n  }, \"http://arxiv.org/abs/1608.01647\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.06591\"\n  }, \"http://arxiv.org/abs/1609.06591\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EmotionNet Challenge\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homrepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html\"\n  }, \"http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"dataset: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html\"\n  }, \"http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Baseline CNN structure analysis for facial expression recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: RO-MAN2016 Conference\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.04251\"\n  }, \"https://arxiv.org/abs/1611.04251\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Facial Expression Recognition using Convolutional Neural Networks: State of the Art\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.02903\"\n  }, \"https://arxiv.org/abs/1612.02903\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.04280\"\n  }, \"https://arxiv.org/abs/1702.04280\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"api: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.sighthound.com/products/cloud\"\n  }, \"https://www.sighthound.com/products/cloud\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep generative-contrastive networks for facial expression recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.07140\"\n  }, \"https://arxiv.org/abs/1703.07140\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Neural Networks for Facial Expression Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1704.06756\"\n  }, \"https://arxiv.org/abs/1704.06756\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Multimodal Emotion Recognition using Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Imperial College London\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.08619\"\n  }, \"https://arxiv.org/abs/1704.08619\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Spatial-Temporal Recurrent Neural Network for Emotion Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.04515\"\n  }, \"https://arxiv.org/abs/1705.04515\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.01509\"\n  }, \"https://arxiv.org/abs/1706.01509\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal Multimodal Fusion for Video Emotion Classification in the Wild\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.07200\"\n  }, \"https://arxiv.org/abs/1709.07200\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Island Loss for Learning Discriminative Features in Facial Expression Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1710.03144\"\n  }, \"https://arxiv.org/abs/1710.03144\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-time Convolutional Neural Networks for Emotion and Gender Classification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1710.07557\"\n  }, \"https://arxiv.org/abs/1710.07557\")), mdx(\"h1\", {\n    \"id\": \"attribution-prediction\"\n  }, \"Attribution Prediction\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PANDA: Pose Aligned Networks for Deep Attribute Modeling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook. CVPR 2014\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1311.5591\"\n  }, \"http://arxiv.org/abs/1311.5591\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/facebook/pose-aligned-deep-networks\"\n  }, \"https://github.com/facebook/pose-aligned-deep-networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Predicting psychological attributions from face photographs with a deep neural network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.01289\"\n  }, \"http://arxiv.org/abs/1512.01289\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Human Identity from Motion Patterns\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.03908\"\n  }, \"http://arxiv.org/abs/1511.03908\"))), mdx(\"h1\", {\n    \"id\": \"place-recognition\"\n  }, \"Place Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"NetVLAD: CNN architecture for weakly supervised place recognition\")), mdx(\"img\", {\n    \"src\": \"http://www.di.ens.fr/willow/research/netvlad/images/teaser.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Street View Time Machine, soft-assignment, Weakly supervised triplet ranking loss\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.di.ens.fr/willow/research/netvlad/\"\n  }, \"http://www.di.ens.fr/willow/research/netvlad/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.07247\"\n  }, \"http://arxiv.org/abs/1511.07247\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PlaNet - Photo Geolocation with Convolutional Neural Networks\")), mdx(\"img\", {\n    \"src\": \"https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=590&cx=0&cy=0&cw=928&ch=614\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.05314\"\n  }, \"http://arxiv.org/abs/1602.05314\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"review(\\\"Google Unveils Neural Network with \\u201CSuperhuman\\u201D Ability to Determine the Location of Almost Any Image\\\"): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/\"\n  }, \"https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(\\\"City-Recognition: CS231n Project for Winter 2016\\\"): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/dmakian/LittlePlaNet\"\n  }, \"https://github.com/dmakian/LittlePlaNet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wulfebw/LittlePlaNet-Models\"\n  }, \"https://github.com/wulfebw/LittlePlaNet-Models\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual place recognition using landmark distribution descriptors\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.04274\"\n  }, \"http://arxiv.org/abs/1608.04274\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Low-effort place recognition with WiFi fingerprints using deep learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.02049\"\n  }, \"https://arxiv.org/abs/1611.02049\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/aqibsaeed/Place-Recognition-using-Autoencoders-and-NN\"\n  }, \"https://github.com/aqibsaeed/Place-Recognition-using-Autoencoders-and-NN\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Keras): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning\"\n  }, \"https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Features at Scale for Visual Place Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICRA 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.05105\"\n  }, \"https://arxiv.org/abs/1701.05105\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Place recognition: An Overview of Vision Perspective\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1707.03470\"\n  }, \"https://arxiv.org/abs/1707.03470\")), mdx(\"h2\", {\n    \"id\": \"camera-relocalization\"\n  }, \"Camera Relocalization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.07427\"\n  }, \"http://arxiv.org/abs/1505.07427\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mi.eng.cam.ac.uk/projects/relocalisation/#results\"\n  }, \"http://mi.eng.cam.ac.uk/projects/relocalisation/#results\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/alexgkendall/caffe-posenet\"\n  }, \"https://github.com/alexgkendall/caffe-posenet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kentsommer/tensorflow-posenet\"\n  }, \"https://github.com/kentsommer/tensorflow-posenet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Modelling Uncertainty in Deep Learning for Camera Relocalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.05909\"\n  }, \"http://arxiv.org/abs/1509.05909\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Random Forests versus Neural Networks - What's Best for Camera Relocalization?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.05797\"\n  }, \"http://arxiv.org/abs/1609.05797\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Convolutional Neural Network for 6-DOF Image Localization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.02776\"\n  }, \"https://arxiv.org/abs/1611.02776\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DSAC - Differentiable RANSAC for Camera Localization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05705\"\n  }, \"https://arxiv.org/abs/1611.05705\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image-based Localization with Spatial LSTMs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.07890\"\n  }, \"https://arxiv.org/abs/1611.07890\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VidLoc: 6-DoF Video-Clip Relocalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.06521\"\n  }, \"https://arxiv.org/abs/1702.06521\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards CNN Map Compression for camera relocalisation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.arxiv.org/abs/1703.00845\"\n  }, \"https://www.arxiv.org/abs/1703.00845\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Aalto University & Indian Institute of Technology\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.09733\"\n  }, \"https://arxiv.org/abs/1707.09733\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MapNet: Geometry-Aware Learning of Maps for Camera Localization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Georgia Institute of Technology & NVIDIA\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.03342\"\n  }, \"https://arxiv.org/abs/1712.03342\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image-to-GPS Verification Through A Bottom-Up Pattern Matching Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.07288\"\n  }, \"https://arxiv.org/abs/1811.07288\")), mdx(\"h1\", {\n    \"id\": \"activity-recognition\"\n  }, \"Activity Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Implementing a CNN for Human Activity Recognition in Tensorflow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/\"\n  }, \"http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN\"\n  }, \"https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Concurrent Activity Recognition with Multimodal CNN-LSTM Structure\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.01638\"\n  }, \"https://arxiv.org/abs/1702.01638\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CERN: Confidence-Energy Recurrent Network for Group Activity Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.03058\"\n  }, \"https://arxiv.org/abs/1704.03058\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deploying Tensorflow model on Andorid device for Human Activity Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://aqibsaeed.github.io/2017-05-02-deploying-tensorflow-model-andorid-device-human-activity-recognition/\"\n  }, \"http://aqibsaeed.github.io/2017-05-02-deploying-tensorflow-model-andorid-device-human-activity-recognition/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/tree/master/ActivityRecognition\"\n  }, \"https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/tree/master/ActivityRecognition\"))), mdx(\"h1\", {\n    \"id\": \"music-classification--sound-classification\"\n  }, \"Music Classification / Sound Classification\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Explaining Deep Convolutional Neural Networks on Music Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.02444\"\n  }, \"http://arxiv.org/abs/1607.02444\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://keunwoochoi.wordpress.com/2015/12/09/ismir-2015-lbd-auralisation-of-deep-convolutional-neural-networks-listening-to-learned-features-auralization/\"\n  }, \"https://keunwoochoi.wordpress.com/2015/12/09/ismir-2015-lbd-auralisation-of-deep-convolutional-neural-networks-listening-to-learned-features-auralization/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://keunwoochoi.wordpress.com/2016/03/23/what-cnns-see-when-cnns-see-spectrograms/\"\n  }, \"https://keunwoochoi.wordpress.com/2016/03/23/what-cnns-see-when-cnns-see-spectrograms/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/keunwoochoi/Auralisation\"\n  }, \"https://github.com/keunwoochoi/Auralisation\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"audio samples: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://soundcloud.com/kchoi-research\"\n  }, \"https://soundcloud.com/kchoi-research\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html\"\n  }, \"http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.04363\"\n  }, \"http://arxiv.org/abs/1608.04363\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Recurrent Neural Networks for Music Classification\")), mdx(\"img\", {\n    \"src\": \"https://keunwoochoi.files.wordpress.com/2016/09/screen-shot-2016-09-14-at-20-38-27.png?w=1200\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.04243\"\n  }, \"http://arxiv.org/abs/1609.04243\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://keunwoochoi.wordpress.com/2016/09/15/paper-is-out-convolutional-recurrent-neural-networks-for-music-classification/\"\n  }, \"https://keunwoochoi.wordpress.com/2016/09/15/paper-is-out-convolutional-recurrent-neural-networks-for-music-classification/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/keunwoochoi/music-auto_tagging-keras\"\n  }, \"https://github.com/keunwoochoi/music-auto_tagging-keras\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CNN Architectures for Large-Scale Audio Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1609.09430\"\n  }, \"https://arxiv.org/abs/1609.09430\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=oAAo_r7ZT8U&feature=youtu.be\"\n  }, \"https://www.youtube.com/watch?v=oAAo_r7ZT8U&feature=youtu.be\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SoundNet: Learning Sound Representations from Unlabeled Video\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT. NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://projects.csail.mit.edu/soundnet/\"\n  }, \"http://projects.csail.mit.edu/soundnet/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.09001\"\n  }, \"https://arxiv.org/abs/1610.09001\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.mit.edu/vondrick/soundnet.pdf\"\n  }, \"http://web.mit.edu/vondrick/soundnet.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cvondrick/soundnet\"\n  }, \"https://github.com/cvondrick/soundnet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/eborboihuc/SoundNet-tensorflow\"\n  }, \"https://github.com/eborboihuc/SoundNet-tensorflow\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=yJCjVvIY4dU\"\n  }, \"https://www.youtube.com/watch?v=yJCjVvIY4dU\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning 'ahem' detector\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/worldofpiggy/deeplearning-ahem-detector\"\n  }, \"https://github.com/worldofpiggy/deeplearning-ahem-detector\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.google.com/presentation/d/1QXQEOiAMj0uF2_Gafr2bn-kMniUJAIM1PLTFm1mUops/edit#slide=id.g35f391192_00\"\n  }, \"https://docs.google.com/presentation/d/1QXQEOiAMj0uF2_Gafr2bn-kMniUJAIM1PLTFm1mUops/edit#slide=id.g35f391192_00\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1c2KGlwO\"\n  }, \"https://pan.baidu.com/s/1c2KGlwO\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GenreFromAudio: Finding the genre of a song with Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A pipeline to build a dataset from your own music library and use it to fill the missing genres\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/despoisj/DeepAudioClassification\"\n  }, \"https://github.com/despoisj/DeepAudioClassification\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.10667\"\n  }, \"https://arxiv.org/abs/1703.10667\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN\"\n  }, \"https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On the Robustness of Deep Convolutional Neural Networks for Music Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Queen Mary University of London & New York University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.02361\"\n  }, \"https://arxiv.org/abs/1706.02361\"))), mdx(\"h1\", {\n    \"id\": \"nsfw-detection--classification\"\n  }, \"NSFW Detection / Classification\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Nipple Detection using Convolutional Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"reddit: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.reddit.com/over18?dest=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F33n77s%2Fandroid_app_nipple_detection_using_convolutional%2F\"\n  }, \"https://www.reddit.com/over18?dest=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F33n77s%2Fandroid_app_nipple_detection_using_convolutional%2F\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Applying deep learning to classify pornographic images and videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.08899\"\n  }, \"http://arxiv.org/abs/1511.08899\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MODERATE, FILTER, OR CURATE ADULT CONTENT WITH CLARIFAI\\u2019S NSFW MODEL\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.clarifai.com/moderate-filter-or-curate-adult-content-with-clarifais-nsfw-model/#.VzVhM-yECZY\"\n  }, \"http://blog.clarifai.com/moderate-filter-or-curate-adult-content-with-clarifais-nsfw-model/#.VzVhM-yECZY\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"WHAT CONVOLUTIONAL NEURAL NETWORKS LOOK AT WHEN THEY SEE NUDITY\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity#.VzVh_-yECZY\"\n  }, \"http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity#.VzVh_-yECZY\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Open Sourcing a Deep Learning Solution for Detecting NSFW Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Yahoo\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for\"\n  }, \"https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yahoo/open_nsfw\"\n  }, \"https://github.com/yahoo/open_nsfw\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Miles Deep - AI Porn Video Editor\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Deep Learning Porn Video Classifier/Editor with Caffe\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ryanjay0/miles-deep\"\n  }, \"https://github.com/ryanjay0/miles-deep\"))), mdx(\"h1\", {\n    \"id\": \"image-reconstruction--inpainting\"\n  }, \"Image Reconstruction / Inpainting\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Context Encoders: Feature Learning by Inpainting\")), mdx(\"img\", {\n    \"src\": \"http://www.cs.berkeley.edu/~pathak/context_encoder/resources/result_fig.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Unsupervised Feature Learning by Image Inpainting using GANs\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.berkeley.edu/~pathak/context_encoder/\"\n  }, \"http://www.cs.berkeley.edu/~pathak/context_encoder/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1604.07379\"\n  }, \"https://arxiv.org/abs/1604.07379\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pathak22/context-encoder\"\n  }, \"https://github.com/pathak22/context-encoder\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/BoyuanJiang/context_encoder_pytorch\"\n  }, \"https://github.com/BoyuanJiang/context_encoder_pytorch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Image Inpainting with Perceptual and Contextual Losses\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Image Inpainting with Deep Generative Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Deep Convolutional Generative Adversarial Network (DCGAN)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.07539\"\n  }, \"http://arxiv.org/abs/1607.07539\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bamos/dcgan-completion.tensorflow\"\n  }, \"https://github.com/bamos/dcgan-completion.tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Southern California & Adobe Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.09969\"\n  }, \"https://arxiv.org/abs/1611.09969\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Face Image Reconstruction from Deep Templates\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.arxiv.org/abs/1703.00832\"\n  }, \"https://www.arxiv.org/abs/1703.00832\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning-Guided Image Reconstruction from Incomplete Data\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.00584\"\n  }, \"https://arxiv.org/abs/1709.00584\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Inpainting using Multi-Scale Feature Image Translation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.08590\"\n  }, \"https://arxiv.org/abs/1711.08590\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Inpainting for High-Resolution Textures using CNN Texture Synthesis\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.03111\"\n  }, \"https://arxiv.org/abs/1712.03111\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Context-Aware Semantic Inpainting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.07778\"\n  }, \"https://arxiv.org/abs/1712.07778\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Blind Image Inpainting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.09078\"\n  }, \"https://arxiv.org/abs/1712.09078\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Stacked Networks with Residual Polishing for Image Inpainting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.00289\"\n  }, \"https://arxiv.org/abs/1801.00289\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Light-weight pixel context encoders for image inpainting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.05585\"\n  }, \"https://arxiv.org/abs/1801.05585\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Structured Energy-Based Image Inpainting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.07939\"\n  }, \"https://arxiv.org/abs/1801.07939\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Shift-Net: Image Inpainting via Deep Feature Rearrangement\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.09392\"\n  }, \"https://arxiv.org/abs/1801.09392\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cascade context encoder for improved inpainting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.04033\"\n  }, \"https://arxiv.org/abs/1803.04033\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SPG-Net: Segmentation Prediction and Guidance Network for Image Inpainting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Southern California & Baidu Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.03356\"\n  }, \"https://arxiv.org/abs/1805.03356\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Free-Form Image Inpainting with Gated Convolution\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.03589\"\n  }, \"https://arxiv.org/abs/1806.03589\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Keras implementation of Image OutPainting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford CS230 project\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://cs230.stanford.edu/projects_spring_2018/posters/8265861.pdf\"\n  }, \"https://cs230.stanford.edu/projects_spring_2018/posters/8265861.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bendangnuksung/Image-OutPainting\"\n  }, \"https://github.com/bendangnuksung/Image-OutPainting\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Inpainting via Generative Multi-column Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.08771\"\n  }, \"https://arxiv.org/abs/1810.08771\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Inception Generative Network for Cognitive Image Inpainting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1812.01458\"\n  }, \"https://arxiv.org/abs/1812.01458\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Foreground-aware Image Inpainting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Rochester & University of Illinois at Urbana-Champaign & Adobe Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1901.05945\"\n  }, \"https://arxiv.org/abs/1901.05945\"))), mdx(\"h1\", {\n    \"id\": \"image-restoration\"\n  }, \"Image Restoration\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.09056\"\n  }, \"http://arxiv.org/abs/1603.09056\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.08921\"\n  }, \"http://arxiv.org/abs/1606.08921\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Completion with Deep Learning in TensorFlow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://bamos.github.io/2016/08/09/deep-completion/\"\n  }, \"http://bamos.github.io/2016/08/09/deep-completion/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deeply Aggregated Alternating Minimization for Image Restoration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.06508\"\n  }, \"https://arxiv.org/abs/1612.06508\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Seoul National University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.06190\"\n  }, \"https://arxiv.org/abs/1701.06190\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MemNet: A Persistent Memory Network for Image Restoration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017 (Spotlight presentation)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.02209\"\n  }, \"https://arxiv.org/abs/1708.02209\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tyshiwo/MemNet\"\n  }, \"https://github.com/tyshiwo/MemNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Mean-Shift Priors for Image Restoration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.03749\"\n  }, \"https://arxiv.org/abs/1709.03749\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"xUnit: Learning a Spatial Activation Function for Efficient Image Restoration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.06445\"\n  }, \"https://arxiv.org/abs/1711.06445\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kligvasser/xUnit\"\n  }, \"https://github.com/kligvasser/xUnit\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Image Prior\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Skolkovo Institute of Science and Technology & University of Oxford\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://dmitryulyanov.github.io/deep_image_prior\"\n  }, \"https://dmitryulyanov.github.io/deep_image_prior\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.10925\"\n  }, \"https://arxiv.org/abs/1711.10925\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sites.skoltech.ru/app/data/uploads/sites/25/2017/11/deep_image_prior.pdf\"\n  }, \"https://sites.skoltech.ru/app/data/uploads/sites/25/2017/11/deep_image_prior.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//DmitryUlyanov/deep-image-prior\"\n  }, \"https://github.com//DmitryUlyanov/deep-image-prior\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"reddit: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.reddit.com/r/MachineLearning/comments/7gls3j/r_deep_image_prior_deep_superresolution/\"\n  }, \"https://www.reddit.com/r/MachineLearning/comments/7gls3j/r_deep_image_prior_deep_superresolution/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MemNet: A Persistent Memory Network for Image Restoration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017 spotlight\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf\"\n  }, \"http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//tyshiwo/MemNet\"\n  }, \"https://github.com//tyshiwo/MemNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Denoising Prior Driven Deep Neural Network for Image Restoration\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.06756\"\n  }, \"https://arxiv.org/abs/1801.06756\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Globally and Locally Consistent Image Completion\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: SIGGRAPH 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/\"\n  }, \"http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf\"\n  }, \"http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/satoshiiizuka/siggraph2017_inpainting\"\n  }, \"https://github.com/satoshiiizuka/siggraph2017_inpainting\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/akmtn/pytorch-siggraph2017-inpainting\"\n  }, \"https://github.com/akmtn/pytorch-siggraph2017-inpainting\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-level Wavelet-CNN for Image Restoration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018 NTIRE Workshop\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.07071\"\n  }, \"https://arxiv.org/abs/1805.07071\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Non-Local Recurrent Network for Image Restoration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Illinois at Urbana-Champaign & The Chinese University of Hong Kong\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.02919\"\n  }, \"https://arxiv.org/abs/1806.02919\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Residual Non-local Attention Networks for Image Restoration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.10082\"\n  }, \"https://arxiv.org/abs/1903.10082\"))), mdx(\"h2\", {\n    \"id\": \"face-completion\"\n  }, \"Face Completion\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generative Face Completion\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.05838\"\n  }, \"https://arxiv.org/abs/1704.05838\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"High Resolution Face Completion with Multiple Controllable Attributes via Fully End-to-End Progressive Generative Adversarial Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: North Carolina State University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.07632\"\n  }, \"https://arxiv.org/abs/1801.07632\"))), mdx(\"h1\", {\n    \"id\": \"image-denoising\"\n  }, \"Image Denoising\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.03981\"\n  }, \"http://arxiv.org/abs/1608.03981\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cszn/DnCNN\"\n  }, \"https://github.com/cszn/DnCNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Medical image denoising using convolutional denoising autoencoders\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.04667\"\n  }, \"http://arxiv.org/abs/1608.04667\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rectifier Neural Network with a Dual-Pathway Architecture for Image Denoising\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.03024\"\n  }, \"http://arxiv.org/abs/1609.03024\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Non-Local Color Image Denoising with Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06757\"\n  }, \"https://arxiv.org/abs/1611.06757\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joint Visual Denoising and Classification using Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICIP 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.01075\"\n  }, \"https://arxiv.org/abs/1612.01075\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ganggit/jointmodel\"\n  }, \"https://github.com/ganggit/jointmodel\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Convolutional Denoising of Low-Light Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.01687\"\n  }, \"https://arxiv.org/abs/1701.01687\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Class Aware Denoising\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.01698\"\n  }, \"https://arxiv.org/abs/1701.01698\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Learning for Structured Prediction Energy Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Massachusetts & CMU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.05667\"\n  }, \"https://arxiv.org/abs/1703.05667\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Block-Matching Convolutional Neural Network for Image Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1704.00524\"\n  }, \"https://arxiv.org/abs/1704.00524\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.04284\"\n  }, \"https://arxiv.org/abs/1706.04284\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Wide Inference Network for Image Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"hmttps://arxiv.org/abs/1707.05414\"\n  }, \"https://arxiv.org/abs/1707.05414\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Pixel-Distribution Prior with Wider Convolution for Image Denoising\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.09135\"\n  }, \"https://arxiv.org/abs/1707.09135\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(MatConvNet): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cswin/WIN\"\n  }, \"https://github.com/cswin/WIN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Denoising via CNNs: An Adversarial Approach\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Indian Institute of Science\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.00159\"\n  }, \"https://arxiv.org/abs/1708.00159\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An ELU Network with Total Variation for Image Denoising\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 24th International Conference on Neural Information Processing (2017)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.04317\"\n  }, \"https://arxiv.org/abs/1708.04317\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dilated Residual Network for Image Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.05473\"\n  }, \"https://arxiv.org/abs/1708.05473\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.04026\"\n  }, \"https://arxiv.org/abs/1710.04026\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(MatConvNet): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cszn/FFDNet\"\n  }, \"https://github.com/cszn/FFDNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Universal Denoising Networks : A Novel CNN-based Network Architecture for Image Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.07807\"\n  }, \"https://arxiv.org/abs/1711.07807\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Burst Denoising with Kernel Prediction Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://people.eecs.berkeley.edu/~bmild/kpn/\"\n  }, \"http://people.eecs.berkeley.edu/~bmild/kpn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.02327\"\n  }, \"https://arxiv.org/abs/1712.02327\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Chaining Identity Mapping Modules for Image Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.02933\"\n  }, \"https://arxiv.org/abs/1712.02933\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Burst Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.05790\"\n  }, \"https://arxiv.org/abs/1712.05790\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast, Trainable, Multiscale Denoising\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.06130\"\n  }, \"https://arxiv.org/abs/1802.06130\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Deep Learning based Denoisers without Ground Truth Data\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.01314\"\n  }, \"https://arxiv.org/abs/1803.01314\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.05229\"\n  }, \"https://arxiv.org/abs/1806.05229\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Class-Aware Fully-Convolutional Gaussian and Poisson Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.06562\"\n  }, \"https://arxiv.org/abs/1808.06562\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Connecting Image Denoising and High-Level Vision Tasks via Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1809.01826\"\n  }, \"https://arxiv.org/abs/1809.01826\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Ding-Liu/DeepDenoising\"\n  }, \"https://github.com/Ding-Liu/DeepDenoising\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DN-ResNet: Efficient Deep Residual Network for Image Denoising\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.06766\"\n  }, \"https://arxiv.org/abs/1810.06766\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for Image Denoising: A Survey\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.05052\"\n  }, \"https://arxiv.org/abs/1810.05052\")), mdx(\"h1\", {\n    \"id\": \"image-dehazing--image-haze-removal\"\n  }, \"Image Dehazing / Image Haze Removal\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DehazeNet: An End-to-End System for Single Image Haze Removal\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1601.07661\"\n  }, \"http://arxiv.org/abs/1601.07661\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An All-in-One Network for Dehazing and Beyond\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: All-in-One Dehazing Network (AOD-Net)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.06543\"\n  }, \"https://arxiv.org/abs/1707.06543\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joint Transmission Map Estimation and Dehazing using Deep Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.00581\"\n  }, \"https://arxiv.org/abs/1708.00581\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End United Video Dehazing and Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.03919\"\n  }, \"https://arxiv.org/abs/1709.03919\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Dehazing using Bilinear Composition Loss Function\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1710.00279\"\n  }, \"https://arxiv.org/abs/1710.00279\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.06787\"\n  }, \"https://arxiv.org/abs/1711.06787\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CANDY: Conditional Adversarial Networks based Fully End-to-End System for Single Image Haze Removal\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.02892\"\n  }, \"https://arxiv.org/abs/1801.02892\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"C2MSNet: A Novel approach for single image haze removal\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.08406\"\n  }, \"https://arxiv.org/abs/1801.08406\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Cascaded Convolutional Neural Network for Single Image Dehazing\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IEEE ACCESS\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.07955\"\n  }, \"https://arxiv.org/abs/1803.07955\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Densely Connected Pyramid Dehazing Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.08396\"\n  }, \"https://arxiv.org/abs/1803.08396\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hezhangsprinter/DCPDN\"\n  }, \"https://github.com/hezhangsprinter/DCPDN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gated Fusion Network for Single Image Dehazing\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sites.google.com/site/renwenqi888/research/dehazing/gfn\"\n  }, \"https://sites.google.com/site/renwenqi888/research/dehazing/gfn\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.00213\"\n  }, \"https://arxiv.org/abs/1804.00213\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Single-Image Dehazing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.05624\"\n  }, \"https://arxiv.org/abs/1804.05624\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Perceptually Optimized Generative Adversarial Network for Single Image Dehazing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.01084\"\n  }, \"https://arxiv.org/abs/1805.01084\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PAD-Net: A Perception-Aided Single Image Dehazing Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.03146\"\n  }, \"https://arxiv.org/abs/1805.03146\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/guanlongzhao/single-image-dehazing\"\n  }, \"https://github.com/guanlongzhao/single-image-dehazing\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Effectiveness of Instance Normalization: a Strong Baseline for Single Image Dehazing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.03305\"\n  }, \"https://arxiv.org/abs/1805.03305\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPRW: NTIRE 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.05308\"\n  }, \"https://arxiv.org/abs/1805.05308\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep learning for dehazing: Comparison and analysis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVCS 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.10923\"\n  }, \"https://arxiv.org/abs/1806.10923\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generic Model-Agnostic Convolutional Neural Network for Single Image Dehazing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.02862\"\n  }, \"https://arxiv.org/abs/1810.02862\")), mdx(\"h1\", {\n    \"id\": \"image-rain-removal--de-raining\"\n  }, \"Image Rain Removal / De-raining\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Clearing the Skies: A deep network architecture for single-image rain removal\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DerainNet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://smartdsp.xmu.edu.cn/derainNet.html\"\n  }, \"http://smartdsp.xmu.edu.cn/derainNet.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.02087\"\n  }, \"http://arxiv.org/abs/1609.02087\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code(Matlab): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/derainNet/code.zip\"\n  }, \"http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/derainNet/code.zip\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joint Rain Detection and Removal via Iterative Region Dependent Multi-Task Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.07769\"\n  }, \"http://arxiv.org/abs/1609.07769\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image De-raining Using a Conditional Generative Adversarial Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.05957\"\n  }, \"https://arxiv.org/abs/1701.05957\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.06830\"\n  }, \"https://arxiv.org/abs/1712.06830\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep joint rain and haze removal from single images\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.06769\"\n  }, \"https://arxiv.org/abs/1801.06769\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Density-aware Single Image De-raining using a Multi-stream Dense Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.08396\"\n  }, \"https://arxiv.org/abs/1803.08396\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hezhangsprinter/DID-MDN\"\n  }, \"https://github.com/hezhangsprinter/DID-MDN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.10433\"\n  }, \"https://arxiv.org/abs/1803.10433\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Single Image Rain Removal via a Deep Decomposition-Composition Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.02688\"\n  }, \"https://arxiv.org/abs/1804.02688\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Residual-Guide Feature Fusion Network for Single Image Deraining\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.07493\"\n  }, \"https://arxiv.org/abs/1804.07493\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Lightweight Pyramid Networks for Image Deraining\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.06173\"\n  }, \"https://arxiv.org/abs/1805.06173\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.05698\"\n  }, \"https://arxiv.org/abs/1807.05698\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://xialipku.github.io/RESCAN/\"\n  }, \"https://xialipku.github.io/RESCAN/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACM Multimedia 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.01491\"\n  }, \"https://arxiv.org/abs/1808.01491\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gated Context Aggregation Network for Image Dehazing and Deraining\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.08747\"\n  }, \"https://arxiv.org/abs/1811.08747\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Deep Tree-Structured Fusion Model for Single Image Deraining\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.08632\"\n  }, \"https://arxiv.org/abs/1811.08632\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A^2Net: Adjacent Aggregation Networks for Image Raindrop Removal\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.09780\"\n  }, \"https://arxiv.org/abs/1811.09780\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single Image Deraining: A Comprehensive Benchmark Analysis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.08558\"\n  }, \"https://arxiv.org/abs/1903.08558\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/lsy17096535/Single-Image-Deraining\"\n  }, \"https://github.com/lsy17096535/Single-Image-Deraining\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project pge: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://stevewongv.github.io/derain-project.html\"\n  }, \"https://stevewongv.github.io/derain-project.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.01538\"\n  }, \"https://arxiv.org/abs/1904.01538\"))), mdx(\"h1\", {\n    \"id\": \"fence-removal\"\n  }, \"Fence Removal\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"My camera can see through fences: A deep learning approach for image de-fencing\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACPR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.07442\"\n  }, \"https://arxiv.org/abs/1805.07442\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep learning based fence segmentation and removal from an image using a video sequence\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV Workshop on Video Segmentation, 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.07727\"\n  }, \"http://arxiv.org/abs/1609.07727\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accurate and efficient video de-fencing using convolutional neural networks and temporal information\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.10781\"\n  }, \"https://arxiv.org/abs/1806.10781\")), mdx(\"h1\", {\n    \"id\": \"snow-removal\"\n  }, \"Snow Removal\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DesnowNet: Context-Aware Deep Network for Snow Removal\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.04512\"\n  }, \"https://arxiv.org/abs/1708.04512\")), mdx(\"h1\", {\n    \"id\": \"blur-detection-and-removal\"\n  }, \"Blur Detection and Removal\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Deblur\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1406.7444\"\n  }, \"http://arxiv.org/abs/1406.7444\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1503.00593\"\n  }, \"http://arxiv.org/abs/1503.00593\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Learning for Image Burst Deblurring\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.04433\"\n  }, \"http://arxiv.org/abs/1607.04433\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Video Deblurring\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017 spotlight paper\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page(code+dataset): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.ubc.ca/labs/imager/tr/2017/DeepVideoDeblurring/\"\n  }, \"http://www.cs.ubc.ca/labs/imager/tr/2017/DeepVideoDeblurring/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.08387\"\n  }, \"https://arxiv.org/abs/1611.08387\"), mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/shuochsu/DeepVideoDeblurring\"\n  }, \"https://github.com/shuochsu/DeepVideoDeblurring\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.02177\"\n  }, \"https://arxiv.org/abs/1612.02177\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official. Torch)): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/SeungjunNah/DeepDeblur_release\"\n  }, \"https://github.com/SeungjunNah/DeepDeblur_release\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.02583\"\n  }, \"https://arxiv.org/abs/1612.02583\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Motion Deblurring in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.01486\"\n  }, \"https://arxiv.org/abs/1701.01486\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Face Deblurring\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1704.08772\"\n  }, \"https://arxiv.org/abs/1704.08772\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Blind Motion Deblurring\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.04208\"\n  }, \"https://arxiv.org/abs/1708.04208\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Generative Filter for Motion Deblurring\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.03481\"\n  }, \"https://arxiv.org/abs/1709.03481\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Ukrainian Catholic University & CTU in Prague\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.07064\"\n  }, \"https://arxiv.org/abs/1711.07064\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/KupynOrest/DeblurGAN\"\n  }, \"https://github.com/KupynOrest/DeblurGAN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepDeblur: Fast one-step blurry face images restoration\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.09515\"\n  }, \"https://arxiv.org/abs/1711.09515\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reblur2Deblur: Deblurring Videos via Self-Supervised Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.05117\"\n  }, \"https://arxiv.org/abs/1801.05117\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"supplementary: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://drive.google.com/file/d/17Itta-z89lpWUdvUjpafKzJRSLoxHF5c/view\"\n  }, \"https://drive.google.com/file/d/17Itta-z89lpWUdvUjpafKzJRSLoxHF5c/view\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scale-recurrent Network for Deep Image Deblurring\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CUHK & Tecent & Megvii Inc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.01770\"\n  }, \"https://arxiv.org/abs/1802.01770\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Semantic Face Deblurring\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018. Beijing Institute of Technology & University of California, Merced & Nvidia Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur\"\n  }, \"https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.03345\"\n  }, \"https://arxiv.org/abs/1803.03345\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Motion deblurring of faces\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.03330\"\n  }, \"https://arxiv.org/abs/1803.03330\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning a Discriminative Prior for Blind Image Deblurring\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.03363\"\n  }, \"https://arxiv.org/abs/1803.03363\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adversarial Spatio-Temporal Learning for Video Deblurring\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.00533\"\n  }, \"https://arxiv.org/abs/1804.00533\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Deblur Images with Exemplars\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: PAMI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.05503\"\n  }, \"https://arxiv.org/abs/1805.05503\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networks for Non-Uniform Single Image Deblurring\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1903.10157\"\n  }, \"https://arxiv.org/abs/1903.10157\")), mdx(\"h1\", {\n    \"id\": \"image-compression\"\n  }, \"Image Compression\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An image compression and encryption scheme based on deep learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.05001\"\n  }, \"http://arxiv.org/abs/1608.05001\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Full Resolution Image Compression with Recurrent Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.05148\"\n  }, \"http://arxiv.org/abs/1608.05148\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tensorflow/models/tree/master/compression\"\n  }, \"https://github.com/tensorflow/models/tree/master/compression\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Compression with Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html\"\n  }, \"https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Lossy Image Compression With Compressive Autoencoders\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openreview.net/pdf?id=rJiNwv9gg\"\n  }, \"http://openreview.net/pdf?id=rJiNwv9gg\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"review: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://qz.com/835569/twitter-is-getting-close-to-making-all-your-pictures-just-a-little-bit-smaller/\"\n  }, \"http://qz.com/835569/twitter-is-getting-close-to-making-all-your-pictures-just-a-little-bit-smaller/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-end Optimized Image Compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01704\"\n  }, \"https://arxiv.org/abs/1611.01704\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://blog.acolyer.org/2017/05/08/end-to-end-optimized-image-compression/\"\n  }, \"https://blog.acolyer.org/2017/05/08/end-to-end-optimized-image-compression/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.07233\"\n  }, \"https://arxiv.org/abs/1611.07233\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Perceptual Image Compression using Deep Convolution Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Accepted to Data Compression Conference\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Semantic JPEG image compression using deep convolutional neural network (CNN)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.08712\"\n  }, \"https://arxiv.org/abs/1612.08712\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/iamaaditya/image-compression-cnn\"\n  }, \"https://github.com/iamaaditya/image-compression-cnn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generative Compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.01467\"\n  }, \"https://arxiv.org/abs/1703.01467\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.10114\"\n  }, \"https://arxiv.org/abs/1703.10114\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Convolutional Networks for Content-weighted Image Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.10553\"\n  }, \"https://arxiv.org/abs/1703.10553\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-Time Adaptive Image Compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: GAN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.wave.one/icml2017\"\n  }, \"http://www.wave.one/icml2017\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.05823\"\n  }, \"https://arxiv.org/abs/1705.05823\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Inpaint for Image Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.08855\"\n  }, \"https://arxiv.org/abs/1709.08855\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Efficient Trimmed Convolutional Arithmetic Encoding for Lossless Image Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.04662\"\n  }, \"https://arxiv.org/abs/1801.04662\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Conditional Probability Models for Deep Image Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.04260\"\n  }, \"https://arxiv.org/abs/1801.04260\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multiple Description Convolutional Neural Networks for Image Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.06611\"\n  }, \"https://arxiv.org/abs/1801.06611\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Near-lossless L-infinity constrained Multi-rate Image Decompression via Deep Neural Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.07987\"\n  }, \"https://arxiv.org/abs/1801.07987\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepSIC: Deep Semantic Image Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.09468\"\n  }, \"https://arxiv.org/abs/1801.09468\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Spatially adaptive image compression using a tiled deep network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICIP 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.02629\"\n  }, \"https://arxiv.org/abs/1802.02629\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.05787\"\n  }, \"https://arxiv.org/abs/1803.05787\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression Framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DAC 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.05788\"\n  }, \"https://arxiv.org/abs/1803.05788\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Effects of JPEG and JPEG2000 Compression on Attacks using Adversarial Examples\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.10418\"\n  }, \"https://arxiv.org/abs/1803.10418\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generative Adversarial Networks for Extreme Learned Image Compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ETH Zurich\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://data.vision.ee.ethz.ch/aeirikur/extremecompression/\"\n  }, \"https://data.vision.ee.ethz.ch/aeirikur/extremecompression/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.02958\"\n  }, \"https://arxiv.org/abs/1804.02958\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deformation Aware Image Compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.04593\"\n  }, \"https://arxiv.org/abs/1804.04593\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Multi-scale Image Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.06386\"\n  }, \"https://arxiv.org/abs/1805.06386\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Image Compression via End-to-End Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.01496\"\n  }, \"https://arxiv.org/abs/1806.01496\")), mdx(\"h1\", {\n    \"id\": \"image-quality-assessment\"\n  }, \"Image Quality Assessment\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.01697\"\n  }, \"https://arxiv.org/abs/1612.01697\"))), mdx(\"h1\", {\n    \"id\": \"image-blending\"\n  }, \"Image Blending\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GP-GAN: Towards Realistic High-Resolution Image Blending\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://wuhuikai.github.io/GP-GAN-Project/\"\n  }, \"https://wuhuikai.github.io/GP-GAN-Project/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.07195\"\n  }, \"https://arxiv.org/abs/1703.07195\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Official, Chainer): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wuhuikai/GP-GAN\"\n  }, \"https://github.com/wuhuikai/GP-GAN\"))), mdx(\"h1\", {\n    \"id\": \"image-enhancement\"\n  }, \"Image Enhancement\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Bilateral Learning for Real-Time Image Enhancement\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT & Google Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.02880\"\n  }, \"https://arxiv.org/abs/1707.02880\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Aesthetic-Driven Image Enhancement by Adversarial Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CUHK\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.05251\"\n  }, \"https://arxiv.org/abs/1707.05251\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learned Perceptual Image Enhancement\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.02864\"\n  }, \"https://arxiv.org/abs/1712.02864\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Underwater Image Enhancement\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1807.03528\"\n  }, \"https://arxiv.org/abs/1807.03528\")), mdx(\"h1\", {\n    \"id\": \"abnormality-detection--anomaly-detection\"\n  }, \"Abnormality Detection / Anomaly Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Toward a Taxonomy and Computational Models of Abnormalities in Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.01325\"\n  }, \"http://arxiv.org/abs/1512.01325\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.06725\"\n  }, \"https://arxiv.org/abs/1805.06725\")), mdx(\"h1\", {\n    \"id\": \"depth-prediction--depth-estimation\"\n  }, \"Depth Prediction / Depth Estimation\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Convolutional Neural Fields for Depth Estimation from a Single Image\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1411.6387\"\n  }, \"https://arxiv.org/abs/1411.6387\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IEEE T. Pattern Analysis and Machine Intelligence\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1502.07411\"\n  }, \"https://arxiv.org/abs/1502.07411\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"bitbucket: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://bitbucket.org/fayao/dcnf-fcsp\"\n  }, \"https://bitbucket.org/fayao/dcnf-fcsp\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1603.049921\"\n  }, \"https://arxiv.org/abs/1603.04992\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Ravi-Garg/Unsupervised_Depth_Estimation\"\n  }, \"https://github.com/Ravi-Garg/Unsupervised_Depth_Estimation\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project pag: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ttic.uchicago.edu/~ayanc/mdepth/\"\n  }, \"http://ttic.uchicago.edu/~ayanc/mdepth/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.07081\"\n  }, \"http://arxiv.org/abs/1605.07081\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ayanc/mdepth/\"\n  }, \"https://github.com/ayanc/mdepth/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deeper Depth Prediction with Fully Convolutional Residual Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.00373\"\n  }, \"https://arxiv.org/abs/1606.00373\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/iro-cp/FCRN-DepthPrediction\"\n  }, \"https://github.com/iro-cp/FCRN-DepthPrediction\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single image depth estimation by dilated deep residual convolutional neural network and soft-weight-sum inference\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.00534\"\n  }, \"https://arxiv.org/abs/1705.00534\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Monocular Depth Estimation with Hierarchical Fusion of Dilated CNNs and Soft-Weighted-Sum Inference\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Northwestern Polytechnical University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.02287\"\n  }, \"https://arxiv.org/abs/1708.02287\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.07492\"\n  }, \"https://arxiv.org/abs/1709.07492\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=vNIIT_M7x7Y\"\n  }, \"https://www.youtube.com/watch?v=vNIIT_M7x7Y\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/fangchangma/sparse-to-dense\"\n  }, \"https://github.com/fangchangma/sparse-to-dense\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Size-to-depth: A New Perspective for Single Image Depth Estimation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.04461\"\n  }, \"https://arxiv.org/abs/1801.04461\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.00150\"\n  }, \"https://arxiv.org/abs/1808.00150\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rethinking Monocular Depth Estimation with Adversarial Training\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.07528\"\n  }, \"https://arxiv.org/abs/1808.07528\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://webdiis.unizar.es/~jmfacil/camconvs/\"\n  }, \"http://webdiis.unizar.es/~jmfacil/camconvs/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.02028\"\n  }, \"https://arxiv.org/abs/1904.02028\"))), mdx(\"h1\", {\n    \"id\": \"texture-synthesis\"\n  }, \"Texture Synthesis\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Texture Synthesis Using Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.07376\"\n  }, \"http://arxiv.org/abs/1505.07376\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Texture Networks: Feed-forward Synthesis of Textures and Stylized Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IMCL 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.03417\"\n  }, \"http://arxiv.org/abs/1603.03417\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/DmitryUlyanov/texture_nets\"\n  }, \"https://github.com/DmitryUlyanov/texture_nets\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/\"\n  }, \"https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.04382\"\n  }, \"http://arxiv.org/abs/1604.04382\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Torch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chuanli11/MGANs\"\n  }, \"https://github.com/chuanli11/MGANs\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Texture Synthesis with Spatial Generative Adversarial Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.08207\"\n  }, \"https://arxiv.org/abs/1611.08207\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Skolkovo Institute of Science and Technology & Yandex & University of Oxford\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.02096\"\n  }, \"https://arxiv.org/abs/1701.02096\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep TEN: Texture Encoding Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://zhanghang1989.github.io/DeepEncoding/\"\n  }, \"http://zhanghang1989.github.io/DeepEncoding/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.02844\"\n  }, \"https://arxiv.org/abs/1612.02844\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhanghang1989/Deep-Encoding\"\n  }, \"https://github.com/zhanghang1989/Deep-Encoding\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://zhuanlan.zhihu.com/p/25013378\"\n  }, \"https://zhuanlan.zhihu.com/p/25013378\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Diversified Texture Synthesis with Feed-forward Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. University of California & Adobe Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.01664\"\n  }, \"https://arxiv.org/abs/1703.01664\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Yijunmaverick/MultiTextureSynthesis\"\n  }, \"https://github.com/Yijunmaverick/MultiTextureSynthesis\"))), mdx(\"h1\", {\n    \"id\": \"image-cropping\"\n  }, \"Image Cropping\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Cropping via Attention Box Prediction and Aesthetics Assessment\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.08014\"\n  }, \"https://arxiv.org/abs/1710.08014\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A2-RL: Aesthetics Aware Reinforcement Learning for Automatic Image Cropping\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://debangli.info/A2RL/\"\n  }, \"http://debangli.info/A2RL/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.04595\"\n  }, \"https://arxiv.org/abs/1709.04595\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wuhuikai/TF-A2RL\"\n  }, \"https://github.com/wuhuikai/TF-A2RL\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://wuhuikai.me/TF-A2RL/\"\n  }, \"http://wuhuikai.me/TF-A2RL/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Automatic Image Cropping for Visual Aesthetic Enhancement Using Deep Neural Networks and Cascaded Regression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IEEE Transactions on Multimedia, 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.09048\"\n  }, \"https://arxiv.org/abs/1712.09048\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Grid Anchor based Image Cropping: A New Benchmark and An Efficient Model\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.08989\"\n  }, \"https://arxiv.org/abs/1909.08989\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch\"\n  }, \"https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Cropping with Composition and Saliency Aware Aesthetic Score Map\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1911.10492\"\n  }, \"https://arxiv.org/abs/1911.10492\"))), mdx(\"h1\", {\n    \"id\": \"image-synthesis\"\n  }, \"Image Synthesis\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1601.04589\"\n  }, \"http://arxiv.org/abs/1601.04589\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generative Adversarial Text to Image Synthesis\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/1925e23b5b6e19efa60f45daa3787f1f4a098ef3/687474703a2f2f692e696d6775722e636f6d2f644e6c32486b5a2e6a7067\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.05396\"\n  }, \"http://arxiv.org/abs/1605.05396\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/paarthneekhara/text-to-image\"\n  }, \"https://github.com/paarthneekhara/text-to-image\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Rutgers University & Lehigh University & The Chinese University of Hong Kong & University of North Carolina at Charlotte\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.03242\"\n  }, \"https://arxiv.org/abs/1612.03242\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hanzhanggit/StackGAN\"\n  }, \"https://github.com/hanzhanggit/StackGAN\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/brangerbriz/docker-StackGAN\"\n  }, \"https://github.com/brangerbriz/docker-StackGAN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Image Synthesis via Adversarial Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.06873\"\n  }, \"https://arxiv.org/abs/1707.06873\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(PyTorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//woozzu/dong_iccv_2017\"\n  }, \"https://github.com//woozzu/dong_iccv_2017\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Introduction to Image Synthesis with Generative Adversarial Nets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Illinois at Chicago & Toutiao AI Lab\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.04469\"\n  }, \"https://arxiv.org/abs/1803.04469\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Text Guided Person Image Synthesis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intr: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Zhejiang University & Nanjing University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.05118\"\n  }, \"https://arxiv.org/abs/1904.05118\"))), mdx(\"h1\", {\n    \"id\": \"image-tagging\"\n  }, \"Image Tagging\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Zero-Shot Image Tagging\")), mdx(\"img\", {\n    \"src\": \"http://crcv.ucf.edu/projects/fastzeroshot/overview.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://crcv.ucf.edu/projects/fastzeroshot/\"\n  }, \"http://crcv.ucf.edu/projects/fastzeroshot/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Flexible Image Tagging with Fast0Tag\")), mdx(\"img\", {\n    \"src\": \"https://cdn-images-1.medium.com/max/800/1*SsIf1Bhe-G4HmN6DPDogmQ.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7\"\n  }, \"https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sampled Image Tagging and Retrieval Methods on User Generated Content\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06962\"\n  }, \"https://arxiv.org/abs/1611.06962\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/lab41/attalos\"\n  }, \"https://github.com/lab41/attalos\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Kill Two Birds with One Stone: Weakly-Supervised Neural Network for Image Annotation and Tag Refinement\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.06998\"\n  }, \"https://arxiv.org/abs/1711.06998\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Multiple Instance Learning for Zero-shot Image Tagging\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.06051\"\n  }, \"https://arxiv.org/abs/1803.06051\")), mdx(\"h1\", {\n    \"id\": \"image-matching\"\n  }, \"Image Matching\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Fine-grained Image Similarity with Deep Ranking\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2014\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Triplet Sampling\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1404.4661\"\n  }, \"http://arxiv.org/abs/1404.4661\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to compare image patches via convolutional neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2015. siamese network\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://imagine.enpc.fr/~zagoruys/deepcompare.html\"\n  }, \"http://imagine.enpc.fr/~zagoruys/deepcompare.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/szagoruyko/cvpr15deepcompare\"\n  }, \"https://github.com/szagoruyko/cvpr15deepcompare\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2015. siamese network\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.unc.edu/~xufeng/cs/papers/cvpr15-matchnet.pdf\"\n  }, \"http://www.cs.unc.edu/~xufeng/cs/papers/cvpr15-matchnet.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"extended abstract: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_114_ext.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_114_ext.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hanxf/matchnet\"\n  }, \"https://github.com/hanxf/matchnet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fashion Style in 128 Floats\")), mdx(\"img\", {\n    \"src\": \"http://hi.cs.waseda.ac.jp/~esimo/images/stylenet/fashionfeat.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016. StyleNet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://hi.cs.waseda.ac.jp/~esimo/en/research/stylenet/\"\n  }, \"http://hi.cs.waseda.ac.jp/~esimo/en/research/stylenet/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf\"\n  }, \"http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bobbens/cvpr2016_stylenet\"\n  }, \"https://github.com/bobbens/cvpr2016_stylenet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fully-Trainable Deep Matching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://lear.inrialpes.fr/src/deepmatching/\"\n  }, \"http://lear.inrialpes.fr/src/deepmatching/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.03532\"\n  }, \"http://arxiv.org/abs/1609.03532\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Local Similarity-Aware Deep Feature Embedding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.08904\"\n  }, \"https://arxiv.org/abs/1610.08904\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional neural network architecture for geometric matching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. Inria\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.di.ens.fr/willow/research/cnngeometric/\"\n  }, \"http://www.di.ens.fr/willow/research/cnngeometric/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.05593\"\n  }, \"https://arxiv.org/abs/1703.05593\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ignacio-rocco/cnngeometric_matconvnet\"\n  }, \"https://github.com/ignacio-rocco/cnngeometric_matconvnet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-Image Semantic Matching by Mining Consistent Features\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.07641\"\n  }, \"https://arxiv.org/abs/1711.07641\")), mdx(\"h1\", {\n    \"id\": \"image-editing\"\n  }, \"Image Editing\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Photo Editing with Introspective Adversarial Networks\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/c66848752d9fa05c3194ae36d48b869ec9d21743/687474703a2f2f692e696d6775722e636f6d2f773155323045492e706e67\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Heriot-Watt University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.07093\"\n  }, \"http://arxiv.org/abs/1609.07093\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ajbrock/Neural-Photo-Editor\"\n  }, \"https://github.com/ajbrock/Neural-Photo-Editor\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Feature Interpolation for Image Content Changes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. Cornell University & Washington University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05507\"\n  }, \"https://arxiv.org/abs/1611.05507\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/paulu/deepfeatinterp\"\n  }, \"https://github.com/paulu/deepfeatinterp\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/slang03/dfi-tensorflow\"\n  }, \"https://github.com/slang03/dfi-tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Invertible Conditional GANs for image editing\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/Guim3/IcGAN/master/images/model_overview.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016 Workshop on Adversarial Training\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06355\"\n  }, \"https://arxiv.org/abs/1611.06355\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Guim3/IcGAN\"\n  }, \"https://github.com/Guim3/IcGAN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Facial Expression Editing using Autoencoded Flow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Illinois at Urbana-Champaign & The Chinese University of Hong Kong & Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.09961\"\n  }, \"https://arxiv.org/abs/1611.09961\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Language-Based Image Editing with Recurrent Attentive Models\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.06288\"\n  }, \"https://arxiv.org/abs/1711.06288\")), mdx(\"h2\", {\n    \"id\": \"face-swap--face-editing\"\n  }, \"Face Swap & Face Editing\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Face-swap Using Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Ghent University & Twitter\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.09577\"\n  }, \"https://arxiv.org/abs/1611.09577\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Face Editing with Intrinsic Image Disentangling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017 oral\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html\"\n  }, \"http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.04131\"\n  }, \"https://arxiv.org/abs/1704.04131\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Arbitrary Facial Attribute Editing: Only Change What You Want\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.10678\"\n  }, \"https://arxiv.org/abs/1711.10678\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/LynnHo/AttGAN-Tensorflow\"\n  }, \"https://github.com/LynnHo/AttGAN-Tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RSGAN: Face Swapping and Editing using Face and Hair Representation in Latent Spaces\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.03447\"\n  }, \"https://arxiv.org/abs/1804.03447\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FaceShop: Deep Sketch-based Face Image Editing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.08972\"\n  }, \"https://arxiv.org/abs/1804.08972\")), mdx(\"h1\", {\n    \"id\": \"stereo\"\n  }, \"Stereo\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Learning of Geometry and Context for Deep Stereo Regression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.04309\"\n  }, \"https://arxiv.org/abs/1703.04309\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Adaptation for Deep Stereo\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf\"\n  }, \"http://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vision.disi.unibo.it/~mpoggi/papers/iccv2017_adaptation.pdf\"\n  }, \"http://vision.disi.unibo.it/~mpoggi/papers/iccv2017_adaptation.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo\"\n  }, \"https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.09204\"\n  }, \"https://arxiv.org/abs/1708.09204\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"StereoConvNet: Stereo convolutional neural network for depth map prediction from stereo images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/LouisFoucard/StereoConvNet\"\n  }, \"https://github.com/LouisFoucard/StereoConvNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.05196\"\n  }, \"https://arxiv.org/abs/1803.05196\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018. SenseTime Research & Sun Yat-sen University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.06641\"\n  }, \"https://arxiv.org/abs/1803.06641\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pyramid Stereo Matching Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.08669\"\n  }, \"https://arxiv.org/abs/1803.08669\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/JiaRenChang/PSMNet\"\n  }, \"https://github.com/JiaRenChang/PSMNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cascaded multi-scale and multi-dimension convolutional neural network for stereo matching\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.09437\"\n  }, \"https://arxiv.org/abs/1803.09437\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Left-Right Comparative Recurrent Model for Stereo Matching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.00796\"\n  }, \"https://arxiv.org/abs/1804.00796\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.01677\"\n  }, \"https://arxiv.org/abs/1806.01677\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Open-World Stereo Video Matching with Deep RNN\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.03959\"\n  }, \"https://arxiv.org/abs/1808.03959\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-time self-adaptive deep stereo\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.05424\"\n  }, \"https://arxiv.org/abs/1810.05424\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo\"\n  }, \"https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Group-wise Correlation Stereo Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.04025\"\n  }, \"https://arxiv.org/abs/1903.04025\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/xy-guo/GwcNet\"\n  }, \"https://github.com/xy-guo/GwcNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Self-calibrating Deep Photometric Stereo Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019 oral\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The University of Hong Kong & University of Oxford & Peking University & Peng Cheng Laboratory & Osaka University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Learning Based Uncalibrated Photometric Stereo for Non-Lambertian Surface\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://gychen.org/SDPS-Net/\"\n  }, \"http://gychen.org/SDPS-Net/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.07366\"\n  }, \"https://arxiv.org/abs/1903.07366\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, PyTorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/guanyingc/SDPS-Net\"\n  }, \"https://github.com/guanyingc/SDPS-Net\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Adapt for Stereo\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Bologna & University of Oxford & Australian National University & FiveAI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.02957\"\n  }, \"https://arxiv.org/abs/1904.02957\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/CVLAB-Unibo/Learning2AdaptForStereo\"\n  }, \"https://github.com/CVLAB-Unibo/Learning2AdaptForStereo\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"StereoDRNet: Dilated Residual Stereo Net\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of North Carolina at Chapel Hill & Facebook Reality Labs\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.02251\"\n  }, \"https://arxiv.org/abs/1904.02251\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GA-Net: Guided Aggregation Net for End-to-end Stereo Matching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019 oral\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Oxford & Baidu Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.06587\"\n  }, \"https://arxiv.org/abs/1904.06587\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-Scale Geometric Consistency Guided Multi-View Stereo\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.08103\"\n  }, \"https://arxiv.org/abs/1904.08103\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Guided Stereo Matching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1905.10107\"\n  }, \"https://arxiv.org/abs/1905.10107\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1908.06257\"\n  }, \"https://arxiv.org/abs/1908.06257\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.05845\"\n  }, \"https://arxiv.org/abs/1909.05845\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Johns Hopkins University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: STereo TRansformer (STTR)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2011.02910\"\n  }, \"https://arxiv.org/abs/2011.02910\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mli0603/stereo-transformer\"\n  }, \"https://github.com/mli0603/stereo-transformer\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tianjin University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2111.14055\"\n  }, \"https://arxiv.org/abs/2111.14055\"))), mdx(\"h1\", {\n    \"id\": \"3d\"\n  }, \"3D\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Spatiotemporal Features with 3D Convolutional Networks\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"C3D: Generic Features for Video Analysis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vlg.cs.dartmouth.edu/c3d/\"\n  }, \"http://vlg.cs.dartmouth.edu/c3d/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.0767\"\n  }, \"http://arxiv.org/abs/1412.0767\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w07-conv3d.pdf\"\n  }, \"http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w07-conv3d.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/facebook/C3D\"\n  }, \"https://github.com/facebook/C3D\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"C3D Model for Keras trained over Sports 1M\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://imatge.upc.edu/web/resources/c3d-model-keras-trained-over-sports-1m\"\n  }, \"https://imatge.upc.edu/web/resources/c3d-model-keras-trained-over-sports-1m\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sports 1M C3D Network to Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notebook: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://nbviewer.jupyter.org/gist/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2/Sports1M%20C3D%20Network%20to%20Keras.ipynb\"\n  }, \"http://nbviewer.jupyter.org/gist/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2/Sports1M%20C3D%20Network%20to%20Keras.ipynb\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep End2End Voxel2Voxel Prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06681\"\n  }, \"http://arxiv.org/abs/1511.06681\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Aligning 3D Models to RGB-D Images of Cluttered Scenes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.berkeley.edu/~rbg/papers/cvpr15/align2rgbd.pdf\"\n  }, \"http://www.cs.berkeley.edu/~rbg/papers/cvpr15/align2rgbd.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images\")), mdx(\"img\", {\n    \"src\": \"http://dss.cs.princeton.edu/teaser.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://dss.cs.princeton.edu/\"\n  }, \"http://dss.cs.princeton.edu/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.02300\"\n  }, \"http://arxiv.org/abs/1511.02300\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-view 3D Models from Single Images with a Convolutional Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06702\"\n  }, \"http://arxiv.org/abs/1511.06702\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RotationNet: Learning Object Classification Using Unsupervised Viewpoint Estimation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.06208\"\n  }, \"http://arxiv.org/abs/1603.06208\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding\")), mdx(\"img\", {\n    \"src\": \"http://deepcontext.cs.princeton.edu/teaser.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://deepcontext.cs.princeton.edu/paper.pdf\"\n  }, \"http://deepcontext.cs.princeton.edu/paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://deepcontext.cs.princeton.edu/\"\n  }, \"http://deepcontext.cs.princeton.edu/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Volumetric and Multi-View CNNs for Object Classification on 3D Data\")), mdx(\"img\", {\n    \"src\": \"http://graphics.stanford.edu/projects/3dcnn/teaser.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://graphics.stanford.edu/projects/3dcnn/\"\n  }, \"http://graphics.stanford.edu/projects/3dcnn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1604.03265\"\n  }, \"https://arxiv.org/abs/1604.03265\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/charlesq34/3dcnn.torch\"\n  }, \"https://github.com/charlesq34/3dcnn.torch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep3D: Automatic 2D-to-3D Video Conversion with CNNs\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/piiswrong/deep3d/master/img/teaser.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html\"\n  }, \"http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://homes.cs.washington.edu/~jxie/pdf/deep3d.pdf\"\n  }, \"http://homes.cs.washington.edu/~jxie/pdf/deep3d.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/piiswrong/deep3d\"\n  }, \"https://github.com/piiswrong/deep3d\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.03650\"\n  }, \"http://arxiv.org/abs/1604.03650\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/chrischoy/3D-R2N2/master/imgs/overview.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.00449\"\n  }, \"http://arxiv.org/abs/1604.00449\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chrischoy/3D-R2N2\"\n  }, \"https://github.com/chrischoy/3D-R2N2\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Body Meshes as Points\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: National University of Singapore & ByteDance AI Lab & Yitu Technology\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2105.02467\"\n  }, \"https://arxiv.org/abs/2105.02467\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jfzhang95/BMP\"\n  }, \"https://github.com/jfzhang95/BMP\"))), mdx(\"h1\", {\n    \"id\": \"deep-learning-for-makeup\"\n  }, \"Deep Learning for Makeup\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Makeup like a superstar: Deep Localized Makeup Transfer Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.07102\"\n  }, \"http://arxiv.org/abs/1604.07102\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Makeup-Go: Blind Reversion of Portrait Edit\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The Chinese University of Hong Kong & Tencent Youtu Lab\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper.pdf\"\n  }, \"http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://open.youtu.qq.com/content/file/iccv17_makeupgo.pdf\"\n  }, \"http://open.youtu.qq.com/content/file/iccv17_makeupgo.pdf\"))), mdx(\"h1\", {\n    \"id\": \"music-tagging\"\n  }, \"Music Tagging\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Automatic tagging using deep convolutional neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.00298\"\n  }, \"https://arxiv.org/abs/1606.00298\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/keunwoochoi/music-auto_tagging-keras\"\n  }, \"https://github.com/keunwoochoi/music-auto_tagging-keras\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Music tagging and feature extraction with MusicTaggerCRNN\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://keras.io/applications/#music-tagging-and-feature-extraction-with-musictaggercrnn\"\n  }, \"https://keras.io/applications/#music-tagging-and-feature-extraction-with-musictaggercrnn\")), mdx(\"h1\", {\n    \"id\": \"action-recognition\"\n  }, \"Action Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single Image Action Recognition by Predicting Space-Time Saliency\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.04641\"\n  }, \"https://arxiv.org/abs/1705.04641\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attentional Pooling for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://rohitgirdhar.github.io/AttentionalPoolingAction/\"\n  }, \"https://rohitgirdhar.github.io/AttentionalPoolingAction/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.01467\"\n  }, \"https://arxiv.org/abs/1711.01467\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/rohitgirdhar/AttentionalPoolingAction/\"\n  }, \"https://github.com/rohitgirdhar/AttentionalPoolingAction/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Memory Attention Networks for Skeleton-based Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Temporal Attention Recalibration Module (TARM) and a Spatio-Temporal Convolution Module (STCM)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arixv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.08254\"\n  }, \"https://arxiv.org/abs/1804.08254\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/memory-attention-networks\"\n  }, \"https://github.com/memory-attention-networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Analysis of CNN-based Spatio-temporal Representations for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT-IBM Watson AI Lab & MIT\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2010.11757\"\n  }, \"https://arxiv.org/abs/2010.11757\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/IBM/action-recognition-pytorch\"\n  }, \"https://github.com/IBM/action-recognition-pytorch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690528.pdf\"\n  }, \"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690528.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kchengiva/DecoupleGCN-DropGraph\"\n  }, \"https://github.com/kchengiva/DecoupleGCN-DropGraph\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal-Relational CrossTransformers for Few-Shot Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Bristol\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2101.06184\"\n  }, \"https://arxiv.org/abs/2101.06184\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tobyperrett/trx\"\n  }, \"https://github.com/tobyperrett/trx\"))), mdx(\"h1\", {\n    \"id\": \"ctr-prediction\"\n  }, \"CTR Prediction\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep CTR Prediction in Display Advertising\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACM Multimedia Conference 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1609.06018\"\n  }, \"https://arxiv.org/abs/1609.06018\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepFM: A Factorization-Machine based Neural Network for CTR Prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Harbin Institute of Technology & Huawei\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.04247\"\n  }, \"https://arxiv.org/abs/1703.04247\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Interest Network for Click-Through Rate Prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Alibaba Inc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.06978\"\n  }, \"https://arxiv.org/abs/1706.06978\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Matters: Jointly Train Advertising CTR Model with Image Representation of Ad and User Behavior\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Alibaba Inc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.06505\"\n  }, \"https://arxiv.org/abs/1711.06505\"))), mdx(\"h1\", {\n    \"id\": \"cryptography\"\n  }, \"Cryptography\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Protect Communications with Adversarial Neural Cryptography\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Brain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.06918\"\n  }, \"https://arxiv.org/abs/1610.06918\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Theano): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/nlml/adversarial-neural-crypt\"\n  }, \"https://github.com/nlml/adversarial-neural-crypt\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ankeshanand/neural-cryptography-tensorflow\"\n  }, \"https://github.com/ankeshanand/neural-cryptography-tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adversarial Neural Cryptography in Theano\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://nlml.github.io/neural-networks/adversarial-neural-cryptography/\"\n  }, \"https://nlml.github.io/neural-networks/adversarial-neural-cryptography/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Embedding Watermarks into Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.04082\"\n  }, \"https://arxiv.org/abs/1701.04082\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yu4u/dnn-watermark\"\n  }, \"https://github.com/yu4u/dnn-watermark\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Digital Watermarking for Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: International Journal of Multimedia Information Retrieval\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.02601\"\n  }, \"https://arxiv.org/abs/1802.02601\"))), mdx(\"h1\", {\n    \"id\": \"cyber-security\"\n  }, \"Cyber Security\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Collection of Deep Learning Cyber Security Research Papers\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042#.fcus2cu9m\"\n  }, \"https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042#.fcus2cu9m\"))), mdx(\"h1\", {\n    \"id\": \"lip-reading\"\n  }, \"Lip Reading\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LipNet: Sentence-level Lipreading\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LipNet: End-to-End Sentence-level Lipreading\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01599\"\n  }, \"https://arxiv.org/abs/1611.01599\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openreview.net/pdf?id=BkjLkSqxg\"\n  }, \"http://openreview.net/pdf?id=BkjLkSqxg\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bshillingford/LipNet\"\n  }, \"https://github.com/bshillingford/LipNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Lip Reading Sentences in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Oxford & Google DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05358\"\n  }, \"https://arxiv.org/abs/1611.05358\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=5aogzAUPilE\"\n  }, \"https://www.youtube.com/watch?v=5aogzAUPilE\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Combining Residual Networks with LSTMs for Lipreading\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.04105\"\n  }, \"https://arxiv.org/abs/1703.04105\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Multi-View Lipreading\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.00443\"\n  }, \"https://arxiv.org/abs/1709.00443\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LCANet: End-to-End Lipreading with Cascaded Attention-CTC\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: FG 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.04988\"\n  }, \"https://arxiv.org/abs/1803.04988\"))), mdx(\"h1\", {\n    \"id\": \"event-recognition\"\n  }, \"Event Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Better Exploiting OS-CNNs for Better Event Recognition in Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.03979\"\n  }, \"http://arxiv.org/abs/1510.03979\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.00162\"\n  }, \"http://arxiv.org/abs/1609.00162\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"IOD-CNN: Integrating Object Detection Networks for Event Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.07431\"\n  }, \"https://arxiv.org/abs/1703.07431\")), mdx(\"h1\", {\n    \"id\": \"trajectory-prediction\"\n  }, \"Trajectory Prediction\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Machine Learning for Autonomous Driving @ NeurIPS 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Carnegie Mellon University & Bosch Research Pittsburgh\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2011.14910\"\n  }, \"https://arxiv.org/abs/2011.14910\"))), mdx(\"h1\", {\n    \"id\": \"human-object-interaction\"\n  }, \"Human-Object Interaction\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Human-Object Interactions by Graph Parsing Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.07962\"\n  }, \"https://arxiv.org/abs/1808.07962\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/SiyuanQi/gpnn\"\n  }, \"https://github.com/SiyuanQi/gpnn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Interact as You Intend: Intention-Driven Human-Object Interaction Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.09796\"\n  }, \"https://arxiv.org/abs/1808.09796\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gaochen315.github.io/iCAN/\"\n  }, \"https://gaochen315.github.io/iCAN/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.10437\"\n  }, \"https://arxiv.org/abs/1808.10437\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/vt-vl-lab/iCAN\"\n  }, \"https://github.com/vt-vl-lab/iCAN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pose-aware Multi-level Feature Network for Human Object Interaction Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.08453\"\n  }, \"https://arxiv.org/abs/1909.08453\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Human Object Interaction Detection with HOI Transformer\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MEGVII Technology\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2103.04503\"\n  }, \"https://arxiv.org/abs/2103.04503\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bbepoch/HoiTransformer\"\n  }, \"https://github.com/bbepoch/HoiTransformer\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: OPPO Research Institute & Xmotors & University of California\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2110.03278\"\n  }, \"https://arxiv.org/abs/2110.03278\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hand-Object Contact Prediction via Motion-Based Pseudo-Labeling and Guided Progressive Label Correction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2110.10174\"\n  }, \"https://arxiv.org/abs/2110.10174\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/takumayagi/hand_object_contact_prediction\"\n  }, \"https://github.com/takumayagi/hand_object_contact_prediction\"))), mdx(\"h1\", {\n    \"id\": \"deep-learning-in-finance\"\n  }, \"Deep Learning in Finance\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning in Finance\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.06561\"\n  }, \"http://arxiv.org/abs/1602.06561\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Survey of Deep Learning Techniques Applied to Trading\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://gregharris.info/a-survey-of-deep-learning-techniques-applied-to-trading/\"\n  }, \"http://gregharris.info/a-survey-of-deep-learning-techniques-applied-to-trading/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning and Long-Term Investing\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 1: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.euclidean.com/deep-learning-long-term-investing-1\"\n  }, \"http://www.euclidean.com/deep-learning-long-term-investing-1\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 2: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.euclidean.com/deep-learning-investing-part-2-preprocessing-data\"\n  }, \"http://www.euclidean.com/deep-learning-investing-part-2-preprocessing-data\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning in Trading\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=FoQKCeDuPiY\"\n  }, \"https://www.youtube.com/watch?v=FoQKCeDuPiY\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1sltRra9\"\n  }, \"https://pan.baidu.com/s/1sltRra9\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Research to Products: Machine & Human Intelligence in Finance\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Peter Sarlin, Hanken School of Economics - Deep Learning in Finance Summit 2016 #reworkfin\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=Fd7Cc-KOVXg\"\n  }, \"https://www.youtube.com/watch?v=Fd7Cc-KOVXg\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1kVpZKur#list/path=%2F\"\n  }, \"https://pan.baidu.com/s/1kVpZKur#list/path=%2F\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"eep Neural Networks for Real-time Market Predictions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=Kzz2-wAEK7A\"\n  }, \"https://www.youtube.com/watch?v=Kzz2-wAEK7A\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning the Stock Market\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.z752rf43u\"\n  }, \"https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.z752rf43u\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/talolard/MarketVectors\"\n  }, \"https://github.com/talolard/MarketVectors\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"rl_portfolio\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: This Repository uses Reinforcement Learning and Supervised learning to Optimize portfolio allocation.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deependersingla/deep_portfolio\"\n  }, \"https://github.com/deependersingla/deep_portfolio\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural networks for algorithmic trading. Multivariate time series\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57\"\n  }, \"https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Rachnog/Deep-Trading/tree/master/multivariate\"\n  }, \"https://github.com/Rachnog/Deep-Trading/tree/master/multivariate\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep-Trading: Algorithmic trading with deep learning experiments\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/Rachnog/Deep-Trading\"\n  }, \"https://github.com/Rachnog/Deep-Trading\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural networks for algorithmic trading. Multimodal and multitask deep learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://becominghuman.ai/neural-networks-for-algorithmic-trading-multimodal-and-multitask-deep-learning-5498e0098caf\"\n  }, \"https://becominghuman.ai/neural-networks-for-algorithmic-trading-multimodal-and-multitask-deep-learning-5498e0098caf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Rachnog/Deep-Trading/tree/master/multimodal\"\n  }, \"https://github.com/Rachnog/Deep-Trading/tree/master/multimodal\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning with Python in Finance - Singapore Python User Group\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=xvm-M-R2fZY\"\n  }, \"https://www.youtube.com/watch?v=xvm-M-R2fZY\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Xi\\u2019an Jiaotong-Liverpool University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: PGPortfolio: Policy Gradient Portfolio\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.10059\"\n  }, \"https://arxiv.org/abs/1706.10059\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//ZhengyaoJiang/PGPortfolio\"\n  }, \"https://github.com//ZhengyaoJiang/PGPortfolio\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stock Prediction: a method based on extraction of news features and recurrent neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Peking University. The 22nd China Conference on Information Retrieval\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.07585\"\n  }, \"https://arxiv.org/abs/1707.07585\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multidimensional LSTM Networks to Predict Bitcoin Price\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price\"\n  }, \"http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jaungiers/Multidimensional-LSTM-BitCoin-Time-Series\"\n  }, \"https://github.com/jaungiers/Multidimensional-LSTM-BitCoin-Time-Series\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Euclidean Technologies & Amazon AI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04837\"\n  }, \"https://arxiv.org/abs/1711.04837\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Findings from our Research on Applying Deep Learning to Long-Term Investing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.euclidean.com/paper-on-deep-learning-long-term-investing\"\n  }, \"http://www.euclidean.com/paper-on-deep-learning-long-term-investing\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Predicting Cryptocurrency Prices With Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: This post brings together cryptos and deep learning in a desperate attempt for Reddit popularity\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/\"\n  }, \"https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Trading Agent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Deep Reinforcement Learning based Trading Agent for Bitcoin\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/samre12/deep-trading-agent\"\n  }, \"https://github.com/samre12/deep-trading-agent\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Financial Trading as a Game: A Deep Reinforcement Learning Approach\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: National Chiao Tung University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.02787\"\n  }, \"https://arxiv.org/abs/1807.02787\"))), mdx(\"h1\", {\n    \"id\": \"deep-learning-in-speech\"\n  }, \"Deep Learning in Speech\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Baidu Research, ICML 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1512.02595\"\n  }, \"https://arxiv.org/abs/1512.02595\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Neon): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/NervanaSystems/deepspeech\"\n  }, \"https://github.com/NervanaSystems/deepspeech\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-end speech recognition with neon\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.nervanasys.com/end-end-speech-recognition-neon/\"\n  }, \"https://www.nervanasys.com/end-end-speech-recognition-neon/\"))), mdx(\"h2\", {\n    \"id\": \"wavenet\"\n  }, \"WaveNet\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"WaveNet: A Generative Model for Raw Audio\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://deepmind.com/blog/wavenet-generative-model-raw-audio/\"\n  }, \"https://deepmind.com/blog/wavenet-generative-model-raw-audio/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view\"\n  }, \"https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1gfmGWaJ\"\n  }, \"https://pan.baidu.com/s/1gfmGWaJ\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/usernaamee/keras-wavenet\"\n  }, \"https://github.com/usernaamee/keras-wavenet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ibab/tensorflow-wavenet\"\n  }, \"https://github.com/ibab/tensorflow-wavenet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/monthly-hack/chainer-wavenet\"\n  }, \"https://github.com/monthly-hack/chainer-wavenet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/huyouare/WaveNet-Theano\"\n  }, \"https://github.com/huyouare/WaveNet-Theano\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Keras): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/basveeling/wavenet\"\n  }, \"https://github.com/basveeling/wavenet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ritheshkumar95/WaveNet\"\n  }, \"https://github.com/ritheshkumar95/WaveNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A TensorFlow implementation of DeepMind's WaveNet paper for text generation.\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Zeta36/tensorflow-tex-wavenet\"\n  }, \"https://github.com/Zeta36/tensorflow-tex-wavenet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Wavenet Generation Algorithm\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: An efficient Wavenet generation implementation\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.09482\"\n  }, \"https://arxiv.org/abs/1611.09482\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tomlepaine/fast-wavenet\"\n  }, \"https://github.com/tomlepaine/fast-wavenet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition based on DeepMind's WaveNet and tensorflow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/buriburisuri/speech-to-text-wavenet\"\n  }, \"https://github.com/buriburisuri/speech-to-text-wavenet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Wav2Letter: an End-to-End ConvNet-based Speech Recognition System\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.03193\"\n  }, \"http://arxiv.org/abs/1609.03193\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TristouNet: Triplet Loss for Speaker Turn Embedding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1609.04301\"\n  }, \"https://arxiv.org/abs/1609.04301\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hbredin/TristouNet\"\n  }, \"https://github.com/hbredin/TristouNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Speech Recognion and Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Baidu Research Silicon Valley AI Lab\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf\"\n  }, \"http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1qYrPkPQ\"\n  }, \"https://pan.baidu.com/s/1qYrPkPQ\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/baidu-research/ba-dls-deepspeech\"\n  }, \"https://github.com/baidu-research/ba-dls-deepspeech\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Robust end-to-end deep audiovisual speech recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CMU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06986\"\n  }, \"https://arxiv.org/abs/1611.06986\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Experimental Comparison of Deep Neural Networks for End-to-end Speech Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.07174\"\n  }, \"https://arxiv.org/abs/1611.07174\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent Deep Stacking Networks for Speech Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The Ohio State University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.04675\"\n  }, \"https://arxiv.org/abs/1612.04675\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Universite de Montreal & CIFAR\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.02720\"\n  }, \"https://arxiv.org/abs/1701.02720\"))), mdx(\"h1\", {\n    \"id\": \"deep-learning-for-sound--music\"\n  }, \"Deep Learning for Sound / Music\"), mdx(\"h2\", {\n    \"id\": \"sound\"\n  }, \"Sound\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Suggesting Sounds for Images from Video Collections\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ETH Zurich & 2Disney Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20161014182443/Suggesting-Sounds-for-Images-from-Video-Collections-Paper.pdf\"\n  }, \"https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20161014182443/Suggesting-Sounds-for-Images-from-Video-Collections-Paper.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Disney AI System Associates Images with Sounds\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://news.developer.nvidia.com/disneys-ai-system-associates-images-with-sounds/\"\n  }, \"https://news.developer.nvidia.com/disneys-ai-system-associates-images-with-sounds/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Recurrent Neural Networks for Bird Audio Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.02317\"\n  }, \"https://arxiv.org/abs/1703.02317\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual to Sound: Generating Natural Sound for Videos in the Wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html\"\n  }, \"http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.01393\"\n  }, \"https://arxiv.org/abs/1712.01393\"))), mdx(\"h2\", {\n    \"id\": \"music\"\n  }, \"Music\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Features of Music from Scratch\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Washington. MusicNet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://homes.cs.washington.edu/~thickstn/musicnet.html\"\n  }, \"http://homes.cs.washington.edu/~thickstn/musicnet.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.09827\"\n  }, \"https://arxiv.org/abs/1611.09827\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://homes.cs.washington.edu/~thickstn/demos.html\"\n  }, \"http://homes.cs.washington.edu/~thickstn/demos.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepBach: a Steerable Model for Bach chorales generation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-generation/\"\n  }, \"http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-generation/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.01010\"\n  }, \"https://arxiv.org/abs/1612.01010\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/SonyCSL-Paris/DeepBach\"\n  }, \"https://github.com/SonyCSL-Paris/DeepBach\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=QiBM7-5hA6o\"\n  }, \"https://www.youtube.com/watch?v=QiBM7-5hA6o\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for Music\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://amundtveit.com/2016/11/22/deep-learning-for-music/\"\n  }, \"https://amundtveit.com/2016/11/22/deep-learning-for-music/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"First International Workshop on Deep Learning and Music\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/html/1706.08675\"\n  }, \"https://arxiv.org/html/1706.08675\")), mdx(\"h1\", {\n    \"id\": \"deep-learning-in-medicine-and-biology\"\n  }, \"Deep Learning in Medicine and Biology\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Low Data Drug Discovery with One-shot Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT & Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.03199\"\n  }, \"https://arxiv.org/abs/1611.03199\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://deepchem.io/\"\n  }, \"http://deepchem.io/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deepchem/deepchem\"\n  }, \"https://github.com/deepchem/deepchem\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Democratizing Drug Discovery with DeepChem\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=sntikyFI8s8\"\n  }, \"https://www.youtube.com/watch?v=sntikyFI8s8\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Introduction to Deep Learning in Medicine and Biology\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://a12d.com/deep-learning-biomedicine\"\n  }, \"http://a12d.com/deep-learning-biomedicine\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for Alzheimer Diagnostics and Decision Support\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://amundtveit.com/2016/11/18/deep-learning-for-alzheimer-diagnostics-and-decision-support/\"\n  }, \"https://amundtveit.com/2016/11/18/deep-learning-for-alzheimer-diagnostics-and-decision-support/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Florida\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.03211\"\n  }, \"https://arxiv.org/abs/1612.03211\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards biologically plausible deep learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Yoshua\\tBengio, NIPS\\u20192016 Workshops\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.iro.umontreal.ca/~bengioy/talks/Brains+Bits-NIPS2016Workshop.pptx.pdf\"\n  }, \"http://www.iro.umontreal.ca/~bengioy/talks/Brains+Bits-NIPS2016Workshop.pptx.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning and Its Applications to Machine Health Monitoring: A Survey\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.07640\"\n  }, \"https://arxiv.org/abs/1612.07640\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.01329\"\n  }, \"https://arxiv.org/abs/1701.01329\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Applications in Medical Imaging\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://techemergence.com/deep-learning-medical-applications/\"\n  }, \"http://techemergence.com/deep-learning-medical-applications/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dermatologist-level classification of skin cancer with deep neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford University. Nature 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature21056.pdf\"\n  }, \"http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature21056.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for Health Informatics\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Imperial College London\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ieeexplore.ieee.org/abstract/document/7801947/\"\n  }, \"http://ieeexplore.ieee.org/abstract/document/7801947/\"))), mdx(\"h1\", {\n    \"id\": \"deep-learning-for-fashion\"\n  }, \"Deep Learning for Fashion\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Neural Networks for Fashion Classification and Object Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CS231N project\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs231n.stanford.edu/reports/BLAO_KJAG_CS231N_FinalPaperFashionClassification.pdf\"\n  }, \"http://cs231n.stanford.edu/reports/BLAO_KJAG_CS231N_FinalPaperFashionClassification.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html\"\n  }, \"http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for Fast and Accurate Fashion Item Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords:  MultiBox and Fast R-CNN, Kuznech-Fashion-156 and Kuznech-Fashion-205 fashion item detection datasets\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf\"\n  }, \"https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning at GILT\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: automated tagging, automatic dress faceting\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://tech.gilt.com/machine/learning,/deep/learning/2016/12/22/deep-learning-at-gilt\"\n  }, \"http://tech.gilt.com/machine/learning,/deep/learning/2016/12/22/deep-learning-at-gilt\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Working with Fashion Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://making.lyst.com/2017/02/21/working-with-fashion-models/\"\n  }, \"https://making.lyst.com/2017/02/21/working-with-fashion-models/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=emr2qaCQOQs\"\n  }, \"https://www.youtube.com/watch?v=emr2qaCQOQs\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fashion Forward: Forecasting Visual Style in Fashion\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Karlsruhe Institute of Technology & The University of Texas at Austin\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.06394\"\n  }, \"https://arxiv.org/abs/1705.06394\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"StreetStyle: Exploring world-wide clothing styles from millions of photos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://streetstyle.cs.cornell.edu/\"\n  }, \"http://streetstyle.cs.cornell.edu/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.01869\"\n  }, \"https://arxiv.org/abs/1706.01869\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://streetstyle.cs.cornell.edu/trends.html\"\n  }, \"http://streetstyle.cs.cornell.edu/trends.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fashioning with Networks: Neural Style Transfer to Design Clothes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ML4Fashion 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.09899\"\n  }, \"https://arxiv.org/abs/1707.09899\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Our Way Through Fashion Week\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://inside.edited.com/deep-learning-our-way-through-fashion-week-ea55bf50bab8\"\n  }, \"https://inside.edited.com/deep-learning-our-way-through-fashion-week-ea55bf50bab8\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Be Your Own Prada: Fashion Synthesis with Structural Coherence\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Be_Your_Own_ICCV_2017_paper.pdf\"\n  }, \"http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Be_Your_Own_ICCV_2017_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhusz/ICCV17-fashionGAN\"\n  }, \"https://github.com/zhusz/ICCV17-fashionGAN\"))), mdx(\"h1\", {\n    \"id\": \"others\"\n  }, \"Others\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Selfai: Predicting Facial Beauty in Selfies\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Selfai: A Method for Understanding Beauty in Selfies\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.erogol.com/selfai-predicting-facial-beauty-selfies/\"\n  }, \"http://www.erogol.com/selfai-predicting-facial-beauty-selfies/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/erogol/beauty.torch\"\n  }, \"https://github.com/erogol/beauty.torch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Enables You to Hide Screen when Your Boss is Approaching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/\"\n  }, \"http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Hironsan/BossSensor\"\n  }, \"https://github.com/Hironsan/BossSensor\"))), mdx(\"h1\", {\n    \"id\": \"blogs\"\n  }, \"Blogs\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"40 Ways Deep Learning is Eating the World\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d#.rxq8xvbfz\"\n  }, \"https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d#.rxq8xvbfz\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Applications\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.deeplearningpatterns.com/doku.php/applications\"\n  }, \"http://www.deeplearningpatterns.com/doku.php/applications\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Systematic Approach To Applications Of Deep Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://gettocode.com/2016/11/25/systematic-approach-to-applications-of-deep-learning/\"\n  }, \"https://gettocode.com/2016/11/25/systematic-approach-to-applications-of-deep-learning/\")), mdx(\"h1\", {\n    \"id\": \"resources\"\n  }, \"Resources\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Gallery - a curated collection of deep learning projects\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://deeplearninggallery.com/\"\n  }, \"http://deeplearninggallery.com/\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Deep Learning Applications\ndate: 2015-10-09\n---\n\n# Applications\n\n**DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations**\n\n- arxiv: [http://arxiv.org/abs/1510.02927](http://arxiv.org/abs/1510.02927)\n\n**Some like it hot - visual guidance for preference prediction**\n\n- arxiv: [http://arxiv.org/abs/1510.07867](http://arxiv.org/abs/1510.07867)\n- demo: [http://howhot.io/](http://howhot.io/)\n\n**Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey**\n\n- arxiv: [http://arxiv.org/abs/1512.03131](http://arxiv.org/abs/1512.03131)\n\n**Deep Relative Attributes**\n\n- intro: ACCV 2016\n- arxiv: [http://arxiv.org/abs/1512.04103](http://arxiv.org/abs/1512.04103)\n- github: [https://github.com/yassersouri/ghiaseddin](https://github.com/yassersouri/ghiaseddin)\n\n**Deep-Spying: Spying using Smartwatch and Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1512.05616](http://arxiv.org/abs/1512.05616)\n- github: [https://github.com/tonybeltramelli/Deep-Spying](https://github.com/tonybeltramelli/Deep-Spying)\n\n**Camera identification with deep convolutional networks**\n\n- key word: copyright infringement cases, ownership attribution\n- arxiv: [http://arxiv.org/abs/1603.01068](http://arxiv.org/abs/1603.01068)\n\n**An Analysis of Deep Neural Network Models for Practical Applications**\n\n- arxiv: [http://arxiv.org/abs/1605.07678](http://arxiv.org/abs/1605.07678)\n\n**8 Inspirational Applications of Deep Learning**\n\n- intro: Colorization of Black and White Images, Adding Sounds To Silent Movies, Automatic Machine Translation\nObject Classification in Photographs, Automatic Handwriting Generation, Character Text Generation, \nImage Caption Generation, Automatic Game Playing\n- blog: [http://machinelearningmastery.com/inspirational-applications-deep-learning/](http://machinelearningmastery.com/inspirational-applications-deep-learning/)\n\n**16 Open Source Deep Learning Models Running as Microservices**\n\n- intro: Places 365 Classifier, Deep Face Recognition, Real Estate Classifier, Colorful Image Colorization, \nIllustration Tagger, InceptionNet, Parsey McParseface, ArtsyNetworks\n- blog: [http://blog.algorithmia.com/2016/07/open-source-deep-learning-algorithm-roundup/](http://blog.algorithmia.com/2016/07/open-source-deep-learning-algorithm-roundup/)\n\n**Deep Cascaded Bi-Network for Face Hallucination**\n\n- project page: [http://mmlab.ie.cuhk.edu.hk/projects/CBN.html](http://mmlab.ie.cuhk.edu.hk/projects/CBN.html)\n- arxiv: [http://arxiv.org/abs/1607.05046](http://arxiv.org/abs/1607.05046)\n\n**DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation**\n\n![](http://sites.skoltech.ru/compvision/projects/deepwarp/images/pipeline.svg)\n\n- project page: [http://yaroslav.ganin.net/static/deepwarp/](http://yaroslav.ganin.net/static/deepwarp/)\n- arxiv: [http://arxiv.org/abs/1607.07215](http://arxiv.org/abs/1607.07215)\n\n**Autoencoding Blade Runner**\n\n- blog: [https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.9kckqg7cq](https://medium.com/@Terrybroad/autoencoding-blade-runner-88941213abbe#.9kckqg7cq)\n- github: [https://github.com/terrybroad/Learned-Sim-Autoencoder-For-Video-Frames](https://github.com/terrybroad/Learned-Sim-Autoencoder-For-Video-Frames)\n\n**A guy trained a machine to \"watch\" Blade Runner. Then things got seriously sci-fi.**\n\n[http://www.vox.com/2016/6/1/11787262/blade-runner-neural-network-encoding](http://www.vox.com/2016/6/1/11787262/blade-runner-neural-network-encoding)\n\n**Deep Convolution Networks for Compression Artifacts Reduction**\n\n![](http://mmlab.ie.cuhk.edu.hk/projects/ARCNN/img/fig1.png)\n\n- intro: ICCV 2015\n- project page(code): [http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html](http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html)\n- arxiv: [http://arxiv.org/abs/1608.02778](http://arxiv.org/abs/1608.02778)\n\n**Deep GDashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks**\n\n- intro: Deep Genomic Dashboard (Deep GDashboard)\n- arxiv: [http://arxiv.org/abs/1608.03644](http://arxiv.org/abs/1608.03644)\n\n**Instagram photos reveal predictive markers of depression**\n\n- arxiv: [http://arxiv.org/abs/1608.03282](http://arxiv.org/abs/1608.03282)\n\n**How an Algorithm Learned to Identify Depressed Individuals by Studying Their Instagram Photos**\n\n- review: [https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/](https://www.technologyreview.com/s/602208/how-an-algorithm-learned-to-identify-depressed-individuals-by-studying-their-instagram/)\n\n**IM2CAD**\n\n- arxiv: [http://arxiv.org/abs/1608.05137](http://arxiv.org/abs/1608.05137)\n\n**Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks**\n\n- paper: [https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/melicher](https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/melicher)\n- github: [https://github.com/cupslab/neural_network_cracking](https://github.com/cupslab/neural_network_cracking)\n\n**Defeating Image Obfuscation with Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1609.00408](http://arxiv.org/abs/1609.00408)\n\n**Detecting Music BPM using Neural Networks**\n\n![](https://nlml.github.io/images/convnet_diagram.png)\n\n- keywords: BPM (Beats Per Minutes)\n- blog: [https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/](https://nlml.github.io/neural-networks/detecting-bpm-neural-networks/)\n- github: [https://github.com/nlml/bpm](https://github.com/nlml/bpm)\n\n**Generative Visual Manipulation on the Natural Image Manifold**\n\n![](https://raw.githubusercontent.com/junyanz/iGAN/master/pics/demo_teaser.jpg)\n\n- intro: ECCV 2016\n- project page: [https://people.eecs.berkeley.edu/~junyanz/projects/gvm/](https://people.eecs.berkeley.edu/~junyanz/projects/gvm/)\n- arxiv: [http://arxiv.org/abs/1609.03552](http://arxiv.org/abs/1609.03552)\n- github: [https://github.com/junyanz/iGAN](https://github.com/junyanz/iGAN)\n\n**Deep Impression: Audiovisual Deep Residual Networks for Multimodal Apparent Personality Trait Recognition**\n\n- arxiv: [http://arxiv.org/abs/1609.05119](http://arxiv.org/abs/1609.05119)\n\n**Deep Gold: Using Convolution Networks to Find Minerals**\n\n- blog: [https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df#.lgh95ub4a](https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df#.lgh95ub4a)\n- github: [https://github.com/scottvallance/DeepGold](https://github.com/scottvallance/DeepGold)\n\n**Predicting First Impressions with Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1610.08119](https://arxiv.org/abs/1610.08119)\n\n**Judging a Book By its Cover**\n\n- arxiv: [https://arxiv.org/abs/1610.09204](https://arxiv.org/abs/1610.09204)\n- review: [https://www.technologyreview.com/s/602807/deep-neural-network-learns-to-judge-books-by-their-covers/](https://www.technologyreview.com/s/602807/deep-neural-network-learns-to-judge-books-by-their-covers/)\n\n**Image Credibility Analysis with Effective Domain Transferred Deep Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.05328](https://arxiv.org/abs/1611.05328)\n\n**A novel image tag completion method based on convolutional neural network**\n\n- arxiv: [https://www.arxiv.org/abs/1703.00586](https://www.arxiv.org/abs/1703.00586)\n\n**Image operator learning coupled with CNN classification and its application to staff line removal**\n\n- intro: ICDAR 2017\n- arxiv: [https://arxiv.org/abs/1709.06476](https://arxiv.org/abs/1709.06476)\n\n**Joint Image Filtering with Deep Convolutional Networks**\n\n- intro: University of California, Merced & Virginia Tech & University of Illinois\n- project page: [http://vllab1.ucmerced.edu/~yli62/DJF_residual/](http://vllab1.ucmerced.edu/~yli62/DJF_residual/)\n- arxiv: [https://arxiv.org/abs/1710.04200](https://arxiv.org/abs/1710.04200)\n- github: [https://github.com/Yijunmaverick/DeepJointFilter](https://github.com/Yijunmaverick/DeepJointFilter)\n\n**DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1704.02470](https://arxiv.org/abs/1704.02470)\n- github: [https://github.com/aiff22/DPED](https://github.com/aiff22/DPED)\n\n**Neural Scene De-rendering**\n\n- intro: CVPR 2017\n- project page: [http://nsd.csail.mit.edu/](http://nsd.csail.mit.edu/)\n- paper: [http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf](http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf)\n- gihtub: [https://github.com/jiajunwu/nsd](https://github.com/jiajunwu/nsd)\n\n**Image2GIF: Generating Cinemagraphs using Recurrent Deep Q-Networks**\n\n- intro: WACV 2018\n- project page: [http://bvision11.cs.unc.edu/bigpen/yipin/WACV2018/](http://bvision11.cs.unc.edu/bigpen/yipin/WACV2018/)\n- arxiv: [https://arxiv.org/abs/1801.09042](https://arxiv.org/abs/1801.09042)\n\n**Deep Neural Networks In Fully Connected CRF For Image Labeling With Social Network Metadata**\n\n[https://arxiv.org/abs/1801.09108](https://arxiv.org/abs/1801.09108)\n\n**Single Image Reflection Removal Using Deep Encoder-Decoder Network**\n\n[https://arxiv.org/abs/1802.00094](https://arxiv.org/abs/1802.00094)\n\n**CRRN: Multi-Scale Guided Concurrent Reflection Removal Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1805.11802](https://arxiv.org/abs/1805.11802)\n\n**Learning Deep Convolutional Networks for Demosaicing**\n\n[https://arxiv.org/abs/1802.03769](https://arxiv.org/abs/1802.03769)\n\n**Fully convolutional watermark removal attack**\n\n- github: [https://github.com/marcbelmont/cnn-watermark-removal](https://github.com/marcbelmont/cnn-watermark-removal)\n\n**ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes**\n\n- arxiv: [https://arxiv.org/abs/1803.10562](https://arxiv.org/abs/1803.10562)\n- github: [https://github.com/Prinsphield/ELEGANT](https://github.com/Prinsphield/ELEGANT)\n\n**Learning to See in the Dark**\n\n- intro: CVPR 2018\n- project page: [http://web.engr.illinois.edu/~cchen156/SID.html](http://web.engr.illinois.edu/~cchen156/SID.html)\n- arxiv: [https://arxiv.org/abs/1805.01934](https://arxiv.org/abs/1805.01934)\n- github: [https://github.com/cchen156/Learning-to-See-in-the-Dark](https://github.com/cchen156/Learning-to-See-in-the-Dark)\n- video: [https://www.youtube.com/watch?v=qWKUFK7MWvg&feature=youtu.be](https://www.youtube.com/watch?v=qWKUFK7MWvg&feature=youtu.be)\n- video: [https://www.bilibili.com/video/av23195280/](https://www.bilibili.com/video/av23195280/)\n\n**Generative Smoke Removal**\n\n[https://arxiv.org/abs/1902.00311](https://arxiv.org/abs/1902.00311)\n\n**Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data**\n\n[https://arxiv.org/abs/1903.10683](https://arxiv.org/abs/1903.10683)\n\n**Blind Visual Motif Removal from a Single Image**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.02756](https://arxiv.org/abs/1904.02756)\n\n**Neural Camera Simulators**\n\n- intro: CVPR 2021\n- arxiv: [https://arxiv.org/abs/2104.05237](https://arxiv.org/abs/2104.05237)\n\n**Lighting the Darkness in the Deep Learning Era**\n\n- arxiv: [https://arxiv.org/abs/2104.10729](https://arxiv.org/abs/2104.10729)\n- github: [https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open](https://github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open)\n\n# Boundary / Edge / Contour Detection\n\n**Holistically-Nested Edge Detection**\n\n![](https://camo.githubusercontent.com/da32e7e3275c2a9693dd2a6925b03a1151e2b098/687474703a2f2f70616765732e756373642e6564752f7e7a74752f6865642e6a7067)\n\n- intro: ICCV 2015, Marr Prize\n- paper: [http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf)\n- arxiv: [http://arxiv.org/abs/1504.06375](http://arxiv.org/abs/1504.06375)\n- github: [https://github.com/s9xie/hed](https://github.com/s9xie/hed)\n- github: [https://github.com/moabitcoin/holy-edge](https://github.com/moabitcoin/holy-edge)\n\n**Unsupervised Learning of Edges**\n\n- intro: CVPR 2016. Facebook AI Research\n- arxiv: [http://arxiv.org/abs/1511.04166](http://arxiv.org/abs/1511.04166)\n- zn-blog: [http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html](http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html)\n\n**Pushing the Boundaries of Boundary Detection using Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1511.07386](http://arxiv.org/abs/1511.07386)\n\n**Convolutional Oriented Boundaries**\n\n- intro: ECCV 2016\n- arxiv: [http://arxiv.org/abs/1608.02755](http://arxiv.org/abs/1608.02755)\n\n**Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks**\n\n- project page: [http://www.vision.ee.ethz.ch/~cvlsegmentation/](http://www.vision.ee.ethz.ch/~cvlsegmentation/)\n- arxiv: [https://arxiv.org/abs/1701.04658](https://arxiv.org/abs/1701.04658)\n- github: [https://github.com/kmaninis/COB](https://github.com/kmaninis/COB)\n\n**Richer Convolutional Features for Edge Detection**\n\n- intro: CVPR 2017\n- keywords: richer convolutional features (RCF)\n- arxiv: [https://arxiv.org/abs/1612.02103](https://arxiv.org/abs/1612.02103)\n- github: [https://github.com/yun-liu/rcf](https://github.com/yun-liu/rcf)\n\n**Contour Detection from Deep Patch-level Boundary Prediction**\n\n[https://arxiv.org/abs/1705.03159](https://arxiv.org/abs/1705.03159)\n\n**CASENet: Deep Category-Aware Semantic Edge Detection**\n\n- intro: CVPR 2017. CMU & Mitsubishi Electric Research Laboratories (MERL)\n- arxiv: [https://arxiv.org/abs/1705.09759](https://arxiv.org/abs/1705.09759)\n- code: [http://www.merl.com/research/license#CASENet](http://www.merl.com/research/license#CASENet)\n- video: [https://www.youtube.com/watch?v=BNE1hAP6Qho](https://www.youtube.com/watch?v=BNE1hAP6Qho)\n\n**Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1801.00524](https://arxiv.org/abs/1801.00524)\n\n**Deep Crisp Boundaries: From Boundaries to Higher-level Tasks**\n\n[https://arxiv.org/abs/1801.02439](https://arxiv.org/abs/1801.02439)\n\n**DOOBNet: Deep Object Occlusion Boundary Detection from an Image**\n\n[https://arxiv.org/abs/1806.03772](https://arxiv.org/abs/1806.03772)\n\n**Dynamic Feature Fusion for Semantic Edge Detection**\n\n[https://arxiv.org/abs/1902.09104](https://arxiv.org/abs/1902.09104)\n\n**EDTER: Edge Detection with Transformer**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.08566](https://arxiv.org/abs/2203.08566)\n- github: [https://github.com/MengyangPu/EDTER](https://github.com/MengyangPu/EDTER)\n\n# Image Processing\n\n**Fast Image Processing with Fully-Convolutional Networks**\n\n- intro: ICCV 2017. Qifeng Chen (陈启峰)\n- project page: [http://www.cqf.io/ImageProcessing/](http://www.cqf.io/ImageProcessing/)\n- arxiv: [https://arxiv.org/abs/1709.00643](https://arxiv.org/abs/1709.00643)\n- supp: [https://youtu.be/eQyfHgLx8Dc](https://youtu.be/eQyfHgLx8Dc)\n- github: [https://github.com/CQFIO/FastImageProcessing](https://github.com/CQFIO/FastImageProcessing)\n\n**DeepISP: Learning End-to-End Image Processing Pipeline**\n\n[https://arxiv.org/abs/1801.06724](https://arxiv.org/abs/1801.06724)\n\n**Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.04323](https://arxiv.org/abs/1811.04323)\n\n# Image-Text\n\n**Learning Two-Branch Neural Networks for Image-Text Matching Tasks**\n\n[https://arxiv.org/abs/1704.03470](https://arxiv.org/abs/1704.03470)\n\n**Dual-Path Convolutional Image-Text Embedding**\n\n- arxiv: [https://arxiv.org/abs/1711.05535](https://arxiv.org/abs/1711.05535)\n- github: [https://github.com//layumi/Image-Text-Embedding](https://github.com//layumi/Image-Text-Embedding)\n\n**Conditional Image-Text Embedding Networks**\n\n[https://arxiv.org/abs/1711.08389](https://arxiv.org/abs/1711.08389)\n\n**AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1711.10485](https://arxiv.org/abs/1711.10485)\n\n**Stacked Cross Attention for Image-Text Matching**\n\n[https://arxiv.org/abs/1803.08024](https://arxiv.org/abs/1803.08024)\n\n# Age Estimation\n\n**Deeply-Learned Feature for Age Estimation**\n\n- paper: [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7045931&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7045931](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7045931&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7045931)\n\n**Age and Gender Classification using Convolutional Neural Networks**\n\n- paper: [http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf](http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf)\n- project page: [http://www.openu.ac.il/home/hassner/projects/cnn_agegender/](http://www.openu.ac.il/home/hassner/projects/cnn_agegender/)\n- github: [https://github.com/GilLevi/AgeGenderDeepLearning](https://github.com/GilLevi/AgeGenderDeepLearning)\n\n**Group-Aware Deep Feature Learning For Facial Age Estimation**\n\n- paper: [http://www.sciencedirect.com/science/article/pii/S0031320316303417](http://www.sciencedirect.com/science/article/pii/S0031320316303417)\n\n**Local Deep Neural Networks for Age and Gender Classification**\n\n[https://arxiv.org/abs/1703.08497](https://arxiv.org/abs/1703.08497)\n\n**Understanding and Comparing Deep Neural Networks for Age and Gender Classification**\n\n[https://arxiv.org/abs/1708.07689](https://arxiv.org/abs/1708.07689)\n\n**Age Group and Gender Estimation in the Wild with Deep RoR Architecture**\n\n- intro: IEEE ACCESS\n- arxiv: [https://arxiv.org/abs/1710.02985](https://arxiv.org/abs/1710.02985)\n\n**Age and gender estimation based on Convolutional Neural Network and TensorFlow**\n\n[https://github.com/BoyuanJiang/Age-Gender-Estimate-TF](https://github.com/BoyuanJiang/Age-Gender-Estimate-TF)\n\n**Deep Regression Forests for Age Estimation**\n\n- intro: Shanghai University & Johns Hopkins University & Nankai University\n- arxiv: [https://arxiv.org/abs/1712.07195](https://arxiv.org/abs/1712.07195)\n\n# Face Aging\n\n**Recurrent Face Aging**\n\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf)\n\n**Face Aging With Conditional Generative Adversarial Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.01983](https://arxiv.org/abs/1702.01983)\n\n**Learning Face Age Progression: A Pyramid Architecture of GANs**\n\n[https://arxiv.org/abs/1711.10352](https://arxiv.org/abs/1711.10352)\n\n**Face Aging with Contextual Generative Adversarial Nets**\n\n- intro: ACM Multimedia 2017\n- arxiv: [https://arxiv.org/abs/1802.00237](https://arxiv.org/abs/1802.00237)\n\n**Recursive Chaining of Reversible Image-to-image Translators For Face Aging**\n\n[https://arxiv.org/abs/1802.05023](https://arxiv.org/abs/1802.05023)\n\n# Emotion Recognition / Expression Recognition\n\n**Real-time emotion recognition for gaming using deep convolutional network features**\n\n- paper: [http://arxiv.org/abs/1408.3750v1](http://arxiv.org/abs/1408.3750v1)\n- code: [https://github.com/Zebreu/ConvolutionalEmotion](https://github.com/Zebreu/ConvolutionalEmotion)\n\n**Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns**\n\n- project page: [http://www.openu.ac.il/home/hassner/projects/cnn_emotions/](http://www.openu.ac.il/home/hassner/projects/cnn_emotions/)\n- paper: [http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf](http://www.openu.ac.il/home/hassner/projects/cnn_emotions/LeviHassnerICMI15.pdf)\n- github: [https://gist.github.com/GilLevi/54aee1b8b0397721aa4b](https://gist.github.com/GilLevi/54aee1b8b0397721aa4b)\n- blog: [https://gilscvblog.com/2017/01/31/emotion-recognition-in-the-wild-via-convolutional-neural-networks-and-mapped-binary-patterns/](https://gilscvblog.com/2017/01/31/emotion-recognition-in-the-wild-via-convolutional-neural-networks-and-mapped-binary-patterns/)\n\n**DeXpression: Deep Convolutional Neural Network for Expression Recognition**\n\n- paper: [http://arxiv.org/abs/1509.05371](http://arxiv.org/abs/1509.05371)\n\n**DEX: Deep EXpectation of apparent age from a single image**\n\n![](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/img/pipeline.png)\n\n- intro: ICCV 2015\n- paper: [https://www.vision.ee.ethz.ch/en/publications/papers/proceedings/eth_biwi_01229.pdf](https://www.vision.ee.ethz.ch/en/publications/papers/proceedings/eth_biwi_01229.pdf)\n- homepage: [https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/)\n\n**EmotioNet: EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild**\n\n- intro: CVPR 2016\n- paper: [http://cbcsl.ece.ohio-state.edu/cvpr16.pdf](http://cbcsl.ece.ohio-state.edu/cvpr16.pdf)\n- database: [http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html](http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html)\n\n**How Deep Neural Networks Can Improve Emotion Recognition on Video Data**\n\n- intro: ICIP 2016\n- arxiv: [http://arxiv.org/abs/1602.07377](http://arxiv.org/abs/1602.07377)\n\n**Peak-Piloted Deep Network for Facial Expression Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.06997](http://arxiv.org/abs/1607.06997)\n\n**Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution**\n\n- arxiv: [http://arxiv.org/abs/1608.01041](http://arxiv.org/abs/1608.01041)\n\n**A Recursive Framework for Expression Recognition: From Web Images to Deep Models to Game Dataset**\n\n- arxiv: [http://arxiv.org/abs/1608.01647](http://arxiv.org/abs/1608.01647)\n\n**FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition**\n\n- arxiv: [http://arxiv.org/abs/1609.06591](http://arxiv.org/abs/1609.06591)\n\n**EmotionNet Challenge**\n\n- homrepage: [http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html](http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html)\n- dataset: [http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html](http://cbcsl.ece.ohio-state.edu/dbform_emotionet.html)\n\n**Baseline CNN structure analysis for facial expression recognition**\n\n- intro: RO-MAN2016 Conference\n- arxiv: [https://arxiv.org/abs/1611.04251](https://arxiv.org/abs/1611.04251)\n\n**Facial Expression Recognition using Convolutional Neural Networks: State of the Art**\n\n- arxiv: [https://arxiv.org/abs/1612.02903](https://arxiv.org/abs/1612.02903)\n\n**DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional Neural Network**\n\n- arxiv: [https://arxiv.org/abs/1702.04280](https://arxiv.org/abs/1702.04280)\n- api: [https://www.sighthound.com/products/cloud](https://www.sighthound.com/products/cloud)\n\n**Deep generative-contrastive networks for facial expression recognition**\n\n[https://arxiv.org/abs/1703.07140](https://arxiv.org/abs/1703.07140)\n\n**Convolutional Neural Networks for Facial Expression Recognition**\n\n[https://arxiv.org/abs/1704.06756](https://arxiv.org/abs/1704.06756)\n\n**End-to-End Multimodal Emotion Recognition using Deep Neural Networks**\n\n- intro: Imperial College London\n- arxiv: [https://arxiv.org/abs/1704.08619](https://arxiv.org/abs/1704.08619)\n\n**Spatial-Temporal Recurrent Neural Network for Emotion Recognition**\n\n[https://arxiv.org/abs/1705.04515](https://arxiv.org/abs/1705.04515)\n\n**Facial Emotion Detection Using Convolutional Neural Networks and Representational Autoencoder Units**\n\n[https://arxiv.org/abs/1706.01509](https://arxiv.org/abs/1706.01509)\n\n**Temporal Multimodal Fusion for Video Emotion Classification in the Wild**\n\n[https://arxiv.org/abs/1709.07200](https://arxiv.org/abs/1709.07200)\n\n**Island Loss for Learning Discriminative Features in Facial Expression Recognition**\n\n[https://arxiv.org/abs/1710.03144](https://arxiv.org/abs/1710.03144)\n\n**Real-time Convolutional Neural Networks for Emotion and Gender Classification**\n\n[https://arxiv.org/abs/1710.07557](https://arxiv.org/abs/1710.07557)\n\n# Attribution Prediction\n\n**PANDA: Pose Aligned Networks for Deep Attribute Modeling**\n\n- intro: Facebook. CVPR 2014\n- arxiv: [http://arxiv.org/abs/1311.5591](http://arxiv.org/abs/1311.5591)\n- github: [https://github.com/facebook/pose-aligned-deep-networks](https://github.com/facebook/pose-aligned-deep-networks)\n\n**Predicting psychological attributions from face photographs with a deep neural network**\n\n- arxiv: [http://arxiv.org/abs/1512.01289](http://arxiv.org/abs/1512.01289)\n\n**Learning Human Identity from Motion Patterns**\n\n- arxiv: [http://arxiv.org/abs/1511.03908](http://arxiv.org/abs/1511.03908)\n\n# Place Recognition\n\n**NetVLAD: CNN architecture for weakly supervised place recognition**\n\n![](http://www.di.ens.fr/willow/research/netvlad/images/teaser.png)\n\n- intro: CVPR 2016\n- intro: Google Street View Time Machine, soft-assignment, Weakly supervised triplet ranking loss\n- homepage: [http://www.di.ens.fr/willow/research/netvlad/](http://www.di.ens.fr/willow/research/netvlad/)\n- arxiv: [http://arxiv.org/abs/1511.07247](http://arxiv.org/abs/1511.07247)\n\n**PlaNet - Photo Geolocation with Convolutional Neural Networks**\n\n![](https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=590&cx=0&cy=0&cw=928&ch=614)\n\n- arxiv: [http://arxiv.org/abs/1602.05314](http://arxiv.org/abs/1602.05314)\n- review(\"Google Unveils Neural Network with “Superhuman” Ability to Determine the Location of Almost Any Image\"): [https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/](https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/)\n- github(\"City-Recognition: CS231n Project for Winter 2016\"): [https://github.com/dmakian/LittlePlaNet](https://github.com/dmakian/LittlePlaNet)\n- github: [https://github.com/wulfebw/LittlePlaNet-Models](https://github.com/wulfebw/LittlePlaNet-Models)\n\n**Visual place recognition using landmark distribution descriptors**\n\n- arxiv: [http://arxiv.org/abs/1608.04274](http://arxiv.org/abs/1608.04274)\n\n**Low-effort place recognition with WiFi fingerprints using deep learning**\n\n- arxiv: [https://arxiv.org/abs/1611.02049](https://arxiv.org/abs/1611.02049)\n- github: [https://github.com/aqibsaeed/Place-Recognition-using-Autoencoders-and-NN](https://github.com/aqibsaeed/Place-Recognition-using-Autoencoders-and-NN)\n- github(Keras): [https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning](https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning)\n\n**Deep Learning Features at Scale for Visual Place Recognition**\n\n- intro: ICRA 2017\n- arxiv: [https://arxiv.org/abs/1701.05105](https://arxiv.org/abs/1701.05105)\n\n**Place recognition: An Overview of Vision Perspective**\n\n[https://arxiv.org/abs/1707.03470](https://arxiv.org/abs/1707.03470)\n\n## Camera Relocalization\n\n**PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization**\n\n- paper: [http://arxiv.org/abs/1505.07427](http://arxiv.org/abs/1505.07427)\n- project page: [http://mi.eng.cam.ac.uk/projects/relocalisation/#results](http://mi.eng.cam.ac.uk/projects/relocalisation/#results)\n- github: [https://github.com/alexgkendall/caffe-posenet](https://github.com/alexgkendall/caffe-posenet)\n- github(TensorFlow): [https://github.com/kentsommer/tensorflow-posenet](https://github.com/kentsommer/tensorflow-posenet)\n\n**Modelling Uncertainty in Deep Learning for Camera Relocalization**\n\n- paper: [http://arxiv.org/abs/1509.05909](http://arxiv.org/abs/1509.05909)\n\n**Random Forests versus Neural Networks - What's Best for Camera Relocalization?**\n\n- arxiv: [http://arxiv.org/abs/1609.05797](http://arxiv.org/abs/1609.05797)\n\n**Deep Convolutional Neural Network for 6-DOF Image Localization**\n\n- arxiv: [https://arxiv.org/abs/1611.02776](https://arxiv.org/abs/1611.02776)\n\n**DSAC - Differentiable RANSAC for Camera Localization**\n\n- arxiv: [https://arxiv.org/abs/1611.05705](https://arxiv.org/abs/1611.05705)\n\n**Image-based Localization with Spatial LSTMs**\n\n- arxiv: [https://arxiv.org/abs/1611.07890](https://arxiv.org/abs/1611.07890)\n\n**VidLoc: 6-DoF Video-Clip Relocalization**\n\n- arxiv: [https://arxiv.org/abs/1702.06521](https://arxiv.org/abs/1702.06521)\n\n**Towards CNN Map Compression for camera relocalisation**\n\n- arxiv: [https://www.arxiv.org/abs/1703.00845](https://www.arxiv.org/abs/1703.00845)\n\n**Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network**\n\n- intro: Aalto University & Indian Institute of Technology\n- arxiv: [https://arxiv.org/abs/1707.09733](https://arxiv.org/abs/1707.09733)\n\n**MapNet: Geometry-Aware Learning of Maps for Camera Localization**\n\n- intro: Georgia Institute of Technology & NVIDIA\n- arxiv: [https://arxiv.org/abs/1712.03342](https://arxiv.org/abs/1712.03342)\n\n**Image-to-GPS Verification Through A Bottom-Up Pattern Matching Network**\n\n[https://arxiv.org/abs/1811.07288](https://arxiv.org/abs/1811.07288)\n\n# Activity Recognition\n\n**Implementing a CNN for Human Activity Recognition in Tensorflow**\n\n- blog: [http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/](http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/)\n- github: [https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN](https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN)\n\n**Concurrent Activity Recognition with Multimodal CNN-LSTM Structure**\n\n- arxiv: [https://arxiv.org/abs/1702.01638](https://arxiv.org/abs/1702.01638)\n\n**CERN: Confidence-Energy Recurrent Network for Group Activity Recognition**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.03058](https://arxiv.org/abs/1704.03058)\n\n**Deploying Tensorflow model on Andorid device for Human Activity Recognition**\n\n- blog: [http://aqibsaeed.github.io/2017-05-02-deploying-tensorflow-model-andorid-device-human-activity-recognition/](http://aqibsaeed.github.io/2017-05-02-deploying-tensorflow-model-andorid-device-human-activity-recognition/)\n- github: [https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/tree/master/ActivityRecognition](https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/tree/master/ActivityRecognition)\n\n# Music Classification / Sound Classification\n\n**Explaining Deep Convolutional Neural Networks on Music Classification**\n\n- arxiv: [http://arxiv.org/abs/1607.02444](http://arxiv.org/abs/1607.02444)\n- blog: [https://keunwoochoi.wordpress.com/2015/12/09/ismir-2015-lbd-auralisation-of-deep-convolutional-neural-networks-listening-to-learned-features-auralization/](https://keunwoochoi.wordpress.com/2015/12/09/ismir-2015-lbd-auralisation-of-deep-convolutional-neural-networks-listening-to-learned-features-auralization/)\n- blog: [https://keunwoochoi.wordpress.com/2016/03/23/what-cnns-see-when-cnns-see-spectrograms/](https://keunwoochoi.wordpress.com/2016/03/23/what-cnns-see-when-cnns-see-spectrograms/)\n- github: [https://github.com/keunwoochoi/Auralisation](https://github.com/keunwoochoi/Auralisation)\n- audio samples: [https://soundcloud.com/kchoi-research](https://soundcloud.com/kchoi-research)\n\n**Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification**\n\n- project page: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html)\n- arxiv: [http://arxiv.org/abs/1608.04363](http://arxiv.org/abs/1608.04363)\n\n**Convolutional Recurrent Neural Networks for Music Classification**\n\n![](https://keunwoochoi.files.wordpress.com/2016/09/screen-shot-2016-09-14-at-20-38-27.png?w=1200)\n\n- arxiv: [http://arxiv.org/abs/1609.04243](http://arxiv.org/abs/1609.04243)\n- blog: [https://keunwoochoi.wordpress.com/2016/09/15/paper-is-out-convolutional-recurrent-neural-networks-for-music-classification/](https://keunwoochoi.wordpress.com/2016/09/15/paper-is-out-convolutional-recurrent-neural-networks-for-music-classification/)\n- github: [https://github.com/keunwoochoi/music-auto_tagging-keras](https://github.com/keunwoochoi/music-auto_tagging-keras)\n\n**CNN Architectures for Large-Scale Audio Classification**\n\n- intro: Google\n- arxiv: [https://arxiv.org/abs/1609.09430](https://arxiv.org/abs/1609.09430)\n- demo: [https://www.youtube.com/watch?v=oAAo_r7ZT8U&feature=youtu.be](https://www.youtube.com/watch?v=oAAo_r7ZT8U&feature=youtu.be)\n\n**SoundNet: Learning Sound Representations from Unlabeled Video**\n\n- intro: MIT. NIPS 2016\n- project page: [http://projects.csail.mit.edu/soundnet/](http://projects.csail.mit.edu/soundnet/)\n- arxiv: [https://arxiv.org/abs/1610.09001](https://arxiv.org/abs/1610.09001)\n- paper: [http://web.mit.edu/vondrick/soundnet.pdf](http://web.mit.edu/vondrick/soundnet.pdf)\n- github: [https://github.com/cvondrick/soundnet](https://github.com/cvondrick/soundnet)\n- github: [https://github.com/eborboihuc/SoundNet-tensorflow](https://github.com/eborboihuc/SoundNet-tensorflow)\n- youtube: [https://www.youtube.com/watch?v=yJCjVvIY4dU](https://www.youtube.com/watch?v=yJCjVvIY4dU)\n\n**Deep Learning 'ahem' detector**\n\n- github: [https://github.com/worldofpiggy/deeplearning-ahem-detector](https://github.com/worldofpiggy/deeplearning-ahem-detector)\n- slides: [https://docs.google.com/presentation/d/1QXQEOiAMj0uF2_Gafr2bn-kMniUJAIM1PLTFm1mUops/edit#slide=id.g35f391192_00](https://docs.google.com/presentation/d/1QXQEOiAMj0uF2_Gafr2bn-kMniUJAIM1PLTFm1mUops/edit#slide=id.g35f391192_00)\n- mirror: [https://pan.baidu.com/s/1c2KGlwO](https://pan.baidu.com/s/1c2KGlwO)\n\n**GenreFromAudio: Finding the genre of a song with Deep Learning**\n\n- intro: A pipeline to build a dataset from your own music library and use it to fill the missing genres\n- github: [https://github.com/despoisj/DeepAudioClassification](https://github.com/despoisj/DeepAudioClassification)\n\n**TS-LSTM and Temporal-Inception: Exploiting Spatiotemporal Dynamics for Activity Recognition**\n\n- arxiv: [https://arxiv.org/abs/1703.10667](https://arxiv.org/abs/1703.10667)\n- github: [https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN](https://github.com/chihyaoma/Activity-Recognition-with-CNN-and-RNN)\n\n**On the Robustness of Deep Convolutional Neural Networks for Music Classification**\n\n- intro: Queen Mary University of London & New York University\n- arxiv: [https://arxiv.org/abs/1706.02361](https://arxiv.org/abs/1706.02361)\n\n# NSFW Detection / Classification\n\n**Nipple Detection using Convolutional Neural Network**\n\n- reddit: [https://www.reddit.com/over18?dest=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F33n77s%2Fandroid_app_nipple_detection_using_convolutional%2F](https://www.reddit.com/over18?dest=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F33n77s%2Fandroid_app_nipple_detection_using_convolutional%2F)\n\n**Applying deep learning to classify pornographic images and videos**\n\n- arxiv: [http://arxiv.org/abs/1511.08899](http://arxiv.org/abs/1511.08899)\n\n**MODERATE, FILTER, OR CURATE ADULT CONTENT WITH CLARIFAI’S NSFW MODEL**\n\n- blog: [http://blog.clarifai.com/moderate-filter-or-curate-adult-content-with-clarifais-nsfw-model/#.VzVhM-yECZY](http://blog.clarifai.com/moderate-filter-or-curate-adult-content-with-clarifais-nsfw-model/#.VzVhM-yECZY)\n\n**WHAT CONVOLUTIONAL NEURAL NETWORKS LOOK AT WHEN THEY SEE NUDITY**\n\n- blog: [http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity#.VzVh_-yECZY](http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity#.VzVh_-yECZY)\n\n**Open Sourcing a Deep Learning Solution for Detecting NSFW Images**\n\n- intro: Yahoo\n- blog: [https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for](https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for)\n- github: [https://github.com/yahoo/open_nsfw](https://github.com/yahoo/open_nsfw)\n\n**Miles Deep - AI Porn Video Editor**\n\n- intro: Deep Learning Porn Video Classifier/Editor with Caffe\n- github: [https://github.com/ryanjay0/miles-deep](https://github.com/ryanjay0/miles-deep)\n\n# Image Reconstruction / Inpainting\n\n**Context Encoders: Feature Learning by Inpainting**\n\n![](http://www.cs.berkeley.edu/~pathak/context_encoder/resources/result_fig.jpg)\n\n- intro: CVPR 2016\n- intro: Unsupervised Feature Learning by Image Inpainting using GANs\n- project page: [http://www.cs.berkeley.edu/~pathak/context_encoder/](http://www.cs.berkeley.edu/~pathak/context_encoder/)\n- arxiv: [https://arxiv.org/abs/1604.07379](https://arxiv.org/abs/1604.07379)\n- github(official): [https://github.com/pathak22/context-encoder](https://github.com/pathak22/context-encoder)\n- github: [https://github.com/BoyuanJiang/context_encoder_pytorch](https://github.com/BoyuanJiang/context_encoder_pytorch)\n\n**Semantic Image Inpainting with Perceptual and Contextual Losses**\n\n**Semantic Image Inpainting with Deep Generative Models**\n\n- keywords: Deep Convolutional Generative Adversarial Network (DCGAN)\n- arxiv: [http://arxiv.org/abs/1607.07539](http://arxiv.org/abs/1607.07539)\n- github: [https://github.com/bamos/dcgan-completion.tensorflow](https://github.com/bamos/dcgan-completion.tensorflow)\n\n**High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis**\n\n- intro: University of Southern California & Adobe Research\n- arxiv: [https://arxiv.org/abs/1611.09969](https://arxiv.org/abs/1611.09969)\n\n**Face Image Reconstruction from Deep Templates**\n\n[https://www.arxiv.org/abs/1703.00832](https://www.arxiv.org/abs/1703.00832)\n\n**Deep Learning-Guided Image Reconstruction from Incomplete Data**\n\n[https://arxiv.org/abs/1709.00584](https://arxiv.org/abs/1709.00584)\n\n**Image Inpainting using Multi-Scale Feature Image Translation**\n\n[https://arxiv.org/abs/1711.08590](https://arxiv.org/abs/1711.08590)\n\n**Image Inpainting for High-Resolution Textures using CNN Texture Synthesis**\n\n[https://arxiv.org/abs/1712.03111](https://arxiv.org/abs/1712.03111)\n\n**Context-Aware Semantic Inpainting**\n\n[https://arxiv.org/abs/1712.07778](https://arxiv.org/abs/1712.07778)\n\n**Deep Blind Image Inpainting**\n\n[https://arxiv.org/abs/1712.09078](https://arxiv.org/abs/1712.09078)\n\n**Deep Stacked Networks with Residual Polishing for Image Inpainting**\n\n[https://arxiv.org/abs/1801.00289](https://arxiv.org/abs/1801.00289)\n\n**Light-weight pixel context encoders for image inpainting**\n\n[https://arxiv.org/abs/1801.05585](https://arxiv.org/abs/1801.05585)\n\n**Deep Structured Energy-Based Image Inpainting**\n\n[https://arxiv.org/abs/1801.07939](https://arxiv.org/abs/1801.07939)\n\n**Shift-Net: Image Inpainting via Deep Feature Rearrangement**\n\n[https://arxiv.org/abs/1801.09392](https://arxiv.org/abs/1801.09392)\n\n**Cascade context encoder for improved inpainting**\n\n[https://arxiv.org/abs/1803.04033](https://arxiv.org/abs/1803.04033)\n\n**SPG-Net: Segmentation Prediction and Guidance Network for Image Inpainting**\n\n- intro: University of Southern California & Baidu Research\n- arxiv: [https://arxiv.org/abs/1805.03356](https://arxiv.org/abs/1805.03356)\n\n**Free-Form Image Inpainting with Gated Convolution**\n\n[https://arxiv.org/abs/1806.03589](https://arxiv.org/abs/1806.03589)\n\n**Keras implementation of Image OutPainting**\n\n- intro: Stanford CS230 project\n- paper: [https://cs230.stanford.edu/projects_spring_2018/posters/8265861.pdf](https://cs230.stanford.edu/projects_spring_2018/posters/8265861.pdf)\n- github: [https://github.com/bendangnuksung/Image-OutPainting](https://github.com/bendangnuksung/Image-OutPainting)\n\n**Image Inpainting via Generative Multi-column Convolutional Neural Networks**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.08771](https://arxiv.org/abs/1810.08771)\n\n**Deep Inception Generative Network for Cognitive Image Inpainting**\n\n[https://arxiv.org/abs/1812.01458](https://arxiv.org/abs/1812.01458)\n\n**Foreground-aware Image Inpainting**\n\n- intro: University of Rochester & University of Illinois at Urbana-Champaign & Adobe Research\n- arxiv: [https://arxiv.org/abs/1901.05945](https://arxiv.org/abs/1901.05945)\n\n# Image Restoration\n\n**Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections**\n\n- intro: NIPS 2016\n- arxiv: [http://arxiv.org/abs/1603.09056](http://arxiv.org/abs/1603.09056)\n\n**Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections**\n\n- arxiv: [http://arxiv.org/abs/1606.08921](http://arxiv.org/abs/1606.08921)\n\n**Image Completion with Deep Learning in TensorFlow**\n\n- blog: [http://bamos.github.io/2016/08/09/deep-completion/](http://bamos.github.io/2016/08/09/deep-completion/)\n\n**Deeply Aggregated Alternating Minimization for Image Restoration**\n\n- arxiv: [https://arxiv.org/abs/1612.06508](https://arxiv.org/abs/1612.06508)\n\n**A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction**\n\n- intro: Seoul National University\n- arxiv: [https://arxiv.org/abs/1701.06190](https://arxiv.org/abs/1701.06190)\n\n**MemNet: A Persistent Memory Network for Image Restoration**\n\n- intro: ICCV 2017 (Spotlight presentation)\n- arxiv: [https://arxiv.org/abs/1708.02209](https://arxiv.org/abs/1708.02209)\n- github: [https://github.com/tyshiwo/MemNet](https://github.com/tyshiwo/MemNet)\n\n**Deep Mean-Shift Priors for Image Restoration**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1709.03749](https://arxiv.org/abs/1709.03749)\n\n**xUnit: Learning a Spatial Activation Function for Efficient Image Restoration**\n\n- arxiv: [https://arxiv.org/abs/1711.06445](https://arxiv.org/abs/1711.06445)\n- github: [https://github.com/kligvasser/xUnit](https://github.com/kligvasser/xUnit)\n\n**Deep Image Prior**\n\n- intro: Skolkovo Institute of Science and Technology & University of Oxford\n- project page: [https://dmitryulyanov.github.io/deep_image_prior](https://dmitryulyanov.github.io/deep_image_prior)\n- arxiv: [https://arxiv.org/abs/1711.10925](https://arxiv.org/abs/1711.10925)\n- paper: [https://sites.skoltech.ru/app/data/uploads/sites/25/2017/11/deep_image_prior.pdf](https://sites.skoltech.ru/app/data/uploads/sites/25/2017/11/deep_image_prior.pdf)\n- github: [https://github.com//DmitryUlyanov/deep-image-prior](https://github.com//DmitryUlyanov/deep-image-prior)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/7gls3j/r_deep_image_prior_deep_superresolution/](https://www.reddit.com/r/MachineLearning/comments/7gls3j/r_deep_image_prior_deep_superresolution/)\n\n**MemNet: A Persistent Memory Network for Image Restoration**\n\n- intro: ICCV 2017 spotlight\n- paper: [http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf](http://cvlab.cse.msu.edu/pdfs/Image_Restoration%20using_Persistent_Memory_Network.pdf)\n- github: [https://github.com//tyshiwo/MemNet](https://github.com//tyshiwo/MemNet)\n\n**Denoising Prior Driven Deep Neural Network for Image Restoration**\n\n[https://arxiv.org/abs/1801.06756](https://arxiv.org/abs/1801.06756)\n\n**Globally and Locally Consistent Image Completion**\n\n- intro: SIGGRAPH 2017\n- project page: [http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/](http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/)\n- paper: [http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf](http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf)\n- github(official): [https://github.com/satoshiiizuka/siggraph2017_inpainting](https://github.com/satoshiiizuka/siggraph2017_inpainting)\n- github: [https://github.com/akmtn/pytorch-siggraph2017-inpainting](https://github.com/akmtn/pytorch-siggraph2017-inpainting)\n\n**Multi-level Wavelet-CNN for Image Restoration**\n\n- intro: CVPR 2018 NTIRE Workshop\n- arxiv: [https://arxiv.org/abs/1805.07071](https://arxiv.org/abs/1805.07071)\n\n**Non-Local Recurrent Network for Image Restoration**\n\n- intro: University of Illinois at Urbana-Champaign & The Chinese University of Hong Kong\n- arxiv: [https://arxiv.org/abs/1806.02919](https://arxiv.org/abs/1806.02919)\n\n**Residual Non-local Attention Networks for Image Restoration**\n\n- intro: ICLR 2019\n- arxiv: [https://arxiv.org/abs/1903.10082](https://arxiv.org/abs/1903.10082)\n\n## Face Completion\n\n**Generative Face Completion**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1704.05838](https://arxiv.org/abs/1704.05838)\n\n**High Resolution Face Completion with Multiple Controllable Attributes via Fully End-to-End Progressive Generative Adversarial Networks**\n\n- intro: North Carolina State University\n- arxiv: [https://arxiv.org/abs/1801.07632](https://arxiv.org/abs/1801.07632)\n\n# Image Denoising\n\n**Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising**\n\n- arxiv: [http://arxiv.org/abs/1608.03981](http://arxiv.org/abs/1608.03981)\n- github: [https://github.com/cszn/DnCNN](https://github.com/cszn/DnCNN)\n\n**Medical image denoising using convolutional denoising autoencoders**\n\n- arxiv: [http://arxiv.org/abs/1608.04667](http://arxiv.org/abs/1608.04667)\n\n**Rectifier Neural Network with a Dual-Pathway Architecture for Image Denoising**\n\n- arxiv: [http://arxiv.org/abs/1609.03024](http://arxiv.org/abs/1609.03024)\n\n**Non-Local Color Image Denoising with Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.06757](https://arxiv.org/abs/1611.06757)\n\n**Joint Visual Denoising and Classification using Deep Learning**\n\n- intro: ICIP 2016\n- arxiv: [https://arxiv.org/abs/1612.01075](https://arxiv.org/abs/1612.01075)\n- github: [https://github.com/ganggit/jointmodel](https://github.com/ganggit/jointmodel)\n\n**Deep Convolutional Denoising of Low-Light Images**\n\n- arxiv: [https://arxiv.org/abs/1701.01687](https://arxiv.org/abs/1701.01687)\n\n**Deep Class Aware Denoising**\n\n- arxiv: [https://arxiv.org/abs/1701.01698](https://arxiv.org/abs/1701.01698)\n\n**End-to-End Learning for Structured Prediction Energy Networks**\n\n- intro: University of Massachusetts & CMU\n- arxiv: [https://arxiv.org/abs/1703.05667](https://arxiv.org/abs/1703.05667)\n\n**Block-Matching Convolutional Neural Network for Image Denoising**\n\n[https://arxiv.org/abs/1704.00524](https://arxiv.org/abs/1704.00524)\n\n**When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach**\n\n[https://arxiv.org/abs/1706.04284](https://arxiv.org/abs/1706.04284)\n\n**Wide Inference Network for Image Denoising**\n\n[https://arxiv.org/abs/1707.05414](hmttps://arxiv.org/abs/1707.05414)\n\n**Learning Pixel-Distribution Prior with Wider Convolution for Image Denoising**\n\n- arxiv: [https://arxiv.org/abs/1707.09135](https://arxiv.org/abs/1707.09135)\n- github(MatConvNet): [https://github.com/cswin/WIN](https://github.com/cswin/WIN)\n\n**Image Denoising via CNNs: An Adversarial Approach**\n\n- intro: Indian Institute of Science\n- arxiv: [https://arxiv.org/abs/1708.00159](https://arxiv.org/abs/1708.00159)\n\n**An ELU Network with Total Variation for Image Denoising**\n\n- intro: 24th International Conference on Neural Information Processing (2017)\n- arxiv: [https://arxiv.org/abs/1708.04317](https://arxiv.org/abs/1708.04317)\n\n**Dilated Residual Network for Image Denoising**\n\n[https://arxiv.org/abs/1708.05473](https://arxiv.org/abs/1708.05473)\n\n**FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising**\n\n- arxiv: [https://arxiv.org/abs/1710.04026](https://arxiv.org/abs/1710.04026)\n- github(MatConvNet): [https://github.com/cszn/FFDNet](https://github.com/cszn/FFDNet)\n\n**Universal Denoising Networks : A Novel CNN-based Network Architecture for Image Denoising**\n\n[https://arxiv.org/abs/1711.07807](https://arxiv.org/abs/1711.07807)\n\n**Burst Denoising with Kernel Prediction Networks**\n\n- project page: [http://people.eecs.berkeley.edu/~bmild/kpn/](http://people.eecs.berkeley.edu/~bmild/kpn/)\n- arxiv: [https://arxiv.org/abs/1712.02327](https://arxiv.org/abs/1712.02327)\n\n**Chaining Identity Mapping Modules for Image Denoising**\n\n[https://arxiv.org/abs/1712.02933](https://arxiv.org/abs/1712.02933)\n\n**Deep Burst Denoising**\n\n[https://arxiv.org/abs/1712.05790](https://arxiv.org/abs/1712.05790)\n\n**Fast, Trainable, Multiscale Denoising**\n\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/1802.06130](https://arxiv.org/abs/1802.06130)\n\n**Training Deep Learning based Denoisers without Ground Truth Data**\n\n[https://arxiv.org/abs/1803.01314](https://arxiv.org/abs/1803.01314)\n\n**Identifying Recurring Patterns with Deep Neural Networks for Natural Image Denoising**\n\n[https://arxiv.org/abs/1806.05229](https://arxiv.org/abs/1806.05229)\n\n**Class-Aware Fully-Convolutional Gaussian and Poisson Denoising**\n\n[https://arxiv.org/abs/1808.06562](https://arxiv.org/abs/1808.06562)\n\n**Connecting Image Denoising and High-Level Vision Tasks via Deep Learning**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1809.01826](https://arxiv.org/abs/1809.01826)\n- github: [https://github.com/Ding-Liu/DeepDenoising](https://github.com/Ding-Liu/DeepDenoising)\n\n**DN-ResNet: Efficient Deep Residual Network for Image Denoising**\n\n[https://arxiv.org/abs/1810.06766](https://arxiv.org/abs/1810.06766)\n\n**Deep Learning for Image Denoising: A Survey**\n\n[https://arxiv.org/abs/1810.05052](https://arxiv.org/abs/1810.05052)\n\n# Image Dehazing / Image Haze Removal\n\n**DehazeNet: An End-to-End System for Single Image Haze Removal**\n\n- arxiv: [http://arxiv.org/abs/1601.07661](http://arxiv.org/abs/1601.07661)\n\n**An All-in-One Network for Dehazing and Beyond**\n\n- intro: All-in-One Dehazing Network (AOD-Net)\n- arxiv: [https://arxiv.org/abs/1707.06543](https://arxiv.org/abs/1707.06543)\n\n**Joint Transmission Map Estimation and Dehazing using Deep Networks**\n\n[https://arxiv.org/abs/1708.00581](https://arxiv.org/abs/1708.00581)\n\n**End-to-End United Video Dehazing and Detection**\n\n[https://arxiv.org/abs/1709.03919](https://arxiv.org/abs/1709.03919)\n\n**Image Dehazing using Bilinear Composition Loss Function**\n\n[https://arxiv.org/abs/1710.00279](https://arxiv.org/abs/1710.00279)\n\n**Learning Aggregated Transmission Propagation Networks for Haze Removal and Beyond**\n\n[https://arxiv.org/abs/1711.06787](https://arxiv.org/abs/1711.06787)\n\n**CANDY: Conditional Adversarial Networks based Fully End-to-End System for Single Image Haze Removal**\n\n[https://arxiv.org/abs/1801.02892](https://arxiv.org/abs/1801.02892)\n\n**C2MSNet: A Novel approach for single image haze removal**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.08406](https://arxiv.org/abs/1801.08406)\n\n**A Cascaded Convolutional Neural Network for Single Image Dehazing**\n\n- intro: IEEE ACCESS\n- arxiv: [https://arxiv.org/abs/1803.07955](https://arxiv.org/abs/1803.07955)\n\n**Densely Connected Pyramid Dehazing Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.08396](https://arxiv.org/abs/1803.08396)\n- github: [https://github.com/hezhangsprinter/DCPDN](https://github.com/hezhangsprinter/DCPDN)\n\n**Gated Fusion Network for Single Image Dehazing**\n\n- project page: [https://sites.google.com/site/renwenqi888/research/dehazing/gfn](https://sites.google.com/site/renwenqi888/research/dehazing/gfn)\n- arxiv: [https://arxiv.org/abs/1804.00213](https://arxiv.org/abs/1804.00213)\n\n**Semantic Single-Image Dehazing**\n\n[https://arxiv.org/abs/1804.05624](https://arxiv.org/abs/1804.05624)\n\n**Perceptually Optimized Generative Adversarial Network for Single Image Dehazing**\n\n[https://arxiv.org/abs/1805.01084](https://arxiv.org/abs/1805.01084)\n\n**PAD-Net: A Perception-Aided Single Image Dehazing Network**\n\n- arxiv: [https://arxiv.org/abs/1805.03146](https://arxiv.org/abs/1805.03146)\n- github: [https://github.com/guanlongzhao/single-image-dehazing](https://github.com/guanlongzhao/single-image-dehazing)\n\n**The Effectiveness of Instance Normalization: a Strong Baseline for Single Image Dehazing**\n\n[https://arxiv.org/abs/1805.03305](https://arxiv.org/abs/1805.03305)\n\n**Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing**\n\n- intro: CVPRW: NTIRE 2018\n- arxiv: [https://arxiv.org/abs/1805.05308](https://arxiv.org/abs/1805.05308)\n\n**Deep learning for dehazing: Comparison and analysis**\n\n- intro: CVCS 2018\n- arxiv: [https://arxiv.org/abs/1806.10923](https://arxiv.org/abs/1806.10923)\n\n**Generic Model-Agnostic Convolutional Neural Network for Single Image Dehazing**\n\n[https://arxiv.org/abs/1810.02862](https://arxiv.org/abs/1810.02862)\n\n# Image Rain Removal / De-raining\n\n**Clearing the Skies: A deep network architecture for single-image rain removal**\n\n- intro: DerainNet\n- project page: [http://smartdsp.xmu.edu.cn/derainNet.html](http://smartdsp.xmu.edu.cn/derainNet.html)\n- arxiv: [http://arxiv.org/abs/1609.02087](http://arxiv.org/abs/1609.02087)\n- code(Matlab): [http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/derainNet/code.zip](http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/derainNet/code.zip)\n\n**Joint Rain Detection and Removal via Iterative Region Dependent Multi-Task Learning**\n\n- arxiv: [http://arxiv.org/abs/1609.07769](http://arxiv.org/abs/1609.07769)\n\n**Image De-raining Using a Conditional Generative Adversarial Network**\n\n- arxiv: [https://arxiv.org/abs/1701.05957](https://arxiv.org/abs/1701.05957)\n\n**Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1712.06830](https://arxiv.org/abs/1712.06830)\n\n**Deep joint rain and haze removal from single images**\n\n[https://arxiv.org/abs/1801.06769](https://arxiv.org/abs/1801.06769)\n\n**Density-aware Single Image De-raining using a Multi-stream Dense Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.08396](https://arxiv.org/abs/1803.08396)\n- github: [https://github.com/hezhangsprinter/DID-MDN](https://github.com/hezhangsprinter/DID-MDN)\n\n**Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework**\n\n[https://arxiv.org/abs/1803.10433](https://arxiv.org/abs/1803.10433)\n\n**Fast Single Image Rain Removal via a Deep Decomposition-Composition Network**\n\n[https://arxiv.org/abs/1804.02688](https://arxiv.org/abs/1804.02688)\n\n**Residual-Guide Feature Fusion Network for Single Image Deraining**\n\n[https://arxiv.org/abs/1804.07493](https://arxiv.org/abs/1804.07493)\n\n**Lightweight Pyramid Networks for Image Deraining**\n\n[https://arxiv.org/abs/1805.06173](https://arxiv.org/abs/1805.06173)\n\n**Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.05698](https://arxiv.org/abs/1807.05698)\n- code: [https://xialipku.github.io/RESCAN/](https://xialipku.github.io/RESCAN/)\n\n**Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining**\n\n- intro: ACM Multimedia 2018\n- arxiv: [https://arxiv.org/abs/1808.01491](https://arxiv.org/abs/1808.01491)\n\n**Gated Context Aggregation Network for Image Dehazing and Deraining**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1811.08747](https://arxiv.org/abs/1811.08747)\n\n**A Deep Tree-Structured Fusion Model for Single Image Deraining**\n\n[https://arxiv.org/abs/1811.08632](https://arxiv.org/abs/1811.08632)\n\n**A^2Net: Adjacent Aggregation Networks for Image Raindrop Removal**\n\n[https://arxiv.org/abs/1811.09780](https://arxiv.org/abs/1811.09780)\n\n**Single Image Deraining: A Comprehensive Benchmark Analysis**\n\n- arxiv: [https://arxiv.org/abs/1903.08558](https://arxiv.org/abs/1903.08558)\n- github: [https://github.com/lsy17096535/Single-Image-Deraining](https://github.com/lsy17096535/Single-Image-Deraining)\n\n**Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset**\n\n- intro: CVPR 2019\n- project pge: [https://stevewongv.github.io/derain-project.html](https://stevewongv.github.io/derain-project.html)\n- arxiv: [https://arxiv.org/abs/1904.01538](https://arxiv.org/abs/1904.01538)\n\n# Fence Removal\n\n**My camera can see through fences: A deep learning approach for image de-fencing**\n\n- intro: ACPR 2015\n- arxiv: [https://arxiv.org/abs/1805.07442](https://arxiv.org/abs/1805.07442)\n\n**Deep learning based fence segmentation and removal from an image using a video sequence**\n\n- intro: ECCV Workshop on Video Segmentation, 2016\n- arxiv: [http://arxiv.org/abs/1609.07727](http://arxiv.org/abs/1609.07727)\n\n**Accurate and efficient video de-fencing using convolutional neural networks and temporal information**\n\n[https://arxiv.org/abs/1806.10781](https://arxiv.org/abs/1806.10781)\n\n# Snow Removal\n\n**DesnowNet: Context-Aware Deep Network for Snow Removal**\n\n[https://arxiv.org/abs/1708.04512](https://arxiv.org/abs/1708.04512)\n\n# Blur Detection and Removal\n\n**Learning to Deblur**\n\n- arxiv: [http://arxiv.org/abs/1406.7444](http://arxiv.org/abs/1406.7444)\n\n**Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal**\n\n- arxiv: [http://arxiv.org/abs/1503.00593](http://arxiv.org/abs/1503.00593)\n\n**End-to-End Learning for Image Burst Deblurring**\n\n- arxiv: [http://arxiv.org/abs/1607.04433](http://arxiv.org/abs/1607.04433)\n\n**Deep Video Deblurring**\n\n- intro: CVPR 2017 spotlight paper\n- project page(code+dataset): [http://www.cs.ubc.ca/labs/imager/tr/2017/DeepVideoDeblurring/](http://www.cs.ubc.ca/labs/imager/tr/2017/DeepVideoDeblurring/)\n- arxiv: [https://arxiv.org/abs/1611.08387](https://arxiv.org/abs/1611.08387)\n[https://github.com/shuochsu/DeepVideoDeblurring](https://github.com/shuochsu/DeepVideoDeblurring)\n\n**Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring**\n\n- arxiv: [https://arxiv.org/abs/1612.02177](https://arxiv.org/abs/1612.02177)\n- github(official. Torch)): [https://github.com/SeungjunNah/DeepDeblur_release](https://github.com/SeungjunNah/DeepDeblur_release)\n\n**From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur**\n\n- arxiv: [https://arxiv.org/abs/1612.02583](https://arxiv.org/abs/1612.02583)\n\n**Motion Deblurring in the Wild**\n\n- arxiv: [https://arxiv.org/abs/1701.01486](https://arxiv.org/abs/1701.01486)\n\n**Deep Face Deblurring**\n\n[https://arxiv.org/abs/1704.08772](https://arxiv.org/abs/1704.08772)\n\n**Learning Blind Motion Deblurring**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.04208](https://arxiv.org/abs/1708.04208)\n\n**Deep Generative Filter for Motion Deblurring**\n\n[https://arxiv.org/abs/1709.03481](https://arxiv.org/abs/1709.03481)\n\n**DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks**\n\n- intro: Ukrainian Catholic University & CTU in Prague\n- arxiv: [https://arxiv.org/abs/1711.07064](https://arxiv.org/abs/1711.07064)\n- github: [https://github.com/KupynOrest/DeblurGAN](https://github.com/KupynOrest/DeblurGAN)\n\n**DeepDeblur: Fast one-step blurry face images restoration**\n\n[https://arxiv.org/abs/1711.09515](https://arxiv.org/abs/1711.09515)\n\n**Reblur2Deblur: Deblurring Videos via Self-Supervised Learning**\n\n- arxiv: [https://arxiv.org/abs/1801.05117](https://arxiv.org/abs/1801.05117)\n- supplementary: [https://drive.google.com/file/d/17Itta-z89lpWUdvUjpafKzJRSLoxHF5c/view](https://drive.google.com/file/d/17Itta-z89lpWUdvUjpafKzJRSLoxHF5c/view)\n\n**Scale-recurrent Network for Deep Image Deblurring**\n\n- intro: CUHK & Tecent & Megvii Inc.\n- arxiv: [https://arxiv.org/abs/1802.01770](https://arxiv.org/abs/1802.01770)\n\n**Deep Semantic Face Deblurring**\n\n- intro: CVPR 2018. Beijing Institute of Technology & University of California, Merced & Nvidia Research\n- project page: [https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur](https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur)\n- arxiv: [https://arxiv.org/abs/1803.03345](https://arxiv.org/abs/1803.03345)\n\n**Motion deblurring of faces**\n\n[https://arxiv.org/abs/1803.03330](https://arxiv.org/abs/1803.03330)\n\n**Learning a Discriminative Prior for Blind Image Deblurring**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.03363](https://arxiv.org/abs/1803.03363)\n\n**Adversarial Spatio-Temporal Learning for Video Deblurring**\n\n[https://arxiv.org/abs/1804.00533](https://arxiv.org/abs/1804.00533)\n\n**Learning to Deblur Images with Exemplars**\n\n- intro: PAMI 2018\n- arxiv: [https://arxiv.org/abs/1805.05503](https://arxiv.org/abs/1805.05503)\n\n**Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networks for Non-Uniform Single Image Deblurring**\n\n[https://arxiv.org/abs/1903.10157](https://arxiv.org/abs/1903.10157)\n\n# Image Compression\n\n**An image compression and encryption scheme based on deep learning**\n\n- arxiv: [http://arxiv.org/abs/1608.05001](http://arxiv.org/abs/1608.05001)\n\n**Full Resolution Image Compression with Recurrent Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1608.05148](http://arxiv.org/abs/1608.05148)\n- github: [https://github.com/tensorflow/models/tree/master/compression](https://github.com/tensorflow/models/tree/master/compression)\n\n**Image Compression with Neural Networks**\n\n- blog: [https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html](https://research.googleblog.com/2016/09/image-compression-with-neural-networks.html)\n\n**Lossy Image Compression With Compressive Autoencoders**\n\n- paper: [http://openreview.net/pdf?id=rJiNwv9gg](http://openreview.net/pdf?id=rJiNwv9gg)\n- review: [http://qz.com/835569/twitter-is-getting-close-to-making-all-your-pictures-just-a-little-bit-smaller/](http://qz.com/835569/twitter-is-getting-close-to-making-all-your-pictures-just-a-little-bit-smaller/)\n\n**End-to-end Optimized Image Compression**\n\n- arxiv: [https://arxiv.org/abs/1611.01704](https://arxiv.org/abs/1611.01704)\n- notes: [https://blog.acolyer.org/2017/05/08/end-to-end-optimized-image-compression/](https://blog.acolyer.org/2017/05/08/end-to-end-optimized-image-compression/)\n\n**CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression**\n\n- arxiv: [https://arxiv.org/abs/1611.07233](https://arxiv.org/abs/1611.07233)\n\n**Semantic Perceptual Image Compression using Deep Convolution Networks**\n\n- intro: Accepted to Data Compression Conference\n- intro: Semantic JPEG image compression using deep convolutional neural network (CNN)\n- arxiv: [https://arxiv.org/abs/1612.08712](https://arxiv.org/abs/1612.08712)\n- github: [https://github.com/iamaaditya/image-compression-cnn](https://github.com/iamaaditya/image-compression-cnn)\n\n**Generative Compression**\n\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1703.01467](https://arxiv.org/abs/1703.01467)\n\n**Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks**\n\n[https://arxiv.org/abs/1703.10114](https://arxiv.org/abs/1703.10114)\n\n**Learning Convolutional Networks for Content-weighted Image Compression**\n\n[https://arxiv.org/abs/1703.10553](https://arxiv.org/abs/1703.10553)\n\n**Real-Time Adaptive Image Compression**\n\n- intro: ICML 2017\n- keywords: GAN\n- project page: [http://www.wave.one/icml2017](http://www.wave.one/icml2017)\n- arxiv: [https://arxiv.org/abs/1705.05823](https://arxiv.org/abs/1705.05823)\n\n**Learning to Inpaint for Image Compression**\n\n[https://arxiv.org/abs/1709.08855](https://arxiv.org/abs/1709.08855)\n\n**Efficient Trimmed Convolutional Arithmetic Encoding for Lossless Image Compression**\n\n[https://arxiv.org/abs/1801.04662](https://arxiv.org/abs/1801.04662)\n\n**Conditional Probability Models for Deep Image Compression**\n\n[https://arxiv.org/abs/1801.04260](https://arxiv.org/abs/1801.04260)\n\n**Multiple Description Convolutional Neural Networks for Image Compression**\n\n[https://arxiv.org/abs/1801.06611](https://arxiv.org/abs/1801.06611)\n\n**Near-lossless L-infinity constrained Multi-rate Image Decompression via Deep Neural Network**\n\n[https://arxiv.org/abs/1801.07987](https://arxiv.org/abs/1801.07987)\n\n**DeepSIC: Deep Semantic Image Compression**\n\n[https://arxiv.org/abs/1801.09468](https://arxiv.org/abs/1801.09468)\n\n**Spatially adaptive image compression using a tiled deep network**\n\n- intro: ICIP 2017\n- arxiv: [https://arxiv.org/abs/1802.02629](https://arxiv.org/abs/1802.02629)\n\n**Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1803.05787](https://arxiv.org/abs/1803.05787)\n\n**DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression Framework**\n\n- intro: DAC 2018\n- arxiv: [https://arxiv.org/abs/1803.05788](https://arxiv.org/abs/1803.05788)\n\n**The Effects of JPEG and JPEG2000 Compression on Attacks using Adversarial Examples**\n\n[https://arxiv.org/abs/1803.10418](https://arxiv.org/abs/1803.10418)\n\n**Generative Adversarial Networks for Extreme Learned Image Compression**\n\n- intro: ETH Zurich\n- homepage: [https://data.vision.ee.ethz.ch/aeirikur/extremecompression/](https://data.vision.ee.ethz.ch/aeirikur/extremecompression/)\n- arxiv: [https://arxiv.org/abs/1804.02958](https://arxiv.org/abs/1804.02958)\n\n**Deformation Aware Image Compression**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.04593](https://arxiv.org/abs/1804.04593)\n\n**Neural Multi-scale Image Compression**\n\n[https://arxiv.org/abs/1805.06386](https://arxiv.org/abs/1805.06386)\n\n**Deep Image Compression via End-to-End Learning**\n\n[https://arxiv.org/abs/1806.01496](https://arxiv.org/abs/1806.01496)\n\n# Image Quality Assessment\n\n**Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment**\n\n- arxiv: [https://arxiv.org/abs/1612.01697](https://arxiv.org/abs/1612.01697)\n\n# Image Blending\n\n**GP-GAN: Towards Realistic High-Resolution Image Blending**\n\n- project page: [https://wuhuikai.github.io/GP-GAN-Project/](https://wuhuikai.github.io/GP-GAN-Project/)\n- arxiv: [https://arxiv.org/abs/1703.07195](https://arxiv.org/abs/1703.07195)\n- github(Official, Chainer): [https://github.com/wuhuikai/GP-GAN](https://github.com/wuhuikai/GP-GAN)\n\n# Image Enhancement\n\n**Deep Bilateral Learning for Real-Time Image Enhancement**\n\n- intro: MIT & Google Research\n- arxiv: [https://arxiv.org/abs/1707.02880](https://arxiv.org/abs/1707.02880)\n\n**Aesthetic-Driven Image Enhancement by Adversarial Learning**\n\n- intro: CUHK\n- arxiv: [https://arxiv.org/abs/1707.05251](https://arxiv.org/abs/1707.05251)\n\n**Learned Perceptual Image Enhancement**\n\n[https://arxiv.org/abs/1712.02864](https://arxiv.org/abs/1712.02864)\n\n**Deep Underwater Image Enhancement**\n\n[https://arxiv.org/abs/1807.03528](https://arxiv.org/abs/1807.03528)\n\n# Abnormality Detection / Anomaly Detection\n\n**Toward a Taxonomy and Computational Models of Abnormalities in Images**\n\n- arxiv: [http://arxiv.org/abs/1512.01325](http://arxiv.org/abs/1512.01325)\n\n**GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training**\n\n[https://arxiv.org/abs/1805.06725](https://arxiv.org/abs/1805.06725)\n\n# Depth Prediction / Depth Estimation\n\n**Deep Convolutional Neural Fields for Depth Estimation from a Single Image**\n\n- intro: CVPR 2015\n- arxiv: [https://arxiv.org/abs/1411.6387](https://arxiv.org/abs/1411.6387)\n\n**Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields**\n\n- intro: IEEE T. Pattern Analysis and Machine Intelligence\n- arxiv: [https://arxiv.org/abs/1502.07411](https://arxiv.org/abs/1502.07411)\n- bitbucket: [https://bitbucket.org/fayao/dcnf-fcsp](https://bitbucket.org/fayao/dcnf-fcsp)\n\n**Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue**\n\n- intro: ECCV 2016\n- arxiv: [https://arxiv.org/abs/1603.04992](https://arxiv.org/abs/1603.049921)\n- github: [https://github.com/Ravi-Garg/Unsupervised_Depth_Estimation](https://github.com/Ravi-Garg/Unsupervised_Depth_Estimation)\n\n**Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions**\n\n- intro: NIPS 2016\n- project pag: [http://ttic.uchicago.edu/~ayanc/mdepth/](http://ttic.uchicago.edu/~ayanc/mdepth/)\n- arxiv: [http://arxiv.org/abs/1605.07081](http://arxiv.org/abs/1605.07081)\n- github: [https://github.com/ayanc/mdepth/](https://github.com/ayanc/mdepth/)\n\n**Deeper Depth Prediction with Fully Convolutional Residual Networks**\n\n- arxiv: [https://arxiv.org/abs/1606.00373](https://arxiv.org/abs/1606.00373)\n- github: [https://github.com/iro-cp/FCRN-DepthPrediction](https://github.com/iro-cp/FCRN-DepthPrediction)\n\n**Single image depth estimation by dilated deep residual convolutional neural network and soft-weight-sum inference**\n\n[https://arxiv.org/abs/1705.00534](https://arxiv.org/abs/1705.00534)\n\n**Monocular Depth Estimation with Hierarchical Fusion of Dilated CNNs and Soft-Weighted-Sum Inference**\n\n- intro: Northwestern Polytechnical University\n- arxiv: [https://arxiv.org/abs/1708.02287](https://arxiv.org/abs/1708.02287)\n\n**Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image**\n\n- arxiv: [https://arxiv.org/abs/1709.07492](https://arxiv.org/abs/1709.07492)\n- video: [https://www.youtube.com/watch?v=vNIIT_M7x7Y](https://www.youtube.com/watch?v=vNIIT_M7x7Y)\n- github: [https://github.com/fangchangma/sparse-to-dense](https://github.com/fangchangma/sparse-to-dense)\n\n**Size-to-depth: A New Perspective for Single Image Depth Estimation**\n\n[https://arxiv.org/abs/1801.04461](https://arxiv.org/abs/1801.04461)\n\n**Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.00150](https://arxiv.org/abs/1808.00150)\n\n**Rethinking Monocular Depth Estimation with Adversarial Training**\n\n[https://arxiv.org/abs/1808.07528](https://arxiv.org/abs/1808.07528)\n\n**CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth**\n\n- intro: CVPR 2019\n- project page: [http://webdiis.unizar.es/~jmfacil/camconvs/](http://webdiis.unizar.es/~jmfacil/camconvs/)\n- arxiv: [https://arxiv.org/abs/1904.02028](https://arxiv.org/abs/1904.02028)\n\n# Texture Synthesis\n\n**Texture Synthesis Using Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1505.07376](http://arxiv.org/abs/1505.07376)\n\n**Texture Networks: Feed-forward Synthesis of Textures and Stylized Images**\n\n- intro: IMCL 2016\n- arxiv: [http://arxiv.org/abs/1603.03417](http://arxiv.org/abs/1603.03417)\n- github: [https://github.com/DmitryUlyanov/texture_nets](https://github.com/DmitryUlyanov/texture_nets)\n- notes: [https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/](https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/)\n\n**Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.04382](http://arxiv.org/abs/1604.04382)\n- github(Torch): [https://github.com/chuanli11/MGANs](https://github.com/chuanli11/MGANs)\n\n**Texture Synthesis with Spatial Generative Adversarial Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.08207](https://arxiv.org/abs/1611.08207)\n\n**Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis**\n\n- intro: Skolkovo Institute of Science and Technology & Yandex & University of Oxford\n- arxiv: [https://arxiv.org/abs/1701.02096](https://arxiv.org/abs/1701.02096)\n\n**Deep TEN: Texture Encoding Network**\n\n- intro: CVPR 2017\n- project page: [http://zhanghang1989.github.io/DeepEncoding/](http://zhanghang1989.github.io/DeepEncoding/)\n- arxiv: [https://arxiv.org/abs/1612.02844](https://arxiv.org/abs/1612.02844)\n- github: [https://github.com/zhanghang1989/Deep-Encoding](https://github.com/zhanghang1989/Deep-Encoding)\n- notes: [https://zhuanlan.zhihu.com/p/25013378](https://zhuanlan.zhihu.com/p/25013378)\n\n**Diversified Texture Synthesis with Feed-forward Networks**\n\n- intro: CVPR 2017. University of California & Adobe Research\n- arxiv: [https://arxiv.org/abs/1703.01664](https://arxiv.org/abs/1703.01664)\n- github: [https://github.com/Yijunmaverick/MultiTextureSynthesis](https://github.com/Yijunmaverick/MultiTextureSynthesis)\n\n# Image Cropping\n\n**Deep Cropping via Attention Box Prediction and Aesthetics Assessment**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1710.08014](https://arxiv.org/abs/1710.08014)\n\n**A2-RL: Aesthetics Aware Reinforcement Learning for Automatic Image Cropping**\n\n- intro: CVPR 2018\n- project page: [http://debangli.info/A2RL/](http://debangli.info/A2RL/)\n- arxiv: [https://arxiv.org/abs/1709.04595](https://arxiv.org/abs/1709.04595)\n- github(official): [https://github.com/wuhuikai/TF-A2RL](https://github.com/wuhuikai/TF-A2RL)\n- demo: [http://wuhuikai.me/TF-A2RL/](http://wuhuikai.me/TF-A2RL/)\n\n**Automatic Image Cropping for Visual Aesthetic Enhancement Using Deep Neural Networks and Cascaded Regression**\n\n- intro: IEEE Transactions on Multimedia, 2017\n- arxiv: [https://arxiv.org/abs/1712.09048](https://arxiv.org/abs/1712.09048)\n\n**Grid Anchor based Image Cropping: A New Benchmark and An Efficient Model**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1909.08989](https://arxiv.org/abs/1909.08989)\n- github: [https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch](https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch)\n\n**Image Cropping with Composition and Saliency Aware Aesthetic Score Map**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.10492](https://arxiv.org/abs/1911.10492)\n\n# Image Synthesis\n\n**Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis**\n\n- arxiv: [http://arxiv.org/abs/1601.04589](http://arxiv.org/abs/1601.04589)\n\n**Generative Adversarial Text to Image Synthesis**\n\n![](https://camo.githubusercontent.com/1925e23b5b6e19efa60f45daa3787f1f4a098ef3/687474703a2f2f692e696d6775722e636f6d2f644e6c32486b5a2e6a7067)\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1605.05396](http://arxiv.org/abs/1605.05396)\n- github(Tensorflow): [https://github.com/paarthneekhara/text-to-image](https://github.com/paarthneekhara/text-to-image)\n\n**StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks**\n\n- intro: Rutgers University & Lehigh University & The Chinese University of Hong Kong & University of North Carolina at Charlotte\n- arxiv: [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242)\n- github: [https://github.com/hanzhanggit/StackGAN](https://github.com/hanzhanggit/StackGAN)\n- github: [https://github.com/brangerbriz/docker-StackGAN](https://github.com/brangerbriz/docker-StackGAN)\n\n**Semantic Image Synthesis via Adversarial Learning**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.06873](https://arxiv.org/abs/1707.06873)\n- github(PyTorch): [https://github.com//woozzu/dong_iccv_2017](https://github.com//woozzu/dong_iccv_2017)\n\n**An Introduction to Image Synthesis with Generative Adversarial Nets**\n\n- intro: University of Illinois at Chicago & Toutiao AI Lab\n- arxiv: [https://arxiv.org/abs/1803.04469](https://arxiv.org/abs/1803.04469)\n\n**Text Guided Person Image Synthesis**\n\n- intr: CVPR 2019\n- intro: Zhejiang University & Nanjing University\n- arxiv: [https://arxiv.org/abs/1904.05118](https://arxiv.org/abs/1904.05118)\n\n# Image Tagging\n\n**Fast Zero-Shot Image Tagging**\n\n![](http://crcv.ucf.edu/projects/fastzeroshot/overview.png)\n\n- project: [http://crcv.ucf.edu/projects/fastzeroshot/](http://crcv.ucf.edu/projects/fastzeroshot/)\n\n**Flexible Image Tagging with Fast0Tag**\n\n![](https://cdn-images-1.medium.com/max/800/1*SsIf1Bhe-G4HmN6DPDogmQ.png)\n\n- blog: [https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7](https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7)\n\n**Sampled Image Tagging and Retrieval Methods on User Generated Content**\n\n- arxiv: [https://arxiv.org/abs/1611.06962](https://arxiv.org/abs/1611.06962)\n- github: [https://github.com/lab41/attalos](https://github.com/lab41/attalos)\n\n**Kill Two Birds with One Stone: Weakly-Supervised Neural Network for Image Annotation and Tag Refinement**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1711.06998](https://arxiv.org/abs/1711.06998)\n\n**Deep Multiple Instance Learning for Zero-shot Image Tagging**\n\n[https://arxiv.org/abs/1803.06051](https://arxiv.org/abs/1803.06051)\n\n# Image Matching\n\n**Learning Fine-grained Image Similarity with Deep Ranking**\n\n- intro: CVPR 2014\n- intro: Triplet Sampling\n- arxiv: [http://arxiv.org/abs/1404.4661](http://arxiv.org/abs/1404.4661)\n\n**Learning to compare image patches via convolutional neural networks**\n\n- intro: CVPR 2015. siamese network\n- project page: [http://imagine.enpc.fr/~zagoruys/deepcompare.html](http://imagine.enpc.fr/~zagoruys/deepcompare.html)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zagoruyko_Learning_to_Compare_2015_CVPR_paper.pdf)\n- github: [https://github.com/szagoruyko/cvpr15deepcompare](https://github.com/szagoruyko/cvpr15deepcompare)\n\n**MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching**\n\n- intro: CVPR 2015. siamese network\n- paper: [http://www.cs.unc.edu/~xufeng/cs/papers/cvpr15-matchnet.pdf](http://www.cs.unc.edu/~xufeng/cs/papers/cvpr15-matchnet.pdf)\n- extended abstract: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_114_ext.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/ext/2A_114_ext.pdf)\n- github: [https://github.com/hanxf/matchnet](https://github.com/hanxf/matchnet)\n\n**Fashion Style in 128 Floats**\n\n![](http://hi.cs.waseda.ac.jp/~esimo/images/stylenet/fashionfeat.png)\n\n- intro: CVPR 2016. StyleNet\n- project page: [http://hi.cs.waseda.ac.jp/~esimo/en/research/stylenet/](http://hi.cs.waseda.ac.jp/~esimo/en/research/stylenet/)\n- paper: [http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf](http://hi.cs.waseda.ac.jp/~esimo/publications/SimoSerraCVPR2016.pdf)\n- github: [https://github.com/bobbens/cvpr2016_stylenet](https://github.com/bobbens/cvpr2016_stylenet)\n\n**Fully-Trainable Deep Matching**\n\n- intro: BMVC 2016\n- project page: [http://lear.inrialpes.fr/src/deepmatching/](http://lear.inrialpes.fr/src/deepmatching/)\n- arxiv: [http://arxiv.org/abs/1609.03532](http://arxiv.org/abs/1609.03532)\n\n**Local Similarity-Aware Deep Feature Embedding**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1610.08904](https://arxiv.org/abs/1610.08904)\n\n**Convolutional neural network architecture for geometric matching**\n\n- intro: CVPR 2017. Inria\n- project page: [http://www.di.ens.fr/willow/research/cnngeometric/](http://www.di.ens.fr/willow/research/cnngeometric/)\n- arxiv: [https://arxiv.org/abs/1703.05593](https://arxiv.org/abs/1703.05593)\n- github: [https://github.com/ignacio-rocco/cnngeometric_matconvnet](https://github.com/ignacio-rocco/cnngeometric_matconvnet)\n\n**Multi-Image Semantic Matching by Mining Consistent Features**\n\n[https://arxiv.org/abs/1711.07641](https://arxiv.org/abs/1711.07641)\n\n# Image Editing\n\n**Neural Photo Editing with Introspective Adversarial Networks**\n\n![](https://camo.githubusercontent.com/c66848752d9fa05c3194ae36d48b869ec9d21743/687474703a2f2f692e696d6775722e636f6d2f773155323045492e706e67)\n\n- intro: Heriot-Watt University\n- arxiv: [http://arxiv.org/abs/1609.07093](http://arxiv.org/abs/1609.07093)\n- github: [https://github.com/ajbrock/Neural-Photo-Editor](https://github.com/ajbrock/Neural-Photo-Editor)\n\n**Deep Feature Interpolation for Image Content Changes**\n\n- intro: CVPR 2017. Cornell University & Washington University\n- arxiv: [https://arxiv.org/abs/1611.05507](https://arxiv.org/abs/1611.05507)\n- github(official): [https://github.com/paulu/deepfeatinterp](https://github.com/paulu/deepfeatinterp)\n- github: [https://github.com/slang03/dfi-tensorflow](https://github.com/slang03/dfi-tensorflow)\n\n**Invertible Conditional GANs for image editing**\n\n![](https://raw.githubusercontent.com/Guim3/IcGAN/master/images/model_overview.png)\n\n- intro: NIPS 2016 Workshop on Adversarial Training\n- arxiv: [https://arxiv.org/abs/1611.06355](https://arxiv.org/abs/1611.06355)\n- github: [https://github.com/Guim3/IcGAN](https://github.com/Guim3/IcGAN)\n\n**Semantic Facial Expression Editing using Autoencoded Flow**\n\n- intro: University of Illinois at Urbana-Champaign & The Chinese University of Hong Kong & Google\n- arxiv: [https://arxiv.org/abs/1611.09961](https://arxiv.org/abs/1611.09961)\n\n**Language-Based Image Editing with Recurrent Attentive Models**\n\n[https://arxiv.org/abs/1711.06288](https://arxiv.org/abs/1711.06288)\n\n## Face Swap & Face Editing\n\n**Fast Face-swap Using Convolutional Neural Networks**\n\n- intro: Ghent University & Twitter\n- arxiv: [https://arxiv.org/abs/1611.09577](https://arxiv.org/abs/1611.09577)\n\n**Neural Face Editing with Intrinsic Image Disentangling**\n\n- intro: CVPR 2017 oral\n- project page: [http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html](http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html)\n- arxiv: [https://arxiv.org/abs/1704.04131](https://arxiv.org/abs/1704.04131)\n\n**Arbitrary Facial Attribute Editing: Only Change What You Want**\n\n- arxiv: [https://arxiv.org/abs/1711.10678](https://arxiv.org/abs/1711.10678)\n- github: [https://github.com/LynnHo/AttGAN-Tensorflow](https://github.com/LynnHo/AttGAN-Tensorflow)\n\n**RSGAN: Face Swapping and Editing using Face and Hair Representation in Latent Spaces**\n\n[https://arxiv.org/abs/1804.03447](https://arxiv.org/abs/1804.03447)\n\n**FaceShop: Deep Sketch-based Face Image Editing**\n\n[https://arxiv.org/abs/1804.08972](https://arxiv.org/abs/1804.08972)\n\n# Stereo\n\n**End-to-End Learning of Geometry and Context for Deep Stereo Regression**\n\n[https://arxiv.org/abs/1703.04309](https://arxiv.org/abs/1703.04309)\n\n**Unsupervised Adaptation for Deep Stereo**\n\n- intro: ICCV 2017\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf)\n- paper: [http://vision.disi.unibo.it/~mpoggi/papers/iccv2017_adaptation.pdf](http://vision.disi.unibo.it/~mpoggi/papers/iccv2017_adaptation.pdf)\n- github: [https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo](https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo)\n\n**Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching**\n\n[https://arxiv.org/abs/1708.09204](https://arxiv.org/abs/1708.09204)\n\n**StereoConvNet: Stereo convolutional neural network for depth map prediction from stereo images**\n\n- github: [https://github.com/LouisFoucard/StereoConvNet](https://github.com/LouisFoucard/StereoConvNet)\n\n**EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching**\n\n[https://arxiv.org/abs/1803.05196](https://arxiv.org/abs/1803.05196)\n\n**Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains**\n\n- intro: CVPR 2018. SenseTime Research & Sun Yat-sen University\n- arxiv: [https://arxiv.org/abs/1803.06641](https://arxiv.org/abs/1803.06641)\n\n**Pyramid Stereo Matching Network**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.08669](https://arxiv.org/abs/1803.08669)\n- github: [https://github.com/JiaRenChang/PSMNet](https://github.com/JiaRenChang/PSMNet)\n\n**Cascaded multi-scale and multi-dimension convolutional neural network for stereo matching**\n\n[https://arxiv.org/abs/1803.09437](https://arxiv.org/abs/1803.09437)\n\n**Left-Right Comparative Recurrent Model for Stereo Matching**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.00796](https://arxiv.org/abs/1804.00796)\n\n**Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching**\n\n[https://arxiv.org/abs/1806.01677](https://arxiv.org/abs/1806.01677)\n\n**Open-World Stereo Video Matching with Deep RNN**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.03959](https://arxiv.org/abs/1808.03959)\n\n**Real-time self-adaptive deep stereo**\n\n[https://arxiv.org/abs/1810.05424](https://arxiv.org/abs/1810.05424)\n[https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo](https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo)\n\n**Group-wise Correlation Stereo Network**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.04025](https://arxiv.org/abs/1903.04025)\n- github(official): [https://github.com/xy-guo/GwcNet](https://github.com/xy-guo/GwcNet)\n\n**Self-calibrating Deep Photometric Stereo Networks**\n\n- intro: CVPR 2019 oral\n- intro: The University of Hong Kong & University of Oxford & Peking University & Peng Cheng Laboratory & Osaka University\n- keywords: Learning Based Uncalibrated Photometric Stereo for Non-Lambertian Surface\n- project page: [http://gychen.org/SDPS-Net/](http://gychen.org/SDPS-Net/)\n- arxiv: [https://arxiv.org/abs/1903.07366](https://arxiv.org/abs/1903.07366)\n- github(official, PyTorch): [https://github.com/guanyingc/SDPS-Net](https://github.com/guanyingc/SDPS-Net)\n\n**Learning to Adapt for Stereo**\n\n- intro: CVPR 2019\n- intro: University of Bologna & University of Oxford & Australian National University & FiveAI\n- arxiv: [https://arxiv.org/abs/1904.02957](https://arxiv.org/abs/1904.02957)\n- github: [https://github.com/CVLAB-Unibo/Learning2AdaptForStereo](https://github.com/CVLAB-Unibo/Learning2AdaptForStereo)\n\n**StereoDRNet: Dilated Residual Stereo Net**\n\n- intro: CVPR 2019\n- intro: University of North Carolina at Chapel Hill & Facebook Reality Labs\n- arxiv: [https://arxiv.org/abs/1904.02251](https://arxiv.org/abs/1904.02251)\n\n**GA-Net: Guided Aggregation Net for End-to-end Stereo Matching**\n\n- intro: CVPR 2019 oral\n- intro: University of Oxford & Baidu Research\n- arxiv: [https://arxiv.org/abs/1904.06587](https://arxiv.org/abs/1904.06587)\n\n**Multi-Scale Geometric Consistency Guided Multi-View Stereo**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1904.08103](https://arxiv.org/abs/1904.08103)\n\n**Guided Stereo Matching**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1905.10107](https://arxiv.org/abs/1905.10107)\n\n**OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.06257](https://arxiv.org/abs/1908.06257)\n\n**DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.05845](https://arxiv.org/abs/1909.05845)\n\n**Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers**\n\n- intro: Johns Hopkins University\n- keywords: STereo TRansformer (STTR)\n- arxiv: [https://arxiv.org/abs/2011.02910](https://arxiv.org/abs/2011.02910)\n- github: [https://github.com/mli0603/stereo-transformer](https://github.com/mli0603/stereo-transformer)\n\n**EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection**\n\n- intro: Tianjin University\n- arxiv: [https://arxiv.org/abs/2111.14055](https://arxiv.org/abs/2111.14055)\n\n# 3D\n\n**Learning Spatiotemporal Features with 3D Convolutional Networks**\n\n**C3D: Generic Features for Video Analysis**\n\n- project page: [http://vlg.cs.dartmouth.edu/c3d/](http://vlg.cs.dartmouth.edu/c3d/)\n- arxiv: [http://arxiv.org/abs/1412.0767](http://arxiv.org/abs/1412.0767)\n- slides: [http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w07-conv3d.pdf](http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w07-conv3d.pdf)\n- github: [https://github.com/facebook/C3D](https://github.com/facebook/C3D)\n\n**C3D Model for Keras trained over Sports 1M**\n\n- project page: [https://imatge.upc.edu/web/resources/c3d-model-keras-trained-over-sports-1m](https://imatge.upc.edu/web/resources/c3d-model-keras-trained-over-sports-1m)\n\n**Sports 1M C3D Network to Keras**\n\n- notebook: [http://nbviewer.jupyter.org/gist/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2/Sports1M%20C3D%20Network%20to%20Keras.ipynb](http://nbviewer.jupyter.org/gist/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2/Sports1M%20C3D%20Network%20to%20Keras.ipynb)\n\n**Deep End2End Voxel2Voxel Prediction**\n\n- arxiv: [http://arxiv.org/abs/1511.06681](http://arxiv.org/abs/1511.06681)\n\n**Aligning 3D Models to RGB-D Images of Cluttered Scenes**\n\n- paper: [http://www.cs.berkeley.edu/~rbg/papers/cvpr15/align2rgbd.pdf](http://www.cs.berkeley.edu/~rbg/papers/cvpr15/align2rgbd.pdf)\n\n**Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images**\n\n![](http://dss.cs.princeton.edu/teaser.jpg)\n\n- homepage: [http://dss.cs.princeton.edu/](http://dss.cs.princeton.edu/)\n- arxiv: [http://arxiv.org/abs/1511.02300](http://arxiv.org/abs/1511.02300)\n\n**Multi-view 3D Models from Single Images with a Convolutional Network**\n\n- arxiv: [http://arxiv.org/abs/1511.06702](http://arxiv.org/abs/1511.06702)\n\n**RotationNet: Learning Object Classification Using Unsupervised Viewpoint Estimation**\n\n- arxiv: [http://arxiv.org/abs/1603.06208](http://arxiv.org/abs/1603.06208)\n\n**DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding**\n\n![](http://deepcontext.cs.princeton.edu/teaser.png)\n\n- paper: [http://deepcontext.cs.princeton.edu/paper.pdf](http://deepcontext.cs.princeton.edu/paper.pdf)\n- project page: [http://deepcontext.cs.princeton.edu/](http://deepcontext.cs.princeton.edu/)\n\n**Volumetric and Multi-View CNNs for Object Classification on 3D Data**\n\n![](http://graphics.stanford.edu/projects/3dcnn/teaser.jpg)\n\n- homepage: [http://graphics.stanford.edu/projects/3dcnn/](http://graphics.stanford.edu/projects/3dcnn/)\n- arxiv: [https://arxiv.org/abs/1604.03265](https://arxiv.org/abs/1604.03265)\n- github: [https://github.com/charlesq34/3dcnn.torch](https://github.com/charlesq34/3dcnn.torch)\n\n**Deep3D: Automatic 2D-to-3D Video Conversion with CNNs**\n\n![](https://raw.githubusercontent.com/piiswrong/deep3d/master/img/teaser.png)\n\n- project page: [http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html](http://dmlc.ml/mxnet/2016/04/04/deep3d-automatic-2d-to-3d-conversion-with-CNN.html)\n- paper: [http://homes.cs.washington.edu/~jxie/pdf/deep3d.pdf](http://homes.cs.washington.edu/~jxie/pdf/deep3d.pdf)\n- github: [https://github.com/piiswrong/deep3d](https://github.com/piiswrong/deep3d)\n\n**Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.03650](http://arxiv.org/abs/1604.03650)\n\n**3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction**\n\n![](https://raw.githubusercontent.com/chrischoy/3D-R2N2/master/imgs/overview.png)\n\n- arxiv: [http://arxiv.org/abs/1604.00449](http://arxiv.org/abs/1604.00449)\n- github: [https://github.com/chrischoy/3D-R2N2](https://github.com/chrischoy/3D-R2N2)\n\n**Body Meshes as Points**\n\n- intro: CVPR 2021\n- intro: National University of Singapore & ByteDance AI Lab & Yitu Technology\n- arxiv: [https://arxiv.org/abs/2105.02467](https://arxiv.org/abs/2105.02467)\n- github: [https://github.com/jfzhang95/BMP](https://github.com/jfzhang95/BMP)\n\n# Deep Learning for Makeup\n\n**Makeup like a superstar: Deep Localized Makeup Transfer Network**\n\n- intro: IJCAI 2016\n- arxiv: [http://arxiv.org/abs/1604.07102](http://arxiv.org/abs/1604.07102)\n\n**Makeup-Go: Blind Reversion of Portrait Edit**\n\n- intro: The Chinese University of Hong Kong & Tencent Youtu Lab\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Makeup-Go_Blind_Reversion_ICCV_2017_paper.pdf)\n- paper: [http://open.youtu.qq.com/content/file/iccv17_makeupgo.pdf](http://open.youtu.qq.com/content/file/iccv17_makeupgo.pdf)\n\n# Music Tagging\n\n**Automatic tagging using deep convolutional neural networks**\n\n- arxiv: [https://arxiv.org/abs/1606.00298](https://arxiv.org/abs/1606.00298)\n- github: [https://github.com/keunwoochoi/music-auto_tagging-keras](https://github.com/keunwoochoi/music-auto_tagging-keras)\n\n**Music tagging and feature extraction with MusicTaggerCRNN**\n\n[https://keras.io/applications/#music-tagging-and-feature-extraction-with-musictaggercrnn](https://keras.io/applications/#music-tagging-and-feature-extraction-with-musictaggercrnn)\n\n# Action Recognition\n\n**Single Image Action Recognition by Predicting Space-Time Saliency**\n\n[https://arxiv.org/abs/1705.04641](https://arxiv.org/abs/1705.04641)\n\n**Attentional Pooling for Action Recognition**\n\n- intro: NIPS 2017\n- project page: [https://rohitgirdhar.github.io/AttentionalPoolingAction/](https://rohitgirdhar.github.io/AttentionalPoolingAction/)\n- arxiv: [https://arxiv.org/abs/1711.01467](https://arxiv.org/abs/1711.01467)\n- github: [https://github.com/rohitgirdhar/AttentionalPoolingAction/](https://github.com/rohitgirdhar/AttentionalPoolingAction/)\n\n**Memory Attention Networks for Skeleton-based Action Recognition**\n\n- intro: IJCAI 2018\n- keywords: Temporal Attention Recalibration Module (TARM) and a Spatio-Temporal Convolution Module (STCM)\n- arixv: [https://arxiv.org/abs/1804.08254](https://arxiv.org/abs/1804.08254)\n- github: [https://github.com/memory-attention-networks](https://github.com/memory-attention-networks)\n\n**Deep Analysis of CNN-based Spatio-temporal Representations for Action Recognition**\n\n- intro: MIT-IBM Watson AI Lab & MIT\n- arxiv: [https://arxiv.org/abs/2010.11757](https://arxiv.org/abs/2010.11757)\n- github: [https://github.com/IBM/action-recognition-pytorch](https://github.com/IBM/action-recognition-pytorch)\n\n**Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition**\n\n- intro: ECCV 2020\n- paper: [https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690528.pdf](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690528.pdf)\n- github: [https://github.com/kchengiva/DecoupleGCN-DropGraph](https://github.com/kchengiva/DecoupleGCN-DropGraph)\n\n**Temporal-Relational CrossTransformers for Few-Shot Action Recognition**\n\n- intro: University of Bristol\n- arxiv: [https://arxiv.org/abs/2101.06184](https://arxiv.org/abs/2101.06184)\n- github: [https://github.com/tobyperrett/trx](https://github.com/tobyperrett/trx)\n\n# CTR Prediction\n\n**Deep CTR Prediction in Display Advertising**\n\n- intro: ACM Multimedia Conference 2016\n- arxiv: [https://arxiv.org/abs/1609.06018](https://arxiv.org/abs/1609.06018)\n\n**DeepFM: A Factorization-Machine based Neural Network for CTR Prediction**\n\n- intro: Harbin Institute of Technology & Huawei\n- arxiv: [https://arxiv.org/abs/1703.04247](https://arxiv.org/abs/1703.04247)\n\n**Deep Interest Network for Click-Through Rate Prediction**\n\n- intro: Alibaba Inc.\n- arxiv: [https://arxiv.org/abs/1706.06978](https://arxiv.org/abs/1706.06978)\n\n**Image Matters: Jointly Train Advertising CTR Model with Image Representation of Ad and User Behavior**\n\n- intro: Alibaba Inc.\n- arxiv: [https://arxiv.org/abs/1711.06505](https://arxiv.org/abs/1711.06505)\n\n# Cryptography\n\n**Learning to Protect Communications with Adversarial Neural Cryptography**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1610.06918](https://arxiv.org/abs/1610.06918)\n- github(Theano): [https://github.com/nlml/adversarial-neural-crypt](https://github.com/nlml/adversarial-neural-crypt)\n- github(TensorFlow): [https://github.com/ankeshanand/neural-cryptography-tensorflow](https://github.com/ankeshanand/neural-cryptography-tensorflow)\n\n**Adversarial Neural Cryptography in Theano**\n\n- blog: [https://nlml.github.io/neural-networks/adversarial-neural-cryptography/](https://nlml.github.io/neural-networks/adversarial-neural-cryptography/)\n\n**Embedding Watermarks into Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1701.04082](https://arxiv.org/abs/1701.04082)\n- github: [https://github.com/yu4u/dnn-watermark](https://github.com/yu4u/dnn-watermark)\n\n**Digital Watermarking for Deep Neural Networks**\n\n- intro: International Journal of Multimedia Information Retrieval\n- arxiv: [https://arxiv.org/abs/1802.02601](https://arxiv.org/abs/1802.02601)\n\n# Cyber Security\n\n**Collection of Deep Learning Cyber Security Research Papers**\n\n- blog: [https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042#.fcus2cu9m](https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042#.fcus2cu9m)\n\n# Lip Reading\n\n**LipNet: Sentence-level Lipreading**\n\n**LipNet: End-to-End Sentence-level Lipreading**\n\n- arxiv: [https://arxiv.org/abs/1611.01599](https://arxiv.org/abs/1611.01599)\n- paper: [http://openreview.net/pdf?id=BkjLkSqxg](http://openreview.net/pdf?id=BkjLkSqxg)\n- github: [https://github.com/bshillingford/LipNet](https://github.com/bshillingford/LipNet)\n\n**Lip Reading Sentences in the Wild**\n\n- intro: University of Oxford & Google DeepMind\n- arxiv: [https://arxiv.org/abs/1611.05358](https://arxiv.org/abs/1611.05358)\n- youtube: [https://www.youtube.com/watch?v=5aogzAUPilE](https://www.youtube.com/watch?v=5aogzAUPilE)\n\n**Combining Residual Networks with LSTMs for Lipreading**\n\n- arxiv: [https://arxiv.org/abs/1703.04105](https://arxiv.org/abs/1703.04105)\n\n**End-to-End Multi-View Lipreading**\n\n- intro: BMVC 2017\n- arxiv: [https://arxiv.org/abs/1709.00443](https://arxiv.org/abs/1709.00443)\n\n**LCANet: End-to-End Lipreading with Cascaded Attention-CTC**\n\n- intro: FG 2018\n- arxiv: [https://arxiv.org/abs/1803.04988](https://arxiv.org/abs/1803.04988)\n\n# Event Recognition\n\n**Better Exploiting OS-CNNs for Better Event Recognition in Images**\n\n- arxiv: [http://arxiv.org/abs/1510.03979](http://arxiv.org/abs/1510.03979)\n\n**Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images**\n\n- arxiv: [http://arxiv.org/abs/1609.00162](http://arxiv.org/abs/1609.00162)\n\n**IOD-CNN: Integrating Object Detection Networks for Event Recognition**\n\n[https://arxiv.org/abs/1703.07431](https://arxiv.org/abs/1703.07431)\n\n# Trajectory Prediction\n\n**Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving**\n\n- intro: Machine Learning for Autonomous Driving @ NeurIPS 2020\n- intro: Carnegie Mellon University & Bosch Research Pittsburgh\n- arxiv: [https://arxiv.org/abs/2011.14910](https://arxiv.org/abs/2011.14910)\n\n# Human-Object Interaction\n\n**Learning Human-Object Interactions by Graph Parsing Neural Networks**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.07962](https://arxiv.org/abs/1808.07962)\n- github: [https://github.com/SiyuanQi/gpnn](https://github.com/SiyuanQi/gpnn)\n\n**Interact as You Intend: Intention-Driven Human-Object Interaction Detection**\n\n[https://arxiv.org/abs/1808.09796](https://arxiv.org/abs/1808.09796)\n\n**iCAN: Instance-Centric Attention Network for Human-Object Interaction Detection**\n\n- intro: BMVC 2018\n- project page: [https://gaochen315.github.io/iCAN/](https://gaochen315.github.io/iCAN/)\n- arxiv: [https://arxiv.org/abs/1808.10437](https://arxiv.org/abs/1808.10437)\n- github: [https://github.com/vt-vl-lab/iCAN](https://github.com/vt-vl-lab/iCAN)\n\n**Pose-aware Multi-level Feature Network for Human Object Interaction Detection**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1909.08453](https://arxiv.org/abs/1909.08453)\n\n**End-to-End Human Object Interaction Detection with HOI Transformer**\n\n- intro: CVPR 2021\n- intro: MEGVII Technology\n- arxiv: [https://arxiv.org/abs/2103.04503](https://arxiv.org/abs/2103.04503)\n- github: [https://github.com/bbepoch/HoiTransformer](https://github.com/bbepoch/HoiTransformer)\n\n**Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction**\n\n- intro: ICCV 2021\n- intro: OPPO Research Institute & Xmotors & University of California\n- arxiv: [https://arxiv.org/abs/2110.03278](https://arxiv.org/abs/2110.03278)\n\n**Hand-Object Contact Prediction via Motion-Based Pseudo-Labeling and Guided Progressive Label Correction**\n\n- intro: BMVC 2021\n- arxiv: [https://arxiv.org/abs/2110.10174](https://arxiv.org/abs/2110.10174)\n- github: [https://github.com/takumayagi/hand_object_contact_prediction](https://github.com/takumayagi/hand_object_contact_prediction)\n\n# Deep Learning in Finance\n\n**Deep Learning in Finance**\n\n- arxiv: [http://arxiv.org/abs/1602.06561](http://arxiv.org/abs/1602.06561)\n\n**A Survey of Deep Learning Techniques Applied to Trading**\n\n- blog: [http://gregharris.info/a-survey-of-deep-learning-techniques-applied-to-trading/](http://gregharris.info/a-survey-of-deep-learning-techniques-applied-to-trading/)\n\n**Deep Learning and Long-Term Investing**\n\n- part 1: [http://www.euclidean.com/deep-learning-long-term-investing-1](http://www.euclidean.com/deep-learning-long-term-investing-1)\n- part 2: [http://www.euclidean.com/deep-learning-investing-part-2-preprocessing-data](http://www.euclidean.com/deep-learning-investing-part-2-preprocessing-data)\n\n**Deep Learning in Trading**\n\n- youtube: [https://www.youtube.com/watch?v=FoQKCeDuPiY](https://www.youtube.com/watch?v=FoQKCeDuPiY)\n- mirror: [https://pan.baidu.com/s/1sltRra9](https://pan.baidu.com/s/1sltRra9)\n\n**Research to Products: Machine & Human Intelligence in Finance**\n\n- intro: Peter Sarlin, Hanken School of Economics - Deep Learning in Finance Summit 2016 #reworkfin\n- youtube: [https://www.youtube.com/watch?v=Fd7Cc-KOVXg](https://www.youtube.com/watch?v=Fd7Cc-KOVXg)\n- mirror: [https://pan.baidu.com/s/1kVpZKur#list/path=%2F](https://pan.baidu.com/s/1kVpZKur#list/path=%2F)\n\n**eep Neural Networks for Real-time Market Predictions**\n\n- youtube: [https://www.youtube.com/watch?v=Kzz2-wAEK7A](https://www.youtube.com/watch?v=Kzz2-wAEK7A)\n\n**Deep Learning the Stock Market**\n\n- blog: [https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.z752rf43u](https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.z752rf43u)\n- github: [https://github.com/talolard/MarketVectors](https://github.com/talolard/MarketVectors)\n\n**rl_portfolio**\n\n- intro: This Repository uses Reinforcement Learning and Supervised learning to Optimize portfolio allocation.\n- github: [https://github.com/deependersingla/deep_portfolio](https://github.com/deependersingla/deep_portfolio)\n\n**Neural networks for algorithmic trading. Multivariate time series**\n\n- blog: [https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57](https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57)\n- github: [https://github.com/Rachnog/Deep-Trading/tree/master/multivariate](https://github.com/Rachnog/Deep-Trading/tree/master/multivariate)\n\n**Deep-Trading: Algorithmic trading with deep learning experiments**\n\n[https://github.com/Rachnog/Deep-Trading](https://github.com/Rachnog/Deep-Trading)\n\n**Neural networks for algorithmic trading. Multimodal and multitask deep learning**\n\n- blog: [https://becominghuman.ai/neural-networks-for-algorithmic-trading-multimodal-and-multitask-deep-learning-5498e0098caf](https://becominghuman.ai/neural-networks-for-algorithmic-trading-multimodal-and-multitask-deep-learning-5498e0098caf)\n- github: [https://github.com/Rachnog/Deep-Trading/tree/master/multimodal](https://github.com/Rachnog/Deep-Trading/tree/master/multimodal)\n\n**Deep Learning with Python in Finance - Singapore Python User Group**\n\n- youtube: [https://www.youtube.com/watch?v=xvm-M-R2fZY](https://www.youtube.com/watch?v=xvm-M-R2fZY)\n\n**A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem**\n\n- intro: Xi’an Jiaotong-Liverpool University\n- keywords: PGPortfolio: Policy Gradient Portfolio\n- arxiv: [https://arxiv.org/abs/1706.10059](https://arxiv.org/abs/1706.10059)\n- github: [https://github.com//ZhengyaoJiang/PGPortfolio](https://github.com//ZhengyaoJiang/PGPortfolio)\n\n**Stock Prediction: a method based on extraction of news features and recurrent neural networks**\n\n- intro: Peking University. The 22nd China Conference on Information Retrieval\n- arxiv: [https://arxiv.org/abs/1707.07585](https://arxiv.org/abs/1707.07585)\n\n**Multidimensional LSTM Networks to Predict Bitcoin Price**\n\n- blog: [http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price](http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price)\n- github: [https://github.com/jaungiers/Multidimensional-LSTM-BitCoin-Time-Series](https://github.com/jaungiers/Multidimensional-LSTM-BitCoin-Time-Series)\n\n**Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals**\n\n- intro: Euclidean Technologies & Amazon AI\n- arxiv: [https://arxiv.org/abs/1711.04837](https://arxiv.org/abs/1711.04837)\n\n**Findings from our Research on Applying Deep Learning to Long-Term Investing**\n\n[http://www.euclidean.com/paper-on-deep-learning-long-term-investing](http://www.euclidean.com/paper-on-deep-learning-long-term-investing)\n\n**Predicting Cryptocurrency Prices With Deep Learning**\n\n- intro: This post brings together cryptos and deep learning in a desperate attempt for Reddit popularity\n- blog: [https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/](https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/)\n\n**Deep Trading Agent**\n\n- intro: Deep Reinforcement Learning based Trading Agent for Bitcoin\n- arxiv: [https://github.com/samre12/deep-trading-agent](https://github.com/samre12/deep-trading-agent)\n\n**Financial Trading as a Game: A Deep Reinforcement Learning Approach**\n\n- intro: National Chiao Tung University\n- arxiv: [https://arxiv.org/abs/1807.02787](https://arxiv.org/abs/1807.02787)\n\n# Deep Learning in Speech\n\n**Deep Speech 2: End-to-End Speech Recognition in English and Mandarin**\n\n- intro: Baidu Research, ICML 2016\n- arxiv: [https://arxiv.org/abs/1512.02595](https://arxiv.org/abs/1512.02595)\n- github(Neon): [https://github.com/NervanaSystems/deepspeech](https://github.com/NervanaSystems/deepspeech)\n\n**End-to-end speech recognition with neon**\n\n- blog: [https://www.nervanasys.com/end-end-speech-recognition-neon/](https://www.nervanasys.com/end-end-speech-recognition-neon/)\n\n## WaveNet\n\n**WaveNet: A Generative Model for Raw Audio**\n\n- homepage: [https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)\n- paper: [https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view](https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view)\n- mirror: [https://pan.baidu.com/s/1gfmGWaJ](https://pan.baidu.com/s/1gfmGWaJ)\n- github: [https://github.com/usernaamee/keras-wavenet](https://github.com/usernaamee/keras-wavenet)\n- github: [https://github.com/ibab/tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet)\n- github: [https://github.com/monthly-hack/chainer-wavenet](https://github.com/monthly-hack/chainer-wavenet)\n- github: [https://github.com/huyouare/WaveNet-Theano](https://github.com/huyouare/WaveNet-Theano)\n- github(Keras): [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet)\n- github: [https://github.com/ritheshkumar95/WaveNet](https://github.com/ritheshkumar95/WaveNet)\n\n**A TensorFlow implementation of DeepMind's WaveNet paper for text generation.**\n\n- github: [https://github.com/Zeta36/tensorflow-tex-wavenet](https://github.com/Zeta36/tensorflow-tex-wavenet)\n\n**Fast Wavenet Generation Algorithm**\n\n- intro: An efficient Wavenet generation implementation\n- arxiv: [https://arxiv.org/abs/1611.09482](https://arxiv.org/abs/1611.09482)\n- github [https://github.com/tomlepaine/fast-wavenet](https://github.com/tomlepaine/fast-wavenet)\n\n**Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition based on DeepMind's WaveNet and tensorflow**\n\n- github: [https://github.com/buriburisuri/speech-to-text-wavenet](https://github.com/buriburisuri/speech-to-text-wavenet)\n\n**Wav2Letter: an End-to-End ConvNet-based Speech Recognition System**\n\n- arxiv: [http://arxiv.org/abs/1609.03193](http://arxiv.org/abs/1609.03193)\n\n**TristouNet: Triplet Loss for Speaker Turn Embedding**\n\n- arxiv: [https://arxiv.org/abs/1609.04301](https://arxiv.org/abs/1609.04301)\n- github: [https://github.com/hbredin/TristouNet](https://github.com/hbredin/TristouNet)\n\n**Speech Recognion and Deep Learning**\n\n- intro: Baidu Research Silicon Valley AI Lab\n- slides: [http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf](http://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf)\n- mirror: [https://pan.baidu.com/s/1qYrPkPQ](https://pan.baidu.com/s/1qYrPkPQ)\n- github: [https://github.com/baidu-research/ba-dls-deepspeech](https://github.com/baidu-research/ba-dls-deepspeech)\n\n**Robust end-to-end deep audiovisual speech recognition**\n\n- intro: CMU\n- github: [https://arxiv.org/abs/1611.06986](https://arxiv.org/abs/1611.06986)\n\n**An Experimental Comparison of Deep Neural Networks for End-to-end Speech Recognition**\n\n- arxiv: [https://arxiv.org/abs/1611.07174](https://arxiv.org/abs/1611.07174)\n\n**Recurrent Deep Stacking Networks for Speech Recognition**\n\n- intro: The Ohio State University\n- arxiv: [https://arxiv.org/abs/1612.04675](https://arxiv.org/abs/1612.04675)\n\n**Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks**\n\n- intro: Universite de Montreal & CIFAR\n- arxiv: [https://arxiv.org/abs/1701.02720](https://arxiv.org/abs/1701.02720)\n\n# Deep Learning for Sound / Music\n\n## Sound\n\n**Suggesting Sounds for Images from Video Collections**\n\n- intro: ETH Zurich & 2Disney Research\n- paper: [https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20161014182443/Suggesting-Sounds-for-Images-from-Video-Collections-Paper.pdf](https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20161014182443/Suggesting-Sounds-for-Images-from-Video-Collections-Paper.pdf)\n\n**Disney AI System Associates Images with Sounds**\n\n- blog: [https://news.developer.nvidia.com/disneys-ai-system-associates-images-with-sounds/](https://news.developer.nvidia.com/disneys-ai-system-associates-images-with-sounds/)\n\n**Convolutional Recurrent Neural Networks for Bird Audio Detection**\n\n[https://arxiv.org/abs/1703.02317](https://arxiv.org/abs/1703.02317)\n\n**Visual to Sound: Generating Natural Sound for Videos in the Wild**\n\n- project page: [http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html](http://bvision11.cs.unc.edu/bigpen/yipin/visual2sound_webpage/visual2sound.html)\n- arxiv: [https://arxiv.org/abs/1712.01393](https://arxiv.org/abs/1712.01393)\n\n## Music\n\n**Learning Features of Music from Scratch**\n\n- intro: University of Washington. MusicNet\n- project page: [http://homes.cs.washington.edu/~thickstn/musicnet.html](http://homes.cs.washington.edu/~thickstn/musicnet.html)\n- arxiv: [https://arxiv.org/abs/1611.09827](https://arxiv.org/abs/1611.09827)\n- demo: [http://homes.cs.washington.edu/~thickstn/demos.html](http://homes.cs.washington.edu/~thickstn/demos.html)\n\n**DeepBach: a Steerable Model for Bach chorales generation**\n\n- project page: [http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-generation/](http://www.flow-machines.com/deepbach-steerable-model-bach-chorales-generation/)\n- arxiv: [https://arxiv.org/abs/1612.01010](https://arxiv.org/abs/1612.01010)\n- github: [https://github.com/SonyCSL-Paris/DeepBach](https://github.com/SonyCSL-Paris/DeepBach)\n- youtube: [https://www.youtube.com/watch?v=QiBM7-5hA6o](https://www.youtube.com/watch?v=QiBM7-5hA6o)\n\n**Deep Learning for Music**\n\n- blog: [https://amundtveit.com/2016/11/22/deep-learning-for-music/](https://amundtveit.com/2016/11/22/deep-learning-for-music/)\n\n**First International Workshop on Deep Learning and Music**\n\n[https://arxiv.org/html/1706.08675](https://arxiv.org/html/1706.08675)\n\n# Deep Learning in Medicine and Biology\n\n**Low Data Drug Discovery with One-shot Learning**\n\n- intro: MIT & Stanford University\n- arxiv: [https://arxiv.org/abs/1611.03199](https://arxiv.org/abs/1611.03199)\n- homepage: [http://deepchem.io/](http://deepchem.io/)\n- github: [https://github.com/deepchem/deepchem](https://github.com/deepchem/deepchem)\n\n**Democratizing Drug Discovery with DeepChem**\n\n- youtube: [https://www.youtube.com/watch?v=sntikyFI8s8](https://www.youtube.com/watch?v=sntikyFI8s8)\n\n**Introduction to Deep Learning in Medicine and Biology**\n\n- blog: [http://a12d.com/deep-learning-biomedicine](http://a12d.com/deep-learning-biomedicine)\n\n**Deep Learning for Alzheimer Diagnostics and Decision Support**\n\n[https://amundtveit.com/2016/11/18/deep-learning-for-alzheimer-diagnostics-and-decision-support/](https://amundtveit.com/2016/11/18/deep-learning-for-alzheimer-diagnostics-and-decision-support/)\n\n**DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning**\n\n- intro: University of Florida\n- arxiv: [https://arxiv.org/abs/1612.03211](https://arxiv.org/abs/1612.03211)\n\n**Towards biologically plausible deep learning**\n\n- intro: Yoshua\tBengio, NIPS’2016 Workshops\n- slides: [http://www.iro.umontreal.ca/~bengioy/talks/Brains+Bits-NIPS2016Workshop.pptx.pdf](http://www.iro.umontreal.ca/~bengioy/talks/Brains+Bits-NIPS2016Workshop.pptx.pdf)\n\n**Deep Learning and Its Applications to Machine Health Monitoring: A Survey**\n\n- arxiv: [https://arxiv.org/abs/1612.07640](https://arxiv.org/abs/1612.07640)\n\n**Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1701.01329](https://arxiv.org/abs/1701.01329)\n\n**Deep Learning Applications in Medical Imaging**\n\n- blog: [http://techemergence.com/deep-learning-medical-applications/](http://techemergence.com/deep-learning-medical-applications/)\n\n**Dermatologist-level classification of skin cancer with deep neural networks**\n\n- intro: Stanford University. Nature 2017\n- paper: [http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature21056.pdf](http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature21056.pdf)\n\n**Deep Learning for Health Informatics**\n\n- intro: Imperial College London\n- paper: [http://ieeexplore.ieee.org/abstract/document/7801947/](http://ieeexplore.ieee.org/abstract/document/7801947/)\n\n# Deep Learning for Fashion\n\n**Convolutional Neural Networks for Fashion Classification and Object Detection**\n\n- intro: CS231N project\n- paper: [http://cs231n.stanford.edu/reports/BLAO_KJAG_CS231N_FinalPaperFashionClassification.pdf](http://cs231n.stanford.edu/reports/BLAO_KJAG_CS231N_FinalPaperFashionClassification.pdf)\n\n**DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations**\n\n- intro: CVPR 2016\n- project page: [http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html](http://personal.ie.cuhk.edu.hk/~lz013/projects/DeepFashion.html)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf)\n\n**Deep Learning for Fast and Accurate Fashion Item Detection**\n\n- keywords:  MultiBox and Fast R-CNN, Kuznech-Fashion-156 and Kuznech-Fashion-205 fashion item detection datasets\n- paper: [https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf](https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf)\n\n**Deep Learning at GILT**\n\n- keywords: automated tagging, automatic dress faceting\n- blog: [http://tech.gilt.com/machine/learning,/deep/learning/2016/12/22/deep-learning-at-gilt](http://tech.gilt.com/machine/learning,/deep/learning/2016/12/22/deep-learning-at-gilt)\n\n**Working with Fashion Models**\n\n- blog: [https://making.lyst.com/2017/02/21/working-with-fashion-models/](https://making.lyst.com/2017/02/21/working-with-fashion-models/)\n- youtube: [https://www.youtube.com/watch?v=emr2qaCQOQs](https://www.youtube.com/watch?v=emr2qaCQOQs)\n\n**Fashion Forward: Forecasting Visual Style in Fashion**\n\n- intro: Karlsruhe Institute of Technology & The University of Texas at Austin\n- arxiv: [https://arxiv.org/abs/1705.06394](https://arxiv.org/abs/1705.06394)\n\n**StreetStyle: Exploring world-wide clothing styles from millions of photos**\n\n- homepage: [http://streetstyle.cs.cornell.edu/](http://streetstyle.cs.cornell.edu/)\n- arxiv: [https://arxiv.org/abs/1706.01869](https://arxiv.org/abs/1706.01869)\n- demo: [http://streetstyle.cs.cornell.edu/trends.html](http://streetstyle.cs.cornell.edu/trends.html)\n\n**Fashioning with Networks: Neural Style Transfer to Design Clothes**\n\n- intro: ML4Fashion 2017\n- arxiv: [https://arxiv.org/abs/1707.09899](https://arxiv.org/abs/1707.09899)\n\n**Deep Learning Our Way Through Fashion Week**\n\n[https://inside.edited.com/deep-learning-our-way-through-fashion-week-ea55bf50bab8](https://inside.edited.com/deep-learning-our-way-through-fashion-week-ea55bf50bab8)\n\n**Be Your Own Prada: Fashion Synthesis with Structural Coherence**\n\n- intro: ICCV 2017\n- paper: [http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Be_Your_Own_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Be_Your_Own_ICCV_2017_paper.pdf)\n- github: [https://github.com/zhusz/ICCV17-fashionGAN](https://github.com/zhusz/ICCV17-fashionGAN)\n\n# Others\n\n**Selfai: Predicting Facial Beauty in Selfies**\n\n**Selfai: A Method for Understanding Beauty in Selfies**\n\n- blog: [http://www.erogol.com/selfai-predicting-facial-beauty-selfies/](http://www.erogol.com/selfai-predicting-facial-beauty-selfies/)\n- github: [https://github.com/erogol/beauty.torch](https://github.com/erogol/beauty.torch)\n\n**Deep Learning Enables You to Hide Screen when Your Boss is Approaching**\n\n- blog: [http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/](http://ahogrammer.com/2016/11/15/deep-learning-enables-you-to-hide-screen-when-your-boss-is-approaching/)\n- github: [https://github.com/Hironsan/BossSensor](https://github.com/Hironsan/BossSensor)\n\n# Blogs\n\n**40 Ways Deep Learning is Eating the World**\n\n[https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d#.rxq8xvbfz](https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d#.rxq8xvbfz)\n\n**Applications**\n\n[http://www.deeplearningpatterns.com/doku.php/applications](http://www.deeplearningpatterns.com/doku.php/applications)\n\n**Systematic Approach To Applications Of Deep Learning**\n\n[https://gettocode.com/2016/11/25/systematic-approach-to-applications-of-deep-learning/](https://gettocode.com/2016/11/25/systematic-approach-to-applications-of-deep-learning/)\n\n# Resources\n\n**Deep Learning Gallery - a curated collection of deep learning projects**\n\n[http://deeplearninggallery.com/](http://deeplearninggallery.com/)\n","excerpt":"Applications DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations arxiv:  http://arxiv.org/abs/1510.02927 Some …","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations → Principles → The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Fake-News-Considerations-%E2%86%92-Principles-%E2%86%92-the-Institution-of-Socio-Economic-Values","url":"","items":[{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations-%e2%86%92-principles-%e2%86%92-the-institution-of-socio-economic-values/solutions-to-fakenews-linked-data-ontologies-and-verifiable-claims/","items":[]}]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between ‘privacy’ and ‘dignity’.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Embed Link","url":"/old-work-archives/2018-webizen-net-au/embed-link/","items":[]},{"title":"Posts","url":"/old-work-archives/2018-webizen-net-au/posts/","items":[]},{"title":"Privacy Policy","url":"/old-work-archives/2018-webizen-net-au/privacy-policy/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Pro Summary","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","title":"Fake News: Considerations → Principles → The Institution of Socio &#8211; Economic Values","lastUpdatedAt":"2022-12-28T19:29:53.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","title":"Handong1587","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","title":"Notes on Suffix Array and Manacher Algorithm","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","title":"Notes On Perceptrons","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","title":"Notes On Object Detection","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","title":"Notes On Caffe Development","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","title":"Notes On L-BFGS","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","title":"Softmax Vs Logistic Vs Sigmoid","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","title":"Notes On Deep Learning Training","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","title":"Notes On YOLO","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}