{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/",
    "result": {"data":{"mdx":{"id":"e70071a8-c38b-54aa-8902-2ace8b799ff5","tableOfContents":{"items":[{"url":"#papers","title":"Papers"},{"url":"#video-classification","title":"Video Classification","items":[{"url":"#action-detection--activity-recognition","title":"Action Detection / Activity Recognition"}]},{"url":"#event-recognition","title":"Event Recognition"},{"url":"#event-detection","title":"Event Detection"},{"url":"#abnormality--anomaly-detection","title":"Abnormality / Anomaly Detection"},{"url":"#video-prediction","title":"Video Prediction","items":[{"url":"#prednet","title":"PredNet"}]},{"url":"#video-tagging","title":"Video Tagging"},{"url":"#shot-boundary-detection","title":"Shot Boundary Detection"},{"url":"#video-action-segmentation","title":"Video Action Segmentation"},{"url":"#video2gif","title":"Video2GIF"},{"url":"#video2speech","title":"Video2Speech"},{"url":"#video-captioning","title":"Video Captioning"},{"url":"#video-summarization","title":"Video Summarization"},{"url":"#video-highlight-detection","title":"Video Highlight Detection"},{"url":"#video-understanding","title":"Video Understanding"},{"url":"#challenges","title":"Challenges"}]},"fields":{"title":"Video Applications","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications.md","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"Video Applications","description":null,"imageAlt":null,"tags":[],"date":"2015-10-09T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"deep_learning\",\n  \"title\": \"Video Applications\",\n  \"date\": \"2015-10-09T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"papers\"\n  }, \"Papers\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"You Lead, We Exceed: Labor-Free Video Concept Learningby Jointly Exploiting Web Videos and Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Lead\\u2013Exceed Neural Network (LENN), LSTM\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/CVPR16_webly_final.pdf\"\n  }, \"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/CVPR16_webly_final.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Fill in the Blank with Merging LSTMs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: for Large Scale Movie Description and Understanding Challenge (LSMDC) 2016, \\\"Movie fill-in-the-blank\\\" Challenge, UCF_CRCV\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Video-Fill-in-the-Blank (ViFitB)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.04062\"\n  }, \"https://arxiv.org/abs/1610.04062\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Pixel Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.00527\"\n  }, \"https://arxiv.org/abs/1610.00527\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Robust Video Synchronization using Unsupervised Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.05985\"\n  }, \"https://arxiv.org/abs/1610.05985\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Propagation Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. Max Planck Institute for Intelligent Systems & Bernstein Center for Computational Neuroscience\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://varunjampani.github.io/vpn/\"\n  }, \"https://varunjampani.github.io/vpn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.05478\"\n  }, \"https://arxiv.org/abs/1612.05478\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/varunjampani/video_prop_networks\"\n  }, \"https://github.com/varunjampani/video_prop_networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Frame Synthesis using Deep Voxel Flow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://liuziwei7.github.io/projects/VoxelFlow.html\"\n  }, \"https://liuziwei7.github.io/projects/VoxelFlow.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.02463\"\n  }, \"https://arxiv.org/abs/1702.02463\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Optimizing Deep CNN-Based Queries over Video Streams at Scale\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford InfoLab\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: NoScope. difference detectors, specialized models\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.02529\"\n  }, \"https://arxiv.org/abs/1703.02529\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/stanford-futuredata/noscope\"\n  }, \"https://github.com/stanford-futuredata/noscope\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/stanford-futuredata/tensorflow-noscope\"\n  }, \"https://github.com/stanford-futuredata/tensorflow-noscope\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"NoScope: 1000x Faster Deep Learning Queries over Video\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://dawn.cs.stanford.edu/2017/06/22/noscope/\"\n  }, \"http://dawn.cs.stanford.edu/2017/06/22/noscope/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. Stanford University & University of Southern California\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.02521\"\n  }, \"https://arxiv.org/abs/1703.02521\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ProcNets: Learning to Segment Procedures in Untrimmed and Unconstrained Videos\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.09788\"\n  }, \"https://arxiv.org/abs/1703.09788\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Learning Layers for Video Analysis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Baidu Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"The experiments demonstrated the potential applications of UL layers and online learning algorithm to head orientation estimation and moving object localization\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.08918\"\n  }, \"https://arxiv.org/abs/1705.08918\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Look, Listen and Learn\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"Audio-Visual Correspondence\\\" learning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.08168\"\n  }, \"https://arxiv.org/abs/1705.08168\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Imagination from a Single Image with Transformation Generation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Peking University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.04124\"\n  }, \"https://arxiv.org/abs/1706.04124\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gitpub327/VideoImagination\"\n  }, \"https://github.com/gitpub327/VideoImagination\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Learn from Noisy Web Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. Stanford University & CMU & Simon Fraser University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.02884\"\n  }, \"https://arxiv.org/abs/1706.02884\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Long Short-Term Memory Networks for Recognizing First Person Interactions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Accepted on the second International Workshop on Egocentric Perception, Interaction and Computing(EPIC) at International Conference on Computer Vision(ICCV-17)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.06495\"\n  }, \"https://arxiv.org/abs/1709.06495\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Binary Residual Representations for Domain-specific Video Streaming\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://research.nvidia.com/publication/2018-02_Learning-Binary-Residual\"\n  }, \"http://research.nvidia.com/publication/2018-02_Learning-Binary-Residual\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.05087\"\n  }, \"https://arxiv.org/abs/1712.05087\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Representation Learning Using Discriminative Pooling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.10628\"\n  }, \"https://arxiv.org/abs/1803.10628\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rethinking the Faster R-CNN Architecture for Temporal Action Localization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.07667\"\n  }, \"https://arxiv.org/abs/1804.07667\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Keyframe Detection in Human Action Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: two-stream ConvNet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.10021\"\n  }, \"https://arxiv.org/abs/1804.10021\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FFNet: Video Fast-Forwarding via Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.02792\"\n  }, \"https://arxiv.org/abs/1805.02792\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast forwarding Egocentric Videos by Listening and Watching\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.04620\"\n  }, \"https://arxiv.org/abs/1806.04620\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scanner: Efficient Video Analysis at Scale\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CMU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.07339\"\n  }, \"https://arxiv.org/abs/1805.07339\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Massively Parallel Video Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DeepMind & University of Oxford\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.03863\"\n  }, \"https://arxiv.org/abs/1806.03863\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Object Level Visual Reasoning in Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: LIRIS & Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.06157\"\n  }, \"https://arxiv.org/abs/1806.06157\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Time: Properties, Encoders and Evaluation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.06980\"\n  }, \"https://arxiv.org/abs/1807.06980\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Inserting Videos into Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.06571\"\n  }, \"https://arxiv.org/abs/1903.06571\"))), mdx(\"h1\", {\n    \"id\": \"video-classification\"\n  }, \"Video Classification\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large-scale Video Classification with Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2014\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/people/karpathy/deepvideo/\"\n  }, \"http://cs.stanford.edu/people/karpathy/deepvideo/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf/\"\n  }, \"www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Exploiting Image-trained CNN Architectures for Unconstrained Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Video-level event detection. extracting deep features for each frame, averaging frame-level deep features\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1503.04144\"\n  }, \"http://arxiv.org/abs/1503.04144\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Beyond Short Snippets: Deep Networks for Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CNN + LSTM\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1503.08909\"\n  }, \"http://arxiv.org/abs/1503.08909\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1eQ9zLZk\"\n  }, \"http://pan.baidu.com/s/1eQ9zLZk\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACM Multimedia, 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1504.01561\"\n  }, \"http://arxiv.org/abs/1504.01561\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Content Recognition with Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"author: Zuxuan Wu, Fudan University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vision.ouc.edu.cn/valse/slides/20160420/Zuxuan%20Wu%20-%20Video%20Content%20Recognition%20with%20Deep%20Learning-Zuxuan%20Wu.pdf\"\n  }, \"http://vision.ouc.edu.cn/valse/slides/20160420/Zuxuan%20Wu%20-%20Video%20Content%20Recognition%20with%20Deep%20Learning-Zuxuan%20Wu.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Content Recognition with Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"author: Yu-Gang Jiang, Lab for Big Video Data Analytics (BigVid), Fudan University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.yugangjiang.info/slides/DeepVideoTalk-2015.pdf\"\n  }, \"http://www.yugangjiang.info/slides/DeepVideoTalk-2015.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Efficient Large Scale Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.06250\"\n  }, \"http://arxiv.org/abs/1505.06250\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fusing Multi-Stream Deep Networks for Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.06086\"\n  }, \"http://arxiv.org/abs/1509.06086\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning End-to-end Video Classification with Rank-Pooling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jmlr.org/proceedings/papers/v48/fernando16.html\"\n  }, \"http://jmlr.org/proceedings/papers/v48/fernando16.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jmlr.csail.mit.edu/proceedings/papers/v48/fernando16.pdf\"\n  }, \"http://jmlr.csail.mit.edu/proceedings/papers/v48/fernando16.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"summary(by Hugo Larochelle): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.shortscience.org/paper?bibtexKey=conf/icml/FernandoG16#hlarochelle\"\n  }, \"http://www.shortscience.org/paper?bibtexKey=conf/icml/FernandoG16#hlarochelle\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for Video Classification and Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.06782\"\n  }, \"http://arxiv.org/abs/1609.06782\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Video Classification via Adaptive Cascading of Deep Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06453\"\n  }, \"https://arxiv.org/abs/1611.06453\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Feature Flow for Video Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: It provides a simple, fast, accurate, and end-to-end framework for video recognition (e.g., object detection and semantic segmentation in videos)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.07715\"\n  }, \"https://arxiv.org/abs/1611.07715\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, MXNet): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/msracver/Deep-Feature-Flow\"\n  }, \"https://github.com/msracver/Deep-Feature-Flow\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=J0rMHE6ehGw\"\n  }, \"https://www.youtube.com/watch?v=J0rMHE6ehGw\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large-Scale YouTube-8M Video Understanding with Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.04488\"\n  }, \"https://arxiv.org/abs/1706.04488\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Methods for Efficient Large Scale Video Labeling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Solution to the Kaggle's competition Google Cloud & YouTube-8M Video Understanding Challenge\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.04572\"\n  }, \"https://arxiv.org/abs/1706.04572\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mpekalski/Y8M\"\n  }, \"https://github.com/mpekalski/Y8M\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learnable pooling with Context Gating for video classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR17 Youtube 8M workshop. Kaggle 1st place\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.06905\"\n  }, \"https://arxiv.org/abs/1706.06905\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/antoine77340/LOUPE\"\n  }, \"https://github.com/antoine77340/LOUPE\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Aggregating Frame-level Features for Large-Scale Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Youtube-8M Challenge, 4th place\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.00803\"\n  }, \"https://arxiv.org/abs/1707.00803\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tensor-Train Recurrent Neural Networks for Video Classification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1707.01786\"\n  }, \"https://arxiv.org/abs/1707.01786\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Deep Recurrent Architecture for Video Understanding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Classification Challenge Track paper in CVPR 2017 Workshop on YouTube-8M Large-Scale Video Understanding\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.03296\"\n  }, \"https://arxiv.org/abs/1707.03296\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large-scale Video Classification guided by Batch Normalized LSTM Translator\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR2017 Workshop on Youtube-8M Large-scale Video Understanding\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.04045\"\n  }, \"https://arxiv.org/abs/1707.04045\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"UTS submission to Google YouTube-8M Challenge 2017\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR'17 Workshop on YouTube-8M\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.04143\"\n  }, \"https://arxiv.org/abs/1707.04143\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ffmpbgrnn/yt8m\"\n  }, \"https://github.com/ffmpbgrnn/yt8m\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A spatiotemporal model with visual attention for video classification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1707.02069\"\n  }, \"https://arxiv.org/abs/1707.02069\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cultivating DNN Diversity for Large Scale Video Labelling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017 Youtube-8M Workshop\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.04272\"\n  }, \"https://arxiv.org/abs/1707.04272\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attention Transfer from Web Images for Video Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACM Multimedia, 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.00973\"\n  }, \"https://arxiv.org/abs/1708.00973\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Non-local Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018. CMU & Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.07971\"\n  }, \"https://arxiv.org/abs/1711.07971\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe2): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/facebookresearch/video-nonlocal-net\"\n  }, \"https://github.com/facebookresearch/video-nonlocal-net\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.08200\"\n  }, \"https://arxiv.org/abs/1711.08200\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Appearance-and-Relation Networks for Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.09125\"\n  }, \"https://arxiv.org/abs/1711.09125\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wanglimin/ARTNet\"\n  }, \"https://github.com/wanglimin/ARTNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018. Google Research & University of California San Diego\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.04851\"\n  }, \"https://arxiv.org/abs/1712.04851\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Long Activity Video Understanding using Functional Object-Oriented Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1807.00983\"\n  }, \"https://arxiv.org/abs/1807.00983\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Architectures and Ensembles for Semantic Video Classification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1807.01026\"\n  }, \"https://arxiv.org/abs/1807.01026\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Discriminative Model for Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.08259\"\n  }, \"https://arxiv.org/abs/1807.08259\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Video Color Propagation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxuv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.03232\"\n  }, \"https://arxiv.org/abs/1808.03232\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Non-local NetVLAD Encoding for Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018 workshop on YouTube-8M Large-Scale Video Understanding\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tencent AI Lab & Fudan University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.00207\"\n  }, \"https://arxiv.org/abs/1810.00207\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learnable Pooling Methods for Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Youtube 8M ECCV18 Workshop\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.00530\"\n  }, \"https://arxiv.org/abs/1810.00530\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pomonam/LearnablePoolingMethods\"\n  }, \"https://github.com/pomonam/LearnablePoolingMethods\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018 workshop\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.05014\"\n  }, \"https://arxiv.org/abs/1811.05014\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/linrongc/youtube-8m\"\n  }, \"https://github.com/linrongc/youtube-8m\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"High Order Neural Networks for Video Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Fudan University, Carnegie Mellon University, Qiniu Inc., ByteDance AI Lab\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.07519\"\n  }, \"https://arxiv.org/abs/1811.07519\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TSM: Temporal Shift Module for Efficient Video Understanding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT & MIT-IBM Watson AI Lab\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.08383\"\n  }, \"https://arxiv.org/abs/1811.08383\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mit-han-lab/temporal-shift-module\"\n  }, \"https://github.com/mit-han-lab/temporal-shift-module\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SlowFast Networks for Video Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research (FAIR)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.03982\"\n  }, \"https://arxiv.org/abs/1812.03982\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Efficient Video Classification Using Fewer Frames\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1902.10640\"\n  }, \"https://arxiv.org/abs/1902.10640\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Classification with Channel-Separated Convolutional Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.02811\"\n  }, \"https://arxiv.org/abs/1904.02811\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Two-Stream Video Classification with Cross-Modality Attention\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1908.00497\"\n  }, \"https://arxiv.org/abs/1908.00497\")), mdx(\"h2\", {\n    \"id\": \"action-detection--activity-recognition\"\n  }, \"Action Detection / Activity Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"3d convolutional neural networks for human action recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.odu.edu/~sji/papers/pdf/Ji_ICML10.pdf\"\n  }, \"http://www.cs.odu.edu/~sji/papers/pdf/Ji_ICML10.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sequential Deep Learning for Human Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://liris.cnrs.fr/Documents/Liris-5228.pdf\"\n  }, \"http://liris.cnrs.fr/Documents/Liris-5228.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Two-stream convolutional networks for action recognition in videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1406.2199\"\n  }, \"http://arxiv.org/abs/1406.2199\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Finding action tubes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"built action models from shape and motion cues.\\nThey start from the image proposals and select the motion salient subset of them and\\nextract saptio-temporal features to represent the video using the CNNs.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1411.6031\"\n  }, \"http://arxiv.org/abs/1411.6031\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2015. TDD\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf/\"\n  }, \"www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"ext: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_105_ext.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_105_ext.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"poster: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://wanglimin.github.io/papers/WangQT_CVPR15_Poster.pdf\"\n  }, \"https://wanglimin.github.io/papers/WangQT_CVPR15_Poster.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wanglimin/TDD\"\n  }, \"https://github.com/wanglimin/TDD\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Action Recognition by Hierarchical Mid-level Action Elements\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cvgl.stanford.edu/papers/tian2015.pdf\"\n  }, \"http://cvgl.stanford.edu/papers/tian2015.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Contextual Action Recognition with R CNN\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.01197\"\n  }, \"http://arxiv.org/abs/1505.01197\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gkioxari/RstarCNN\"\n  }, \"https://github.com/gkioxari/RstarCNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards Good Practices for Very Deep Two-Stream ConvNets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1507.02159\"\n  }, \"http://arxiv.org/abs/1507.02159\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yjxiong/caffe\"\n  }, \"https://github.com/yjxiong/caffe\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Action Recognition using Visual Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: LSTM / RNN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.04119\"\n  }, \"http://arxiv.org/abs/1511.04119\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://shikharsharma.com/projects/action-recognition-attention/\"\n  }, \"http://shikharsharma.com/projects/action-recognition-attention/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Python/Theano): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kracwarlock/action-recognition-visual-attention\"\n  }, \"https://github.com/kracwarlock/action-recognition-visual-attention\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-end Learning of Action Detection from Frame Glimpses in Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ai.stanford.edu/~syyeung/frameglimpses.html\"\n  }, \"http://ai.stanford.edu/~syyeung/frameglimpses.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06984\"\n  }, \"http://arxiv.org/abs/1511.06984\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vision.stanford.edu/pdf/yeung2016cvpr.pdf\"\n  }, \"http://vision.stanford.edu/pdf/yeung2016cvpr.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-velocity neural networks for gesture recognition in videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.06829\"\n  }, \"http://arxiv.org/abs/1603.06829\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Active Learning for Online Recognition of Human Activities from Streaming Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.02855\"\n  }, \"http://arxiv.org/abs/1604.02855\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Two-Stream Network Fusion for Video Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.06573\"\n  }, \"http://arxiv.org/abs/1604.06573\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/feichtenhofer/twostreamfusion\"\n  }, \"https://github.com/feichtenhofer/twostreamfusion\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.08880\"\n  }, \"http://arxiv.org/abs/1604.08880\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Semantic Action Discovery from Video Collections\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.03324\"\n  }, \"http://arxiv.org/abs/1605.03324\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Anticipating Visual Representations from Unlabeled Video\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.mit.edu/vondrick/prediction.pdf\"\n  }, \"http://web.mit.edu/vondrick/prediction.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VideoLSTM Convolves, Attends and Flows for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.01794\"\n  }, \"http://arxiv.org/abs/1607.01794\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Attention Network for Action Recognition in Videos (HAN)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.06416\"\n  }, \"http://arxiv.org/abs/1607.06416\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.07043\"\n  }, \"http://arxiv.org/abs/1607.07043\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Connectionist Temporal Modeling for Weakly Supervised Action Labeling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.08584\"\n  }, \"http://arxiv.org/abs/1607.08584\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: won the 1st place in the untrimmed video classification task of ActivityNet Challenge 2016. TSN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.00797\"\n  }, \"http://arxiv.org/abs/1608.00797\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yjxiong/anet2016-cuhk\"\n  }, \"https://github.com/yjxiong/anet2016-cuhk\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Actionness Estimation Using Hybrid FCNs\")), mdx(\"img\", {\n    \"src\": \"http://wanglimin.github.io/actionness_hfcn/actionness.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016. H-FCN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://wanglimin.github.io/actionness_hfcn/index.html\"\n  }, \"http://wanglimin.github.io/actionness_hfcn/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf\"\n  }, \"http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wanglimin/actionness-estimation/\"\n  }, \"https://github.com/wanglimin/actionness-estimation/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-time Action Recognition with Enhanced Motion Vector CNNs\")), mdx(\"img\", {\n    \"src\": \"http://zbwglory.github.io/MV-CNN/framework.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://zbwglory.github.io/MV-CNN/index.html\"\n  }, \"http://zbwglory.github.io/MV-CNN/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://wanglimin.github.io/papers/ZhangWWQW_CVPR16.pdf\"\n  }, \"http://wanglimin.github.io/papers/ZhangWWQW_CVPR16.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zbwglory/MV-release\"\n  }, \"https://github.com/zbwglory/MV-release\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2016. HMDB51: 69.4%, UCF101: 94.2%\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.00859\"\n  }, \"http://arxiv.org/abs/1608.00859\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf\"\n  }, \"http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yjxiong/temporal-segment-networks\"\n  }, \"https://github.com/yjxiong/temporal-segment-networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal Segment Networks for Action Recognition in Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: An extension of submission \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.00859\"\n  }, \"http://arxiv.org/abs/1608.00859\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.02953\"\n  }, \"https://arxiv.org/abs/1705.02953\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Attention Network for Action Recognition in Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.06416\"\n  }, \"http://arxiv.org/abs/1607.06416\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepCAMP: Deep Convolutional Action & Attribute Mid-Level Patterns\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.03217\"\n  }, \"http://arxiv.org/abs/1608.03217\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.04339\"\n  }, \"http://arxiv.org/abs/1608.04339\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamic Image Networks for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://users.cecs.anu.edu.au/~sgould/papers/cvpr16-dynamic_images.pdf\"\n  }, \"http://users.cecs.anu.edu.au/~sgould/papers/cvpr16-dynamic_images.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hbilen/dynamic-image-nets\"\n  }, \"https://github.com/hbilen/dynamic-image-nets\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Human Action Recognition without Human\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.07876\"\n  }, \"http://arxiv.org/abs/1608.07876\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal Convolutional Networks: A Unified Approach to Action Segmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.08242\"\n  }, \"http://arxiv.org/abs/1608.08242\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"ECCV 2016 workshop: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://bravenewmotion.github.io/\"\n  }, \"http://bravenewmotion.github.io/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Bachelor Thesis Report at ETSETB TelecomBCN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://imatge-upc.github.io/activitynet-2016-cvprw/\"\n  }, \"https://imatge-upc.github.io/activitynet-2016-cvprw/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.08128\"\n  }, \"http://arxiv.org/abs/1608.08128\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/imatge-upc/activitynet-2016-cvprw\"\n  }, \"https://github.com/imatge-upc/activitynet-2016-cvprw\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sequential Deep Trajectory Descriptor for Action Recognition with Three-stream CNN\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.03056\"\n  }, \"http://arxiv.org/abs/1609.03056\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at Extremely Low Resolutions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.03898\"\n  }, \"https://arxiv.org/abs/1610.03898\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Spatiotemporal Residual Networks for Video Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.02155\"\n  }, \"https://arxiv.org/abs/1611.02155\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Action Recognition Based on Joint Trajectory Maps Using Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.02447\"\n  }, \"https://arxiv.org/abs/1611.02447\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Recurrent Neural Network for Mobile Human Activity Recognition with High Throughput\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.03607\"\n  }, \"https://arxiv.org/abs/1611.03607\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joint Network based Attention for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05215\"\n  }, \"https://arxiv.org/abs/1611.05215\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal Convolutional Networks for Action Segmentation and Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05267\"\n  }, \"https://arxiv.org/abs/1611.05267\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.08240\"\n  }, \"https://arxiv.org/abs/1611.08240\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ActionFlowNet: Learning Motion Representation for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.03052\"\n  }, \"https://arxiv.org/abs/1612.03052\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Higher-order Pooling of CNN Features via Kernel Linearization for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Australian Center for Robotic Vision & Data61/CSIRO\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.05432\"\n  }, \"https://arxiv.org/abs/1701.05432\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1703.10664\"\n  }, \"https://arxiv.org/abs/1703.10664\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal Action Detection with Structured Segment Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://yjxiong.me/others/ssn/\"\n  }, \"http://yjxiong.me/others/ssn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.06228\"\n  }, \"https://arxiv.org/abs/1704.06228\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yjxiong/action-detection\"\n  }, \"https://github.com/yjxiong/action-detection\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent Residual Learning for Action Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.08807\"\n  }, \"https://arxiv.org/abs/1706.08807\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Multi-scale Attention Networks for Action Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.07590\"\n  }, \"https://arxiv.org/abs/1708.07590\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Two-stream Flow-guided Convolutional Attention Networks for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: International Conference of Computer Vision Workshop (ICCVW), 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.09268\"\n  }, \"https://arxiv.org/abs/1708.09268\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Action Classification and Highlighting in Videos\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.09522\"\n  }, \"https://arxiv.org/abs/1708.09522\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-Time Action Detection in Video Surveillance using Sub-Action Descriptor with Multi-CNN\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1710.03383\"\n  }, \"https://arxiv.org/abs/1710.03383\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-end Video-level Representation Learning for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Deep networks with Temporal Pyramid Pooling (DTPP)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04161\"\n  }, \"https://arxiv.org/abs/1711.04161\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low Resolution Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.03983\"\n  }, \"https://arxiv.org/abs/1801.03983\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DiscrimNet: Semi-Supervised Action Recognition from Videos using Generative Adversarial Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.07230\"\n  }, \"https://arxiv.org/abs/1801.07230\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Fusion of Appearance based CNNs and Temporal evolution of Skeleton with LSTM for Daily Living Action Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.00421\"\n  }, \"https://arxiv.org/abs/1802.00421\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-Time End-to-End Action Detection with Two-Stream Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.08362\"\n  }, \"https://arxiv.org/abs/1802.08362\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Closer Look at Spatiotemporal Convolutions for Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018. Facebook Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: R(2+1)D and Mixed-Convolutions for Action Recognition.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://dutran.github.io/R2Plus1D/\"\n  }, \"https://dutran.github.io/R2Plus1D/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.11248\"\n  }, \"https://arxiv.org/abs/1711.11248\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/facebookresearch/R2Plus1D\"\n  }, \"https://github.com/facebookresearch/R2Plus1D\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VideoCapsuleNet: A Simplified Network for Action Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.08162\"\n  }, \"https://arxiv.org/abs/1805.08162\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Where and When to Look? Spatio-temporal Attention for Action Recognition in Videos\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.04511\"\n  }, \"https://arxiv.org/abs/1810.04511\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Relational Long Short-Term Memory for Video Action Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.07059\"\n  }, \"https://arxiv.org/abs/1811.07059\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Temporal Recurrent Networks for Online Action Detection\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.07391\"\n  }, \"https://arxiv.org/abs/1811.073910\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Action Transformer Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Carnegie Mellon University & DeepMind & University of Oxford\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Ranked first on the AVA (computer vision only) leaderboard of the ActivityNet Challenge 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://rohitgirdhar.github.io/ActionTransformer/\"\n  }, \"https://rohitgirdhar.github.io/ActionTransformer/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.02707\"\n  }, \"https://arxiv.org/abs/1812.02707\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"D3D: Distilled 3D Networks for Video Action Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google & University of Michigan & Princeton University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.08249\"\n  }, \"https://arxiv.org/abs/1812.08249\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1905.13417\"\n  }, \"https://arxiv.org/abs/1905.13417\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deformable Tube Network for Action Detection in Videos\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1907.01847\"\n  }, \"https://arxiv.org/abs/1907.01847\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1911.06644\"\n  }, \"https://arxiv.org/abs/1911.06644\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TubeR: Tube-Transformer for Action Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Amsterdam & Amazon Web Service\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2104.00969\"\n  }, \"https://arxiv.org/abs/2104.00969\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Revisiting Skeleton-based Action Recognition\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2104.13586\"\n  }, \"https://arxiv.org/abs/2104.13586\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-end Temporal Action Detection with Transformer\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2106.10271\"\n  }, \"https://arxiv.org/abs/2106.10271\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/xlliu7/TadTR\"\n  }, \"https://github.com/xlliu7/TadTR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"OadTR: Online Action Detection with Transformers\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2106.11149\"\n  }, \"https://arxiv.org/abs/2106.11149\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wangxiang1230/OadTR\"\n  }, \"https://github.com/wangxiang1230/OadTR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VideoLightFormer: Lightweight Action Recognition using Transformers\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2107.00451\"\n  }, \"https://arxiv.org/abs/2107.00451\")), mdx(\"h3\", {\n    \"id\": \"projects\"\n  }, \"Projects\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Torch Library for Action Recognition and Detection Using CNNs and LSTMs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CS231n student project report\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs231n.stanford.edu/reports2016/221_Report.pdf\"\n  }, \"http://cs231n.stanford.edu/reports2016/221_Report.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/garythung/torch-lrcn\"\n  }, \"https://github.com/garythung/torch-lrcn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"2016 ActivityNet action recognition challenge. CNN + LSTM approach. Multi-threaded loading.\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jrbtaylor/ActivityNet\"\n  }, \"https://github.com/jrbtaylor/ActivityNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LSTM for Human Activity Recognition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/\"\n  }, \"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(MXNet): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition\"\n  }, \"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scanner: Efficient Video Analysis at Scale\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Locate and recognize faces in a video, Detect shots in a film, Search videos by image\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/scanner-research/scanner\"\n  }, \"https://github.com/scanner-research/scanner\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Charades Starter Code for Activity Classification and Localization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Activity Recognition Algorithms for the Charades Dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gsig/charades-algorithms\"\n  }, \"https://github.com/gsig/charades-algorithms\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"NonLocalNetwork and Sequeeze-Excitation Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MXNet implementation of Non-Local and Squeeze-Excitation network\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/WillSuen/NonLocalandSEnet\"\n  }, \"https://github.com/WillSuen/NonLocalandSEnet\"))), mdx(\"h1\", {\n    \"id\": \"event-recognition\"\n  }, \"Event Recognition\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TagBook: A Semantic Video Representation without Supervision for Event Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.02899\"\n  }, \"http://arxiv.org/abs/1510.02899\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AENet: Learning Deep Audio Features for Video Analysis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.00599\"\n  }, \"https://arxiv.org/abs/1701.00599\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/znaoya/aenet\"\n  }, \"https://github.com/znaoya/aenet\"))), mdx(\"h1\", {\n    \"id\": \"event-detection\"\n  }, \"Event Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DevNet: A Deep Event Network for Multimedia Event Detection and Evidence Recounting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://120.52.72.47/winsty.net/c3pr90ntcsf0/papers/devnet.pdf\"\n  }, \"http://120.52.72.47/winsty.net/c3pr90ntcsf0/papers/devnet.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Detecting events and key actors in multi-person videos\")), mdx(\"img\", {\n    \"src\": \"https://tctechcrunch2011.files.wordpress.com/2016/06/basketball_actors.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.02917\"\n  }, \"http://arxiv.org/abs/1511.02917\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf/\"\n  }, \"www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vision.stanford.edu/pdf/johnson2016cvpr.pdf\"\n  }, \"http://vision.stanford.edu/pdf/johnson2016cvpr.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.leiphone.com/news/201606/l1TKIRFLO3DUFNNu.html\"\n  }, \"http://www.leiphone.com/news/201606/l1TKIRFLO3DUFNNu.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: INTERSPEECH 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1604.07160\"\n  }, \"https://arxiv.org/abs/1604.07160\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Efficient Action Detection in Untrimmed Videos via Multi-Task Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.07403\"\n  }, \"https://arxiv.org/abs/1612.07403\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joint Event Detection and Description in Continuous Video Streams\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Joint Event Detection and Description Network (JEDDi-Net)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.10250\"\n  }, \"https://arxiv.org/abs/1802.10250\"))), mdx(\"h1\", {\n    \"id\": \"abnormality--anomaly-detection\"\n  }, \"Abnormality / Anomaly Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fully Convolutional Neural Network for Fast Anomaly Detection in Crowded Scenes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.00866\"\n  }, \"http://arxiv.org/abs/1609.00866\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Anomaly Detection in Video Using Predictive Convolutional Long Short-Term Memory Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Rochester Institute of Technology\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.00390\"\n  }, \"https://arxiv.org/abs/1612.00390\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Abnormal Event Detection in Videos using Spatiotemporal Autoencoder\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.01546\"\n  }, \"https://arxiv.org/abs/1701.01546\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yshean/abnormal-spatiotemporal-ae\"\n  }, \"https://github.com/yshean/abnormal-spatiotemporal-ae\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Abnormal Event Detection in Videos using Generative Adversarial Nets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Best Paper / Student Paper Award Finalist, IEEE International Conference on Image Processing (ICIP), 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.09644\"\n  }, \"https://arxiv.org/abs/1708.09644\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.09121\"\n  }, \"https://arxiv.org/abs/1709.09121\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Uncanny Vision Solutions\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.03149\"\n  }, \"https://arxiv.org/abs/1801.03149\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"STAN: Spatio-Temporal Adversarial Networks for Abnormal Event Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICASSP 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.08381\"\n  }, \"https://arxiv.org/abs/1804.08381\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.11223\"\n  }, \"https://arxiv.org/abs/1805.11223\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attentioned Convolutional LSTM InpaintingNetwork for Anomaly Detection in Videos\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.10228\"\n  }, \"https://arxiv.org/abs/1811.10228\")), mdx(\"h1\", {\n    \"id\": \"video-prediction\"\n  }, \"Video Prediction\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep multi-scale video prediction beyond mean square error\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.05440\"\n  }, \"http://arxiv.org/abs/1511.05440\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/coupriec/VideoPredictionICLR2016\"\n  }, \"https://github.com/coupriec/VideoPredictionICLR2016\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/dyelax/Adversarial_Video_Generation\"\n  }, \"https://github.com/dyelax/Adversarial_Video_Generation\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.nyu.edu/~mathieu/iclr2016.html\"\n  }, \"http://cs.nyu.edu/~mathieu/iclr2016.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Learning for Physical Interaction through Video Prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1605.07157\"\n  }, \"https://arxiv.org/abs/1605.07157\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tensorflow/models/tree/master/video_prediction\"\n  }, \"https://github.com/tensorflow/models/tree/master/video_prediction\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generating Videos with Scene Dynamics\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The model learns to generate tiny videos using adversarial networks\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.mit.edu/vondrick/tinyvideo/\"\n  }, \"http://web.mit.edu/vondrick/tinyvideo/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.mit.edu/vondrick/tinyvideo/paper.pdf\"\n  }, \"http://web.mit.edu/vondrick/tinyvideo/paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cvondrick/videogan\"\n  }, \"https://github.com/cvondrick/videogan\"))), mdx(\"h2\", {\n    \"id\": \"prednet\"\n  }, \"PredNet\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://coxlab.github.io/prednet/\"\n  }, \"https://coxlab.github.io/prednet/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.08104\"\n  }, \"http://arxiv.org/abs/1605.08104\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/coxlab/prednet\"\n  }, \"https://github.com/coxlab/prednet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/e-lab/torch-prednet\"\n  }, \"https://github.com/e-lab/torch-prednet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Diversity encouraged learning of unsupervised LSTM ensemble for neural activity video prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.04899\"\n  }, \"https://arxiv.org/abs/1611.04899\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Ladder Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"inro: NIPS 2016 workshop on ML for Spatiotemporal Forecasting\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.01756\"\n  }, \"https://arxiv.org/abs/1612.01756\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Learning of Long-Term Motion Dynamics for Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.01821\"\n  }, \"https://arxiv.org/abs/1701.01821\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NCCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.04125\"\n  }, \"https://arxiv.org/abs/1702.04125\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fully Context-Aware Video Prediction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ETH Zurich & NNAISENSE\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: unsupervised learning through video prediction, Parallel Multi-Dimensional LSTM\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sites.google.com/view/contextvp\"\n  }, \"https://sites.google.com/view/contextvp\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.08518\"\n  }, \"https://arxiv.org/abs/1710.08518\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Novel Video Prediction for Large-scale Scene using Optical Flow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Victoria & Tongji University & Horizon Robotics\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.12243\"\n  }, \"https://arxiv.org/abs/1805.12243\"))), mdx(\"h1\", {\n    \"id\": \"video-tagging\"\n  }, \"Video Tagging\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Automatic Image and Video Tagging\")), mdx(\"img\", {\n    \"src\": \"http://i2.wp.com/scottge.net/wp-content/uploads/2015/06/imagetagging.png?w=576\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://scottge.net/2015/06/30/automatic-image-and-video-tagging/\"\n  }, \"http://scottge.net/2015/06/30/automatic-image-and-video-tagging/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tagging YouTube music videos with deep learning - Alexandre Passant\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Clarifai's deep learning API\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://apassant.net/2015/07/03/tagging-youtube-music-clarifai-deep-learning/\"\n  }, \"http://apassant.net/2015/07/03/tagging-youtube-music-clarifai-deep-learning/\"))), mdx(\"h1\", {\n    \"id\": \"shot-boundary-detection\"\n  }, \"Shot Boundary Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large-scale, Fast and Accurate Shot Boundary Detection through Spatio-temporal Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.03281\"\n  }, \"https://arxiv.org/abs/1705.03281\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: obtains state-of-the-art results while running at an unprecedented speed of more than 120x real-time.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.08214\"\n  }, \"https://arxiv.org/abs/1705.08214\"))), mdx(\"h1\", {\n    \"id\": \"video-action-segmentation\"\n  }, \"Video Action Segmentation\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Rochester\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.07818\"\n  }, \"https://arxiv.org/abs/1705.07818\"))), mdx(\"h1\", {\n    \"id\": \"video2gif\"\n  }, \"Video2GIF\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video2GIF: Automatic Generation of Animated GIFs from Video (Robust Deep RankNet)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 3D CNN, ranking model, Huber loss, 100K GIFs/video sources dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.04850\"\n  }, \"http://arxiv.org/abs/1605.04850\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(dataset): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gyglim/video2gif_dataset\"\n  }, \"https://github.com/gyglim/video2gif_dataset\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"results: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://video2gif.info/\"\n  }, \"http://video2gif.info/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo site: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://people.ee.ethz.ch/~gyglim/work_public/autogif/\"\n  }, \"http://people.ee.ethz.ch/~gyglim/work_public/autogif/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"review: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://motherboard.vice.com/read/these-fire-gifs-were-made-by-artificial-intelligence-yahoo\"\n  }, \"http://motherboard.vice.com/read/these-fire-gifs-were-made-by-artificial-intelligence-yahoo\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Creating Animated GIFs Automatically from Video\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://yahooresearch.tumblr.com/post/148009705216/creating-animated-gifs-automatically-from-video\"\n  }, \"https://yahooresearch.tumblr.com/post/148009705216/creating-animated-gifs-automatically-from-video\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GIF2Video: Color Dequantization and Temporal Interpolation of GIF images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stony Brook University & Megvii Research USA & UCLA\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1901.02840\"\n  }, \"https://arxiv.org/abs/1901.02840\"))), mdx(\"h1\", {\n    \"id\": \"video2speech\"\n  }, \"Video2Speech\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Vid2speech: Speech Reconstruction from Silent Video\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICASSP 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.vision.huji.ac.il/vid2speech/\"\n  }, \"http://www.vision.huji.ac.il/vid2speech/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.00495\"\n  }, \"https://arxiv.org/abs/1701.00495\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/arielephrat/vid2speech\"\n  }, \"https://github.com/arielephrat/vid2speech\"))), mdx(\"h1\", {\n    \"id\": \"video-captioning\"\n  }, \"Video Captioning\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://handong1587.github.io/deep_learning/2015/10/09/image-video-captioning.html#video-captioning\"\n  }, \"http://handong1587.github.io/deep_learning/2015/10/09/image-video-captioning.html#video-captioning\")), mdx(\"h1\", {\n    \"id\": \"video-summarization\"\n  }, \"Video Summarization\"), mdx(\"p\", null, \"Video summarization produces a short summary of a full-length video and ideally encapsulates its most informative parts,\\nalleviates the problem of video browsing, editing and indexing.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Summarization with Long Short-term Memory\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.08110\"\n  }, \"http://arxiv.org/abs/1605.08110\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepVideo: Video Summarization using Temporal Sequence Modelling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CS231n student project report\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs231n.stanford.edu/reports2016/216_Report.pdf\"\n  }, \"http://cs231n.stanford.edu/reports2016/216_Report.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Video Trailers\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.01819\"\n  }, \"http://arxiv.org/abs/1609.01819\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Summarization using Deep Semantic Features\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"inro: ACCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.08758\"\n  }, \"http://arxiv.org/abs/1609.08758\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CNN-Based Prediction of Frame-Level Shot Importance for Video Summarization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: International Conference on new Trends in Computer Sciences (ICTCS), Amman-Jordan, 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.07023\"\n  }, \"https://arxiv.org/abs/1708.07023\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Summarization with Attention-Based Encoder-Decoder Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.09545\"\n  }, \"https://arxiv.org/abs/1708.09545\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018. Chinese Academy of Sciences & Queen Mary University of London\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://kaiyangzhou.github.io/project_vsumm_reinforce/index.html\"\n  }, \"https://kaiyangzhou.github.io/project_vsumm_reinforce/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.00054\"\n  }, \"https://arxiv.org/abs/1801.00054\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//KaiyangZhou/vsumm-reinforce\"\n  }, \"https://github.com//KaiyangZhou/vsumm-reinforce\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Viewpoint-aware Video Summarization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.02843\"\n  }, \"https://arxiv.org/abs/1804.02843\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DTR-GAN: Dilated Temporal Relational Adversarial Network for Video Summarization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.11228\"\n  }, \"https://arxiv.org/abs/1804.11228\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Video Summarization Using Unpaired Data\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.12174\"\n  }, \"https://arxiv.org/abs/1805.12174\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Summarization Using Fully Convolutional Sequence Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.10538\"\n  }, \"https://arxiv.org/abs/1805.10538\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Summarisation by Classification with Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.03089\"\n  }, \"https://arxiv.org/abs/1807.03089\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Query-Conditioned Three-Player Adversarial Network for Video Summarization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.06677\"\n  }, \"https://arxiv.org/abs/1807.06677\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Discriminative Feature Learning for Unsupervised Video Summarization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.09791\"\n  }, \"https://arxiv.org/abs/1811.09791\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rethinking the Evaluation of Video Summaries\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019 poster\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.11328\"\n  }, \"https://arxiv.org/abs/1903.11328\"))), mdx(\"h1\", {\n    \"id\": \"video-highlight-detection\"\n  }, \"Video Highlight Detection\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: rely on an assumption that highlights of an event category are more frequently captured in short videos than non-highlights\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.01442\"\n  }, \"http://arxiv.org/abs/1510.01442\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: wearable device\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://research.microsoft.com/apps/pubs/default.aspx?id=264919\"\n  }, \"http://research.microsoft.com/apps/pubs/default.aspx?id=264919\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Using Deep Learning to Find Basketball Highlights\")), mdx(\"img\", {\n    \"src\": \"https://cloud.githubusercontent.com/assets/10147637/7966603/228179fe-09f3-11e5-9ea7-31e76c8248fe.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://public.hudl.com/bits/archives/2015/06/05/highlights/?utm_source=tuicool&utm_medium=referral\"\n  }, \"http://public.hudl.com/bits/archives/2015/06/05/highlights/?utm_source=tuicool&utm_medium=referral\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Real-Time Video Highlights for Yahoo Esports\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.08780\"\n  }, \"https://arxiv.org/abs/1611.08780\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Deep Ranking Model for Spatio-Temporal Highlight Detection from a 360 Video\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.10312\"\n  }, \"https://arxiv.org/abs/1801.10312\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Nanyang Technological University & Google Research, Zurich\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: personalized highlight detection (PHD)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.06604\"\n  }, \"https://arxiv.org/abs/1804.06604\"))), mdx(\"h1\", {\n    \"id\": \"video-understanding\"\n  }, \"Video Understanding\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scale Up Video Understandingwith Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 2016, Tsinghua University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/iiis.tsinghua.edu.cn/~jianli/courses/ATCS2016spring/talk_chuang.pptx/\"\n  }, \"iiis.tsinghua.edu.cn/~jianli/courses/ATCS2016spring/talk_chuang.pptx\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Slicing Convolutional Neural Network for Crowd Video Understanding\")), mdx(\"img\", {\n    \"src\": \"http://www.ee.cuhk.edu.hk/~jshao/SCNN_files/fig_network5.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: It aims at learning generic spatio-temporal features from crowd videos, especially for long-term temporal learning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.ee.cuhk.edu.hk/~jshao/SCNN.html\"\n  }, \"http://www.ee.cuhk.edu.hk/~jshao/SCNN.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.ee.cuhk.edu.hk/~jshao/papers_jshao/jshao_cvpr16_scnn.pdf\"\n  }, \"http://www.ee.cuhk.edu.hk/~jshao/papers_jshao/jshao_cvpr16_scnn.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/amandajshao/Slicing-CNN\"\n  }, \"https://github.com/amandajshao/Slicing-CNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rethinking Spatiotemporal Feature Learning For Video Understanding\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.04851\"\n  }, \"https://arxiv.org/abs/1712.04851\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Video Understanding\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.03316\"\n  }, \"https://arxiv.org/abs/1809.03316\")), mdx(\"h1\", {\n    \"id\": \"challenges\"\n  }, \"Challenges\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"THUMOS Challenge 2014\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://crcv.ucf.edu/THUMOS14/home.html\"\n  }, \"http://crcv.ucf.edu/THUMOS14/home.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"download: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://crcv.ucf.edu/THUMOS14/download.html\"\n  }, \"http://crcv.ucf.edu/THUMOS14/download.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"THUMOS Challenge 2015\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.thumos.info/\"\n  }, \"http://www.thumos.info/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"download: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.thumos.info/download.html\"\n  }, \"http://www.thumos.info/download.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ActivityNet Challenge 2016\")), mdx(\"img\", {\n    \"src\": \"http://activity-net.org/challenges/2016/images/anet_cover.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://activity-net.org/challenges/2016/\"\n  }, \"http://activity-net.org/challenges/2016/\"))));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Video Applications\ndate: 2015-10-09\n---\n\n# Papers\n\n**You Lead, We Exceed: Labor-Free Video Concept Learningby Jointly Exploiting Web Videos and Images**\n\n- intro: CVPR 2016\n- intro: LeadExceed Neural Network (LENN), LSTM\n- paper: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/CVPR16_webly_final.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/CVPR16_webly_final.pdf)\n\n**Video Fill in the Blank with Merging LSTMs**\n\n- intro: for Large Scale Movie Description and Understanding Challenge (LSMDC) 2016, \"Movie fill-in-the-blank\" Challenge, UCF_CRCV\n- intro: Video-Fill-in-the-Blank (ViFitB)\n- arxiv: [https://arxiv.org/abs/1610.04062](https://arxiv.org/abs/1610.04062)\n\n**Video Pixel Networks**\n\n- intro: Google DeepMind\n- arxiv: [https://arxiv.org/abs/1610.00527](https://arxiv.org/abs/1610.00527)\n\n**Robust Video Synchronization using Unsupervised Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1610.05985](https://arxiv.org/abs/1610.05985)\n\n**Video Propagation Networks**\n\n- intro: CVPR 2017. Max Planck Institute for Intelligent Systems & Bernstein Center for Computational Neuroscience\n- project page: [https://varunjampani.github.io/vpn/](https://varunjampani.github.io/vpn/)\n- arxiv: [https://arxiv.org/abs/1612.05478](https://arxiv.org/abs/1612.05478)\n- github(Caffe): [https://github.com/varunjampani/video_prop_networks](https://github.com/varunjampani/video_prop_networks)\n\n**Video Frame Synthesis using Deep Voxel Flow**\n\n- project page: [https://liuziwei7.github.io/projects/VoxelFlow.html](https://liuziwei7.github.io/projects/VoxelFlow.html)\n- arxiv: [https://arxiv.org/abs/1702.02463](https://arxiv.org/abs/1702.02463)\n\n**Optimizing Deep CNN-Based Queries over Video Streams at Scale**\n\n- intro: Stanford InfoLab\n- keywords: NoScope. difference detectors, specialized models\n- arxiv: [https://arxiv.org/abs/1703.02529](https://arxiv.org/abs/1703.02529)\n- github: [https://github.com/stanford-futuredata/noscope](https://github.com/stanford-futuredata/noscope)\n- github: [https://github.com/stanford-futuredata/tensorflow-noscope](https://github.com/stanford-futuredata/tensorflow-noscope)\n\n**NoScope: 1000x Faster Deep Learning Queries over Video**\n\n[http://dawn.cs.stanford.edu/2017/06/22/noscope/](http://dawn.cs.stanford.edu/2017/06/22/noscope/)\n\n**Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos**\n\n- intro: CVPR 2017. Stanford University & University of Southern California\n- arxiv: [https://arxiv.org/abs/1703.02521](https://arxiv.org/abs/1703.02521)\n\n**ProcNets: Learning to Segment Procedures in Untrimmed and Unconstrained Videos**\n\n[https://arxiv.org/abs/1703.09788](https://arxiv.org/abs/1703.09788)\n\n**Unsupervised Learning Layers for Video Analysis**\n\n- intro: Baidu Research\n- intro: \"The experiments demonstrated the potential applications of UL layers and online learning algorithm to head orientation estimation and moving object localization\"\n- arxiv: [https://arxiv.org/abs/1705.08918](https://arxiv.org/abs/1705.08918)\n\n**Look, Listen and Learn**\n\n- intro: DeepMind\n- intro: \"Audio-Visual Correspondence\" learning\n- arxiv: [https://arxiv.org/abs/1705.08168](https://arxiv.org/abs/1705.08168)\n\n**Video Imagination from a Single Image with Transformation Generation**\n\n- intro: Peking University\n- arxiv: [https://arxiv.org/abs/1706.04124](https://arxiv.org/abs/1706.04124)\n- github: [https://github.com/gitpub327/VideoImagination](https://github.com/gitpub327/VideoImagination)\n\n**Learning to Learn from Noisy Web Videos**\n\n- intro: CVPR 2017. Stanford University & CMU & Simon Fraser University\n- arxiv: [https://arxiv.org/abs/1706.02884](https://arxiv.org/abs/1706.02884)\n\n**Convolutional Long Short-Term Memory Networks for Recognizing First Person Interactions**\n\n- intro: Accepted on the second International Workshop on Egocentric Perception, Interaction and Computing(EPIC) at International Conference on Computer Vision(ICCV-17)\n- arxiv: [https://arxiv.org/abs/1709.06495](https://arxiv.org/abs/1709.06495)\n\n**Learning Binary Residual Representations for Domain-specific Video Streaming**\n\n- intro: AAAI 2018\n- project page: [http://research.nvidia.com/publication/2018-02_Learning-Binary-Residual](http://research.nvidia.com/publication/2018-02_Learning-Binary-Residual)\n- arxiv: [https://arxiv.org/abs/1712.05087](https://arxiv.org/abs/1712.05087)\n\n**Video Representation Learning Using Discriminative Pooling**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1803.10628](https://arxiv.org/abs/1803.10628)\n\n**Rethinking the Faster R-CNN Architecture for Temporal Action Localization**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.07667](https://arxiv.org/abs/1804.07667)\n\n**Deep Keyframe Detection in Human Action Videos**\n\n- intro: two-stream ConvNet\n- arxiv: [https://arxiv.org/abs/1804.10021](https://arxiv.org/abs/1804.10021)\n\n**FFNet: Video Fast-Forwarding via Reinforcement Learning**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1805.02792](https://arxiv.org/abs/1805.02792)\n\n**Fast forwarding Egocentric Videos by Listening and Watching**\n\n[https://arxiv.org/abs/1806.04620](https://arxiv.org/abs/1806.04620)\n\n**Scanner: Efficient Video Analysis at Scale**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1805.07339](https://arxiv.org/abs/1805.07339)\n\n**Massively Parallel Video Networks**\n\n- intro: DeepMind & University of Oxford\n- arxiv: [https://arxiv.org/abs/1806.03863](https://arxiv.org/abs/1806.03863)\n\n**Object Level Visual Reasoning in Videos**\n\n- intro: LIRIS & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1806.06157](https://arxiv.org/abs/1806.06157)\n\n**Video Time: Properties, Encoders and Evaluation**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.06980](https://arxiv.org/abs/1807.06980)\n\n**Inserting Videos into Videos**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1903.06571](https://arxiv.org/abs/1903.06571)\n\n# Video Classification\n\n**Large-scale Video Classification with Convolutional Neural Networks**\n\n- intro: CVPR 2014\n- project page: [http://cs.stanford.edu/people/karpathy/deepvideo/](http://cs.stanford.edu/people/karpathy/deepvideo/)\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.pdf)\n\n**Exploiting Image-trained CNN Architectures for Unconstrained Video Classification**\n\n- intro: Video-level event detection. extracting deep features for each frame, averaging frame-level deep features\n- arxiv: [http://arxiv.org/abs/1503.04144](http://arxiv.org/abs/1503.04144)\n\n**Beyond Short Snippets: Deep Networks for Video Classification**\n\n- intro: CNN + LSTM\n- arxiv: [http://arxiv.org/abs/1503.08909](http://arxiv.org/abs/1503.08909)\n- demo: [http://pan.baidu.com/s/1eQ9zLZk](http://pan.baidu.com/s/1eQ9zLZk)\n\n**Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification**\n\n- intro: ACM Multimedia, 2015\n- arxiv: [http://arxiv.org/abs/1504.01561](http://arxiv.org/abs/1504.01561)\n\n**Video Content Recognition with Deep Learning**\n\n- author: Zuxuan Wu, Fudan University\n- slides: [http://vision.ouc.edu.cn/valse/slides/20160420/Zuxuan%20Wu%20-%20Video%20Content%20Recognition%20with%20Deep%20Learning-Zuxuan%20Wu.pdf](http://vision.ouc.edu.cn/valse/slides/20160420/Zuxuan%20Wu%20-%20Video%20Content%20Recognition%20with%20Deep%20Learning-Zuxuan%20Wu.pdf)\n\n**Video Content Recognition with Deep Learning**\n\n- author: Yu-Gang Jiang, Lab for Big Video Data Analytics (BigVid), Fudan University\n- slides: [http://www.yugangjiang.info/slides/DeepVideoTalk-2015.pdf](http://www.yugangjiang.info/slides/DeepVideoTalk-2015.pdf)\n\n**Efficient Large Scale Video Classification**\n\n- intro: Google\n- arxiv: [http://arxiv.org/abs/1505.06250](http://arxiv.org/abs/1505.06250)\n\n**Fusing Multi-Stream Deep Networks for Video Classification**\n\n- arxiv: [http://arxiv.org/abs/1509.06086](http://arxiv.org/abs/1509.06086)\n\n**Learning End-to-end Video Classification with Rank-Pooling**\n\n- paper: [http://jmlr.org/proceedings/papers/v48/fernando16.html](http://jmlr.org/proceedings/papers/v48/fernando16.html)\n- paper: [http://jmlr.csail.mit.edu/proceedings/papers/v48/fernando16.pdf](http://jmlr.csail.mit.edu/proceedings/papers/v48/fernando16.pdf)\n- summary(by Hugo Larochelle): [http://www.shortscience.org/paper?bibtexKey=conf/icml/FernandoG16#hlarochelle](http://www.shortscience.org/paper?bibtexKey=conf/icml/FernandoG16#hlarochelle)\n\n**Deep Learning for Video Classification and Captioning**\n\n- arxiv: [http://arxiv.org/abs/1609.06782](http://arxiv.org/abs/1609.06782)\n\n**Fast Video Classification via Adaptive Cascading of Deep Models**\n\n- arxiv: [https://arxiv.org/abs/1611.06453](https://arxiv.org/abs/1611.06453)\n\n**Deep Feature Flow for Video Recognition**\n\n- intro: CVPR 2017\n- intro: It provides a simple, fast, accurate, and end-to-end framework for video recognition (e.g., object detection and semantic segmentation in videos)\n- arxiv: [https://arxiv.org/abs/1611.07715](https://arxiv.org/abs/1611.07715)\n- github(official, MXNet): [https://github.com/msracver/Deep-Feature-Flow](https://github.com/msracver/Deep-Feature-Flow)\n- youtube: [https://www.youtube.com/watch?v=J0rMHE6ehGw](https://www.youtube.com/watch?v=J0rMHE6ehGw)\n\n**Large-Scale YouTube-8M Video Understanding with Deep Neural Networks**\n\n[https://arxiv.org/abs/1706.04488](https://arxiv.org/abs/1706.04488)\n\n**Deep Learning Methods for Efficient Large Scale Video Labeling**\n\n- intro: Solution to the Kaggle's competition Google Cloud & YouTube-8M Video Understanding Challenge\n- arxiv: [https://arxiv.org/abs/1706.04572](https://arxiv.org/abs/1706.04572)\n- github: [https://github.com/mpekalski/Y8M](https://github.com/mpekalski/Y8M)\n\n**Learnable pooling with Context Gating for video classification**\n\n- intro: CVPR17 Youtube 8M workshop. Kaggle 1st place\n- arxiv: [https://arxiv.org/abs/1706.06905](https://arxiv.org/abs/1706.06905)\n- github: [https://github.com/antoine77340/LOUPE](https://github.com/antoine77340/LOUPE)\n\n**Aggregating Frame-level Features for Large-Scale Video Classification**\n\n- intro: Youtube-8M Challenge, 4th place\n- arxiv: [https://arxiv.org/abs/1707.00803](https://arxiv.org/abs/1707.00803)\n\n**Tensor-Train Recurrent Neural Networks for Video Classification**\n\n[https://arxiv.org/abs/1707.01786](https://arxiv.org/abs/1707.01786)\n\n**Hierarchical Deep Recurrent Architecture for Video Understanding**\n\n- intro: Classification Challenge Track paper in CVPR 2017 Workshop on YouTube-8M Large-Scale Video Understanding\n- arxiv: [https://arxiv.org/abs/1707.03296](https://arxiv.org/abs/1707.03296)\n\n**Large-scale Video Classification guided by Batch Normalized LSTM Translator**\n\n- intro: CVPR2017 Workshop on Youtube-8M Large-scale Video Understanding\n- arxiv: [https://arxiv.org/abs/1707.04045](https://arxiv.org/abs/1707.04045)\n\n**UTS submission to Google YouTube-8M Challenge 2017**\n\n- intro: CVPR'17 Workshop on YouTube-8M\n- arxiv: [https://arxiv.org/abs/1707.04143](https://arxiv.org/abs/1707.04143)\n- github: [https://github.com/ffmpbgrnn/yt8m](https://github.com/ffmpbgrnn/yt8m)\n\n**A spatiotemporal model with visual attention for video classification**\n\n[https://arxiv.org/abs/1707.02069](https://arxiv.org/abs/1707.02069)\n\n**Cultivating DNN Diversity for Large Scale Video Labelling**\n\n- intro: CVPR 2017 Youtube-8M Workshop\n- arxiv: [https://arxiv.org/abs/1707.04272](https://arxiv.org/abs/1707.04272)\n\n**Attention Transfer from Web Images for Video Recognition**\n\n- intro: ACM Multimedia, 2017\n- arxiv: [https://arxiv.org/abs/1708.00973](https://arxiv.org/abs/1708.00973)\n\n**Non-local Neural Networks**\n\n- intro: CVPR 2018. CMU & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1711.07971](https://arxiv.org/abs/1711.07971)\n- github(Caffe2): [https://github.com/facebookresearch/video-nonlocal-net](https://github.com/facebookresearch/video-nonlocal-net)\n\n**Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification**\n\n[https://arxiv.org/abs/1711.08200](https://arxiv.org/abs/1711.08200)\n\n**Appearance-and-Relation Networks for Video Classification**\n\n- arxiv: [https://arxiv.org/abs/1711.09125](https://arxiv.org/abs/1711.09125)\n- github: [https://github.com/wanglimin/ARTNet](https://github.com/wanglimin/ARTNet)\n\n**Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification**\n\n- intro: ECCV 2018. Google Research & University of California San Diego\n- arxiv: [https://arxiv.org/abs/1712.04851](https://arxiv.org/abs/1712.04851)\n\n**Long Activity Video Understanding using Functional Object-Oriented Network**\n\n[https://arxiv.org/abs/1807.00983](https://arxiv.org/abs/1807.00983)\n\n**Deep Architectures and Ensembles for Semantic Video Classification**\n\n[https://arxiv.org/abs/1807.01026](https://arxiv.org/abs/1807.01026)\n\n**Deep Discriminative Model for Video Classification**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.08259](https://arxiv.org/abs/1807.08259)\n\n**Deep Video Color Propagation**\n\n- intro: BMVC 2018\n- arxuv: [https://arxiv.org/abs/1808.03232](https://arxiv.org/abs/1808.03232)\n\n**Non-local NetVLAD Encoding for Video Classification**\n\n- intro: ECCV 2018 workshop on YouTube-8M Large-Scale Video Understanding\n- intro: Tencent AI Lab & Fudan University\n- arxiv: [https://arxiv.org/abs/1810.00207](https://arxiv.org/abs/1810.00207)\n\n**Learnable Pooling Methods for Video Classification**\n\n- intro: Youtube 8M ECCV18 Workshop\n- arxiv: [https://arxiv.org/abs/1810.00530](https://arxiv.org/abs/1810.00530)\n- github: [https://github.com/pomonam/LearnablePoolingMethods](https://github.com/pomonam/LearnablePoolingMethods)\n\n**NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification**\n\n- intro: ECCV 2018 workshop\n- arxiv: [https://arxiv.org/abs/1811.05014](https://arxiv.org/abs/1811.05014)\n- github: [https://github.com/linrongc/youtube-8m](https://github.com/linrongc/youtube-8m)\n\n**High Order Neural Networks for Video Classification**\n\n- intro: Fudan University, Carnegie Mellon University, Qiniu Inc., ByteDance AI Lab\n- arxiv: [https://arxiv.org/abs/1811.07519](https://arxiv.org/abs/1811.07519)\n\n**TSM: Temporal Shift Module for Efficient Video Understanding**\n\n- intro: ICCV 2019\n- intro: MIT & MIT-IBM Watson AI Lab\n- arxiv: [https://arxiv.org/abs/1811.08383](https://arxiv.org/abs/1811.08383)\n- github: [https://github.com/mit-han-lab/temporal-shift-module](https://github.com/mit-han-lab/temporal-shift-module)\n\n**SlowFast Networks for Video Recognition**\n\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/1812.03982](https://arxiv.org/abs/1812.03982)\n\n**Efficient Video Classification Using Fewer Frames**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1902.10640](https://arxiv.org/abs/1902.10640)\n\n**Video Classification with Channel-Separated Convolutional Networks**\n\n- intro: Facebook AI\n- arxiv: [https://arxiv.org/abs/1904.02811](https://arxiv.org/abs/1904.02811)\n\n**Two-Stream Video Classification with Cross-Modality Attention**\n\n[https://arxiv.org/abs/1908.00497](https://arxiv.org/abs/1908.00497)\n\n## Action Detection / Activity Recognition\n\n**3d convolutional neural networks for human action recognition**\n\n- paper: [http://www.cs.odu.edu/~sji/papers/pdf/Ji_ICML10.pdf](http://www.cs.odu.edu/~sji/papers/pdf/Ji_ICML10.pdf)\n\n**Sequential Deep Learning for Human Action Recognition**\n\n- paper: [http://liris.cnrs.fr/Documents/Liris-5228.pdf](http://liris.cnrs.fr/Documents/Liris-5228.pdf)\n\n**Two-stream convolutional networks for action recognition in videos**\n\n- arxiv: [http://arxiv.org/abs/1406.2199](http://arxiv.org/abs/1406.2199)\n\n**Finding action tubes**\n\n- intro: \"built action models from shape and motion cues. \nThey start from the image proposals and select the motion salient subset of them and\nextract saptio-temporal features to represent the video using the CNNs.\"\n- arxiv: [http://arxiv.org/abs/1411.6031](http://arxiv.org/abs/1411.6031)\n\n**Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition**\n\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf)\n\n**Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors**\n\n- intro: CVPR 2015. TDD\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf)\n- ext: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_105_ext.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_105_ext.pdf)\n- poster: [https://wanglimin.github.io/papers/WangQT_CVPR15_Poster.pdf](https://wanglimin.github.io/papers/WangQT_CVPR15_Poster.pdf)\n- github: [https://github.com/wanglimin/TDD](https://github.com/wanglimin/TDD)\n\n**Action Recognition by Hierarchical Mid-level Action Elements**\n\n- paper: [http://cvgl.stanford.edu/papers/tian2015.pdf](http://cvgl.stanford.edu/papers/tian2015.pdf)\n\n**Contextual Action Recognition with R CNN**\n\n- arxiv: [http://arxiv.org/abs/1505.01197](http://arxiv.org/abs/1505.01197)\n- github: [https://github.com/gkioxari/RstarCNN](https://github.com/gkioxari/RstarCNN)\n\n**Towards Good Practices for Very Deep Two-Stream ConvNets**\n\n- arxiv: [http://arxiv.org/abs/1507.02159](http://arxiv.org/abs/1507.02159)\n- github: [https://github.com/yjxiong/caffe](https://github.com/yjxiong/caffe)\n\n**Action Recognition using Visual Attention**\n\n- intro: LSTM / RNN\n- arxiv: [http://arxiv.org/abs/1511.04119](http://arxiv.org/abs/1511.04119)\n- project page: [http://shikharsharma.com/projects/action-recognition-attention/](http://shikharsharma.com/projects/action-recognition-attention/)\n- github(Python/Theano): [https://github.com/kracwarlock/action-recognition-visual-attention](https://github.com/kracwarlock/action-recognition-visual-attention)\n\n**End-to-end Learning of Action Detection from Frame Glimpses in Videos**\n\n- intro: CVPR 2016\n- project page: [http://ai.stanford.edu/~syyeung/frameglimpses.html](http://ai.stanford.edu/~syyeung/frameglimpses.html)\n- arxiv: [http://arxiv.org/abs/1511.06984](http://arxiv.org/abs/1511.06984)\n- paper: [http://vision.stanford.edu/pdf/yeung2016cvpr.pdf](http://vision.stanford.edu/pdf/yeung2016cvpr.pdf)\n\n**Multi-velocity neural networks for gesture recognition in videos**\n\n- arxiv: [http://arxiv.org/abs/1603.06829](http://arxiv.org/abs/1603.06829)\n\n**Active Learning for Online Recognition of Human Activities from Streaming Videos**\n\n- arxiv: [http://arxiv.org/abs/1604.02855](http://arxiv.org/abs/1604.02855)\n\n**Convolutional Two-Stream Network Fusion for Video Action Recognition**\n\n- arxiv: [http://arxiv.org/abs/1604.06573](http://arxiv.org/abs/1604.06573)\n- github: [https://github.com/feichtenhofer/twostreamfusion](https://github.com/feichtenhofer/twostreamfusion)\n\n**Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables**\n\n- arxiv: [http://arxiv.org/abs/1604.08880](http://arxiv.org/abs/1604.08880)\n\n**Unsupervised Semantic Action Discovery from Video Collections**\n\n- arxiv: [http://arxiv.org/abs/1605.03324](http://arxiv.org/abs/1605.03324)\n\n**Anticipating Visual Representations from Unlabeled Video**\n\n- paper: [http://web.mit.edu/vondrick/prediction.pdf](http://web.mit.edu/vondrick/prediction.pdf)\n\n**VideoLSTM Convolves, Attends and Flows for Action Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.01794](http://arxiv.org/abs/1607.01794)\n\n**Hierarchical Attention Network for Action Recognition in Videos (HAN)**\n\n- arxiv: [http://arxiv.org/abs/1607.06416](http://arxiv.org/abs/1607.06416)\n\n**Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition**\n\n- arxiv: [http://arxiv.org/abs/1607.07043](http://arxiv.org/abs/1607.07043)\n\n**Connectionist Temporal Modeling for Weakly Supervised Action Labeling**\n\n- arxiv: [http://arxiv.org/abs/1607.08584](http://arxiv.org/abs/1607.08584)\n\n**CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016**\n\n- intro: won the 1st place in the untrimmed video classification task of ActivityNet Challenge 2016. TSN\n- arxiv: [http://arxiv.org/abs/1608.00797](http://arxiv.org/abs/1608.00797)\n- github: [https://github.com/yjxiong/anet2016-cuhk](https://github.com/yjxiong/anet2016-cuhk)\n\n**Actionness Estimation Using Hybrid FCNs**\n\n![](http://wanglimin.github.io/actionness_hfcn/actionness.png)\n\n- intro: CVPR 2016. H-FCN\n- project page: [http://wanglimin.github.io/actionness_hfcn/index.html](http://wanglimin.github.io/actionness_hfcn/index.html)\n- paper: [http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf](http://wanglimin.github.io/papers/WangQTV_CVPR16.pdf)\n- github: [https://github.com/wanglimin/actionness-estimation/](https://github.com/wanglimin/actionness-estimation/)\n\n**Real-time Action Recognition with Enhanced Motion Vector CNNs**\n\n![](http://zbwglory.github.io/MV-CNN/framework.jpg)\n\n- intro: CVPR 2016\n- project page: [http://zbwglory.github.io/MV-CNN/index.html](http://zbwglory.github.io/MV-CNN/index.html)\n- paper: [http://wanglimin.github.io/papers/ZhangWWQW_CVPR16.pdf](http://wanglimin.github.io/papers/ZhangWWQW_CVPR16.pdf)\n- github: [https://github.com/zbwglory/MV-release](https://github.com/zbwglory/MV-release)\n\n**Temporal Segment Networks: Towards Good Practices for Deep Action Recognition**\n\n- intro: ECCV 2016. HMDB51: 69.4%, UCF101: 94.2%\n- arxiv: [http://arxiv.org/abs/1608.00859](http://arxiv.org/abs/1608.00859)\n- paper: [http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf](http://wanglimin.github.io/papers/WangXWQLTV_ECCV16.pdf)\n- github: [https://github.com/yjxiong/temporal-segment-networks](https://github.com/yjxiong/temporal-segment-networks)\n\n**Temporal Segment Networks for Action Recognition in Videos**\n\n- intro: An extension of submission [http://arxiv.org/abs/1608.00859](http://arxiv.org/abs/1608.00859)\n- arxiv: [https://arxiv.org/abs/1705.02953](https://arxiv.org/abs/1705.02953)\n\n**Hierarchical Attention Network for Action Recognition in Videos**\n\n- arxiv: [http://arxiv.org/abs/1607.06416](http://arxiv.org/abs/1607.06416)\n\n**DeepCAMP: Deep Convolutional Action & Attribute Mid-Level Patterns**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1608.03217](http://arxiv.org/abs/1608.03217)\n\n**Depth2Action: Exploring Embedded Depth for Large-Scale Action Recognition**\n\n- arxiv: [http://arxiv.org/abs/1608.04339](http://arxiv.org/abs/1608.04339)\n\n**Dynamic Image Networks for Action Recognition**\n\n- intro: CVPR 2016\n- arxiv: [http://users.cecs.anu.edu.au/~sgould/papers/cvpr16-dynamic_images.pdf](http://users.cecs.anu.edu.au/~sgould/papers/cvpr16-dynamic_images.pdf)\n- github: [https://github.com/hbilen/dynamic-image-nets](https://github.com/hbilen/dynamic-image-nets)\n\n**Human Action Recognition without Human**\n\n- arxiv: [http://arxiv.org/abs/1608.07876](http://arxiv.org/abs/1608.07876)\n\n**Temporal Convolutional Networks: A Unified Approach to Action Segmentation**\n\n- arxiv: [http://arxiv.org/abs/1608.08242](http://arxiv.org/abs/1608.08242)\n- ECCV 2016 workshop: [http://bravenewmotion.github.io/](http://bravenewmotion.github.io/)\n\n**Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks**\n\n- intro: Bachelor Thesis Report at ETSETB TelecomBCN\n- project page: [https://imatge-upc.github.io/activitynet-2016-cvprw/](https://imatge-upc.github.io/activitynet-2016-cvprw/)\n- arxiv: [http://arxiv.org/abs/1608.08128](http://arxiv.org/abs/1608.08128)\n- github: [https://github.com/imatge-upc/activitynet-2016-cvprw](https://github.com/imatge-upc/activitynet-2016-cvprw)\n\n**Sequential Deep Trajectory Descriptor for Action Recognition with Three-stream CNN**\n\n- arxiv: [http://arxiv.org/abs/1609.03056](http://arxiv.org/abs/1609.03056)\n\n**Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at Extremely Low Resolutions**\n\n- arxiv: [https://arxiv.org/abs/1610.03898](https://arxiv.org/abs/1610.03898)\n\n**Spatiotemporal Residual Networks for Video Action Recognition**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1611.02155](https://arxiv.org/abs/1611.02155)\n\n**Action Recognition Based on Joint Trajectory Maps Using Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.02447](https://arxiv.org/abs/1611.02447)\n\n**Deep Recurrent Neural Network for Mobile Human Activity Recognition with High Throughput**\n\n- arxiv: [https://arxiv.org/abs/1611.03607](https://arxiv.org/abs/1611.03607)\n\n**Joint Network based Attention for Action Recognition**\n\n- arxiv: [https://arxiv.org/abs/1611.05215](https://arxiv.org/abs/1611.05215)\n\n**Temporal Convolutional Networks for Action Segmentation and Detection**\n\n- arxiv: [https://arxiv.org/abs/1611.05267](https://arxiv.org/abs/1611.05267)\n\n**AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos**\n\n- arxiv: [https://arxiv.org/abs/1611.08240](https://arxiv.org/abs/1611.08240)\n\n**ActionFlowNet: Learning Motion Representation for Action Recognition**\n\n- arxiv: [https://arxiv.org/abs/1612.03052](https://arxiv.org/abs/1612.03052)\n\n**Higher-order Pooling of CNN Features via Kernel Linearization for Action Recognition**\n\n- intro: Australian Center for Robotic Vision & Data61/CSIRO\n- arxiv: [https://arxiv.org/abs/1701.05432](https://arxiv.org/abs/1701.05432)\n\n**Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos**\n\n[https://arxiv.org/abs/1703.10664](https://arxiv.org/abs/1703.10664)\n\n**Temporal Action Detection with Structured Segment Networks**\n\n- project page: [http://yjxiong.me/others/ssn/](http://yjxiong.me/others/ssn/)\n- arxiv: [https://arxiv.org/abs/1704.06228](https://arxiv.org/abs/1704.06228)\n- github: [https://github.com/yjxiong/action-detection](https://github.com/yjxiong/action-detection)\n\n**Recurrent Residual Learning for Action Recognition**\n\n[https://arxiv.org/abs/1706.08807](https://arxiv.org/abs/1706.08807)\n\n**Hierarchical Multi-scale Attention Networks for Action Recognition**\n\n[https://arxiv.org/abs/1708.07590](https://arxiv.org/abs/1708.07590)\n\n**Two-stream Flow-guided Convolutional Attention Networks for Action Recognition**\n\n- intro: International Conference of Computer Vision Workshop (ICCVW), 2017\n- arxiv: [https://arxiv.org/abs/1708.09268](https://arxiv.org/abs/1708.09268)\n\n**Action Classification and Highlighting in Videos**\n\n[https://arxiv.org/abs/1708.09522](https://arxiv.org/abs/1708.09522)\n\n**Real-Time Action Detection in Video Surveillance using Sub-Action Descriptor with Multi-CNN**\n\n[https://arxiv.org/abs/1710.03383](https://arxiv.org/abs/1710.03383)\n\n**End-to-end Video-level Representation Learning for Action Recognition**\n\n- keywords: Deep networks with Temporal Pyramid Pooling (DTPP)\n- arxiv: [https://arxiv.org/abs/1711.04161](https://arxiv.org/abs/1711.04161)\n\n**Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low Resolution Action Recognition**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.03983](https://arxiv.org/abs/1801.03983)\n\n**DiscrimNet: Semi-Supervised Action Recognition from Videos using Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1801.07230](https://arxiv.org/abs/1801.07230)\n\n**A Fusion of Appearance based CNNs and Temporal evolution of Skeleton with LSTM for Daily Living Action Recognition**\n\n[https://arxiv.org/abs/1802.00421](https://arxiv.org/abs/1802.00421)\n\n**Real-Time End-to-End Action Detection with Two-Stream Networks**\n\n[https://arxiv.org/abs/1802.08362](https://arxiv.org/abs/1802.08362)\n\n**A Closer Look at Spatiotemporal Convolutions for Action Recognition**\n\n- intro: CVPR 2018. Facebook Research\n- intro: R(2+1)D and Mixed-Convolutions for Action Recognition.\n- project page: [https://dutran.github.io/R2Plus1D/](https://dutran.github.io/R2Plus1D/)\n- arxiv: [https://arxiv.org/abs/1711.11248](https://arxiv.org/abs/1711.11248)\n- github: [https://github.com/facebookresearch/R2Plus1D](https://github.com/facebookresearch/R2Plus1D)\n\n**VideoCapsuleNet: A Simplified Network for Action Detection**\n\n[https://arxiv.org/abs/1805.08162](https://arxiv.org/abs/1805.08162)\n\n**Where and When to Look? Spatio-temporal Attention for Action Recognition in Videos**\n\n[https://arxiv.org/abs/1810.04511](https://arxiv.org/abs/1810.04511)\n\n**Relational Long Short-Term Memory for Video Action Recognition**\n\n[https://arxiv.org/abs/1811.07059](https://arxiv.org/abs/1811.07059)\n\n**Temporal Recurrent Networks for Online Action Detection**\n\n[https://arxiv.org/abs/1811.073910](https://arxiv.org/abs/1811.07391)\n\n**Video Action Transformer Network**\n\n- intro: Carnegie Mellon University & DeepMind & University of Oxford\n- intro: Ranked first on the AVA (computer vision only) leaderboard of the ActivityNet Challenge 2018\n- project page: [https://rohitgirdhar.github.io/ActionTransformer/](https://rohitgirdhar.github.io/ActionTransformer/)\n- arxiv: [https://arxiv.org/abs/1812.02707](https://arxiv.org/abs/1812.02707)\n\n**D3D: Distilled 3D Networks for Video Action Recognition**\n\n- intro: Google & University of Michigan & Princeton University\n- arxiv: [https://arxiv.org/abs/1812.08249](https://arxiv.org/abs/1812.08249)\n\n**TACNet: Transition-Aware Context Network for Spatio-Temporal Action Detection**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1905.13417](https://arxiv.org/abs/1905.13417)\n\n**Deformable Tube Network for Action Detection in Videos**\n\n[https://arxiv.org/abs/1907.01847](https://arxiv.org/abs/1907.01847)\n\n**You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization**\n\n[https://arxiv.org/abs/1911.06644](https://arxiv.org/abs/1911.06644)\n\n**TubeR: Tube-Transformer for Action Detection**\n\n- intro: University of Amsterdam & Amazon Web Service\n- arxiv: [https://arxiv.org/abs/2104.00969](https://arxiv.org/abs/2104.00969)\n\n**Revisiting Skeleton-based Action Recognition**\n\n[https://arxiv.org/abs/2104.13586](https://arxiv.org/abs/2104.13586)\n\n**End-to-end Temporal Action Detection with Transformer**\n\n- arxiv: [https://arxiv.org/abs/2106.10271](https://arxiv.org/abs/2106.10271)\n- github: [https://github.com/xlliu7/TadTR](https://github.com/xlliu7/TadTR)\n\n**OadTR: Online Action Detection with Transformers**\n\n- arxiv: [https://arxiv.org/abs/2106.11149](https://arxiv.org/abs/2106.11149)\n- github: [https://github.com/wangxiang1230/OadTR](https://github.com/wangxiang1230/OadTR)\n\n**VideoLightFormer: Lightweight Action Recognition using Transformers**\n\n[https://arxiv.org/abs/2107.00451](https://arxiv.org/abs/2107.00451)\n\n### Projects\n\n**A Torch Library for Action Recognition and Detection Using CNNs and LSTMs**\n\n- intro: CS231n student project report\n- paper: [http://cs231n.stanford.edu/reports2016/221_Report.pdf](http://cs231n.stanford.edu/reports2016/221_Report.pdf)\n- github: [https://github.com/garythung/torch-lrcn](https://github.com/garythung/torch-lrcn)\n\n**2016 ActivityNet action recognition challenge. CNN + LSTM approach. Multi-threaded loading.**\n\n- github: [https://github.com/jrbtaylor/ActivityNet](https://github.com/jrbtaylor/ActivityNet)\n\n**LSTM for Human Activity Recognition**\n\n- github: [https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/)\n- github(MXNet): [https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition](https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/HumanActivityRecognition)\n\n**Scanner: Efficient Video Analysis at Scale**\n\n- intro: Locate and recognize faces in a video, Detect shots in a film, Search videos by image\n- github: [https://github.com/scanner-research/scanner](https://github.com/scanner-research/scanner)\n\n**Charades Starter Code for Activity Classification and Localization**\n\n- intro: Activity Recognition Algorithms for the Charades Dataset\n- github: [https://github.com/gsig/charades-algorithms](https://github.com/gsig/charades-algorithms)\n\n**NonLocalNetwork and Sequeeze-Excitation Network**\n\n- intro: MXNet implementation of Non-Local and Squeeze-Excitation network\n- github: [https://github.com/WillSuen/NonLocalandSEnet](https://github.com/WillSuen/NonLocalandSEnet)\n\n# Event Recognition\n\n**TagBook: A Semantic Video Representation without Supervision for Event Detection**\n\n- arxiv: [http://arxiv.org/abs/1510.02899](http://arxiv.org/abs/1510.02899)\n\n**AENet: Learning Deep Audio Features for Video Analysis**\n\n- arxiv: [https://arxiv.org/abs/1701.00599](https://arxiv.org/abs/1701.00599)\n- github: [https://github.com/znaoya/aenet](https://github.com/znaoya/aenet)\n\n# Event Detection\n\n**DevNet: A Deep Event Network for Multimedia Event Detection and Evidence Recounting**\n\n- paper: [http://120.52.72.47/winsty.net/c3pr90ntcsf0/papers/devnet.pdf](http://120.52.72.47/winsty.net/c3pr90ntcsf0/papers/devnet.pdf)\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf)\n\n**Detecting events and key actors in multi-person videos**\n\n![](https://tctechcrunch2011.files.wordpress.com/2016/06/basketball_actors.jpg)\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1511.02917](http://arxiv.org/abs/1511.02917)\n- paper: [www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf](www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf)\n- paper: [http://vision.stanford.edu/pdf/johnson2016cvpr.pdf](http://vision.stanford.edu/pdf/johnson2016cvpr.pdf)\n- blog: [http://www.leiphone.com/news/201606/l1TKIRFLO3DUFNNu.html](http://www.leiphone.com/news/201606/l1TKIRFLO3DUFNNu.html)\n\n**Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection**\n\n- intro: INTERSPEECH 2016\n- arxiv: [https://arxiv.org/abs/1604.07160](https://arxiv.org/abs/1604.07160)\n\n**Efficient Action Detection in Untrimmed Videos via Multi-Task Learning**\n\n- arxiv: [https://arxiv.org/abs/1612.07403](https://arxiv.org/abs/1612.07403)\n\n**Joint Event Detection and Description in Continuous Video Streams**\n\n- intro: Joint Event Detection and Description Network (JEDDi-Net)\n- arxiv: [https://arxiv.org/abs/1802.10250](https://arxiv.org/abs/1802.10250)\n\n# Abnormality / Anomaly Detection\n\n**Fully Convolutional Neural Network for Fast Anomaly Detection in Crowded Scenes**\n\n- arxiv: [http://arxiv.org/abs/1609.00866](http://arxiv.org/abs/1609.00866)\n\n**Anomaly Detection in Video Using Predictive Convolutional Long Short-Term Memory Networks**\n\n- intro: Rochester Institute of Technology\n- arxiv: [https://arxiv.org/abs/1612.00390](https://arxiv.org/abs/1612.00390)\n\n**Abnormal Event Detection in Videos using Spatiotemporal Autoencoder**\n\n- arxiv: [https://arxiv.org/abs/1701.01546](https://arxiv.org/abs/1701.01546)\n- github: [https://github.com/yshean/abnormal-spatiotemporal-ae](https://github.com/yshean/abnormal-spatiotemporal-ae)\n\n**Abnormal Event Detection in Videos using Generative Adversarial Nets**\n\n- intro: Best Paper / Student Paper Award Finalist, IEEE International Conference on Image Processing (ICIP), 2017\n- arxiv: [https://arxiv.org/abs/1708.09644](https://arxiv.org/abs/1708.09644)\n\n**Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.09121](https://arxiv.org/abs/1709.09121)\n\n**An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos**\n\n- intro: Uncanny Vision Solutions\n- arxiv: [https://arxiv.org/abs/1801.03149](https://arxiv.org/abs/1801.03149)\n\n**STAN: Spatio-Temporal Adversarial Networks for Abnormal Event Detection**\n\n- intro: ICASSP 2018\n- arxiv: [https://arxiv.org/abs/1804.08381](https://arxiv.org/abs/1804.08381)\n\n**Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder**\n\n[https://arxiv.org/abs/1805.11223](https://arxiv.org/abs/1805.11223)\n\n**Attentioned Convolutional LSTM InpaintingNetwork for Anomaly Detection in Videos**\n\n[https://arxiv.org/abs/1811.10228](https://arxiv.org/abs/1811.10228)\n\n# Video Prediction\n\n**Deep multi-scale video prediction beyond mean square error**\n\n- intro: ICLR 2016\n- arxiv: [http://arxiv.org/abs/1511.05440](http://arxiv.org/abs/1511.05440)\n- github: [https://github.com/coupriec/VideoPredictionICLR2016](https://github.com/coupriec/VideoPredictionICLR2016)\n- github(TensorFlow): [https://github.com/dyelax/Adversarial_Video_Generation](https://github.com/dyelax/Adversarial_Video_Generation)\n- demo: [http://cs.nyu.edu/~mathieu/iclr2016.html](http://cs.nyu.edu/~mathieu/iclr2016.html)\n\n**Unsupervised Learning for Physical Interaction through Video Prediction**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1605.07157](https://arxiv.org/abs/1605.07157)\n- github: [https://github.com/tensorflow/models/tree/master/video_prediction](https://github.com/tensorflow/models/tree/master/video_prediction)\n\n**Generating Videos with Scene Dynamics**\n\n- intro: NIPS 2016\n- intro: The model learns to generate tiny videos using adversarial networks\n- project page: [http://web.mit.edu/vondrick/tinyvideo/](http://web.mit.edu/vondrick/tinyvideo/)\n- paper: [http://web.mit.edu/vondrick/tinyvideo/paper.pdf](http://web.mit.edu/vondrick/tinyvideo/paper.pdf)\n- github: [https://github.com/cvondrick/videogan](https://github.com/cvondrick/videogan)\n\n## PredNet\n\n**Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning**\n\n- project page: [https://coxlab.github.io/prednet/](https://coxlab.github.io/prednet/)\n- arxiv: [http://arxiv.org/abs/1605.08104](http://arxiv.org/abs/1605.08104)\n- github: [https://github.com/coxlab/prednet](https://github.com/coxlab/prednet)\n- github: [https://github.com/e-lab/torch-prednet](https://github.com/e-lab/torch-prednet)\n\n**Diversity encouraged learning of unsupervised LSTM ensemble for neural activity video prediction**\n\n- arxiv: [https://arxiv.org/abs/1611.04899](https://arxiv.org/abs/1611.04899)\n\n**Video Ladder Networks**\n\n- inro: NIPS 2016 workshop on ML for Spatiotemporal Forecasting\n- arxiv: [https://arxiv.org/abs/1612.01756](https://arxiv.org/abs/1612.01756)\n\n**Unsupervised Learning of Long-Term Motion Dynamics for Videos**\n\n- intro: Stanford University\n- arxiv: [https://arxiv.org/abs/1701.01821](https://arxiv.org/abs/1701.01821)\n\n**One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network**\n\n- intro: NCCV 2016\n- arxiv: [https://arxiv.org/abs/1702.04125](https://arxiv.org/abs/1702.04125)\n\n**Fully Context-Aware Video Prediction**\n\n- intro: ETH Zurich & NNAISENSE\n- keywords: unsupervised learning through video prediction, Parallel Multi-Dimensional LSTM\n- project page: [https://sites.google.com/view/contextvp](https://sites.google.com/view/contextvp)\n- arxiv: [https://arxiv.org/abs/1710.08518](https://arxiv.org/abs/1710.08518)\n\n**Novel Video Prediction for Large-scale Scene using Optical Flow**\n\n- intro: University of Victoria & Tongji University & Horizon Robotics\n- arxiv: [https://arxiv.org/abs/1805.12243](https://arxiv.org/abs/1805.12243)\n\n# Video Tagging\n\n**Automatic Image and Video Tagging**\n\n![](http://i2.wp.com/scottge.net/wp-content/uploads/2015/06/imagetagging.png?w=576)\n\n- blog: [http://scottge.net/2015/06/30/automatic-image-and-video-tagging/](http://scottge.net/2015/06/30/automatic-image-and-video-tagging/)\n\n**Tagging YouTube music videos with deep learning - Alexandre Passant**\n\n- keywords: Clarifai's deep learning API\n- blog: [http://apassant.net/2015/07/03/tagging-youtube-music-clarifai-deep-learning/](http://apassant.net/2015/07/03/tagging-youtube-music-clarifai-deep-learning/)\n\n# Shot Boundary Detection\n\n**Large-scale, Fast and Accurate Shot Boundary Detection through Spatio-temporal Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1705.03281](https://arxiv.org/abs/1705.03281)\n\n**Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks**\n\n- intro: obtains state-of-the-art results while running at an unprecedented speed of more than 120x real-time.\n- arxiv: [https://arxiv.org/abs/1705.08214](https://arxiv.org/abs/1705.08214)\n\n# Video Action Segmentation\n\n**TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation**\n\n- intro: University of Rochester\n- arxiv: [https://arxiv.org/abs/1705.07818](https://arxiv.org/abs/1705.07818)\n\n# Video2GIF\n\n**Video2GIF: Automatic Generation of Animated GIFs from Video (Robust Deep RankNet)**\n\n- intro: 3D CNN, ranking model, Huber loss, 100K GIFs/video sources dataset\n- arxiv: [http://arxiv.org/abs/1605.04850](http://arxiv.org/abs/1605.04850)\n- github(dataset): [https://github.com/gyglim/video2gif_dataset](https://github.com/gyglim/video2gif_dataset)\n- results: [http://video2gif.info/](http://video2gif.info/)\n- demo site: [http://people.ee.ethz.ch/~gyglim/work_public/autogif/](http://people.ee.ethz.ch/~gyglim/work_public/autogif/)\n- review: [http://motherboard.vice.com/read/these-fire-gifs-were-made-by-artificial-intelligence-yahoo](http://motherboard.vice.com/read/these-fire-gifs-were-made-by-artificial-intelligence-yahoo)\n\n**Creating Animated GIFs Automatically from Video**\n\n[https://yahooresearch.tumblr.com/post/148009705216/creating-animated-gifs-automatically-from-video](https://yahooresearch.tumblr.com/post/148009705216/creating-animated-gifs-automatically-from-video)\n\n**GIF2Video: Color Dequantization and Temporal Interpolation of GIF images**\n\n- intro: Stony Brook University & Megvii Research USA & UCLA\n- arxiv: [https://arxiv.org/abs/1901.02840](https://arxiv.org/abs/1901.02840)\n\n# Video2Speech\n\n**Vid2speech: Speech Reconstruction from Silent Video**\n\n- intro: ICASSP 2017\n- project page: [http://www.vision.huji.ac.il/vid2speech/](http://www.vision.huji.ac.il/vid2speech/)\n- arxiv: [https://arxiv.org/abs/1701.00495](https://arxiv.org/abs/1701.00495)\n- github(official): [https://github.com/arielephrat/vid2speech](https://github.com/arielephrat/vid2speech)\n\n# Video Captioning\n\n[http://handong1587.github.io/deep_learning/2015/10/09/image-video-captioning.html#video-captioning](http://handong1587.github.io/deep_learning/2015/10/09/image-video-captioning.html#video-captioning)\n\n# Video Summarization\n\nVideo summarization produces a short summary of a full-length video and ideally encapsulates its most informative parts, \nalleviates the problem of video browsing, editing and indexing.\n\n**Video Summarization with Long Short-term Memory**\n\n- arxiv: [http://arxiv.org/abs/1605.08110](http://arxiv.org/abs/1605.08110)\n\n**DeepVideo: Video Summarization using Temporal Sequence Modelling**\n\n- intro: CS231n student project report\n- paper: [http://cs231n.stanford.edu/reports2016/216_Report.pdf](http://cs231n.stanford.edu/reports2016/216_Report.pdf)\n\n**Semantic Video Trailers**\n\n- arxiv: [http://arxiv.org/abs/1609.01819](http://arxiv.org/abs/1609.01819)\n\n**Video Summarization using Deep Semantic Features**\n\n- inro: ACCV 2016\n- arxiv: [http://arxiv.org/abs/1609.08758](http://arxiv.org/abs/1609.08758)\n\n**CNN-Based Prediction of Frame-Level Shot Importance for Video Summarization**\n\n- intro: International Conference on new Trends in Computer Sciences (ICTCS), Amman-Jordan, 2017\n- arxiv: [https://arxiv.org/abs/1708.07023](https://arxiv.org/abs/1708.07023)\n\n**Video Summarization with Attention-Based Encoder-Decoder Networks**\n\n[https://arxiv.org/abs/1708.09545](https://arxiv.org/abs/1708.09545)\n\n**Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward**\n\n- intro: AAAI 2018. Chinese Academy of Sciences & Queen Mary University of London\n- project page: [https://kaiyangzhou.github.io/project_vsumm_reinforce/index.html](https://kaiyangzhou.github.io/project_vsumm_reinforce/index.html)\n- arxiv: [https://arxiv.org/abs/1801.00054](https://arxiv.org/abs/1801.00054)\n- github: [https://github.com//KaiyangZhou/vsumm-reinforce](https://github.com//KaiyangZhou/vsumm-reinforce)\n\n**Viewpoint-aware Video Summarization**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.02843](https://arxiv.org/abs/1804.02843)\n\n**DTR-GAN: Dilated Temporal Relational Adversarial Network for Video Summarization**\n\n[https://arxiv.org/abs/1804.11228](https://arxiv.org/abs/1804.11228)\n\n**Learning Video Summarization Using Unpaired Data**\n\n[https://arxiv.org/abs/1805.12174](https://arxiv.org/abs/1805.12174)\n\n**Video Summarization Using Fully Convolutional Sequence Networks**\n\n[https://arxiv.org/abs/1805.10538](https://arxiv.org/abs/1805.10538)\n\n**Video Summarisation by Classification with Deep Reinforcement Learning**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.03089](https://arxiv.org/abs/1807.03089)\n\n**Query-Conditioned Three-Player Adversarial Network for Video Summarization**\n\n- intro: BMVC 2018\n- arxiv: [https://arxiv.org/abs/1807.06677](https://arxiv.org/abs/1807.06677)\n\n**Discriminative Feature Learning for Unsupervised Video Summarization**\n\n- intro: AAAI 2019\n- arxiv: [https://arxiv.org/abs/1811.09791](https://arxiv.org/abs/1811.09791)\n\n**Rethinking the Evaluation of Video Summaries**\n\n- intro: CVPR 2019 poster\n- arxiv: [https://arxiv.org/abs/1903.11328](https://arxiv.org/abs/1903.11328)\n\n# Video Highlight Detection\n\n**Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders**\n\n- intro: ICCV 2015\n- intro: rely on an assumption that highlights of an event category are more frequently captured in short videos than non-highlights\n- arxiv: [http://arxiv.org/abs/1510.01442](http://arxiv.org/abs/1510.01442)\n\n**Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization**\n\n- keywords: wearable device\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf)\n- paper: [http://research.microsoft.com/apps/pubs/default.aspx?id=264919](http://research.microsoft.com/apps/pubs/default.aspx?id=264919)\n\n**Using Deep Learning to Find Basketball Highlights**\n\n![](https://cloud.githubusercontent.com/assets/10147637/7966603/228179fe-09f3-11e5-9ea7-31e76c8248fe.png)\n\n- blog: [http://public.hudl.com/bits/archives/2015/06/05/highlights/?utm_source=tuicool&utm_medium=referral](http://public.hudl.com/bits/archives/2015/06/05/highlights/?utm_source=tuicool&utm_medium=referral)\n\n**Real-Time Video Highlights for Yahoo Esports**\n\n- arxiv: [https://arxiv.org/abs/1611.08780](https://arxiv.org/abs/1611.08780)\n\n**A Deep Ranking Model for Spatio-Temporal Highlight Detection from a 360 Video**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1801.10312](https://arxiv.org/abs/1801.10312)\n\n**PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation**\n\n- intro: Nanyang Technological University & Google Research, Zurich\n- keywords: personalized highlight detection (PHD)\n- arxiv: [https://arxiv.org/abs/1804.06604](https://arxiv.org/abs/1804.06604)\n\n# Video Understanding\n\n**Scale Up Video Understandingwith Deep Learning**\n\n- intro: 2016, Tsinghua University\n- slides: [iiis.tsinghua.edu.cn/~jianli/courses/ATCS2016spring/talk_chuang.pptx](iiis.tsinghua.edu.cn/~jianli/courses/ATCS2016spring/talk_chuang.pptx)\n\n**Slicing Convolutional Neural Network for Crowd Video Understanding**\n\n![](http://www.ee.cuhk.edu.hk/~jshao/SCNN_files/fig_network5.jpg)\n\n- intro: CVPR 2016\n- intro: It aims at learning generic spatio-temporal features from crowd videos, especially for long-term temporal learning\n- project page: [http://www.ee.cuhk.edu.hk/~jshao/SCNN.html](http://www.ee.cuhk.edu.hk/~jshao/SCNN.html)\n- paper: [http://www.ee.cuhk.edu.hk/~jshao/papers_jshao/jshao_cvpr16_scnn.pdf](http://www.ee.cuhk.edu.hk/~jshao/papers_jshao/jshao_cvpr16_scnn.pdf)\n- github: [https://github.com/amandajshao/Slicing-CNN](https://github.com/amandajshao/Slicing-CNN)\n\n**Rethinking Spatiotemporal Feature Learning For Video Understanding**\n\n[https://arxiv.org/abs/1712.04851](https://arxiv.org/abs/1712.04851)\n\n**Hierarchical Video Understanding**\n\n[https://arxiv.org/abs/1809.03316](https://arxiv.org/abs/1809.03316)\n\n# Challenges\n\n**THUMOS Challenge 2014**\n\n- homepage: [http://crcv.ucf.edu/THUMOS14/home.html](http://crcv.ucf.edu/THUMOS14/home.html)\n- download: [http://crcv.ucf.edu/THUMOS14/download.html](http://crcv.ucf.edu/THUMOS14/download.html)\n\n**THUMOS Challenge 2015**\n\n- homepage: [http://www.thumos.info/](http://www.thumos.info/)\n- download: [http://www.thumos.info/download.html](http://www.thumos.info/download.html)\n\n**ActivityNet Challenge 2016**\n\n![](http://activity-net.org/challenges/2016/images/anet_cover.png)\n\n- homepage: [http://activity-net.org/challenges/2016/](http://activity-net.org/challenges/2016/)\n","excerpt":"Papers You Lead, We Exceed: Labor-Free Video Concept Learningby Jointly Exploiting Web Videos and Images intro: CVPR 2016 intro: LeadExcee","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations  Principles  The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]},{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between privacy and dignity.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Pro Summary","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","title":"An Overview","lastUpdatedAt":"2022-12-28T19:39:53.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","title":"About The Author","lastUpdatedAt":"2022-12-28T19:39:53.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","lastUpdatedAt":"2022-12-28T19:34:43.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","title":"Fake News: Considerations  Principles  The Institution of Socio &#8211; Economic Values","lastUpdatedAt":"2022-12-28T19:29:53.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","title":"Handong1587","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","title":"Notes on Suffix Array and Manacher Algorithm","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","title":"Notes On Perceptrons","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","title":"Notes On Object Detection","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","title":"Notes On Caffe Development","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","title":"Notes On L-BFGS","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}