{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/",
    "result": {"data":{"mdx":{"id":"36467522-cc1d-5e4b-8bc0-f0dc6b5551c7","tableOfContents":{"items":[{"url":"#papers","title":"Papers"},{"url":"#compressing-deep-neural-network","title":"Compressing Deep Neural Network"},{"url":"#pruning","title":"Pruning"},{"url":"#low-precision-networks","title":"Low-Precision Networks"},{"url":"#quantized-neural-networks","title":"Quantized Neural Networks"},{"url":"#binary-convolutional-neural-networks--binarized-neural-networks","title":"Binary Convolutional Neural Networks / Binarized Neural Networks"},{"url":"#accelerating--fast-algorithms","title":"Accelerating / Fast Algorithms"},{"url":"#code-optimization","title":"Code Optimization"},{"url":"#projects","title":"Projects","items":[{"url":"#optnet","title":"OptNet"}]},{"url":"#blogs","title":"Blogs"},{"url":"#talks--videos","title":"Talks / Videos"},{"url":"#resources","title":"Resources"}]},"fields":{"title":"Acceleration and Model Compression","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration.md","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"Acceleration and Model Compression","description":null,"imageAlt":null,"tags":[],"date":"2015-10-09T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"deep_learning\",\n  \"title\": \"Acceleration and Model Compression\",\n  \"date\": \"2015-10-09T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"papers\"\n  }, \"Papers\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"High-Performance Neural Networks for Visual Object Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"reduced network parameters by randomly removing connections before training\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1102.0183\"\n  }, \"http://arxiv.org/abs/1102.0183\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Predicting Parameters in Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"decomposed the weighting matrix into two low-rank matrices\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1306.0543\"\n  }, \"http://arxiv.org/abs/1306.0543\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neurons vs Weights Pruning in Artificial Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://journals.ru.lv/index.php/ETR/article/view/166\"\n  }, \"http://journals.ru.lv/index.php/ETR/article/view/166\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Exploiting Linear Structure Within Convolutional Networks for Ef\\uFB01cient Evaluation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"presented a series of low-rank decomposition designs for convolutional kernels.\\nsingular value decomposition was adopted for the matrix factorization\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf\"\n  }, \"http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"cuDNN: Efficient Primitives for Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1410.0759\"\n  }, \"https://arxiv.org/abs/1410.0759\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"download: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://developer.nvidia.com/cudnn\"\n  }, \"https://developer.nvidia.com/cudnn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Efficient and accurate approximations of nonlinear convolutional networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"considered the subsequent nonlinear units while learning the low-rank decomposition\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1411.4229\"\n  }, \"http://arxiv.org/abs/1411.4229\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Neural Networks at Constrained Time Cost\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1412.1710\"\n  }, \"https://arxiv.org/abs/1412.1710\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Flattened Convolutional Neural Networks for Feedforward Acceleration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.5474\"\n  }, \"http://arxiv.org/abs/1412.5474\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jhjin/flattened-cnn\"\n  }, \"https://github.com/jhjin/flattened-cnn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Compressing Deep Convolutional Networks using Vector Quantization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"this paper showed that vector quantization had a clear advantage\\nover matrix factorization methods in compressing fully-connected layers.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.6115\"\n  }, \"http://arxiv.org/abs/1412.6115\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"a low-rank CPdecomposition was adopted to\\ntransform a convolutional layer into multiple layers of lower complexity\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.6553\"\n  }, \"http://arxiv.org/abs/1412.6553\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Fried Convnets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"fully-connected layers were replaced by a single \\u201CFastfood\\u201D layer for end-to-end training with convolutional layers\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.7149\"\n  }, \"http://arxiv.org/abs/1412.7149\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Convolutional Nets With fbfft: A GPU Performance Evaluation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook. ICLR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.7580\"\n  }, \"http://arxiv.org/abs/1412.7580\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://facebook.github.io/fbcunn/fbcunn/\"\n  }, \"http://facebook.github.io/fbcunn/fbcunn/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Caffe con Troll: Shallow Ideas to Speed Up Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1504.04343\"\n  }, \"http://arxiv.org/abs/1504.04343\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Compressing Neural Networks with the Hashing Trick\")), mdx(\"img\", {\n    \"src\": \"http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/hashednets.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: HashedNets. ICML 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"randomly grouped connection weights into hash buckets, and then fine-tuned network parameters with back-propagation\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/index.html\"\n  }, \"http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1504.04788\"\n  }, \"http://arxiv.org/abs/1504.04788\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/HashedNets.zip\"\n  }, \"http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/HashedNets.zip\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1504.08362\"\n  }, \"https://arxiv.org/abs/1504.08362\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mfigurnov/perforated-cnn-matconvnet\"\n  }, \"https://github.com/mfigurnov/perforated-cnn-matconvnet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mfigurnov/perforated-cnn-caffe\"\n  }, \"https://github.com/mfigurnov/perforated-cnn-caffe\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating Very Deep Convolutional Networks for Classification and Detection\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"considered the subsequent nonlinear units while learning the low-rank decomposition\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.06798\"\n  }, \"http://arxiv.org/abs/1505.06798\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast ConvNets Using Group-wise Brain Damage\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"applied group-wise pruning to the convolutional tensor\\nto decompose it into the multiplications of thinned dense matrices\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1506.02515\"\n  }, \"http://arxiv.org/abs/1506.02515\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning both Weights and Connections for Efficient Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1506.02626\"\n  }, \"http://arxiv.org/abs/1506.02626\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data-free parameter pruning for Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"proposed to remove redundant neurons instead of network connections\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1507.06149\"\n  }, \"http://arxiv.org/abs/1507.06149\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2016 Best Paper\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"reduced the size of AlexNet by 35x from 240MB to 6.9MB, the size of VGG16 by 49x from 552MB to 11.3MB, with no loss of accuracy\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.00149\"\n  }, \"http://arxiv.org/abs/1510.00149\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://videolectures.net/iclr2016_han_deep_compression/\"\n  }, \"http://videolectures.net/iclr2016_han_deep_compression/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Structured Transforms for Small-Footprint Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1510.01722\"\n  }, \"https://arxiv.org/abs/1510.01722\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning\"\n  }, \"https://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.06706\"\n  }, \"http://arxiv.org/abs/1510.06706\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/seung-lab/znn-release\"\n  }, \"https://github.com/seung-lab/znn-release\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reducing the Training Time of Neural Networks by Partitioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.02954\"\n  }, \"http://arxiv.org/abs/1511.02954\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional neural networks with low-rank regularization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06067\"\n  }, \"http://arxiv.org/abs/1511.06067\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chengtaipu/lowrankcnn\"\n  }, \"https://github.com/chengtaipu/lowrankcnn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CNNdroid: Open Source Library for GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1511.07376\"\n  }, \"https://arxiv.org/abs/1511.07376\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://dl.acm.org/authorize.cfm?key=N14731\"\n  }, \"http://dl.acm.org/authorize.cfm?key=N14731\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://sharif.edu/~matin/pub/2016_mm_slides.pdf\"\n  }, \"http://sharif.edu/~matin/pub/2016_mm_slides.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ENCP/CNNdroid\"\n  }, \"https://github.com/ENCP/CNNdroid\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EIE: Efficient Inference Engine on Compressed Deep Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ISCA 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.01528\"\n  }, \"http://arxiv.org/abs/1602.01528\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf\"\n  }, \"http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf\"\n  }, \"http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Tables Ensemble: classification in microseconds\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.04489\"\n  }, \"http://arxiv.org/abs/1602.04489\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DeepScale & UC Berkeley\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.07360\"\n  }, \"http://arxiv.org/abs/1602.07360\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/DeepScale/SqueezeNet\"\n  }, \"https://github.com/DeepScale/SqueezeNet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://songhan.github.io/SqueezeNet-Deep-Compression/\"\n  }, \"http://songhan.github.io/SqueezeNet-Deep-Compression/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/songhan/SqueezeNet-Deep-Compression\"\n  }, \"https://github.com/songhan/SqueezeNet-Deep-Compression\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"note: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.evernote.com/shard/s146/sh/108eea91-349b-48ba-b7eb-7ac8f548bee9/5171dc6b1088fba05a4e317f7f5d32a3\"\n  }, \"https://www.evernote.com/shard/s146/sh/108eea91-349b-48ba-b7eb-7ac8f548bee9/5171dc6b1088fba05a4e317f7f5d32a3\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Keras): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/DT42/squeezenet_demo\"\n  }, \"https://github.com/DT42/squeezenet_demo\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Keras): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/rcmalli/keras-squeezenet\"\n  }, \"https://github.com/rcmalli/keras-squeezenet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(PyTorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gsp-27/pytorch_Squeezenet\"\n  }, \"https://github.com/gsp-27/pytorch_Squeezenet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SqueezeNet-Residual\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Residual-SqueezeNet improves the top-1 accuracy of SqueezeNet by 2.9% on ImageNet without changing the model size(only 4.8MB).\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/songhan/SqueezeNet-Residual\"\n  }, \"https://github.com/songhan/SqueezeNet-Residual\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Lab41 Reading Group: SqueezeNet\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/m/global-identity?redirectUrl=https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75\"\n  }, \"https://medium.com/m/global-identity?redirectUrl=https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Simplified_SqueezeNet\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: An improved version of SqueezeNet networks\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/NidabaSystems/Simplified_SqueezeNet\"\n  }, \"https://github.com/NidabaSystems/Simplified_SqueezeNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SqueezeNet Keras Dogs vs. Cats demo\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chasingbob/squeezenet-keras\"\n  }, \"https://github.com/chasingbob/squeezenet-keras\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Neural Networks using Logarithmic Data Representation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.01025\"\n  }, \"http://arxiv.org/abs/1603.01025\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://niclane.org/pubs/deepx_ipsn.pdf\"\n  }, \"http://niclane.org/pubs/deepx_ipsn.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hardware-oriented Approximation of Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.03168\"\n  }, \"http://arxiv.org/abs/1604.03168\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ristretto.lepsucd.com/\"\n  }, \"http://ristretto.lepsucd.com/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(\\\"Ristretto: Caffe-based approximation of convolutional neural networks\\\"): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pmgysel/caffe\"\n  }, \"https://github.com/pmgysel/caffe\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Neural Networks Under Stress\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICIP 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.03498\"\n  }, \"http://arxiv.org/abs/1605.03498\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MicaelCarvalho/DNNsUnderStress\"\n  }, \"https://github.com/MicaelCarvalho/DNNsUnderStress\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks using Angle Sensitive Pixels\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.03621\"\n  }, \"http://arxiv.org/abs/1605.03621\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU).\\nFor the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters,\\nwhile maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1605.06489\"\n  }, \"https://arxiv.org/abs/1605.06489\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Functional Hashing for Compressing Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: FunHashNN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.06560\"\n  }, \"http://arxiv.org/abs/1605.06560\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.06402\"\n  }, \"http://arxiv.org/abs/1605.06402\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"YodaNN: An Ultra-Low Power Convolutional Neural Network Accelerator Based on Binary Weights\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.05487\"\n  }, \"https://arxiv.org/abs/1606.05487\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Structured Sparsity in Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.03665\"\n  }, \"http://arxiv.org/abs/1608.03665\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial \\\"Bottleneck\\\" Structure\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1608.04337\"\n  }, \"https://arxiv.org/abs/1608.04337\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamic Network Surgery for Efficient DNNs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: compress the number of parameters in LeNet-5 and AlexNet by a factor of 108\\xD7 and 17.7\\xD7 respectively\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.04493\"\n  }, \"http://arxiv.org/abs/1608.04493\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official. Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yiwenguo/Dynamic-Network-Surgery\"\n  }, \"https://github.com/yiwenguo/Dynamic-Network-Surgery\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scalable Compression of Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACM Multimedia 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.07365\"\n  }, \"http://arxiv.org/abs/1608.07365\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pruning Filters for Efficient ConvNets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS Workshop on Efficient Methods for Deep Neural Networks (EMDNN), 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.08710\"\n  }, \"http://arxiv.org/abs/1608.08710\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fixed-point Factorized Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01972\"\n  }, \"https://arxiv.org/abs/1611.01972\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Ultimate tensorization: compressing convolutional and FC layers alike\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016 workshop: Learning with Tensors: Why Now and How?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.03214\"\n  }, \"https://arxiv.org/abs/1611.03214\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/timgaripov/TensorNet-TF\"\n  }, \"https://github.com/timgaripov/TensorNet-TF\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05128\"\n  }, \"https://arxiv.org/abs/1611.05128\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Net-Trim: A Layer-wise Convex Pruning of Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05162\"\n  }, \"https://arxiv.org/abs/1611.05162\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LCNN: Lookup-based Convolutional Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"Our fastest LCNN offers 37.6x speed up over AlexNet while maintaining 44.3% top-1 accuracy.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06473\"\n  }, \"https://arxiv.org/abs/1611.06473\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Tensor Convolution on Multicores\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: present the first practical CPU implementation of tensor convolution optimized for deep networks of small kernels\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06565\"\n  }, \"https://arxiv.org/abs/1611.06565\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Sparse Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06694\"\n  }, \"https://arxiv.org/abs/1611.06694\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FINN: A Framework for Fast, Scalable Binarized Neural Network Inference\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Xilinx Research Labs & Norwegian University of Science and Technology & University of Sydney\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 25th International Symposium on Field-Programmable Gate Arrays\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: FPGA\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf\"\n  }, \"http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.07119\"\n  }, \"https://arxiv.org/abs/1612.07119\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Xilinx/BNN-PYNQ\"\n  }, \"https://github.com/Xilinx/BNN-PYNQ\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning with INT8 Optimization on Xilinx Devices\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"Xilinx's integrated DSP architecture can achieve 1.75X solution-level performance\\nat INT8 deep learning operations than other FPGA DSP architectures\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf\"\n  }, \"https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Parameter Compression of Recurrent Neural Networks and Degredation of Short-term Memory\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.00891\"\n  }, \"https://arxiv.org/abs/1612.00891\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An OpenCL(TM) Deep Learning Accelerator on Arria 10\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: FPGA 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.03534\"\n  }, \"https://arxiv.org/abs/1701.03534\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CMU & Universitat Paderborn]\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.04465\"\n  }, \"https://arxiv.org/abs/1701.04465\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DL-gleaning: An Approach For Improving Inference Speed And Accuracy\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Electronics Telecommunications Research Institute (ETRI)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/pdf?id=Hynn8SHOx\"\n  }, \"https://openreview.net/pdf?id=Hynn8SHOx\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Energy Saving Additive Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Middle East Technical University & Bilkent University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.02676\"\n  }, \"https://arxiv.org/abs/1702.02676\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Soft Weight-Sharing for Neural Network Compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2017. University of Amsterdam\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.04008\"\n  }, \"https://arxiv.org/abs/1702.04008\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression\"\n  }, \"https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.04071\"\n  }, \"https://arxiv.org/abs/1703.04071\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ARM Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.03073\"\n  }, \"https://arxiv.org/abs/1703.03073\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DyVEDeep: Dynamic Variable Effort Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1704.01137\"\n  }, \"https://arxiv.org/abs/1704.01137\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bayesian Compression for Deep Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.08665\"\n  }, \"https://arxiv.org/abs/1705.08665\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Kernel Redundancy Removing Policy for Convolutional Neural Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.10748\"\n  }, \"https://arxiv.org/abs/1705.10748\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gated XNOR Networks: Deep Neural Networks with Ternary Weights and Activations under a Unified Discretization Framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: discrete state transition (DST)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.09283\"\n  }, \"https://arxiv.org/abs/1705.09283\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SEP-Nets: Small and Effective Pattern Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The University of Iowa & Snap Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.03912\"\n  }, \"https://arxiv.org/abs/1706.03912\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MEC: Memory-efficient Convolution for Deep Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.06873\"\n  }, \"https://arxiv.org/abs/1706.06873\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data-Driven Sparse Structure Selection for Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1707.01213\"\n  }, \"https://arxiv.org/abs/1707.01213\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An End-to-End Compression Framework Based on Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.00838\"\n  }, \"https://arxiv.org/abs/1708.00838\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Domain-adaptive deep network compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.01041\"\n  }, \"https://arxiv.org/abs/1709.01041\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mmasana/DALR\"\n  }, \"https://github.com/mmasana/DALR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Binary-decomposed DCNN for accelerating computation and compressing model without retraining\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.04731\"\n  }, \"https://arxiv.org/abs/1709.04731\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Efficiency in Convolutional Neural Network with Multilinear Filters\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.09902\"\n  }, \"https://arxiv.org/abs/1709.09902\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Survey of Model Compression and Acceleration for Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IEEE Signal Processing Magazine. IBM Thoms J. Watson Research Center & Tsinghua University & Huazhong University of Science and Technology\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.09282\"\n  }, \"https://arxiv.org/abs/1710.09282\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Compression-aware Training of Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.02638\"\n  }, \"https://arxiv.org/abs/1711.02638\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.06528\"\n  }, \"https://arxiv.org/abs/1711.06528\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reducing Deep Network Complexity with Fourier Transform Methods\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Harvard University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.01451\"\n  }, \"https://arxiv.org/abs/1801.01451\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods\"\n  }, \"https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EffNet: An Efficient Structure for Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Aptiv & University of Wupperta\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.06434\"\n  }, \"https://arxiv.org/abs/1801.06434\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Universal Deep Neural Network Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.02271\"\n  }, \"https://arxiv.org/abs/1802.02271\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Paraphrasing Complex Network: Network Compression via Factor Transfer\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.04977\"\n  }, \"https://arxiv.org/abs/1802.04977\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Compressing Neural Networks using the Variational Information Bottleneck\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tsinghua University & ShanghaiTech University & Microsoft Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.10399\"\n  }, \"https://arxiv.org/abs/1802.10399\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adversarial Network Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.10750\"\n  }, \"https://arxiv.org/abs/1803.10750\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Expanding a robot's life: Low power object recognition via FPGA-based DCNN deployment\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MOCAST 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.00512\"\n  }, \"https://arxiv.org/abs/1804.00512\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating CNN inference on FPGAs: A Survey\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \", \"[Institut Pascal]\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.01683\"\n  }, \"https://arxiv.org/abs/1806.01683\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Doubly Nested Network for Resource-Efficient Inference\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.07568\"\n  }, \"https://arxiv.org/abs/1806.07568\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Smallify: Learning Network Size while Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.03723\"\n  }, \"https://arxiv.org/abs/1806.03723\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on Embedded FPGAs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 27th International Symposium on Field-Programmable Gate Arrays, February 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.08634\"\n  }, \"https://arxiv.org/abs/1811.08634\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cascaded Projection: End-to-End Network Compression and Acceleration\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1903.04988\"\n  }, \"https://arxiv.org/abs/1903.04988\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FALCON: Fast and Lightweight Convolution for Compressing and Accelerating CNN\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1909.11321\"\n  }, \"https://arxiv.org/abs/1909.11321\")), mdx(\"h1\", {\n    \"id\": \"compressing-deep-neural-network\"\n  }, \"Compressing Deep Neural Network\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.09228\"\n  }, \"https://arxiv.org/abs/1806.09228\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Sandbox3aster/Deep-K-Means-pytorch\"\n  }, \"https://github.com/Sandbox3aster/Deep-K-Means-pytorch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Optimize Deep Convolutional Neural Network with Ternarized Weights and High Accuracy\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Central Florida & Tencent AI lab, Seattle\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.07948\"\n  }, \"https://arxiv.org/abs/1807.07948\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.05240\"\n  }, \"https://arxiv.org/abs/1808.05240\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1809.01330\"\n  }, \"https://arxiv.org/abs/1809.01330\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Framework for Fast and Efficient Neural Network Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.12781\"\n  }, \"https://arxiv.org/abs/1811.12781\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.12673\"\n  }, \"https://arxiv.org/abs/1811.12673\")), mdx(\"h1\", {\n    \"id\": \"pruning\"\n  }, \"Pruning\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017. Nanjing University & Shanghai Jiao Tong University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.06342\"\n  }, \"https://arxiv.org/abs/1707.06342\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Roll920/ThiNet\"\n  }, \"https://github.com/Roll920/ThiNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neuron Pruning for Compressing Deep Networks using Maxout Architectures\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: GCPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.06838\"\n  }, \"https://arxiv.org/abs/1707.06838\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional Network with Bayesian Optimization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2017 oral. Simon Fraser University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.09102\"\n  }, \"https://arxiv.org/abs/1707.09102\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Prune the Convolutional Neural Networks with Sparse Shrink\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.02439\"\n  }, \"https://arxiv.org/abs/1708.02439\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"NISP: Pruning Networks using Neuron Importance Score Propagation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Maryland & IBM T. J. Watson Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.05908\"\n  }, \"https://arxiv.org/abs/1711.05908\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Automated Pruning for Deep Neural Network Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.01721\"\n  }, \"https://arxiv.org/abs/1712.01721\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Prune Filters in Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.07365\"\n  }, \"https://arxiv.org/abs/1801.07365\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.10447\"\n  }, \"https://arxiv.org/abs/1801.10447\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A novel channel pruning method for deep neural network compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.11394\"\n  }, \"https://arxiv.org/abs/1805.11394\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"PCAS: Pruning Channels with Attention Statistics\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Oki Electric Industry Co., Ltd\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.05382\"\n  }, \"https://arxiv.org/abs/1806.05382\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.06866\"\n  }, \"https://arxiv.org/abs/1808.06866\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/he-y/soft-filter-pruning\"\n  }, \"https://github.com/he-y/soft-filter-pruning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Progressive Deep Neural Networks Acceleration via Soft Filter Pruning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.07471\"\n  }, \"https://arxiv.org/abs/1808.07471\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pruning neural networks: is it time to nip it in the bud?\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.04622\"\n  }, \"https://arxiv.org/abs/1810.04622\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rethinking the Value of Network Pruning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.05270\"\n  }, \"https://arxiv.org/abs/1810.05270\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamic Channel Pruning: Feature Boosting and Suppression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.05331\"\n  }, \"https://arxiv.org/abs/1810.05331\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Interpretable Convolutional Filter Pruning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.07322\"\n  }, \"https://arxiv.org/abs/1810.07322\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Progressive Weight Pruning of Deep Neural Networks using ADMM\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.07378\"\n  }, \"https://arxiv.org/abs/1810.07378\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pruning Deep Neural Networks using Partial Least Squares\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.07610\"\n  }, \"https://arxiv.org/abs/1810.07610\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/arturjordao/PruningNeuralNetworks\"\n  }, \"https://github.com/arturjordao/PruningNeuralNetworks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hybrid Pruning: Thinner Sparse Networks for Fast Inference on Edge Devices\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.00482\"\n  }, \"https://arxiv.org/abs/1811.00482\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Discrimination-aware Channel Pruning for Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.11809\"\n  }, \"https://arxiv.org/abs/1810.11809\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stability Based Filter Pruning for Accelerating Deep CNNs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.08321\"\n  }, \"https://arxiv.org/abs/1811.08321\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Structured Pruning for Efficient ConvNets via Incremental Regularization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2018 workshop on \\\"Compact Deep Neural Network Representation with Industrial Applications\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.08390\"\n  }, \"https://arxiv.org/abs/1811.08390\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.08589\"\n  }, \"https://arxiv.org/abs/1811.08589\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Layer Decomposition-Recomposition Framework for Neuron Pruning towards Accurate Lightweight Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2019 as oral\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Hikvision Research Institute\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.06611\"\n  }, \"https://arxiv.org/abs/1812.06611\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Quantized Guided Pruning for Efficient Hardware Implementations of Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1812.11337\"\n  }, \"https://arxiv.org/abs/1812.11337\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1901.07827\"\n  }, \"https://arxiv.org/abs/1901.07827\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Partition Pruning: Parallelization-Aware Pruning for Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1901.11391\"\n  }, \"https://arxiv.org/abs/1901.11391\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pruning from Scratch\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tsinghua University & Ant Financial & Huawei Noah\\u2019s Ark Lab\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.12579\"\n  }, \"https://arxiv.org/abs/1909.12579\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Global Sparse Momentum SGD for Pruning Very Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NeurIPS 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.12778\"\n  }, \"https://arxiv.org/abs/1909.12778\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"openreview: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/forum?id=rJeUPlrYvr\"\n  }, \"https://openreview.et/forum?id=rJeUPlrYvr\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/pdf?id=rJeUPlrYvr\"\n  }, \"https://openreview.net/pdf?id=rJeUPlrYvr\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/anonymous47823493/FNNP\"\n  }, \"https://github.com/anonymous47823493/FNNP\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pruning Filter in Filter\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NeurIPS 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Harbin Institute of Technology & Tencent Youtu Lab\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2009.14410\"\n  }, \"https://arxiv.org/abs/2009.14410\"))), mdx(\"h1\", {\n    \"id\": \"low-precision-networks\"\n  }, \"Low-Precision Networks\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating Deep Convolutional Networks using low-precision and sparsity\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Intel Labs\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.00324\"\n  }, \"https://arxiv.org/abs/1610.00324\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning with Low Precision by Half-wave Gaussian Quantization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: HWGQ-Net\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.00953\"\n  }, \"https://arxiv.org/abs/1702.00953\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.03044\"\n  }, \"https://arxiv.org/abs/1702.03044\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"openreview: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/forum?id=HyQJ-mclg&noteId=HyQJ-mclg\"\n  }, \"https://openreview.net/forum?id=HyQJ-mclg\", \"\\xAC\", \"eId=HyQJ-mclg\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.02393\"\n  }, \"https://arxiv.org/abs/1706.02393\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gudovskiy/ShiftCNN\"\n  }, \"https://github.com/gudovskiy/ShiftCNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Alibaba Group\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: alternating direction method of multipliers (ADMM)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.09870\"\n  }, \"https://arxiv.org/abs/1707.09870\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2017 Oral\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.01001\"\n  }, \"https://arxiv.org/abs/1708.01001\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Compressing Low Precision Deep Neural Networks Using Sparsity-Induced Regularization in Ternary Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICONIP 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.06262\"\n  }, \"https://arxiv.org/abs/1709.06262\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Low Precision Deep Neural Networks through Regularization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.00095\"\n  }, \"https://arxiv.org/abs/1809.00095\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.04191\"\n  }, \"https://arxiv.org/abs/1809.04191\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SQuantizer: Simultaneous Learning for Both Sparse and Low-precision Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Movidius, AIPG, Intel\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.08301\"\n  }, \"https://arxiv.org/abs/1812.08301\"))), mdx(\"h1\", {\n    \"id\": \"quantized-neural-networks\"\n  }, \"Quantized Neural Networks\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Quantized Convolutional Neural Networks for Mobile Devices\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Q-CNN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"Extensive experiments on the ILSVRC-12 benchmark demonstrate\\n4 \\u223C 6\\xD7 speed-up and 15 \\u223C 20\\xD7 compression with merely one percentage loss of classification accuracy\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.06473\"\n  }, \"http://arxiv.org/abs/1512.06473\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jiaxiang-wu/quantized-cnn\"\n  }, \"https://github.com/jiaxiang-wu/quantized-cnn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Quantized Nets: A Deeper Understanding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Maryland & Cornell University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.02379\"\n  }, \"https://arxiv.org/abs/1706.02379\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: the top-5 error rate of 4-bit quantized GoogLeNet model is 12.7%\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.07145\"\n  }, \"https://arxiv.org/abs/1706.07145\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018. Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.05877\"\n  }, \"https://arxiv.org/abs/1712.05877\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Neural Network Compression with Single and Multiple Level Quantization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018. Shanghai Jiao Tong University & University of Chinese Academy of Sciences\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.03289\"\n  }, \"https://arxiv.org/abs/1803.03289\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Quantizing deep convolutional networks for efficient inference: A whitepaper\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.08342\"\n  }, \"https://arxiv.org/abs/1806.08342\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CascadeCNN: Pushing the Performance Limits of Quantisation in Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 28th International Conference on Field Programmable Logic & Applications (FPL), 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.05053\"\n  }, \"https://arxiv.org/abs/1807.05053\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bridging the Accuracy Gap for 2-bit Quantized Neural Networks (QNN)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IBM Research AI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.06964\"\n  }, \"https://arxiv.org/abs/1807.06964\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Joint Training of Low-Precision Neural Network with Quantization Interval Parameters\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.05779\"\n  }, \"https://arxiv.org/abs/1808.05779\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Differentiable Fine-grained Quantization for Deep Neural Network Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.10351\"\n  }, \"https://arxiv.org/abs/1810.10351\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"HAQ: Hardware-Aware Automated Quantization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.08886\"\n  }, \"https://arxiv.org/abs/1811.08886\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DNQ: Dynamic Network Quantization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Shanghai Jiao Tong University & Qualcomm AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.02375\"\n  }, \"https://arxiv.org/abs/1812.02375\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Trained Rank Pruning for Efficient Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Shanghai Jiao Tong University & Qualcomm AI Research & Duke University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.02402\"\n  }, \"https://arxiv.org/abs/1812.02402\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Quantized Network with Auxiliary Gradient Module\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The University of Adelaide & Australian Centre for Robotic Vision & South China University of Technology\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.11236\"\n  }, \"https://arxiv.org/abs/1903.11236\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FLightNNs: Lightweight Quantized Deep Neural Networks for Fast and Accurate Inference\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Carnegie Mellon University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.02835\"\n  }, \"https://arxiv.org/abs/1904.02835\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research & Univ Rennes\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1907.05686\"\n  }, \"https://arxiv.org/abs/1907.05686\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1908.05033\"\n  }, \"https://arxiv.org/abs/1908.05033\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bit Efficient Quantization for Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NeurIPS workshop 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1910.04877\"\n  }, \"https://arxiv.org/abs/1910.04877\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Quantization Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1911.09464\"\n  }, \"https://arxiv.org/abs/1911.09464\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adaptive Loss-aware Quantization for Multi-bit Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1912.08883\"\n  }, \"https://arxiv.org/abs/1912.08883\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distribution Adaptive INT8 Quantization for Training CNNs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Machine Intelligence Technology Lab, Alibaba Group\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2102.04782\"\n  }, \"https://arxiv.org/abs/2102.04782\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distance-aware Quantization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2108.06983\"\n  }, \"https://arxiv.org/abs/2108.06983\"))), mdx(\"h1\", {\n    \"id\": \"binary-convolutional-neural-networks--binarized-neural-networks\"\n  }, \"Binary Convolutional Neural Networks / Binarized Neural Networks\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1602.02830\"\n  }, \"https://arxiv.org/abs/1602.02830\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1609.07061\"\n  }, \"https://arxiv.org/abs/1609.07061\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.05279\"\n  }, \"http://arxiv.org/abs/1603.05279\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Torch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mrastegari/XNOR-Net\"\n  }, \"https://github.com/mrastegari/XNOR-Net\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"XNOR-Net++: Improved Binary Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMVC 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.13863\"\n  }, \"https://arxiv.org/abs/1909.13863\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1606.06160\"\n  }, \"https://arxiv.org/abs/1606.06160\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A 7.663-TOPS 8.2-W Energy-efficient FPGA Accelerator for Binary Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.06392\"\n  }, \"https://arxiv.org/abs/1702.06392\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Espresso: Efficient Forward Propagation for BCNNs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.07175\"\n  }, \"https://arxiv.org/abs/1705.07175\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/fpeder/espresso\"\n  }, \"https://github.com/fpeder/espresso\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Binary Neural Networks (BNNs)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.09864\"\n  }, \"https://arxiv.org/abs/1705.09864\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hpi-xnor\"\n  }, \"https://github.com/hpi-xnor\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.02393\"\n  }, \"https://arxiv.org/abs/1706.02393\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Embedded Vision Workshop (CVPRW). UC San Diego & UC Los Angeles & Cornell University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.04693\"\n  }, \"https://arxiv.org/abs/1707.04693\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Embedded Binarized Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.02260\"\n  }, \"https://arxiv.org/abs/1709.02260\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Compact Hash Code Learning with Binary Deep Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Singapore University of Technology and Design\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.02956\"\n  }, \"https://arxiv.org/abs/1712.02956\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Build a Compact Binary Neural Network through Bit-level Sensitivity and Data Pruning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.00904\"\n  }, \"https://arxiv.org/abs/1802.00904\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"From Hashing to CNNs: Training BinaryWeight Networks via Hashing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.02733\"\n  }, \"https://arxiv.org/abs/1802.02733\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Energy Efficient Hadamard Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Binary Weight and Hadamard-transformed Image Network (BWHIN), Binary Weight Network (BWN), Hadamard-transformed Image Network (HIN)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.05421\"\n  }, \"https://arxiv.org/abs/1805.05421\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.07550\"\n  }, \"https://arxiv.org/abs/1806.07550\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.00278\"\n  }, \"https://arxiv.org/abs/1808.00278\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Compact Neural Networks with Binary Weights and Low Precision Activations\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.02631\"\n  }, \"https://arxiv.org/abs/1808.02631\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training wide residual networks for deployment using a single bit for each weight\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.08530\"\n  }, \"https://arxiv.org/abs/1802.08530\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, PyTorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/szagoruyko/binary-wide-resnet\"\n  }, \"https://github.com/szagoruyko/binary-wide-resnet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Composite Binary Decomposition Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.06668\"\n  }, \"https://arxiv.org/abs/1811.06668\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Competitive Binary Neural Networks from Scratch\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Potsdam\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BMXNet v2: An Open-Source Binary Neural Network Implementation Based on MXNet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.01965\"\n  }, \"https://arxiv.org/abs/1812.01965\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/hpi-xnor/BMXNet-v2\"\n  }, \"https://github.com/hpi-xnor/BMXNet-v2\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularizing Activation Distribution for Training Binarized Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Carnegie Mellon University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.02823\"\n  }, \"https://arxiv.org/abs/1904.02823\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GBCNs: Genetic Binary Convolutional Networks for Enhancing the Performance of 1-bit DCNNs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1911.11634\"\n  }, \"https://arxiv.org/abs/1911.11634\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Binary Neural Networks with Real-to-Binary Convolutions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Samsung AI Research Center, Cambridge, UK & The University of Nottingham\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2003.11535\"\n  }, \"https://arxiv.org/abs/2003.11535\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/brais-martinez/real2binary\"\n  }, \"https://github.com/brais-martinez/real2binary\"))), mdx(\"h1\", {\n    \"id\": \"accelerating--fast-algorithms\"\n  }, \"Accelerating / Fast Algorithms\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Algorithms for Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"2.6x as fast as Caffe when comparing CPU implementations\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Winograd's minimal filtering algorithms\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.09308\"\n  }, \"http://arxiv.org/abs/1509.09308\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/andravin/wincnn\"\n  }, \"https://github.com/andravin/wincnn\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://homes.cs.washington.edu/~cdel/presentations/Fast_Algorithms_for_Convolutional_Neural_Networks_Slides_reading_group_uw_delmundo_slides.pdf\"\n  }, \"http://homes.cs.washington.edu/~cdel/presentations/Fast_Algorithms_for_Convolutional_Neural_Networks_Slides_reading_group_uw_delmundo_slides.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"discussion: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895\"\n  }, \"https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"reddit: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/?\"\n  }, \"https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/?\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Hong Kong Baptist University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.07724\"\n  }, \"https://arxiv.org/abs/1704.07724\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.01406\"\n  }, \"https://arxiv.org/abs/1706.01406\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Channel Pruning for Accelerating Very Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017. Megvii Inc\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.06168\"\n  }, \"https://arxiv.org/abs/1707.06168\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yihui-he/channel-pruning\"\n  }, \"https://github.com/yihui-he/channel-pruning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.04728\"\n  }, \"https://arxiv.org/abs/1708.04728\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Efficient Convolutional Networks through Network Slimming\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.06519\"\n  }, \"https://arxiv.org/abs/1708.06519\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SparCE: Sparsity aware General Purpose Core Extensions to Accelerate Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.06315\"\n  }, \"https://arxiv.org/abs/1711.06315\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating Convolutional Neural Networks for Continuous Mobile Vision via Cache Reuse\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: CNNCache\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.01670\"\n  }, \"https://arxiv.org/abs/1712.01670\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.07493\"\n  }, \"https://arxiv.org/abs/1712.07493\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SBNet: Sparse Blocks Network for Fast Inference\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Uber\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://eng.uber.com/sbnet/\"\n  }, \"https://eng.uber.com/sbnet/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.02108\"\n  }, \"https://arxiv.org/abs/1801.02108\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/uber/sbnet\"\n  }, \"https://github.com/uber/sbnet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating deep neural networks with tensor decompositions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning\"\n  }, \"https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jacobgil/pytorch-tensor-decompositions\"\n  }, \"https://github.com/jacobgil/pytorch-tensor-decompositions\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Survey on Acceleration of Deep Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.00939\"\n  }, \"https://arxiv.org/abs/1802.00939\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent Residual Module for Fast Inference in Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.09723\"\n  }, \"https://arxiv.org/abs/1802.09723\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Co-Design of Deep Neural Nets and Neural Net Accelerators for Embedded Vision Applications\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UC Berkeley & Samsung Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.10642\"\n  }, \"https://arxiv.org/abs/1804.10642\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards Efficient Convolutional Neural Network for Domain-Specific Applications on FPGA\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.03318\"\n  }, \"https://arxiv.org/abs/1809.03318\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating Deep Neural Networks with Spatial Bottleneck Modules\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.02601\"\n  }, \"https://arxiv.org/abs/1809.02601\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FPGA Implementation of Convolutional Neural Networks with Fixed-Point Calculations\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.09945\"\n  }, \"https://arxiv.org/abs/1808.09945\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Extended Bit-Plane Compression for Convolutional Neural Network Accelerators\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.03979\"\n  }, \"https://arxiv.org/abs/1810.03979\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DAC: Data-free Automatic Acceleration of Convolutional Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Qualcomm AI Research & Lehigh University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.08374\"\n  }, \"https://arxiv.org/abs/1812.08374\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Instance-wise Sparsity for Accelerating Deep Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1907.11840\"\n  }, \"https://arxiv.org/abs/1907.11840\"))), mdx(\"h1\", {\n    \"id\": \"code-optimization\"\n  }, \"Code Optimization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Production Deep Learning with NVIDIA GPU Inference Engine\")), mdx(\"img\", {\n    \"src\": \"https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/06/GIE_GoogLeNet_top10kernels-1.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: convolution, bias, and ReLU layers are fused to form a single layer: CBR\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/\"\n  }, \"https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"speed improvement by merging batch normalization and scale #5\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github issue: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/sanghoon/pva-faster-rcnn/issues/5\"\n  }, \"https://github.com/sanghoon/pva-faster-rcnn/issues/5\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Add a tool to merge 'Conv-BN-Scale' into a single 'Conv' layer.\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c/\"\n  }, \"https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Low-memory GEMM-based convolution algorithms for deep neural networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.03395\"\n  }, \"https://arxiv.org/abs/1709.03395\")), mdx(\"h1\", {\n    \"id\": \"projects\"\n  }, \"Projects\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerate Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"This tool aims to accelerate the test-time computation and decrease number of parameters of deep CNNs.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/dmlc/mxnet/tree/master/tools/accnn\"\n  }, \"https://github.com/dmlc/mxnet/tree/master/tools/accnn\"))), mdx(\"h2\", {\n    \"id\": \"optnet\"\n  }, \"OptNet\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"OptNet - reducing memory usage in torch neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/fmassa/optimize-net\"\n  }, \"https://github.com/fmassa/optimize-net\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"NNPACK: Acceleration package for neural networks on multi-core CPUs\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/376828536285f7a1a4f054aaae998e805023f489/68747470733a2f2f6d6172617479737a637a612e6769746875622e696f2f4e4e5041434b2f4e4e5041434b2e706e67\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Maratyszcza/NNPACK\"\n  }, \"https://github.com/Maratyszcza/NNPACK\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"comments(Yann LeCun): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.facebook.com/yann.lecun/posts/10153459577707143\"\n  }, \"https://www.facebook.com/yann.lecun/posts/10153459577707143\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Compression on AlexNet\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/songhan/Deep-Compression-AlexNet\"\n  }, \"https://github.com/songhan/Deep-Compression-AlexNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tiny Darknet\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pjreddie.com/darknet/tiny-darknet/\"\n  }, \"http://pjreddie.com/darknet/tiny-darknet/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CACU: Calculate deep convolution neurAl network on Cell Unit\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/luhaofang/CACU\"\n  }, \"https://github.com/luhaofang/CACU\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"keras_compressor: Model Compression CLI Tool for Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://nico-opendata.jp/ja/casestudy/model_compression/index.html\"\n  }, \"https://nico-opendata.jp/ja/casestudy/model_compression/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/nico-opendata/keras_compressor\"\n  }, \"https://github.com/nico-opendata/keras_compressor\"))), mdx(\"h1\", {\n    \"id\": \"blogs\"\n  }, \"Blogs\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Networks Are Impressively Good At Compression\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/\"\n  }, \"https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"\\u201CMobile friendly\\u201D deep convolutional neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 1: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-1-331120ad40f9#.uy64ladvz\"\n  }, \"https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-1-331120ad40f9#.uy64ladvz\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 2: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9#.u58fkuak3\"\n  }, \"https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9#.u58fkuak3\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Lab41 Reading Group: Deep Compression\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.hbqzn8wfu\"\n  }, \"https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.hbqzn8wfu\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating Machine Learning\")), mdx(\"img\", {\n    \"src\": \"http://www.linleygroup.com/mpr/h/2016/11561/U26_F4v2.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.linleygroup.com/mpr/article.php?id=11561\"\n  }, \"http://www.linleygroup.com/mpr/article.php?id=11561\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Compressing and regularizing deep neural networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks\"\n  }, \"https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How fast is my model?\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://machinethink.net/blog/how-fast-is-my-model/\"\n  }, \"http://machinethink.net/blog/how-fast-is-my-model/\")), mdx(\"h1\", {\n    \"id\": \"talks--videos\"\n  }, \"Talks / Videos\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep compression and EIE: Deep learning model compression, design space exploration and hardware acceleration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=baZOmGSSUAg\"\n  }, \"https://www.youtube.com/watch?v=baZOmGSSUAg\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Compression, DSD Training and EIE: Deep Neural Network Model Compression, Regularization and Hardware Acceleration\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://research.microsoft.com/apps/video/default.aspx?id=266664\"\n  }, \"http://research.microsoft.com/apps/video/default.aspx?id=266664\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tailoring Convolutional Neural Networks for Low-Cost, Low-Power Implementation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: tutorial at the May 2015 Embedded Vision Summit\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=xACJBACStaU\"\n  }, \"https://www.youtube.com/watch?v=xACJBACStaU\"))), mdx(\"h1\", {\n    \"id\": \"resources\"\n  }, \"Resources\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"awesome-model-compression-and-acceleration\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/sun254/awesome-model-compression-and-acceleration\"\n  }, \"https://github.com/sun254/awesome-model-compression-and-acceleration\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Embedded-Neural-Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: collection of works aiming at reducing model sizes or the ASIC/FPGA accelerator for machine learning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ZhishengWang/Embedded-Neural-Network\"\n  }, \"https://github.com/ZhishengWang/Embedded-Neural-Network\"))));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Acceleration and Model Compression\ndate: 2015-10-09\n---\n\n# Papers\n\n**High-Performance Neural Networks for Visual Object Classification**\n\n- intro: \"reduced network parameters by randomly removing connections before training\"\n- arxiv: [http://arxiv.org/abs/1102.0183](http://arxiv.org/abs/1102.0183)\n\n**Predicting Parameters in Deep Learning**\n\n- intro: \"decomposed the weighting matrix into two low-rank matrices\"\n- arxiv: [http://arxiv.org/abs/1306.0543](http://arxiv.org/abs/1306.0543)\n\n**Neurons vs Weights Pruning in Artificial Neural Networks**\n\n- paper: [http://journals.ru.lv/index.php/ETR/article/view/166](http://journals.ru.lv/index.php/ETR/article/view/166)\n\n**Exploiting Linear Structure Within Convolutional Networks for Efcient Evaluation**\n\n- intro: \"presented a series of low-rank decomposition designs for convolutional kernels. \nsingular value decomposition was adopted for the matrix factorization\"\n- paper: [http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf](http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf)\n\n**cuDNN: Efficient Primitives for Deep Learning**\n\n- arxiv: [https://arxiv.org/abs/1410.0759](https://arxiv.org/abs/1410.0759)\n- download: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)\n\n**Efficient and accurate approximations of nonlinear convolutional networks**\n\n- intro: \"considered the subsequent nonlinear units while learning the low-rank decomposition\"\n- arxiv: [http://arxiv.org/abs/1411.4229](http://arxiv.org/abs/1411.4229)\n\n**Convolutional Neural Networks at Constrained Time Cost**\n\n- arxiv: [https://arxiv.org/abs/1412.1710](https://arxiv.org/abs/1412.1710)\n\n**Flattened Convolutional Neural Networks for Feedforward Acceleration**\n\n- intro: ICLR 2015\n- arxiv: [http://arxiv.org/abs/1412.5474](http://arxiv.org/abs/1412.5474)\n- github: [https://github.com/jhjin/flattened-cnn](https://github.com/jhjin/flattened-cnn)\n\n**Compressing Deep Convolutional Networks using Vector Quantization**\n\n- intro: \"this paper showed that vector quantization had a clear advantage \nover matrix factorization methods in compressing fully-connected layers.\"\n- arxiv: [http://arxiv.org/abs/1412.6115](http://arxiv.org/abs/1412.6115)\n\n**Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition**\n\n- intro: \"a low-rank CPdecomposition was adopted to \ntransform a convolutional layer into multiple layers of lower complexity\"\n- arxiv: [http://arxiv.org/abs/1412.6553](http://arxiv.org/abs/1412.6553)\n\n**Deep Fried Convnets**\n\n- intro: \"fully-connected layers were replaced by a single Fastfood layer for end-to-end training with convolutional layers\"\n- arxiv: [http://arxiv.org/abs/1412.7149](http://arxiv.org/abs/1412.7149)\n\n**Fast Convolutional Nets With fbfft: A GPU Performance Evaluation**\n\n- intro: Facebook. ICLR 2015\n- arxiv: [http://arxiv.org/abs/1412.7580](http://arxiv.org/abs/1412.7580)\n- github: [http://facebook.github.io/fbcunn/fbcunn/](http://facebook.github.io/fbcunn/fbcunn/)\n\n**Caffe con Troll: Shallow Ideas to Speed Up Deep Learning**\n\n- intro: a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals\n- arxiv: [http://arxiv.org/abs/1504.04343](http://arxiv.org/abs/1504.04343)\n\n**Compressing Neural Networks with the Hashing Trick**\n\n![](http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/hashednets.png)\n\n- intro: HashedNets. ICML 2015\n- intro: \"randomly grouped connection weights into hash buckets, and then fine-tuned network parameters with back-propagation\"\n- project page: [http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/index.html](http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/index.html)\n- arxiv: [http://arxiv.org/abs/1504.04788](http://arxiv.org/abs/1504.04788)\n- code: [http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/HashedNets.zip](http://www.cse.wustl.edu/~wenlinchen/project/HashedNets/HashedNets.zip)\n\n**PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions**\n\n- intro: NIPS 2016\n- arxiv: [https://arxiv.org/abs/1504.08362](https://arxiv.org/abs/1504.08362)\n- github: [https://github.com/mfigurnov/perforated-cnn-matconvnet](https://github.com/mfigurnov/perforated-cnn-matconvnet)\n- github: [https://github.com/mfigurnov/perforated-cnn-caffe](https://github.com/mfigurnov/perforated-cnn-caffe)\n\n**Accelerating Very Deep Convolutional Networks for Classification and Detection**\n\n- intro: \"considered the subsequent nonlinear units while learning the low-rank decomposition\"\n- arxiv: [http://arxiv.org/abs/1505.06798](http://arxiv.org/abs/1505.06798)\n\n**Fast ConvNets Using Group-wise Brain Damage**\n\n- intro: \"applied group-wise pruning to the convolutional tensor \nto decompose it into the multiplications of thinned dense matrices\"\n- arxiv: [http://arxiv.org/abs/1506.02515](http://arxiv.org/abs/1506.02515)\n\n**Learning both Weights and Connections for Efficient Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1506.02626](http://arxiv.org/abs/1506.02626)\n\n**Data-free parameter pruning for Deep Neural Networks**\n\n- intro: \"proposed to remove redundant neurons instead of network connections\"\n- arxiv: [http://arxiv.org/abs/1507.06149](http://arxiv.org/abs/1507.06149)\n\n**Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding**\n\n- intro: ICLR 2016 Best Paper\n- intro: \"reduced the size of AlexNet by 35x from 240MB to 6.9MB, the size of VGG16 by 49x from 552MB to 11.3MB, with no loss of accuracy\"\n- arxiv: [http://arxiv.org/abs/1510.00149](http://arxiv.org/abs/1510.00149)\n- video: [http://videolectures.net/iclr2016_han_deep_compression/](http://videolectures.net/iclr2016_han_deep_compression/)\n\n**Structured Transforms for Small-Footprint Deep Learning**\n\n- intro: NIPS 2015\n- arxiv: [https://arxiv.org/abs/1510.01722](https://arxiv.org/abs/1510.01722)\n- paper: [https://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning](https://papers.nips.cc/paper/5869-structured-transforms-for-small-footprint-deep-learning)\n\n**ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines**\n\n- arxiv: [http://arxiv.org/abs/1510.06706](http://arxiv.org/abs/1510.06706)\n- github: [https://github.com/seung-lab/znn-release](https://github.com/seung-lab/znn-release)\n\n**Reducing the Training Time of Neural Networks by Partitioning**\n\n- arxiv: [http://arxiv.org/abs/1511.02954](http://arxiv.org/abs/1511.02954)\n\n**Convolutional neural networks with low-rank regularization**\n\n- arxiv: [http://arxiv.org/abs/1511.06067](http://arxiv.org/abs/1511.06067)\n- github: [https://github.com/chengtaipu/lowrankcnn](https://github.com/chengtaipu/lowrankcnn)\n\n**CNNdroid: Open Source Library for GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android**\n\n- arxiv: [https://arxiv.org/abs/1511.07376](https://arxiv.org/abs/1511.07376)\n- paper: [http://dl.acm.org/authorize.cfm?key=N14731](http://dl.acm.org/authorize.cfm?key=N14731)\n- slides: [http://sharif.edu/~matin/pub/2016_mm_slides.pdf](http://sharif.edu/~matin/pub/2016_mm_slides.pdf)\n- github: [https://github.com/ENCP/CNNdroid](https://github.com/ENCP/CNNdroid)\n\n**EIE: Efficient Inference Engine on Compressed Deep Neural Network**\n\n- intro: ISCA 2016\n- arxiv: [http://arxiv.org/abs/1602.01528](http://arxiv.org/abs/1602.01528)\n- slides: [http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf](http://on-demand.gputechconf.com/gtc/2016/presentation/s6561-song-han-deep-compression.pdf)\n- slides: [http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf](http://web.stanford.edu/class/ee380/Abstracts/160106-slides.pdf)\n\n**Convolutional Tables Ensemble: classification in microseconds**\n\n- arxiv: [http://arxiv.org/abs/1602.04489](http://arxiv.org/abs/1602.04489)\n\n**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size**\n\n- intro: DeepScale & UC Berkeley\n- arxiv: [http://arxiv.org/abs/1602.07360](http://arxiv.org/abs/1602.07360)\n- github: [https://github.com/DeepScale/SqueezeNet](https://github.com/DeepScale/SqueezeNet)\n- homepage: [http://songhan.github.io/SqueezeNet-Deep-Compression/](http://songhan.github.io/SqueezeNet-Deep-Compression/)\n- github: [https://github.com/songhan/SqueezeNet-Deep-Compression](https://github.com/songhan/SqueezeNet-Deep-Compression)\n- note: [https://www.evernote.com/shard/s146/sh/108eea91-349b-48ba-b7eb-7ac8f548bee9/5171dc6b1088fba05a4e317f7f5d32a3](https://www.evernote.com/shard/s146/sh/108eea91-349b-48ba-b7eb-7ac8f548bee9/5171dc6b1088fba05a4e317f7f5d32a3)\n- github(Keras): [https://github.com/DT42/squeezenet_demo](https://github.com/DT42/squeezenet_demo)\n- github(Keras): [https://github.com/rcmalli/keras-squeezenet](https://github.com/rcmalli/keras-squeezenet)\n- github(PyTorch): [https://github.com/gsp-27/pytorch_Squeezenet](https://github.com/gsp-27/pytorch_Squeezenet)\n\n**SqueezeNet-Residual**\n\n- intro: Residual-SqueezeNet improves the top-1 accuracy of SqueezeNet by 2.9% on ImageNet without changing the model size(only 4.8MB).\n- github: [https://github.com/songhan/SqueezeNet-Residual](https://github.com/songhan/SqueezeNet-Residual)\n\n**Lab41 Reading Group: SqueezeNet**\n\n[https://medium.com/m/global-identity?redirectUrl=https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75](https://medium.com/m/global-identity?redirectUrl=https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75)\n\n**Simplified_SqueezeNet**\n\n- intro: An improved version of SqueezeNet networks\n- github(Caffe): [https://github.com/NidabaSystems/Simplified_SqueezeNet](https://github.com/NidabaSystems/Simplified_SqueezeNet)\n\n**SqueezeNet Keras Dogs vs. Cats demo**\n\n- github: [https://github.com/chasingbob/squeezenet-keras](https://github.com/chasingbob/squeezenet-keras)\n\n**Convolutional Neural Networks using Logarithmic Data Representation**\n\n- arxiv: [http://arxiv.org/abs/1603.01025](http://arxiv.org/abs/1603.01025)\n\n**DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices**\n\n- paper: [http://niclane.org/pubs/deepx_ipsn.pdf](http://niclane.org/pubs/deepx_ipsn.pdf)\n\n**Hardware-oriented Approximation of Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1604.03168](http://arxiv.org/abs/1604.03168)\n- homepage: [http://ristretto.lepsucd.com/](http://ristretto.lepsucd.com/)\n- github(\"Ristretto: Caffe-based approximation of convolutional neural networks\"): [https://github.com/pmgysel/caffe](https://github.com/pmgysel/caffe)\n\n**Deep Neural Networks Under Stress**\n\n- intro: ICIP 2016\n- arxiv: [http://arxiv.org/abs/1605.03498](http://arxiv.org/abs/1605.03498)\n- github: [https://github.com/MicaelCarvalho/DNNsUnderStress](https://github.com/MicaelCarvalho/DNNsUnderStress)\n\n**ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks using Angle Sensitive Pixels**\n\n- arxiv: [http://arxiv.org/abs/1605.03621](http://arxiv.org/abs/1605.03621)\n\n**Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups**\n\n- intro: \"for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU).\nFor the deeper ResNet 200 our model has 25% fewer floating point operations and 44% fewer parameters, \nwhile maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).\"\n- arxiv: [https://arxiv.org/abs/1605.06489](https://arxiv.org/abs/1605.06489)\n\n**Functional Hashing for Compressing Neural Networks**\n\n- intro: FunHashNN\n- arxiv: [http://arxiv.org/abs/1605.06560](http://arxiv.org/abs/1605.06560)\n\n**Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1605.06402](http://arxiv.org/abs/1605.06402)\n\n**YodaNN: An Ultra-Low Power Convolutional Neural Network Accelerator Based on Binary Weights**\n\n- arxiv: [https://arxiv.org/abs/1606.05487](https://arxiv.org/abs/1606.05487)\n\n**Learning Structured Sparsity in Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1608.03665](http://arxiv.org/abs/1608.03665)\n\n**Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial \"Bottleneck\" Structure**\n\n[https://arxiv.org/abs/1608.04337](https://arxiv.org/abs/1608.04337)\n\n**Dynamic Network Surgery for Efficient DNNs**\n\n- intro: NIPS 2016\n- intro: compress the number of parameters in LeNet-5 and AlexNet by a factor of 108 and 17.7 respectively\n- arxiv: [http://arxiv.org/abs/1608.04493](http://arxiv.org/abs/1608.04493)\n- github(official. Caffe): [https://github.com/yiwenguo/Dynamic-Network-Surgery](https://github.com/yiwenguo/Dynamic-Network-Surgery)\n\n**Scalable Compression of Deep Neural Networks**\n\n- intro: ACM Multimedia 2016\n- arxiv: [http://arxiv.org/abs/1608.07365](http://arxiv.org/abs/1608.07365)\n\n**Pruning Filters for Efficient ConvNets**\n\n- intro: NIPS Workshop on Efficient Methods for Deep Neural Networks (EMDNN), 2016\n- arxiv: [http://arxiv.org/abs/1608.08710](http://arxiv.org/abs/1608.08710)\n\n**Fixed-point Factorized Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.01972](https://arxiv.org/abs/1611.01972)\n\n**Ultimate tensorization: compressing convolutional and FC layers alike**\n\n- intro: NIPS 2016 workshop: Learning with Tensors: Why Now and How?\n- arxiv: [https://arxiv.org/abs/1611.03214](https://arxiv.org/abs/1611.03214)\n- github: [https://github.com/timgaripov/TensorNet-TF](https://github.com/timgaripov/TensorNet-TF)\n\n**Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning**\n\n- intro: \"the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss\"\n- arxiv: [https://arxiv.org/abs/1611.05128](https://arxiv.org/abs/1611.05128)\n\n**Net-Trim: A Layer-wise Convex Pruning of Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.05162](https://arxiv.org/abs/1611.05162)\n\n**LCNN: Lookup-based Convolutional Neural Network**\n\n- intro: \"Our fastest LCNN offers 37.6x speed up over AlexNet while maintaining 44.3% top-1 accuracy.\"\n- arxiv: [https://arxiv.org/abs/1611.06473](https://arxiv.org/abs/1611.06473)\n\n**Deep Tensor Convolution on Multicores**\n\n- intro: present the first practical CPU implementation of tensor convolution optimized for deep networks of small kernels\n- arxiv: [https://arxiv.org/abs/1611.06565](https://arxiv.org/abs/1611.06565)\n\n**Training Sparse Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1611.06694](https://arxiv.org/abs/1611.06694)\n\n**FINN: A Framework for Fast, Scalable Binarized Neural Network Inference**\n\n- intro: Xilinx Research Labs & Norwegian University of Science and Technology & University of Sydney\n- intro: 25th International Symposium on Field-Programmable Gate Arrays\n- keywords: FPGA\n- paper: [http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf](http://www.idi.ntnu.no/~yamanu/2017-fpga-finn-preprint.pdf)\n- arxiv: [https://arxiv.org/abs/1612.07119](https://arxiv.org/abs/1612.07119)\n- github: [https://github.com/Xilinx/BNN-PYNQ](https://github.com/Xilinx/BNN-PYNQ)\n\n**Deep Learning with INT8 Optimization on Xilinx Devices**\n\n- intro: \"Xilinx's integrated DSP architecture can achieve 1.75X solution-level performance \nat INT8 deep learning operations than other FPGA DSP architectures\"\n- paper: [https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf](https://www.xilinx.com/support/documentation/white_papers/wp486-deep-learning-int8.pdf)\n\n**Parameter Compression of Recurrent Neural Networks and Degredation of Short-term Memory**\n\n- arxiv: [https://arxiv.org/abs/1612.00891](https://arxiv.org/abs/1612.00891)\n\n**An OpenCL(TM) Deep Learning Accelerator on Arria 10**\n\n- intro: FPGA 2017\n- arxiv: [https://arxiv.org/abs/1701.03534](https://arxiv.org/abs/1701.03534)\n\n**The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning**\n\n- intro: CMU & Universitat Paderborn]\n- arxiv: [https://arxiv.org/abs/1701.04465](https://arxiv.org/abs/1701.04465)\n\n**DL-gleaning: An Approach For Improving Inference Speed And Accuracy**\n\n- intro: Electronics Telecommunications Research Institute (ETRI)\n- paper: [https://openreview.net/pdf?id=Hynn8SHOx](https://openreview.net/pdf?id=Hynn8SHOx)\n\n**Energy Saving Additive Neural Network**\n\n- intro: Middle East Technical University & Bilkent University\n- arxiv: [https://arxiv.org/abs/1702.02676](https://arxiv.org/abs/1702.02676)\n\n**Soft Weight-Sharing for Neural Network Compression**\n\n- intro: ICLR 2017. University of Amsterdam\n- arxiv: [https://arxiv.org/abs/1702.04008](https://arxiv.org/abs/1702.04008)\n- github: [https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression](https://github.com/KarenUllrich/Tutorial-SoftWeightSharingForNNCompression)\n\n**A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1703.04071](https://arxiv.org/abs/1703.04071)\n\n**Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations**\n\n- intro: ARM Research\n- arxiv: [https://arxiv.org/abs/1703.03073](https://arxiv.org/abs/1703.03073)\n\n**DyVEDeep: Dynamic Variable Effort Deep Neural Networks**\n\n[https://arxiv.org/abs/1704.01137](https://arxiv.org/abs/1704.01137)\n\n**Bayesian Compression for Deep Learning**\n\n[https://arxiv.org/abs/1705.08665](https://arxiv.org/abs/1705.08665)\n\n**A Kernel Redundancy Removing Policy for Convolutional Neural Network**\n\n[https://arxiv.org/abs/1705.10748](https://arxiv.org/abs/1705.10748)\n\n**Gated XNOR Networks: Deep Neural Networks with Ternary Weights and Activations under a Unified Discretization Framework**\n\n- keywords: discrete state transition (DST)\n- arxiv: [https://arxiv.org/abs/1705.09283](https://arxiv.org/abs/1705.09283)\n\n**SEP-Nets: Small and Effective Pattern Networks**\n\n- intro: The University of Iowa & Snap Research\n- arxiv: [https://arxiv.org/abs/1706.03912](https://arxiv.org/abs/1706.03912)\n\n**MEC: Memory-efficient Convolution for Deep Neural Network**\n\n- intro: ICML 2017\n- arxiv: [https://arxiv.org/abs/1706.06873](https://arxiv.org/abs/1706.06873)\n\n**Data-Driven Sparse Structure Selection for Deep Neural Networks**\n\n[https://arxiv.org/abs/1707.01213](https://arxiv.org/abs/1707.01213)\n\n**An End-to-End Compression Framework Based on Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1708.00838](https://arxiv.org/abs/1708.00838)\n\n**Domain-adaptive deep network compression**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.01041](https://arxiv.org/abs/1709.01041)\n- github: [https://github.com/mmasana/DALR](https://github.com/mmasana/DALR)\n\n**Binary-decomposed DCNN for accelerating computation and compressing model without retraining**\n\n[https://arxiv.org/abs/1709.04731](https://arxiv.org/abs/1709.04731)\n\n**Improving Efficiency in Convolutional Neural Network with Multilinear Filters**\n\n[https://arxiv.org/abs/1709.09902](https://arxiv.org/abs/1709.09902)\n\n**A Survey of Model Compression and Acceleration for Deep Neural Networks**\n\n- intro: IEEE Signal Processing Magazine. IBM Thoms J. Watson Research Center & Tsinghua University & Huazhong University of Science and Technology\n- arxiv: [https://arxiv.org/abs/1710.09282](https://arxiv.org/abs/1710.09282)\n\n**Compression-aware Training of Deep Networks**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1711.02638](https://arxiv.org/abs/1711.02638)\n\n**Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method**\n\n[https://arxiv.org/abs/1711.06528](https://arxiv.org/abs/1711.06528)\n\n**Reducing Deep Network Complexity with Fourier Transform Methods**\n\n- intro: Harvard University\n- arxiv: [https://arxiv.org/abs/1801.01451](https://arxiv.org/abs/1801.01451)\n- github: [https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods](https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Methods)\n\n**EffNet: An Efficient Structure for Convolutional Neural Networks**\n\n- intro: Aptiv & University of Wupperta\n- arxiv: [https://arxiv.org/abs/1801.06434](https://arxiv.org/abs/1801.06434)\n\n**Universal Deep Neural Network Compression**\n\n[https://arxiv.org/abs/1802.02271](https://arxiv.org/abs/1802.02271)\n\n**Paraphrasing Complex Network: Network Compression via Factor Transfer**\n\n[https://arxiv.org/abs/1802.04977](https://arxiv.org/abs/1802.04977)\n\n**Compressing Neural Networks using the Variational Information Bottleneck**\n\n- intro: Tsinghua University & ShanghaiTech University & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1802.10399](https://arxiv.org/abs/1802.10399)\n\n**Adversarial Network Compression**\n\n[https://arxiv.org/abs/1803.10750](https://arxiv.org/abs/1803.10750)\n\n**Expanding a robot's life: Low power object recognition via FPGA-based DCNN deployment**\n\n- intro: MOCAST 2018\n- arxiv: [https://arxiv.org/abs/1804.00512](https://arxiv.org/abs/1804.00512)\n\n**Accelerating CNN inference on FPGAs: A Survey**\n\n- intro: [Institut Pascal]\n- arxiv: [https://arxiv.org/abs/1806.01683](https://arxiv.org/abs/1806.01683)\n\n**Doubly Nested Network for Resource-Efficient Inference**\n\n[https://arxiv.org/abs/1806.07568](https://arxiv.org/abs/1806.07568)\n\n**Smallify: Learning Network Size while Training**\n\n- intro: MIT\n- arxiv: [https://arxiv.org/abs/1806.03723](https://arxiv.org/abs/1806.03723)\n\n**Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on Embedded FPGAs**\n\n- intro: 27th International Symposium on Field-Programmable Gate Arrays, February 2019\n- arxiv: [https://arxiv.org/abs/1811.08634](https://arxiv.org/abs/1811.08634)\n\n**Cascaded Projection: End-to-End Network Compression and Acceleration**\n\n[https://arxiv.org/abs/1903.04988](https://arxiv.org/abs/1903.04988)\n\n**FALCON: Fast and Lightweight Convolution for Compressing and Accelerating CNN**\n\n[https://arxiv.org/abs/1909.11321](https://arxiv.org/abs/1909.11321)\n\n# Compressing Deep Neural Network\n\n**Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions**\n\n- intro: ICML 2018\n- arxiv: [https://arxiv.org/abs/1806.09228](https://arxiv.org/abs/1806.09228)\n- github: [https://github.com/Sandbox3aster/Deep-K-Means-pytorch](https://github.com/Sandbox3aster/Deep-K-Means-pytorch)\n\n**Optimize Deep Convolutional Neural Network with Ternarized Weights and High Accuracy**\n\n- intro: University of Central Florida & Tencent AI lab, Seattle\n- arxiv: [https://arxiv.org/abs/1807.07948](https://arxiv.org/abs/1807.07948)\n\n**Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks**\n\n[https://arxiv.org/abs/1808.05240](https://arxiv.org/abs/1808.05240)\n\n**ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1809.01330](https://arxiv.org/abs/1809.01330)\n\n**A Framework for Fast and Efficient Neural Network Compression**\n\n[https://arxiv.org/abs/1811.12781](https://arxiv.org/abs/1811.12781)\n\n**ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples**\n\n[https://arxiv.org/abs/1811.12673](https://arxiv.org/abs/1811.12673)\n\n# Pruning\n\n**ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression**\n\n- intro: ICCV 2017. Nanjing University & Shanghai Jiao Tong University\n- arxiv: [https://arxiv.org/abs/1707.06342](https://arxiv.org/abs/1707.06342)\n- github(Caffe): [https://github.com/Roll920/ThiNet](https://github.com/Roll920/ThiNet)\n\n**Neuron Pruning for Compressing Deep Networks using Maxout Architectures**\n\n- intro: GCPR 2017\n- arxiv: [https://arxiv.org/abs/1707.06838](https://arxiv.org/abs/1707.06838)\n\n**Fine-Pruning: Joint Fine-Tuning and Compression of a Convolutional Network with Bayesian Optimization**\n\n- intro: BMVC 2017 oral. Simon Fraser University\n- arxiv: [https://arxiv.org/abs/1707.09102](https://arxiv.org/abs/1707.09102)\n\n**Prune the Convolutional Neural Networks with Sparse Shrink**\n\n[https://arxiv.org/abs/1708.02439](https://arxiv.org/abs/1708.02439)\n\n**NISP: Pruning Networks using Neuron Importance Score Propagation**\n\n- intro: University of Maryland & IBM T. J. Watson Research\n- arxiv: [https://arxiv.org/abs/1711.05908](https://arxiv.org/abs/1711.05908)\n\n**Automated Pruning for Deep Neural Network Compression**\n\n[https://arxiv.org/abs/1712.01721](https://arxiv.org/abs/1712.01721)\n\n**Learning to Prune Filters in Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1801.07365](https://arxiv.org/abs/1801.07365)\n\n**Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.10447](https://arxiv.org/abs/1801.10447)\n\n**A novel channel pruning method for deep neural network compression**\n\n[https://arxiv.org/abs/1805.11394](https://arxiv.org/abs/1805.11394)\n\n**PCAS: Pruning Channels with Attention Statistics**\n\n- intro: Oki Electric Industry Co., Ltd\n- arxiv: [https://arxiv.org/abs/1806.05382](https://arxiv.org/abs/1806.05382)\n\n**Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1808.06866](https://arxiv.org/abs/1808.06866)\n- github: [https://github.com/he-y/soft-filter-pruning](https://github.com/he-y/soft-filter-pruning)\n\n**Progressive Deep Neural Networks Acceleration via Soft Filter Pruning**\n\n[https://arxiv.org/abs/1808.07471](https://arxiv.org/abs/1808.07471)\n\n**Pruning neural networks: is it time to nip it in the bud?**\n\n[https://arxiv.org/abs/1810.04622](https://arxiv.org/abs/1810.04622)\n\n**Rethinking the Value of Network Pruning**\n\n[https://arxiv.org/abs/1810.05270](https://arxiv.org/abs/1810.05270)\n\n**Dynamic Channel Pruning: Feature Boosting and Suppression**\n\n[https://arxiv.org/abs/1810.05331](https://arxiv.org/abs/1810.05331)\n\n**Interpretable Convolutional Filter Pruning**\n\n[https://arxiv.org/abs/1810.07322](https://arxiv.org/abs/1810.07322)\n\n**Progressive Weight Pruning of Deep Neural Networks using ADMM**\n\n[https://arxiv.org/abs/1810.07378](https://arxiv.org/abs/1810.07378)\n\n**Pruning Deep Neural Networks using Partial Least Squares**\n\n- arxiv: [https://arxiv.org/abs/1810.07610](https://arxiv.org/abs/1810.07610)\n- github: [https://github.com/arturjordao/PruningNeuralNetworks](https://github.com/arturjordao/PruningNeuralNetworks)\n\n**Hybrid Pruning: Thinner Sparse Networks for Fast Inference on Edge Devices**\n\n[https://arxiv.org/abs/1811.00482](https://arxiv.org/abs/1811.00482)\n\n**Discrimination-aware Channel Pruning for Deep Neural Networks**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.11809](https://arxiv.org/abs/1810.11809)\n\n**Stability Based Filter Pruning for Accelerating Deep CNNs**\n\n- intro: WACV 2019\n- arxiv: [https://arxiv.org/abs/1811.08321](https://arxiv.org/abs/1811.08321)\n\n**Structured Pruning for Efficient ConvNets via Incremental Regularization**\n\n- intro: NIPS 2018 workshop on \"Compact Deep Neural Network Representation with Industrial Applications\"\n- arxiv: [https://arxiv.org/abs/1811.08390](https://arxiv.org/abs/1811.08390)\n\n**Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1811.08589](https://arxiv.org/abs/1811.08589)\n\n**A Layer Decomposition-Recomposition Framework for Neuron Pruning towards Accurate Lightweight Networks**\n\n- intro: AAAI 2019 as oral\n- intro: Hikvision Research Institute\n- arxiv: [https://arxiv.org/abs/1812.06611](https://arxiv.org/abs/1812.06611)\n\n**Quantized Guided Pruning for Efficient Hardware Implementations of Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1812.11337](https://arxiv.org/abs/1812.11337)\n\n**Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning**\n\n[https://arxiv.org/abs/1901.07827](https://arxiv.org/abs/1901.07827)\n\n**Partition Pruning: Parallelization-Aware Pruning for Deep Neural Networks**\n\n[https://arxiv.org/abs/1901.11391](https://arxiv.org/abs/1901.11391)\n\n**Pruning from Scratch**\n\n- intro: Tsinghua University & Ant Financial & Huawei Noahs Ark Lab\n- arxiv: [https://arxiv.org/abs/1909.12579](https://arxiv.org/abs/1909.12579)\n\n**Global Sparse Momentum SGD for Pruning Very Deep Neural Networks**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1909.12778](https://arxiv.org/abs/1909.12778)\n\n**FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization**\n\n- openreview: [https://openreview.et/forum?id=rJeUPlrYvr](https://openreview.net/forum?id=rJeUPlrYvr)\n- paper: [https://openreview.net/pdf?id=rJeUPlrYvr](https://openreview.net/pdf?id=rJeUPlrYvr)\n- github: [https://github.com/anonymous47823493/FNNP](https://github.com/anonymous47823493/FNNP)\n\n**Pruning Filter in Filter**\n\n- intro: NeurIPS 2020\n- intro: Harbin Institute of Technology & Tencent Youtu Lab\n- arxiv: [https://arxiv.org/abs/2009.14410](https://arxiv.org/abs/2009.14410)\n\n# Low-Precision Networks\n\n**Accelerating Deep Convolutional Networks using low-precision and sparsity**\n\n- intro: Intel Labs\n- arxiv: [https://arxiv.org/abs/1610.00324](https://arxiv.org/abs/1610.00324)\n\n**Deep Learning with Low Precision by Half-wave Gaussian Quantization**\n\n- intro: HWGQ-Net\n- arxiv: [https://arxiv.org/abs/1702.00953](https://arxiv.org/abs/1702.00953)\n\n**Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights**\n\n- intro: ICLR 2017\n- arxiv: [https://arxiv.org/abs/1702.03044](https://arxiv.org/abs/1702.03044)\n- openreview: [https://openreview.net/forum?id=HyQJ-mclg&noteId=HyQJ-mclg](https://openreview.net/forum?id=HyQJ-mclg&noteId=HyQJ-mclg)\n\n**ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1706.02393](https://arxiv.org/abs/1706.02393)\n- github: [https://github.com/gudovskiy/ShiftCNN](https://github.com/gudovskiy/ShiftCNN)\n\n**Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM**\n\n- intro: Alibaba Group\n- keywords: alternating direction method of multipliers (ADMM)\n- arxiv: [https://arxiv.org/abs/1707.09870](https://arxiv.org/abs/1707.09870)\n\n**Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization**\n\n- intro: BMVC 2017 Oral\n- arxiv: [https://arxiv.org/abs/1708.01001](https://arxiv.org/abs/1708.01001)\n\n**Compressing Low Precision Deep Neural Networks Using Sparsity-Induced Regularization in Ternary Networks**\n\n- intro: ICONIP 2017\n- arxiv: [https://arxiv.org/abs/1709.06262](https://arxiv.org/abs/1709.06262)\n\n**Learning Low Precision Deep Neural Networks through Regularization**\n\n[https://arxiv.org/abs/1809.00095](https://arxiv.org/abs/1809.00095)\n\n**Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference**\n\n[https://arxiv.org/abs/1809.04191](https://arxiv.org/abs/1809.04191)\n\n**SQuantizer: Simultaneous Learning for Both Sparse and Low-precision Neural Networks**\n\n- intro: Movidius, AIPG, Intel\n- arxiv: [https://arxiv.org/abs/1812.08301](https://arxiv.org/abs/1812.08301)\n\n# Quantized Neural Networks\n\n**Quantized Convolutional Neural Networks for Mobile Devices**\n\n- intro: Q-CNN\n- intro: \"Extensive experiments on the ILSVRC-12 benchmark demonstrate \n4  6 speed-up and 15  20 compression with merely one percentage loss of classification accuracy\"\n- arxiv: [http://arxiv.org/abs/1512.06473](http://arxiv.org/abs/1512.06473)\n- github: [https://github.com/jiaxiang-wu/quantized-cnn](https://github.com/jiaxiang-wu/quantized-cnn)\n\n**Training Quantized Nets: A Deeper Understanding**\n\n- intro: University of Maryland & Cornell University\n- arxiv: [https://arxiv.org/abs/1706.02379](https://arxiv.org/abs/1706.02379)\n\n**Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks**\n\n- intro: the top-5 error rate of 4-bit quantized GoogLeNet model is 12.7%\n- arxiv: [https://arxiv.org/abs/1706.07145](https://arxiv.org/abs/1706.07145)\n\n**Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference**\n\n- intro: CVPR 2018. Google\n- arxiv: [https://arxiv.org/abs/1712.05877](https://arxiv.org/abs/1712.05877)\n\n**Deep Neural Network Compression with Single and Multiple Level Quantization**\n\n- intro: AAAI 2018. Shanghai Jiao Tong University & University of Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/1803.03289](https://arxiv.org/abs/1803.03289)\n\n**Quantizing deep convolutional networks for efficient inference: A whitepaper**\n\n- intro: Google\n- arxiv: [https://arxiv.org/abs/1806.08342](https://arxiv.org/abs/1806.08342)\n\n**CascadeCNN: Pushing the Performance Limits of Quantisation in Convolutional Neural Networks**\n\n- intro: 28th International Conference on Field Programmable Logic & Applications (FPL), 2018\n- arxiv: [https://arxiv.org/abs/1807.05053](https://arxiv.org/abs/1807.05053)\n\n**Bridging the Accuracy Gap for 2-bit Quantized Neural Networks (QNN)**\n\n- intro: IBM Research AI\n- arxiv: [https://arxiv.org/abs/1807.06964](https://arxiv.org/abs/1807.06964)\n\n**Joint Training of Low-Precision Neural Network with Quantization Interval Parameters**\n\n[https://arxiv.org/abs/1808.05779](https://arxiv.org/abs/1808.05779)\n\n**Differentiable Fine-grained Quantization for Deep Neural Network Compression**\n\n[https://arxiv.org/abs/1810.10351](https://arxiv.org/abs/1810.10351)\n\n**HAQ: Hardware-Aware Automated Quantization**\n\n[https://arxiv.org/abs/1811.08886](https://arxiv.org/abs/1811.08886)\n\n**DNQ: Dynamic Network Quantization**\n\n- intro: Shanghai Jiao Tong University & Qualcomm AI Research\n- arxiv: [https://arxiv.org/abs/1812.02375](https://arxiv.org/abs/1812.02375)\n\n**Trained Rank Pruning for Efficient Deep Neural Networks**\n\n- intro: Shanghai Jiao Tong University & Qualcomm AI Research & Duke University\n- arxiv: [https://arxiv.org/abs/1812.02402](https://arxiv.org/abs/1812.02402)\n\n**Training Quantized Network with Auxiliary Gradient Module**\n\n- intro: The University of Adelaide & Australian Centre for Robotic Vision & South China University of Technology\n- arxiv: [https://arxiv.org/abs/1903.11236](https://arxiv.org/abs/1903.11236)\n\n**FLightNNs: Lightweight Quantized Deep Neural Networks for Fast and Accurate Inference**\n\n- intro: Carnegie Mellon University\n- intro: [https://arxiv.org/abs/1904.02835](https://arxiv.org/abs/1904.02835)\n\n**And the Bit Goes Down: Revisiting the Quantization of Neural Networks**\n\n- intro: Facebook AI Research & Univ Rennes\n- arxiv: [https://arxiv.org/abs/1907.05686](https://arxiv.org/abs/1907.05686)\n\n**Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks**\n\n- intro: ICCV 2019\n- arxiv: [https://arxiv.org/abs/1908.05033](https://arxiv.org/abs/1908.05033)\n\n**Bit Efficient Quantization for Deep Neural Networks**\n\n- intro: NeurIPS workshop 2019\n- arxiv: [https://arxiv.org/abs/1910.04877](https://arxiv.org/abs/1910.04877)\n\n**Quantization Networks**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1911.09464](https://arxiv.org/abs/1911.09464)\n\n**Adaptive Loss-aware Quantization for Multi-bit Networks**\n\n[https://arxiv.org/abs/1912.08883](https://arxiv.org/abs/1912.08883)\n\n**Distribution Adaptive INT8 Quantization for Training CNNs**\n\n- intro: AAAI 2021\n- intro: Machine Intelligence Technology Lab, Alibaba Group\n- arxiv: [https://arxiv.org/abs/2102.04782](https://arxiv.org/abs/2102.04782)\n\n**Distance-aware Quantization**\n\n- intro: ICCV 2021\n- arxiv: [https://arxiv.org/abs/2108.06983](https://arxiv.org/abs/2108.06983)\n\n# Binary Convolutional Neural Networks / Binarized Neural Networks\n\n**BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**\n\n**Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**\n\n[https://arxiv.org/abs/1602.02830](https://arxiv.org/abs/1602.02830)\n\n**Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations**\n\n[https://arxiv.org/abs/1609.07061](https://arxiv.org/abs/1609.07061)\n\n**XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1603.05279](http://arxiv.org/abs/1603.05279)\n- github(Torch): [https://github.com/mrastegari/XNOR-Net](https://github.com/mrastegari/XNOR-Net)\n\n**XNOR-Net++: Improved Binary Neural Networks**\n\n- intro: BMVC 2019\n- arxiv: [https://arxiv.org/abs/1909.13863](https://arxiv.org/abs/1909.13863)\n\n**DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients**\n\n[https://arxiv.org/abs/1606.06160](https://arxiv.org/abs/1606.06160)\n\n**A 7.663-TOPS 8.2-W Energy-efficient FPGA Accelerator for Binary Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.06392](https://arxiv.org/abs/1702.06392)\n\n**Espresso: Efficient Forward Propagation for BCNNs**\n\n- arxiv: [https://arxiv.org/abs/1705.07175](https://arxiv.org/abs/1705.07175)\n- github: [https://github.com/fpeder/espresso](https://github.com/fpeder/espresso)\n\n**BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet**\n\n- keywords: Binary Neural Networks (BNNs)\n- arxiv: [https://arxiv.org/abs/1705.09864](https://arxiv.org/abs/1705.09864)\n- github: [https://github.com/hpi-xnor](https://github.com/hpi-xnor)\n\n**ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1706.02393](https://arxiv.org/abs/1706.02393)\n\n**Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration**\n\n- intro: Embedded Vision Workshop (CVPRW). UC San Diego & UC Los Angeles & Cornell University\n- arxiv: [https://arxiv.org/abs/1707.04693](https://arxiv.org/abs/1707.04693)\n\n**Embedded Binarized Neural Networks**\n\n[https://arxiv.org/abs/1709.02260](https://arxiv.org/abs/1709.02260)\n\n**Compact Hash Code Learning with Binary Deep Neural Network**\n\n- intro: Singapore University of Technology and Design\n- arxiv: [https://arxiv.org/abs/1712.02956](https://arxiv.org/abs/1712.02956)\n\n**Build a Compact Binary Neural Network through Bit-level Sensitivity and Data Pruning**\n\n[https://arxiv.org/abs/1802.00904](https://arxiv.org/abs/1802.00904)\n\n**From Hashing to CNNs: Training BinaryWeight Networks via Hashing**\n\n[https://arxiv.org/abs/1802.02733](https://arxiv.org/abs/1802.02733)\n\n**Energy Efficient Hadamard Neural Networks**\n\n- keywords: Binary Weight and Hadamard-transformed Image Network (BWHIN), Binary Weight Network (BWN), Hadamard-transformed Image Network (HIN)\n- arxiv: [https://arxiv.org/abs/1805.05421](https://arxiv.org/abs/1805.05421)\n\n**Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?**\n\n[https://arxiv.org/abs/1806.07550](https://arxiv.org/abs/1806.07550)\n\n**Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.00278](https://arxiv.org/abs/1808.00278)\n\n**Training Compact Neural Networks with Binary Weights and Low Precision Activations**\n\n[https://arxiv.org/abs/1808.02631](https://arxiv.org/abs/1808.02631)\n\n**Training wide residual networks for deployment using a single bit for each weight**\n\n- intro: ICLR 2018\n- arxiv: [https://arxiv.org/abs/1802.08530](https://arxiv.org/abs/1802.08530)\n- github(official, PyTorch): [https://github.com/szagoruyko/binary-wide-resnet](https://github.com/szagoruyko/binary-wide-resnet)\n\n**Composite Binary Decomposition Networks**\n\n[https://arxiv.org/abs/1811.06668](https://arxiv.org/abs/1811.06668)\n\n**Training Competitive Binary Neural Networks from Scratch**\n\n- intro: University of Potsdam\n- intro: BMXNet v2: An Open-Source Binary Neural Network Implementation Based on MXNet\n- arxiv: [https://arxiv.org/abs/1812.01965](https://arxiv.org/abs/1812.01965)\n- github: [https://github.com/hpi-xnor/BMXNet-v2](https://github.com/hpi-xnor/BMXNet-v2)\n\n**Regularizing Activation Distribution for Training Binarized Deep Networks**\n\n- intro: Carnegie Mellon University\n- arxiv: [https://arxiv.org/abs/1904.02823](https://arxiv.org/abs/1904.02823)\n\n**GBCNs: Genetic Binary Convolutional Networks for Enhancing the Performance of 1-bit DCNNs**\n\n- intro: AAAI 2020\n- arxiv: [https://arxiv.org/abs/1911.11634](https://arxiv.org/abs/1911.11634)\n\n**Training Binary Neural Networks with Real-to-Binary Convolutions**\n\n- intro: ICLR 2020\n- intro: Samsung AI Research Center, Cambridge, UK & The University of Nottingham\n- arxiv: [https://arxiv.org/abs/2003.11535](https://arxiv.org/abs/2003.11535)\n- github: [https://github.com/brais-martinez/real2binary](https://github.com/brais-martinez/real2binary)\n\n# Accelerating / Fast Algorithms\n\n**Fast Algorithms for Convolutional Neural Networks**\n\n- intro: \"2.6x as fast as Caffe when comparing CPU implementations\"\n- keywords: Winograd's minimal filtering algorithms\n- arxiv: [http://arxiv.org/abs/1509.09308](http://arxiv.org/abs/1509.09308)\n- github: [https://github.com/andravin/wincnn](https://github.com/andravin/wincnn)\n- slides: [http://homes.cs.washington.edu/~cdel/presentations/Fast_Algorithms_for_Convolutional_Neural_Networks_Slides_reading_group_uw_delmundo_slides.pdf](http://homes.cs.washington.edu/~cdel/presentations/Fast_Algorithms_for_Convolutional_Neural_Networks_Slides_reading_group_uw_delmundo_slides.pdf)\n- discussion: [https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895](https://github.com/soumith/convnet-benchmarks/issues/59#issuecomment-150111895)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/?](https://www.reddit.com/r/MachineLearning/comments/3nocg5/fast_algorithms_for_convolutional_neural_networks/?)\n\n**Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units**\n\n- intro: Hong Kong Baptist University\n- arxiv: [https://arxiv.org/abs/1704.07724](https://arxiv.org/abs/1704.07724)\n\n**NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps**\n\n[https://arxiv.org/abs/1706.01406](https://arxiv.org/abs/1706.01406)\n\n**Channel Pruning for Accelerating Very Deep Neural Networks**\n\n- intro: ICCV 2017. Megvii Inc\n- arxiv: [https://arxiv.org/abs/1707.06168](https://arxiv.org/abs/1707.06168)\n- github: [https://github.com/yihui-he/channel-pruning](https://github.com/yihui-he/channel-pruning)\n\n**DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices**\n\n[https://arxiv.org/abs/1708.04728](https://arxiv.org/abs/1708.04728)\n\n**Learning Efficient Convolutional Networks through Network Slimming**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.06519](https://arxiv.org/abs/1708.06519)\n\n**SparCE: Sparsity aware General Purpose Core Extensions to Accelerate Deep Neural Networks**\n\n[https://arxiv.org/abs/1711.06315](https://arxiv.org/abs/1711.06315)\n\n**Accelerating Convolutional Neural Networks for Continuous Mobile Vision via Cache Reuse**\n\n- keywords: CNNCache\n- arxiv: [https://arxiv.org/abs/1712.01670](https://arxiv.org/abs/1712.01670)\n\n**Learning a Wavelet-like Auto-Encoder to Accelerate Deep Neural Networks**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1712.07493](https://arxiv.org/abs/1712.07493)\n\n**SBNet: Sparse Blocks Network for Fast Inference**\n\n- intro: Uber\n- project page: [https://eng.uber.com/sbnet/](https://eng.uber.com/sbnet/)\n- arxiv: [https://arxiv.org/abs/1801.02108](https://arxiv.org/abs/1801.02108)\n- github: [https://github.com/uber/sbnet](https://github.com/uber/sbnet)\n\n**Accelerating deep neural networks with tensor decompositions**\n\n- blog: [https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning](https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning)\n- github: [https://github.com/jacobgil/pytorch-tensor-decompositions](https://github.com/jacobgil/pytorch-tensor-decompositions)\n\n**A Survey on Acceleration of Deep Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1802.00939](https://arxiv.org/abs/1802.00939)\n\n**Recurrent Residual Module for Fast Inference in Videos**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1802.09723](https://arxiv.org/abs/1802.09723)\n\n**Co-Design of Deep Neural Nets and Neural Net Accelerators for Embedded Vision Applications**\n\n- intro: UC Berkeley & Samsung Research\n- arxiv: [https://arxiv.org/abs/1804.10642](https://arxiv.org/abs/1804.10642)\n\n**Towards Efficient Convolutional Neural Network for Domain-Specific Applications on FPGA**\n\n[https://arxiv.org/abs/1809.03318](https://arxiv.org/abs/1809.03318)\n\n**Accelerating Deep Neural Networks with Spatial Bottleneck Modules**\n\n[https://arxiv.org/abs/1809.02601](https://arxiv.org/abs/1809.02601)\n\n**FPGA Implementation of Convolutional Neural Networks with Fixed-Point Calculations**\n\n[https://arxiv.org/abs/1808.09945](https://arxiv.org/abs/1808.09945)\n\n**Extended Bit-Plane Compression for Convolutional Neural Network Accelerators**\n\n[https://arxiv.org/abs/1810.03979](https://arxiv.org/abs/1810.03979)\n\n**DAC: Data-free Automatic Acceleration of Convolutional Networks**\n\n- intro: WACV 2019\n- intro: Qualcomm AI Research & Lehigh University\n- arxiv: [https://arxiv.org/abs/1812.08374](https://arxiv.org/abs/1812.08374)\n\n**Learning Instance-wise Sparsity for Accelerating Deep Models**\n\n- intro: IJCAI 2019\n- arxiv: [https://arxiv.org/abs/1907.11840](https://arxiv.org/abs/1907.11840)\n\n# Code Optimization\n\n**Production Deep Learning with NVIDIA GPU Inference Engine**\n\n![](https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/06/GIE_GoogLeNet_top10kernels-1.png)\n\n- intro: convolution, bias, and ReLU layers are fused to form a single layer: CBR\n- blog: [https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/](https://devblogs.nvidia.com/parallelforall/production-deep-learning-nvidia-gpu-inference-engine/)\n\n**speed improvement by merging batch normalization and scale #5**\n\n- github issue: [https://github.com/sanghoon/pva-faster-rcnn/issues/5](https://github.com/sanghoon/pva-faster-rcnn/issues/5)\n\n**Add a tool to merge 'Conv-BN-Scale' into a single 'Conv' layer.**\n\n[https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c/](https://github.com/sanghoon/pva-faster-rcnn/commit/39570aab8c6513f0e76e5ab5dba8dfbf63e9c68c/)\n\n**Low-memory GEMM-based convolution algorithms for deep neural networks**\n\n[https://arxiv.org/abs/1709.03395](https://arxiv.org/abs/1709.03395)\n\n# Projects\n\n**Accelerate Convolutional Neural Networks**\n\n- intro: \"This tool aims to accelerate the test-time computation and decrease number of parameters of deep CNNs.\"\n- github: [https://github.com/dmlc/mxnet/tree/master/tools/accnn](https://github.com/dmlc/mxnet/tree/master/tools/accnn)\n\n## OptNet\n\n**OptNet - reducing memory usage in torch neural networks**\n\n- github: [https://github.com/fmassa/optimize-net](https://github.com/fmassa/optimize-net)\n\n**NNPACK: Acceleration package for neural networks on multi-core CPUs**\n\n![](https://camo.githubusercontent.com/376828536285f7a1a4f054aaae998e805023f489/68747470733a2f2f6d6172617479737a637a612e6769746875622e696f2f4e4e5041434b2f4e4e5041434b2e706e67)\n\n- github: [https://github.com/Maratyszcza/NNPACK](https://github.com/Maratyszcza/NNPACK)\n- comments(Yann LeCun): [https://www.facebook.com/yann.lecun/posts/10153459577707143](https://www.facebook.com/yann.lecun/posts/10153459577707143)\n\n**Deep Compression on AlexNet**\n\n- github: [https://github.com/songhan/Deep-Compression-AlexNet](https://github.com/songhan/Deep-Compression-AlexNet)\n\n**Tiny Darknet**\n\n- github: [http://pjreddie.com/darknet/tiny-darknet/](http://pjreddie.com/darknet/tiny-darknet/)\n\n**CACU: Calculate deep convolution neurAl network on Cell Unit**\n\n- github: [https://github.com/luhaofang/CACU](https://github.com/luhaofang/CACU)\n\n**keras_compressor: Model Compression CLI Tool for Keras**\n\n- blog: [https://nico-opendata.jp/ja/casestudy/model_compression/index.html](https://nico-opendata.jp/ja/casestudy/model_compression/index.html)\n- github: [https://github.com/nico-opendata/keras_compressor](https://github.com/nico-opendata/keras_compressor)\n\n# Blogs\n\n**Neural Networks Are Impressively Good At Compression**\n\n[https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/)\n\n**Mobile friendly deep convolutional neural networks**\n\n- part 1: [https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-1-331120ad40f9#.uy64ladvz](https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-1-331120ad40f9#.uy64ladvz)\n- part 2: [https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9#.u58fkuak3](https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9#.u58fkuak3)\n\n**Lab41 Reading Group: Deep Compression**\n\n- blog: [https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.hbqzn8wfu](https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209#.hbqzn8wfu)\n\n**Accelerating Machine Learning**\n\n![](http://www.linleygroup.com/mpr/h/2016/11561/U26_F4v2.png)\n\n- blog: [http://www.linleygroup.com/mpr/article.php?id=11561](http://www.linleygroup.com/mpr/article.php?id=11561)\n\n**Compressing and regularizing deep neural networks**\n\n[https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks](https://www.oreilly.com/ideas/compressing-and-regularizing-deep-neural-networks)\n\n**How fast is my model?**\n\n[http://machinethink.net/blog/how-fast-is-my-model/](http://machinethink.net/blog/how-fast-is-my-model/)\n\n# Talks / Videos\n\n**Deep compression and EIE: Deep learning model compression, design space exploration and hardware acceleration**\n\n- youtube: [https://www.youtube.com/watch?v=baZOmGSSUAg](https://www.youtube.com/watch?v=baZOmGSSUAg)\n\n**Deep Compression, DSD Training and EIE: Deep Neural Network Model Compression, Regularization and Hardware Acceleration**\n\n[http://research.microsoft.com/apps/video/default.aspx?id=266664](http://research.microsoft.com/apps/video/default.aspx?id=266664)\n\n**Tailoring Convolutional Neural Networks for Low-Cost, Low-Power Implementation**\n\n- intro: tutorial at the May 2015 Embedded Vision Summit\n- youtube: [https://www.youtube.com/watch?v=xACJBACStaU](https://www.youtube.com/watch?v=xACJBACStaU)\n\n# Resources\n\n**awesome-model-compression-and-acceleration**\n\n[https://github.com/sun254/awesome-model-compression-and-acceleration](https://github.com/sun254/awesome-model-compression-and-acceleration)\n\n**Embedded-Neural-Network**\n\n- intro: collection of works aiming at reducing model sizes or the ASIC/FPGA accelerator for machine learning\n- github: [https://github.com/ZhishengWang/Embedded-Neural-Network](https://github.com/ZhishengWang/Embedded-Neural-Network)\n","excerpt":"Papers High-Performance Neural Networks for Visual Object Classification intro: \"reduced network parameters by randomly removing connection","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations  Principles  The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]},{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between privacy and dignity.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"awesomeLists","url":"","items":[{"title":"Awesome Computer Vision: Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-computer-vision/","items":[]},{"title":"Awesome Natural Language Generation Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awsome-nl-gen/","items":[]},{"title":"Awesome Semantic Web Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-semweb/","items":[]},{"title":"Awesome-General","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-general/","items":[]}]},{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Pro Summary","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","title":"Knowledge Banking: A Technical Architecture Summary","lastUpdatedAt":"2022-12-28T20:36:06.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","title":"An Overview","lastUpdatedAt":"2022-12-28T20:26:34.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","title":"The design of new medium","lastUpdatedAt":"2022-12-28T20:26:34.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","title":"The need to modernise socioeconomic infrastructure","lastUpdatedAt":"2022-12-28T20:26:34.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/the-vision/","title":"The Vision","lastUpdatedAt":"2022-12-28T20:26:34.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awsome-nl-gen/","title":"Awesome Natural Language Generation Awesome","lastUpdatedAt":"2022-12-28T20:06:33.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-computer-vision/","title":"Awesome Computer Vision: Awesome","lastUpdatedAt":"2022-12-28T20:06:17.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","title":"Handong1587","lastUpdatedAt":"2022-12-28T20:06:17.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-general/","title":"Awesome-General","lastUpdatedAt":"2022-12-28T20:06:17.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-semweb/","title":"Awesome Semantic Web Awesome","lastUpdatedAt":"2022-12-28T20:06:17.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}