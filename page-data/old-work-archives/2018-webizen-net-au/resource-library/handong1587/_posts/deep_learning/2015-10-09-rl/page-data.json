{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/",
    "result": {"data":{"mdx":{"id":"393f151b-5f5a-55ed-beae-b89658ca41e9","tableOfContents":{"items":[{"url":"#tutorials","title":"Tutorials","items":[{"url":"#simple-reinforcement-learning-with-tensorflow","title":"Simple Reinforcement Learning with Tensorflow"}]},{"url":"#courses","title":"Courses"},{"url":"#papers","title":"Papers","items":[{"url":"#surveys","title":"Surveys"},{"url":"#playing-doom","title":"Playing Doom"}]},{"url":"#projects","title":"Projects","items":[{"url":"#autonomous-vehicle-navigation","title":"Autonomous vehicle navigation"},{"url":"#play-flappy-bird","title":"Play Flappy Bird"}]},{"url":"#pong","title":"Pong","items":[{"url":"#tips-and-tricks","title":"Tips and Tricks"}]},{"url":"#library","title":"Library"},{"url":"#blogs","title":"Blogs","items":[{"url":"#lets-make-a-dqn","title":"Letâ€™s make a DQN"}]},{"url":"#books","title":"Books"},{"url":"#resources","title":"Resources"},{"url":"#reading-and-questions","title":"Reading and Questions"}]},"fields":{"title":"Reinforcement Learning","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl.md","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"Reinforcement Learning","description":null,"imageAlt":null,"tags":[],"date":"2015-10-09T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"deep_learning\",\n  \"title\": \"Reinforcement Learning\",\n  \"date\": \"2015-10-09T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"tutorials\"\n  }, \"Tutorials\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Demystifying Deep Reinforcement Learning (Part1)\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/\"\n  }, \"http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning With Neon (Part2)\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/\"\n  }, \"http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: David Silver, Google DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf\"\n  }, \"http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1qWBOJGo\"\n  }, \"http://pan.baidu.com/s/1qWBOJGo\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro:  MLSS 2016. John Schulman\", \"[UC Berkeley]\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/index.html\"\n  }, \"http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1jIatusA#path=%252F\"\n  }, \"http://pan.baidu.com/s/1jIatusA#path=%252F\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning: Pong from Pixels\")), mdx(\"img\", {\n    \"src\": \"http://karpathy.github.io/assets/rl/preview.jpeg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Andrej Karpathy\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://karpathy.github.io/2016/05/31/rl/\"\n  }, \"http://karpathy.github.io/2016/05/31/rl/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gist: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\"\n  }, \"https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"instructor: David Silver. RLDM 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://videolectures.net/rldm2015_silver_reinforcement_learning/\"\n  }, \"http://videolectures.net/rldm2015_silver_reinforcement_learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: David Silver \", \"[Google DeepMind]\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://techtalks.tv/talks/deep-reinforcement-learning/62360/\"\n  }, \"http://techtalks.tv/talks/deep-reinforcement-learning/62360/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://hunch.net/~beygel/deep_rl_tutorial.pdf\"\n  }, \"http://hunch.net/~beygel/deep_rl_tutorial.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Nuts and Bolts of Deep RL Research\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016, John Schulman, OpenAI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf\"\n  }, \"http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1kVkBLkF\"\n  }, \"https://pan.baidu.com/s/1kVkBLkF\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ML Tutorial: Modern Reinforcement Learning and Video Games\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: by Marc Bellemare \", \"[DeepMind]\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=WuFMrk3ZbkE\"\n  }, \"https://www.youtube.com/watch?v=WuFMrk3ZbkE\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.bilibili.com/video/av17360035/\"\n  }, \"https://www.bilibili.com/video/av17360035/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement learning explained\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.oreilly.com/ideas/reinforcement-learning-explained\"\n  }, \"https://www.oreilly.com/ideas/reinforcement-learning-explained\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Beginner's guide to Reinforcement Learning & its implementation in Python\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/\"\n  }, \"https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning on the Web\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Andrej Karpathy\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://docs.google.com/presentation/d/1lcYrN56V2_SuX1rSmpzOUeMnheF6Jsu33-MsvLW9O_4/edit#slide=id.p\"\n  }, \"https://docs.google.com/presentation/d/1lcYrN56V2_SuX1rSmpzOUeMnheF6Jsu33-MsvLW9O_4/edit#slide=id.p\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://alpha.openai.com/ak_rework_2017.pdf\"\n  }, \"http://alpha.openai.com/ak_rework_2017.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Q Learning with Keras and Gym\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://keon.io/rl/deep-q-learning-with-keras-and-gym/\"\n  }, \"https://keon.io/rl/deep-q-learning-with-keras-and-gym/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/keon/deep-q-learning\"\n  }, \"https://github.com/keon/deep-q-learning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"\\u201CDeep Reinforcement Learning, Decision Making, and Control\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2017 Tutorial\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sites.google.com/view/icml17deeprl\"\n  }, \"https://sites.google.com/view/icml17deeprl\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Tour of Reinforcement Learning: The View from Continuous Control\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: by Benjamin Recht, UC Berkeley\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/Recht_ICML_Control-RL_tutorial.pdf\"\n  }, \"https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/Recht_ICML_Control-RL_tutorial.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Introduction to Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: McGill University & Google Brain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.12560\"\n  }, \"https://arxiv.org/abs/1811.12560\"))), mdx(\"h2\", {\n    \"id\": \"simple-reinforcement-learning-with-tensorflow\"\n  }, \"Simple Reinforcement Learning with Tensorflow\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Part 0: Q-Learning with Tables and Neural Networks\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.oo105wa2t\"\n  }, \"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.oo105wa2t\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Part 1 - Two-armed Bandit\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.tk89k51ob\"\n  }, \"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.tk89k51ob\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Part 2 - Policy-based Agents\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.n2wytg9q0\"\n  }, \"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.n2wytg9q0\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Part 3 - Model-Based RL\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.742i2yj6p\"\n  }, \"https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.742i2yj6p\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Part 4: Deep Q-Networks and Beyond\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.jox069crz\"\n  }, \"https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.jox069crz\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Part 5: Visualizing an Agent\\u2019s Thoughts and Actions\"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.pluh6cygm\"\n  }, \"https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.pluh6cygm\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Part 6: Partial Observability and Deep Recurrent Q-Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.3se46qkzy\"\n  }, \"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.3se46qkzy\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/awjuliani/35d2ab3409fc818011b6519f0f1629df\"\n  }, \"https://gist.github.com/awjuliani/35d2ab3409fc818011b6519f0f1629df\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Part 7: Action-Selection Strategies for Exploration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.8mcaa5nbe\"\n  }, \"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.8mcaa5nbe\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://awjuliani.github.io/exploration/index.html\"\n  }, \"https://awjuliani.github.io/exploration/index.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dissecting Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 1: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\"\n  }, \"https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 2: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html\"\n  }, \"https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 3: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html\"\n  }, \"https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mpatacchiola/dissecting-reinforcement-learning\"\n  }, \"https://github.com/mpatacchiola/dissecting-reinforcement-learning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"REINFORCE tutorial\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A small collection of code snippets and notes explaining the foundations of the REINFORCE algorithm.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mathias-madsen/reinforce_tutorial\"\n  }, \"https://github.com/mathias-madsen/reinforce_tutorial\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Q-Learning Recap\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://blog.davidqiu.com/Research/%5B%20Recap%20%5D%20Deep%20Q-Learning%20Recap/\"\n  }, \"http://blog.davidqiu.com/Research/%5B%20Recap%20%5D%20Deep%20Q-Learning%20Recap/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Introduction to Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Joelle Pineau \", \"[McGill University]\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/\"\n  }, \"http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://videolectures.net/site/normal_dl/tag=1051677/deeplearning2016_pineau_reinforcement_learning_01.pdf\"\n  }, \"http://videolectures.net/site/normal_dl/tag=1051677/deeplearning2016_pineau_reinforcement_learning_01.pdf\"))), mdx(\"h1\", {\n    \"id\": \"courses\"\n  }, \"Courses\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Advanced Topics: RL\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"UCL Course on RL\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"instructors: David Silver (Google DeepMind, AlphaGo)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\"\n  }, \"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\"\n  }, \"https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1bnWGuIz/\"\n  }, \"http://pan.baidu.com/s/1bnWGuIz/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"assignment: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf\"\n  }, \"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CS 294: Deep Reinforcement Learning, Fall 2017\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"instructor: Sergey Levine\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rll.berkeley.edu/deeprlcourse/\"\n  }, \"http://rll.berkeley.edu/deeprlcourse/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3\"\n  }, \"https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"bilibili: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.bilibili.com/video/av21501169/\"\n  }, \"https://www.bilibili.com/video/av21501169/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CS 294: Deep Reinforcement Learning, Spring 2017\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"course page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rll.berkeley.edu/deeprlcoursesp17/\"\n  }, \"http://rll.berkeley.edu/deeprlcoursesp17/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//txizzle/drl\"\n  }, \"https://github.com//txizzle/drl\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Berkeley CS 294: Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"instructors: John Schulman, Pieter Abbeel\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rll.berkeley.edu/deeprlcourse/\"\n  }, \"http://rll.berkeley.edu/deeprlcourse/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX\"\n  }, \"https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1hsQcm1Y\"\n  }, \"https://pan.baidu.com/s/1hsQcm1Y\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"(Udacity) Reinforcement Learning - Offered at Georgia Tech as CS 8803\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"instructor: Charles Isbell, Michael Littman\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.udacity.com/course/reinforcement-learning--ud600\"\n  }, \"https://www.udacity.com/course/reinforcement-learning--ud600\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://classroom.udacity.com/courses/ud820/lessons/684808907/concepts/6512308530923\"\n  }, \"https://classroom.udacity.com/courses/ud820/lessons/684808907/concepts/6512308530923\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CS229 Lecture notes Part XIII: Reinforcement Learning and Control\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Andrew Ng\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"lecture notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs229.stanford.edu/notes/cs229-notes12.pdf\"\n  }, \"http://cs229.stanford.edu/notes/cs229-notes12.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Practical_RL: A course in reinforcement learning in the wild\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yandexdataschool/Practical_RL\"\n  }, \"https://github.com/yandexdataschool/Practical_RL\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning (COMP-762) Winter 2017\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"course page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.mcgill.ca/~dprecup/courses/rl.html\"\n  }, \"http://www.cs.mcgill.ca/~dprecup/courses/rl.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"lectures: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html\"\n  }, \"http://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep RL Bootcamp - 26-27 August 2017 | Berkeley CA\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"lectures: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sites.google.com/view/deep-rl-bootcamp/lectures\"\n  }, \"https://sites.google.com/view/deep-rl-bootcamp/lectures\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.bilibili.com/video/av15568836/\"\n  }, \"https://www.bilibili.com/video/av15568836/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CMPUT 366: Intelligent Systems and CMPUT 609: Reinforcement Learning & Artificial Intelligence\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: by Rich Sutton, Adam White\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"lecture video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://drive.google.com/drive/folders/0B3w765rOKuKAMG9lbmRacFdsLWM?direction=a\"\n  }, \"https://drive.google.com/drive/folders/0B3w765rOKuKAMG9lbmRacFdsLWM?direction=a\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning and Control (Spring 2017, CMU 10703)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"instructors: Katerina Fragkiadaki, Ruslan Satakhutdinov\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://katefvision.github.io/\"\n  }, \"https://katefvision.github.io/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/playlist?list=PLpIxOj-HnDsNPFdu2UqCu2McJKHs-eWXv\"\n  }, \"https://www.youtube.com/playlist?list=PLpIxOj-HnDsNPFdu2UqCu2McJKHs-eWXv\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.bilibili.com/video/av18865689/\"\n  }, \"https://www.bilibili.com/video/av18865689/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Advanced Deep Learning & Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs\"\n  }, \"https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"bilibili: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.bilibili.com/video/av36621866/\"\n  }, \"https://www.bilibili.com/video/av36621866/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/RylanSchaeffer/ucl-adv-dl-rl\"\n  }, \"https://github.com/RylanSchaeffer/ucl-adv-dl-rl\"))), mdx(\"h1\", {\n    \"id\": \"papers\"\n  }, \"Papers\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Playing Atari with Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google DeepMind. NIPS Deep Learning Workshop 2013\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1312.5602\"\n  }, \"http://arxiv.org/abs/1312.5602\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kristjankorjus/Replicating-DeepMind\"\n  }, \"https://github.com/kristjankorjus/Replicating-DeepMind\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html\"\n  }, \"http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Kaixhin/Atari\"\n  }, \"https://github.com/Kaixhin/Atari\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gliese581gg/DQN_tensorflow\"\n  }, \"https://github.com/gliese581gg/DQN_tensorflow\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"summary: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md\"\n  }, \"https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2014\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: DQN, MCTS\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging\"\n  }, \"http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf\"\n  }, \"https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Replicating the Paper \\u201CPlaying Atari with Deep Reinforcement Learning\\u201D\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Tartu\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"technical report: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://courses.cs.ut.ee/MTAT.03.291/2014_spring/uploads/Main/Replicating%20DeepMind.pdf\"\n  }, \"https://courses.cs.ut.ee/MTAT.03.291/2014_spring/uploads/Main/Replicating%20DeepMind.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Tutorial for Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.mst.edu/~gosavia/tutorial.pdf\"\n  }, \"http://web.mst.edu/~gosavia/tutorial.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code(C): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.mst.edu/~gosavia/bookcodes.html\"\n  }, \"http://web.mst.edu/~gosavia/bookcodes.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code(Matlab): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.mst.edu/~gosavia/mrrl_website.html\"\n  }, \"http://web.mst.edu/~gosavia/mrrl_website.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1507.00814\"\n  }, \"http://arxiv.org/abs/1507.00814\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.evernote.com/shard/s189/sh/a4262b84-a322-4f77-9a76-569278be84af/b8c3e146a76ca3853f560bb03b60a481\"\n  }, \"https://www.evernote.com/shard/s189/sh/a4262b84-a322-4f77-9a76-569278be84af/b8c3e146a76ca3853f560bb03b60a481\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Massively Parallel Methods for Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2015. DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: DQN, Gorila\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1507.04296\"\n  }, \"https://arxiv.org/abs/1507.04296\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Action-Conditional Video Prediction using Deep Networks in Atari Games\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction\"\n  }, \"https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1507.08750\"\n  }, \"http://arxiv.org/abs/1507.08750\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/junhyukoh/nips2015-action-conditional-video-prediction\"\n  }, \"https://github.com/junhyukoh/nips2015-action-conditional-video-prediction\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://video.weibo.com/show?fid=1034:98062f3d83e41da6faa99cde5aa1ac97\"\n  }, \"http://video.weibo.com/show?fid=1034:98062f3d83e41da6faa99cde5aa1ac97\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Recurrent Q-Learning for Partially Observable MDPs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1507.06527\"\n  }, \"https://arxiv.org/abs/1507.06527\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Continuous control with deep reinforcement learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.02971\"\n  }, \"http://arxiv.org/abs/1509.02971\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/iassael/torch-policy-gradient\"\n  }, \"https://github.com/iassael/torch-policy-gradient\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/stevenpjg/ddpg-aigym\"\n  }, \"https://github.com/stevenpjg/ddpg-aigym\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow + OpenAI Gym): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/SimonRamstedt/ddpg\"\n  }, \"https://github.com/SimonRamstedt/ddpg\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Benchmarking for Bayesian Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.04064\"\n  }, \"http://arxiv.org/abs/1509.04064\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mcastron/BBRL/\"\n  }, \"https://github.com/mcastron/BBRL/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"reading: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blogs.ulg.ac.be/damien-ernst/benchmarking-for-bayesian-reinforcement-learning/\"\n  }, \"http://blogs.ulg.ac.be/damien-ernst/benchmarking-for-bayesian-reinforcement-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning with Double Q-learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1509.06461\"\n  }, \"https://arxiv.org/abs/1509.06461\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Giraffe: Using Deep Reinforcement Learning to Play Chess\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.01549\"\n  }, \"http://arxiv.org/abs/1509.01549\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Human-level control through deep reinforcement learning\")), mdx(\"img\", {\n    \"src\": \"/assets/reinforcement-learning-materials/DeepMind_Atari_Deep_Q_Learner-breakout.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google DeepMind. 2015 Nature\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D\"\n  }, \"http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\"\n  }, \"http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Lua/Torch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deepmind/dqn\"\n  }, \"https://github.com/deepmind/dqn\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1kTiwzOF\"\n  }, \"http://pan.baidu.com/s/1kTiwzOF\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sites.google.com/a/deepmind.com/dqn/\"\n  }, \"https://sites.google.com/a/deepmind.com/dqn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=V2wzkPmiB_A\"\n  }, \"https://www.youtube.com/watch?v=V2wzkPmiB_A\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner\"\n  }, \"https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tambetm/simple_dqn\"\n  }, \"https://github.com/tambetm/simple_dqn\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/devsisters/DQN-tensorflow\"\n  }, \"https://github.com/devsisters/DQN-tensorflow\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"reddit: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.reddit.com/r/MachineLearning/comments/2x4yy1/google_deepmind_nature_paper_humanlevel_control\"\n  }, \"https://www.reddit.com/r/MachineLearning/comments/2x4yy1/google_deepmind_nature_paper_humanlevel_control\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1510.02173\"\n  }, \"http://arxiv.org/abs/1510.02173\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.08731\"\n  }, \"http://arxiv.org/abs/1509.08731\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889\"\n  }, \"https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06342\"\n  }, \"http://arxiv.org/abs/1511.06342\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/eparisotto/ActorMimic\"\n  }, \"https://github.com/eparisotto/ActorMimic\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MazeBase: A Sandbox for Learning from Games\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: New York University & Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.07401\"\n  }, \"http://arxiv.org/abs/1511.07401\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Simple Algorithms from Examples\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: New York University & Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.07275\"\n  }, \"http://arxiv.org/abs/1511.07275\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wojzaremba/algorithm-learning\"\n  }, \"https://github.com/wojzaremba/algorithm-learning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Algorithms from Data\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"PhD thesis: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.nyu.edu/media/publications/zaremba_wojciech.pdf\"\n  }, \"http://www.cs.nyu.edu/media/publications/zaremba_wojciech.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wojzaremba/algorithm-learning\"\n  }, \"https://github.com/wojzaremba/algorithm-learning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multiagent Cooperation and Competition with Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.08779\"\n  }, \"http://arxiv.org/abs/1511.08779\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player\"\n  }, \"https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Active Object Localization with Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06015\"\n  }, \"http://arxiv.org/abs/1511.06015\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.01124\"\n  }, \"http://arxiv.org/abs/1512.01124\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.02011\"\n  }, \"http://arxiv.org/abs/1512.02011\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"State of the Art Control of Atari Games Using Shallow Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.01563\"\n  }, \"http://arxiv.org/abs/1512.01563\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Angrier Birds: Bayesian reinforcement learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1601.01297\"\n  }, \"http://arxiv.org/abs/1601.01297\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/imanolarrieta/angrybirds\"\n  }, \"https://github.com/imanolarrieta/angrybirds\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gitxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://gitxiv.com/posts/Nr2N7j4YrR4gnCYK9/angrier-birds-bayesian-reinforcement-learning\"\n  }, \"http://gitxiv.com/posts/Nr2N7j4YrR4gnCYK9/angrier-birds-bayesian-reinforcement-learning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Prioritized Experience Replay\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.05952\"\n  }, \"http://arxiv.org/abs/1511.05952\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dueling Network Architectures for Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2016 best paper\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06581\"\n  }, \"http://arxiv.org/abs/1511.06581\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://hadovanhasselt.wordpress.com/2016/06/20/best-paper-at-icml-dueling-network-architectures-for-deep-reinforcement-learning/\"\n  }, \"https://hadovanhasselt.wordpress.com/2016/06/20/best-paper-at-icml-dueling-network-architectures-for-deep-reinforcement-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Asynchronous Methods for Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.01783\"\n  }, \"http://arxiv.org/abs/1602.01783\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/traai/async-deep-rl\"\n  }, \"https://github.com/traai/async-deep-rl\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow+Keras+OpenAI Gym): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/coreylynch/async-rl\"\n  }, \"https://github.com/coreylynch/async-rl\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/devsisters/async-rl-tensorflow\"\n  }, \"https://github.com/devsisters/async-rl-tensorflow\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(PyTorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ikostrikov/pytorch-a3c\"\n  }, \"https://github.com/ikostrikov/pytorch-a3c\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/\"\n  }, \"https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Graying the black box: Understanding DQNs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02658\"\n  }, \"http://arxiv.org/abs/1602.02658\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02672\"\n  }, \"http://arxiv.org/abs/1602.02672\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Value Iteration Networks\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%204.58.42%20PM.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016, Best Paper Award. University of California, Berkeley\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02867\"\n  }, \"http://arxiv.org/abs/1602.02867\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, Theano): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/avivt/VIN\"\n  }, \"https://github.com/avivt/VIN\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks\"\n  }, \"https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/onlytailei/PyTorch-value-iteration-networks\"\n  }, \"https://github.com/onlytailei/PyTorch-value-iteration-networks\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kentsommer/pytorch-value-iteration-networks\"\n  }, \"https://github.com/kentsommer/pytorch-value-iteration-networks\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neka-nat/vin-keras\"\n  }, \"https://github.com/neka-nat/vin-keras\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes(by Andrej Karpathy): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/karpathy/paper-notes/blob/master/vin.md\"\n  }, \"https://github.com/karpathy/paper-notes/blob/master/vin.md\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Insights in Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MSc thesis\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1bn51BYJ\"\n  }, \"http://pan.baidu.com/s/1bn51BYJ\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Using Deep Q-Learning to Control Optimization Hyperparameters\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.04062\"\n  }, \"http://arxiv.org/abs/1602.04062\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Continuous Deep Q-Learning with Model-based Acceleration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.00748\"\n  }, \"http://arxiv.org/abs/1603.00748\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning from Self-Play in Imperfect-Information Games\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.01121\"\n  }, \"http://arxiv.org/abs/1603.01121\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1604.06057\"\n  }, \"https://arxiv.org/abs/1604.06057\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/EthanMacdonald/h-DQN\"\n  }, \"https://github.com/EthanMacdonald/h-DQN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Benchmarking Deep Reinforcement Learning for Continuous Control\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.06778\"\n  }, \"http://arxiv.org/abs/1604.06778\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/rllab/rllab\"\n  }, \"https://github.com/rllab/rllab\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"doc: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://rllab.readthedocs.org/en/latest/\"\n  }, \"https://rllab.readthedocs.org/en/latest/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning\")), mdx(\"img\", {\n    \"src\": \"http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/dog_teaser.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html\"\n  }, \"http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf\"\n  }, \"http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/xbpeng/DeepTerrainRL\"\n  }, \"https://github.com/xbpeng/DeepTerrainRL\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.05359\"\n  }, \"http://arxiv.org/abs/1605.05359\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Successor Reinforcement Learning (MIT)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.02396\"\n  }, \"http://arxiv.org/abs/1606.02396\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Ardavans/DSR\"\n  }, \"https://github.com/Ardavans/DSR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Communicate with Deep Multi-Agent Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1605.06676\"\n  }, \"https://arxiv.org/abs/1605.06676\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/iassael/learning-to-communicate\"\n  }, \"https://github.com/iassael/learning-to-communicate\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning with Regularized Convolutional Neural Fitted Q Iteration\"), \"\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RC-NFQ: Regularized Convolutional Neural Fitted Q Iteration\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A batch algorithm for deep reinforcement learning.\\nIncorporates dropout regularization and convolutional neural networks with a separate target Q network.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://machineintelligence.org/papers/rc-nfq.pdf\"\n  }, \"http://machineintelligence.org/papers/rc-nfq.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cosmoharrigan/rc-nfq\"\n  }, \"https://github.com/cosmoharrigan/rc-nfq\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.02993\"\n  }, \"http://arxiv.org/abs/1609.02993\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bayesian Reinforcement Learning: A Survey\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.04436\"\n  }, \"http://arxiv.org/abs/1609.04436\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Playing FPS Games with Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.05521\"\n  }, \"http://arxiv.org/abs/1609.05521\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT\"\n  }, \"https://www.youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/\"\n  }, \"https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning with Stochastic Initial States\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Washington & UC Berkeley\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.01112\"\n  }, \"https://arxiv.org/abs/1610.01112\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Utilization of Deep Reinforcement Learning for saccadic-based object visual search\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.06492\"\n  }, \"https://arxiv.org/abs/1610.06492\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Navigate in Complex Environments\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.03673\"\n  }, \"https://arxiv.org/abs/1611.03673\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deepmind/lab\"\n  }, \"https://github.com/deepmind/lab\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=lNoaTyMZsWI\"\n  }, \"https://www.youtube.com/watch?v=lNoaTyMZsWI\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning with Unsupervised Auxiliary Tasks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DeepMind. ICLR 2017 oral\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05397\"\n  }, \"https://arxiv.org/abs/1611.05397\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to reinforcement learn\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05763\"\n  }, \"https://arxiv.org/abs/1611.05763\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Graduate Training Center of Neuroscience & MSR\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.07078\"\n  }, \"https://arxiv.org/abs/1611.07078\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Exploration for Multi-task Reinforcement Learning with Deep Generative Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS Deep Reinforcement Learning Workshop 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.09894\"\n  }, \"https://arxiv.org/abs/1611.09894\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Combinatorial Optimization with Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Brain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: traveling salesman problem (TSP)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.09940\"\n  }, \"https://arxiv.org/abs/1611.09940\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Loss is its own Reward: Self-Supervision for Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.07307\"\n  }, \"https://arxiv.org/abs/1612.07307\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning Using Quantum Boltzmann Machines\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 1QB Information Technologies (1QBit)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.05695\"\n  }, \"https://arxiv.org/abs/1612.05695\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning applied to the game Bubble Shooter\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"bachelor thesis: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://staff.fnwi.uva.nl/b.bredeweg/pdf/BSc/20152016/Samson.pdf\"\n  }, \"https://staff.fnwi.uva.nl/b.bredeweg/pdf/BSc/20152016/Samson.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/laurenssam/AlphaBubble\"\n  }, \"https://github.com/laurenssam/AlphaBubble\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=DPAKFenNgbs\"\n  }, \"https://www.youtube.com/watch?v=DPAKFenNgbs\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning: An Overview\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.07274\"\n  }, \"https://arxiv.org/abs/1701.07274\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Robust Adversarial Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CMU & Google Brain & Google Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.02702\"\n  }, \"https://arxiv.org/abs/1703.02702\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Beating Atari with Natural Language Guided Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.05539\"\n  }, \"https://arxiv.org/abs/1704.05539\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Imperial College London\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.06769\"\n  }, \"https://arxiv.org/abs/1705.06769\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Nat-D/FeatureControlHRL\"\n  }, \"https://github.com/Nat-D/FeatureControlHRL\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distral: Robust Multitask Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Distill, transfer learning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.04175\"\n  }, \"https://arxiv.org/abs/1707.04175\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning: Framework, Applications, and Embedded Implementations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Syracuse University & University of California, Riverside\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.03792\"\n  }, \"https://arxiv.org/abs/1710.03792\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Robust Deep Reinforcement Learning with Adversarial Attacks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.03632\"\n  }, \"https://arxiv.org/abs/1712.03632\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Variational Deep Q Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Second workshop on Bayesian Deep Learning (NIPS 2017). Columbia University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.11225\"\n  }, \"https://arxiv.org/abs/1711.11225\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On Monte Carlo Tree Search and Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.jair.org/media/5507/live-5507-10333-jair.pdf\"\n  }, \"https://www.jair.org/media/5507/live-5507-10333-jair.pdf\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: deepsense.ai & Intel & Polish Academy of Sciences\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.02852\"\n  }, \"https://arxiv.org/abs/1801.02852\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gihtub: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//anonymous-author1/DDRL\"\n  }, \"https://github.com//anonymous-author1/DDRL\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GAN Q-learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.04874\"\n  }, \"https://arxiv.org/abs/1805.04874\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Visual Geometry Group, University of Oxford & Element AI & Polytechnique Montreal, Mila & Canada CIFAR AI Chair\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.01318\"\n  }, \"https://arxiv.org/abs/1904.01318\"))), mdx(\"h2\", {\n    \"id\": \"surveys\"\n  }, \"Surveys\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning: A Survey\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: JAIR 1996\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/rl-survey.html\"\n  }, \"http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/rl-survey.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/cs/9605103\"\n  }, \"http://arxiv.org/abs/cs/9605103\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Brief Survey of Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IEEE Signal Processing Magazine, Special Issue on Deep Learning for Image Understanding\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Imperial College London & Arizona State University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.05866\"\n  }, \"https://arxiv.org/abs/1708.05866\"))), mdx(\"h2\", {\n    \"id\": \"playing-doom\"\n  }, \"Playing Doom\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning\")), mdx(\"img\", {\n    \"src\": \"http://vizdoom.cs.put.edu.pl/user/pages/01.home/depthbuffer.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.02097\"\n  }, \"http://arxiv.org/abs/1605.02097\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Marqt/ViZDoom\"\n  }, \"https://github.com/Marqt/ViZDoom\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vizdoom.cs.put.edu.pl/\"\n  }, \"http://vizdoom.cs.put.edu.pl/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"tutorial: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vizdoom.cs.put.edu.pl/tutorial\"\n  }, \"http://vizdoom.cs.put.edu.pl/tutorial\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning From Raw Pixels in Doom\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Bachelor's thesis\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.02164\"\n  }, \"https://arxiv.org/abs/1610.02164\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Playing Doom with SLAM-Augmented Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Oxford\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.00380\"\n  }, \"https://arxiv.org/abs/1612.00380\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning via Recurrent Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.02392\"\n  }, \"https://arxiv.org/abs/1701.02392\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tanmayshankar/RCNN_MDP\"\n  }, \"https://github.com/tanmayshankar/RCNN_MDP\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Shallow Updates for Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The Technion & UC Berkeley\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.07461\"\n  }, \"https://arxiv.org/abs/1705.07461\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Shallow-Updates-for-Deep-RL/Shallow_Updates_for_Deep_RL\"\n  }, \"https://github.com/Shallow-Updates-for-Deep-RL/Shallow_Updates_for_Deep_RL\"))), mdx(\"h1\", {\n    \"id\": \"projects\"\n  }, \"Projects\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TorchQLearning\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/SeanNaren/TorchQLearningExample/master/images/torchplayscatch.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/SeanNaren/TorchQLearningExample\"\n  }, \"https://github.com/SeanNaren/TorchQLearningExample\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"General_Deep_Q_RL: General deep Q learning framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/VinF/General_Deep_Q_RL\"\n  }, \"https://github.com/VinF/General_Deep_Q_RL\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"wiki: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/VinF/General_Deep_Q_RL/wiki\"\n  }, \"https://github.com/VinF/General_Deep_Q_RL/wiki\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Snake: Toy example of deep reinforcement model playing the game of snake\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/bitwise-ben/Snake/master/images/snake.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bitwise-ben/Snake\"\n  }, \"https://github.com/bitwise-ben/Snake\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Using Deep Q Networks to Learn Video Game Strategies\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/asrivat1/DeepLearningVideoGames\"\n  }, \"https://github.com/asrivat1/DeepLearningVideoGames\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"qlearning4k: Q-learning for Keras\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/farizrahman4u/qlearning4k/master/gifs/catch.gif\",\n    \"alt\": null\n  }), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/farizrahman4u/qlearning4k/master/gifs/snake.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"Qlearning4k is a reinforcement learning add-on for the python deep learning library Keras.\\nIts simple, and is ideal for rapid prototyping.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/farizrahman4u/qlearning4k\"\n  }, \"https://github.com/farizrahman4u/qlearning4k\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"rlenvs: Reinforcement learning environments for Torch7, inspired by RL-Glue\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Kaixhin/rlenvs\"\n  }, \"https://github.com/Kaixhin/rlenvs\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"deep_rl_ale: An implementation of Deep Reinforcement Learning / Deep Q-Networks for Atari games in TensorFlow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Jabberwockyll/deep_rl_ale\"\n  }, \"https://github.com/Jabberwockyll/deep_rl_ale\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Chimp: General purpose framework for deep reinforcement learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/sisl/Chimp\"\n  }, \"https://github.com/sisl/Chimp\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Q Learning for ATARI using Tensorflow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mrkulk/deepQN_tensorflow\"\n  }, \"https://github.com/mrkulk/deepQN_tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepQLearning: A powerful machine learning algorithm utilizing Q-Learning and Neural Networks, implemented using Torch and Lua.\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/blakeMilner/DeepQLearning\"\n  }, \"https://github.com/blakeMilner/DeepQLearning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"OpenAI Gym: A toolkit for developing and comparing reinforcement learning algorithms\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gym.openai.com/\"\n  }, \"https://gym.openai.com/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/openai/gym\"\n  }, \"https://github.com/openai/gym\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeeR: DEEp Reinforcement learning framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/VinF/deer/\"\n  }, \"https://github.com/VinF/deer/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"docs: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://deer.readthedocs.io/en/latest/\"\n  }, \"http://deer.readthedocs.io/en/latest/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"KeRLym: A Deep Reinforcement Learning Toolbox in Keras\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/osh/kerlym/master/examples/example.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://oshearesearch.com/index.php/2016/06/14/kerlym-a-deep-reinforcement-learning-toolbox-in-keras/\"\n  }, \"https://oshearesearch.com/index.php/2016/06/14/kerlym-a-deep-reinforcement-learning-toolbox-in-keras/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/osh/kerlym\"\n  }, \"https://github.com/osh/kerlym\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pack of Drones: Layered reinforcement learning for complex behaviors\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MickyDowns/deep-theano-rnn-lstm-car\"\n  }, \"https://github.com/MickyDowns/deep-theano-rnn-lstm-car\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=WrLRGzbfeZc\"\n  }, \"https://www.youtube.com/watch?v=WrLRGzbfeZc\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RL Helicopter Game: Q-Learning and DQN Reinforcement Learning to play the Helicopter Game - Keras based!\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://dandxy89.github.io/rf_helicopter/\"\n  }, \"http://dandxy89.github.io/rf_helicopter/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/dandxy89/rf_helicopter\"\n  }, \"https://github.com/dandxy89/rf_helicopter\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Playing Mario with Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/aleju/mario-ai\"\n  }, \"https://github.com/aleju/mario-ai\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Attention Recurrent Q-Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Deep Reinforcement Learning Workshop, NIPS 2015. DeepHack Game\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1512.01693\"\n  }, \"https://arxiv.org/abs/1512.01693\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/5vision/DARQN\"\n  }, \"https://github.com/5vision/DARQN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning in TensorFlow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: TensorFlow implementation of Deep Reinforcement Learning papers\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/carpedm20/deep-rl-tensorflow\"\n  }, \"https://github.com/carpedm20/deep-rl-tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"rltorch: A RL package for Torch that can also be used with openai gym\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ludc/rltorch\"\n  }, \"https://github.com/ludc/rltorch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"deep_q_rl: Theano-based implementation of Deep Q-learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/spragunr/deep_q_rl\"\n  }, \"https://github.com/spragunr/deep_q_rl\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement-trading\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: This project uses reinforcement learning on stock market and agent tries to learn trading.\\nThe goal is to check if the agent can learn to read tape.\\nThe project is dedicated to hero in life great Jesse Livermore.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deependersingla/deep_trader\"\n  }, \"https://github.com/deependersingla/deep_trader\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"dist-dqn\\uFF1ADistributed Reinforcement Learning using Deep Q-Network in TensorFlow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/viswanathgs/dist-dqn\"\n  }, \"https://github.com/viswanathgs/dist-dqn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning for Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/matthiasplappert/keras-rl\"\n  }, \"https://github.com/matthiasplappert/keras-rl\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RL4J: Reinforcement Learning for the JVM\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Reinforcement learning framework integrated with deeplearning4j.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deeplearning4j/rl4j\"\n  }, \"https://github.com/deeplearning4j/rl4j\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Teaching Your Computer To Play Super Mario Bros. \\u2013 A Fork of the Google DeepMind Atari Machine Learning Project\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/\"\n  }, \"http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ehrenbrav/DeepQNetwork\"\n  }, \"https://github.com/ehrenbrav/DeepQNetwork\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"dprl: Deep reinforcement learning package for torch7\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/PoHsunSu/dprl\"\n  }, \"https://github.com/PoHsunSu/dprl\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning for Torch: Introducing torch-twrl\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl\"\n  }, \"https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/twitter/torch-twrl\"\n  }, \"https://github.com/twitter/torch-twrl\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Alpha Toe - Using Deep learning to master Tic-Tac-Toe - Daniel Slater\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.danielslater.net/2016/10/alphatoe.html\"\n  }, \"http://www.danielslater.net/2016/10/alphatoe.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=Meb5hApAnj4\"\n  }, \"https://www.youtube.com/watch?v=Meb5hApAnj4\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/DanielSlater/AlphaToe\"\n  }, \"https://github.com/DanielSlater/AlphaToe\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tensorflow-Reinforce: Implementation of Reinforcement Learning Models in Tensorflow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yukezhu/tensorflow-reinforce\"\n  }, \"https://github.com/yukezhu/tensorflow-reinforce\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"deep RL hacking on minecraft with malmo\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/matpalm/malmomo\"\n  }, \"https://github.com/matpalm/malmomo\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ReinforcementLearning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MC control, Q-learning, SARSA, Cross Entropy Method\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/janivanecky/ReinforcementLearning\"\n  }, \"https://github.com/janivanecky/ReinforcementLearning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"markovjs: Reinforcement Learning in JavaScript\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/lsunsi/markovjs\"\n  }, \"https://github.com/lsunsi/markovjs\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Q: Deep reinforcement learning with TensorFlow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tobegit3hub/deep_q\"\n  }, \"https://github.com/tobegit3hub/deep_q\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Q-Learning Network in pytorch\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/transedward/pytorch-dqn\"\n  }, \"https://github.com/transedward/pytorch-dqn\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tensorflow-RL: Implementations of deep RL papers and random experimentation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/steveKapturowski/tensorflow-rl\"\n  }, \"https://github.com/steveKapturowski/tensorflow-rl\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Minimal and Clean Reinforcement Learning Examples\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/rlcode/reinforcement-learning\"\n  }, \"https://github.com/rlcode/reinforcement-learning\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepRL: Highly modularized implementation of popular deep RL algorithms by PyTorch\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/ShangtongZhang/DeepRL\"\n  }, \"https://github.com/ShangtongZhang/DeepRL\")), mdx(\"h2\", {\n    \"id\": \"autonomous-vehicle-navigation\"\n  }, \"Autonomous vehicle navigation\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Self-Driving-Car-AI\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A simple self-driving car AI python script using the deep Q-learning algorithm\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//JianyangZhang/Self-Driving-Car-AI\"\n  }, \"https://github.com//JianyangZhang/Self-Driving-Car-AI\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Autonomous vehicle navigation based on Deep Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com//kaihuchen/DRL-AutonomousVehicles\"\n  }, \"https://github.com//kaihuchen/DRL-AutonomousVehicles\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Car Racing using Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf\"\n  }, \"https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf\"))), mdx(\"h2\", {\n    \"id\": \"play-flappy-bird\"\n  }, \"Play Flappy Bird\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Using Deep Q-Network to Learn How To Play Flappy Bird\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yenchenlin/DeepLearningFlappyBird\"\n  }, \"https://github.com/yenchenlin/DeepLearningFlappyBird\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Playing Flappy Bird Using Deep Reinforcement Learning (Based on Deep Q Learning DQN using Tensorflow)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.csdn.net/songrotek/article/details/50951537\"\n  }, \"http://blog.csdn.net/songrotek/article/details/50951537\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/songrotek/DRL-FlappyBird\"\n  }, \"https://github.com/songrotek/DRL-FlappyBird\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Playing Flappy Bird Using Deep Reinforcement Learning (Based on Deep Q Learning DQN)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/li-haoran/DRL-FlappyBird\"\n  }, \"https://github.com/li-haoran/DRL-FlappyBird\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MXNET-Scala Playing Flappy Bird Using Deep Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/DRLFlappyBird\"\n  }, \"https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/DRLFlappyBird\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Flappy Bird Bot using Reinforcement Learning in Python\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/chncyhn/flappybird-qlearning-bot\"\n  }, \"https://github.com/chncyhn/flappybird-qlearning-bot\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Using Keras and Deep Q-Network to Play FlappyBird\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html\"\n  }, \"https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yanpanlau/Keras-FlappyBird\"\n  }, \"https://github.com/yanpanlau/Keras-FlappyBird\"))), mdx(\"h1\", {\n    \"id\": \"pong\"\n  }, \"Pong\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Building a Pong playing AI in just 1 hour(plus 4 days training...)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"sildes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://speakerdeck.com/danielslater/building-a-pong-ai\"\n  }, \"https://speakerdeck.com/danielslater/building-a-pong-ai\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/DanielSlater/PyDataLondon2016\"\n  }, \"https://github.com/DanielSlater/PyDataLondon2016\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=n8NdT_3y9oY\"\n  }, \"https://www.youtube.com/watch?v=n8NdT_3y9oY\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pong Neural Network(LIVE)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=Hqf__FlRlzg\"\n  }, \"https://www.youtube.com/watch?v=Hqf__FlRlzg\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/llSourcell/pong_neural_network_live\"\n  }, \"https://github.com/llSourcell/pong_neural_network_live\"))), mdx(\"h2\", {\n    \"id\": \"tips-and-tricks\"\n  }, \"Tips and Tricks\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepRLHacks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The Nuts and Bolts of Deep RL Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/williamFalcon/DeepRLHacks\"\n  }, \"https://github.com/williamFalcon/DeepRLHacks\"))), mdx(\"h1\", {\n    \"id\": \"library\"\n  }, \"Library\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BURLAP: Brown-UMBC Reinforcement Learning and Planning (BURLAP) java code library\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: for the use and development of single or multi-agent planning and learning algorithms and domains to accompany them\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://burlap.cs.brown.edu/\"\n  }, \"http://burlap.cs.brown.edu/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AgentNet: Deep Reinforcement Learning library for humans\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/5593b2c8184c4bc08372f919063e826d9bcc2c67/687474703a2f2f7333332e706f7374696d672e6f72672f79747836336b7763762f7768617469735f6167656e746e65745f706e672e706e67\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: A lightweight library to build and train deep reinforcement learning and custom recurrent networks using Theano+Lasagne \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yandexdataschool/AgentNet\"\n  }, \"https://github.com/yandexdataschool/AgentNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Atari Multitask & Transfer Learning Benchmark (AMTLB)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Atari gauntlet for RL agents\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ai-on.org/projects/multitask-and-transfer-learning.html\"\n  }, \"http://ai-on.org/projects/multitask-and-transfer-learning.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deontologician/atari_multitask\"\n  }, \"https://github.com/deontologician/atari_multitask\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Coach: a python reinforcement learning research framework containing implementation of many state-of-the-art algorithms\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Reinforcement Learning Coach by Intel\\xAE Nervana\\u2122 enables easy experimentation with state of the art Reinforcement Learning algorithms\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://coach.nervanasys.com/\"\n  }, \"http://coach.nervanasys.com/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/NervanaSystems/coach\"\n  }, \"https://github.com/NervanaSystems/coach\"))), mdx(\"h1\", {\n    \"id\": \"blogs\"\n  }, \"Blogs\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement learning\\u2019s foundational flaw\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://thegradient.pub/why-rl-is-flawed/\"\n  }, \"https://thegradient.pub/why-rl-is-flawed/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Short Introduction To Some Reinforcement Learning Algorithms\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://webdocs.cs.ualberta.ca/~vanhasse/rl_algs/rl_algs.html\"\n  }, \"http://webdocs.cs.ualberta.ca/~vanhasse/rl_algs/rl_algs.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Painless Q-Learning Tutorial\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://mnemstudio.org/path-finding-q-learning-tutorial.htm\"\n  }, \"http://mnemstudio.org/path-finding-q-learning-tutorial.htm\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning - Part 1\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://outlace.com/Reinforcement-Learning-Part-1/\"\n  }, \"http://outlace.com/Reinforcement-Learning-Part-1/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning - Monte Carlo Methods\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://outlace.com/Reinforcement-Learning-Part-2/\"\n  }, \"http://outlace.com/Reinforcement-Learning-Part-2/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Q-learning with Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://outlace.com/Reinforcement-Learning-Part-3/\"\n  }, \"http://outlace.com/Reinforcement-Learning-Part-3/\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Guest Post (Part I): Demystifying Deep Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.nervanasys.com/demystifying-deep-reinforcement-learning/\"\n  }, \"http://www.nervanasys.com/demystifying-deep-reinforcement-learning/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Using reinforcement learning in Python to teach a virtual car to avoid obstacles: An experiment in Q-learning, neural networks and Pygame.\")), mdx(\"img\", {\n    \"src\": \"https://cdn-images-1.medium.com/max/800/1*Ht0KPSlYVsLUp-wqr-ab7Q.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.p8ug6snri\"\n  }, \"https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.p8ug6snri\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/harvitronix/reinforcement-learning-car\"\n  }, \"https://github.com/harvitronix/reinforcement-learning-car\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement learning in Python to teach a virtual car to avoid obstacles\\u200A\\u2014\\u200Apart 2\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.i0o643m1h\"\n  }, \"https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.i0o643m1h\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Some Reinforcement Learning Algorithms in Python, C++\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"pan: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1mhcYf3M#path=%252FImplementations%2520of%2520Some%2520Reinforcement%2520Learning%2520Algorithms\"\n  }, \"http://pan.baidu.com/s/1mhcYf3M#path=%252FImplementations%2520of%2520Some%2520Reinforcement%2520Learning%2520Algorithms\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"learning to do laps with reinforcement learning and neural nets\")), mdx(\"img\", {\n    \"src\": \"http://matpalm.com/blog/imgs/2016/drivebot/walk.gif\",\n    \"alt\": null\n  }), mdx(\"img\", {\n    \"src\": \"http://matpalm.com/blog/imgs/2016/drivebot/runt.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://matpalm.com/blog/drivebot/\"\n  }, \"http://matpalm.com/blog/drivebot/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/matpalm/drivebot\"\n  }, \"https://github.com/matpalm/drivebot\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Get a taste of reinforcement learning\\u200A\\u2014\\u200Aimplement a tic tac toe agent\")), mdx(\"img\", {\n    \"src\": \"https://cdn-images-1.medium.com/max/800/1*Ntrov1dzaerfesi9vRKdow.gif\",\n    \"alt\": null\n  }), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4#.59bx71a2h\"\n  }, \"https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4#.59bx71a2h\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Best reinforcement learning libraries?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"reddit: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/\"\n  }, \"https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Super Simple Reinforcement Learning Tutorial\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 1: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149\"\n  }, \"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 2: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.dyhxww1u6\"\n  }, \"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.dyhxww1u6\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part 3: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.r4c7i7tjq\"\n  }, \"https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.r4c7i7tjq\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gist: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/awjuliani/16608e1c4968baaa692b9b8c7dd94d04\"\n  }, \"https://gist.github.com/awjuliani/16608e1c4968baaa692b9b8c7dd94d04\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning in Python\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/NathanEpstein/pydata-reinforce\"\n  }, \"https://github.com/NathanEpstein/pydata-reinforce\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Skynet Salesman\")), mdx(\"img\", {\n    \"src\": \"http://multithreaded.stitchfix.com/assets/posts/2016-07-18-skynet-salesman/np4_5_w.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keyworkds: traveling salesman problem (TSP), deep Q learning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://multithreaded.stitchfix.com/blog/2016/07/21/skynet-salesman/\"\n  }, \"http://multithreaded.stitchfix.com/blog/2016/07/21/skynet-salesman/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jn2clark/ReinforcementLearning/tree/master/DeepQ\"\n  }, \"https://github.com/jn2clark/ReinforcementLearning/tree/master/DeepQ\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Apprenticeship learning using Inverse Reinforcement Learning\")), mdx(\"img\", {\n    \"src\": \"https://jangirrishabh.github.io/assets/IRL/irl_des.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/\"\n  }, \"https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jangirrishabh/toyCarIRL\"\n  }, \"https://github.com/jangirrishabh/toyCarIRL\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning and DQN, learning to play from pixels\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html\"\n  }, \"https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning in a Nutshell: Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/\"\n  }, \"https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Write an AI to win at Pong from scratch with Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0#.n1pgn9chr\"\n  }, \"https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0#.n1pgn9chr\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Reinforcement Learning (with Code, Exercises and Solutions)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.wildml.com/2016/10/learning-reinforcement-learning/\"\n  }, \"http://www.wildml.com/2016/10/learning-reinforcement-learning/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/dennybritz/reinforcement-learning\"\n  }, \"https://github.com/dennybritz/reinforcement-learning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning: Playing a Racing Game\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html\"\n  }, \"https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Experimenting with Reinforcement Learning and Active Inference\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.araya.org/archives/955\"\n  }, \"http://www.araya.org/archives/955\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/arayabrain/BinarySearchLSTM\"\n  }, \"https://github.com/arayabrain/BinarySearchLSTM\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep reinforcement learning, battleship\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://efavdb.com/battleship/\"\n  }, \"http://efavdb.com/battleship/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/EFavDB/battleship\"\n  }, \"https://github.com/EFavDB/battleship\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Research Review Week 2: Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning\"\n  }, \"https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning: Artificial Intelligence in Game Playing\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@pavelkordik/reinforcement-learning-the-hardest-part-of-machine-learning-b667a22995ca#.jjiitflok\"\n  }, \"https://medium.com/@pavelkordik/reinforcement-learning-the-hardest-part-of-machine-learning-b667a22995ca#.jjiitflok\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Artificial Intelligence\\u2019s Next Big Step: Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://thenewstack.io/reinforcement-learning-ready-real-world/\"\n  }, \"http://thenewstack.io/reinforcement-learning-ready-real-world/\")), mdx(\"h2\", {\n    \"id\": \"lets-make-a-dqn\"\n  }, \"Let\\u2019s make a DQN\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Let\\u2019s make a DQN\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Theory: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/\"\n  }, \"https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Implementation: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/\"\n  }, \"https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Debugging: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/\"\n  }, \"https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Full DQN: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/\"\n  }, \"https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py\"\n  }, \"https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py\"))), mdx(\"h1\", {\n    \"id\": \"books\"\n  }, \"Books\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning: State-of-the-Art\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"The main goal of this book is to present an up-to-date series of survey articles\\non the main contemporary sub-fields of reinforcement learning.\\nThis includes surveys on partially observable environments, hierarchical task decompositions,\\nrelational knowledge representation and predictive state representations.\\nFurthermore, topics such as transfer, evolutionary methods and continuous spaces in reinforcement learning are surveyed.\\nIn addition, several chapters review reinforcement learning methods in robotics, in games, and in computational neuroscience.\\nIn total seventeen different subfields are presented by mostly young experts in those areas,\\nand together they truly represent a state-of-the-art of current reinforcement learning research.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"book: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.springer.com/gp/book/9783642276446#\"\n  }, \"http://www.springer.com/gp/book/9783642276446#\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning: An Introduction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Mononofu/reinforcement-learning\"\n  }, \"https://github.com/Mononofu/reinforcement-learning\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\"\n  }, \"http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"course: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html\"\n  }, \"http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"book(1st edition): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1jkaMq\"\n  }, \"http://pan.baidu.com/s/1jkaMq\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"book(2rd edition): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1dDnNEnR\"\n  }, \"http://pan.baidu.com/s/1dDnNEnR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning: An Introduction (Second edition, Draft)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"book: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf\"\n  }, \"https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1slrMYkP\"\n  }, \"https://pan.baidu.com/s/1slrMYkP\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ShangtongZhang/reinforcement-learning-an-introduction\"\n  }, \"https://github.com/ShangtongZhang/reinforcement-learning-an-introduction\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Self Learning Quant\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: explain and show the concept of self reinforcement learning combined with a neural network\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@danielzakrisson/the-self-learning-quant-d3329fcc9915#.9lsa5rh3e\"\n  }, \"https://medium.com/@danielzakrisson/the-self-learning-quant-d3329fcc9915#.9lsa5rh3e\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gihtub: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/danielzak/sl-quant\"\n  }, \"https://github.com/danielzak/sl-quant\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforcement Learning: An Introduction\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"author: Richard S. Sutton and Andrew G. Barto\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"book: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html\"\n  }, \"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"solutions: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/btaba/intro-to-rl\"\n  }, \"https://github.com/btaba/intro-to-rl\"))), mdx(\"h1\", {\n    \"id\": \"resources\"\n  }, \"Resources\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning Papers\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/junhyukoh/deep-reinforcement-learning-papers\"\n  }, \"https://github.com/junhyukoh/deep-reinforcement-learning-papers\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Awesome Reinforcement Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"website: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://aikorea.org/awesome-rl/?utm_content=buffer5d0f3&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer#online-demos\"\n  }, \"http://aikorea.org/awesome-rl/?utm_content=buffer5d0f3&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer#online-demos\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/aikorea/awesome-rl\"\n  }, \"https://github.com/aikorea/awesome-rl\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning Papers\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/muupan/deep-reinforcement-learning-papers\"\n  }, \"https://github.com/muupan/deep-reinforcement-learning-papers\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning \\u6DF1\\u5EA6\\u589E\\u5F3A\\u5B66\\u4E60\\u8D44\\u6E90\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://zhuanlan.zhihu.com/p/20885568\"\n  }, \"https://zhuanlan.zhihu.com/p/20885568\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"deep-reinforcement-learning-networks: A list of deep neural network architectures for reinforcement learning tasks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/5vision/deep-reinforcement-learning-networks\"\n  }, \"https://github.com/5vision/deep-reinforcement-learning-networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning survey\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey\"\n  }, \"https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Studying Reinforcement Learning Guide\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/0bserver07/Study-Reinforcement-Learning\"\n  }, \"https://github.com/0bserver07/Study-Reinforcement-Learning\"))), mdx(\"h1\", {\n    \"id\": \"reading-and-questions\"\n  }, \"Reading and Questions\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"What are the best books about reinforcement learning?\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.quora.com/What-are-the-best-books-about-reinforcement-learning\"\n  }, \"https://www.quora.com/What-are-the-best-books-about-reinforcement-learning\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Reinforcement Learning\r\ndate: 2015-10-09\r\n---\r\n\r\n# Tutorials\r\n\r\n**Demystifying Deep Reinforcement Learning (Part1)**\r\n\r\n[http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/](http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/)\r\n\r\n**Deep Reinforcement Learning With Neon (Part2)**\r\n\r\n[http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/](http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/)\r\n\r\n**Deep Reinforcement Learning**\r\n\r\n- intro: David Silver, Google DeepMind\r\n- slides: [http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf)\r\n- mirror: [http://pan.baidu.com/s/1qWBOJGo](http://pan.baidu.com/s/1qWBOJGo)\r\n\r\n**Deep Reinforcement Learning**\r\n\r\n- intro:  MLSS 2016. John Schulman[UC Berkeley]\r\n- homepage: [http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/index.html](http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/index.html)\r\n- slides: [http://pan.baidu.com/s/1jIatusA#path=%252F](http://pan.baidu.com/s/1jIatusA#path=%252F)\r\n\r\n**Deep Reinforcement Learning: Pong from Pixels**\r\n\r\n![](http://karpathy.github.io/assets/rl/preview.jpeg)\r\n\r\n- intro: Andrej Karpathy\r\n- blog: [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)\r\n- gist: [https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\r\n\r\n**Deep Reinforcement Learning**\r\n\r\n- instructor: David Silver. RLDM 2015\r\n- video: [http://videolectures.net/rldm2015_silver_reinforcement_learning/](http://videolectures.net/rldm2015_silver_reinforcement_learning/)\r\n\r\n**Deep Reinforcement Learning**\r\n\r\n- intro: David Silver [Google DeepMind]\r\n- video: [http://techtalks.tv/talks/deep-reinforcement-learning/62360/](http://techtalks.tv/talks/deep-reinforcement-learning/62360/)\r\n- slides: [http://hunch.net/~beygel/deep_rl_tutorial.pdf](http://hunch.net/~beygel/deep_rl_tutorial.pdf)\r\n\r\n**The Nuts and Bolts of Deep RL Research**\r\n\r\n- intro: NIPS 2016, John Schulman, OpenAI\r\n- slides: [http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf](http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf)\r\n- mirror: [https://pan.baidu.com/s/1kVkBLkF](https://pan.baidu.com/s/1kVkBLkF)\r\n\r\n**ML Tutorial: Modern Reinforcement Learning and Video Games**\r\n\r\n- intro: by Marc Bellemare [DeepMind]\r\n- youtube: [https://www.youtube.com/watch?v=WuFMrk3ZbkE](https://www.youtube.com/watch?v=WuFMrk3ZbkE)\r\n- mirror: [https://www.bilibili.com/video/av17360035/](https://www.bilibili.com/video/av17360035/)\r\n\r\n**Reinforcement learning explained**\r\n\r\n- blog: [https://www.oreilly.com/ideas/reinforcement-learning-explained](https://www.oreilly.com/ideas/reinforcement-learning-explained)\r\n\r\n**Beginner's guide to Reinforcement Learning & its implementation in Python**\r\n\r\n[https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)\r\n\r\n**Reinforcement Learning on the Web**\r\n\r\n- intro: Andrej Karpathy\r\n- slides: [https://docs.google.com/presentation/d/1lcYrN56V2_SuX1rSmpzOUeMnheF6Jsu33-MsvLW9O_4/edit#slide=id.p](https://docs.google.com/presentation/d/1lcYrN56V2_SuX1rSmpzOUeMnheF6Jsu33-MsvLW9O_4/edit#slide=id.p)\r\n- slides: [http://alpha.openai.com/ak_rework_2017.pdf](http://alpha.openai.com/ak_rework_2017.pdf)\r\n\r\n**Deep Q Learning with Keras and Gym**\r\n\r\n- blog: [https://keon.io/rl/deep-q-learning-with-keras-and-gym/](https://keon.io/rl/deep-q-learning-with-keras-and-gym/)\r\n- github: [https://github.com/keon/deep-q-learning](https://github.com/keon/deep-q-learning)\r\n\r\n**â€œDeep Reinforcement Learning, Decision Making, and Control**\r\n\r\n- intro: ICML 2017 Tutorial\r\n- slides: [https://sites.google.com/view/icml17deeprl](https://sites.google.com/view/icml17deeprl)\r\n\r\n**A Tour of Reinforcement Learning: The View from Continuous Control**\r\n\r\n- intro: by Benjamin Recht, UC Berkeley\r\n- slides: [https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/Recht_ICML_Control-RL_tutorial.pdf](https://people.eecs.berkeley.edu/~brecht/l2c-icml2018/Recht_ICML_Control-RL_tutorial.pdf)\r\n\r\n**An Introduction to Deep Reinforcement Learning**\r\n\r\n- intro: McGill University & Google Brain\r\n- arxiv: [https://arxiv.org/abs/1811.12560](https://arxiv.org/abs/1811.12560)\r\n\r\n## Simple Reinforcement Learning with Tensorflow\r\n\r\n**Part 0: Q-Learning with Tables and Neural Networks**\r\n[https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.oo105wa2t](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.oo105wa2t)\r\n\r\n**Part 1 - Two-armed Bandit**\r\n\r\n[https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.tk89k51ob](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.tk89k51ob)\r\n\r\n**Part 2 - Policy-based Agents**\r\n\r\n[https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.n2wytg9q0](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.n2wytg9q0)\r\n\r\n**Part 3 - Model-Based RL**\r\n[https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.742i2yj6p](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.742i2yj6p)\r\n\r\n**Part 4: Deep Q-Networks and Beyond**\r\n[https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.jox069crz](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.jox069crz)\r\n\r\n**Part 5: Visualizing an Agentâ€™s Thoughts and Actions**\r\n[https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.pluh6cygm](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.pluh6cygm)\r\n\r\n**Part 6: Partial Observability and Deep Recurrent Q-Networks**\r\n\r\n- blog: [https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.3se46qkzy](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.3se46qkzy)\r\n- github: [https://gist.github.com/awjuliani/35d2ab3409fc818011b6519f0f1629df](https://gist.github.com/awjuliani/35d2ab3409fc818011b6519f0f1629df)\r\n\r\n**Part 7: Action-Selection Strategies for Exploration**\r\n\r\n- blog: [https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.8mcaa5nbe](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.8mcaa5nbe)\r\n- demo: [https://awjuliani.github.io/exploration/index.html](https://awjuliani.github.io/exploration/index.html)\r\n\r\n**Dissecting Reinforcement Learning**\r\n\r\n- part 1: [https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html)\r\n- part 2: [https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html](https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html)\r\n- part 3: [https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html](https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html)\r\n- github: [https://github.com/mpatacchiola/dissecting-reinforcement-learning](https://github.com/mpatacchiola/dissecting-reinforcement-learning)\r\n\r\n**REINFORCE tutorial**\r\n\r\n- intro: A small collection of code snippets and notes explaining the foundations of the REINFORCE algorithm.\r\n- github: [https://github.com/mathias-madsen/reinforce_tutorial](https://github.com/mathias-madsen/reinforce_tutorial)\r\n\r\n**Deep Q-Learning Recap**\r\n\r\n[http://blog.davidqiu.com/Research/%5B%20Recap%20%5D%20Deep%20Q-Learning%20Recap/](http://blog.davidqiu.com/Research/%5B%20Recap%20%5D%20Deep%20Q-Learning%20Recap/)\r\n\r\n**Introduction to Reinforcement Learning**\r\n\r\n- intro: Joelle Pineau [McGill University]\r\n- video: [http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/](http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/)\r\n- slides: [http://videolectures.net/site/normal_dl/tag=1051677/deeplearning2016_pineau_reinforcement_learning_01.pdf](http://videolectures.net/site/normal_dl/tag=1051677/deeplearning2016_pineau_reinforcement_learning_01.pdf)\r\n\r\n# Courses\r\n\r\n**Advanced Topics: RL**\r\n\r\n**UCL Course on RL**\r\n\r\n- instructors: David Silver (Google DeepMind, AlphaGo)\r\n- homepage: [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)\r\n- youtube: [https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\r\n- video: [http://pan.baidu.com/s/1bnWGuIz/](http://pan.baidu.com/s/1bnWGuIz/)\r\n- assignment: [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/Easy21-Johannes.pdf)\r\n\r\n**CS 294: Deep Reinforcement Learning, Fall 2017**\r\n\r\n- instructor: Sergey Levine\r\n- homepage: [http://rll.berkeley.edu/deeprlcourse/](http://rll.berkeley.edu/deeprlcourse/)\r\n- youtube: [https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3](https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3)\r\n- bilibili: [https://www.bilibili.com/video/av21501169/](https://www.bilibili.com/video/av21501169/)\r\n\r\n**CS 294: Deep Reinforcement Learning, Spring 2017**\r\n\r\n- course page: [http://rll.berkeley.edu/deeprlcoursesp17/](http://rll.berkeley.edu/deeprlcoursesp17/)\r\n- github: [https://github.com//txizzle/drl](https://github.com//txizzle/drl)\r\n\r\n**Berkeley CS 294: Deep Reinforcement Learning**\r\n\r\n- instructors: John Schulman, Pieter Abbeel\r\n- homepage: [http://rll.berkeley.edu/deeprlcourse/](http://rll.berkeley.edu/deeprlcourse/)\r\n- youtube: [https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX](https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX)\r\n- mirror: [https://pan.baidu.com/s/1hsQcm1Y](https://pan.baidu.com/s/1hsQcm1Y)\r\n\r\n**(Udacity) Reinforcement Learning - Offered at Georgia Tech as CS 8803**\r\n\r\n- instructor: Charles Isbell, Michael Littman\r\n- homepage: [https://www.udacity.com/course/reinforcement-learning--ud600](https://www.udacity.com/course/reinforcement-learning--ud600)\r\n- homepage: [https://classroom.udacity.com/courses/ud820/lessons/684808907/concepts/6512308530923](https://classroom.udacity.com/courses/ud820/lessons/684808907/concepts/6512308530923)\r\n\r\n**CS229 Lecture notes Part XIII: Reinforcement Learning and Control**\r\n\r\n- intro: Andrew Ng\r\n- lecture notes: [http://cs229.stanford.edu/notes/cs229-notes12.pdf](http://cs229.stanford.edu/notes/cs229-notes12.pdf)\r\n\r\n**Practical_RL: A course in reinforcement learning in the wild**\r\n\r\n- github: [https://github.com/yandexdataschool/Practical_RL](https://github.com/yandexdataschool/Practical_RL)\r\n\r\n**Reinforcement Learning (COMP-762) Winter 2017**\r\n\r\n- course page: [http://www.cs.mcgill.ca/~dprecup/courses/rl.html](http://www.cs.mcgill.ca/~dprecup/courses/rl.html)\r\n- lectures: [http://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html](http://www.cs.mcgill.ca/~dprecup/courses/RL/lectures.html)\r\n\r\n**Deep RL Bootcamp - 26-27 August 2017 | Berkeley CA**\r\n\r\n- lectures: [https://sites.google.com/view/deep-rl-bootcamp/lectures](https://sites.google.com/view/deep-rl-bootcamp/lectures)\r\n- video: [https://www.bilibili.com/video/av15568836/](https://www.bilibili.com/video/av15568836/)\r\n\r\n**CMPUT 366: Intelligent Systems and CMPUT 609: Reinforcement Learning & Artificial Intelligence**\r\n\r\n- intro: by Rich Sutton, Adam White\r\n- lecture video: [https://drive.google.com/drive/folders/0B3w765rOKuKAMG9lbmRacFdsLWM?direction=a](https://drive.google.com/drive/folders/0B3w765rOKuKAMG9lbmRacFdsLWM?direction=a)\r\n\r\n**Deep Reinforcement Learning and Control (Spring 2017, CMU 10703)**\r\n\r\n- instructors: Katerina Fragkiadaki, Ruslan Satakhutdinov\r\n- homepage: [https://katefvision.github.io/](https://katefvision.github.io/)\r\n- video: [https://www.youtube.com/playlist?list=PLpIxOj-HnDsNPFdu2UqCu2McJKHs-eWXv](https://www.youtube.com/playlist?list=PLpIxOj-HnDsNPFdu2UqCu2McJKHs-eWXv)\r\n- mirror: [https://www.bilibili.com/video/av18865689/](https://www.bilibili.com/video/av18865689/)\r\n\r\n**Advanced Deep Learning & Reinforcement Learning**\r\n\r\n- intro: DeepMind\r\n- youtube: [https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs](https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)\r\n- bilibili: [https://www.bilibili.com/video/av36621866/](https://www.bilibili.com/video/av36621866/)\r\n- github: [https://github.com/RylanSchaeffer/ucl-adv-dl-rl](https://github.com/RylanSchaeffer/ucl-adv-dl-rl)\r\n\r\n# Papers\r\n\r\n**Playing Atari with Deep Reinforcement Learning**\r\n\r\n- intro: Google DeepMind. NIPS Deep Learning Workshop 2013\r\n- arxiv: [http://arxiv.org/abs/1312.5602](http://arxiv.org/abs/1312.5602)\r\n- github: [https://github.com/kristjankorjus/Replicating-DeepMind](https://github.com/kristjankorjus/Replicating-DeepMind)\r\n- demo: [http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html](http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html)\r\n- github: [https://github.com/Kaixhin/Atari](https://github.com/Kaixhin/Atari)\r\n- github(Tensorflow): [https://github.com/gliese581gg/DQN_tensorflow](https://github.com/gliese581gg/DQN_tensorflow)\r\n- summary: [https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md](https://github.com/aleju/papers/blob/master/neural-nets/Playing_Atari_with_Deep_Reinforcement_Learning.md)\r\n\r\n**Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning**\r\n\r\n- intro: NIPS 2014\r\n- keywords: DQN, MCTS\r\n- paper: [http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging](http://papers.nips.cc/paper/5421-scalable-inference-for-neuronal-connectivity-from-calcium-imaging)\r\n- paper: [https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf](https://web.eecs.umich.edu/~baveja/Papers/UCTtoCNNsAtariGames-FinalVersion.pdf)\r\n\r\n**Replicating the Paper â€œPlaying Atari with Deep Reinforcement Learningâ€**\r\n\r\n- intro: University of Tartu\r\n- technical report: [https://courses.cs.ut.ee/MTAT.03.291/2014_spring/uploads/Main/Replicating%20DeepMind.pdf](https://courses.cs.ut.ee/MTAT.03.291/2014_spring/uploads/Main/Replicating%20DeepMind.pdf)\r\n\r\n**A Tutorial for Reinforcement Learning**\r\n\r\n- paper: [http://web.mst.edu/~gosavia/tutorial.pdf](http://web.mst.edu/~gosavia/tutorial.pdf)\r\n- code(C): [http://web.mst.edu/~gosavia/bookcodes.html](http://web.mst.edu/~gosavia/bookcodes.html)\r\n- code(Matlab): [http://web.mst.edu/~gosavia/mrrl_website.html](http://web.mst.edu/~gosavia/mrrl_website.html)\r\n\r\n**Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models**\r\n\r\n- arxiv: [http://arxiv.org/abs/1507.00814](http://arxiv.org/abs/1507.00814)\r\n- notes: [https://www.evernote.com/shard/s189/sh/a4262b84-a322-4f77-9a76-569278be84af/b8c3e146a76ca3853f560bb03b60a481](https://www.evernote.com/shard/s189/sh/a4262b84-a322-4f77-9a76-569278be84af/b8c3e146a76ca3853f560bb03b60a481)\r\n\r\n**Massively Parallel Methods for Deep Reinforcement Learning**\r\n\r\n- intro: ICML 2015. DeepMind\r\n- keywords: DQN, Gorila\r\n- arxiv: [https://arxiv.org/abs/1507.04296](https://arxiv.org/abs/1507.04296)\r\n\r\n**Action-Conditional Video Prediction using Deep Networks in Atari Games**\r\n\r\n- homepage: [https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction](https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction)\r\n- arxiv: [http://arxiv.org/abs/1507.08750](http://arxiv.org/abs/1507.08750)\r\n- github: [https://github.com/junhyukoh/nips2015-action-conditional-video-prediction](https://github.com/junhyukoh/nips2015-action-conditional-video-prediction)\r\n- video: [http://video.weibo.com/show?fid=1034:98062f3d83e41da6faa99cde5aa1ac97](http://video.weibo.com/show?fid=1034:98062f3d83e41da6faa99cde5aa1ac97)\r\n\r\n**Deep Recurrent Q-Learning for Partially Observable MDPs**\r\n\r\n- intro: AAAI 2015\r\n- arxiv: [https://arxiv.org/abs/1507.06527](https://arxiv.org/abs/1507.06527)\r\n\r\n**Continuous control with deep reinforcement learning**\r\n\r\n- intro: Google DeepMind\r\n- arxiv: [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)\r\n- github: [https://github.com/iassael/torch-policy-gradient](https://github.com/iassael/torch-policy-gradient)\r\n- github: [https://github.com/stevenpjg/ddpg-aigym](https://github.com/stevenpjg/ddpg-aigym)\r\n- github(TensorFlow + OpenAI Gym): [https://github.com/SimonRamstedt/ddpg](https://github.com/SimonRamstedt/ddpg)\r\n\r\n**Benchmarking for Bayesian Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1509.04064](http://arxiv.org/abs/1509.04064)\r\n- code: [https://github.com/mcastron/BBRL/](https://github.com/mcastron/BBRL/)\r\n- reading: [http://blogs.ulg.ac.be/damien-ernst/benchmarking-for-bayesian-reinforcement-learning/](http://blogs.ulg.ac.be/damien-ernst/benchmarking-for-bayesian-reinforcement-learning/)\r\n\r\n**Deep Reinforcement Learning with Double Q-learning**\r\n\r\n- intro: AAAI 2016\r\n- arxiv: [https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)\r\n\r\n**Giraffe: Using Deep Reinforcement Learning to Play Chess**\r\n\r\n- arxiv: [http://arxiv.org/abs/1509.01549](http://arxiv.org/abs/1509.01549)\r\n\r\n**Human-level control through deep reinforcement learning**\r\n\r\n![](/assets/reinforcement-learning-materials/DeepMind_Atari_Deep_Q_Learner-breakout.gif)\r\n\r\n- intro: Google DeepMind. 2015 Nature\r\n- paper: [http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D](http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D)\r\n- paper: [http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf](http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)\r\n- github(Lua/Torch): [https://github.com/deepmind/dqn](https://github.com/deepmind/dqn)\r\n- mirror: [http://pan.baidu.com/s/1kTiwzOF](http://pan.baidu.com/s/1kTiwzOF)\r\n- code: [https://sites.google.com/a/deepmind.com/dqn/](https://sites.google.com/a/deepmind.com/dqn/)\r\n- youtube: [https://www.youtube.com/watch?v=V2wzkPmiB_A](https://www.youtube.com/watch?v=V2wzkPmiB_A)\r\n- github: [https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner](https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner)\r\n- github: [https://github.com/tambetm/simple_dqn](https://github.com/tambetm/simple_dqn)\r\n- github: [https://github.com/devsisters/DQN-tensorflow](https://github.com/devsisters/DQN-tensorflow)\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/2x4yy1/google_deepmind_nature_paper_humanlevel_control](https://www.reddit.com/r/MachineLearning/comments/2x4yy1/google_deepmind_nature_paper_humanlevel_control)\r\n\r\n**Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models**\r\n\r\n- arxiv: [http://arxiv.org/abs/1510.02173](http://arxiv.org/abs/1510.02173)\r\n\r\n**Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning**\r\n\r\n- intro: Google DeepMind\r\n- arxiv: [http://arxiv.org/abs/1509.08731](http://arxiv.org/abs/1509.08731)\r\n- notes: [https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889](https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889)\r\n\r\n**Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning**\r\n\r\n- intro: ICLR 2016\r\n- arxiv: [http://arxiv.org/abs/1511.06342](http://arxiv.org/abs/1511.06342)\r\n- github: [https://github.com/eparisotto/ActorMimic](https://github.com/eparisotto/ActorMimic)\r\n\r\n**MazeBase: A Sandbox for Learning from Games**\r\n\r\n- intro: New York University & Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1511.07401](http://arxiv.org/abs/1511.07401)\r\n\r\n**Learning Simple Algorithms from Examples**\r\n\r\n- intro: New York University & Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1511.07275](http://arxiv.org/abs/1511.07275)\r\n- github: [https://github.com/wojzaremba/algorithm-learning](https://github.com/wojzaremba/algorithm-learning)\r\n\r\n**Learning Algorithms from Data**\r\n\r\n- PhD thesis: [http://www.cs.nyu.edu/media/publications/zaremba_wojciech.pdf](http://www.cs.nyu.edu/media/publications/zaremba_wojciech.pdf)\r\n- github: [https://github.com/wojzaremba/algorithm-learning](https://github.com/wojzaremba/algorithm-learning)\r\n\r\n**Multiagent Cooperation and Competition with Deep Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.08779](http://arxiv.org/abs/1511.08779)\r\n- github: [https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player](https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player)\r\n\r\n**Active Object Localization with Deep Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.06015](http://arxiv.org/abs/1511.06015)\r\n\r\n**Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.01124](http://arxiv.org/abs/1512.01124)\r\n\r\n**How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.02011](http://arxiv.org/abs/1512.02011)\r\n\r\n**State of the Art Control of Atari Games Using Shallow Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1512.01563](http://arxiv.org/abs/1512.01563)\r\n\r\n**Angrier Birds: Bayesian reinforcement learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1601.01297](http://arxiv.org/abs/1601.01297)\r\n- github: [https://github.com/imanolarrieta/angrybirds](https://github.com/imanolarrieta/angrybirds)\r\n- gitxiv: [http://gitxiv.com/posts/Nr2N7j4YrR4gnCYK9/angrier-birds-bayesian-reinforcement-learning](http://gitxiv.com/posts/Nr2N7j4YrR4gnCYK9/angrier-birds-bayesian-reinforcement-learning)\r\n\r\n**Prioritized Experience Replay**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.05952](http://arxiv.org/abs/1511.05952)\r\n\r\n**Dueling Network Architectures for Deep Reinforcement Learning**\r\n\r\n- intro: ICML 2016 best paper\r\n- arxiv: [http://arxiv.org/abs/1511.06581](http://arxiv.org/abs/1511.06581)\r\n- notes: [https://hadovanhasselt.wordpress.com/2016/06/20/best-paper-at-icml-dueling-network-architectures-for-deep-reinforcement-learning/](https://hadovanhasselt.wordpress.com/2016/06/20/best-paper-at-icml-dueling-network-architectures-for-deep-reinforcement-learning/)\r\n\r\n**Asynchronous Methods for Deep Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.01783](http://arxiv.org/abs/1602.01783)\r\n- github(Tensorflow): [https://github.com/traai/async-deep-rl](https://github.com/traai/async-deep-rl)\r\n- github(Tensorflow+Keras+OpenAI Gym): [https://github.com/coreylynch/async-rl](https://github.com/coreylynch/async-rl)\r\n- github(Tensorflow): [https://github.com/devsisters/async-rl-tensorflow](https://github.com/devsisters/async-rl-tensorflow)\r\n- github(PyTorch): [https://github.com/ikostrikov/pytorch-a3c](https://github.com/ikostrikov/pytorch-a3c)\r\n- notes: [https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/](https://blog.acolyer.org/2016/10/10/asynchronous-methods-for-deep-reinforcement-learning/)\r\n\r\n**Graying the black box: Understanding DQNs**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.02658](http://arxiv.org/abs/1602.02658)\r\n\r\n**Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.02672](http://arxiv.org/abs/1602.02672)\r\n\r\n**Value Iteration Networks**\r\n\r\n![](https://raw.githubusercontent.com/karpathy/paper-notes/master/img/vin/Screen%20Shot%202016-08-13%20at%204.58.42%20PM.png)\r\n\r\n- intro: NIPS 2016, Best Paper Award. University of California, Berkeley\r\n- arxiv: [http://arxiv.org/abs/1602.02867](http://arxiv.org/abs/1602.02867)\r\n- github(official, Theano): [https://github.com/avivt/VIN](https://github.com/avivt/VIN)\r\n- github: [https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks](https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks)\r\n- github: [https://github.com/onlytailei/PyTorch-value-iteration-networks](https://github.com/onlytailei/PyTorch-value-iteration-networks)\r\n- github: [https://github.com/kentsommer/pytorch-value-iteration-networks](https://github.com/kentsommer/pytorch-value-iteration-networks)\r\n- github: [https://github.com/neka-nat/vin-keras](https://github.com/neka-nat/vin-keras)\r\n- notes(by Andrej Karpathy): [https://github.com/karpathy/paper-notes/blob/master/vin.md](https://github.com/karpathy/paper-notes/blob/master/vin.md)\r\n\r\n**Insights in Reinforcement Learning**\r\n\r\n- intro: MSc thesis\r\n- mirror: [http://pan.baidu.com/s/1bn51BYJ](http://pan.baidu.com/s/1bn51BYJ)\r\n\r\n**Using Deep Q-Learning to Control Optimization Hyperparameters**\r\n\r\n- arxiv: [http://arxiv.org/abs/1602.04062](http://arxiv.org/abs/1602.04062)\r\n\r\n**Continuous Deep Q-Learning with Model-based Acceleration**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.00748](http://arxiv.org/abs/1603.00748)\r\n\r\n**Deep Reinforcement Learning from Self-Play in Imperfect-Information Games**\r\n\r\n- arxiv: [http://arxiv.org/abs/1603.01121](http://arxiv.org/abs/1603.01121)\r\n\r\n**Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation**\r\n\r\n- intro: MIT\r\n- arxiv: [https://arxiv.org/abs/1604.06057](https://arxiv.org/abs/1604.06057)\r\n- github: [https://github.com/EthanMacdonald/h-DQN](https://github.com/EthanMacdonald/h-DQN)\r\n\r\n**Benchmarking Deep Reinforcement Learning for Continuous Control**\r\n\r\n- arxiv: [http://arxiv.org/abs/1604.06778](http://arxiv.org/abs/1604.06778)\r\n- github: [https://github.com/rllab/rllab](https://github.com/rllab/rllab)\r\n- doc: [https://rllab.readthedocs.org/en/latest/](https://rllab.readthedocs.org/en/latest/)\r\n\r\n**Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning**\r\n\r\n![](http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/dog_teaser.png)\r\n\r\n- homepage: [http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html](http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html)\r\n- paper: [http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf](http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf)\r\n- github: [https://github.com/xbpeng/DeepTerrainRL](https://github.com/xbpeng/DeepTerrainRL)\r\n\r\n**Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.05359](http://arxiv.org/abs/1605.05359)\r\n\r\n**Deep Successor Reinforcement Learning (MIT)**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.02396](http://arxiv.org/abs/1606.02396)\r\n- github: [https://github.com/Ardavans/DSR](https://github.com/Ardavans/DSR)\r\n\r\n**Learning to Communicate with Deep Multi-Agent Reinforcement Learning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1605.06676](https://arxiv.org/abs/1605.06676)\r\n- github: [https://github.com/iassael/learning-to-communicate](https://github.com/iassael/learning-to-communicate)\r\n\r\n**Deep Reinforcement Learning with Regularized Convolutional Neural Fitted Q Iteration**\r\n**RC-NFQ: Regularized Convolutional Neural Fitted Q Iteration**\r\n\r\n- intro: A batch algorithm for deep reinforcement learning. \r\nIncorporates dropout regularization and convolutional neural networks with a separate target Q network.\r\n- paper: [http://machineintelligence.org/papers/rc-nfq.pdf](http://machineintelligence.org/papers/rc-nfq.pdf)\r\n- github: [https://github.com/cosmoharrigan/rc-nfq](https://github.com/cosmoharrigan/rc-nfq)\r\n\r\n**Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks**\r\n\r\n- intro: Facebook AI Research\r\n- arxiv: [http://arxiv.org/abs/1609.02993](http://arxiv.org/abs/1609.02993)\r\n\r\n**Bayesian Reinforcement Learning: A Survey**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.04436](http://arxiv.org/abs/1609.04436)\r\n\r\n**Playing FPS Games with Deep Reinforcement Learning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.05521](http://arxiv.org/abs/1609.05521)\r\n- demo: [https://www.youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT](https://www.youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT)\r\n- notes: [https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/](https://blog.acolyer.org/2016/11/23/playing-fps-games-with-deep-reinforcement-learning/)\r\n\r\n**Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning with Stochastic Initial States**\r\n\r\n- intro: University of Washington & UC Berkeley\r\n- arxiv: [https://arxiv.org/abs/1610.01112](https://arxiv.org/abs/1610.01112)\r\n\r\n**Utilization of Deep Reinforcement Learning for saccadic-based object visual search**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.06492](https://arxiv.org/abs/1610.06492)\r\n\r\n**Learning to Navigate in Complex Environments**\r\n\r\n- intro: Google DeepMind\r\n- arxiv: [https://arxiv.org/abs/1611.03673](https://arxiv.org/abs/1611.03673)\r\n- github: [https://github.com/deepmind/lab](https://github.com/deepmind/lab)\r\n- youtube: [https://www.youtube.com/watch?v=lNoaTyMZsWI](https://www.youtube.com/watch?v=lNoaTyMZsWI)\r\n\r\n**Reinforcement Learning with Unsupervised Auxiliary Tasks**\r\n\r\n- intro: DeepMind. ICLR 2017 oral\r\n- arxiv: [https://arxiv.org/abs/1611.05397](https://arxiv.org/abs/1611.05397)\r\n\r\n**Learning to reinforcement learn**\r\n\r\n- intro: DeepMind\r\n- arxiv: [https://arxiv.org/abs/1611.05763](https://arxiv.org/abs/1611.05763)\r\n\r\n**A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games**\r\n\r\n- intro: Graduate Training Center of Neuroscience & MSR\r\n- arxiv: [https://arxiv.org/abs/1611.07078](https://arxiv.org/abs/1611.07078)\r\n\r\n**Exploration for Multi-task Reinforcement Learning with Deep Generative Models**\r\n\r\n- intro: NIPS Deep Reinforcement Learning Workshop 2016\r\n- arxiv: [https://arxiv.org/abs/1611.09894](https://arxiv.org/abs/1611.09894)\r\n\r\n**Neural Combinatorial Optimization with Reinforcement Learning**\r\n\r\n- intro: Google Brain\r\n- keywords: traveling salesman problem (TSP)\r\n- arxiv: [https://arxiv.org/abs/1611.09940](https://arxiv.org/abs/1611.09940)\r\n\r\n**Loss is its own Reward: Self-Supervision for Reinforcement Learning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.07307](https://arxiv.org/abs/1612.07307)\r\n\r\n**Reinforcement Learning Using Quantum Boltzmann Machines**\r\n\r\n- intro: 1QB Information Technologies (1QBit)\r\n- arxiv: [https://arxiv.org/abs/1612.05695](https://arxiv.org/abs/1612.05695)\r\n\r\n**Deep Reinforcement Learning applied to the game Bubble Shooter**\r\n\r\n- bachelor thesis: [https://staff.fnwi.uva.nl/b.bredeweg/pdf/BSc/20152016/Samson.pdf](https://staff.fnwi.uva.nl/b.bredeweg/pdf/BSc/20152016/Samson.pdf)\r\n- github: [https://github.com/laurenssam/AlphaBubble](https://github.com/laurenssam/AlphaBubble)\r\n- demo: [https://www.youtube.com/watch?v=DPAKFenNgbs](https://www.youtube.com/watch?v=DPAKFenNgbs)\r\n\r\n**Deep Reinforcement Learning: An Overview**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.07274](https://arxiv.org/abs/1701.07274)\r\n\r\n**Robust Adversarial Reinforcement Learning**\r\n\r\n- intro: CMU & Google Brain & Google Research\r\n- arxiv: [https://arxiv.org/abs/1703.02702](https://arxiv.org/abs/1703.02702)\r\n\r\n**Beating Atari with Natural Language Guided Reinforcement Learning**\r\n\r\n- intro: Stanford University\r\n- arxiv: [https://arxiv.org/abs/1704.05539](https://arxiv.org/abs/1704.05539)\r\n\r\n**Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning**\r\n\r\n- intro: Imperial College London\r\n- arxiv: [https://arxiv.org/abs/1705.06769](https://arxiv.org/abs/1705.06769)\r\n- github: [https://github.com/Nat-D/FeatureControlHRL](https://github.com/Nat-D/FeatureControlHRL)\r\n\r\n**Distral: Robust Multitask Reinforcement Learning**\r\n\r\n- intro: DeepMind\r\n- keywords: Distill, transfer learning\r\n- arxiv: [https://arxiv.org/abs/1707.04175](https://arxiv.org/abs/1707.04175)\r\n\r\n**Deep Reinforcement Learning: Framework, Applications, and Embedded Implementations**\r\n\r\n- intro: Syracuse University & University of California, Riverside\r\n- arxiv: [https://arxiv.org/abs/1710.03792](https://arxiv.org/abs/1710.03792)\r\n\r\n**Robust Deep Reinforcement Learning with Adversarial Attacks**\r\n\r\n[https://arxiv.org/abs/1712.03632](https://arxiv.org/abs/1712.03632)\r\n\r\n**Variational Deep Q Network**\r\n\r\n- intro: Second workshop on Bayesian Deep Learning (NIPS 2017). Columbia University\r\n- arxiv: [https://arxiv.org/abs/1711.11225](https://arxiv.org/abs/1711.11225)\r\n\r\n**On Monte Carlo Tree Search and Reinforcement Learning**\r\n\r\n[https://www.jair.org/media/5507/live-5507-10333-jair.pdf](https://www.jair.org/media/5507/live-5507-10333-jair.pdf)\r\n\r\n**Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes**\r\n\r\n- intro: deepsense.ai & Intel & Polish Academy of Sciences\r\n- arxiv: [https://arxiv.org/abs/1801.02852](https://arxiv.org/abs/1801.02852)\r\n- gihtub: [https://github.com//anonymous-author1/DDRL](https://github.com//anonymous-author1/DDRL)\r\n\r\n**GAN Q-learning**\r\n\r\n[https://arxiv.org/abs/1805.04874](https://arxiv.org/abs/1805.04874)\r\n\r\n**Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents**\r\n\r\n- intro: Visual Geometry Group, University of Oxford & Element AI & Polytechnique Montreal, Mila & Canada CIFAR AI Chair\r\n- arxiv: [https://arxiv.org/abs/1904.01318](https://arxiv.org/abs/1904.01318)\r\n\r\n## Surveys\r\n\r\n**Reinforcement Learning: A Survey**\r\n\r\n- intro: JAIR 1996\r\n- project page: [http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/rl-survey.html](http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/rl-survey.html)\r\n- arxiv: [http://arxiv.org/abs/cs/9605103](http://arxiv.org/abs/cs/9605103)\r\n\r\n**A Brief Survey of Deep Reinforcement Learning**\r\n\r\n- intro: IEEE Signal Processing Magazine, Special Issue on Deep Learning for Image Understanding\r\n- intro: Imperial College London & Arizona State University\r\n- arxiv: [https://arxiv.org/abs/1708.05866](https://arxiv.org/abs/1708.05866)\r\n\r\n## Playing Doom\r\n\r\n**ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning**\r\n\r\n![](http://vizdoom.cs.put.edu.pl/user/pages/01.home/depthbuffer.png)\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.02097](http://arxiv.org/abs/1605.02097)\r\n- github: [https://github.com/Marqt/ViZDoom](https://github.com/Marqt/ViZDoom)\r\n- homepage: [http://vizdoom.cs.put.edu.pl/](http://vizdoom.cs.put.edu.pl/)\r\n- tutorial: [http://vizdoom.cs.put.edu.pl/tutorial](http://vizdoom.cs.put.edu.pl/tutorial)\r\n\r\n**Deep Reinforcement Learning From Raw Pixels in Doom**\r\n\r\n- intro: Bachelor's thesis\r\n- arxiv: [https://arxiv.org/abs/1610.02164](https://arxiv.org/abs/1610.02164)\r\n\r\n**Playing Doom with SLAM-Augmented Deep Reinforcement Learning**\r\n\r\n- intro: University of Oxford\r\n- arxiv: [https://arxiv.org/abs/1612.00380](https://arxiv.org/abs/1612.00380)\r\n\r\n**Reinforcement Learning via Recurrent Convolutional Neural Networks**\r\n\r\n- intro: ICPR 2016\r\n- arxiv: [https://arxiv.org/abs/1701.02392](https://arxiv.org/abs/1701.02392)\r\n- github: [https://github.com/tanmayshankar/RCNN_MDP](https://github.com/tanmayshankar/RCNN_MDP)\r\n\r\n**Shallow Updates for Deep Reinforcement Learning**\r\n\r\n- intro: The Technion & UC Berkeley\r\n- arxiv: [https://arxiv.org/abs/1705.07461](https://arxiv.org/abs/1705.07461)\r\n- github(Official): [https://github.com/Shallow-Updates-for-Deep-RL/Shallow_Updates_for_Deep_RL](https://github.com/Shallow-Updates-for-Deep-RL/Shallow_Updates_for_Deep_RL)\r\n\r\n# Projects\r\n\r\n**TorchQLearning**\r\n\r\n![](https://raw.githubusercontent.com/SeanNaren/TorchQLearningExample/master/images/torchplayscatch.gif)\r\n\r\n- github: [https://github.com/SeanNaren/TorchQLearningExample](https://github.com/SeanNaren/TorchQLearningExample)\r\n\r\n**General_Deep_Q_RL: General deep Q learning framework**\r\n\r\n- github: [https://github.com/VinF/General_Deep_Q_RL](https://github.com/VinF/General_Deep_Q_RL)\r\n- wiki: [https://github.com/VinF/General_Deep_Q_RL/wiki](https://github.com/VinF/General_Deep_Q_RL/wiki)\r\n\r\n**Snake: Toy example of deep reinforcement model playing the game of snake**\r\n\r\n![](https://raw.githubusercontent.com/bitwise-ben/Snake/master/images/snake.gif)\r\n\r\n- github: [https://github.com/bitwise-ben/Snake](https://github.com/bitwise-ben/Snake)\r\n\r\n**Using Deep Q Networks to Learn Video Game Strategies**\r\n\r\n- github: [https://github.com/asrivat1/DeepLearningVideoGames](https://github.com/asrivat1/DeepLearningVideoGames)\r\n\r\n**qlearning4k: Q-learning for Keras**\r\n\r\n![](https://raw.githubusercontent.com/farizrahman4u/qlearning4k/master/gifs/catch.gif)\r\n![](https://raw.githubusercontent.com/farizrahman4u/qlearning4k/master/gifs/snake.gif)\r\n\r\n- intro: \"Qlearning4k is a reinforcement learning add-on for the python deep learning library Keras. \r\nIts simple, and is ideal for rapid prototyping.\"\r\n- github: [https://github.com/farizrahman4u/qlearning4k](https://github.com/farizrahman4u/qlearning4k)\r\n\r\n**rlenvs: Reinforcement learning environments for Torch7, inspired by RL-Glue**\r\n\r\n- github: [https://github.com/Kaixhin/rlenvs](https://github.com/Kaixhin/rlenvs)\r\n\r\n**deep_rl_ale: An implementation of Deep Reinforcement Learning / Deep Q-Networks for Atari games in TensorFlow**\r\n\r\n- github: [https://github.com/Jabberwockyll/deep_rl_ale](https://github.com/Jabberwockyll/deep_rl_ale)\r\n\r\n**Chimp: General purpose framework for deep reinforcement learning**\r\n\r\n- github: [https://github.com/sisl/Chimp](https://github.com/sisl/Chimp)\r\n\r\n**Deep Q Learning for ATARI using Tensorflow**\r\n\r\n- github: [https://github.com/mrkulk/deepQN_tensorflow](https://github.com/mrkulk/deepQN_tensorflow)\r\n\r\n**DeepQLearning: A powerful machine learning algorithm utilizing Q-Learning and Neural Networks, implemented using Torch and Lua.**\r\n\r\n- github: [https://github.com/blakeMilner/DeepQLearning](https://github.com/blakeMilner/DeepQLearning)\r\n\r\n**OpenAI Gym: A toolkit for developing and comparing reinforcement learning algorithms**\r\n\r\n- homepage: [https://gym.openai.com/](https://gym.openai.com/)\r\n- github: [https://github.com/openai/gym](https://github.com/openai/gym)\r\n\r\n**DeeR: DEEp Reinforcement learning framework**\r\n\r\n- github: [https://github.com/VinF/deer/](https://github.com/VinF/deer/)\r\n- docs: [http://deer.readthedocs.io/en/latest/](http://deer.readthedocs.io/en/latest/)\r\n\r\n**KeRLym: A Deep Reinforcement Learning Toolbox in Keras**\r\n\r\n![](https://raw.githubusercontent.com/osh/kerlym/master/examples/example.png)\r\n\r\n- homepage: [https://oshearesearch.com/index.php/2016/06/14/kerlym-a-deep-reinforcement-learning-toolbox-in-keras/](https://oshearesearch.com/index.php/2016/06/14/kerlym-a-deep-reinforcement-learning-toolbox-in-keras/)\r\n- github: [https://github.com/osh/kerlym](https://github.com/osh/kerlym)\r\n\r\n**Pack of Drones: Layered reinforcement learning for complex behaviors**\r\n\r\n- github: [https://github.com/MickyDowns/deep-theano-rnn-lstm-car](https://github.com/MickyDowns/deep-theano-rnn-lstm-car)\r\n- youtube: [https://www.youtube.com/watch?v=WrLRGzbfeZc](https://www.youtube.com/watch?v=WrLRGzbfeZc)\r\n\r\n**RL Helicopter Game: Q-Learning and DQN Reinforcement Learning to play the Helicopter Game - Keras based!**\r\n\r\n- project page: [http://dandxy89.github.io/rf_helicopter/](http://dandxy89.github.io/rf_helicopter/)\r\n- github: [https://github.com/dandxy89/rf_helicopter](https://github.com/dandxy89/rf_helicopter)\r\n\r\n**Playing Mario with Deep Reinforcement Learning**\r\n\r\n- github: [https://github.com/aleju/mario-ai](https://github.com/aleju/mario-ai)\r\n\r\n**Deep Attention Recurrent Q-Network**\r\n\r\n- intro: Deep Reinforcement Learning Workshop, NIPS 2015. DeepHack Game\r\n- arxiv: [https://arxiv.org/abs/1512.01693](https://arxiv.org/abs/1512.01693)\r\n- github: [https://github.com/5vision/DARQN](https://github.com/5vision/DARQN)\r\n\r\n**Deep Reinforcement Learning in TensorFlow**\r\n\r\n- intro: TensorFlow implementation of Deep Reinforcement Learning papers\r\n- github: [https://github.com/carpedm20/deep-rl-tensorflow](https://github.com/carpedm20/deep-rl-tensorflow)\r\n\r\n**rltorch: A RL package for Torch that can also be used with openai gym**\r\n\r\n- github: [https://github.com/ludc/rltorch](https://github.com/ludc/rltorch)\r\n\r\n**deep_q_rl: Theano-based implementation of Deep Q-learning**\r\n\r\n- github: [https://github.com/spragunr/deep_q_rl](https://github.com/spragunr/deep_q_rl)\r\n\r\n**Reinforcement-trading**\r\n\r\n- intro: This project uses reinforcement learning on stock market and agent tries to learn trading. \r\nThe goal is to check if the agent can learn to read tape. \r\nThe project is dedicated to hero in life great Jesse Livermore.\r\n- github: [https://github.com/deependersingla/deep_trader](https://github.com/deependersingla/deep_trader)\r\n\r\n**dist-dqnï¼šDistributed Reinforcement Learning using Deep Q-Network in TensorFlow**\r\n\r\n- github: [https://github.com/viswanathgs/dist-dqn](https://github.com/viswanathgs/dist-dqn)\r\n\r\n**Deep Reinforcement Learning for Keras**\r\n\r\n- github: [https://github.com/matthiasplappert/keras-rl](https://github.com/matthiasplappert/keras-rl)\r\n\r\n**RL4J: Reinforcement Learning for the JVM**\r\n\r\n- intro: Reinforcement learning framework integrated with deeplearning4j.\r\n- github: [https://github.com/deeplearning4j/rl4j](https://github.com/deeplearning4j/rl4j)\r\n\r\n**Teaching Your Computer To Play Super Mario Bros. â€“ A Fork of the Google DeepMind Atari Machine Learning Project**\r\n\r\n- blog: [http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/](http://www.ehrenbrav.com/2016/08/teaching-your-computer-to-play-super-mario-bros-a-fork-of-the-google-deepmind-atari-machine-learning-project/)\r\n- github: [https://github.com/ehrenbrav/DeepQNetwork](https://github.com/ehrenbrav/DeepQNetwork)\r\n\r\n**dprl: Deep reinforcement learning package for torch7**\r\n\r\n- github: [https://github.com/PoHsunSu/dprl](https://github.com/PoHsunSu/dprl)\r\n\r\n**Reinforcement Learning for Torch: Introducing torch-twrl**\r\n\r\n- blog: [https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl](https://blog.twitter.com/2016/reinforcement-learning-for-torch-introducing-torch-twrl)\r\n- github: [https://github.com/twitter/torch-twrl](https://github.com/twitter/torch-twrl)\r\n\r\n**Alpha Toe - Using Deep learning to master Tic-Tac-Toe - Daniel Slater**\r\n\r\n- blog: [http://www.danielslater.net/2016/10/alphatoe.html](http://www.danielslater.net/2016/10/alphatoe.html)\r\n- youtube: [https://www.youtube.com/watch?v=Meb5hApAnj4](https://www.youtube.com/watch?v=Meb5hApAnj4)\r\n- github: [https://github.com/DanielSlater/AlphaToe](https://github.com/DanielSlater/AlphaToe)\r\n\r\n**Tensorflow-Reinforce: Implementation of Reinforcement Learning Models in Tensorflow**\r\n\r\n- github: [https://github.com/yukezhu/tensorflow-reinforce](https://github.com/yukezhu/tensorflow-reinforce)\r\n\r\n**deep RL hacking on minecraft with malmo**\r\n\r\n- github: [https://github.com/matpalm/malmomo](https://github.com/matpalm/malmomo)\r\n\r\n**ReinforcementLearning**\r\n\r\n- intro: MC control, Q-learning, SARSA, Cross Entropy Method\r\n- github: [https://github.com/janivanecky/ReinforcementLearning](https://github.com/janivanecky/ReinforcementLearning)\r\n\r\n**markovjs: Reinforcement Learning in JavaScript**\r\n\r\n- github: [https://github.com/lsunsi/markovjs](https://github.com/lsunsi/markovjs)\r\n\r\n**Deep Q: Deep reinforcement learning with TensorFlow**\r\n\r\n- github: [https://github.com/tobegit3hub/deep_q](https://github.com/tobegit3hub/deep_q)\r\n\r\n**Deep Q-Learning Network in pytorch**\r\n\r\n[https://github.com/transedward/pytorch-dqn](https://github.com/transedward/pytorch-dqn)\r\n\r\n**Tensorflow-RL: Implementations of deep RL papers and random experimentation**\r\n\r\n[https://github.com/steveKapturowski/tensorflow-rl](https://github.com/steveKapturowski/tensorflow-rl)\r\n\r\n**Minimal and Clean Reinforcement Learning Examples**\r\n\r\n[https://github.com/rlcode/reinforcement-learning](https://github.com/rlcode/reinforcement-learning)\r\n\r\n**DeepRL: Highly modularized implementation of popular deep RL algorithms by PyTorch**\r\n\r\n[https://github.com/ShangtongZhang/DeepRL](https://github.com/ShangtongZhang/DeepRL)\r\n\r\n## Autonomous vehicle navigation\r\n\r\n**Self-Driving-Car-AI**\r\n\r\n- intro: A simple self-driving car AI python script using the deep Q-learning algorithm\r\n- github: [https://github.com//JianyangZhang/Self-Driving-Car-AI](https://github.com//JianyangZhang/Self-Driving-Car-AI)\r\n\r\n**Autonomous vehicle navigation based on Deep Reinforcement Learning**\r\n\r\n[https://github.com//kaihuchen/DRL-AutonomousVehicles](https://github.com//kaihuchen/DRL-AutonomousVehicles)\r\n\r\n**Car Racing using Reinforcement Learning**\r\n\r\n- intro: Stanford University\r\n- paper: [https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf](https://web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf)\r\n\r\n## Play Flappy Bird\r\n\r\n**Using Deep Q-Network to Learn How To Play Flappy Bird**\r\n\r\n- github: [https://github.com/yenchenlin/DeepLearningFlappyBird](https://github.com/yenchenlin/DeepLearningFlappyBird)\r\n\r\n**Playing Flappy Bird Using Deep Reinforcement Learning (Based on Deep Q Learning DQN using Tensorflow)**\r\n\r\n- blog: [http://blog.csdn.net/songrotek/article/details/50951537](http://blog.csdn.net/songrotek/article/details/50951537)\r\n- github: [https://github.com/songrotek/DRL-FlappyBird](https://github.com/songrotek/DRL-FlappyBird)\r\n\r\n**Playing Flappy Bird Using Deep Reinforcement Learning (Based on Deep Q Learning DQN)**\r\n\r\n- github: [https://github.com/li-haoran/DRL-FlappyBird](https://github.com/li-haoran/DRL-FlappyBird)\r\n\r\n**MXNET-Scala Playing Flappy Bird Using Deep Reinforcement Learning**\r\n\r\n- github: [https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/DRLFlappyBird](https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/DRLFlappyBird)\r\n\r\n**Flappy Bird Bot using Reinforcement Learning in Python**\r\n\r\n- github: [https://github.com/chncyhn/flappybird-qlearning-bot](https://github.com/chncyhn/flappybird-qlearning-bot)\r\n\r\n**Using Keras and Deep Q-Network to Play FlappyBird**\r\n\r\n- blog: [https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html](https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html)\r\n- github: [https://github.com/yanpanlau/Keras-FlappyBird](https://github.com/yanpanlau/Keras-FlappyBird)\r\n\r\n# Pong\r\n\r\n**Building a Pong playing AI in just 1 hour(plus 4 days training...)**\r\n\r\n- sildes: [https://speakerdeck.com/danielslater/building-a-pong-ai](https://speakerdeck.com/danielslater/building-a-pong-ai)\r\n- github: [https://github.com/DanielSlater/PyDataLondon2016](https://github.com/DanielSlater/PyDataLondon2016)\r\n- youtube: [https://www.youtube.com/watch?v=n8NdT_3y9oY](https://www.youtube.com/watch?v=n8NdT_3y9oY)\r\n\r\n**Pong Neural Network(LIVE)**\r\n\r\n- youtube: [https://www.youtube.com/watch?v=Hqf__FlRlzg](https://www.youtube.com/watch?v=Hqf__FlRlzg)\r\n- github: [https://github.com/llSourcell/pong_neural_network_live](https://github.com/llSourcell/pong_neural_network_live)\r\n\r\n## Tips and Tricks\r\n\r\n**DeepRLHacks**\r\n\r\n- intro: The Nuts and Bolts of Deep RL Research\r\n- github: [https://github.com/williamFalcon/DeepRLHacks](https://github.com/williamFalcon/DeepRLHacks)\r\n\r\n# Library\r\n\r\n**BURLAP: Brown-UMBC Reinforcement Learning and Planning (BURLAP) java code library**\r\n\r\n- intro: for the use and development of single or multi-agent planning and learning algorithms and domains to accompany them\r\n- homepage: [http://burlap.cs.brown.edu/](http://burlap.cs.brown.edu/)\r\n\r\n**AgentNet: Deep Reinforcement Learning library for humans**\r\n\r\n![](https://camo.githubusercontent.com/5593b2c8184c4bc08372f919063e826d9bcc2c67/687474703a2f2f7333332e706f7374696d672e6f72672f79747836336b7763762f7768617469735f6167656e746e65745f706e672e706e67)\r\n\r\n- intro: A lightweight library to build and train deep reinforcement learning and custom recurrent networks using Theano+Lasagne \r\n- github: [https://github.com/yandexdataschool/AgentNet](https://github.com/yandexdataschool/AgentNet)\r\n\r\n**Atari Multitask & Transfer Learning Benchmark (AMTLB)**\r\n\r\n- intro: Atari gauntlet for RL agents\r\n- project page: [http://ai-on.org/projects/multitask-and-transfer-learning.html](http://ai-on.org/projects/multitask-and-transfer-learning.html)\r\n- github: [https://github.com/deontologician/atari_multitask](https://github.com/deontologician/atari_multitask)\r\n\r\n**Coach: a python reinforcement learning research framework containing implementation of many state-of-the-art algorithms**\r\n\r\n- intro: Reinforcement Learning Coach by IntelÂ® Nervanaâ„¢ enables easy experimentation with state of the art Reinforcement Learning algorithms\r\n- homepage: [http://coach.nervanasys.com/](http://coach.nervanasys.com/)\r\n- github: [https://github.com/NervanaSystems/coach](https://github.com/NervanaSystems/coach)\r\n\r\n# Blogs\r\n\r\n**Reinforcement learningâ€™s foundational flaw**\r\n\r\n[https://thegradient.pub/why-rl-is-flawed/](https://thegradient.pub/why-rl-is-flawed/)\r\n\r\n**A Short Introduction To Some Reinforcement Learning Algorithms**\r\n\r\n[http://webdocs.cs.ualberta.ca/~vanhasse/rl_algs/rl_algs.html](http://webdocs.cs.ualberta.ca/~vanhasse/rl_algs/rl_algs.html)\r\n\r\n**A Painless Q-Learning Tutorial**\r\n\r\n[http://mnemstudio.org/path-finding-q-learning-tutorial.htm](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)\r\n\r\n- - -\r\n\r\n**Reinforcement Learning - Part 1**\r\n\r\n[http://outlace.com/Reinforcement-Learning-Part-1/](http://outlace.com/Reinforcement-Learning-Part-1/)\r\n\r\n**Reinforcement Learning - Monte Carlo Methods**\r\n\r\n[http://outlace.com/Reinforcement-Learning-Part-2/](http://outlace.com/Reinforcement-Learning-Part-2/)\r\n\r\n**Q-learning with Neural Networks**\r\n\r\n[http://outlace.com/Reinforcement-Learning-Part-3/](http://outlace.com/Reinforcement-Learning-Part-3/)\r\n\r\n- - -\r\n\r\n**Guest Post (Part I): Demystifying Deep Reinforcement Learning**\r\n\r\n[http://www.nervanasys.com/demystifying-deep-reinforcement-learning/](http://www.nervanasys.com/demystifying-deep-reinforcement-learning/)\r\n\r\n**Using reinforcement learning in Python to teach a virtual car to avoid obstacles: An experiment in Q-learning, neural networks and Pygame.**\r\n\r\n![](https://cdn-images-1.medium.com/max/800/1*Ht0KPSlYVsLUp-wqr-ab7Q.png)\r\n\r\n- blog: [https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.p8ug6snri](https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.p8ug6snri)\r\n- github: [https://github.com/harvitronix/reinforcement-learning-car](https://github.com/harvitronix/reinforcement-learning-car)\r\n\r\n**Reinforcement learning in Python to teach a virtual car to avoid obstaclesâ€Šâ€”â€Špart 2**\r\n\r\n[https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.i0o643m1h](https://medium.com/@harvitronix/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238#.i0o643m1h)\r\n\r\n**Some Reinforcement Learning Algorithms in Python, C++**\r\n\r\n- pan: [http://pan.baidu.com/s/1mhcYf3M#path=%252FImplementations%2520of%2520Some%2520Reinforcement%2520Learning%2520Algorithms](http://pan.baidu.com/s/1mhcYf3M#path=%252FImplementations%2520of%2520Some%2520Reinforcement%2520Learning%2520Algorithms)\r\n\r\n**learning to do laps with reinforcement learning and neural nets**\r\n\r\n![](http://matpalm.com/blog/imgs/2016/drivebot/walk.gif)\r\n![](http://matpalm.com/blog/imgs/2016/drivebot/runt.jpg)\r\n\r\n- blog: [http://matpalm.com/blog/drivebot/](http://matpalm.com/blog/drivebot/)\r\n- github: [https://github.com/matpalm/drivebot](https://github.com/matpalm/drivebot)\r\n\r\n**Get a taste of reinforcement learningâ€Šâ€”â€Šimplement a tic tac toe agent**\r\n\r\n![](https://cdn-images-1.medium.com/max/800/1*Ntrov1dzaerfesi9vRKdow.gif)\r\n\r\n[https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4#.59bx71a2h](https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4#.59bx71a2h)\r\n\r\n**Best reinforcement learning libraries?**\r\n\r\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/](https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/)\r\n\r\n**Super Simple Reinforcement Learning Tutorial**\r\n\r\n- part 1: [https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149)\r\n- part 2: [https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.dyhxww1u6](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.dyhxww1u6)\r\n- part 3: [https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.r4c7i7tjq](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99#.r4c7i7tjq)\r\n- gist: [https://gist.github.com/awjuliani/16608e1c4968baaa692b9b8c7dd94d04](https://gist.github.com/awjuliani/16608e1c4968baaa692b9b8c7dd94d04)\r\n\r\n**Reinforcement Learning in Python**\r\n\r\n- github: [https://github.com/NathanEpstein/pydata-reinforce](https://github.com/NathanEpstein/pydata-reinforce)\r\n\r\n**The Skynet Salesman**\r\n\r\n![](http://multithreaded.stitchfix.com/assets/posts/2016-07-18-skynet-salesman/np4_5_w.gif)\r\n\r\n- keyworkds: traveling salesman problem (TSP), deep Q learning\r\n- blog: [http://multithreaded.stitchfix.com/blog/2016/07/21/skynet-salesman/](http://multithreaded.stitchfix.com/blog/2016/07/21/skynet-salesman/)\r\n- github: [https://github.com/jn2clark/ReinforcementLearning/tree/master/DeepQ](https://github.com/jn2clark/ReinforcementLearning/tree/master/DeepQ)\r\n\r\n**Apprenticeship learning using Inverse Reinforcement Learning**\r\n\r\n![](https://jangirrishabh.github.io/assets/IRL/irl_des.png)\r\n\r\n- blog: [https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/](https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/)\r\n- github: [https://github.com/jangirrishabh/toyCarIRL](https://github.com/jangirrishabh/toyCarIRL)\r\n\r\n**Reinforcement Learning and DQN, learning to play from pixels**\r\n\r\n- blog: [https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html](https://rubenfiszel.github.io/posts/rl4j/2016-08-24-Reinforcement-Learning-and-DQN.html)\r\n\r\n**Deep Learning in a Nutshell: Reinforcement Learning**\r\n\r\n[https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/)\r\n\r\n**Write an AI to win at Pong from scratch with Reinforcement Learning**\r\n\r\n[https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0#.n1pgn9chr](https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0#.n1pgn9chr)\r\n\r\n**Learning Reinforcement Learning (with Code, Exercises and Solutions)**\r\n\r\n- blog: [http://www.wildml.com/2016/10/learning-reinforcement-learning/](http://www.wildml.com/2016/10/learning-reinforcement-learning/)\r\n- github: [https://github.com/dennybritz/reinforcement-learning](https://github.com/dennybritz/reinforcement-learning)\r\n\r\n**Deep Reinforcement Learning: Playing a Racing Game**\r\n\r\n[https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html](https://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html)\r\n\r\n**Experimenting with Reinforcement Learning and Active Inference**\r\n\r\n- blog: [http://www.araya.org/archives/955](http://www.araya.org/archives/955)\r\n- github: [https://github.com/arayabrain/BinarySearchLSTM](https://github.com/arayabrain/BinarySearchLSTM)\r\n\r\n**Deep reinforcement learning, battleship**\r\n\r\n- blog: [http://efavdb.com/battleship/](http://efavdb.com/battleship/)\r\n- github: [https://github.com/EFavDB/battleship](https://github.com/EFavDB/battleship)\r\n\r\n**Deep Learning Research Review Week 2: Reinforcement Learning**\r\n\r\n[https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning)\r\n\r\n**Reinforcement Learning: Artificial Intelligence in Game Playing**\r\n\r\n[https://medium.com/@pavelkordik/reinforcement-learning-the-hardest-part-of-machine-learning-b667a22995ca#.jjiitflok](https://medium.com/@pavelkordik/reinforcement-learning-the-hardest-part-of-machine-learning-b667a22995ca#.jjiitflok)\r\n\r\n**Artificial Intelligenceâ€™s Next Big Step: Reinforcement Learning**\r\n\r\n[http://thenewstack.io/reinforcement-learning-ready-real-world/](http://thenewstack.io/reinforcement-learning-ready-real-world/)\r\n\r\n## Letâ€™s make a DQN\r\n\r\n**Letâ€™s make a DQN**\r\n\r\n- Theory: [https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/](https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/)\r\n- Implementation: [https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/](https://jaromiru.com/2016/10/03/lets-make-a-dqn-implementation/)\r\n- Debugging: [https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/](https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/)\r\n- Full DQN: [https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/](https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/)\r\n- github: [https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py](https://github.com/jaara/AI-blog/blob/master/CartPole-basic.py)\r\n\r\n# Books\r\n\r\n**Reinforcement Learning: State-of-the-Art**\r\n\r\n- intro: \"The main goal of this book is to present an up-to-date series of survey articles \r\non the main contemporary sub-fields of reinforcement learning. \r\nThis includes surveys on partially observable environments, hierarchical task decompositions, \r\nrelational knowledge representation and predictive state representations. \r\nFurthermore, topics such as transfer, evolutionary methods and continuous spaces in reinforcement learning are surveyed. \r\nIn addition, several chapters review reinforcement learning methods in robotics, in games, and in computational neuroscience. \r\nIn total seventeen different subfields are presented by mostly young experts in those areas, \r\nand together they truly represent a state-of-the-art of current reinforcement learning research.\"\r\n- book: [http://www.springer.com/gp/book/9783642276446#](http://www.springer.com/gp/book/9783642276446#)\r\n\r\n**Reinforcement Learning: An Introduction**\r\n\r\n- github: [https://github.com/Mononofu/reinforcement-learning](https://github.com/Mononofu/reinforcement-learning)\r\n- homepage: [http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html](http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)\r\n- course: [http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html](http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html)\r\n- book(1st edition): [http://pan.baidu.com/s/1jkaMq](http://pan.baidu.com/s/1jkaMq)\r\n- book(2rd edition): [http://pan.baidu.com/s/1dDnNEnR](http://pan.baidu.com/s/1dDnNEnR)\r\n\r\n**Reinforcement Learning: An Introduction (Second edition, Draft)**\r\n\r\n- book: [https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)\r\n- mirror: [https://pan.baidu.com/s/1slrMYkP](https://pan.baidu.com/s/1slrMYkP)\r\n- github: [https://github.com/ShangtongZhang/reinforcement-learning-an-introduction](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)\r\n\r\n**The Self Learning Quant**\r\n\r\n- intro: explain and show the concept of self reinforcement learning combined with a neural network\r\n- blog: [https://medium.com/@danielzakrisson/the-self-learning-quant-d3329fcc9915#.9lsa5rh3e](https://medium.com/@danielzakrisson/the-self-learning-quant-d3329fcc9915#.9lsa5rh3e)\r\n- gihtub: [https://github.com/danielzak/sl-quant](https://github.com/danielzak/sl-quant)\r\n\r\n**Reinforcement Learning: An Introduction**\r\n\r\n- author: Richard S. Sutton and Andrew G. Barto\r\n- book: [https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html)\r\n- solutions: [https://github.com/btaba/intro-to-rl](https://github.com/btaba/intro-to-rl)\r\n\r\n# Resources\r\n\r\n**Deep Reinforcement Learning Papers**\r\n\r\n[https://github.com/junhyukoh/deep-reinforcement-learning-papers](https://github.com/junhyukoh/deep-reinforcement-learning-papers)\r\n\r\n**Awesome Reinforcement Learning**\r\n\r\n- website: [http://aikorea.org/awesome-rl/?utm_content=buffer5d0f3&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer#online-demos](http://aikorea.org/awesome-rl/?utm_content=buffer5d0f3&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer#online-demos)\r\n- github: [https://github.com/aikorea/awesome-rl](https://github.com/aikorea/awesome-rl)\r\n\r\n**Deep Reinforcement Learning Papers**\r\n\r\n- github: [https://github.com/muupan/deep-reinforcement-learning-papers](https://github.com/muupan/deep-reinforcement-learning-papers)\r\n\r\n**Deep Reinforcement Learning æ·±åº¦å¢žå¼ºå­¦ä¹ èµ„æº**\r\n\r\n- blog: [https://zhuanlan.zhihu.com/p/20885568](https://zhuanlan.zhihu.com/p/20885568)\r\n\r\n**deep-reinforcement-learning-networks: A list of deep neural network architectures for reinforcement learning tasks**\r\n\r\n- github: [https://github.com/5vision/deep-reinforcement-learning-networks](https://github.com/5vision/deep-reinforcement-learning-networks)\r\n\r\n**Deep Reinforcement Learning survey**\r\n\r\n- github: [https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey](https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey)\r\n\r\n**Studying Reinforcement Learning Guide**\r\n\r\n- github: [https://github.com/0bserver07/Study-Reinforcement-Learning](https://github.com/0bserver07/Study-Reinforcement-Learning)\r\n\r\n# Reading and Questions\r\n\r\n**What are the best books about reinforcement learning?**\r\n\r\n[https://www.quora.com/What-are-the-best-books-about-reinforcement-learning](https://www.quora.com/What-are-the-best-books-about-reinforcement-learning)\r\n","excerpt":"Tutorials Demystifying Deep Reinforcement Learning (Part1) http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/ Deep Reinforcemeâ€¦","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations â†’ Principles â†’ The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]},{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between â€˜privacyâ€™ and â€˜dignityâ€™.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Embed Link","url":"/old-work-archives/2018-webizen-net-au/embed-link/","items":[]},{"title":"Posts","url":"/old-work-archives/2018-webizen-net-au/posts/","items":[]},{"title":"Privacy Policy","url":"/old-work-archives/2018-webizen-net-au/privacy-policy/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Pro Summary","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","lastUpdatedAt":"2022-12-28T19:34:43.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","title":"Fake News: Considerations â†’ Principles â†’ The Institution of Socio &#8211; Economic Values","lastUpdatedAt":"2022-12-28T19:29:53.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","title":"Handong1587","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","title":"Notes on Suffix Array and Manacher Algorithm","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","title":"Notes On Perceptrons","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","title":"Notes On Object Detection","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","title":"Notes On Caffe Development","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","title":"Notes On L-BFGS","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","title":"Softmax Vs Logistic Vs Sigmoid","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","title":"Notes On Deep Learning Training","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}