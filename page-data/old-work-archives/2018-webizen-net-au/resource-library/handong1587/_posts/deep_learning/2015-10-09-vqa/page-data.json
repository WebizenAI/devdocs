{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/",
    "result": {"data":{"mdx":{"id":"4172604a-5969-5624-9b6e-a9d0f4dd43ff","tableOfContents":{"items":[{"url":"#video-question-answering","title":"Video Question Answering"},{"url":"#projects","title":"Projects"},{"url":"#dataset","title":"Dataset"},{"url":"#resources","title":"Resources"}]},"fields":{"title":"Visual Question Answering","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa.md","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"Visual Question Answering","description":null,"imageAlt":null,"tags":[],"date":"2015-10-09T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"deep_learning\",\n  \"title\": \"Visual Question Answering\",\n  \"date\": \"2015-10-09T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1502.05698v1\"\n  }, \"http://arxiv.org/abs/1502.05698v1\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/facebook/bAbI-tasks\"\n  }, \"https://github.com/facebook/bAbI-tasks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VQA: Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.00468\"\n  }, \"http://arxiv.org/abs/1505.00468\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://visualqa.org/\"\n  }, \"http://visualqa.org/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Ask Your Neurons: A Neural-based Approach to Answering Questions about Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.01121\"\n  }, \"http://arxiv.org/abs/1505.01121\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/\"\n  }, \"https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1\"\n  }, \"https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Exploring Models and Data for Image Question Answering\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/ac498616bb6ea1db7aaabb1cf567d07e4bbef395/687474703a2f2f692e696d6775722e636f6d2f4a7669787832572e6a7067\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.02074\"\n  }, \"http://arxiv.org/abs/1505.02074\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gtihub(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/paarthneekhara/neural-vqa-tensorflow\"\n  }, \"https://github.com/paarthneekhara/neural-vqa-tensorflow\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Python+Keras): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ayushoriginal/NeuralNetwork-ImageQA\"\n  }, \"https://github.com/ayushoriginal/NeuralNetwork-ImageQA\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.05612\"\n  }, \"http://arxiv.org/abs/1505.05612\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Teaching Machines to Read and Comprehend\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1506.03340\"\n  }, \"http://arxiv.org/abs/1506.03340\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deepmind/rc-data\"\n  }, \"https://github.com/deepmind/rc-data\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Theano/Blocks): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend\"\n  }, \"https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Tensorflow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/carpedm20/attentive-reader-tensorflow\"\n  }, \"https://github.com/carpedm20/attentive-reader-tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Module Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.02799\"\n  }, \"http://arxiv.org/abs/1511.02799\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jacobandreas/nmn2\"\n  }, \"https://github.com/jacobandreas/nmn2\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction\")), mdx(\"img\", {\n    \"src\": \"http://cvlab.postech.ac.kr/research/dppnet/images/figure2.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.05756\"\n  }, \"http://arxiv.org/abs/1511.05756\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/HyeonwooNoh/DPPnet\"\n  }, \"https://github.com/HyeonwooNoh/DPPnet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cvlab.postech.ac.kr/research/dppnet/\"\n  }, \"http://cvlab.postech.ac.kr/research/dppnet/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Generative Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.01337\"\n  }, \"http://arxiv.org/abs/1512.01337\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stacked Attention Networks for Image Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.02274\"\n  }, \"http://arxiv.org/abs/1511.02274\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/abhshkdz/neural-vqa-attention\"\n  }, \"https://github.com/abhshkdz/neural-vqa-attention\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.05234\"\n  }, \"http://arxiv.org/abs/1511.05234\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Simple Baseline for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research. Bag-of-word\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.02167\"\n  }, \"http://arxiv.org/abs/1512.02167\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/metalbubble/VQAbaseline\"\n  }, \"https://github.com/metalbubble/VQAbaseline\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://visualqa.csail.mit.edu/\"\n  }, \"http://visualqa.csail.mit.edu/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MovieQA: Understanding Stories in Movies through Question-Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://movieqa.cs.toronto.edu/home/\"\n  }, \"http://movieqa.cs.toronto.edu/home/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.02902\"\n  }, \"http://arxiv.org/abs/1512.02902\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gtihub: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\"\n  }, \"https://github.com/makarandtapaswi/MovieQA_CVPR2016/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deeper LSTM+ normalized CNN for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"This current code can get 58.16 on Open-Ended and 63.09 on Multiple-Choice on test-standard split\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/VT-vision-lab/VQA_LSTM_CNN\"\n  }, \"https://github.com/VT-vision-lab/VQA_LSTM_CNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Neural Network for Factoid Question Answering over Paragraphs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.umd.edu/~miyyer/qblearn/\"\n  }, \"http://cs.umd.edu/~miyyer/qblearn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf\"\n  }, \"https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code+data: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz\"\n  }, \"https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Compose Neural Networks for Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NAACL 2016 Best paper\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1601.01705\"\n  }, \"http://arxiv.org/abs/1601.01705\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generating Natural Questions About an Image\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.06059\"\n  }, \"http://arxiv.org/abs/1603.06059\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Question Answering on Freebase via Relation Extraction and Textual Evidence\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACL 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1603.00957\"\n  }, \"https://arxiv.org/abs/1603.00957\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/syxu828/QuestionAnsweringOverFB\"\n  }, \"https://github.com/syxu828/QuestionAnsweringOverFB\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.06807\"\n  }, \"http://arxiv.org/abs/1603.06807\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Character-Level Question Answering with Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.00727\"\n  }, \"http://arxiv.org/abs/1604.00727\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"comment(by @Wenpeng_Yin): \\\"fancy model with minor improvement\\\"\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Focused Dynamic Attention Model for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.01485\"\n  }, \"http://arxiv.org/abs/1604.01485\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual Question Answering Literature Survey\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://iamaaditya.github.io/research/literature/\"\n  }, \"http://iamaaditya.github.io/research/literature/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The DIY Guide to Visual Question Answering\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/53c28e13bd645acbf49c9e71e82a36202d1981bc/687474703a2f2f7333322e706f7374696d672e6f72672f77636a6c7a7a7532742f53637265656e5f53686f745f323031365f30355f30385f61745f325f34325f30375f504d2e706e67\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md\"\n  }, \"https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Question Answering via Integer Programming over Semi-Structured Knowledge\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.06076\"\n  }, \"http://arxiv.org/abs/1604.06076\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/allenai/tableilp\"\n  }, \"https://github.com/allenai/tableilp\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=7NS53icQRrs\"\n  }, \"https://www.youtube.com/watch?v=7NS53icQRrs\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Question-Image Co-Attention for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.00061\"\n  }, \"http://arxiv.org/abs/1606.00061\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jiasenlu/HieCoAttenVQA\"\n  }, \"https://github.com/jiasenlu/HieCoAttenVQA\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multimodal Residual Learning for Visual QA\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.01455\"\n  }, \"http://arxiv.org/abs/1606.01455\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Simple Question Answering by Attentive Convolutional Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.03391\"\n  }, \"http://arxiv.org/abs/1606.03391\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?\")), mdx(\"img\", {\n    \"src\": \"https://computing.ece.vt.edu/~abhshkdz/vqa-hat/img/att_comparison_2row.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://computing.ece.vt.edu/~abhshkdz/vqa-hat/\"\n  }, \"https://computing.ece.vt.edu/~abhshkdz/vqa-hat/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.03556\"\n  }, \"http://arxiv.org/abs/1606.03556\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Simple and Effective Question Answering with Recurrent Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.05029\"\n  }, \"http://arxiv.org/abs/1606.05029\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Analyzing the Behavior of Visual Question Answering Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.07356\"\n  }, \"http://arxiv.org/abs/1606.07356\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.01847\"\n  }, \"https://arxiv.org/abs/1606.01847\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/akirafukui/vqa-mcb\"\n  }, \"https://github.com/akirafukui/vqa-mcb\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Language Modeling for Question Answering using Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://benjaminbolte.com/blog/2016/keras-language-modeling.html\"\n  }, \"http://benjaminbolte.com/blog/2016/keras-language-modeling.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/codekansas/keras-language-modeling\"\n  }, \"https://github.com/codekansas/keras-language-modeling\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Interpreting Visual Question Answering Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.08974\"\n  }, \"http://arxiv.org/abs/1608.08974\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: FSVQA\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.06657\"\n  }, \"http://arxiv.org/abs/1609.06657\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tutorial on Answering Questions about Images with Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The tutorial was presented at '2nd Summer School on Integrating Vision and Language: Deep Learning' in Malta, 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.01076\"\n  }, \"https://arxiv.org/abs/1610.01076\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hadamard Product for Low-rank Bilinear Pooling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.04325\"\n  }, \"https://arxiv.org/abs/1610.04325\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jnhwkim/MulLowBiVQA\"\n  }, \"https://github.com/jnhwkim/MulLowBiVQA\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Open-Ended Visual Question-Answering\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/model.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Bachelor thesis report graded with A with honours at ETSETB Telecom BCN school, Universitat Polit\", \"`\", \"ecnica de Catalunya (UPC). June 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://imatge-upc.github.io/vqa-2016-cvprw/\"\n  }, \"http://imatge-upc.github.io/vqa-2016-cvprw/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.02692\"\n  }, \"https://arxiv.org/abs/1610.02692\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.slideshare.net/xavigiro/openended-visual-questionanswering\"\n  }, \"http://www.slideshare.net/xavigiro/openended-visual-questionanswering\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/imatge-upc/vqa-2016-cvprw\"\n  }, \"https://github.com/imatge-upc/vqa-2016-cvprw\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning for Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UMD. Mohit Iyyer.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Recurrent Neural Networks, Recursive Neural Network\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.umd.edu/~miyyer/data/deepqa.pdf\"\n  }, \"http://cs.umd.edu/~miyyer/data/deepqa.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dual Attention Networks for Multimodal Reasoning and Matching\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.00471\"\n  }, \"https://arxiv.org/abs/1611.00471\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamic Coattention Networks For Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01604\"\n  }, \"https://arxiv.org/abs/1611.01604\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"State of the art deep learning model for question answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/\"\n  }, \"http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Zero-Shot Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05546\"\n  }, \"https://arxiv.org/abs/1611.05546\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Rochester & Microsoft & University College London\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.08251\"\n  }, \"https://arxiv.org/abs/1701.08251\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Question Answering through Transfer Learning from Large Fine-grained Supervision Data\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Seoul National University & University of Washington\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.02171\"\n  }, \"https://arxiv.org/abs/1702.02171\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Question Answering from Unstructured Text by Retrieval and Comprehension\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.08885\"\n  }, \"https://arxiv.org/abs/1703.08885\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://theneuralperspective.com/2017/04/26/question-answering-from-unstructured-text-by-retrieval-and-comprehension/\"\n  }, \"https://theneuralperspective.com/2017/04/26/question-answering-from-unstructured-text-by-retrieval-and-comprehension/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.03162\"\n  }, \"https://arxiv.org/abs/1704.03162\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Reason: End-to-End Module Networks for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UC Berkeley, Boston University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.05526\"\n  }, \"https://arxiv.org/abs/1704.05526\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017.Seoul National University & Yahoo Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.04497\"\n  }, \"https://arxiv.org/abs/1704.04497\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/YunseokJANG/tgif-qa\"\n  }, \"https://github.com/YunseokJANG/tgif-qa\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACL 2017 (short)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://rajarshd.github.io/TextKBQA/\"\n  }, \"https://rajarshd.github.io/TextKBQA/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.08384\"\n  }, \"https://arxiv.org/abs/1704.08384\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/rajarshd/TextKBQA\"\n  }, \"https://github.com/rajarshd/TextKBQA\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Convolutional Text Representations for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.06824\"\n  }, \"https://arxiv.org/abs/1705.06824\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/divelab/vqa-text\"\n  }, \"https://github.com/divelab/vqa-text\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Compact Tensor Pooling for Visual Question Answering\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.06706\"\n  }, \"https://arxiv.org/abs/1706.06706\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Long-Term Memory Networks for Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: SUNY Buffalo & LinkedIn & LinkedIn\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.01961\"\n  }, \"https://arxiv.org/abs/1707.01961\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bottom-Up and Top-Down Attention for Image Captioning and VQA\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Winner of the Visual Question Answering Challenge at CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.panderson.me/up-down-attention/\"\n  }, \"http://www.panderson.me/up-down-attention/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.07998\"\n  }, \"https://arxiv.org/abs/1707.07998\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.panderson.me/images/1707.07998-up-down.pdf\"\n  }, \"http://www.panderson.me/images/1707.07998-up-down.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//peteanderson80/bottom-up-attention\"\n  }, \"https://github.com//peteanderson80/bottom-up-attention\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Structured Attentions for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.02071\"\n  }, \"https://arxiv.org/abs/1708.02071\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhuchen03/vqa-sva\"\n  }, \"https://github.com/zhuchen03/vqa-sva\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Winner of the 2017 Visual Question Answering (VQA) Challenge at CVPR\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The University of Adelaide & Australian National University & Microsoft Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.02711\"\n  }, \"https://arxiv.org/abs/1708.02711\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MemexQA: Visual Memex Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Carnegie Mellon University, Customer Service AI, Yahoo\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://memexqa.cs.cmu.edu/\"\n  }, \"https://memexqa.cs.cmu.edu/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.01336\"\n  }, \"https://arxiv.org/abs/1708.01336\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Automatic Question-Answering Using A Deep Similarity Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: New York University & AT&T Research Labs\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.01713\"\n  }, \"https://arxiv.org/abs/1708.01713\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Question Dependent Recurrent Entity Network for Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Pisa\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.07922\"\n  }, \"https://arxiv.org/abs/1707.07922\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/andreamad8/QDREN\"\n  }, \"https://github.com/andreamad8/QDREN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual Question Generation as Dual Task of Visual Question Answering\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.07192\"\n  }, \"https://arxiv.org/abs/1709.07192\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Read-Write Memory Network for Movie Story Understanding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.09345\"\n  }, \"https://arxiv.org/abs/1709.09345\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"iVQA: Inverse Visual Question Answering\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1710.03370\"\n  }, \"https://arxiv.org/abs/1710.03370\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DCN+: Mixed Objective and Deep Residual Coattention for Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Salesforce Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.00106\"\n  }, \"https://arxiv.org/abs/1711.00106\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mjacar/tensorflow-dcn-plus\"\n  }, \"https://github.com/mjacar/tensorflow-dcn-plus\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"High-Order Attention Models for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04323\"\n  }, \"https://arxiv.org/abs/1711.04323\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Asking the Difficult Questions: Goal-Oriented Visual Question Generation via Intermediate Rewards\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The University of Adelaide & University of Technology Sydney & Nanjing University of Science and Technology\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.07614\"\n  }, \"https://arxiv.org/abs/1711.07614\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual Question Answering as a Meta Learning Task\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.08105\"\n  }, \"https://arxiv.org/abs/1711.08105\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Embodied Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Georgia Institute of Technology & Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://embodiedqa.org/\"\n  }, \"https://embodiedqa.org/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.11543\"\n  }, \"https://arxiv.org/abs/1711.11543\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/facebookresearch/EmbodiedQA\"\n  }, \"https://github.com/facebookresearch/EmbodiedQA\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning by Asking Questions\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.01238\"\n  }, \"https://arxiv.org/abs/1712.01238\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Interpretable Counting for Visual Question Answering\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.08697\"\n  }, \"https://arxiv.org/abs/1712.08697\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Structured Triplet Learning with POS-tag Guided Attention for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.07853\"\n  }, \"https://arxiv.org/abs/1801.07853\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wangzheallen/STL-VQA\"\n  }, \"https://github.com/wangzheallen/STL-VQA\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DVQA: Understanding Data Visualizations via Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: RIT & Adobe Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.08163\"\n  }, \"https://arxiv.org/abs/1801.08163\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Object-based reasoning in VQA\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: WACV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.09718\"\n  }, \"https://arxiv.org/abs/1801.09718\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dual Recurrent Attention Units for Visual Question Answering\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.00209\"\n  }, \"https://arxiv.org/abs/1802.00209\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Differential Attention for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.00298\"\n  }, \"https://arxiv.org/abs/1804.00298\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Question Type Guided Attention in Visual Question Answering\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.02088\"\n  }, \"https://arxiv.org/abs/1804.02088\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.09412\"\n  }, \"https://arxiv.org/abs/1804.09412\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reciprocal Attention Fusion for Visual Question Answering\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.04247\"\n  }, \"https://arxiv.org/abs/1805.04247\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Count Objects in Natural Images for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.05766\"\n  }, \"https://arxiv.org/abs/1802.05766\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bilinear Attention Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018. Seoul National University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.07932\"\n  }, \"https://arxiv.org/abs/1805.07932\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://bi.snu.ac.kr/~jhkim/slides/bilinear%20attention%20networks_8min.pdf\"\n  }, \"https://bi.snu.ac.kr/~jhkim/slides/bilinear%20attention%20networks_8min.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, PyTorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jnhwkim/ban-vqa\"\n  }, \"https://github.com/jnhwkim/ban-vqa\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reproducibility Report for \\\"Learning To Count Objects In Natural Images For Visual Question Answering\\\"\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.08174\"\n  }, \"https://arxiv.org/abs/1805.08174\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cross-Dataset Adaptation for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.03726\"\n  }, \"https://arxiv.org/abs/1806.03726\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Answer Embeddings for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.03724\"\n  }, \"https://arxiv.org/abs/1806.03724\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Visual Question Answering by Bootstrapping Hard Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"axrxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.00300\"\n  }, \"https://arxiv.org/abs/1808.00300\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Question-Guided Hybrid Convolution for Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.02632\"\n  }, \"https://arxiv.org/abs/1808.02632\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Interpretable Visual Question Answering by Reasoning on Dependency Trees\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.01810\"\n  }, \"https://arxiv.org/abs/1809.01810\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Visual QA Devil in the Details: The Impact of Early Fusion and Batch Norm on CLEVR\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018 Workshop on Shortcomings in Vision and Language\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1809.04482\"\n  }, \"https://arxiv.org/abs/1809.04482\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Knowing Where to Look? Analysis on Attention of Visual Question Answering System\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV SiVL Workshop paper\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.03821\"\n  }, \"https://arxiv.org/abs/1810.03821\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VQA with no questions-answers training\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.08481\"\n  }, \"https://arxiv.org/abs/1811.08481\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual Commonsense R-CNN\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2002.12204\"\n  }, \"https://arxiv.org/abs/2002.12204\"))), mdx(\"h1\", {\n    \"id\": \"video-question-answering\"\n  }, \"Video Question Answering\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepStory: Video Story QA by Deep Embedded Memory Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2017. Seoul National University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.00836\"\n  }, \"https://arxiv.org/abs/1707.00836\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Question Answering via Attribute-Augmented Attention Network Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: SIGIR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.06355\"\n  }, \"https://arxiv.org/abs/1707.06355\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Leveraging Video Descriptions to Learn Video Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.04021\"\n  }, \"https://arxiv.org/abs/1611.04021\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Joint Sequence Fusion Model for Video Question Answering and Retrieval\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arixv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.02559\"\n  }, \"https://arxiv.org/abs/1808.02559\"))), mdx(\"h1\", {\n    \"id\": \"projects\"\n  }, \"Projects\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VQA Demo: Visual Question Answering Demo on pretrained model\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/iamaaditya/VQA_Demo\"\n  }, \"https://github.com/iamaaditya/VQA_Demo\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"ref: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://iamaaditya.github.io/research/\"\n  }, \"http://iamaaditya.github.io/research/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"deep-qa: Implementation of the Convolution Neural Network for factoid QA on the answer sentence selection task\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/aseveryn/deep-qa\"\n  }, \"https://github.com/aseveryn/deep-qa\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"YodaQA: A Question Answering system built on top of the Apache UIMA framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ailao.eu/yodaqa/\"\n  }, \"http://ailao.eu/yodaqa/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/brmson/yodaqa\"\n  }, \"https://github.com/brmson/yodaqa\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"insuranceQA-cnn-lstm: tensorflow and theano cnn code for insurance QA(question Answer matching)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/white127/insuranceQA-cnn-lstm\"\n  }, \"https://github.com/white127/insuranceQA-cnn-lstm\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tensorflow Implementation of Deeper LSTM+ normalized CNN for Visual Question Answering\")), mdx(\"img\", {\n    \"src\": \"https://cloud.githubusercontent.com/assets/19935904/16358326/e6812310-3add-11e6-914f-c61c19d6ab5a.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/JamesChuanggg/VQA-tensorflow\"\n  }, \"https://github.com/JamesChuanggg/VQA-tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual Question Answering with Keras\")), mdx(\"img\", {\n    \"src\": \"https://camo.githubusercontent.com/f52d44199710c8f3939fb182a339d1d6a0b09a3f/687474703a2f2f692e696d6775722e636f6d2f327a4a30396d512e706e67\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/\"\n  }, \"https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering\"\n  }, \"https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning Models for Question Answering with Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html\"\n  }, \"http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GuessWhat?! Visual object discovery through multi-modal dialogue\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Montreal & Univ. Lille & Google DeepMind & Twitter\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.08481\"\n  }, \"https://arxiv.org/abs/1611.08481\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep QA: Using deep learning to answer Aristo's science questions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/allenai/deep_qa\"\n  }, \"https://github.com/allenai/deep_qa\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual Question Answering in Pytorch\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/Cadene/vqa.pytorch\"\n  }, \"https://github.com/Cadene/vqa.pytorch\")), mdx(\"h1\", {\n    \"id\": \"dataset\"\n  }, \"Dataset\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual7W: Grounded Question Answering in Images\")), mdx(\"img\", {\n    \"src\": \"http://web.stanford.edu/~yukez/images/img/visual7w_examples.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.stanford.edu/~yukez/visual7w/\"\n  }, \"http://web.stanford.edu/~yukez/visual7w/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yukezhu/visual7w-toolkit\"\n  }, \"https://github.com/yukezhu/visual7w-toolkit\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yukezhu/visual7w-qa-models\"\n  }, \"https://github.com/yukezhu/visual7w-qa-models\"))), mdx(\"h1\", {\n    \"id\": \"resources\"\n  }, \"Resources\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Awesome Visual Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/JamesChuanggg/awesome-vqa\"\n  }, \"https://github.com/JamesChuanggg/awesome-vqa\"))));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Visual Question Answering\ndate: 2015-10-09\n---\n\n**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks**\n\n- intro: Facebook AI Research\n- arxiv: [http://arxiv.org/abs/1502.05698v1](http://arxiv.org/abs/1502.05698v1)\n- github: [https://github.com/facebook/bAbI-tasks](https://github.com/facebook/bAbI-tasks)\n\n**VQA: Visual Question Answering**\n\n- intro: ICCV 2015\n- arxiv: [http://arxiv.org/abs/1505.00468](http://arxiv.org/abs/1505.00468)\n- homepage: [http://visualqa.org/](http://visualqa.org/)\n\n**Ask Your Neurons: A Neural-based Approach to Answering Questions about Images**\n\n- intro: ICCV 2015\n- arxiv: [http://arxiv.org/abs/1505.01121](http://arxiv.org/abs/1505.01121)\n- project: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)\n- video: [https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1](https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1)\n\n**Exploring Models and Data for Image Question Answering**\n\n![](https://camo.githubusercontent.com/ac498616bb6ea1db7aaabb1cf567d07e4bbef395/687474703a2f2f692e696d6775722e636f6d2f4a7669787832572e6a7067)\n\n- arxiv: [http://arxiv.org/abs/1505.02074](http://arxiv.org/abs/1505.02074)\n- gtihub(Tensorflow): [https://github.com/paarthneekhara/neural-vqa-tensorflow](https://github.com/paarthneekhara/neural-vqa-tensorflow)\n- github(Python+Keras): [https://github.com/ayushoriginal/NeuralNetwork-ImageQA](https://github.com/ayushoriginal/NeuralNetwork-ImageQA)\n\n**Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1505.05612](http://arxiv.org/abs/1505.05612)\n\n**Teaching Machines to Read and Comprehend**\n\n- intro: Google DeepMind\n- arxiv: [http://arxiv.org/abs/1506.03340](http://arxiv.org/abs/1506.03340)\n- github: [https://github.com/deepmind/rc-data](https://github.com/deepmind/rc-data)\n- github(Theano/Blocks): [https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend](https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend)\n- github(Tensorflow): [https://github.com/carpedm20/attentive-reader-tensorflow](https://github.com/carpedm20/attentive-reader-tensorflow)\n\n**Neural Module Networks**\n\n- intro: CVPR 2016\n- arxiv: [http://arxiv.org/abs/1511.02799](http://arxiv.org/abs/1511.02799)\n- github: [https://github.com/jacobandreas/nmn2](https://github.com/jacobandreas/nmn2)\n\n**Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction**\n\n![](http://cvlab.postech.ac.kr/research/dppnet/images/figure2.png)\n\n- arxiv: [http://arxiv.org/abs/1511.05756](http://arxiv.org/abs/1511.05756)\n- github: [https://github.com/HyeonwooNoh/DPPnet](https://github.com/HyeonwooNoh/DPPnet)\n- project page: [http://cvlab.postech.ac.kr/research/dppnet/](http://cvlab.postech.ac.kr/research/dppnet/)\n\n**Neural Generative Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1512.01337](http://arxiv.org/abs/1512.01337)\n\n**Stacked Attention Networks for Image Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1511.02274](http://arxiv.org/abs/1511.02274)\n- github: [https://github.com/abhshkdz/neural-vqa-attention](https://github.com/abhshkdz/neural-vqa-attention)\n\n**Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1511.05234](http://arxiv.org/abs/1511.05234)\n\n**Simple Baseline for Visual Question Answering**\n\n- intro: Facebook AI Research. Bag-of-word\n- arxiv: [http://arxiv.org/abs/1512.02167](http://arxiv.org/abs/1512.02167)\n- github: [https://github.com/metalbubble/VQAbaseline](https://github.com/metalbubble/VQAbaseline)\n- demo: [http://visualqa.csail.mit.edu/](http://visualqa.csail.mit.edu/)\n\n**MovieQA: Understanding Stories in Movies through Question-Answering**\n\n- intro: CVPR 2016\n- project page: [http://movieqa.cs.toronto.edu/home/](http://movieqa.cs.toronto.edu/home/)\n- arxiv: [http://arxiv.org/abs/1512.02902](http://arxiv.org/abs/1512.02902)\n- gtihub: [https://github.com/makarandtapaswi/MovieQA_CVPR2016/](https://github.com/makarandtapaswi/MovieQA_CVPR2016/)\n\n**Deeper LSTM+ normalized CNN for Visual Question Answering**\n\n- intro: \"This current code can get 58.16 on Open-Ended and 63.09 on Multiple-Choice on test-standard split\"\n- github: [https://github.com/VT-vision-lab/VQA_LSTM_CNN](https://github.com/VT-vision-lab/VQA_LSTM_CNN)\n\n**A Neural Network for Factoid Question Answering over Paragraphs**\n\n- project page: [http://cs.umd.edu/~miyyer/qblearn/](http://cs.umd.edu/~miyyer/qblearn/)\n- paper: [https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf](https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf)\n- code+data: [https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz](https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz)\n\n**Learning to Compose Neural Networks for Question Answering**\n\n- intro: NAACL 2016 Best paper\n- arxiv: [http://arxiv.org/abs/1601.01705](http://arxiv.org/abs/1601.01705)\n\n**Generating Natural Questions About an Image**\n\n- arxiv: [http://arxiv.org/abs/1603.06059](http://arxiv.org/abs/1603.06059)\n\n**Question Answering on Freebase via Relation Extraction and Textual Evidence**\n\n- intro: ACL 2016\n- arxiv: [https://arxiv.org/abs/1603.00957](https://arxiv.org/abs/1603.00957)\n- github: [https://github.com/syxu828/QuestionAnsweringOverFB](https://github.com/syxu828/QuestionAnsweringOverFB)\n\n**Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus**\n\n- arxiv: [http://arxiv.org/abs/1603.06807](http://arxiv.org/abs/1603.06807)\n\n**Character-Level Question Answering with Attention**\n\n- arxiv: [http://arxiv.org/abs/1604.00727](http://arxiv.org/abs/1604.00727)\n- comment(by @Wenpeng_Yin): \"fancy model with minor improvement\"\n\n**A Focused Dynamic Attention Model for Visual Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1604.01485](http://arxiv.org/abs/1604.01485)\n\n**Visual Question Answering Literature Survey**\n\n- blog: [http://iamaaditya.github.io/research/literature/](http://iamaaditya.github.io/research/literature/)\n\n**The DIY Guide to Visual Question Answering**\n\n![](https://camo.githubusercontent.com/53c28e13bd645acbf49c9e71e82a36202d1981bc/687474703a2f2f7333322e706f7374696d672e6f72672f77636a6c7a7a7532742f53637265656e5f53686f745f323031365f30355f30385f61745f325f34325f30375f504d2e706e67)\n\n- github: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md)\n\n**Question Answering via Integer Programming over Semi-Structured Knowledge**\n\n- arxiv: [http://arxiv.org/abs/1604.06076](http://arxiv.org/abs/1604.06076)\n- github: [https://github.com/allenai/tableilp](https://github.com/allenai/tableilp)\n- youtube: [https://www.youtube.com/watch?v=7NS53icQRrs](https://www.youtube.com/watch?v=7NS53icQRrs)\n\n**Hierarchical Question-Image Co-Attention for Visual Question Answering**\n\n- arxiv: [http://arxiv.org/abs/1606.00061](http://arxiv.org/abs/1606.00061)\n- github: [https://github.com/jiasenlu/HieCoAttenVQA](https://github.com/jiasenlu/HieCoAttenVQA)\n\n**Multimodal Residual Learning for Visual QA**\n\n- arxiv: [http://arxiv.org/abs/1606.01455](http://arxiv.org/abs/1606.01455)\n\n**Simple Question Answering by Attentive Convolutional Neural Network**\n\n- arxiv: [http://arxiv.org/abs/1606.03391](http://arxiv.org/abs/1606.03391)\n\n**Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?**\n\n![](https://computing.ece.vt.edu/~abhshkdz/vqa-hat/img/att_comparison_2row.jpg)\n\n- homepage: [https://computing.ece.vt.edu/~abhshkdz/vqa-hat/](https://computing.ece.vt.edu/~abhshkdz/vqa-hat/)\n- arxiv: [http://arxiv.org/abs/1606.03556](http://arxiv.org/abs/1606.03556)\n\n**Simple and Effective Question Answering with Recurrent Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1606.05029](http://arxiv.org/abs/1606.05029)\n\n**Analyzing the Behavior of Visual Question Answering Models**\n\n- arxiv: [http://arxiv.org/abs/1606.07356](http://arxiv.org/abs/1606.07356)\n\n**Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding**\n\n- arxiv: [https://arxiv.org/abs/1606.01847](https://arxiv.org/abs/1606.01847)\n- github: [https://github.com/akirafukui/vqa-mcb](https://github.com/akirafukui/vqa-mcb)\n\n**Deep Language Modeling for Question Answering using Keras**\n\n- blog: [http://benjaminbolte.com/blog/2016/keras-language-modeling.html](http://benjaminbolte.com/blog/2016/keras-language-modeling.html)\n- github: [https://github.com/codekansas/keras-language-modeling](https://github.com/codekansas/keras-language-modeling)\n\n**Interpreting Visual Question Answering Models**\n\n- arxiv: [http://arxiv.org/abs/1608.08974](http://arxiv.org/abs/1608.08974)\n\n**The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering**\n\n- intro: FSVQA\n- arxiv: [http://arxiv.org/abs/1609.06657](http://arxiv.org/abs/1609.06657)\n\n**Tutorial on Answering Questions about Images with Deep Learning**\n\n- intro: The tutorial was presented at '2nd Summer School on Integrating Vision and Language: Deep Learning' in Malta, 2016\n- arxiv: [https://arxiv.org/abs/1610.01076](https://arxiv.org/abs/1610.01076)\n\n**Hadamard Product for Low-rank Bilinear Pooling**\n\n- arxiv: [https://arxiv.org/abs/1610.04325](https://arxiv.org/abs/1610.04325)\n- github: [https://github.com/jnhwkim/MulLowBiVQA](https://github.com/jnhwkim/MulLowBiVQA)\n\n**Open-Ended Visual Question-Answering**\n\n![](https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/model.jpg)\n\n- intro: Bachelor thesis report graded with A with honours at ETSETB Telecom BCN school, Universitat Polit\\`ecnica de Catalunya (UPC). June 2016\n- project page: [http://imatge-upc.github.io/vqa-2016-cvprw/](http://imatge-upc.github.io/vqa-2016-cvprw/)\n- arxiv: [https://arxiv.org/abs/1610.02692](https://arxiv.org/abs/1610.02692)\n- slides: [http://www.slideshare.net/xavigiro/openended-visual-questionanswering](http://www.slideshare.net/xavigiro/openended-visual-questionanswering)\n- github: [https://github.com/imatge-upc/vqa-2016-cvprw](https://github.com/imatge-upc/vqa-2016-cvprw)\n\n**Deep Learning for Question Answering**\n\n- intro: UMD. Mohit Iyyer.\n- intro: Recurrent Neural Networks, Recursive Neural Network\n- slides: [http://cs.umd.edu/~miyyer/data/deepqa.pdf](http://cs.umd.edu/~miyyer/data/deepqa.pdf)\n\n**Dual Attention Networks for Multimodal Reasoning and Matching**\n\n- arxiv: [https://arxiv.org/abs/1611.00471](https://arxiv.org/abs/1611.00471)\n\n**Dynamic Coattention Networks For Question Answering**\n\n- arxiv: [https://arxiv.org/abs/1611.01604](https://arxiv.org/abs/1611.01604)\n\n**State of the art deep learning model for question answering**\n\n- blog: [http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/](http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/)\n\n**Zero-Shot Visual Question Answering**\n\n- arxiv: [https://arxiv.org/abs/1611.05546](https://arxiv.org/abs/1611.05546)\n\n**Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation**\n\n- intro: University of Rochester & Microsoft & University College London\n- arxiv: [https://arxiv.org/abs/1701.08251](https://arxiv.org/abs/1701.08251)\n\n**Question Answering through Transfer Learning from Large Fine-grained Supervision Data**\n\n- intro: Seoul National University & University of Washington\n- arxiv: [https://arxiv.org/abs/1702.02171](https://arxiv.org/abs/1702.02171)\n\n**Question Answering from Unstructured Text by Retrieval and Comprehension**\n\n- arxiv: [https://arxiv.org/abs/1703.08885](https://arxiv.org/abs/1703.08885)\n- notes: [https://theneuralperspective.com/2017/04/26/question-answering-from-unstructured-text-by-retrieval-and-comprehension/](https://theneuralperspective.com/2017/04/26/question-answering-from-unstructured-text-by-retrieval-and-comprehension/)\n\n**Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering**\n\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/1704.03162](https://arxiv.org/abs/1704.03162)\n\n**Learning to Reason: End-to-End Module Networks for Visual Question Answering**\n\n- intro: UC Berkeley, Boston University\n- arxiv: [https://arxiv.org/abs/1704.05526](https://arxiv.org/abs/1704.05526)\n\n**TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering**\n\n- intro: CVPR 2017.Seoul National University & Yahoo Research\n- arxiv: [https://arxiv.org/abs/1704.04497](https://arxiv.org/abs/1704.04497)\n- github: [https://github.com/YunseokJANG/tgif-qa](https://github.com/YunseokJANG/tgif-qa)\n\n**Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks**\n\n- intro: ACL 2017 (short)\n- project page: [https://rajarshd.github.io/TextKBQA/](https://rajarshd.github.io/TextKBQA/)\n- arxiv: [https://arxiv.org/abs/1704.08384](https://arxiv.org/abs/1704.08384)\n- github: [https://github.com/rajarshd/TextKBQA](https://github.com/rajarshd/TextKBQA)\n\n**Learning Convolutional Text Representations for Visual Question Answering**\n\n- arxiv: [https://arxiv.org/abs/1705.06824](https://arxiv.org/abs/1705.06824)\n- github: [https://github.com/divelab/vqa-text](https://github.com/divelab/vqa-text)\n\n**Compact Tensor Pooling for Visual Question Answering**\n\n[https://arxiv.org/abs/1706.06706](https://arxiv.org/abs/1706.06706)\n\n**Long-Term Memory Networks for Question Answering**\n\n- intro: SUNY Buffalo & LinkedIn & LinkedIn\n- arxiv: [https://arxiv.org/abs/1707.01961](https://arxiv.org/abs/1707.01961)\n\n**Bottom-Up and Top-Down Attention for Image Captioning and VQA**\n\n**Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering**\n\n- intro: Winner of the Visual Question Answering Challenge at CVPR 2017\n- project page: [http://www.panderson.me/up-down-attention/](http://www.panderson.me/up-down-attention/)\n- arxiv: [https://arxiv.org/abs/1707.07998](https://arxiv.org/abs/1707.07998)\n- paper: [http://www.panderson.me/images/1707.07998-up-down.pdf](http://www.panderson.me/images/1707.07998-up-down.pdf)\n- github: [https://github.com//peteanderson80/bottom-up-attention](https://github.com//peteanderson80/bottom-up-attention)\n\n**Structured Attentions for Visual Question Answering**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.02071](https://arxiv.org/abs/1708.02071)\n- github: [https://github.com/zhuchen03/vqa-sva](https://github.com/zhuchen03/vqa-sva)\n\n**Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge**\n\n- intro: Winner of the 2017 Visual Question Answering (VQA) Challenge at CVPR\n- intro: The University of Adelaide & Australian National University & Microsoft Research\n- arxiv: [https://arxiv.org/abs/1708.02711](https://arxiv.org/abs/1708.02711)\n\n**MemexQA: Visual Memex Question Answering**\n\n- intro: Carnegie Mellon University, Customer Service AI, Yahoo\n- project page: [https://memexqa.cs.cmu.edu/](https://memexqa.cs.cmu.edu/)\n- arxiv: [https://arxiv.org/abs/1708.01336](https://arxiv.org/abs/1708.01336)\n\n**Automatic Question-Answering Using A Deep Similarity Neural Network**\n\n- intro: New York University & AT&T Research Labs\n- arxiv: [https://arxiv.org/abs/1708.01713](https://arxiv.org/abs/1708.01713)\n\n**Question Dependent Recurrent Entity Network for Question Answering**\n\n- intro: University of Pisa\n- arxiv: [https://arxiv.org/abs/1707.07922](https://arxiv.org/abs/1707.07922)\n- github: [https://github.com/andreamad8/QDREN](https://github.com/andreamad8/QDREN)\n\n**Visual Question Generation as Dual Task of Visual Question Answering**\n\n[https://arxiv.org/abs/1709.07192](https://arxiv.org/abs/1709.07192)\n\n**A Read-Write Memory Network for Movie Story Understanding**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1709.09345](https://arxiv.org/abs/1709.09345)\n\n**iVQA: Inverse Visual Question Answering**\n\n[https://arxiv.org/abs/1710.03370](https://arxiv.org/abs/1710.03370)\n\n**DCN+: Mixed Objective and Deep Residual Coattention for Question Answering**\n\n- intro: Salesforce Research\n- arxiv: [https://arxiv.org/abs/1711.00106](https://arxiv.org/abs/1711.00106)\n- github: [https://github.com/mjacar/tensorflow-dcn-plus](https://github.com/mjacar/tensorflow-dcn-plus)\n\n**High-Order Attention Models for Visual Question Answering**\n\n- intro: NIPS 2017\n- arxiv: [https://arxiv.org/abs/1711.04323](https://arxiv.org/abs/1711.04323)\n\n**Asking the Difficult Questions: Goal-Oriented Visual Question Generation via Intermediate Rewards**\n\n- intro: The University of Adelaide & University of Technology Sydney & Nanjing University of Science and Technology\n- arxiv: [https://arxiv.org/abs/1711.07614](https://arxiv.org/abs/1711.07614)\n\n**Visual Question Answering as a Meta Learning Task**\n\n[https://arxiv.org/abs/1711.08105](https://arxiv.org/abs/1711.08105)\n\n**Embodied Question Answering**\n\n- intro: Georgia Institute of Technology & Facebook AI Research\n- project page: [https://embodiedqa.org/](https://embodiedqa.org/)\n- arxiv: [https://arxiv.org/abs/1711.11543](https://arxiv.org/abs/1711.11543)\n- github: [https://github.com/facebookresearch/EmbodiedQA](https://github.com/facebookresearch/EmbodiedQA)\n\n**Learning by Asking Questions**\n\n[https://arxiv.org/abs/1712.01238](https://arxiv.org/abs/1712.01238)\n\n**Interpretable Counting for Visual Question Answering**\n\n[https://arxiv.org/abs/1712.08697](https://arxiv.org/abs/1712.08697)\n\n**Structured Triplet Learning with POS-tag Guided Attention for Visual Question Answering**\n\n- arxiv: [https://arxiv.org/abs/1801.07853](https://arxiv.org/abs/1801.07853)\n- github: [https://github.com/wangzheallen/STL-VQA](https://github.com/wangzheallen/STL-VQA)\n\n**DVQA: Understanding Data Visualizations via Question Answering**\n\n- intro: RIT & Adobe Research\n- arxiv: [https://arxiv.org/abs/1801.08163](https://arxiv.org/abs/1801.08163)\n\n**Object-based reasoning in VQA**\n\n- intro: WACV 2018\n- arxiv: [https://arxiv.org/abs/1801.09718](https://arxiv.org/abs/1801.09718)\n\n**Dual Recurrent Attention Units for Visual Question Answering**\n\n[https://arxiv.org/abs/1802.00209](https://arxiv.org/abs/1802.00209)\n\n**Differential Attention for Visual Question Answering**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.00298](https://arxiv.org/abs/1804.00298)\n\n**Question Type Guided Attention in Visual Question Answering**\n\n[https://arxiv.org/abs/1804.02088](https://arxiv.org/abs/1804.02088)\n\n**Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents**\n\n- intro: AAAI 2018\n- arxiv: [https://arxiv.org/abs/1804.09412](https://arxiv.org/abs/1804.09412)\n\n**Reciprocal Attention Fusion for Visual Question Answering**\n\n[https://arxiv.org/abs/1805.04247](https://arxiv.org/abs/1805.04247)\n\n**Learning to Count Objects in Natural Images for Visual Question Answering**\n\n- intro: ICLR 2018\n- arxiv: [https://arxiv.org/abs/1802.05766](https://arxiv.org/abs/1802.05766)\n\n**Bilinear Attention Networks**\n\n- intro: CVPR 2018. Seoul National University\n- arxiv: [https://arxiv.org/abs/1805.07932](https://arxiv.org/abs/1805.07932)\n- slides: [https://bi.snu.ac.kr/~jhkim/slides/bilinear%20attention%20networks_8min.pdf](https://bi.snu.ac.kr/~jhkim/slides/bilinear%20attention%20networks_8min.pdf)\n- github(official, PyTorch): [https://github.com/jnhwkim/ban-vqa](https://github.com/jnhwkim/ban-vqa)\n\n**Reproducibility Report for \"Learning To Count Objects In Natural Images For Visual Question Answering\"**\n\n[https://arxiv.org/abs/1805.08174](https://arxiv.org/abs/1805.08174)\n\n**Cross-Dataset Adaptation for Visual Question Answering**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1806.03726](https://arxiv.org/abs/1806.03726)\n\n**Learning Answer Embeddings for Visual Question Answering**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1806.03724](https://arxiv.org/abs/1806.03724)\n\n**Learning Visual Question Answering by Bootstrapping Hard Attention**\n\n- intro: ECCV 2018\n- axrxiv: [https://arxiv.org/abs/1808.00300](https://arxiv.org/abs/1808.00300)\n\n**Question-Guided Hybrid Convolution for Visual Question Answering**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.02632](https://arxiv.org/abs/1808.02632)\n\n**Interpretable Visual Question Answering by Reasoning on Dependency Trees**\n\n[https://arxiv.org/abs/1809.01810](https://arxiv.org/abs/1809.01810)\n\n**The Visual QA Devil in the Details: The Impact of Early Fusion and Batch Norm on CLEVR**\n\n- intro: ECCV 2018 Workshop on Shortcomings in Vision and Language\n- arxiv: [https://arxiv.org/abs/1809.04482](https://arxiv.org/abs/1809.04482)\n\n**Knowing Where to Look? Analysis on Attention of Visual Question Answering System**\n\n- intro: ECCV SiVL Workshop paper\n- arxiv: [https://arxiv.org/abs/1810.03821](https://arxiv.org/abs/1810.03821)\n\n**VQA with no questions-answers training**\n\n[https://arxiv.org/abs/1811.08481](https://arxiv.org/abs/1811.08481)\n\n**Visual Commonsense R-CNN**\n\n- intro: CVPR 2020\n- arxiv: [https://arxiv.org/abs/2002.12204](https://arxiv.org/abs/2002.12204)\n\n# Video Question Answering\n\n**DeepStory: Video Story QA by Deep Embedded Memory Networks**\n\n- intro: IJCAI 2017. Seoul National University\n- arxiv: [https://arxiv.org/abs/1707.00836](https://arxiv.org/abs/1707.00836)\n\n**Video Question Answering via Attribute-Augmented Attention Network Learning**\n\n- intro: SIGIR 2017\n- arxiv: [https://arxiv.org/abs/1707.06355](https://arxiv.org/abs/1707.06355)\n\n**Leveraging Video Descriptions to Learn Video Question Answering**\n\n- intro: AAAI 2017\n- arxiv: [https://arxiv.org/abs/1611.04021](https://arxiv.org/abs/1611.04021)\n\n**A Joint Sequence Fusion Model for Video Question Answering and Retrieval**\n\n- intro: ECCV 2018\n- arixv: [https://arxiv.org/abs/1808.02559](https://arxiv.org/abs/1808.02559)\n\n# Projects\n\n**VQA Demo: Visual Question Answering Demo on pretrained model**\n\n- github: [https://github.com/iamaaditya/VQA_Demo](https://github.com/iamaaditya/VQA_Demo)\n- ref: [http://iamaaditya.github.io/research/](http://iamaaditya.github.io/research/)\n\n**deep-qa: Implementation of the Convolution Neural Network for factoid QA on the answer sentence selection task**\n\n- github: [https://github.com/aseveryn/deep-qa](https://github.com/aseveryn/deep-qa)\n\n**YodaQA: A Question Answering system built on top of the Apache UIMA framework**\n\n- homepage: [http://ailao.eu/yodaqa/](http://ailao.eu/yodaqa/)\n- github: [https://github.com/brmson/yodaqa](https://github.com/brmson/yodaqa)\n\n**insuranceQA-cnn-lstm: tensorflow and theano cnn code for insurance QA(question Answer matching)**\n\n- github: [https://github.com/white127/insuranceQA-cnn-lstm](https://github.com/white127/insuranceQA-cnn-lstm)\n\n**Tensorflow Implementation of Deeper LSTM+ normalized CNN for Visual Question Answering**\n\n![](https://cloud.githubusercontent.com/assets/19935904/16358326/e6812310-3add-11e6-914f-c61c19d6ab5a.png)\n\n- github: [https://github.com/JamesChuanggg/VQA-tensorflow](https://github.com/JamesChuanggg/VQA-tensorflow)\n\n**Visual Question Answering with Keras**\n\n![](https://camo.githubusercontent.com/f52d44199710c8f3939fb182a339d1d6a0b09a3f/687474703a2f2f692e696d6775722e636f6d2f327a4a30396d512e706e67)\n\n- project page: [https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/](https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/)\n- github: [https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering](https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering)\n\n**Deep Learning Models for Question Answering with Keras**\n\n- blog: [http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html](http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html)\n\n**GuessWhat?! Visual object discovery through multi-modal dialogue**\n\n- intro: University of Montreal & Univ. Lille & Google DeepMind & Twitter\n- arxiv: [https://arxiv.org/abs/1611.08481](https://arxiv.org/abs/1611.08481)\n\n**Deep QA: Using deep learning to answer Aristo's science questions**\n\n- github: [https://github.com/allenai/deep_qa](https://github.com/allenai/deep_qa)\n\n**Visual Question Answering in Pytorch**\n\n[https://github.com/Cadene/vqa.pytorch](https://github.com/Cadene/vqa.pytorch)\n\n# Dataset\n\n**Visual7W: Grounded Question Answering in Images**\n\n![](http://web.stanford.edu/~yukez/images/img/visual7w_examples.png)\n\n- homepage: [http://web.stanford.edu/~yukez/visual7w/](http://web.stanford.edu/~yukez/visual7w/)\n- github: [https://github.com/yukezhu/visual7w-toolkit](https://github.com/yukezhu/visual7w-toolkit)\n- github: [https://github.com/yukezhu/visual7w-qa-models](https://github.com/yukezhu/visual7w-qa-models)\n\n# Resources\n\n**Awesome Visual Question Answering**\n\n- github: [https://github.com/JamesChuanggg/awesome-vqa](https://github.com/JamesChuanggg/awesome-vqa)\n","excerpt":"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks intro: Facebook AI Research arxiv:  http://arxiv.org/abs/1502.05698","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Content","url":"/Commercial/Value Accounting Initiatives/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Background","url":"/Core Services/Webizen Socio-Economics/The Values Project/","items":[]},{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"HyperMedia Library","url":"/HyperMedia Library/","items":[]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"OVERVIEW","url":"/Implementation V1/Networking Considerations/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations  Principles  The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]},{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/ld-solutions-to-fakenews/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between privacy and dignity.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"awesomeLists","url":"","items":[{"title":"Awesome Computer Vision: Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-computer-vision/","items":[]},{"title":"Awesome Natural Language Generation Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awsome-nl-gen/","items":[]},{"title":"Awesome Semantic Web Awesome","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-semweb/","items":[]},{"title":"Awesome-General","url":"/old-work-archives/2018-webizen-net-au/resource-library/awesomeLists/awesome-general/","items":[]}]},{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]},{"title":"EXECUTIVE SUMMARY","url":"/old-work-archives/2018 - Web Civics BizPlan/","items":[]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Overview","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Vision","url":"/Webizen 2.0/Webizen Vision/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/Host Service Requirements/Email Services/","title":"Email Services","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","title":"LD_PostOffice_SemanticMGR","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Host Service Requirements/Media Processing/","title":"Media Processing","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Non-HTTP(s) Protocols/GUNECO/","title":"GUNECO","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Non-HTTP(s) Protocols/IPFS/","title":"IPFS","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Non-HTTP(s) Protocols/WebRTC/","title":"WebRTC","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Non-HTTP(s) Protocols/WebSockets/","title":"WebSockets","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Non-HTTP(s) Protocols/WebTorrent/","title":"WebTorrent","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","title":"Sustainable Development Goals (ESG)","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/Implementation V1/edge/Webizen Local App Functionality/","title":"Webizen Local App Functionality","lastUpdatedAt":"2022-12-30T11:53:44.000Z","lastUpdated":"12/30/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}