{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/",
    "result": {"data":{"mdx":{"id":"e6851573-9820-5f5b-af4c-5a04addd97d7","tableOfContents":{"items":[{"url":"#papers","title":"Papers","items":[{"url":"#show-and-tell","title":"Show and Tell"},{"url":"#show-attend-and-tell","title":"Show, Attend and Tell"}]},{"url":"#object-descriptions","title":"Object Descriptions"},{"url":"#video-captioning--description","title":"Video Captioning / Description"},{"url":"#projects","title":"Projects"},{"url":"#tools","title":"Tools"},{"url":"#blogs","title":"Blogs"}]},"fields":{"title":"Image / Video Captioning","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning.md","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"Image / Video Captioning","description":null,"imageAlt":null,"tags":[],"date":"2015-10-09T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"deep_learning\",\n  \"title\": \"Image / Video Captioning\",\n  \"date\": \"2015-10-09T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"papers\"\n  }, \"Papers\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Im2Text: Describing Images Using 1 Million Captioned Photographs\")), mdx(\"img\", {\n    \"src\": \"http://vision.cs.stonybrook.edu/~vicente/sbucaptions/im2text_files/im2text.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://tamaraberg.com/papers/generation_nips2011.pdf\"\n  }, \"http://tamaraberg.com/papers/generation_nips2011.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vision.cs.stonybrook.edu/~vicente/sbucaptions/\"\n  }, \"http://vision.cs.stonybrook.edu/~vicente/sbucaptions/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Long-term Recurrent Convolutional Networks for Visual Recognition and Description\")), mdx(\"img\", {\n    \"src\": \"http://jeffdonahue.com/lrcn/images/lrcn_tasks.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Oral presentation at CVPR 2015. LRCN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jeffdonahue.com/lrcn/\"\n  }, \"http://jeffdonahue.com/lrcn/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1411.4389\"\n  }, \"http://arxiv.org/abs/1411.4389\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/BVLC/caffe/pull/2033\"\n  }, \"https://github.com/BVLC/caffe/pull/2033\"))), mdx(\"h2\", {\n    \"id\": \"show-and-tell\"\n  }, \"Show and Tell\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Show and Tell: A Neural Image Caption Generator\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1411.4555\"\n  }, \"http://arxiv.org/abs/1411.4555\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/karpathy/neuraltalk\"\n  }, \"https://github.com/karpathy/neuraltalk\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gitxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://gitxiv.com/posts/7nofxjoYBXga5XjtL/show-and-tell-a-neural-image-caption-nic-generator\"\n  }, \"http://gitxiv.com/posts/7nofxjoYBXga5XjtL/show-and-tell-a-neural-image-caption-nic-generator\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/apple2373/chainer_caption_generation\"\n  }, \"https://github.com/apple2373/chainer_caption_generation\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tensorflow/models/tree/master/im2txt\"\n  }, \"https://github.com/tensorflow/models/tree/master/im2txt\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zsdonghao/Image-Captioning\"\n  }, \"https://github.com/zsdonghao/Image-Captioning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image caption generation by CNN and LSTM\"), \" \"), mdx(\"img\", {\n    \"src\": \"http://1.bp.blogspot.com/-RgNlhmSJvgU/Vna0V_GNGXI/AAAAAAAAACw/tjYqwuGHekM/s400/Screen%2BShot%2B2015-12-20%2Bat%2B08.59.26.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://t-satoshi.blogspot.com/2015/12/image-caption-generation-by-cnn-and-lstm.html\"\n  }, \"http://t-satoshi.blogspot.com/2015/12/image-caption-generation-by-cnn-and-lstm.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jazzsaxmafia/show_and_tell.tensorflow\"\n  }, \"https://github.com/jazzsaxmafia/show_and_tell.tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.06647\"\n  }, \"http://arxiv.org/abs/1609.06647\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tensorflow/models/tree/master/im2txt\"\n  }, \"https://github.com/tensorflow/models/tree/master/im2txt\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning a Recurrent Visual Representation for Image Caption Generation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1411.5654\"\n  }, \"http://arxiv.org/abs/1411.5654\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Mind\\u2019s Eye: A Recurrent Visual Representation for Image Caption Generation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf\"\n  }, \"http://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Visual-Semantic Alignments for Generating Image Descriptions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"propose a multimodal deep network that aligns various interesting\\nregions of the image, represented using a CNN feature, with associated words.\\nThe learned correspondences are then used to train a bi-directional RNN.\\nThis model is able, not only to generate descriptions for images, but also\\nto localize different segments of the sentence to their corresponding image regions.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/people/karpathy/deepimagesent/\"\n  }, \"http://cs.stanford.edu/people/karpathy/deepimagesent/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.2306\"\n  }, \"http://arxiv.org/abs/1412.2306\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.toronto.edu/~vendrov/DeepVisualSemanticAlignments_Class_Presentation.pdf\"\n  }, \"http://www.cs.toronto.edu/~vendrov/DeepVisualSemanticAlignments_Class_Presentation.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/karpathy/neuraltalk\"\n  }, \"https://github.com/karpathy/neuraltalk\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/\"\n  }, \"http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Captioning with Multimodal Recurrent Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: m-RNN. ICLR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"combines the functionalities of the CNN and RNN by introducing a new multimodal layer,\\nafter the embedding and recurrent layers of the RNN.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.stat.ucla.edu/~junhua.mao/m-RNN.html\"\n  }, \"http://www.stat.ucla.edu/~junhua.mao/m-RNN.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.6632\"\n  }, \"http://arxiv.org/abs/1412.6632\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mjhucla/mRNN-CR\"\n  }, \"https://github.com/mjhucla/mRNN-CR\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mjhucla/TF-mRNN\"\n  }, \"https://github.com/mjhucla/TF-mRNN\"))), mdx(\"h2\", {\n    \"id\": \"show-attend-and-tell\"\n  }, \"Show, Attend and Tell\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (ICML 2015)\")), mdx(\"img\", {\n    \"src\": \"http://kelvinxu.github.io/projects/diags/model_diag.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://kelvinxu.github.io/projects/capgen.html\"\n  }, \"http://kelvinxu.github.io/projects/capgen.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1502.03044\"\n  }, \"http://arxiv.org/abs/1502.03044\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kelvinxu/arctic-captions\"\n  }, \"https://github.com/kelvinxu/arctic-captions\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow\"\n  }, \"https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yunjey/show-attend-and-tell-tensorflow\"\n  }, \"https://github.com/yunjey/show-attend-and-tell-tensorflow\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.toronto.edu/~rkiros/abstract_captions.html\"\n  }, \"http://www.cs.toronto.edu/~rkiros/abstract_captions.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Automatically describing historic photographs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"website: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://staff.fnwi.uva.nl/d.elliott/loc/\"\n  }, \"https://staff.fnwi.uva.nl/d.elliott/loc/\"))), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1504.06692\"\n  }, \"http://arxiv.org/abs/1504.06692\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html\"\n  }, \"http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mjhucla/NVC-Dataset\"\n  }, \"https://github.com/mjhucla/NVC-Dataset\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"What value do explicit high level concepts have in vision to language problems?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1506.01144\"\n  }, \"http://arxiv.org/abs/1506.01144\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Aligning where to see and what to tell: image caption with region-based attention and scene factorization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1506.06272\"\n  }, \"http://arxiv.org/abs/1506.06272\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning FRAME Models Using CNN Filters for Knowledge Visualization (CVPR 2015)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html\"\n  }, \"http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.08379\"\n  }, \"http://arxiv.org/abs/1509.08379\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code+data: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/deepFRAME_1.1.zip\"\n  }, \"http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/deepFRAME_1.1.zip\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generating Images from Captions with Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.02793\"\n  }, \"http://arxiv.org/abs/1511.02793\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/emansim/text2image\"\n  }, \"https://github.com/emansim/text2image\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.toronto.edu/~emansim/cap2im.html\"\n  }, \"http://www.cs.toronto.edu/~emansim/cap2im.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Order-Embeddings of Images and Language\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06361\"\n  }, \"http://arxiv.org/abs/1511.06361\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ivendrov/order-embedding\"\n  }, \"https://github.com/ivendrov/order-embedding\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DenseCap: Fully Convolutional Localization Networks for Dense Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/people/karpathy/densecap/\"\n  }, \"http://cs.stanford.edu/people/karpathy/densecap/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.07571\"\n  }, \"http://arxiv.org/abs/1511.07571\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Torch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jcjohnson/densecap\"\n  }, \"https://github.com/jcjohnson/densecap\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Expressing an Image Stream with a Sequence of Natural Sentences\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2015. CRCN\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"nips-page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences\"\n  }, \"http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf\"\n  }, \"http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf\"\n  }, \"http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"author-page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.cmu.edu/~gunhee/\"\n  }, \"http://www.cs.cmu.edu/~gunhee/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cesc-park/CRCN\"\n  }, \"https://github.com/cesc-park/CRCN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multimodal Pivots for Image Caption Translation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACL 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1601.03916\"\n  }, \"http://arxiv.org/abs/1601.03916\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Captioning with Deep Bidirectional LSTMs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACMMM 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.00790\"\n  }, \"http://arxiv.org/abs/1604.00790\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deepsemantic/image_captioning\"\n  }, \"https://github.com/deepsemantic/image_captioning\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"demo: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://youtu.be/a0bh9_2LE24\"\n  }, \"https://youtu.be/a0bh9_2LE24\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Encode, Review, and Decode: Reviewer Module for Caption Generation\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Review Network for Caption Generation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1605.07912\"\n  }, \"https://arxiv.org/abs/1605.07912\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kimiyoung/review_net\"\n  }, \"https://github.com/kimiyoung/review_net\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attention Correctness in Neural Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.09553\"\n  }, \"http://arxiv.org/abs/1605.09553\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Caption Generation with Text-Conditional Semantic Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.04621\"\n  }, \"https://arxiv.org/abs/1606.04621\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/LuoweiZhou/e2e-gLSTM-sc\"\n  }, \"https://github.com/LuoweiZhou/e2e-gLSTM-sc\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepDiary: Automatic Caption Generation for Lifelogging Image Streams\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV International Workshop on Egocentric Perception, Interaction, and Computing\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.03819\"\n  }, \"http://arxiv.org/abs/1608.03819\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.05813\"\n  }, \"http://arxiv.org/abs/1608.05813\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Captioning Images with Diverse Objects\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.07770\"\n  }, \"http://arxiv.org/abs/1606.07770\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to generalize to new compositions in image understanding\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.07639\"\n  }, \"http://arxiv.org/abs/1608.07639\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generating captions without looking beyond objects\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.03708\"\n  }, \"https://arxiv.org/abs/1610.03708\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SPICE: Semantic Propositional Image Caption Evaluation\")), mdx(\"img\", {\n    \"src\": \"http://www.panderson.me/images/spice-concept.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.panderson.me/spice/\"\n  }, \"http://www.panderson.me/spice/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.panderson.me/images/SPICE.pdf\"\n  }, \"http://www.panderson.me/images/SPICE.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/peteanderson80/SPICE\"\n  }, \"https://github.com/peteanderson80/SPICE\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Boosting Image Captioning with Attributes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01646\"\n  }, \"https://arxiv.org/abs/1611.01646\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bootstrap, Review, Decode: Using Out-of-Domain Textual Data to Improve Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05321\"\n  }, \"https://arxiv.org/abs/1611.05321\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Hierarchical Approach for Generating Descriptive Image Paragraphs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06607\"\n  }, \"https://arxiv.org/abs/1611.06607\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dense Captioning with Joint Inference and Visual Context\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Snap Inc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06949\"\n  }, \"https://arxiv.org/abs/1611.06949\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Optimization of image description metrics using policy gradient methods\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Oxford & Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.00370\"\n  }, \"https://arxiv.org/abs/1612.00370\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Areas of Attention for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.01033\"\n  }, \"https://arxiv.org/abs/1612.01033\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.01887\"\n  }, \"https://arxiv.org/abs/1612.01887\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jiasenlu/AdaptiveAttention\"\n  }, \"https://github.com/jiasenlu/AdaptiveAttention\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent Image Captioner: Describing Images with Spatial-Invariant Transformation and Attention Filtering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.04949\"\n  }, \"https://arxiv.org/abs/1612.04949\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent Highway Networks with Language CNN for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.07086\"\n  }, \"https://arxiv.org/abs/1612.07086\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Top-down Visual Saliency Guided by Captions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.07360\"\n  }, \"https://arxiv.org/abs/1612.07360\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/VisionLearningGroup/caption-guided-saliency\"\n  }, \"https://github.com/VisionLearningGroup/caption-guided-saliency\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MAT: A Multimodal Attentive Translator for Image Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1702.05658\"\n  }, \"https://arxiv.org/abs/1702.05658\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Reinforcement Learning-based Image Captioning with Embedding Reward\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Snap Inc & Google Inc\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.03899\"\n  }, \"https://arxiv.org/abs/1704.03899\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attend to You: Personalized Image Captioning with Context Sequence Memory Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.06485\"\n  }, \"https://arxiv.org/abs/1704.06485\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cesc-park/attend2u\"\n  }, \"https://github.com/cesc-park/attend2u\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Punny Captions: Witty Wordplay in Image Descriptions\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1704.08224\"\n  }, \"https://arxiv.org/abs/1704.08224\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.00930\"\n  }, \"https://arxiv.org/abs/1705.00930\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Actor-Critic Sequence Training for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Queen Mary University of London & Yang\\u2019s Accounting Consultancy Ltd\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: actor-critic reinforcement learning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.09601\"\n  }, \"https://arxiv.org/abs/1706.09601\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Proceedings of the 10th International Conference on Natural Language Generation (INLG'17)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.02043\"\n  }, \"https://arxiv.org/abs/1708.02043\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stack-Captioning: Coarse-to-Fine Learning for Image Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.03376\"\n  }, \"https://arxiv.org/abs/1709.03376\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Self-Guiding Multimodal LSTM - when we do not have a perfect training dataset for image captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.05038\"\n  }, \"https://arxiv.org/abs/1709.05038\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Contrastive Learning for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.02534\"\n  }, \"https://arxiv.org/abs/1710.02534\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Phrase-based Image Captioning with Hierarchical LSTM Model\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACCV2016 extension, phrase-based image captioning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.05557\"\n  }, \"https://arxiv.org/abs/1711.05557\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convolutional Image Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.09151\"\n  }, \"https://arxiv.org/abs/1711.09151\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Show-and-Fool: Crafting Adversarial Examples for Neural Image Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.02051\"\n  }, \"https://arxiv.org/abs/1712.02051\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improved Image Captioning with Adversarial Semantic Alignment\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IBM Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.00063\"\n  }, \"https://arxiv.org/abs/1805.00063\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Object Counts! Bringing Explicit Detections Back into Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NAACL 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.00314\"\n  }, \"https://arxiv.org/abs/1805.00314\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Defoiling Foiled Image Captions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NAACL 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.06549\"\n  }, \"https://arxiv.org/abs/1805.06549\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.07030\"\n  }, \"https://arxiv.org/abs/1805.07030\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Image Captioning with Conditional Generative Adversarial Nets\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.07112\"\n  }, \"https://arxiv.org/abs/1805.07112\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CNN+CNN: Convolutional Decoders for Image Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.09019\"\n  }, \"https://arxiv.org/abs/1805.09019\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Diverse and Controllable Image Captioning with Part-of-Speech Guidance\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.12589\"\n  }, \"https://arxiv.org/abs/1805.12589\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Evaluate Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.06422\"\n  }, \"https://arxiv.org/abs/1806.06422\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Topic-Guided Attention for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICIP 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.03514\"\n  }, \"https://arxiv.org/abs/1807.03514\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Context-Aware Visual Policy Network for Sequence-Level Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACM MM 2018 oral\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.05864\"\n  }, \"https://arxiv.org/abs/1808.05864\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/daqingliu/CAVP\"\n  }, \"https://github.com/daqingliu/CAVP\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Exploring Visual Relationship for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1809.07041\"\n  }, \"https://arxiv.org/abs/1809.07041\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Boosted Attention: Leveraging Human Attention for Image Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.00767\"\n  }, \"https://arxiv.org/abs/1904.00767\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Captioning as Neural Machine Translation Task in SOCKEYE\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.04101\"\n  }, \"https://arxiv.org/abs/1810.04101\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Unsupervised Image Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.10787\"\n  }, \"https://arxiv.org/abs/1811.10787\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attend More Times for Image Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1812.03283\"\n  }, \"https://arxiv.org/abs/1812.03283\")), mdx(\"h1\", {\n    \"id\": \"object-descriptions\"\n  }, \"Object Descriptions\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generation and Comprehension of Unambiguous Object Descriptions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1511.02283\"\n  }, \"https://arxiv.org/abs/1511.02283\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mjhucla/Google_Refexp_toolbox\"\n  }, \"https://github.com/mjhucla/Google_Refexp_toolbox\"))), mdx(\"h1\", {\n    \"id\": \"video-captioning--description\"\n  }, \"Video Captioning / Description\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf\"\n  }, \"http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf\"\n  }, \"http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\")), mdx(\"img\", {\n    \"src\": \"https://www.cs.utexas.edu/~vsub/imgs/naacl-15-overview.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NAACL-HLT 2015 camera ready\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.cs.utexas.edu/~vsub/naacl15_project.html\"\n  }, \"https://www.cs.utexas.edu/~vsub/naacl15_project.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.4729\"\n  }, \"http://arxiv.org/abs/1412.4729\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_slides.pdf\"\n  }, \"https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_slides.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"code+data: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.cs.utexas.edu/~vsub/naacl15_project.html#code\"\n  }, \"https://www.cs.utexas.edu/~vsub/naacl15_project.html#code\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Describing Videos by Exploiting Temporal Structure\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1502.08029\"\n  }, \"http://arxiv.org/abs/1502.08029\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yaoli/arctic-capgen-vid\"\n  }, \"https://github.com/yaoli/arctic-capgen-vid\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SA-tensorflow: Soft attention mechanism for video caption generation\")), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/tsenghungchen/SA-tensorflow/master/README_files/head.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tsenghungchen/SA-tensorflow\"\n  }, \"https://github.com/tsenghungchen/SA-tensorflow\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sequence to Sequence -- Video to Text\")), mdx(\"img\", {\n    \"src\": \"http://www.cs.utexas.edu/~vsub/imgs/S2VTarchitecture.png\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2015. S2VT\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vsubhashini.github.io/s2vt.html\"\n  }, \"http://vsubhashini.github.io/s2vt.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.00487\"\n  }, \"http://arxiv.org/abs/1505.00487\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.cs.utexas.edu/~vsub/pdf/S2VT_slides.pdf\"\n  }, \"https://www.cs.utexas.edu/~vsub/pdf/S2VT_slides.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt\"\n  }, \"https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jazzsaxmafia/video_to_sequence\"\n  }, \"https://github.com/jazzsaxmafia/video_to_sequence\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Jointly Modeling Embedding and Translation to Bridge Video and Language\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.01861\"\n  }, \"http://arxiv.org/abs/1505.01861\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Description using Bidirectional Recurrent Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.03390\"\n  }, \"http://arxiv.org/abs/1604.03390\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bidirectional Long-Short Term Memory for Video Description\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.04631\"\n  }, \"https://arxiv.org/abs/1606.04631\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"3 Ways to Subtitle and Caption Your Videos Automatically Using Artificial Intelligence\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://photography.tutsplus.com/tutorials/3-ways-to-subtitle-and-caption-your-videos-automatically-using-artificial-intelligence--cms-26834\"\n  }, \"http://photography.tutsplus.com/tutorials/3-ways-to-subtitle-and-caption-your-videos-automatically-using-artificial-intelligence--cms-26834\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.04959\"\n  }, \"http://arxiv.org/abs/1608.04959\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Grounding and Generation of Natural Language Descriptions for Images and Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Anna Rohrbach. Allen Institute for Artificial Intelligence (AI2)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=fE3FX8FowiU\"\n  }, \"https://www.youtube.com/watch?v=fE3FX8FowiU\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Captioning and Retrieval Models with Semantic Attention\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Winner of three (fill-in-the-blank, multiple-choice test, and movie retrieval) out of four tasks of the LSMDC 2016 Challenge (Workshop in ECCV 2016)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.02947\"\n  }, \"https://arxiv.org/abs/1610.02947\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Spatio-Temporal Attention Models for Grounded Video Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.04997\"\n  }, \"https://arxiv.org/abs/1610.04997\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video and Language: Bridging Video and Language with Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV-MM 2016. captioning, commenting, alignment\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/Video-and-Language-ECCV-MM-2016-Tao-Mei-Pub.pdf\"\n  }, \"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/Video-and-Language-ECCV-MM-2016-Tao-Mei-Pub.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recurrent Memory Addressing for describing videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06492\"\n  }, \"https://arxiv.org/abs/1611.06492\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Captioning with Transferred Semantic Attributes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.07675\"\n  }, \"https://arxiv.org/abs/1611.07675\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adaptive Feature Abstraction for Translating Video to Language\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.07837\"\n  }, \"https://arxiv.org/abs/1611.07837\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Semantic Compositional Networks for Visual Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. Duke University & Tsinghua University & MSR\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.08002\"\n  }, \"https://arxiv.org/abs/1611.08002\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhegan27/SCN_for_video_captioning\"\n  }, \"https://github.com/zhegan27/SCN_for_video_captioning\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Boundary-Aware Neural Encoder for Video Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.09312\"\n  }, \"https://arxiv.org/abs/1611.09312\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Attention-Based Multimodal Fusion for Video Description\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.03126\"\n  }, \"https://arxiv.org/abs/1701.03126\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Weakly Supervised Dense Video Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.01502\"\n  }, \"https://arxiv.org/abs/1704.01502\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generating Descriptions with Grounded and Co-Referenced People\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. movie description\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.01518\"\n  }, \"https://arxiv.org/abs/1704.01518\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-Task Video Captioning with Video and Entailment Generation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ACL 2017. UNC Chapel Hill\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.07489\"\n  }, \"https://arxiv.org/abs/1704.07489\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dense-Captioning Events in Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/people/ranjaykrishna/densevid/\"\n  }, \"http://cs.stanford.edu/people/ranjaykrishna/densevid/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.00754\"\n  }, \"https://arxiv.org/abs/1705.00754\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1706.01231\"\n  }, \"https://arxiv.org/abs/1706.01231\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reinforced Video Captioning with Entailment Rewards\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: EMNLP 2017. UNC Chapel Hill\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.02300\"\n  }, \"https://arxiv.org/abs/1708.02300\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. Winner of three (fill-in-the-blank, multiple-choice test, and movie retrieval) out of four tasks of the LSMDC 2016 Challenge\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.02947\"\n  }, \"https://arxiv.org/abs/1610.02947\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://drive.google.com/file/d/0B9nOObAFqKC9aHl2VWJVNFp1bFk/view\"\n  }, \"https://drive.google.com/file/d/0B9nOObAFqKC9aHl2VWJVNFp1bFk/view\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"From Deterministic to Generative: Multi-Modal Stochastic RNNs for Video Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.02478\"\n  }, \"https://arxiv.org/abs/1708.02478\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Grounded Objects and Interactions for Video Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.06354\"\n  }, \"https://arxiv.org/abs/1711.06354\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Integrating both Visual and Audio Cues for Enhanced Video Caption\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.08097\"\n  }, \"https://arxiv.org/abs/1711.08097\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Video Captioning via Hierarchical Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.11135\"\n  }, \"https://arxiv.org/abs/1711.11135\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Consensus-based Sequence Training for Video Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.09532\"\n  }, \"https://arxiv.org/abs/1712.09532\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Less Is More: Picking Informative Frames for Video Captioning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.01457\"\n  }, \"https://arxiv.org/abs/1803.01457\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Video Captioning with Multitask Reinforcement Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.07950\"\n  }, \"https://arxiv.org/abs/1803.07950\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-End Dense Video Captioning with Masked Transformer\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018. University of Michigan & Salesforce Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.00819\"\n  }, \"https://arxiv.org/abs/1804.00819\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Reconstruction Network for Video Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.11438\"\n  }, \"https://arxiv.org/abs/1803.11438\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018 spotlight paper\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.00100\"\n  }, \"https://arxiv.org/abs/1804.00100\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Jointly Localizing and Describing Events for Dense Video Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018 Spotlight, Rank 1 in ActivityNet Captions Challenge 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.08274\"\n  }, \"https://arxiv.org/abs/1804.08274\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Contextualize, Show and Tell: A Neural Visual Storyteller\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.00738\"\n  }, \"https://arxiv.org/abs/1806.00738\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RUC+CMU: System Report for Dense Captioning Events in Videos\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Winner in ActivityNet 2018 Dense Video Captioning challenge\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.08854\"\n  }, \"https://arxiv.org/abs/1806.08854\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Streamlined Dense Video Captioning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.03870\"\n  }, \"https://arxiv.org/abs/1904.03870\"))), mdx(\"h1\", {\n    \"id\": \"projects\"\n  }, \"Projects\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning CNN-LSTM Architectures for Image Caption Generation: An implementation of CNN-LSTM image caption generator architecture that achieves close to state-of-the-art results on the MSCOCO dataset.\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mosessoh/CNN-LSTM-Caption-Generator\"\n  }, \"https://github.com/mosessoh/CNN-LSTM-Caption-Generator\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"screengrab-caption: an openframeworks app that live-captions your desktop screen with a neural net\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: openframeworks app which grabs your desktop screen, then sends it to darknet for captioning.\\nworks great with video calls.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/genekogan/screengrab-caption\"\n  }, \"https://github.com/genekogan/screengrab-caption\"))), mdx(\"h1\", {\n    \"id\": \"tools\"\n  }, \"Tools\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CaptionBot (Microsoft)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"website: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.captionbot.ai/\"\n  }, \"https://www.captionbot.ai/\"))), mdx(\"h1\", {\n    \"id\": \"blogs\"\n  }, \"Blogs\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Captioning Novel Objects in Images\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://bair.berkeley.edu/jacky/2017/08/08/novel-object-captioning/\"\n  }, \"http://bair.berkeley.edu/jacky/2017/08/08/novel-object-captioning/\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\r\nlayout: post\r\ncategory: deep_learning\r\ntitle: Image / Video Captioning\r\ndate: 2015-10-09\r\n---\r\n\r\n# Papers\r\n\r\n**Im2Text: Describing Images Using 1 Million Captioned Photographs**\r\n\r\n![](http://vision.cs.stonybrook.edu/~vicente/sbucaptions/im2text_files/im2text.png)\r\n\r\n- paper: [http://tamaraberg.com/papers/generation_nips2011.pdf](http://tamaraberg.com/papers/generation_nips2011.pdf)\r\n- project: [http://vision.cs.stonybrook.edu/~vicente/sbucaptions/](http://vision.cs.stonybrook.edu/~vicente/sbucaptions/)\r\n\r\n**Long-term Recurrent Convolutional Networks for Visual Recognition and Description**\r\n\r\n![](http://jeffdonahue.com/lrcn/images/lrcn_tasks.png)\r\n\r\n- intro: Oral presentation at CVPR 2015. LRCN\r\n- project page: [http://jeffdonahue.com/lrcn/](http://jeffdonahue.com/lrcn/)\r\n- arxiv: [http://arxiv.org/abs/1411.4389](http://arxiv.org/abs/1411.4389)\r\n- github: [https://github.com/BVLC/caffe/pull/2033](https://github.com/BVLC/caffe/pull/2033)\r\n\r\n## Show and Tell\r\n\r\n**Show and Tell: A Neural Image Caption Generator**\r\n\r\n- intro: Google\r\n- arxiv: [http://arxiv.org/abs/1411.4555](http://arxiv.org/abs/1411.4555)\r\n- github: [https://github.com/karpathy/neuraltalk](https://github.com/karpathy/neuraltalk)\r\n- gitxiv: [http://gitxiv.com/posts/7nofxjoYBXga5XjtL/show-and-tell-a-neural-image-caption-nic-generator](http://gitxiv.com/posts/7nofxjoYBXga5XjtL/show-and-tell-a-neural-image-caption-nic-generator)\r\n- github: [https://github.com/apple2373/chainer_caption_generation](https://github.com/apple2373/chainer_caption_generation)\r\n- github(TensorFlow): [https://github.com/tensorflow/models/tree/master/im2txt](https://github.com/tensorflow/models/tree/master/im2txt)\r\n- github(TensorFlow): [https://github.com/zsdonghao/Image-Captioning](https://github.com/zsdonghao/Image-Captioning)\r\n\r\n**Image caption generation by CNN and LSTM** \r\n\r\n![](http://1.bp.blogspot.com/-RgNlhmSJvgU/Vna0V_GNGXI/AAAAAAAAACw/tjYqwuGHekM/s400/Screen%2BShot%2B2015-12-20%2Bat%2B08.59.26.png)\r\n\r\n- blog: [http://t-satoshi.blogspot.com/2015/12/image-caption-generation-by-cnn-and-lstm.html](http://t-satoshi.blogspot.com/2015/12/image-caption-generation-by-cnn-and-lstm.html)\r\n- github: [https://github.com/jazzsaxmafia/show_and_tell.tensorflow](https://github.com/jazzsaxmafia/show_and_tell.tensorflow)\r\n\r\n**Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**\r\n\r\n- arxiv: [http://arxiv.org/abs/1609.06647](http://arxiv.org/abs/1609.06647)\r\n- github: [https://github.com/tensorflow/models/tree/master/im2txt](https://github.com/tensorflow/models/tree/master/im2txt)\r\n\r\n**Learning a Recurrent Visual Representation for Image Caption Generation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1411.5654](http://arxiv.org/abs/1411.5654)\r\n\r\n**Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation**\r\n\r\n- intro: CVPR 2015\r\n- paper: [http://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf](http://www.cs.cmu.edu/~xinleic/papers/cvpr15_rnn.pdf)\r\n\r\n**Deep Visual-Semantic Alignments for Generating Image Descriptions**\r\n\r\n- intro: \"propose a multimodal deep network that aligns various interesting \r\nregions of the image, represented using a CNN feature, with associated words. \r\nThe learned correspondences are then used to train a bi-directional RNN. \r\nThis model is able, not only to generate descriptions for images, but also \r\nto localize different segments of the sentence to their corresponding image regions.\"\r\n- project page: [http://cs.stanford.edu/people/karpathy/deepimagesent/](http://cs.stanford.edu/people/karpathy/deepimagesent/)\r\n- arxiv: [http://arxiv.org/abs/1412.2306](http://arxiv.org/abs/1412.2306)\r\n- slides: [http://www.cs.toronto.edu/~vendrov/DeepVisualSemanticAlignments_Class_Presentation.pdf](http://www.cs.toronto.edu/~vendrov/DeepVisualSemanticAlignments_Class_Presentation.pdf)\r\n- github: [https://github.com/karpathy/neuraltalk](https://github.com/karpathy/neuraltalk)\r\n- demo: [http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/](http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/)\r\n\r\n**Deep Captioning with Multimodal Recurrent Neural Networks**\r\n\r\n- intro: m-RNN. ICLR 2015\r\n- intro: \"combines the functionalities of the CNN and RNN by introducing a new multimodal layer, \r\nafter the embedding and recurrent layers of the RNN.\"\r\n- homepage: [http://www.stat.ucla.edu/~junhua.mao/m-RNN.html](http://www.stat.ucla.edu/~junhua.mao/m-RNN.html)\r\n- arxiv: [http://arxiv.org/abs/1412.6632](http://arxiv.org/abs/1412.6632)\r\n- github: [https://github.com/mjhucla/mRNN-CR](https://github.com/mjhucla/mRNN-CR)\r\n- github: [https://github.com/mjhucla/TF-mRNN](https://github.com/mjhucla/TF-mRNN)\r\n\r\n## Show, Attend and Tell\r\n\r\n**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (ICML 2015)**\r\n\r\n![](http://kelvinxu.github.io/projects/diags/model_diag.png)\r\n\r\n- project page: [http://kelvinxu.github.io/projects/capgen.html](http://kelvinxu.github.io/projects/capgen.html)\r\n- arxiv: [http://arxiv.org/abs/1502.03044](http://arxiv.org/abs/1502.03044)\r\n- github: [https://github.com/kelvinxu/arctic-captions](https://github.com/kelvinxu/arctic-captions)\r\n- github: [https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow](https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow)\r\n- github(TensorFlow): [https://github.com/yunjey/show-attend-and-tell-tensorflow](https://github.com/yunjey/show-attend-and-tell-tensorflow)\r\n- demo: [http://www.cs.toronto.edu/~rkiros/abstract_captions.html](http://www.cs.toronto.edu/~rkiros/abstract_captions.html)\r\n\r\n**Automatically describing historic photographs**\r\n\r\n- website: [https://staff.fnwi.uva.nl/d.elliott/loc/](https://staff.fnwi.uva.nl/d.elliott/loc/)\r\n\r\n- - -\r\n\r\n**Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images**\r\n\r\n- arxiv: [http://arxiv.org/abs/1504.06692](http://arxiv.org/abs/1504.06692)\r\n- homepage: [http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html](http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html)\r\n- github: [https://github.com/mjhucla/NVC-Dataset](https://github.com/mjhucla/NVC-Dataset)\r\n\r\n**What value do explicit high level concepts have in vision to language problems?**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.01144](http://arxiv.org/abs/1506.01144)\r\n\r\n**Aligning where to see and what to tell: image caption with region-based attention and scene factorization**\r\n\r\n- arxiv: [http://arxiv.org/abs/1506.06272](http://arxiv.org/abs/1506.06272)\r\n\r\n**Learning FRAME Models Using CNN Filters for Knowledge Visualization (CVPR 2015)**\r\n\r\n- project page: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/main.html)\r\n- arxiv: [http://arxiv.org/abs/1509.08379](http://arxiv.org/abs/1509.08379)\r\n- code+data: [http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/deepFRAME_1.1.zip](http://www.stat.ucla.edu/~yang.lu/project/deepFrame/doc/deepFRAME_1.1.zip)\r\n\r\n**Generating Images from Captions with Attention**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.02793](http://arxiv.org/abs/1511.02793)\r\n- github: [https://github.com/emansim/text2image](https://github.com/emansim/text2image)\r\n- demo: [http://www.cs.toronto.edu/~emansim/cap2im.html](http://www.cs.toronto.edu/~emansim/cap2im.html)\r\n\r\n**Order-Embeddings of Images and Language**\r\n\r\n- arxiv: [http://arxiv.org/abs/1511.06361](http://arxiv.org/abs/1511.06361)\r\n- github: [https://github.com/ivendrov/order-embedding](https://github.com/ivendrov/order-embedding)\r\n\r\n**DenseCap: Fully Convolutional Localization Networks for Dense Captioning**\r\n\r\n- project page: [http://cs.stanford.edu/people/karpathy/densecap/](http://cs.stanford.edu/people/karpathy/densecap/)\r\n- arxiv: [http://arxiv.org/abs/1511.07571](http://arxiv.org/abs/1511.07571)\r\n- github(Torch): [https://github.com/jcjohnson/densecap](https://github.com/jcjohnson/densecap)\r\n\r\n**Expressing an Image Stream with a Sequence of Natural Sentences**\r\n\r\n- intro: NIPS 2015. CRCN\r\n- nips-page: [http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences](http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences)\r\n- paper: [http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf](http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf)\r\n- paper: [http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf](http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf)\r\n- author-page: [http://www.cs.cmu.edu/~gunhee/](http://www.cs.cmu.edu/~gunhee/)\r\n- github: [https://github.com/cesc-park/CRCN](https://github.com/cesc-park/CRCN)\r\n\r\n**Multimodal Pivots for Image Caption Translation**\r\n\r\n- intro: ACL 2016\r\n- arxiv: [http://arxiv.org/abs/1601.03916](http://arxiv.org/abs/1601.03916)\r\n\r\n**Image Captioning with Deep Bidirectional LSTMs**\r\n\r\n- intro: ACMMM 2016\r\n- arxiv: [http://arxiv.org/abs/1604.00790](http://arxiv.org/abs/1604.00790)\r\n- github(Caffe): [https://github.com/deepsemantic/image_captioning](https://github.com/deepsemantic/image_captioning)\r\n- demo: [https://youtu.be/a0bh9_2LE24](https://youtu.be/a0bh9_2LE24)\r\n\r\n**Encode, Review, and Decode: Reviewer Module for Caption Generation**\r\n\r\n**Review Network for Caption Generation**\r\n\r\n- intro: NIPS 2016\r\n- arxiv: [https://arxiv.org/abs/1605.07912](https://arxiv.org/abs/1605.07912)\r\n- github: [https://github.com/kimiyoung/review_net](https://github.com/kimiyoung/review_net)\r\n\r\n**Attention Correctness in Neural Image Captioning**\r\n\r\n- arxiv: [http://arxiv.org/abs/1605.09553](http://arxiv.org/abs/1605.09553)\r\n\r\n**Image Caption Generation with Text-Conditional Semantic Attention**\r\n\r\n- arxiv: [https://arxiv.org/abs/1606.04621](https://arxiv.org/abs/1606.04621)\r\n- github: [https://github.com/LuoweiZhou/e2e-gLSTM-sc](https://github.com/LuoweiZhou/e2e-gLSTM-sc)\r\n\r\n**DeepDiary: Automatic Caption Generation for Lifelogging Image Streams**\r\n\r\n- intro: ECCV International Workshop on Egocentric Perception, Interaction, and Computing\r\n- arxiv: [http://arxiv.org/abs/1608.03819](http://arxiv.org/abs/1608.03819)\r\n\r\n**phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning**\r\n\r\n- intro: ACCV 2016\r\n- arxiv: [http://arxiv.org/abs/1608.05813](http://arxiv.org/abs/1608.05813)\r\n\r\n**Captioning Images with Diverse Objects**\r\n\r\n- arxiv: [http://arxiv.org/abs/1606.07770](http://arxiv.org/abs/1606.07770)\r\n\r\n**Learning to generalize to new compositions in image understanding**\r\n\r\n- arxiv: [http://arxiv.org/abs/1608.07639](http://arxiv.org/abs/1608.07639)\r\n\r\n**Generating captions without looking beyond objects**\r\n\r\n- intro: ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)\r\n- arxiv: [https://arxiv.org/abs/1610.03708](https://arxiv.org/abs/1610.03708)\r\n\r\n**SPICE: Semantic Propositional Image Caption Evaluation**\r\n\r\n![](http://www.panderson.me/images/spice-concept.png)\r\n\r\n- intro: ECCV 2016\r\n- project page: [http://www.panderson.me/spice/](http://www.panderson.me/spice/)\r\n- paper: [http://www.panderson.me/images/SPICE.pdf](http://www.panderson.me/images/SPICE.pdf)\r\n- github: [https://github.com/peteanderson80/SPICE](https://github.com/peteanderson80/SPICE)\r\n\r\n**Boosting Image Captioning with Attributes**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.01646](https://arxiv.org/abs/1611.01646)\r\n\r\n**Bootstrap, Review, Decode: Using Out-of-Domain Textual Data to Improve Image Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.05321](https://arxiv.org/abs/1611.05321)\r\n\r\n**A Hierarchical Approach for Generating Descriptive Image Paragraphs**\r\n\r\n- intro: Stanford University\r\n- arxiv: [https://arxiv.org/abs/1611.06607](https://arxiv.org/abs/1611.06607)\r\n\r\n**Dense Captioning with Joint Inference and Visual Context**\r\n\r\n- intro: Snap Inc.\r\n- arxiv: [https://arxiv.org/abs/1611.06949](https://arxiv.org/abs/1611.06949)\r\n\r\n**Optimization of image description metrics using policy gradient methods**\r\n\r\n- intro: University of Oxford & Google\r\n- arxiv: [https://arxiv.org/abs/1612.00370](https://arxiv.org/abs/1612.00370)\r\n\r\n**Areas of Attention for Image Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.01033](https://arxiv.org/abs/1612.01033)\r\n\r\n**Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning**\r\n\r\n- intro: CVPR 2017\r\n- arxiv: [https://arxiv.org/abs/1612.01887](https://arxiv.org/abs/1612.01887)\r\n- github: [https://github.com/jiasenlu/AdaptiveAttention](https://github.com/jiasenlu/AdaptiveAttention)\r\n\r\n**Recurrent Image Captioner: Describing Images with Spatial-Invariant Transformation and Attention Filtering**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.04949](https://arxiv.org/abs/1612.04949)\r\n\r\n**Recurrent Highway Networks with Language CNN for Image Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.07086](https://arxiv.org/abs/1612.07086)\r\n\r\n**Top-down Visual Saliency Guided by Captions**\r\n\r\n- arxiv: [https://arxiv.org/abs/1612.07360](https://arxiv.org/abs/1612.07360)\r\n- github: [https://github.com/VisionLearningGroup/caption-guided-saliency](https://github.com/VisionLearningGroup/caption-guided-saliency)\r\n\r\n**MAT: A Multimodal Attentive Translator for Image Captioning**\r\n\r\n[https://arxiv.org/abs/1702.05658](https://arxiv.org/abs/1702.05658)\r\n\r\n**Deep Reinforcement Learning-based Image Captioning with Embedding Reward**\r\n\r\n- intro: Snap Inc & Google Inc\r\n- arxiv: [https://arxiv.org/abs/1704.03899](https://arxiv.org/abs/1704.03899)\r\n\r\n**Attend to You: Personalized Image Captioning with Context Sequence Memory Networks**\r\n\r\n- intro: CVPR 2017\r\n- arxiv: [https://arxiv.org/abs/1704.06485](https://arxiv.org/abs/1704.06485)\r\n- github: [https://github.com/cesc-park/attend2u](https://github.com/cesc-park/attend2u)\r\n\r\n**Punny Captions: Witty Wordplay in Image Descriptions**\r\n\r\n[https://arxiv.org/abs/1704.08224](https://arxiv.org/abs/1704.08224)\r\n\r\n**Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner**\r\n\r\n[https://arxiv.org/abs/1705.00930](https://arxiv.org/abs/1705.00930)\r\n\r\n**Actor-Critic Sequence Training for Image Captioning**\r\n\r\n- intro: Queen Mary University of London & Yang’s Accounting Consultancy Ltd\r\n- keywords: actor-critic reinforcement learning\r\n- arxiv: [https://arxiv.org/abs/1706.09601](https://arxiv.org/abs/1706.09601)\r\n\r\n**What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?**\r\n\r\n- intro: Proceedings of the 10th International Conference on Natural Language Generation (INLG'17)\r\n- arxiv: [https://arxiv.org/abs/1708.02043](https://arxiv.org/abs/1708.02043)\r\n\r\n**Stack-Captioning: Coarse-to-Fine Learning for Image Captioning**\r\n\r\n[https://arxiv.org/abs/1709.03376](https://arxiv.org/abs/1709.03376)\r\n\r\n**Self-Guiding Multimodal LSTM - when we do not have a perfect training dataset for image captioning**\r\n\r\n[https://arxiv.org/abs/1709.05038](https://arxiv.org/abs/1709.05038)\r\n\r\n**Contrastive Learning for Image Captioning**\r\n\r\n- intro: NIPS 2017\r\n- arxiv: [https://arxiv.org/abs/1710.02534](https://arxiv.org/abs/1710.02534)\r\n\r\n**Phrase-based Image Captioning with Hierarchical LSTM Model**\r\n\r\n- intro: ACCV2016 extension, phrase-based image captioning\r\n- arxiv: [https://arxiv.org/abs/1711.05557](https://arxiv.org/abs/1711.05557)\r\n\r\n**Convolutional Image Captioning**\r\n\r\n[https://arxiv.org/abs/1711.09151](https://arxiv.org/abs/1711.09151)\r\n\r\n**Show-and-Fool: Crafting Adversarial Examples for Neural Image Captioning**\r\n\r\n[https://arxiv.org/abs/1712.02051](https://arxiv.org/abs/1712.02051)\r\n\r\n**Improved Image Captioning with Adversarial Semantic Alignment**\r\n\r\n- intro: IBM Research\r\n- arxiv: [https://arxiv.org/abs/1805.00063](https://arxiv.org/abs/1805.00063)\r\n\r\n**Object Counts! Bringing Explicit Detections Back into Image Captioning**\r\n\r\n- intro: NAACL 2018\r\n- arxiv: [https://arxiv.org/abs/1805.00314](https://arxiv.org/abs/1805.00314)\r\n\r\n**Defoiling Foiled Image Captions**\r\n\r\n- intro: NAACL 2018\r\n- arxiv: [https://arxiv.org/abs/1805.06549](https://arxiv.org/abs/1805.06549)\r\n\r\n**SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1805.07030](https://arxiv.org/abs/1805.07030)\r\n\r\n**Improving Image Captioning with Conditional Generative Adversarial Nets**\r\n\r\n[https://arxiv.org/abs/1805.07112](https://arxiv.org/abs/1805.07112)\r\n\r\n**CNN+CNN: Convolutional Decoders for Image Captioning**\r\n\r\n[https://arxiv.org/abs/1805.09019](https://arxiv.org/abs/1805.09019)\r\n\r\n**Diverse and Controllable Image Captioning with Part-of-Speech Guidance**\r\n\r\n[https://arxiv.org/abs/1805.12589](https://arxiv.org/abs/1805.12589)\r\n\r\n**Learning to Evaluate Image Captioning**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1806.06422](https://arxiv.org/abs/1806.06422)\r\n\r\n**Topic-Guided Attention for Image Captioning**\r\n\r\n- intro: ICIP 2018\r\n- arxiv: [https://arxiv.org/abs/1807.03514](https://arxiv.org/abs/1807.03514)\r\n\r\n**Context-Aware Visual Policy Network for Sequence-Level Image Captioning**\r\n\r\n- intro: ACM MM 2018 oral\r\n- arxiv: [https://arxiv.org/abs/1808.05864](https://arxiv.org/abs/1808.05864)\r\n- github: [https://github.com/daqingliu/CAVP](https://github.com/daqingliu/CAVP)\r\n\r\n**Exploring Visual Relationship for Image Captioning**\r\n\r\n- intro: ECCV 2018\r\n- arxiv: [https://arxiv.org/abs/1809.07041](https://arxiv.org/abs/1809.07041)\r\n\r\n**Boosted Attention: Leveraging Human Attention for Image Captioning**\r\n\r\n- intro: ECCV 2018\r\n- arxiv: [https://arxiv.org/abs/1904.00767](https://arxiv.org/abs/1904.00767)\r\n\r\n**Image Captioning as Neural Machine Translation Task in SOCKEYE**\r\n\r\n[https://arxiv.org/abs/1810.04101](https://arxiv.org/abs/1810.04101)\r\n\r\n**Unsupervised Image Captioning**\r\n\r\n[https://arxiv.org/abs/1811.10787](https://arxiv.org/abs/1811.10787)\r\n\r\n**Attend More Times for Image Captioning**\r\n\r\n[https://arxiv.org/abs/1812.03283](https://arxiv.org/abs/1812.03283)\r\n\r\n# Object Descriptions\r\n\r\n**Generation and Comprehension of Unambiguous Object Descriptions**\r\n\r\n- arxiv: [https://arxiv.org/abs/1511.02283](https://arxiv.org/abs/1511.02283)\r\n- github: [https://github.com/mjhucla/Google_Refexp_toolbox](https://github.com/mjhucla/Google_Refexp_toolbox)\r\n\r\n# Video Captioning / Description\r\n\r\n**Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework**\r\n\r\n- intro: AAAI 2015\r\n- paper: [http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf)\r\n- paper: [http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf](http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf)\r\n\r\n**Translating Videos to Natural Language Using Deep Recurrent Neural Networks**\r\n\r\n![](https://www.cs.utexas.edu/~vsub/imgs/naacl-15-overview.png)\r\n\r\n- intro: NAACL-HLT 2015 camera ready\r\n- project page: [https://www.cs.utexas.edu/~vsub/naacl15_project.html](https://www.cs.utexas.edu/~vsub/naacl15_project.html)\r\n- arxiv: [http://arxiv.org/abs/1412.4729](http://arxiv.org/abs/1412.4729)\r\n- slides: [https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_slides.pdf](https://www.cs.utexas.edu/~vsub/pdf/Translating_Videos_slides.pdf)\r\n- code+data: [https://www.cs.utexas.edu/~vsub/naacl15_project.html#code](https://www.cs.utexas.edu/~vsub/naacl15_project.html#code)\r\n\r\n**Describing Videos by Exploiting Temporal Structure**\r\n\r\n- arxiv: [http://arxiv.org/abs/1502.08029](http://arxiv.org/abs/1502.08029)\r\n- github: [https://github.com/yaoli/arctic-capgen-vid](https://github.com/yaoli/arctic-capgen-vid)\r\n\r\n**SA-tensorflow: Soft attention mechanism for video caption generation**\r\n\r\n![](https://raw.githubusercontent.com/tsenghungchen/SA-tensorflow/master/README_files/head.png)\r\n\r\n- github: [https://github.com/tsenghungchen/SA-tensorflow](https://github.com/tsenghungchen/SA-tensorflow)\r\n\r\n**Sequence to Sequence -- Video to Text**\r\n\r\n![](http://www.cs.utexas.edu/~vsub/imgs/S2VTarchitecture.png)\r\n\r\n- intro: ICCV 2015. S2VT\r\n- project page: [http://vsubhashini.github.io/s2vt.html](http://vsubhashini.github.io/s2vt.html)\r\n- arxiv: [http://arxiv.org/abs/1505.00487](http://arxiv.org/abs/1505.00487)\r\n- slides: [https://www.cs.utexas.edu/~vsub/pdf/S2VT_slides.pdf](https://www.cs.utexas.edu/~vsub/pdf/S2VT_slides.pdf)\r\n- github(Caffe): [https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt](https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt)\r\n- github(TensorFlow): [https://github.com/jazzsaxmafia/video_to_sequence](https://github.com/jazzsaxmafia/video_to_sequence)\r\n\r\n**Jointly Modeling Embedding and Translation to Bridge Video and Language**\r\n\r\n- arxiv: [http://arxiv.org/abs/1505.01861](http://arxiv.org/abs/1505.01861)\r\n\r\n**Video Description using Bidirectional Recurrent Neural Networks**\r\n\r\n- arxiv: [http://arxiv.org/abs/1604.03390](http://arxiv.org/abs/1604.03390)\r\n\r\n**Bidirectional Long-Short Term Memory for Video Description**\r\n\r\n- arxiv: [https://arxiv.org/abs/1606.04631](https://arxiv.org/abs/1606.04631)\r\n\r\n**3 Ways to Subtitle and Caption Your Videos Automatically Using Artificial Intelligence**\r\n\r\n- blog: [http://photography.tutsplus.com/tutorials/3-ways-to-subtitle-and-caption-your-videos-automatically-using-artificial-intelligence--cms-26834](http://photography.tutsplus.com/tutorials/3-ways-to-subtitle-and-caption-your-videos-automatically-using-artificial-intelligence--cms-26834)\r\n\r\n**Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation**\r\n\r\n- arxiv: [http://arxiv.org/abs/1608.04959](http://arxiv.org/abs/1608.04959)\r\n\r\n**Grounding and Generation of Natural Language Descriptions for Images and Videos**\r\n\r\n- intro: Anna Rohrbach. Allen Institute for Artificial Intelligence (AI2)\r\n- youtube: [https://www.youtube.com/watch?v=fE3FX8FowiU](https://www.youtube.com/watch?v=fE3FX8FowiU)\r\n\r\n**Video Captioning and Retrieval Models with Semantic Attention**\r\n\r\n- intro: Winner of three (fill-in-the-blank, multiple-choice test, and movie retrieval) out of four tasks of the LSMDC 2016 Challenge (Workshop in ECCV 2016)\r\n- arxiv: [https://arxiv.org/abs/1610.02947](https://arxiv.org/abs/1610.02947)\r\n\r\n**Spatio-Temporal Attention Models for Grounded Video Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1610.04997](https://arxiv.org/abs/1610.04997)\r\n\r\n**Video and Language: Bridging Video and Language with Deep Learning**\r\n\r\n- intro: ECCV-MM 2016. captioning, commenting, alignment\r\n- slides: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/Video-and-Language-ECCV-MM-2016-Tao-Mei-Pub.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/10/Video-and-Language-ECCV-MM-2016-Tao-Mei-Pub.pdf)\r\n\r\n**Recurrent Memory Addressing for describing videos**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.06492](https://arxiv.org/abs/1611.06492)\r\n\r\n**Video Captioning with Transferred Semantic Attributes**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.07675](https://arxiv.org/abs/1611.07675)\r\n\r\n**Adaptive Feature Abstraction for Translating Video to Language**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.07837](https://arxiv.org/abs/1611.07837)\r\n\r\n**Semantic Compositional Networks for Visual Captioning**\r\n\r\n- intro: CVPR 2017. Duke University & Tsinghua University & MSR\r\n- arxiv: [https://arxiv.org/abs/1611.08002](https://arxiv.org/abs/1611.08002)\r\n- github: [https://github.com/zhegan27/SCN_for_video_captioning](https://github.com/zhegan27/SCN_for_video_captioning)\r\n\r\n**Hierarchical Boundary-Aware Neural Encoder for Video Captioning**\r\n\r\n- arxiv: [https://arxiv.org/abs/1611.09312](https://arxiv.org/abs/1611.09312)\r\n\r\n**Attention-Based Multimodal Fusion for Video Description**\r\n\r\n- arxiv: [https://arxiv.org/abs/1701.03126](https://arxiv.org/abs/1701.03126)\r\n\r\n**Weakly Supervised Dense Video Captioning**\r\n\r\n- intro: CVPR 2017\r\n- arxiv: [https://arxiv.org/abs/1704.01502](https://arxiv.org/abs/1704.01502)\r\n\r\n**Generating Descriptions with Grounded and Co-Referenced People**\r\n\r\n- intro: CVPR 2017. movie description\r\n- arxiv: [https://arxiv.org/abs/1704.01518](https://arxiv.org/abs/1704.01518)\r\n\r\n**Multi-Task Video Captioning with Video and Entailment Generation**\r\n\r\n- intro: ACL 2017. UNC Chapel Hill\r\n- arxiv: [https://arxiv.org/abs/1704.07489](https://arxiv.org/abs/1704.07489)\r\n\r\n**Dense-Captioning Events in Videos**\r\n\r\n- project page: [http://cs.stanford.edu/people/ranjaykrishna/densevid/](http://cs.stanford.edu/people/ranjaykrishna/densevid/)\r\n- arxiv: [https://arxiv.org/abs/1705.00754](https://arxiv.org/abs/1705.00754)\r\n\r\n**Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1706.01231](https://arxiv.org/abs/1706.01231)\r\n\r\n**Reinforced Video Captioning with Entailment Rewards**\r\n\r\n- intro: EMNLP 2017. UNC Chapel Hill\r\n- arxiv: [https://arxiv.org/abs/1708.02300](https://arxiv.org/abs/1708.02300)\r\n\r\n**End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering**\r\n\r\n- intro: CVPR 2017. Winner of three (fill-in-the-blank, multiple-choice test, and movie retrieval) out of four tasks of the LSMDC 2016 Challenge\r\n- arxiv: [https://arxiv.org/abs/1610.02947](https://arxiv.org/abs/1610.02947)\r\n- slides: [https://drive.google.com/file/d/0B9nOObAFqKC9aHl2VWJVNFp1bFk/view](https://drive.google.com/file/d/0B9nOObAFqKC9aHl2VWJVNFp1bFk/view)\r\n\r\n**From Deterministic to Generative: Multi-Modal Stochastic RNNs for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1708.02478](https://arxiv.org/abs/1708.02478)\r\n\r\n**Grounded Objects and Interactions for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1711.06354](https://arxiv.org/abs/1711.06354)\r\n\r\n**Integrating both Visual and Audio Cues for Enhanced Video Caption**\r\n\r\n[https://arxiv.org/abs/1711.08097](https://arxiv.org/abs/1711.08097)\r\n\r\n**Video Captioning via Hierarchical Reinforcement Learning**\r\n\r\n[https://arxiv.org/abs/1711.11135](https://arxiv.org/abs/1711.11135)\r\n\r\n**Consensus-based Sequence Training for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1712.09532](https://arxiv.org/abs/1712.09532)\r\n\r\n**Less Is More: Picking Informative Frames for Video Captioning**\r\n\r\n[https://arxiv.org/abs/1803.01457](https://arxiv.org/abs/1803.01457)\r\n\r\n**End-to-End Video Captioning with Multitask Reinforcement Learning**\r\n\r\n[https://arxiv.org/abs/1803.07950](https://arxiv.org/abs/1803.07950)\r\n\r\n**End-to-End Dense Video Captioning with Masked Transformer**\r\n\r\n- intro: CVPR 2018. University of Michigan & Salesforce Research\r\n- arxiv: [https://arxiv.org/abs/1804.00819](https://arxiv.org/abs/1804.00819)\r\n\r\n**Reconstruction Network for Video Captioning**\r\n\r\n- intro: CVPR 2018\r\n- arxiv: [https://arxiv.org/abs/1803.11438](https://arxiv.org/abs/1803.11438)\r\n\r\n**Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning**\r\n\r\n- intro: CVPR 2018 spotlight paper\r\n- arxiv: [https://arxiv.org/abs/1804.00100](https://arxiv.org/abs/1804.00100)\r\n\r\n**Jointly Localizing and Describing Events for Dense Video Captioning**\r\n\r\n- intro: CVPR 2018 Spotlight, Rank 1 in ActivityNet Captions Challenge 2017\r\n- arxiv: [https://arxiv.org/abs/1804.08274](https://arxiv.org/abs/1804.08274)\r\n\r\n**Contextualize, Show and Tell: A Neural Visual Storyteller**\r\n\r\n[https://arxiv.org/abs/1806.00738](https://arxiv.org/abs/1806.00738)\r\n\r\n**RUC+CMU: System Report for Dense Captioning Events in Videos**\r\n\r\n- intro: Winner in ActivityNet 2018 Dense Video Captioning challenge\r\n- arxiv: [https://arxiv.org/abs/1806.08854](https://arxiv.org/abs/1806.08854)\r\n\r\n**Streamlined Dense Video Captioning**\r\n\r\n- intro: CVPR 2019\r\n- arxiv: [https://arxiv.org/abs/1904.03870](https://arxiv.org/abs/1904.03870)\r\n\r\n# Projects\r\n\r\n**Learning CNN-LSTM Architectures for Image Caption Generation: An implementation of CNN-LSTM image caption generator architecture that achieves close to state-of-the-art results on the MSCOCO dataset.**\r\n\r\n- github: [https://github.com/mosessoh/CNN-LSTM-Caption-Generator](https://github.com/mosessoh/CNN-LSTM-Caption-Generator)\r\n\r\n**screengrab-caption: an openframeworks app that live-captions your desktop screen with a neural net**\r\n\r\n- intro: openframeworks app which grabs your desktop screen, then sends it to darknet for captioning. \r\nworks great with video calls.\r\n- github: [https://github.com/genekogan/screengrab-caption](https://github.com/genekogan/screengrab-caption)\r\n\r\n# Tools\r\n\r\n**CaptionBot (Microsoft)**\r\n\r\n- website: [https://www.captionbot.ai/](https://www.captionbot.ai/)\r\n\r\n# Blogs\r\n\r\n**Captioning Novel Objects in Images**\r\n\r\n[http://bair.berkeley.edu/jacky/2017/08/08/novel-object-captioning/](http://bair.berkeley.edu/jacky/2017/08/08/novel-object-captioning/)\r\n","excerpt":"Papers Im2Text: Describing Images Using 1 Million Captioned Photographs paper:  http://tamaraberg.com/papers/generation_nips2011.pdf projec…","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations → Principles → The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Fake-News-Considerations-%E2%86%92-Principles-%E2%86%92-the-Institution-of-Socio-Economic-Values","url":"","items":[{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations-%e2%86%92-principles-%e2%86%92-the-institution-of-socio-economic-values/solutions-to-fakenews-linked-data-ontologies-and-verifiable-claims/","items":[]}]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between ‘privacy’ and ‘dignity’.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Embed Link","url":"/old-work-archives/2018-webizen-net-au/embed-link/","items":[]},{"title":"Posts","url":"/old-work-archives/2018-webizen-net-au/posts/","items":[]},{"title":"Privacy Policy","url":"/old-work-archives/2018-webizen-net-au/privacy-policy/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Pro Summary","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","title":"Fake News: Considerations → Principles → The Institution of Socio &#8211; Economic Values","lastUpdatedAt":"2022-12-28T19:29:53.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","title":"Handong1587","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","title":"Notes on Suffix Array and Manacher Algorithm","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","title":"Notes On Perceptrons","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","title":"Notes On Object Detection","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","title":"Notes On Caffe Development","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","title":"Notes On L-BFGS","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","title":"Softmax Vs Logistic Vs Sigmoid","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","title":"Notes On Deep Learning Training","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","title":"Notes On YOLO","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}