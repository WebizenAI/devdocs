{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/",
    "result": {"data":{"mdx":{"id":"fc2de05e-ea2d-56e4-b417-acba8b53075f","tableOfContents":{"items":[{"url":"#tutorials","title":"Tutorials"},{"url":"#papers","title":"Papers"},{"url":"#activation-functions","title":"Activation functions","items":[{"url":"#relu","title":"ReLU"},{"url":"#lrelu","title":"LReLU"},{"url":"#prelu","title":"PReLU"},{"url":"#srelu","title":"SReLU"},{"url":"#mba","title":"MBA"},{"url":"#concatenated-relu-crelu","title":"Concatenated ReLU (CRelu)"},{"url":"#gelu","title":"GELU"},{"url":"#selu","title":"SELU"},{"url":"#eraserelu","title":"EraseReLU"},{"url":"#swish","title":"Swish"},{"url":"#series-on-initialization-of-weights-for-dnn","title":"Series on Initialization of Weights for DNN"}]},{"url":"#weights-initialization","title":"Weights Initialization","items":[{"url":"#batch-normalization","title":"Batch Normalization"},{"url":"#layer-normalization","title":"Layer Normalization"},{"url":"#group-normalization","title":"Group Normalization"},{"url":"#batch-instance-normalization","title":"Batch-Instance Normalization"},{"url":"#dynamic-normalization","title":"Dynamic Normalization"}]},{"url":"#loss-function","title":"Loss Function"},{"url":"#learning-rates","title":"Learning Rates"},{"url":"#convolution-filters","title":"Convolution Filters"},{"url":"#pooling","title":"Pooling"},{"url":"#mini-batch","title":"Mini-Batch"},{"url":"#optimization-methods","title":"Optimization Methods","items":[{"url":"#adam","title":"Adam"}]},{"url":"#tensor-methods","title":"Tensor Methods"},{"url":"#regularization","title":"Regularization","items":[{"url":"#dropout","title":"Dropout"},{"url":"#dropconnect","title":"DropConnect"},{"url":"#dropneuron","title":"DropNeuron"},{"url":"#dropblock","title":"DropBlock"},{"url":"#maxout","title":"Maxout"},{"url":"#swapout","title":"Swapout"},{"url":"#whiteout","title":"Whiteout"}]},{"url":"#gradient-descent","title":"Gradient Descent","items":[{"url":"#adagrad","title":"AdaGrad"},{"url":"#momentum","title":"Momentum"}]},{"url":"#backpropagation","title":"Backpropagation"},{"url":"#accelerate-training","title":"Accelerate Training","items":[{"url":"#parallelism","title":"Parallelism"}]},{"url":"#handling-datasets","title":"Handling Datasets","items":[{"url":"#data-augmentation","title":"Data Augmentation"},{"url":"#imbalanced-datasets","title":"Imbalanced Datasets"},{"url":"#noisy--unlabelled-data","title":"Noisy / Unlabelled Data"}]},{"url":"#low-numerical-precision","title":"Low Numerical Precision"},{"url":"#distributed-training","title":"Distributed Training","items":[{"url":"#projects","title":"Projects"},{"url":"#videos","title":"Videos"},{"url":"#blogs","title":"Blogs"}]},{"url":"#adversarial-training","title":"Adversarial Training"},{"url":"#low-precision-training","title":"Low-Precision Training"},{"url":"#incremental-training","title":"Incremental Training"},{"url":"#papers-1","title":"Papers"},{"url":"#tools","title":"Tools"},{"url":"#blogs-1","title":"Blogs"}]},"fields":{"title":"Training Deep Neural Networks","slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","url":"https://devdocs.webizen.org/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","editUrl":"https://github.com/webizenai/devdocs/tree/main/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn.md","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022","gitCreatedAt":"2022-12-28T19:22:29.000Z","shouldShowTitle":true},"frontmatter":{"title":"Training Deep Neural Networks","description":null,"imageAlt":null,"tags":[],"date":"2015-10-09T00:00:00.000Z","dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"layout\": \"post\",\n  \"category\": \"deep_learning\",\n  \"title\": \"Training Deep Neural Networks\",\n  \"date\": \"2015-10-09T00:00:00.000Z\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"tutorials\"\n  }, \"Tutorials\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Popular Training Approaches of DNNs\\u200A\\u2014\\u200AA Quick Overview\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@asjad/popular-training-approaches-of-dnns-a-quick-overview-26ee37ad7e96#.pqyo039bb\"\n  }, \"https://medium.com/@asjad/popular-training-approaches-of-dnns-a-quick-overview-26ee37ad7e96#.pqyo039bb\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Optimisation and training techniques for deep learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/\"\n  }, \"https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/\")), mdx(\"h1\", {\n    \"id\": \"papers\"\n  }, \"Papers\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SNIPER: Efficient Multi-Scale Training\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.09300\"\n  }, \"https://arxiv.org/abs/1805.09300\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RePr: Improved Training of Convolutional Filters\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.07275\"\n  }, \"https://arxiv.org/abs/1811.07275\")), mdx(\"h1\", {\n    \"id\": \"activation-functions\"\n  }, \"Activation functions\"), mdx(\"h2\", {\n    \"id\": \"relu\"\n  }, \"ReLU\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rectified linear units improve restricted boltzmann machines\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ReLU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf\"\n  }, \"http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Expressiveness of Rectifier Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: This paper studies the expressiveness of ReLU Networks\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1511.05678\"\n  }, \"https://arxiv.org/abs/1511.05678\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How can a deep neural network with ReLU activations in its hidden layers approximate any function?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"quora: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.quora.com/How-can-a-deep-neural-network-with-ReLU-activations-in-its-hidden-layers-approximate-any-function\"\n  }, \"https://www.quora.com/How-can-a-deep-neural-network-with-ReLU-activations-in-its-hidden-layers-approximate-any-function\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Understanding Deep Neural Networks with Rectified Linear Units\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Johns Hopkins University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01491\"\n  }, \"https://arxiv.org/abs/1611.01491\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning ReLUs via Gradient Descent\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1705.04591\"\n  }, \"https://arxiv.org/abs/1705.04591\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Better CNNs Requires to Rethink ReLU\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.06247\"\n  }, \"https://arxiv.org/abs/1709.06247\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning using Rectified Linear Units (ReLU)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Adamson University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.08375\"\n  }, \"https://arxiv.org/abs/1803.08375\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/AFAgarap/relu-classifier\"\n  }, \"https://github.com/AFAgarap/relu-classifier\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of California, Los Angeles\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.08888\"\n  }, \"https://arxiv.org/abs/1811.08888\"))), mdx(\"h2\", {\n    \"id\": \"lrelu\"\n  }, \"LReLU\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rectifier Nonlinearities Improve Neural Network Acoustic Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: leaky-ReLU, aka LReLU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf\"\n  }, \"http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Sparse Rectifier Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf\"\n  }, \"http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf\"))), mdx(\"h2\", {\n    \"id\": \"prelu\"\n  }, \"PReLU\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: PReLU, Caffe \\\"msra\\\" weights initilization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1502.01852\"\n  }, \"http://arxiv.org/abs/1502.01852\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Empirical Evaluation of Rectified Activations in Convolutional Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ReLU / LReLU / PReLU / RReLU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1505.00853\"\n  }, \"http://arxiv.org/abs/1505.00853\"))), mdx(\"h2\", {\n    \"id\": \"srelu\"\n  }, \"SReLU\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning with S-shaped Rectified Linear Activation Units\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro:  SReLU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.07030\"\n  }, \"http://arxiv.org/abs/1512.07030\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Parametric Activation Pools greatly increase performance and consistency in ConvNets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.claymcleod.io/2016/02/06/Parametric-Activation-Pools-greatly-increase-performance-and-consistency-in-ConvNets/\"\n  }, \"http://blog.claymcleod.io/2016/02/06/Parametric-Activation-Pools-greatly-increase-performance-and-consistency-in-ConvNets/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02068\"\n  }, \"http://arxiv.org/abs/1602.02068\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/gokceneraslan/SparseMax.torch\"\n  }, \"https://github.com/gokceneraslan/SparseMax.torch\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Unbabel/sparsemax\"\n  }, \"https://github.com/Unbabel/sparsemax\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Revise Saturated Activation Functions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.05980\"\n  }, \"http://arxiv.org/abs/1602.05980\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Noisy Activation Functions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.00391\"\n  }, \"http://arxiv.org/abs/1603.00391\"))), mdx(\"h2\", {\n    \"id\": \"mba\"\n  }, \"MBA\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-Bias Non-linear Activation in Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MBA\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1604.00676\"\n  }, \"https://arxiv.org/abs/1604.00676\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning activation functions from data using cubic spline interpolation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.05509\"\n  }, \"http://arxiv.org/abs/1605.05509\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"bitbucket: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://bitbucket.org/ispamm/spline-nn\"\n  }, \"https://bitbucket.org/ispamm/spline-nn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"What is the role of the activation function in a neural network?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"quora: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network\"\n  }, \"https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network\"))), mdx(\"h2\", {\n    \"id\": \"concatenated-relu-crelu\"\n  }, \"Concatenated ReLU (CRelu)\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.05201\"\n  }, \"http://arxiv.org/abs/1603.05201\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Implement CReLU (Concatenated ReLU)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pfnet/chainer/pull/1142\"\n  }, \"https://github.com/pfnet/chainer/pull/1142\"))), mdx(\"h2\", {\n    \"id\": \"gelu\"\n  }, \"GELU\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.08415\"\n  }, \"https://arxiv.org/abs/1606.08415\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Formulating The ReLU\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.jefkine.com/general/2016/08/24/formulating-the-relu/\"\n  }, \"http://www.jefkine.com/general/2016/08/24/formulating-the-relu/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Activation Ensembles for Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1702.07790\"\n  }, \"https://arxiv.org/abs/1702.07790\")), mdx(\"h2\", {\n    \"id\": \"selu\"\n  }, \"SELU\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Self-Normalizing Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: SELU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.02515\"\n  }, \"https://arxiv.org/abs/1706.02515\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bioinf-jku/SNNs\"\n  }, \"https://github.com/bioinf-jku/SNNs\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/kevinzakka/research-paper-notes/blob/master/snn.md\"\n  }, \"https://github.com/kevinzakka/research-paper-notes/blob/master/snn.md\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Chainer): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/musyoku/self-normalizing-networks\"\n  }, \"https://github.com/musyoku/self-normalizing-networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SELUs (scaled exponential linear units) - Visualized and Histogramed Comparisons among ReLU and Leaky ReLU\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/shaohua0116/Activation-Visualization-Histogram\"\n  }, \"https://github.com/shaohua0116/Activation-Visualization-Histogram\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Difference Between Softmax Function and Sigmoid Function\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\"\n  }, \"http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Flexible Rectified Linear Units for Improving Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: flexible rectified linear unit (FReLU)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.08098\"\n  }, \"https://arxiv.org/abs/1706.08098\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Be Careful What You Backpropagate: A Case For Linear Output Activations & Gradient Boosting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CMU\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.04199\"\n  }, \"https://arxiv.org/abs/1707.04199\"))), mdx(\"h2\", {\n    \"id\": \"eraserelu\"\n  }, \"EraseReLU\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.07634\"\n  }, \"https://arxiv.org/abs/1709.07634\")), mdx(\"h2\", {\n    \"id\": \"swish\"\n  }, \"Swish\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Swish: a Self-Gated Activation Function\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Searching for Activation Functions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Brain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.05941\"\n  }, \"https://arxiv.org/abs/1710.05941\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"reddit: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.reddit.com/r/MachineLearning/comments/77gcrv/d_swish_is_not_performing_very_well/\"\n  }, \"https://www.reddit.com/r/MachineLearning/comments/77gcrv/d_swish_is_not_performing_very_well/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning with Data Dependent Implicit Activation Function\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.00168\"\n  }, \"https://arxiv.org/abs/1802.00168\")), mdx(\"h2\", {\n    \"id\": \"series-on-initialization-of-weights-for-dnn\"\n  }, \"Series on Initialization of Weights for DNN\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Initialization Of Feedfoward Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.jefkine.com/deep/2016/07/27/initialization-of-feedfoward-networks/\"\n  }, \"http://www.jefkine.com/deep/2016/07/27/initialization-of-feedfoward-networks/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Initialization Of Deep Feedfoward Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.jefkine.com/deep/2016/08/01/initialization-of-deep-feedfoward-networks/\"\n  }, \"http://www.jefkine.com/deep/2016/08/01/initialization-of-deep-feedfoward-networks/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Initialization Of Deep Networks Case of Rectifiers\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/\"\n  }, \"http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/\"))), mdx(\"h1\", {\n    \"id\": \"weights-initialization\"\n  }, \"Weights Initialization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Explanation of Xavier Initialization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\"\n  }, \"http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Random Walk Initialization for Training Very Deep Feedforward Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.6558\"\n  }, \"http://arxiv.org/abs/1412.6558\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1504.08291\"\n  }, \"http://arxiv.org/abs/1504.08291\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"All you need is a good init\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Layer-sequential unit-variance (LSUV) initialization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06422\"\n  }, \"http://arxiv.org/abs/1511.06422\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ducha-aiki/LSUVinit\"\n  }, \"https://github.com/ducha-aiki/LSUVinit\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Torch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yobibyte/torch-lsuv\"\n  }, \"https://github.com/yobibyte/torch-lsuv\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yobibyte/yobiblog/blob/master/posts/all-you-need-is-a-good-init.md\"\n  }, \"https://github.com/yobibyte/yobiblog/blob/master/posts/all-you-need-is-a-good-init.md\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Keras): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ducha-aiki/LSUV-keras\"\n  }, \"https://github.com/ducha-aiki/LSUV-keras\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"review: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.erogol.com/need-good-init/\"\n  }, \"http://www.erogol.com/need-good-init/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017. HIKVision\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.01827\"\n  }, \"https://arxiv.org/abs/1703.01827\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data-dependent Initializations of Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06856\"\n  }, \"http://arxiv.org/abs/1511.06856\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/philkr/magic_init\"\n  }, \"https://github.com/philkr/magic_init\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"What are good initial weights in a neural network?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"stackexchange: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network\"\n  }, \"http://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RandomOut: Using a convolutional gradient norm to win The Filter Lottery\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.05931\"\n  }, \"http://arxiv.org/abs/1602.05931\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Categorical Reparameterization with Gumbel-Softmax\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Brain & University of Cambridge & Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01144\"\n  }, \"https://arxiv.org/abs/1611.01144\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ericjang/gumbel-softmax\"\n  }, \"https://github.com/ericjang/gumbel-softmax\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On weight initialization in deep neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1704.08863\"\n  }, \"https://arxiv.org/abs/1704.08863\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/sidkk86/weight_initialization\"\n  }, \"https://github.com/sidkk86/weight_initialization\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2018. Google Brain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.05393\"\n  }, \"https://arxiv.org/abs/1806.05393\"))), mdx(\"h2\", {\n    \"id\": \"batch-normalization\"\n  }, \"Batch Normalization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ImageNet top-5 error: 4.82%\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: internal covariate shift problem\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1502.03167\"\n  }, \"http://arxiv.org/abs/1502.03167\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/\"\n  }, \"https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.csdn.net/happynear/article/details/44238541\"\n  }, \"http://blog.csdn.net/happynear/article/details/44238541\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.07868\"\n  }, \"http://arxiv.org/abs/1602.07868\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Lasagne): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/TimSalimans/weight_norm\"\n  }, \"https://github.com/TimSalimans/weight_norm\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/openai/weightnorm\"\n  }, \"https://github.com/openai/weightnorm\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.erogol.com/my-notes-weight-normalization/\"\n  }, \"http://www.erogol.com/my-notes-weight-normalization/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.01431\"\n  }, \"http://arxiv.org/abs/1603.01431\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Revisiting Batch Normalization For Practical Domain Adaptation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Peking University & TuSimple & SenseTime\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Pattern Recognition\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Adaptive Batch Normalization (AdaBN)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1603.04779\"\n  }, \"https://arxiv.org/abs/1603.04779\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Implementing Batch Normalization in Tensorflow\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://r2rt.com/implementing-batch-normalization-in-tensorflow.html\"\n  }, \"http://r2rt.com/implementing-batch-normalization-in-tensorflow.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deriving the Gradient for the Backward Pass of Batch Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://kevinzakka.github.io/2016/09/14/batch_normalization/\"\n  }, \"https://kevinzakka.github.io/2016/09/14/batch_normalization/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Exploring Normalization in Deep Residual Networks with Concatenated Rectified Linear Units\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Oculus VR & Facebook & NEC Labs America\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://research.fb.com/publications/exploring-normalization-in-deep-residual-networks-with-concatenated-rectified-linear-units/\"\n  }, \"https://research.fb.com/publications/exploring-normalization-in-deep-residual-networks-with-concatenated-rectified-linear-units/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Sergey Ioffe, Google\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.03275\"\n  }, \"https://arxiv.org/abs/1702.03275\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Comparison of Batch Normalization and Weight Normalization Algorithms for the Large-scale Image Classification\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.08145\"\n  }, \"https://arxiv.org/abs/1709.08145\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"In-Place Activated BatchNorm for Memory-Optimized Training of DNNs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Mapillary Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.02616\"\n  }, \"https://arxiv.org/abs/1712.02616\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mapillary/inplace_abn\"\n  }, \"https://github.com/mapillary/inplace_abn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Batch Kalman Normalization: Towards Training Deep Neural Networks with Micro-Batches\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.03133\"\n  }, \"https://arxiv.org/abs/1802.03133\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Decorrelated Batch Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.08450\"\n  }, \"https://arxiv.org/abs/1804.08450\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/umich-vl/DecorrelatedBN\"\n  }, \"https://github.com/umich-vl/DecorrelatedBN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Understanding Batch Normalization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.02375\"\n  }, \"https://arxiv.org/abs/1806.02375\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Implementing Synchronized Multi-GPU Batch Normalization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://hangzh.com/PyTorch-Encoding/notes/syncbn.html\"\n  }, \"http://hangzh.com/PyTorch-Encoding/notes/syncbn.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Restructuring Batch Normalization to Accelerate CNN Training\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1807.01702\"\n  }, \"https://arxiv.org/abs/1807.01702\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Intro to optimization in deep learning: Busting the myth about batch normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://blog.paperspace.com/busting-the-myths-about-batch-normalization/\"\n  }, \"https://blog.paperspace.com/busting-the-myths-about-batch-normalization/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Understanding Regularization in Batch Normalization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.00846\"\n  }, \"https://arxiv.org/abs/1809.00846\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How Does Batch Normalization Help Optimization?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NeurIPS 2018. MIT\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.11604\"\n  }, \"https://arxiv.org/abs/1805.11604\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=ZOabsYbmBRM\"\n  }, \"https://www.youtube.com/watch?v=ZOabsYbmBRM\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cross-Iteration Batch Normalization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2002.05712\"\n  }, \"https://arxiv.org/abs/2002.05712\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Extended Batch Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Chinese Academy of Sciences\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2003.05569\"\n  }, \"https://arxiv.org/abs/2003.05569\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2020 Poster\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Moving Average Batch Normalization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"openreview: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/forum?id=SkgGjRVKDS\"\n  }, \"https://openreview.net/forum?id=SkgGjRVKDS\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, Pytorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/megvii-model/MABN\"\n  }, \"https://github.com/megvii-model/MABN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rethinking \\u201CBatch\\u201D in BatchNorm\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2105.07576\"\n  }, \"https://arxiv.org/abs/2105.07576\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Delving into the Estimation Shift of Batch Normalization in a Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2022\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2203.10778\"\n  }, \"https://arxiv.org/abs/2203.10778\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gtihub: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/huangleiBuaa/XBNBlock\"\n  }, \"https://github.com/huangleiBuaa/XBNBlock\"))), mdx(\"h3\", {\n    \"id\": \"backward-pass-of-bn\"\n  }, \"Backward pass of BN\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Understanding the backward pass through Batch Normalization Layer\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\"\n  }, \"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deriving the Gradient for the Backward Pass of Batch Normalization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://kevinzakka.github.io/2016/09/14/batch_normalization/\"\n  }, \"https://kevinzakka.github.io/2016/09/14/batch_normalization/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"What does the gradient flowing through batch normalization looks like ?\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://cthorey.github.io./backpropagation/\"\n  }, \"http://cthorey.github.io./backpropagation/\")), mdx(\"h2\", {\n    \"id\": \"layer-normalization\"\n  }, \"Layer Normalization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Layer Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1607.06450\"\n  }, \"https://arxiv.org/abs/1607.06450\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ryankiros/layer-norm\"\n  }, \"https://github.com/ryankiros/layer-norm\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pbhatia243/tf-layer-norm\"\n  }, \"https://github.com/pbhatia243/tf-layer-norm\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MycChiu/fast-LayerNorm-TF\"\n  }, \"https://github.com/MycChiu/fast-LayerNorm-TF\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Keras GRU with Layer Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gist: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/udibr/7f46e790c9e342d75dcbd9b1deb9d940\"\n  }, \"https://gist.github.com/udibr/7f46e790c9e342d75dcbd9b1deb9d940\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1702.05870\"\n  }, \"https://arxiv.org/abs/1702.05870\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Differentiable Learning-to-Normalize via Switchable Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.10779\"\n  }, \"https://arxiv.org/abs/1806.10779\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/switchablenorms/Switchable-Normalization\"\n  }, \"https://github.com/switchablenorms/Switchable-Normalization\"))), mdx(\"h2\", {\n    \"id\": \"group-normalization\"\n  }, \"Group Normalization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Group Normalization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018 Best Paper Award Honorable Mention\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research (FAIR)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.08494\"\n  }, \"https://arxiv.org/abs/1803.08494\"))), mdx(\"h2\", {\n    \"id\": \"batch-instance-normalization\"\n  }, \"Batch-Instance Normalization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.07925\"\n  }, \"https://arxiv.org/abs/1805.07925\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.09441\"\n  }, \"https://arxiv.org/abs/1807.09441\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official, Pytorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/XingangPan/IBN-Net\"\n  }, \"https://github.com/XingangPan/IBN-Net\"))), mdx(\"h2\", {\n    \"id\": \"dynamic-normalization\"\n  }, \"Dynamic Normalization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamic Normalization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2101.06073\"\n  }, \"https://arxiv.org/abs/2101.06073\")), mdx(\"h1\", {\n    \"id\": \"loss-function\"\n  }, \"Loss Function\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Loss Surfaces of Multilayer Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.0233\"\n  }, \"http://arxiv.org/abs/1412.0233\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Direct Loss Minimization for Training Deep Neural Nets\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06411\"\n  }, \"http://arxiv.org/abs/1511.06411\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Nonconvex Loss Functions for Classifiers and Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://casmls.github.io/general/2016/10/27/NonconvexLosses.html\"\n  }, \"https://casmls.github.io/general/2016/10/27/NonconvexLosses.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Deep Embeddings with Histogram Loss\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.00822\"\n  }, \"https://arxiv.org/abs/1611.00822\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large-Margin Softmax Loss for Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Peking University & South China University of Technology & CMU & Shenzhen University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.02295\"\n  }, \"https://arxiv.org/abs/1612.02295\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Official. Caffe): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/wy1iu/LargeMargin_Softmax_Loss\"\n  }, \"https://github.com/wy1iu/LargeMargin_Softmax_Loss\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/luoyetx/mx-lsoftmax\"\n  }, \"https://github.com/luoyetx/mx-lsoftmax\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tpys/face-recognition-caffe2\"\n  }, \"https://github.com/tpys/face-recognition-caffe2\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jihunchoi/lsoftmax-pytorch\"\n  }, \"https://github.com/jihunchoi/lsoftmax-pytorch\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An empirical analysis of the optimization of deep network loss surfaces\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1612.04010\"\n  }, \"https://arxiv.org/abs/1612.04010\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Peking University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.10239\"\n  }, \"https://arxiv.org/abs/1706.10239\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hierarchical Softmax\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://building-babylon.net/2017/08/01/hierarchical-softmax/\"\n  }, \"http://building-babylon.net/2017/08/01/hierarchical-softmax/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Noisy Softmax: Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.03769\"\n  }, \"https://arxiv.org/abs/1708.03769\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DropMax: Adaptive Stochastic Softmax\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UNIST & Postech & KAIST\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.07834\"\n  }, \"https://arxiv.org/abs/1712.07834\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Rethinking Feature Distribution for Loss Functions in Image Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018 spotlight\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.02988\"\n  }, \"https://arxiv.org/abs/1803.02988\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Ensemble Soft-Margin Softmax Loss for Image Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCAI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.03922\"\n  }, \"https://arxiv.org/abs/1805.03922\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Cornell University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.07836\"\n  }, \"https://arxiv.org/abs/1805.07836\"))), mdx(\"h1\", {\n    \"id\": \"learning-rates\"\n  }, \"Learning Rates\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"No More Pesky Learning Rates\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tom Schaul, Sixin Zhang, Yann LeCun\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1206.1106\"\n  }, \"https://arxiv.org/abs/1206.1106\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Coupling Adaptive Batch Sizes with Learning Rates\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Max Planck Institute for Intelligent Systems\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tensorflow implementation of SGD with Coupled Adaptive Batch Size (CABS)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.05086\"\n  }, \"https://arxiv.org/abs/1612.05086\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ProbabilisticNumerics/cabs\"\n  }, \"https://github.com/ProbabilisticNumerics/cabs\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.07120\"\n  }, \"https://arxiv.org/abs/1708.07120\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving the way we work with learning rate.\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b\"\n  }, \"https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"WNGrad: Learn the Learning Rate in Gradient Descent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Texas at Austin & Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.02865\"\n  }, \"https://arxiv.org/abs/1803.02865\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning with Random Learning Rates\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research & Universite Paris Sud\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: All Learning Rates At Once (Alrao)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://leonardblier.github.io/alrao/\"\n  }, \"https://leonardblier.github.io/alrao/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.01322\"\n  }, \"https://arxiv.org/abs/1810.01322\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(PyTorch, official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/leonardblier/alrao\"\n  }, \"https://github.com/leonardblier/alrao\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Rate Dropout\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 1Xiamen University & Columbia University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1912.00144\"\n  }, \"https://arxiv.org/abs/1912.00144\"))), mdx(\"h1\", {\n    \"id\": \"convolution-filters\"\n  }, \"Convolution Filters\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Non-linear Convolution Filters for CNN-based Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.07038\"\n  }, \"https://arxiv.org/abs/1708.07038\"))), mdx(\"h1\", {\n    \"id\": \"pooling\"\n  }, \"Pooling\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stochastic Pooling for Regularization of Deep Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2013. Matthew D. Zeiler, Rob Fergus\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf\"\n  }, \"http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-scale Orderless Pooling of Deep Convolutional Activation Features\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2014\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MOP-CNN, orderless VLAD pooling, image classification / instance-level retrieval\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1403.1840\"\n  }, \"https://arxiv.org/abs/1403.1840\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://web.engr.illinois.edu/~slazebni/publications/yunchao_eccv14_mopcnn.pdf\"\n  }, \"http://web.engr.illinois.edu/~slazebni/publications/yunchao_eccv14_mopcnn.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fractional Max-Pooling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1412.6071\"\n  }, \"https://arxiv.org/abs/1412.6071\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gist.github.com/shagunsodhani/ccfe3134f46fd3738aa0\"\n  }, \"https://gist.github.com/shagunsodhani/ccfe3134f46fd3738aa0\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/torch/nn/issues/371\"\n  }, \"https://github.com/torch/nn/issues/371\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://dlaptev.org/papers/Laptev16_CVPR.pdf\"\n  }, \"http://dlaptev.org/papers/Laptev16_CVPR.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/dlaptev/TI-pooling\"\n  }, \"https://github.com/dlaptev/TI-pooling\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"S3Pool: Pooling with Stochastic Spatial Sampling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05138\"\n  }, \"https://arxiv.org/abs/1611.05138\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Lasagne): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Shuangfei/s3pool\"\n  }, \"https://github.com/Shuangfei/s3pool\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Inductive Bias of Deep Convolutional Networks through Pooling Geometry\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1605.06743\"\n  }, \"https://arxiv.org/abs/1605.06743\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/HUJI-Deep/inductive-pooling\"\n  }, \"https://github.com/HUJI-Deep/inductive-pooling\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improved Bilinear Pooling with CNNs\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1707.06772\"\n  }, \"https://arxiv.org/abs/1707.06772\")), mdx(\"p\", null, \"**Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.08105\"\n  }, \"https://arxiv.org/abs/1707.08105\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/passalis/cbof\"\n  }, \"https://github.com/passalis/cbof\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A new kind of pooling layer for faster and sharper convergence\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@singlasahil14/a-new-kind-of-pooling-layer-for-faster-and-sharper-convergence-1043c756a221\"\n  }, \"https://medium.com/@singlasahil14/a-new-kind-of-pooling-layer-for-faster-and-sharper-convergence-1043c756a221\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/singlasahil14/sortpool2d\"\n  }, \"https://github.com/singlasahil14/sortpool2d\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Statistically Motivated Second Order Pooling\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.07492\"\n  }, \"https://arxiv.org/abs/1801.07492\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Detail-Preserving Pooling in Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.04076\"\n  }, \"https://arxiv.org/abs/1804.04076\"))), mdx(\"h1\", {\n    \"id\": \"mini-batch\"\n  }, \"Mini-Batch\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Online Batch Selection for Faster Training of Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Workshop paper at ICLR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1511.06343\"\n  }, \"https://arxiv.org/abs/1511.06343\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1609.04836\"\n  }, \"https://arxiv.org/abs/1609.04836\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: Training with 256 GPUs, minibatches of 8192\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.02677\"\n  }, \"https://arxiv.org/abs/1706.02677\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scaling SGD Batch Size to 32K for ImageNet Training\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large Batch Training of Convolutional Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.03888\"\n  }, \"https://arxiv.org/abs/1708.03888\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ImageNet Training in 24 Minutes\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.05011\"\n  }, \"https://arxiv.org/abs/1709.05011\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Don't Decay the Learning Rate, Increase the Batch Size\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Brain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.00489\"\n  }, \"https://arxiv.org/abs/1711.00489\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2017 Workshop: Deep Learning at Supercomputer Scale\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04325\"\n  }, \"https://arxiv.org/abs/1711.04325\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UC Berkeley & NVIDIA\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.02029\"\n  }, \"https://arxiv.org/abs/1712.02029\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hessian-based Analysis of Large Batch Training and Robustness to Adversaries\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UC Berkeley & University of Texas\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1802.08241\"\n  }, \"https://arxiv.org/abs/1802.08241\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: large batch, LARS, adaptive rate scaling\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"openreview: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/forum?id=rJ4uaX2aW\"\n  }, \"https://openreview.net/forum?id=rJ4uaX2aW\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Revisiting Small Batch Training for Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1804.07612\"\n  }, \"https://arxiv.org/abs/1804.07612\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Second-order Optimization Method for Large Mini-batch: Training ResNet-50 on ImageNet in 35 Epochs\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.12019\"\n  }, \"https://arxiv.org/abs/1811.12019\")), mdx(\"h1\", {\n    \"id\": \"optimization-methods\"\n  }, \"Optimization Methods\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On Optimization Methods for Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.icml-2011.org/papers/210_icmlpaper.pdf\"\n  }, \"http://www.icml-2011.org/papers/210_icmlpaper.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Invariant backpropagation: how to train a transformation-invariant neural network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1502.04434\"\n  }, \"http://arxiv.org/abs/1502.04434\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/sdemyanov/ConvNet\"\n  }, \"https://github.com/sdemyanov/ConvNet\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A practical theory for designing very deep convolutional neural network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"kaggle: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code/69284\"\n  }, \"https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code/69284\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://kaggle2.blob.core.windows.net/forum-message-attachments/69182/2287/A%20practical%20theory%20for%20designing%20very%20deep%20convolutional%20neural%20networks.pdf?sv=2012-02-12&se=2015-12-05T15%3A40%3A02Z&sr=b&sp=r&sig=kfBQKduA1pDtu837Y9Iqyrp2VYItTV0HCgOeOok9E3E%3D\"\n  }, \"https://kaggle2.blob.core.windows.net/forum-message-attachments/69182/2287/A%20practical%20theory%20for%20designing%20very%20deep%20convolutional%20neural%20networks.pdf?sv=2012-02-12&se=2015-12-05T15%3A40%3A02Z&sr=b&sp=r&sig=kfBQKduA1pDtu837Y9Iqyrp2VYItTV0HCgOeOok9E3E%3D\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://vdisk.weibo.com/s/3nFsznjLKn\"\n  }, \"http://vdisk.weibo.com/s/3nFsznjLKn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stochastic Optimization Techniques\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: SGD/Momentum/NAG/Adagrad/RMSProp/Adadelta/Adam/ESGD/Adasecant/vSGD/Rprop\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://colinraffel.com/wiki/stochastic_optimization_techniques\"\n  }, \"http://colinraffel.com/wiki/stochastic_optimization_techniques\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Alec Radford's animations for optimization algorithms\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html\"\n  }, \"http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Faster Asynchronous SGD (FASGD)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1601.04033\"\n  }, \"http://arxiv.org/abs/1601.04033\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/DoctorTeeth/fred\"\n  }, \"https://github.com/DoctorTeeth/fred\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An overview of gradient descent optimization algorithms (\\u2605\\u2605\\u2605\\u2605\\u2605)\")), mdx(\"img\", {\n    \"src\": \"http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1609.04747\"\n  }, \"https://arxiv.org/abs/1609.04747\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://sebastianruder.com/optimizing-gradient-descent/\"\n  }, \"http://sebastianruder.com/optimizing-gradient-descent/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02151\"\n  }, \"http://arxiv.org/abs/1602.02151\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Writing fast asynchronous SGD/AdaGrad with RcppParallel\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://gallery.rcpp.org/articles/rcpp-sgd/\"\n  }, \"http://gallery.rcpp.org/articles/rcpp-sgd/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Quick Explanations Of Optimization Methods\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jxieeducation.com/2016-07-02/Quick-Explanations-of-Optimization-Methods/\"\n  }, \"http://jxieeducation.com/2016-07-02/Quick-Explanations-of-Optimization-Methods/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to learn by gradient descent by gradient descent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google DeepMind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1606.04474\"\n  }, \"https://arxiv.org/abs/1606.04474\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/deepmind/learning-to-learn\"\n  }, \"https://github.com/deepmind/learning-to-learn\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/runopti/Learning-To-Learn\"\n  }, \"https://github.com/runopti/Learning-To-Learn\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(PyTorch): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ikostrikov/pytorch-meta-optimizer\"\n  }, \"https://github.com/ikostrikov/pytorch-meta-optimizer\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SGDR: Stochastic Gradient Descent with Restarts\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: cosine annealing strategy\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.03983\"\n  }, \"http://arxiv.org/abs/1608.03983\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/loshchil/SGDR\"\n  }, \"https://github.com/loshchil/SGDR\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The zen of gradient descent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html\"\n  }, \"http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Big Batch SGD: Automated Inference using Adaptive Batch Sizes\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.05792\"\n  }, \"https://arxiv.org/abs/1610.05792\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Stochastic Gradient Descent with Feedback\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01505\"\n  }, \"https://arxiv.org/abs/1611.01505\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jayanthkoushik/sgd-feedback\"\n  }, \"https://github.com/jayanthkoushik/sgd-feedback\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Eve\"\n  }, \"https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Eve\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Gradient Descent: Better Generalization and Longer Horizons\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tsinghua University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.03633\"\n  }, \"https://arxiv.org/abs/1703.03633\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/vfleaking/rnnprop\"\n  }, \"https://github.com/vfleaking/rnnprop\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Optimization Algorithms\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://3dbabove.com/2017/11/14/optimizationalgorithms/\"\n  }, \"https://3dbabove.com/2017/11/14/optimizationalgorithms/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//ManuelGonzalezRivero/3dbabove\"\n  }, \"https://github.com//ManuelGonzalezRivero/3dbabove\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"reddit: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.reddit.com/r/MachineLearning/comments/7ehxky/d_optimization_algorithms_math_and_code/\"\n  }, \"https://www.reddit.com/r/MachineLearning/comments/7ehxky/d_optimization_algorithms_math_and_code/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gradient Normalization & Depth Based Decay For Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Columbia University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.03607\"\n  }, \"https://arxiv.org/abs/1712.03607\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.03298\"\n  }, \"https://arxiv.org/abs/1712.03298\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Optimization for Deep Learning Highlights in 2017\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://ruder.io/deep-learning-optimization-2017/index.html\"\n  }, \"http://ruder.io/deep-learning-optimization-2017/index.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gradients explode - Deep Networks are shallow - ResNet explained\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CMU & UC Berkeley\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.05577\"\n  }, \"https://arxiv.org/abs/1712.05577\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Sufficient Condition for Convergences of Adam and RMSProp\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.09358\"\n  }, \"https://arxiv.org/abs/1811.09358\")), mdx(\"h2\", {\n    \"id\": \"adam\"\n  }, \"Adam\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adam: A Method for Stochastic Optimization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.6980\"\n  }, \"http://arxiv.org/abs/1412.6980\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fixing Weight Decay Regularization in Adam\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Freiburg\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.05101\"\n  }, \"https://arxiv.org/abs/1711.05101\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/loshchil/AdamW-and-SGDW\"\n  }, \"https://github.com/loshchil/AdamW-and-SGDW\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/fastai/fastai/pull/46/files\"\n  }, \"https://github.com/fastai/fastai/pull/46/files\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On the Convergence of Adam and Beyond\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2018 best paper award. CMU & IBM Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/pdf?id=ryQu7f-RZ\"\n  }, \"https://openreview.net/pdf?id=ryQu7f-RZ\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"openreview: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/forum?id=ryQu7f-RZ\"\n  }, \"https://openreview.net/forum?id=ryQu7f-RZ\"))), mdx(\"h1\", {\n    \"id\": \"tensor-methods\"\n  }, \"Tensor Methods\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tensorizing Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: TensorNet\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.06569\"\n  }, \"http://arxiv.org/abs/1509.06569\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Matlab+Theano+Lasagne): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Bihaqo/TensorNet\"\n  }, \"https://github.com/Bihaqo/TensorNet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/timgaripov/TensorNet-TF\"\n  }, \"https://github.com/timgaripov/TensorNet-TF\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tensor methods for training neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://newport.eecs.uci.edu/anandkumar/#home\"\n  }, \"http://newport.eecs.uci.edu/anandkumar/#home\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=B4YvhcGaafw\"\n  }, \"https://www.youtube.com/watch?v=B4YvhcGaafw\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://newport.eecs.uci.edu/anandkumar/slides/Strata-NY.pdf\"\n  }, \"http://newport.eecs.uci.edu/anandkumar/slides/Strata-NY.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"talks: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://newport.eecs.uci.edu/anandkumar/#talks\"\n  }, \"http://newport.eecs.uci.edu/anandkumar/#talks\"))), mdx(\"h1\", {\n    \"id\": \"regularization\"\n  }, \"Regularization\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DisturbLabel: Regularizing CNN on the Loss Layer\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro:  University of California & MSR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"an extremely simple algorithm which randomly replaces a part of labels as incorrect values in each iteration\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://research.microsoft.com/en-us/um/people/jingdw/pubs/cvpr16-disturblabel.pdf\"\n  }, \"http://research.microsoft.com/en-us/um/people/jingdw/pubs/cvpr16-disturblabel.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Robust Convolutional Neural Networks under Adversarial Noise\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro:  ICLR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06306\"\n  }, \"http://arxiv.org/abs/1511.06306\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adding Gradient Noise Improves Learning for Very Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro:  ICLR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06807\"\n  }, \"http://arxiv.org/abs/1511.06807\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Stochastic Function Norm Regularization of Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1605.09085\"\n  }, \"http://arxiv.org/abs/1605.09085\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/AmalRT/DNN_Reg\"\n  }, \"https://github.com/AmalRT/DNN_Reg\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.06693\"\n  }, \"http://arxiv.org/abs/1609.06693\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularizing neural networks by penalizing confident predictions\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Gabriel Pereyra, George Tucker, Lukasz Kaiser, Geoffrey Hinton [Google Brain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"dropbox: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.dropbox.com/s/8kqf4v2c9lbnvar/BayLearn%202016%20(gjt).pdf?dl=0\"\n  }, \"https://www.dropbox.com/s/8kqf4v2c9lbnvar/BayLearn%202016%20(gjt).pdf?dl=0\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pan.baidu.com/s/1kUUtxdl\"\n  }, \"https://pan.baidu.com/s/1kUUtxdl\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Automatic Node Selection for Deep Neural Networks using Group Lasso Regularization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.05527\"\n  }, \"https://arxiv.org/abs/1611.05527\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularization in deep learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@cristina_scheau/regularization-in-deep-learning-f649a45d6e0#.py327hkuv\"\n  }, \"https://medium.com/@cristina_scheau/regularization-in-deep-learning-f649a45d6e0#.py327hkuv\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/cscheau/Examples/blob/master/iris_l1_l2.py\"\n  }, \"https://github.com/cscheau/Examples/blob/master/iris_l1_l2.py\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"LDMNet: Low Dimensional Manifold Regularized Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.06246\"\n  }, \"https://arxiv.org/abs/1711.06246\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Sparse Neural Networks through L0 Regularization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Amsterdam & OpenAI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.01312\"\n  }, \"https://arxiv.org/abs/1712.01312\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularization and Optimization strategies in Deep Convolutional Neural Network\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.04711\"\n  }, \"https://arxiv.org/abs/1712.04711\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularizing Deep Networks by Modeling and Predicting Label Structure\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.02009\"\n  }, \"https://arxiv.org/abs/1804.02009\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adversarial Noise Layer: Regularize Neural Network By Adding Noise\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Peking University & \\u2021University of Electronic Science and Technology of China & Australian National University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.08000\"\n  }, \"https://arxiv.org/abs/1805.08000\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/youzhonghui/ANL\"\n  }, \"https://github.com/youzhonghui/ANL\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Bilevel Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1809.01465\"\n  }, \"https://arxiv.org/abs/1809.01465\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.09102\"\n  }, \"https://arxiv.org/abs/1810.09102\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gradient-Coherent Strong Regularization for Deep Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.08056\"\n  }, \"https://arxiv.org/abs/1811.08056\")), mdx(\"h2\", {\n    \"id\": \"dropout\"\n  }, \"Dropout\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving neural networks by preventing co-adaptation of feature detectors\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Dropout\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1207.0580\"\n  }, \"http://arxiv.org/abs/1207.0580\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\"\n  }, \"https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast dropout training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jmlr.org/proceedings/papers/v28/wang13a.pdf\"\n  }, \"http://jmlr.org/proceedings/papers/v28/wang13a.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/sidaw/fastdropout\"\n  }, \"https://github.com/sidaw/fastdropout\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dropout as data augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1506.08700\"\n  }, \"http://arxiv.org/abs/1506.08700\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"notes: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.evernote.com/shard/s189/sh/ef0c3302-21a4-40d7-b8b4-1c65b8ebb1c9/24ff553fcfb70a27d61ff003df75b5a9\"\n  }, \"https://www.evernote.com/shard/s189/sh/ef0c3302-21a4-40d7-b8b4-1c65b8ebb1c9/24ff553fcfb70a27d61ff003df75b5a9\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1512.05287\"\n  }, \"http://arxiv.org/abs/1512.05287\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yaringal/BayesianRNN\"\n  }, \"https://github.com/yaringal/BayesianRNN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improved Dropout for Shallow and Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02220\"\n  }, \"http://arxiv.org/abs/1602.02220\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dropout Regularization in Deep Learning Models With Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\"\n  }, \"http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dropout with Expectation-linear Regularization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.08017\"\n  }, \"http://arxiv.org/abs/1609.08017\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dropout with Theano\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://rishy.github.io/ml/2016/10/12/dropout-with-theano/\"\n  }, \"http://rishy.github.io/ml/2016/10/12/dropout-with-theano/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"ipn: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://nbviewer.jupyter.org/github/rishy/rishy.github.io/blob/master/ipy_notebooks/Dropout-Theano.ipynb\"\n  }, \"http://nbviewer.jupyter.org/github/rishy/rishy.github.io/blob/master/ipy_notebooks/Dropout-Theano.ipynb\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Information Dropout: learning optimal representations through noise\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.01353\"\n  }, \"https://arxiv.org/abs/1611.01353\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Recent Developments in Dropout\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://casmls.github.io/general/2016/11/11/dropout.html\"\n  }, \"https://casmls.github.io/general/2016/11/11/dropout.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Generalized Dropout\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.06791\"\n  }, \"https://arxiv.org/abs/1611.06791\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Analysis of Dropout\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/\"\n  }, \"https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Variational Dropout Sparsifies Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1701.05369\"\n  }, \"https://arxiv.org/abs/1701.05369\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Deep Networks from Noisy Labels with Dropout Regularization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: 2016 IEEE 16th International Conference on Data Mining\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.03419\"\n  }, \"https://arxiv.org/abs/1705.03419\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Concrete Dropout\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Cambridge\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1705.07832\"\n  }, \"https://arxiv.org/abs/1705.07832\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yaringal/ConcreteDropout\"\n  }, \"https://github.com/yaringal/ConcreteDropout\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Analysis of dropout learning regarded as ensemble learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Nihon University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.06859\"\n  }, \"https://arxiv.org/abs/1706.06859\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Analysis of Dropout for Matrix Factorization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1710.03487\"\n  }, \"https://arxiv.org/abs/1710.03487\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Analysis of Dropout in Online Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.03343\"\n  }, \"https://arxiv.org/abs/1711.03343\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularization of Deep Neural Networks with Spectral Dropout\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.08591\"\n  }, \"https://arxiv.org/abs/1711.08591\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Dropout in Arbitrary Basis for Deep Network Regularization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.00891\"\n  }, \"https://arxiv.org/abs/1712.00891\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A New Angle on L2 Regularization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: An explorable explanation on the phenomenon of adversarial examples in linear classification and its relation to L2 regularization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://thomas-tanay.github.io/post--L2-regularization/\"\n  }, \"https://thomas-tanay.github.io/post--L2-regularization/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.11186\"\n  }, \"https://arxiv.org/abs/1806.11186\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Rutgers University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.03578\"\n  }, \"https://arxiv.org/abs/1808.03578\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/noahfl/densenet-sdr/\"\n  }, \"https://github.com/noahfl/densenet-sdr/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Dropout: Optimizing Training Data for Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.00193\"\n  }, \"https://arxiv.org/abs/1809.00193\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DropFilter: Dropout for Convolutions\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1810.09849\"\n  }, \"https://arxiv.org/abs/1810.09849\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.06783\"\n  }, \"https://arxiv.org/abs/1811.06783\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Targeted Dropout\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Brain & FOR.ai & University of Oxford\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://openreview.net/pdf?id=HkghWScuoQ\"\n  }, \"https://openreview.net/pdf?id=HkghWScuoQ\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/for-ai/TD\"\n  }, \"https://github.com/for-ai/TD\"))), mdx(\"h2\", {\n    \"id\": \"dropconnect\"\n  }, \"DropConnect\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularization of Neural Networks using DropConnect\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"homepage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.nyu.edu/~wanli/dropc/\"\n  }, \"http://cs.nyu.edu/~wanli/dropc/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gitxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://gitxiv.com/posts/rJucpiQiDhQ7HkZoX/regularization-of-neural-networks-using-dropconnect\"\n  }, \"http://gitxiv.com/posts/rJucpiQiDhQ7HkZoX/regularization-of-neural-networks-using-dropconnect\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/iassael/torch-dropconnect\"\n  }, \"https://github.com/iassael/torch-dropconnect\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularizing neural networks with dropout and with DropConnect\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/\"\n  }, \"http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/\"))), mdx(\"h2\", {\n    \"id\": \"dropneuron\"\n  }, \"DropNeuron\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DropNeuron: Simplifying the Structure of Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.07326\"\n  }, \"http://arxiv.org/abs/1606.07326\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/panweihit/DropNeuron\"\n  }, \"https://github.com/panweihit/DropNeuron\"))), mdx(\"h2\", {\n    \"id\": \"dropblock\"\n  }, \"DropBlock\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DropBlock: A regularization method for convolutional networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NIPS 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1810.12890\"\n  }, \"https://arxiv.org/abs/1810.12890\"))), mdx(\"h2\", {\n    \"id\": \"maxout\"\n  }, \"Maxout\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Maxout Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2013\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: \\\"its output is the max of a set of inputs, a natural companion to dropout\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html\"\n  }, \"http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1302.4389\"\n  }, \"https://arxiv.org/abs/1302.4389\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/models/maxout.py\"\n  }, \"https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/models/maxout.py\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Deep Neural Networks with Probabilistic Maxout Units\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1312.6116\"\n  }, \"https://arxiv.org/abs/1312.6116\"))), mdx(\"h2\", {\n    \"id\": \"swapout\"\n  }, \"Swapout\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Swapout: Learning an ensemble of deep architectures\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1605.06465\"\n  }, \"https://arxiv.org/abs/1605.06465\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://gab41.lab41.org/lab41-reading-group-swapout-learning-an-ensemble-of-deep-architectures-e67d2b822f8a#.9r2s4c58n\"\n  }, \"https://gab41.lab41.org/lab41-reading-group-swapout-learning-an-ensemble-of-deep-architectures-e67d2b822f8a#.9r2s4c58n\"))), mdx(\"h2\", {\n    \"id\": \"whiteout\"\n  }, \"Whiteout\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Whiteout: Gaussian Adaptive Regularization Noise in Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Notre Dame & University of Science and Technology of China\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.01490\"\n  }, \"https://arxiv.org/abs/1612.01490\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ShakeDrop regularization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1802.02375\"\n  }, \"https://arxiv.org/abs/1802.02375\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Shakeout: A New Approach to Regularized Deep Neural Network Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: T-PAMI 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1904.06593\"\n  }, \"https://arxiv.org/abs/1904.06593\"))), mdx(\"h1\", {\n    \"id\": \"gradient-descent\"\n  }, \"Gradient Descent\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RMSProp: Divide the gradient by a running average of its recent magnitude\")), mdx(\"img\", {\n    \"src\": \"/assets/train-dnn/rmsprop.jpg\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: it was not proposed in a paper, in fact it was just introduced in a slide in Geoffrey Hinton's Coursera class \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\"\n  }, \"http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fitting a model via closed-form equations vs. Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning. What is the difference?(Normal Equations vs. GD vs. SGD vs. MB-GD)\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://sebastianraschka.com/faq/docs/closed-form-vs-gd.html\"\n  }, \"http://sebastianraschka.com/faq/docs/closed-form-vs-gd.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Introduction to Gradient Descent in Python\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://tillbergmann.com/blog/articles/python-gradient-descent.html\"\n  }, \"http://tillbergmann.com/blog/articles/python-gradient-descent.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Train faster, generalize better: Stability of stochastic gradient descent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1509.01240\"\n  }, \"http://arxiv.org/abs/1509.01240\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Variational Analysis of Stochastic Gradient Algorithms\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02666\"\n  }, \"http://arxiv.org/abs/1602.02666\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The vanishing gradient problem: Oh no\\u200A\\u2014\\u200Aan obstacle to deep learning!\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/a-year-of-artificial-intelligence/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.50hu5vwa8\"\n  }, \"https://medium.com/a-year-of-artificial-intelligence/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.50hu5vwa8\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gradient Descent For Machine Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://machinelearningmastery.com/gradient-descent-for-machine-learning/\"\n  }, \"http://machinelearningmastery.com/gradient-descent-for-machine-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Revisiting Distributed Synchronous SGD\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.00981\"\n  }, \"http://arxiv.org/abs/1604.00981\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Convergence rate of gradient descent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://building-babylon.net/2016/06/23/convergence-rate-of-gradient-descent/\"\n  }, \"https://building-babylon.net/2016/06/23/convergence-rate-of-gradient-descent/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Robust Adaptive Stochastic Gradient Method for Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IJCNN 2017 Accepted Paper, An extension of paper, \\\"ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Universite de Montreal & University of Oxford\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1703.00788\"\n  }, \"https://arxiv.org/abs/1703.00788\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating Stochastic Gradient Descent\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1704.08227\"\n  }, \"https://arxiv.org/abs/1704.08227\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gentle Introduction to the Adam Optimization Algorithm for Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\"\n  }, \"http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Understanding Generalization and Stochastic Gradient Descent\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Bayesian Perspective on Generalization and Stochastic Gradient Descent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Brain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.06451\"\n  }, \"https://arxiv.org/abs/1710.06451\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UC Berkeley & Microsoft Research, India\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.10456\"\n  }, \"https://arxiv.org/abs/1711.10456\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Generalization Performance by Switching from Adam to SGD\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.07628\"\n  }, \"https://arxiv.org/abs/1712.07628\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Laplacian Smoothing Gradient Descent\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UCLA\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.06317\"\n  }, \"https://arxiv.org/abs/1806.06317\"))), mdx(\"h2\", {\n    \"id\": \"adagrad\"\n  }, \"AdaGrad\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\"\n  }, \"http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ADADELTA: An Adaptive Learning Rate Method\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1212.5701\"\n  }, \"http://arxiv.org/abs/1212.5701\"))), mdx(\"h2\", {\n    \"id\": \"momentum\"\n  }, \"Momentum\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On the importance of initialization and momentum in deep learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro:  NAG: Nesterov\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\"\n  }, \"http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jmlr.org/proceedings/papers/v28/sutskever13.pdf\"\n  }, \"http://jmlr.org/proceedings/papers/v28/sutskever13.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"YellowFin and the Art of Momentum Tuning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: auto-tuning momentum SGD optimizer\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs.stanford.edu/~zjian/project/YellowFin/\"\n  }, \"http://cs.stanford.edu/~zjian/project/YellowFin/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.03471\"\n  }, \"https://arxiv.org/abs/1706.03471\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(TensorFlow): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/JianGoForIt/YellowFin\"\n  }, \"https://github.com/JianGoForIt/YellowFin\"), mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/JianGoForIt/YellowFin_Pytorch\"\n  }, \"https://github.com/JianGoForIt/YellowFin_Pytorch\"))), mdx(\"h1\", {\n    \"id\": \"backpropagation\"\n  }, \"Backpropagation\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2016. first place of ILSVRC 2015 Scene Classification Challenge\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1512.05830\"\n  }, \"https://arxiv.org/abs/1512.05830\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2016-ECCV-RelayBP.pdf\"\n  }, \"http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2016-ECCV-RelayBP.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Top-down Neural Attention by Excitation Backprop\")), mdx(\"img\", {\n    \"src\": \"http://cs-people.bu.edu/jmzhang/images/screen%20shot%202016-08-19%20at%2035847%20pm.jpg?crc=3911895888\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV, 2016 (oral)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"projpage: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs-people.bu.edu/jmzhang/excitationbp.html\"\n  }, \"http://cs-people.bu.edu/jmzhang/excitationbp.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.00507\"\n  }, \"http://arxiv.org/abs/1608.00507\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://cs-people.bu.edu/jmzhang/EB/ExcitationBackprop.pdf\"\n  }, \"http://cs-people.bu.edu/jmzhang/EB/ExcitationBackprop.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/jimmie33/Caffe-ExcitationBP\"\n  }, \"https://github.com/jimmie33/Caffe-ExcitationBP\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Towards a Biologically Plausible Backprop\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.05179\"\n  }, \"http://arxiv.org/abs/1602.05179\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bscellier/Towards-a-Biologically-Plausible-Backprop\"\n  }, \"https://github.com/bscellier/Towards-a-Biologically-Plausible-Backprop\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sampled Backpropagation: Training Deep and Wide Neural Networks on Large Scale, User Generated Content Using Label Sampling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://medium.com/@karl1980.lab41/sampled-backpropagation-27ac58d5c51c#.xnbhyxtou\"\n  }, \"https://medium.com/@karl1980.lab41/sampled-backpropagation-27ac58d5c51c#.xnbhyxtou\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Reversible Residual Network: Backpropagation Without Storing Activations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CoRR 2017. University of Toronto\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.04585\"\n  }, \"https://arxiv.org/abs/1707.04585\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/renmengye/revnet-public\"\n  }, \"https://github.com/renmengye/revnet-public\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.06197\"\n  }, \"https://arxiv.org/abs/1706.06197\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//jklj077/meProp\"\n  }, \"https://github.com//jklj077/meProp\"))), mdx(\"h1\", {\n    \"id\": \"accelerate-training\"\n  }, \"Accelerate Training\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Neural Networks with Few Multiplications\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro:  ICLR 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1510.03009\"\n  }, \"https://arxiv.org/abs/1510.03009\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.07341\"\n  }, \"http://arxiv.org/abs/1603.07341\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Q-Networks for Accelerating the Training of Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.01467\"\n  }, \"http://arxiv.org/abs/1606.01467\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/bigaidream-projects/qan\"\n  }, \"https://github.com/bigaidream-projects/qan\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1606.04487\"\n  }, \"http://arxiv.org/abs/1606.04487\"))), mdx(\"h2\", {\n    \"id\": \"parallelism\"\n  }, \"Parallelism\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"One weird trick for parallelizing convolutional neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"author: Alex Krizhevsky\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1404.5997\"\n  }, \"http://arxiv.org/abs/1404.5997\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"8-Bit Approximations for Parallelism in Deep Learning (ICLR 2016)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.04561\"\n  }, \"http://arxiv.org/abs/1511.04561\"))), mdx(\"h1\", {\n    \"id\": \"handling-datasets\"\n  }, \"Handling Datasets\"), mdx(\"h2\", {\n    \"id\": \"data-augmentation\"\n  }, \"Data Augmentation\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DataAugmentation ver1.0: Image data augmentation tool for training of image recognition algorithm\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/takmin/DataAugmentation\"\n  }, \"https://github.com/takmin/DataAugmentation\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Caffe-Data-Augmentation: a branc caffe with feature of Data Augmentation using a configurable stochastic combination of 7 data augmentation techniques\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ShaharKatz/Caffe-Data-Augmentation\"\n  }, \"https://github.com/ShaharKatz/Caffe-Data-Augmentation\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Augmentation for Deep Learning With Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://machinelearningmastery.com/image-augmentation-deep-learning-keras/\"\n  }, \"http://machinelearningmastery.com/image-augmentation-deep-learning-keras/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"What you need to know about data augmentation for machine learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: keras Imagegenerator\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/\"\n  }, \"https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"HZPROC: torch data augmentation toolbox (supports affine transform)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhanghang1989/hzproc\"\n  }, \"https://github.com/zhanghang1989/hzproc\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AGA: Attribute Guided Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: one-shot recognition\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.02559\"\n  }, \"https://arxiv.org/abs/1612.02559\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Accelerating Deep Learning with Multiprocess Image Augmentation in Keras\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://blog.stratospark.com/multiprocess-image-augmentation-keras.html\"\n  }, \"http://blog.stratospark.com/multiprocess-image-augmentation-keras.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/stratospark/keras-multiprocess-image-data-generator\"\n  }, \"https://github.com/stratospark/keras-multiprocess-image-data-generator\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Comprehensive Data Augmentation and Sampling for Pytorch\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ncullen93/torchsample\"\n  }, \"https://github.com/ncullen93/torchsample\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image augmentation for machine learning experiments.\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/aleju/imgaug\"\n  }, \"https://github.com/aleju/imgaug\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Google/inception's data augmentation: scale and aspect ratio augmentation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L130\"\n  }, \"https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L130\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Caffe Augmentation Extension\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Data Augmentation for Caffe\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/twtygqyy/caffe-augmentation\"\n  }, \"https://github.com/twtygqyy/caffe-augmentation\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving Deep Learning using Generic Data Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Cape Town\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.06020\"\n  }, \"https://arxiv.org/abs/1708.06020\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/webstorms/AugmentedDatasets\"\n  }, \"https://github.com/webstorms/AugmentedDatasets\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Augmentor: An Image Augmentation Library for Machine Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.04680\"\n  }, \"https://arxiv.org/abs/1708.04680\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mdbloice/Augmentor\"\n  }, \"https://github.com/mdbloice/Augmentor\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Automatic Dataset Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://auto-da.github.io/\"\n  }, \"https://auto-da.github.io/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.08201\"\n  }, \"https://arxiv.org/abs/1708.08201\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning to Compose Domain-Specific Transformations for Data Augmentation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1709.01643\"\n  }, \"https://arxiv.org/abs/1709.01643\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Augmentation in Classification using GAN\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.00648\"\n  }, \"https://arxiv.org/abs/1711.00648\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Augmentation Generative Adversarial Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1711.04340\"\n  }, \"https://arxiv.org/abs/1711.04340\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Random Erasing Data Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.04896\"\n  }, \"https://arxiv.org/abs/1708.04896\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhunzhong07/Random-Erasing\"\n  }, \"https://github.com/zhunzhong07/Random-Erasing\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Context Augmentation for Convolutional Neural Networks\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.01653\"\n  }, \"https://arxiv.org/abs/1712.01653\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Effectiveness of Data Augmentation in Image Classification using Deep Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.04621\"\n  }, \"https://arxiv.org/abs/1712.04621\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google Inc & Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.05055\"\n  }, \"https://arxiv.org/abs/1712.05055\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"mixup: Beyond Empirical Risk Minimization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: MIT & FAIR\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.09412\"\n  }, \"https://arxiv.org/abs/1710.09412\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//leehomyc/mixup_pytorch\"\n  }, \"https://github.com//leehomyc/mixup_pytorch\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com//unsky/mixup\"\n  }, \"https://github.com//unsky/mixup\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"mixup: Data-Dependent Data Augmentation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.inference.vc/mixup-data-dependent-data-augmentation/\"\n  }, \"http://www.inference.vc/mixup-data-dependent-data-augmentation/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Augmentation by Pairing Samples for Images Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: IBM Research - Tokyo\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.02929\"\n  }, \"https://arxiv.org/abs/1801.02929\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Feature Space Transfer for Data Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: eATure TransfEr Network (FATTEN)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.04356\"\n  }, \"https://arxiv.org/abs/1801.04356\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Visual Data Augmentation through Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1801.06665\"\n  }, \"https://arxiv.org/abs/1801.06665\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Augmentation Generative Adversarial Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1711.04340\"\n  }, \"https://arxiv.org/abs/1711.04340\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/AntreasAntoniou/DAGAN\"\n  }, \"https://github.com/AntreasAntoniou/DAGAN\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BAGAN: Data Augmentation with Balancing GAN\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.09655\"\n  }, \"https://arxiv.org/abs/1803.09655\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Parallel Grid Pooling for Data Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: The University of Tokyo & NTT Communications Science Laboratories\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.11370\"\n  }, \"https://arxiv.org/abs/1803.11370\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(Chainer): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/akitotakeki/pgp-chainer\"\n  }, \"https://github.com/akitotakeki/pgp-chainer\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AutoAugment: Learning Augmentation Policies from Data\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.09501\"\n  }, \"https://arxiv.org/abs/1805.09501\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/DeepVoltaire/AutoAugment\"\n  }, \"https://github.com/DeepVoltaire/AutoAugment\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improved Mixed-Example Data Augmentation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1805.11272\"\n  }, \"https://arxiv.org/abs/1805.11272\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data augmentation instead of explicit regularization\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1806.03852\"\n  }, \"https://arxiv.org/abs/1806.03852\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Augmentation using Random Image Cropping and Patching for Deep CNNs\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: An extended version of a proceeding of ACML2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: random image cropping and patching (RICAP)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1811.09030\"\n  }, \"https://arxiv.org/abs/1811.09030\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GANsfer Learning: Combining labelled and unlabelled data for GAN based data augmentat\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.10669\"\n  }, \"https://arxiv.org/abs/1811.10669\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adversarial Learning of General Transformations for Data Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Ecole de Technologie Sup \\xB4 erieure & Element AI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.09801\"\n  }, \"https://arxiv.org/abs/1909.09801\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Implicit Semantic Data Augmentation for Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: NeurIPS 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.12220\"\n  }, \"https://arxiv.org/abs/1909.12220\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/blackfeather-wang/ISDA-for-Deep-Networks\"\n  }, \"https://github.com/blackfeather-wang/ISDA-for-Deep-Networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Augmentation Revisited: Rethinking the Distribution Gap between Clean and Augmented Data\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1909.09148\"\n  }, \"https://arxiv.org/abs/1909.09148\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2020\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Google & Deepmind\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1912.02781\"\n  }, \"https://arxiv.org/abs/1912.02781\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/google-research/augmix\"\n  }, \"https://github.com/google-research/augmix\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GridMask Data Augmentation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2001.04086\"\n  }, \"https://arxiv.org/abs/2001.04086\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On Feature Normalization and Data Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Cornell University & Cornell Tech & ASAPP Inc. & Facebook AI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: MoEx (Moment Exchange)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2002.11102\"\n  }, \"https://arxiv.org/abs/2002.11102\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Boyiliee/MoEx\"\n  }, \"https://github.com/Boyiliee/MoEx\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DADA: Differentiable Automatic Data Augmentation\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2003.03780\"\n  }, \"https://arxiv.org/abs/2003.03780\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Negative Data Augmentation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Stanford University & Samsung Research America\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2102.05113\"\n  }, \"https://arxiv.org/abs/2102.05113\"))), mdx(\"h2\", {\n    \"id\": \"imbalanced-datasets\"\n  }, \"Imbalanced Datasets\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Investigation on handling Structured & Imbalanced Datasets with Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: smote resampling, cost sensitive learning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.analyticsvidhya.com/blog/2016/10/investigation-on-handling-structured-imbalanced-datasets-with-deep-learning/\"\n  }, \"https://www.analyticsvidhya.com/blog/2016/10/investigation-on-handling-structured-imbalanced-datasets-with-deep-learning/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A systematic study of the class imbalance problem in convolutional neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Duke University & Royal Institute of Technology (KTH)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.05381\"\n  }, \"https://arxiv.org/abs/1710.05381\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Class Rectification Hard Mining for Imbalanced Deep Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.03162\"\n  }, \"https://arxiv.org/abs/1712.03162\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bridging the Gap: Simultaneous Fine Tuning for Data Re-Balancing\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1801.02548\"\n  }, \"https://arxiv.org/abs/1801.02548\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/JohnMcKay/dataImbalance\"\n  }, \"https://github.com/JohnMcKay/dataImbalance\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Imbalanced Deep Learning by Minority Class Incremental Rectification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: TPAMI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1804.10851\"\n  }, \"https://arxiv.org/abs/1804.10851\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pseudo-Feature Generation for Imbalanced Data Analysis in Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: National Institute of Information and Communications Technology, Tokyo Japan\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1807.06538\"\n  }, \"https://arxiv.org/abs/1807.06538\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.slideshare.net/TomohikoKonno/pseudofeature-generation-for-imbalanced-data-analysis-in-deep-learning-tomohiko-105318569\"\n  }, \"https://www.slideshare.net/TomohikoKonno/pseudofeature-generation-for-imbalanced-data-analysis-in-deep-learning-tomohiko-105318569\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Max-margin Class Imbalanced Learning with Gaussian Affinity\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1901.07711\"\n  }, \"https://arxiv.org/abs/1901.07711\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Dynamic Curriculum Learning for Imbalanced Data Classification\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: SenseTime\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1901.06783\"\n  }, \"https://arxiv.org/abs/1901.06783\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Class Rectification Hard Mining for Imbalanced Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2017\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.eecs.qmul.ac.uk/~sgg/papers/DongEtAl_ICCV2017.pdf\"\n  }, \"https://www.eecs.qmul.ac.uk/~sgg/papers/DongEtAl_ICCV2017.pdf\"))), mdx(\"h2\", {\n    \"id\": \"noisy--unlabelled-data\"\n  }, \"Noisy / Unlabelled Data\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Data Distillation: Towards Omni-Supervised Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Facebook AI Research (FAIR)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.04440\"\n  }, \"https://arxiv.org/abs/1712.04440\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning From Noisy Singly-labeled Data\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Illinois Urbana Champaign & CMU & Caltech & Amazon AI\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.04577\"\n  }, \"https://arxiv.org/abs/1712.04577\"))), mdx(\"h1\", {\n    \"id\": \"low-numerical-precision\"\n  }, \"Low Numerical Precision\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training deep neural networks with low precision multiplications\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Maxout networks, 10-bit activations, 12-bit parameter updates\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1412.7024\"\n  }, \"http://arxiv.org/abs/1412.7024\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MatthieuCourbariaux/deep-learning-multipliers\"\n  }, \"https://github.com/MatthieuCourbariaux/deep-learning-multipliers\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning with Limited Numerical Precision\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2015\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1502.02551\"\n  }, \"http://arxiv.org/abs/1502.02551\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BinaryConnect: Training Deep Neural Networks with binary weights during propagations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://papers.nips.cc/paper/5647-shape-and-illumination-from-shading-using-the-generic-viewpoint-assumption\"\n  }, \"http://papers.nips.cc/paper/5647-shape-and-illumination-from-shading-using-the-generic-viewpoint-assumption\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MatthieuCourbariaux/BinaryConnect\"\n  }, \"https://github.com/MatthieuCourbariaux/BinaryConnect\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Binarized Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02505\"\n  }, \"http://arxiv.org/abs/1602.02505\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.02830\"\n  }, \"http://arxiv.org/abs/1602.02830\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/MatthieuCourbariaux/BinaryNet\"\n  }, \"https://github.com/MatthieuCourbariaux/BinaryNet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/codekansas/tinier-nn\"\n  }, \"https://github.com/codekansas/tinier-nn\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.07061\"\n  }, \"http://arxiv.org/abs/1609.07061\"))), mdx(\"h1\", {\n    \"id\": \"distributed-training\"\n  }, \"Distributed Training\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large Scale Distributed Systems for Training Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: By Jeff Dean & Oriol Vinyals, Google. NIPS 2015.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://media.nips.cc/Conferences/2015/tutorialslides/Jeff-Oriol-NIPS-Tutorial-2015.pdf\"\n  }, \"https://media.nips.cc/Conferences/2015/tutorialslides/Jeff-Oriol-NIPS-Tutorial-2015.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://research.microsoft.com/apps/video/default.aspx?id=259564&l=i\"\n  }, \"http://research.microsoft.com/apps/video/default.aspx?id=259564&l=i\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1mgXV0hU\"\n  }, \"http://pan.baidu.com/s/1mgXV0hU\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Large Scale Distributed Deep Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: distributed CPU training, data parallelism, model parallelism\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf\"\n  }, \"http://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://admis.fudan.edu.cn/~yfhuang/files/LSDDN_slide.pdf\"\n  }, \"http://admis.fudan.edu.cn/~yfhuang/files/LSDDN_slide.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Implementation of a Practical Distributed Calculation System with Browsers and JavaScript, and Application to Distributed Deep Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://mil-tokyo.github.io/\"\n  }, \"http://mil-tokyo.github.io/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1503.05743\"\n  }, \"https://arxiv.org/abs/1503.05743\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SparkNet: Training Deep Networks in Spark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1511.06051\"\n  }, \"http://arxiv.org/abs/1511.06051\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/amplab/SparkNet\"\n  }, \"https://github.com/amplab/SparkNet\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.kdnuggets.com/2015/12/spark-deep-learning-training-with-sparknet.html\"\n  }, \"http://www.kdnuggets.com/2015/12/spark-deep-learning-training-with-sparknet.html\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Scalable Implementation of Deep Learning on Spark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Alexander Ulanov\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"slides: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.slideshare.net/AlexanderUlanov1/a-scalable-implementation-of-deep-learning-on-spark-alexander-ulanov\"\n  }, \"http://www.slideshare.net/AlexanderUlanov1/a-scalable-implementation-of-deep-learning-on-spark-alexander-ulanov\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1jHiNW5C\"\n  }, \"http://pan.baidu.com/s/1jHiNW5C\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1603.04467\"\n  }, \"http://arxiv.org/abs/1603.04467\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"gitxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://gitxiv.com/posts/57kjddp3AWt4y5K4h/tensorflow-large-scale-machine-learning-on-heterogeneous\"\n  }, \"http://gitxiv.com/posts/57kjddp3AWt4y5K4h/tensorflow-large-scale-machine-learning-on-heterogeneous\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed Supervised Learning using Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Ph.D. thesis\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1607.06364\"\n  }, \"http://arxiv.org/abs/1607.06364\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed Training of Deep Neuronal Networks: Theoretical and Practical Limits of Parallel Scalability\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1609.06870\"\n  }, \"http://arxiv.org/abs/1609.06870\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How to scale distributed deep learning?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Extended version of paper accepted at ML Sys 2016 (at NIPS 2016)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1611.04581\"\n  }, \"https://arxiv.org/abs/1611.04581\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tsinghua University & Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"comments: we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"keywords: momentum correction, local gradient clipping, momentum factor masking, and warm-up training\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.01887\"\n  }, \"https://arxiv.org/abs/1712.01887\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed learning of CNNs on heterogeneous CPU/GPU architectures\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.02546\"\n  }, \"https://arxiv.org/abs/1712.02546\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Integrated Model and Data Parallelism in Training Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: UC Berkeley & Lawrence Berkeley National Laboratory\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.04432\"\n  }, \"https://arxiv.org/abs/1712.04432\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.01887\"\n  }, \"https://arxiv.org/abs/1712.01887\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"RedSync : Reducing Synchronization Traffic for Distributed Deep Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1808.04357\"\n  }, \"https://arxiv.org/abs/1808.04357\")), mdx(\"h2\", {\n    \"id\": \"projects\"\n  }, \"Projects\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Theano-MPI: a Theano-based Distributed Training Framework\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1605.08325\"\n  }, \"https://arxiv.org/abs/1605.08325\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/uoguelph-mlrg/Theano-MPI\"\n  }, \"https://github.com/uoguelph-mlrg/Theano-MPI\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"CaffeOnSpark: Open Sourced for Distributed Deep Learning on Big Data Clusters\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Yahoo Big ML Team\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep\"\n  }, \"http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/yahoo/CaffeOnSpark\"\n  }, \"https://github.com/yahoo/CaffeOnSpark\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=bqj7nML-aHk\"\n  }, \"https://www.youtube.com/watch?v=bqj7nML-aHk\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Tunnel: Data Driven Framework for Distributed Computing in Torch 7\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhangxiangxiao/tunnel\"\n  }, \"https://github.com/zhangxiangxiao/tunnel\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed deep learning with Keras and Apache Spark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"project page: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://joerihermans.com/work/distributed-keras/\"\n  }, \"http://joerihermans.com/work/distributed-keras/\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/JoeriHermans/dist-keras\"\n  }, \"https://github.com/JoeriHermans/dist-keras\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"BigDL: Distributed Deep learning Library for Apache Spark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/intel-analytics/BigDL\"\n  }, \"https://github.com/intel-analytics/BigDL\"))), mdx(\"h2\", {\n    \"id\": \"videos\"\n  }, \"Videos\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"A Scalable Implementation of Deep Learning on Spark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=pNYBBhuK8yU\"\n  }, \"https://www.youtube.com/watch?v=pNYBBhuK8yU\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1mhzF1uK\"\n  }, \"http://pan.baidu.com/s/1mhzF1uK\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed TensorFlow on Spark: Scaling Google's Deep Learning Library (Spark Summit)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=-QtcP3yRqyM\"\n  }, \"https://www.youtube.com/watch?v=-QtcP3yRqyM\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1mgOR1GG\"\n  }, \"http://pan.baidu.com/s/1mgOR1GG\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Recurrent Neural Networks for Sequence Learning in Spark (Spark Summit)\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"youtube: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=mUuqLcl8Jog\"\n  }, \"https://www.youtube.com/watch?v=mUuqLcl8Jog\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"mirror: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://pan.baidu.com/s/1sklHTPr\"\n  }, \"http://pan.baidu.com/s/1sklHTPr\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed deep learning on Spark\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"author: Alexander Ulanov July 12, 2016\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Alexander Ulanov offers an overview of tools and frameworks that have been proposed for performing deep learning on Spark.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"video: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.oreilly.com/learning/distributed-deep-learning-on-spark\"\n  }, \"https://www.oreilly.com/learning/distributed-deep-learning-on-spark\"))), mdx(\"h2\", {\n    \"id\": \"blogs\"\n  }, \"Blogs\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed Deep Learning Reads\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com//tmulc18/DistributedDeepLearningReads\"\n  }, \"https://github.com//tmulc18/DistributedDeepLearningReads\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hadoop, Spark, Deep Learning Mesh on Single GPU Cluster\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.nextplatform.com/2016/02/24/hadoop-spark-deep-learning-mesh-on-single-gpu-cluster/\"\n  }, \"http://www.nextplatform.com/2016/02/24/hadoop-spark-deep-learning-mesh-on-single-gpu-cluster/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Unreasonable Effectiveness of Deep Learning on Spark\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html\"\n  }, \"https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed Deep Learning with Caffe Using a MapR Cluster\")), mdx(\"img\", {\n    \"src\": \"https://www.mapr.com/sites/default/files/spark-driver.jpg\",\n    \"alt\": null\n  }), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.mapr.com/blog/distributed-deep-learning-caffe-using-mapr-cluster\"\n  }, \"https://www.mapr.com/blog/distributed-deep-learning-caffe-using-mapr-cluster\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deep Learning with Apache Spark and TensorFlow\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html\"\n  }, \"https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Deeplearning4j on Spark\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://deeplearning4j.org/spark\"\n  }, \"http://deeplearning4j.org/spark\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed Deep Learning, Part 1: An Introduction to Distributed Training of Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"blog: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://engineering.skymind.io/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks\"\n  }, \"http://engineering.skymind.io/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"GPU Acceleration in Databricks: Speeding Up Deep Learning on Apache Spark\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html\"\n  }, \"https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Distributed Deep Learning with Apache Spark and Keras\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras\"\n  }, \"https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras\")), mdx(\"h1\", {\n    \"id\": \"adversarial-training\"\n  }, \"Adversarial Training\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning from Simulated and Unsupervised Images through Adversarial Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2017 oral, best paper award. Apple Inc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1612.07828\"\n  }, \"https://arxiv.org/abs/1612.07828\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"The Robust Manifold Defense: Adversarial Training using Generative Models\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.09196\"\n  }, \"https://arxiv.org/abs/1712.09196\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"DeepDefense: Training Deep Neural Networks with Improved Robustness\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.00404\"\n  }, \"https://arxiv.org/abs/1803.00404\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gradient Adversarial Training of Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Magic Leap\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1806.08028\"\n  }, \"https://arxiv.org/abs/1806.08028\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Gray-box Adversarial Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ECCV 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1808.01753\"\n  }, \"https://arxiv.org/abs/1808.01753\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Universal Adversarial Training\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1811.11304\"\n  }, \"https://arxiv.org/abs/1811.11304\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"MEAL: Multi-Model Ensemble via Adversarial Learning\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: AAAI 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Fudan University & University of Illinois at Urbana-Champaign\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.02425\"\n  }, \"https://arxiv.org/abs/1812.02425\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github(official): \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/AaronHeee/MEAL\"\n  }, \"https://github.com/AaronHeee/MEAL\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Regularized Ensembles and Transferability in Adversarial Learning\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1812.01821\"\n  }, \"https://arxiv.org/abs/1812.01821\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Feature denoising for improving adversarial robustness\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Johns Hopkins University & Facebook AI Research\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1812.03411\"\n  }, \"https://arxiv.org/abs/1812.03411\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/facebookresearch/ImageNet-Adversarial-Training\"\n  }, \"https://github.com/facebookresearch/ImageNet-Adversarial-Training\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Second Rethinking of Network Pruning in the Adversarial Setting\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1903.12561\"\n  }, \"https://arxiv.org/abs/1903.12561\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Interpreting Adversarially Trained Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICML 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1905.09797\"\n  }, \"https://arxiv.org/abs/1905.09797\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On Stabilizing Generative Adversarial Training with Noise\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CVPR 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1906.04612\"\n  }, \"https://arxiv.org/abs/1906.04612\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Adversarial Learning with Margin-based Triplet Embedding Regularization\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICCV 2019\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: BUPT\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1909.09481\"\n  }, \"https://arxiv.org/abs/1909.09481\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/zhongyy/Adversarial_MTER\"\n  }, \"https://github.com/zhongyy/Adversarial_MTER\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Bag of Tricks for Adversarial Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Tsinghua University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2010.00467\"\n  }, \"https://arxiv.org/abs/2010.00467\"))), mdx(\"h1\", {\n    \"id\": \"low-precision-training\"\n  }, \"Low-Precision Training\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Mixed Precision Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: ICLR 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1710.03740\"\n  }, \"https://arxiv.org/abs/1710.03740\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"High-Accuracy Low-Precision Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Cornell University & Stanford University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1803.03383\"\n  }, \"https://arxiv.org/abs/1803.03383\"))), mdx(\"h1\", {\n    \"id\": \"incremental-training\"\n  }, \"Incremental Training\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ClickBAIT: Click-based Accelerated Incremental Training of Convolutional Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1709.05021\"\n  }, \"https://arxiv.org/abs/1709.05021\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"dataset: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://clickbait.crossmobile.info/\"\n  }, \"http://clickbait.crossmobile.info/\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ClickBAIT-v2: Training an Object Detector in Real-Time\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1803.10358\"\n  }, \"https://arxiv.org/abs/1803.10358\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Class-incremental Learning via Deep Model Consolidation\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Southern California & Arizona State University & Samsung Research America\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1903.07864\"\n  }, \"https://arxiv.org/abs/1903.07864\"))), mdx(\"h1\", {\n    \"id\": \"papers-1\"\n  }, \"Papers\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Understanding the difficulty of training deep feed forward neural networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Xavier initialization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\"\n  }, \"http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Domain-Adversarial Training of Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1505.07818\"\n  }, \"https://arxiv.org/abs/1505.07818\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"paper: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://jmlr.org/papers/v17/15-239.html\"\n  }, \"http://jmlr.org/papers/v17/15-239.html\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/pumpikano/tf-dann\"\n  }, \"https://github.com/pumpikano/tf-dann\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Scalable and Sustainable Deep Learning via Randomized Hashing\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1602.08194\"\n  }, \"http://arxiv.org/abs/1602.08194\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Training Deep Nets with Sublinear Memory Cost\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1604.06174\"\n  }, \"https://arxiv.org/abs/1604.06174\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/dmlc/mxnet-memonger\"\n  }, \"https://github.com/dmlc/mxnet-memonger\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Bihaqo/tf-memonger\"\n  }, \"https://github.com/Bihaqo/tf-memonger\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Improving the Robustness of Deep Neural Networks via Stability Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1604.04326\"\n  }, \"http://arxiv.org/abs/1604.04326\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Faster Training of Very Deep Networks Via p-Norm Gates\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://arxiv.org/abs/1608.03639\"\n  }, \"http://arxiv.org/abs/1608.03639\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fast Training of Convolutional Neural Networks via Kernel Rescaling\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1610.03623\"\n  }, \"https://arxiv.org/abs/1610.03623\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"FreezeOut: Accelerate Training by Progressively Freezing Layers\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1706.04983\"\n  }, \"https://arxiv.org/abs/1706.04983\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/ajbrock/FreezeOut\"\n  }, \"https://github.com/ajbrock/FreezeOut\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Normalized Gradient with Adaptive Stepsize Method for Deep Neural Network Training\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: CMU & The University of Iowa\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1707.04822\"\n  }, \"https://arxiv.org/abs/1707.04822\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Image Quality Assessment Guided Deep Neural Networks Training\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1708.03880\"\n  }, \"https://arxiv.org/abs/1708.03880\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"An Effective Training Method For Deep Convolutional Neural Network\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Beijing Institute of Technology & Tsinghua University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.01666\"\n  }, \"https://arxiv.org/abs/1708.01666\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"On the Importance of Consistency in Training Deep Neural Networks\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: University of Maryland & Arizona State University\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1708.00631\"\n  }, \"https://arxiv.org/abs/1708.00631\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Solving internal covariate shift in deep learning with linked neurons\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intro: Universitat de Barcelona\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"arxiv: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1712.02609\"\n  }, \"https://arxiv.org/abs/1712.02609\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/blauigris/linked_neurons\"\n  }, \"https://github.com/blauigris/linked_neurons\"))), mdx(\"h1\", {\n    \"id\": \"tools\"\n  }, \"Tools\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"pastalog: Simple, realtime visualization of neural network training performance\")), mdx(\"img\", {\n    \"src\": \"/assets/train-dnn/pastalog-main-big.gif\",\n    \"alt\": null\n  }), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/rewonc/pastalog\"\n  }, \"https://github.com/rewonc/pastalog\"))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"torch-pastalog: A Torch interface for pastalog - simple, realtime visualization of neural network training performance\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"github: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Kaixhin/torch-pastalog\"\n  }, \"https://github.com/Kaixhin/torch-pastalog\"))), mdx(\"h1\", {\n    \"id\": \"blogs-1\"\n  }, \"Blogs\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Important nuances to train deep learning models\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.erogol.com/important-nuances-train-deep-learning-models/\"\n  }, \"http://www.erogol.com/important-nuances-train-deep-learning-models/\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Train your deep model faster and sharper\\u200A\\u2014\\u200Atwo novel techniques\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047\"\n  }, \"https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\nlayout: post\ncategory: deep_learning\ntitle: Training Deep Neural Networks\ndate: 2015-10-09\n---\n\n# Tutorials\n\n**Popular Training Approaches of DNNsA Quick Overview**\n\n[https://medium.com/@asjad/popular-training-approaches-of-dnns-a-quick-overview-26ee37ad7e96#.pqyo039bb](https://medium.com/@asjad/popular-training-approaches-of-dnns-a-quick-overview-26ee37ad7e96#.pqyo039bb)\n\n**Optimisation and training techniques for deep learning**\n\n[https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/](https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/)\n\n# Papers\n\n**SNIPER: Efficient Multi-Scale Training**\n\n[https://arxiv.org/abs/1805.09300](https://arxiv.org/abs/1805.09300)\n\n**RePr: Improved Training of Convolutional Filters**\n\n[https://arxiv.org/abs/1811.07275](https://arxiv.org/abs/1811.07275)\n\n# Activation functions\n\n## ReLU\n\n**Rectified linear units improve restricted boltzmann machines**\n\n- intro: ReLU\n- paper: [http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf)\n\n**Expressiveness of Rectifier Networks**\n\n- intro: ICML 2016\n- intro: This paper studies the expressiveness of ReLU Networks\n- arxiv: [https://arxiv.org/abs/1511.05678](https://arxiv.org/abs/1511.05678)\n\n**How can a deep neural network with ReLU activations in its hidden layers approximate any function?**\n\n- quora: [https://www.quora.com/How-can-a-deep-neural-network-with-ReLU-activations-in-its-hidden-layers-approximate-any-function](https://www.quora.com/How-can-a-deep-neural-network-with-ReLU-activations-in-its-hidden-layers-approximate-any-function)\n\n**Understanding Deep Neural Networks with Rectified Linear Units**\n\n- intro: Johns Hopkins University\n- arxiv: [https://arxiv.org/abs/1611.01491](https://arxiv.org/abs/1611.01491)\n\n**Learning ReLUs via Gradient Descent**\n\n[https://arxiv.org/abs/1705.04591](https://arxiv.org/abs/1705.04591)\n\n**Training Better CNNs Requires to Rethink ReLU**\n\n[https://arxiv.org/abs/1709.06247](https://arxiv.org/abs/1709.06247)\n\n**Deep Learning using Rectified Linear Units (ReLU)**\n\n- intro: Adamson University\n- arxiv: [https://arxiv.org/abs/1803.08375](https://arxiv.org/abs/1803.08375)\n- github: [https://github.com/AFAgarap/relu-classifier](https://github.com/AFAgarap/relu-classifier)\n\n**Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks**\n\n- intro: University of California, Los Angeles\n- arxiv: [https://arxiv.org/abs/1811.08888](https://arxiv.org/abs/1811.08888)\n\n## LReLU\n\n**Rectifier Nonlinearities Improve Neural Network Acoustic Models**\n\n- intro: leaky-ReLU, aka LReLU\n- paper: [http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf](http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)\n\n**Deep Sparse Rectifier Neural Networks**\n\n- paper: [http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf](http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)\n\n## PReLU\n\n**Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification**\n\n- keywords: PReLU, Caffe \"msra\" weights initilization\n- arxiv: [http://arxiv.org/abs/1502.01852](http://arxiv.org/abs/1502.01852)\n\n**Empirical Evaluation of Rectified Activations in Convolutional Network**\n\n- intro: ReLU / LReLU / PReLU / RReLU\n- arxiv: [http://arxiv.org/abs/1505.00853](http://arxiv.org/abs/1505.00853)\n\n## SReLU\n\n**Deep Learning with S-shaped Rectified Linear Activation Units**\n\n- intro:  SReLU\n- arxiv: [http://arxiv.org/abs/1512.07030](http://arxiv.org/abs/1512.07030)\n\n**Parametric Activation Pools greatly increase performance and consistency in ConvNets**\n\n- blog: [http://blog.claymcleod.io/2016/02/06/Parametric-Activation-Pools-greatly-increase-performance-and-consistency-in-ConvNets/](http://blog.claymcleod.io/2016/02/06/Parametric-Activation-Pools-greatly-increase-performance-and-consistency-in-ConvNets/)\n\n**From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification**\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1602.02068](http://arxiv.org/abs/1602.02068)\n- github: [https://github.com/gokceneraslan/SparseMax.torch](https://github.com/gokceneraslan/SparseMax.torch)\n- github: [https://github.com/Unbabel/sparsemax](https://github.com/Unbabel/sparsemax)\n\n**Revise Saturated Activation Functions**\n\n- arxiv: [http://arxiv.org/abs/1602.05980](http://arxiv.org/abs/1602.05980)\n\n**Noisy Activation Functions**\n\n- arxiv: [http://arxiv.org/abs/1603.00391](http://arxiv.org/abs/1603.00391)\n\n## MBA\n\n**Multi-Bias Non-linear Activation in Deep Neural Networks**\n\n- intro: MBA\n- arxiv: [https://arxiv.org/abs/1604.00676](https://arxiv.org/abs/1604.00676)\n\n**Learning activation functions from data using cubic spline interpolation**\n\n- arxiv: [http://arxiv.org/abs/1605.05509](http://arxiv.org/abs/1605.05509)\n- bitbucket: [https://bitbucket.org/ispamm/spline-nn](https://bitbucket.org/ispamm/spline-nn)\n\n**What is the role of the activation function in a neural network?**\n\n- quora: [https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network](https://www.quora.com/What-is-the-role-of-the-activation-function-in-a-neural-network)\n\n## Concatenated ReLU (CRelu)\n\n**Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units**\n\n- intro: ICML 2016\n- arxiv: [http://arxiv.org/abs/1603.05201](http://arxiv.org/abs/1603.05201)\n\n**Implement CReLU (Concatenated ReLU)**\n\n- github: [https://github.com/pfnet/chainer/pull/1142](https://github.com/pfnet/chainer/pull/1142)\n\n## GELU\n\n**Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units**\n\n- arxiv: [https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)\n\n**Formulating The ReLU**\n\n- blog: [http://www.jefkine.com/general/2016/08/24/formulating-the-relu/](http://www.jefkine.com/general/2016/08/24/formulating-the-relu/)\n\n**Activation Ensembles for Deep Neural Networks**\n\n[https://arxiv.org/abs/1702.07790](https://arxiv.org/abs/1702.07790)\n\n## SELU\n\n**Self-Normalizing Neural Networks**\n\n- intro: SELU\n- arxiv: [https://arxiv.org/abs/1706.02515](https://arxiv.org/abs/1706.02515)\n- github: [https://github.com/bioinf-jku/SNNs](https://github.com/bioinf-jku/SNNs)\n- notes: [https://github.com/kevinzakka/research-paper-notes/blob/master/snn.md](https://github.com/kevinzakka/research-paper-notes/blob/master/snn.md)\n- github(Chainer): [https://github.com/musyoku/self-normalizing-networks](https://github.com/musyoku/self-normalizing-networks)\n\n**SELUs (scaled exponential linear units) - Visualized and Histogramed Comparisons among ReLU and Leaky ReLU**\n\n[https://github.com/shaohua0116/Activation-Visualization-Histogram](https://github.com/shaohua0116/Activation-Visualization-Histogram)\n\n**Difference Between Softmax Function and Sigmoid Function**\n\n[http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/](http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/)\n\n**Flexible Rectified Linear Units for Improving Convolutional Neural Networks**\n\n- keywords: flexible rectified linear unit (FReLU)\n- arxiv: [https://arxiv.org/abs/1706.08098](https://arxiv.org/abs/1706.08098)\n\n**Be Careful What You Backpropagate: A Case For Linear Output Activations & Gradient Boosting**\n\n- intro: CMU\n- arxiv: [https://arxiv.org/abs/1707.04199](https://arxiv.org/abs/1707.04199)\n\n## EraseReLU\n\n**EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks**\n\n[https://arxiv.org/abs/1709.07634](https://arxiv.org/abs/1709.07634)\n\n## Swish\n\n**Swish: a Self-Gated Activation Function**\n\n**Searching for Activation Functions**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1710.05941](https://arxiv.org/abs/1710.05941)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/77gcrv/d_swish_is_not_performing_very_well/](https://www.reddit.com/r/MachineLearning/comments/77gcrv/d_swish_is_not_performing_very_well/)\n\n**Deep Learning with Data Dependent Implicit Activation Function**\n\n[https://arxiv.org/abs/1802.00168](https://arxiv.org/abs/1802.00168)\n\n## Series on Initialization of Weights for DNN\n\n**Initialization Of Feedfoward Networks**\n\n- blog: [http://www.jefkine.com/deep/2016/07/27/initialization-of-feedfoward-networks/](http://www.jefkine.com/deep/2016/07/27/initialization-of-feedfoward-networks/)\n\n**Initialization Of Deep Feedfoward Networks**\n\n- blog: [http://www.jefkine.com/deep/2016/08/01/initialization-of-deep-feedfoward-networks/](http://www.jefkine.com/deep/2016/08/01/initialization-of-deep-feedfoward-networks/)\n\n**Initialization Of Deep Networks Case of Rectifiers**\n\n- blog: [http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/](http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)\n\n# Weights Initialization\n\n**An Explanation of Xavier Initialization**\n\n- blog: [http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)\n\n**Random Walk Initialization for Training Very Deep Feedforward Networks**\n\n- arxiv: [http://arxiv.org/abs/1412.6558](http://arxiv.org/abs/1412.6558)\n\n**Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?**\n\n- arxiv: [http://arxiv.org/abs/1504.08291](http://arxiv.org/abs/1504.08291)\n\n**All you need is a good init**\n\n- intro: ICLR 2016\n- intro: Layer-sequential unit-variance (LSUV) initialization\n- arxiv: [http://arxiv.org/abs/1511.06422](http://arxiv.org/abs/1511.06422)\n- github(Caffe): [https://github.com/ducha-aiki/LSUVinit](https://github.com/ducha-aiki/LSUVinit)\n- github(Torch): [https://github.com/yobibyte/torch-lsuv](https://github.com/yobibyte/torch-lsuv)\n- github: [https://github.com/yobibyte/yobiblog/blob/master/posts/all-you-need-is-a-good-init.md](https://github.com/yobibyte/yobiblog/blob/master/posts/all-you-need-is-a-good-init.md)\n- github(Keras): [https://github.com/ducha-aiki/LSUV-keras](https://github.com/ducha-aiki/LSUV-keras)\n- review: [http://www.erogol.com/need-good-init/](http://www.erogol.com/need-good-init/)\n\n**All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation**\n\n- intro: CVPR 2017. HIKVision\n- arxiv: [https://arxiv.org/abs/1703.01827](https://arxiv.org/abs/1703.01827)\n\n**Data-dependent Initializations of Convolutional Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1511.06856](http://arxiv.org/abs/1511.06856)\n- github: [https://github.com/philkr/magic_init](https://github.com/philkr/magic_init)\n\n**What are good initial weights in a neural network?**\n\n- stackexchange: [http://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network](http://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network)\n\n**RandomOut: Using a convolutional gradient norm to win The Filter Lottery**\n\n- arxiv: [http://arxiv.org/abs/1602.05931](http://arxiv.org/abs/1602.05931)\n\n**Categorical Reparameterization with Gumbel-Softmax**\n\n- intro: Google Brain & University of Cambridge & Stanford University\n- arxiv: [https://arxiv.org/abs/1611.01144](https://arxiv.org/abs/1611.01144)\n- github: [https://github.com/ericjang/gumbel-softmax](https://github.com/ericjang/gumbel-softmax)\n\n**On weight initialization in deep neural networks**\n\n- arxiv: [https://arxiv.org/abs/1704.08863](https://arxiv.org/abs/1704.08863)\n- github: [https://github.com/sidkk86/weight_initialization](https://github.com/sidkk86/weight_initialization)\n\n**Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks**\n\n- intro: ICML 2018. Google Brain\n- arxiv: [https://arxiv.org/abs/1806.05393](https://arxiv.org/abs/1806.05393)\n\n## Batch Normalization\n\n**Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n\n- intro: ImageNet top-5 error: 4.82%\n- keywords: internal covariate shift problem\n- arxiv: [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167)\n- blog: [https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/](https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/)\n- notes: [http://blog.csdn.net/happynear/article/details/44238541](http://blog.csdn.net/happynear/article/details/44238541)\n\n**Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1602.07868](http://arxiv.org/abs/1602.07868)\n- github(Lasagne): [https://github.com/TimSalimans/weight_norm](https://github.com/TimSalimans/weight_norm)\n- github: [https://github.com/openai/weightnorm](https://github.com/openai/weightnorm)\n- notes: [http://www.erogol.com/my-notes-weight-normalization/](http://www.erogol.com/my-notes-weight-normalization/)\n\n**Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks**\n\n- arxiv: [http://arxiv.org/abs/1603.01431](http://arxiv.org/abs/1603.01431)\n\n**Revisiting Batch Normalization For Practical Domain Adaptation**\n\n- intro: Peking University & TuSimple & SenseTime\n- intro: Pattern Recognition\n- keywords: Adaptive Batch Normalization (AdaBN)\n- arxiv: [https://arxiv.org/abs/1603.04779](https://arxiv.org/abs/1603.04779)\n\n**Implementing Batch Normalization in Tensorflow**\n\n- blog: [http://r2rt.com/implementing-batch-normalization-in-tensorflow.html](http://r2rt.com/implementing-batch-normalization-in-tensorflow.html)\n\n**Deriving the Gradient for the Backward Pass of Batch Normalization**\n\n- blog: [https://kevinzakka.github.io/2016/09/14/batch_normalization/](https://kevinzakka.github.io/2016/09/14/batch_normalization/)\n\n**Exploring Normalization in Deep Residual Networks with Concatenated Rectified Linear Units**\n\n- intro: Oculus VR & Facebook & NEC Labs America\n- paper: [https://research.fb.com/publications/exploring-normalization-in-deep-residual-networks-with-concatenated-rectified-linear-units/](https://research.fb.com/publications/exploring-normalization-in-deep-residual-networks-with-concatenated-rectified-linear-units/)\n\n**Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models**\n\n- intro: Sergey Ioffe, Google\n- arxiv: [https://arxiv.org/abs/1702.03275](https://arxiv.org/abs/1702.03275)\n\n**Comparison of Batch Normalization and Weight Normalization Algorithms for the Large-scale Image Classification**\n\n[https://arxiv.org/abs/1709.08145](https://arxiv.org/abs/1709.08145)\n\n**In-Place Activated BatchNorm for Memory-Optimized Training of DNNs**\n\n- intro: Mapillary Research\n- arxiv: [https://arxiv.org/abs/1712.02616](https://arxiv.org/abs/1712.02616)\n- github: [https://github.com/mapillary/inplace_abn](https://github.com/mapillary/inplace_abn)\n\n**Batch Kalman Normalization: Towards Training Deep Neural Networks with Micro-Batches**\n\n[https://arxiv.org/abs/1802.03133](https://arxiv.org/abs/1802.03133)\n\n**Decorrelated Batch Normalization**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.08450](https://arxiv.org/abs/1804.08450)\n- github: [https://github.com/umich-vl/DecorrelatedBN](https://github.com/umich-vl/DecorrelatedBN)\n\n**Understanding Batch Normalization**\n\n[https://arxiv.org/abs/1806.02375](https://arxiv.org/abs/1806.02375)\n\n**Implementing Synchronized Multi-GPU Batch Normalization**\n\n[http://hangzh.com/PyTorch-Encoding/notes/syncbn.html](http://hangzh.com/PyTorch-Encoding/notes/syncbn.html)\n\n**Restructuring Batch Normalization to Accelerate CNN Training**\n\n[https://arxiv.org/abs/1807.01702](https://arxiv.org/abs/1807.01702)\n\n**Intro to optimization in deep learning: Busting the myth about batch normalization**\n\n- blog: [https://blog.paperspace.com/busting-the-myths-about-batch-normalization/](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)\n\n**Understanding Regularization in Batch Normalization**\n\n[https://arxiv.org/abs/1809.00846](https://arxiv.org/abs/1809.00846)\n\n**How Does Batch Normalization Help Optimization?**\n\n- intro: NeurIPS 2018. MIT\n- arxiv: [https://arxiv.org/abs/1805.11604](https://arxiv.org/abs/1805.11604)\n- video: [https://www.youtube.com/watch?v=ZOabsYbmBRM](https://www.youtube.com/watch?v=ZOabsYbmBRM)\n\n**Cross-Iteration Batch Normalization**\n\n[https://arxiv.org/abs/2002.05712](https://arxiv.org/abs/2002.05712)\n\n**Extended Batch Normalization**\n\n- intro: Chinese Academy of Sciences\n- arxiv: [https://arxiv.org/abs/2003.05569](https://arxiv.org/abs/2003.05569)\n\n**Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization**\n\n- intro: ICLR 2020 Poster\n- keywords: Moving Average Batch Normalization\n- openreview: [https://openreview.net/forum?id=SkgGjRVKDS](https://openreview.net/forum?id=SkgGjRVKDS)\n- github(official, Pytorch): [https://github.com/megvii-model/MABN](https://github.com/megvii-model/MABN)\n\n**Rethinking Batch in BatchNorm**\n\n- intro: Facebook AI Research\n- arxiv: [https://arxiv.org/abs/2105.07576](https://arxiv.org/abs/2105.07576)\n\n**Delving into the Estimation Shift of Batch Normalization in a Network**\n\n- intro: CVPR 2022\n- arxiv: [https://arxiv.org/abs/2203.10778](https://arxiv.org/abs/2203.10778)\n- gtihub: [https://github.com/huangleiBuaa/XBNBlock](https://github.com/huangleiBuaa/XBNBlock)\n\n### Backward pass of BN\n\n**Understanding the backward pass through Batch Normalization Layer**\n\n[https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n\n**Deriving the Gradient for the Backward Pass of Batch Normalization**\n\n[https://kevinzakka.github.io/2016/09/14/batch_normalization/](https://kevinzakka.github.io/2016/09/14/batch_normalization/)\n\n**What does the gradient flowing through batch normalization looks like ?**\n\n[http://cthorey.github.io./backpropagation/](http://cthorey.github.io./backpropagation/)\n\n## Layer Normalization\n\n**Layer Normalization**\n\n- arxiv: [https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)\n- github: [https://github.com/ryankiros/layer-norm](https://github.com/ryankiros/layer-norm)\n- github(TensorFlow): [https://github.com/pbhatia243/tf-layer-norm](https://github.com/pbhatia243/tf-layer-norm)\n- github: [https://github.com/MycChiu/fast-LayerNorm-TF](https://github.com/MycChiu/fast-LayerNorm-TF)\n\n**Keras GRU with Layer Normalization**\n\n- gist: [https://gist.github.com/udibr/7f46e790c9e342d75dcbd9b1deb9d940](https://gist.github.com/udibr/7f46e790c9e342d75dcbd9b1deb9d940)\n\n**Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1702.05870](https://arxiv.org/abs/1702.05870)\n\n**Differentiable Learning-to-Normalize via Switchable Normalization**\n\n- arxiv: [https://arxiv.org/abs/1806.10779](https://arxiv.org/abs/1806.10779)\n- github: [https://github.com/switchablenorms/Switchable-Normalization](https://github.com/switchablenorms/Switchable-Normalization)\n\n## Group Normalization\n\n**Group Normalization**\n\n- intro: ECCV 2018 Best Paper Award Honorable Mention\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/1803.08494](https://arxiv.org/abs/1803.08494)\n\n## Batch-Instance Normalization\n\n**Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks**\n\n[https://arxiv.org/abs/1805.07925](https://arxiv.org/abs/1805.07925)\n\n**Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1807.09441](https://arxiv.org/abs/1807.09441)\n- github(official, Pytorch): [https://github.com/XingangPan/IBN-Net](https://github.com/XingangPan/IBN-Net)\n\n## Dynamic Normalization\n\n**Dynamic Normalization**\n\n[https://arxiv.org/abs/2101.06073](https://arxiv.org/abs/2101.06073)\n\n# Loss Function\n\n**The Loss Surfaces of Multilayer Networks**\n\n- arxiv: [http://arxiv.org/abs/1412.0233](http://arxiv.org/abs/1412.0233)\n\n**Direct Loss Minimization for Training Deep Neural Nets**\n\n- arxiv: [http://arxiv.org/abs/1511.06411](http://arxiv.org/abs/1511.06411)\n\n**Nonconvex Loss Functions for Classifiers and Deep Networks**\n\n- blog: [https://casmls.github.io/general/2016/10/27/NonconvexLosses.html](https://casmls.github.io/general/2016/10/27/NonconvexLosses.html)\n\n**Learning Deep Embeddings with Histogram Loss**\n\n- arxiv: [https://arxiv.org/abs/1611.00822](https://arxiv.org/abs/1611.00822)\n\n**Large-Margin Softmax Loss for Convolutional Neural Networks**\n\n- intro: ICML 2016\n- intro: Peking University & South China University of Technology & CMU & Shenzhen University\n- arxiv: [https://arxiv.org/abs/1612.02295](https://arxiv.org/abs/1612.02295)\n- github(Official. Caffe): [https://github.com/wy1iu/LargeMargin_Softmax_Loss](https://github.com/wy1iu/LargeMargin_Softmax_Loss)\n- github: [https://github.com/luoyetx/mx-lsoftmax](https://github.com/luoyetx/mx-lsoftmax)\n- github: [https://github.com/tpys/face-recognition-caffe2](https://github.com/tpys/face-recognition-caffe2)\n- github: [https://github.com/jihunchoi/lsoftmax-pytorch](https://github.com/jihunchoi/lsoftmax-pytorch)\n\n**An empirical analysis of the optimization of deep network loss surfaces**\n\n[https://arxiv.org/abs/1612.04010](https://arxiv.org/abs/1612.04010)\n\n**Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes**\n\n- intro: Peking University\n- arxiv: [https://arxiv.org/abs/1706.10239](https://arxiv.org/abs/1706.10239)\n\n**Hierarchical Softmax**\n\n[http://building-babylon.net/2017/08/01/hierarchical-softmax/](http://building-babylon.net/2017/08/01/hierarchical-softmax/)\n\n**Noisy Softmax: Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation**\n\n- intro: CVPR 2017\n- arxiv: [https://arxiv.org/abs/1708.03769](https://arxiv.org/abs/1708.03769)\n\n**DropMax: Adaptive Stochastic Softmax**\n\n- intro: UNIST & Postech & KAIST\n- arxiv: [https://arxiv.org/abs/1712.07834](https://arxiv.org/abs/1712.07834)\n\n**Rethinking Feature Distribution for Loss Functions in Image Classification**\n\n- intro: CVPR 2018 spotlight\n- arxiv: [https://arxiv.org/abs/1803.02988](https://arxiv.org/abs/1803.02988)\n\n**Ensemble Soft-Margin Softmax Loss for Image Classification**\n\n- intro: IJCAI 2018\n- arxiv: [https://arxiv.org/abs/1805.03922](https://arxiv.org/abs/1805.03922)\n\n**Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels**\n\n- intro: Cornell University\n- arxiv: [https://arxiv.org/abs/1805.07836](https://arxiv.org/abs/1805.07836)\n\n# Learning Rates\n\n**No More Pesky Learning Rates**\n\n- intro: Tom Schaul, Sixin Zhang, Yann LeCun\n- arxiv: [https://arxiv.org/abs/1206.1106](https://arxiv.org/abs/1206.1106)\n\n**Coupling Adaptive Batch Sizes with Learning Rates**\n\n- intro: Max Planck Institute for Intelligent Systems\n- intro: Tensorflow implementation of SGD with Coupled Adaptive Batch Size (CABS)\n- arxiv: [https://arxiv.org/abs/1612.05086](https://arxiv.org/abs/1612.05086)\n- github: [https://github.com/ProbabilisticNumerics/cabs](https://github.com/ProbabilisticNumerics/cabs)\n\n**Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates**\n\n[https://arxiv.org/abs/1708.07120](https://arxiv.org/abs/1708.07120)\n\n**Improving the way we work with learning rate.**\n\n[https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b](https://medium.com/@bushaev/improving-the-way-we-work-with-learning-rate-5e99554f163b)\n\n**WNGrad: Learn the Learning Rate in Gradient Descent**\n\n- intro: University of Texas at Austin & Facebook AI Research\n- arxiv: [https://arxiv.org/abs/1803.02865](https://arxiv.org/abs/1803.02865)\n\n**Learning with Random Learning Rates**\n\n- intro: Facebook AI Research & Universite Paris Sud\n- keywords: All Learning Rates At Once (Alrao)\n- project page: [https://leonardblier.github.io/alrao/](https://leonardblier.github.io/alrao/)\n- arxiv: [https://arxiv.org/abs/1810.01322](https://arxiv.org/abs/1810.01322)\n- github(PyTorch, official): [https://github.com/leonardblier/alrao](https://github.com/leonardblier/alrao)\n\n**Learning Rate Dropout**\n\n- intro: 1Xiamen University & Columbia University\n- arxiv: [https://arxiv.org/abs/1912.00144](https://arxiv.org/abs/1912.00144)\n\n# Convolution Filters\n\n**Non-linear Convolution Filters for CNN-based Learning**\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1708.07038](https://arxiv.org/abs/1708.07038)\n\n# Pooling\n\n**Stochastic Pooling for Regularization of Deep Convolutional Neural Networks**\n\n- intro: ICLR 2013. Matthew D. Zeiler, Rob Fergus\n- paper: [http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf](http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf)\n\n**Multi-scale Orderless Pooling of Deep Convolutional Activation Features**\n\n- intro: ECCV 2014\n- intro: MOP-CNN, orderless VLAD pooling, image classification / instance-level retrieval\n- arxiv: [https://arxiv.org/abs/1403.1840](https://arxiv.org/abs/1403.1840)\n- paper: [http://web.engr.illinois.edu/~slazebni/publications/yunchao_eccv14_mopcnn.pdf](http://web.engr.illinois.edu/~slazebni/publications/yunchao_eccv14_mopcnn.pdf)\n\n**Fractional Max-Pooling**\n\n- arxiv: [https://arxiv.org/abs/1412.6071](https://arxiv.org/abs/1412.6071)\n- notes: [https://gist.github.com/shagunsodhani/ccfe3134f46fd3738aa0](https://gist.github.com/shagunsodhani/ccfe3134f46fd3738aa0)\n- github: [https://github.com/torch/nn/issues/371](https://github.com/torch/nn/issues/371)\n\n**TI-POOLING: transformation-invariant pooling for feature learning in Convolutional Neural Networks**\n\n- intro: CVPR 2016\n- paper: [http://dlaptev.org/papers/Laptev16_CVPR.pdf](http://dlaptev.org/papers/Laptev16_CVPR.pdf)\n- github: [https://github.com/dlaptev/TI-pooling](https://github.com/dlaptev/TI-pooling)\n\n**S3Pool: Pooling with Stochastic Spatial Sampling**\n\n- arxiv: [https://arxiv.org/abs/1611.05138](https://arxiv.org/abs/1611.05138)\n- github(Lasagne): [https://github.com/Shuangfei/s3pool](https://github.com/Shuangfei/s3pool)\n\n**Inductive Bias of Deep Convolutional Networks through Pooling Geometry**\n\n- arxiv: [https://arxiv.org/abs/1605.06743](https://arxiv.org/abs/1605.06743)\n- github: [https://github.com/HUJI-Deep/inductive-pooling](https://github.com/HUJI-Deep/inductive-pooling)\n\n**Improved Bilinear Pooling with CNNs**\n\n[https://arxiv.org/abs/1707.06772](https://arxiv.org/abs/1707.06772)\n\n**Learning Bag-of-Features Pooling for Deep Convolutional Neural Networks\n\n- intro: ICCV 2017\n- arxiv: [https://arxiv.org/abs/1707.08105](https://arxiv.org/abs/1707.08105)\n- github: [https://github.com/passalis/cbof](https://github.com/passalis/cbof)\n\n**A new kind of pooling layer for faster and sharper convergence**\n\n- blog: [https://medium.com/@singlasahil14/a-new-kind-of-pooling-layer-for-faster-and-sharper-convergence-1043c756a221](https://medium.com/@singlasahil14/a-new-kind-of-pooling-layer-for-faster-and-sharper-convergence-1043c756a221)\n- github: [https://github.com/singlasahil14/sortpool2d](https://github.com/singlasahil14/sortpool2d)\n\n**Statistically Motivated Second Order Pooling**\n\n[https://arxiv.org/abs/1801.07492](https://arxiv.org/abs/1801.07492)\n\n**Detail-Preserving Pooling in Deep Networks**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.04076](https://arxiv.org/abs/1804.04076)\n\n# Mini-Batch\n\n**Online Batch Selection for Faster Training of Neural Networks**\n\n- intro: Workshop paper at ICLR 2016\n- arxiv: [https://arxiv.org/abs/1511.06343](https://arxiv.org/abs/1511.06343)\n\n**On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima**\n\n- intro: ICLR 2017\n- arxiv: [https://arxiv.org/abs/1609.04836](https://arxiv.org/abs/1609.04836)\n\n**Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour**\n\n- intro: Facebook\n- keywords: Training with 256 GPUs, minibatches of 8192\n- arxiv: [https://arxiv.org/abs/1706.02677](https://arxiv.org/abs/1706.02677)\n\n**Scaling SGD Batch Size to 32K for ImageNet Training**\n\n**Large Batch Training of Convolutional Networks**\n\n[https://arxiv.org/abs/1708.03888](https://arxiv.org/abs/1708.03888)\n\n**ImageNet Training in 24 Minutes**\n\n[https://arxiv.org/abs/1709.05011](https://arxiv.org/abs/1709.05011)\n\n**Don't Decay the Learning Rate, Increase the Batch Size**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1711.00489](https://arxiv.org/abs/1711.00489)\n\n**Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes**\n\n- intro: NIPS 2017 Workshop: Deep Learning at Supercomputer Scale\n- arxiv: [https://arxiv.org/abs/1711.04325](https://arxiv.org/abs/1711.04325)\n\n**AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks**\n\n- intro: UC Berkeley & NVIDIA\n- arxiv: [https://arxiv.org/abs/1712.02029](https://arxiv.org/abs/1712.02029)\n\n**Hessian-based Analysis of Large Batch Training and Robustness to Adversaries**\n\n- intro: UC Berkeley & University of Texas\n- arxiv: [https://arxiv.org/abs/1802.08241](https://arxiv.org/abs/1802.08241)\n\n**Large Batch Training of Convolutional Networks with Layer-wise Adaptive Rate Scaling**\n\n- keywords: large batch, LARS, adaptive rate scaling\n- openreview: [https://openreview.net/forum?id=rJ4uaX2aW](https://openreview.net/forum?id=rJ4uaX2aW)\n\n**Revisiting Small Batch Training for Deep Neural Networks**\n\n[https://arxiv.org/abs/1804.07612](https://arxiv.org/abs/1804.07612)\n\n**Second-order Optimization Method for Large Mini-batch: Training ResNet-50 on ImageNet in 35 Epochs**\n\n[https://arxiv.org/abs/1811.12019](https://arxiv.org/abs/1811.12019)\n\n# Optimization Methods\n\n**On Optimization Methods for Deep Learning**\n\n- paper: [http://www.icml-2011.org/papers/210_icmlpaper.pdf](http://www.icml-2011.org/papers/210_icmlpaper.pdf)\n\n**Invariant backpropagation: how to train a transformation-invariant neural network**\n\n- arxiv: [http://arxiv.org/abs/1502.04434](http://arxiv.org/abs/1502.04434)\n- github: [https://github.com/sdemyanov/ConvNet](https://github.com/sdemyanov/ConvNet)\n\n**A practical theory for designing very deep convolutional neural network**\n\n- kaggle: [https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code/69284](https://www.kaggle.com/c/datasciencebowl/forums/t/13166/happy-lantern-festival-report-and-code/69284)\n- paper: [https://kaggle2.blob.core.windows.net/forum-message-attachments/69182/2287/A%20practical%20theory%20for%20designing%20very%20deep%20convolutional%20neural%20networks.pdf?sv=2012-02-12&se=2015-12-05T15%3A40%3A02Z&sr=b&sp=r&sig=kfBQKduA1pDtu837Y9Iqyrp2VYItTV0HCgOeOok9E3E%3D](https://kaggle2.blob.core.windows.net/forum-message-attachments/69182/2287/A%20practical%20theory%20for%20designing%20very%20deep%20convolutional%20neural%20networks.pdf?sv=2012-02-12&se=2015-12-05T15%3A40%3A02Z&sr=b&sp=r&sig=kfBQKduA1pDtu837Y9Iqyrp2VYItTV0HCgOeOok9E3E%3D)\n- slides: [http://vdisk.weibo.com/s/3nFsznjLKn](http://vdisk.weibo.com/s/3nFsznjLKn)\n\n**Stochastic Optimization Techniques**\n\n- intro: SGD/Momentum/NAG/Adagrad/RMSProp/Adadelta/Adam/ESGD/Adasecant/vSGD/Rprop\n- blog: [http://colinraffel.com/wiki/stochastic_optimization_techniques](http://colinraffel.com/wiki/stochastic_optimization_techniques)\n\n**Alec Radford's animations for optimization algorithms**\n\n[http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html)\n\n**Faster Asynchronous SGD (FASGD)**\n\n- arxiv: [http://arxiv.org/abs/1601.04033](http://arxiv.org/abs/1601.04033)\n- github: [https://github.com/DoctorTeeth/fred](https://github.com/DoctorTeeth/fred)\n\n**An overview of gradient descent optimization algorithms ()**\n\n![](http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif)\n\n- arxiv: [https://arxiv.org/abs/1609.04747](https://arxiv.org/abs/1609.04747)\n- blog: [http://sebastianruder.com/optimizing-gradient-descent/](http://sebastianruder.com/optimizing-gradient-descent/)\n\n**Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters**\n\n- arxiv: [http://arxiv.org/abs/1602.02151](http://arxiv.org/abs/1602.02151)\n\n**Writing fast asynchronous SGD/AdaGrad with RcppParallel**\n\n- blog: [http://gallery.rcpp.org/articles/rcpp-sgd/](http://gallery.rcpp.org/articles/rcpp-sgd/)\n\n**Quick Explanations Of Optimization Methods**\n\n- blog: [http://jxieeducation.com/2016-07-02/Quick-Explanations-of-Optimization-Methods/](http://jxieeducation.com/2016-07-02/Quick-Explanations-of-Optimization-Methods/)\n\n**Learning to learn by gradient descent by gradient descent**\n\n- intro: Google DeepMind\n- arxiv: [https://arxiv.org/abs/1606.04474](https://arxiv.org/abs/1606.04474)\n- github: [https://github.com/deepmind/learning-to-learn](https://github.com/deepmind/learning-to-learn)\n- github(TensorFlow): [https://github.com/runopti/Learning-To-Learn](https://github.com/runopti/Learning-To-Learn)\n- github(PyTorch): [https://github.com/ikostrikov/pytorch-meta-optimizer](https://github.com/ikostrikov/pytorch-meta-optimizer)\n\n**SGDR: Stochastic Gradient Descent with Restarts**\n\n- intro: ICLR 2017\n- keywords: cosine annealing strategy\n- arxiv: [http://arxiv.org/abs/1608.03983](http://arxiv.org/abs/1608.03983)\n- github: [https://github.com/loshchil/SGDR](https://github.com/loshchil/SGDR)\n\n**The zen of gradient descent**\n\n- blog: [http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html](http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html)\n\n**Big Batch SGD: Automated Inference using Adaptive Batch Sizes**\n\n- arxiv: [https://arxiv.org/abs/1610.05792](https://arxiv.org/abs/1610.05792)\n\n**Improving Stochastic Gradient Descent with Feedback**\n\n- arxiv: [https://arxiv.org/abs/1611.01505](https://arxiv.org/abs/1611.01505)\n- github: [https://github.com/jayanthkoushik/sgd-feedback](https://github.com/jayanthkoushik/sgd-feedback)\n- github: [https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Eve](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/Eve)\n\n**Learning Gradient Descent: Better Generalization and Longer Horizons**\n\n- intro: Tsinghua University\n- arxiv: [https://arxiv.org/abs/1703.03633](https://arxiv.org/abs/1703.03633)\n- github(TensorFlow): [https://github.com/vfleaking/rnnprop](https://github.com/vfleaking/rnnprop)\n\n**Optimization Algorithms**\n\n- blog: [https://3dbabove.com/2017/11/14/optimizationalgorithms/](https://3dbabove.com/2017/11/14/optimizationalgorithms/)\n- github: [https://github.com//ManuelGonzalezRivero/3dbabove](https://github.com//ManuelGonzalezRivero/3dbabove)\n- reddit: [https://www.reddit.com/r/MachineLearning/comments/7ehxky/d_optimization_algorithms_math_and_code/](https://www.reddit.com/r/MachineLearning/comments/7ehxky/d_optimization_algorithms_math_and_code/)\n\n**Gradient Normalization & Depth Based Decay For Deep Learning**\n\n- intro: Columbia University\n- arxiv: [https://arxiv.org/abs/1712.03607](https://arxiv.org/abs/1712.03607)\n\n**Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks**\n\n- intro: Google Research\n- arxiv: [https://arxiv.org/abs/1712.03298](https://arxiv.org/abs/1712.03298)\n\n**Optimization for Deep Learning Highlights in 2017**\n\n[http://ruder.io/deep-learning-optimization-2017/index.html](http://ruder.io/deep-learning-optimization-2017/index.html)\n\n**Gradients explode - Deep Networks are shallow - ResNet explained**\n\n- intro: CMU & UC Berkeley\n- arxiv: [https://arxiv.org/abs/1712.05577](https://arxiv.org/abs/1712.05577)\n\n**A Sufficient Condition for Convergences of Adam and RMSProp**\n\n[https://arxiv.org/abs/1811.09358](https://arxiv.org/abs/1811.09358)\n\n## Adam\n\n**Adam: A Method for Stochastic Optimization**\n\n- intro: ICLR 2015\n- arxiv: [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980)\n\n**Fixing Weight Decay Regularization in Adam**\n\n- intro: University of Freiburg\n- arxiv: [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101)\n- github: [https://github.com/loshchil/AdamW-and-SGDW](https://github.com/loshchil/AdamW-and-SGDW)\n- github: [https://github.com/fastai/fastai/pull/46/files](https://github.com/fastai/fastai/pull/46/files)\n\n**On the Convergence of Adam and Beyond**\n\n- intro: ICLR 2018 best paper award. CMU & IBM Research\n- paper: [https://openreview.net/pdf?id=ryQu7f-RZ](https://openreview.net/pdf?id=ryQu7f-RZ)\n- openreview: [https://openreview.net/forum?id=ryQu7f-RZ](https://openreview.net/forum?id=ryQu7f-RZ)\n\n# Tensor Methods\n\n**Tensorizing Neural Networks**\n\n- intro: TensorNet\n- arxiv: [http://arxiv.org/abs/1509.06569](http://arxiv.org/abs/1509.06569)\n- github(Matlab+Theano+Lasagne): [https://github.com/Bihaqo/TensorNet](https://github.com/Bihaqo/TensorNet)\n- github(TensorFlow): [https://github.com/timgaripov/TensorNet-TF](https://github.com/timgaripov/TensorNet-TF)\n\n**Tensor methods for training neural networks**\n\n- homepage: [http://newport.eecs.uci.edu/anandkumar/#home](http://newport.eecs.uci.edu/anandkumar/#home)\n- youtube: [https://www.youtube.com/watch?v=B4YvhcGaafw](https://www.youtube.com/watch?v=B4YvhcGaafw)\n- slides: [http://newport.eecs.uci.edu/anandkumar/slides/Strata-NY.pdf](http://newport.eecs.uci.edu/anandkumar/slides/Strata-NY.pdf)\n- talks: [http://newport.eecs.uci.edu/anandkumar/#talks](http://newport.eecs.uci.edu/anandkumar/#talks)\n\n# Regularization\n\n**DisturbLabel: Regularizing CNN on the Loss Layer**\n\n- intro:  University of California & MSR 2016\n- intro: \"an extremely simple algorithm which randomly replaces a part of labels as incorrect values in each iteration\"\n- paper: [http://research.microsoft.com/en-us/um/people/jingdw/pubs/cvpr16-disturblabel.pdf](http://research.microsoft.com/en-us/um/people/jingdw/pubs/cvpr16-disturblabel.pdf)\n\n**Robust Convolutional Neural Networks under Adversarial Noise**\n\n- intro:  ICLR 2016\n- arxiv: [http://arxiv.org/abs/1511.06306](http://arxiv.org/abs/1511.06306)\n\n**Adding Gradient Noise Improves Learning for Very Deep Networks**\n\n- intro:  ICLR 2016\n- arxiv: [http://arxiv.org/abs/1511.06807](http://arxiv.org/abs/1511.06807)\n\n**Stochastic Function Norm Regularization of Deep Networks**\n\n- arxiv: [http://arxiv.org/abs/1605.09085](http://arxiv.org/abs/1605.09085)\n- github: [https://github.com/AmalRT/DNN_Reg](https://github.com/AmalRT/DNN_Reg)\n\n**SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1609.06693](http://arxiv.org/abs/1609.06693)\n\n**Regularizing neural networks by penalizing confident predictions**\n\n- intro: Gabriel Pereyra, George Tucker, Lukasz Kaiser, Geoffrey Hinton [Google Brain\n- dropbox: [https://www.dropbox.com/s/8kqf4v2c9lbnvar/BayLearn%202016%20(gjt).pdf?dl=0](https://www.dropbox.com/s/8kqf4v2c9lbnvar/BayLearn%202016%20(gjt).pdf?dl=0)\n- mirror: [https://pan.baidu.com/s/1kUUtxdl](https://pan.baidu.com/s/1kUUtxdl)\n\n**Automatic Node Selection for Deep Neural Networks using Group Lasso Regularization**\n\n- arxiv: [https://arxiv.org/abs/1611.05527](https://arxiv.org/abs/1611.05527)\n\n**Regularization in deep learning**\n\n- blog: [https://medium.com/@cristina_scheau/regularization-in-deep-learning-f649a45d6e0#.py327hkuv](https://medium.com/@cristina_scheau/regularization-in-deep-learning-f649a45d6e0#.py327hkuv)\n- github: [https://github.com/cscheau/Examples/blob/master/iris_l1_l2.py](https://github.com/cscheau/Examples/blob/master/iris_l1_l2.py)\n\n**LDMNet: Low Dimensional Manifold Regularized Neural Networks**\n\n[https://arxiv.org/abs/1711.06246](https://arxiv.org/abs/1711.06246)\n\n**Learning Sparse Neural Networks through L0 Regularization**\n\n- intro: University of Amsterdam & OpenAI\n- arxiv: [https://arxiv.org/abs/1712.01312](https://arxiv.org/abs/1712.01312)\n\n**Regularization and Optimization strategies in Deep Convolutional Neural Network**\n\n[https://arxiv.org/abs/1712.04711](https://arxiv.org/abs/1712.04711)\n\n**Regularizing Deep Networks by Modeling and Predicting Label Structure**\n\n- intro: CVPR 2018\n- arxiv: [https://arxiv.org/abs/1804.02009](https://arxiv.org/abs/1804.02009)\n\n**Adversarial Noise Layer: Regularize Neural Network By Adding Noise**\n\n- intro: Peking University & University of Electronic Science and Technology of China & Australian National University\n- arxiv: [https://arxiv.org/abs/1805.08000](https://arxiv.org/abs/1805.08000)\n- github: [https://github.com/youzhonghui/ANL](https://github.com/youzhonghui/ANL)\n\n**Deep Bilevel Learning**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1809.01465](https://arxiv.org/abs/1809.01465)\n\n**Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.09102](https://arxiv.org/abs/1810.09102)\n\n**Gradient-Coherent Strong Regularization for Deep Neural Networks**\n\n[https://arxiv.org/abs/1811.08056](https://arxiv.org/abs/1811.08056)\n\n## Dropout\n\n**Improving neural networks by preventing co-adaptation of feature detectors**\n\n- intro: Dropout\n- arxiv: [http://arxiv.org/abs/1207.0580](http://arxiv.org/abs/1207.0580)\n\n**Dropout: A Simple Way to Prevent Neural Networks from Overfitting**\n\n- paper: [https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n\n**Fast dropout training**\n\n- paper: [http://jmlr.org/proceedings/papers/v28/wang13a.pdf](http://jmlr.org/proceedings/papers/v28/wang13a.pdf)\n- github: [https://github.com/sidaw/fastdropout](https://github.com/sidaw/fastdropout)\n\n**Dropout as data augmentation**\n\n- paper: [http://arxiv.org/abs/1506.08700](http://arxiv.org/abs/1506.08700)\n- notes: [https://www.evernote.com/shard/s189/sh/ef0c3302-21a4-40d7-b8b4-1c65b8ebb1c9/24ff553fcfb70a27d61ff003df75b5a9](https://www.evernote.com/shard/s189/sh/ef0c3302-21a4-40d7-b8b4-1c65b8ebb1c9/24ff553fcfb70a27d61ff003df75b5a9)\n\n**A Theoretically Grounded Application of Dropout in Recurrent Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1512.05287](http://arxiv.org/abs/1512.05287)\n- github: [https://github.com/yaringal/BayesianRNN](https://github.com/yaringal/BayesianRNN)\n\n**Improved Dropout for Shallow and Deep Learning**\n\n- arxiv: [http://arxiv.org/abs/1602.02220](http://arxiv.org/abs/1602.02220)\n\n**Dropout Regularization in Deep Learning Models With Keras**\n\n- blog: [http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/](http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)\n\n**Dropout with Expectation-linear Regularization**\n\n- arxiv: [http://arxiv.org/abs/1609.08017](http://arxiv.org/abs/1609.08017)\n\n**Dropout with Theano**\n\n- blog: [http://rishy.github.io/ml/2016/10/12/dropout-with-theano/](http://rishy.github.io/ml/2016/10/12/dropout-with-theano/)\n- ipn: [http://nbviewer.jupyter.org/github/rishy/rishy.github.io/blob/master/ipy_notebooks/Dropout-Theano.ipynb](http://nbviewer.jupyter.org/github/rishy/rishy.github.io/blob/master/ipy_notebooks/Dropout-Theano.ipynb)\n\n**Information Dropout: learning optimal representations through noise**\n\n- arxiv: [https://arxiv.org/abs/1611.01353](https://arxiv.org/abs/1611.01353)\n\n**Recent Developments in Dropout**\n\n- blog: [https://casmls.github.io/general/2016/11/11/dropout.html](https://casmls.github.io/general/2016/11/11/dropout.html)\n\n**Generalized Dropout**\n\n- arxiv: [https://arxiv.org/abs/1611.06791](https://arxiv.org/abs/1611.06791)\n\n**Analysis of Dropout**\n\n- blog: [https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/](https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/)\n\n**Variational Dropout Sparsifies Deep Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1701.05369](https://arxiv.org/abs/1701.05369)\n\n**Learning Deep Networks from Noisy Labels with Dropout Regularization**\n\n- intro: 2016 IEEE 16th International Conference on Data Mining\n- arxiv: [https://arxiv.org/abs/1705.03419](https://arxiv.org/abs/1705.03419)\n\n**Concrete Dropout**\n\n- intro: University of Cambridge\n- arxiv: [https://arxiv.org/abs/1705.07832](https://arxiv.org/abs/1705.07832)\n- github: [https://github.com/yaringal/ConcreteDropout](https://github.com/yaringal/ConcreteDropout)\n\n**Analysis of dropout learning regarded as ensemble learning**\n\n- intro: Nihon University\n- arxiv: [https://arxiv.org/abs/1706.06859](https://arxiv.org/abs/1706.06859)\n\n**An Analysis of Dropout for Matrix Factorization**\n\n[https://arxiv.org/abs/1710.03487](https://arxiv.org/abs/1710.03487)\n\n**Analysis of Dropout in Online Learning**\n\n[https://arxiv.org/abs/1711.03343](https://arxiv.org/abs/1711.03343)\n\n**Regularization of Deep Neural Networks with Spectral Dropout**\n\n[https://arxiv.org/abs/1711.08591](https://arxiv.org/abs/1711.08591)\n\n**Data Dropout in Arbitrary Basis for Deep Network Regularization**\n\n[https://arxiv.org/abs/1712.00891](https://arxiv.org/abs/1712.00891)\n\n**A New Angle on L2 Regularization**\n\n- intro: An explorable explanation on the phenomenon of adversarial examples in linear classification and its relation to L2 regularization\n- blog: [https://thomas-tanay.github.io/post--L2-regularization/](https://thomas-tanay.github.io/post--L2-regularization/)\n- arxiv: [https://arxiv.org/abs/1806.11186](https://arxiv.org/abs/1806.11186)\n\n**Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning**\n\n- intro: Rutgers University\n- arxiv: [https://arxiv.org/abs/1808.03578](https://arxiv.org/abs/1808.03578)\n- github: [https://github.com/noahfl/densenet-sdr/](https://github.com/noahfl/densenet-sdr/)\n\n**Data Dropout: Optimizing Training Data for Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1809.00193](https://arxiv.org/abs/1809.00193)\n\n**DropFilter: Dropout for Convolutions**\n\n[https://arxiv.org/abs/1810.09849](https://arxiv.org/abs/1810.09849)\n\n**DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1811.06783](https://arxiv.org/abs/1811.06783)\n\n**Targeted Dropout**\n\n- intro: Google Brain & FOR.ai & University of Oxford\n- paper: [https://openreview.net/pdf?id=HkghWScuoQ](https://openreview.net/pdf?id=HkghWScuoQ)\n- github: [https://github.com/for-ai/TD](https://github.com/for-ai/TD)\n\n## DropConnect\n\n**Regularization of Neural Networks using DropConnect**\n\n- homepage: [http://cs.nyu.edu/~wanli/dropc/](http://cs.nyu.edu/~wanli/dropc/)\n- gitxiv: [http://gitxiv.com/posts/rJucpiQiDhQ7HkZoX/regularization-of-neural-networks-using-dropconnect](http://gitxiv.com/posts/rJucpiQiDhQ7HkZoX/regularization-of-neural-networks-using-dropconnect)\n- github: [https://github.com/iassael/torch-dropconnect](https://github.com/iassael/torch-dropconnect)\n\n**Regularizing neural networks with dropout and with DropConnect**\n\n- blog: [http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/](http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/)\n\n## DropNeuron\n\n**DropNeuron: Simplifying the Structure of Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1606.07326](http://arxiv.org/abs/1606.07326)\n- github: [https://github.com/panweihit/DropNeuron](https://github.com/panweihit/DropNeuron)\n\n## DropBlock\n\n**DropBlock: A regularization method for convolutional networks**\n\n- intro: NIPS 2018\n- arxiv: [https://arxiv.org/abs/1810.12890](https://arxiv.org/abs/1810.12890)\n\n## Maxout\n\n**Maxout Networks**\n\n- intro: ICML 2013\n- intro: \"its output is the max of a set of inputs, a natural companion to dropout\"\n- project page: [http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html](http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html)\n- arxiv: [https://arxiv.org/abs/1302.4389](https://arxiv.org/abs/1302.4389)\n- github: [https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/models/maxout.py](https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/models/maxout.py)\n\n**Improving Deep Neural Networks with Probabilistic Maxout Units**\n\n- arxiv: [https://arxiv.org/abs/1312.6116](https://arxiv.org/abs/1312.6116)\n\n## Swapout\n\n**Swapout: Learning an ensemble of deep architectures**\n\n- arxiv: [https://arxiv.org/abs/1605.06465](https://arxiv.org/abs/1605.06465)\n- blog: [https://gab41.lab41.org/lab41-reading-group-swapout-learning-an-ensemble-of-deep-architectures-e67d2b822f8a#.9r2s4c58n](https://gab41.lab41.org/lab41-reading-group-swapout-learning-an-ensemble-of-deep-architectures-e67d2b822f8a#.9r2s4c58n)\n\n## Whiteout\n\n**Whiteout: Gaussian Adaptive Regularization Noise in Deep Neural Networks**\n\n- intro: University of Notre Dame & University of Science and Technology of China\n- arxiv: [https://arxiv.org/abs/1612.01490](https://arxiv.org/abs/1612.01490)\n\n**ShakeDrop regularization**\n\n[https://arxiv.org/abs/1802.02375](https://arxiv.org/abs/1802.02375)\n\n**Shakeout: A New Approach to Regularized Deep Neural Network Training**\n\n- intro: T-PAMI 2018\n- arxiv: [https://arxiv.org/abs/1904.06593](https://arxiv.org/abs/1904.06593)\n\n# Gradient Descent\n\n**RMSProp: Divide the gradient by a running average of its recent magnitude**\n\n![](/assets/train-dnn/rmsprop.jpg)\n\n- intro: it was not proposed in a paper, in fact it was just introduced in a slide in Geoffrey Hinton's Coursera class \n- slides: [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n\n**Fitting a model via closed-form equations vs. Gradient Descent vs Stochastic Gradient Descent vs Mini-Batch Learning. What is the difference?(Normal Equations vs. GD vs. SGD vs. MB-GD)**\n\n[http://sebastianraschka.com/faq/docs/closed-form-vs-gd.html](http://sebastianraschka.com/faq/docs/closed-form-vs-gd.html)\n\n**An Introduction to Gradient Descent in Python**\n\n- blog: [http://tillbergmann.com/blog/articles/python-gradient-descent.html](http://tillbergmann.com/blog/articles/python-gradient-descent.html)\n\n**Train faster, generalize better: Stability of stochastic gradient descent**\n\n- arxiv: [http://arxiv.org/abs/1509.01240](http://arxiv.org/abs/1509.01240)\n\n**A Variational Analysis of Stochastic Gradient Algorithms**\n\n- arxiv: [http://arxiv.org/abs/1602.02666](http://arxiv.org/abs/1602.02666)\n\n**The vanishing gradient problem: Oh noan obstacle to deep learning!**\n\n- blog: [https://medium.com/a-year-of-artificial-intelligence/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.50hu5vwa8](https://medium.com/a-year-of-artificial-intelligence/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.50hu5vwa8)\n\n**Gradient Descent For Machine Learning**\n\n- blog: [http://machinelearningmastery.com/gradient-descent-for-machine-learning/](http://machinelearningmastery.com/gradient-descent-for-machine-learning/)\n\n**Revisiting Distributed Synchronous SGD**\n\n- arxiv: [http://arxiv.org/abs/1604.00981](http://arxiv.org/abs/1604.00981)\n\n**Convergence rate of gradient descent**\n\n- blog: [https://building-babylon.net/2016/06/23/convergence-rate-of-gradient-descent/](https://building-babylon.net/2016/06/23/convergence-rate-of-gradient-descent/)\n\n**A Robust Adaptive Stochastic Gradient Method for Deep Learning**\n\n- intro: IJCNN 2017 Accepted Paper, An extension of paper, \"ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient\"\n- intro: Universite de Montreal & University of Oxford\n- arxiv: [https://arxiv.org/abs/1703.00788](https://arxiv.org/abs/1703.00788)\n\n**Accelerating Stochastic Gradient Descent**\n\n[https://arxiv.org/abs/1704.08227](https://arxiv.org/abs/1704.08227)\n\n**Gentle Introduction to the Adam Optimization Algorithm for Deep Learning**\n\n- blog: [http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/](http://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n\n**Understanding Generalization and Stochastic Gradient Descent**\n\n**A Bayesian Perspective on Generalization and Stochastic Gradient Descent**\n\n- intro: Google Brain\n- arxiv: [https://arxiv.org/abs/1710.06451](https://arxiv.org/abs/1710.06451)\n\n**Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent**\n\n- intro: UC Berkeley & Microsoft Research, India\n- arxiv: [https://arxiv.org/abs/1711.10456](https://arxiv.org/abs/1711.10456)\n\n**Improving Generalization Performance by Switching from Adam to SGD**\n\n[https://arxiv.org/abs/1712.07628](https://arxiv.org/abs/1712.07628)\n\n**Laplacian Smoothing Gradient Descent**\n\n- intro: UCLA\n- arxiv: [https://arxiv.org/abs/1806.06317](https://arxiv.org/abs/1806.06317)\n\n## AdaGrad\n\n**Adaptive Subgradient Methods for Online Learning and Stochastic Optimization**\n\n- paper: [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n\n**ADADELTA: An Adaptive Learning Rate Method**\n\n- arxiv: [http://arxiv.org/abs/1212.5701](http://arxiv.org/abs/1212.5701)\n\n## Momentum\n\n**On the importance of initialization and momentum in deep learning**\n\n- intro:  NAG: Nesterov\n- paper: [http://www.cs.toronto.edu/~fritz/absps/momentum.pdf](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n- paper: [http://jmlr.org/proceedings/papers/v28/sutskever13.pdf](http://jmlr.org/proceedings/papers/v28/sutskever13.pdf)\n\n**YellowFin and the Art of Momentum Tuning**\n\n- intro: Stanford University\n- intro: auto-tuning momentum SGD optimizer\n- project page: [http://cs.stanford.edu/~zjian/project/YellowFin/](http://cs.stanford.edu/~zjian/project/YellowFin/)\n- arxiv: [https://arxiv.org/abs/1706.03471](https://arxiv.org/abs/1706.03471)\n- github(TensorFlow): [https://github.com/JianGoForIt/YellowFin](https://github.com/JianGoForIt/YellowFin)\n[https://github.com/JianGoForIt/YellowFin_Pytorch](https://github.com/JianGoForIt/YellowFin_Pytorch)\n\n# Backpropagation\n\n**Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks**\n\n- intro: ECCV 2016. first place of ILSVRC 2015 Scene Classification Challenge\n- arxiv: [https://arxiv.org/abs/1512.05830](https://arxiv.org/abs/1512.05830)\n- paper: [http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2016-ECCV-RelayBP.pdf](http://www.cis.pku.edu.cn/faculty/vision/zlin/Publications/2016-ECCV-RelayBP.pdf)\n\n**Top-down Neural Attention by Excitation Backprop**\n\n![](http://cs-people.bu.edu/jmzhang/images/screen%20shot%202016-08-19%20at%2035847%20pm.jpg?crc=3911895888)\n\n- intro: ECCV, 2016 (oral)\n- projpage: [http://cs-people.bu.edu/jmzhang/excitationbp.html](http://cs-people.bu.edu/jmzhang/excitationbp.html)\n- arxiv: [http://arxiv.org/abs/1608.00507](http://arxiv.org/abs/1608.00507)\n- paper: [http://cs-people.bu.edu/jmzhang/EB/ExcitationBackprop.pdf](http://cs-people.bu.edu/jmzhang/EB/ExcitationBackprop.pdf)\n- github: [https://github.com/jimmie33/Caffe-ExcitationBP](https://github.com/jimmie33/Caffe-ExcitationBP)\n\n**Towards a Biologically Plausible Backprop**\n\n- arxiv: [http://arxiv.org/abs/1602.05179](http://arxiv.org/abs/1602.05179)\n- github: [https://github.com/bscellier/Towards-a-Biologically-Plausible-Backprop](https://github.com/bscellier/Towards-a-Biologically-Plausible-Backprop)\n\n**Sampled Backpropagation: Training Deep and Wide Neural Networks on Large Scale, User Generated Content Using Label Sampling**\n\n- blog: [https://medium.com/@karl1980.lab41/sampled-backpropagation-27ac58d5c51c#.xnbhyxtou](https://medium.com/@karl1980.lab41/sampled-backpropagation-27ac58d5c51c#.xnbhyxtou)\n\n**The Reversible Residual Network: Backpropagation Without Storing Activations**\n\n- intro: CoRR 2017. University of Toronto\n- arxiv: [https://arxiv.org/abs/1707.04585](https://arxiv.org/abs/1707.04585)\n- github: [https://github.com/renmengye/revnet-public](https://github.com/renmengye/revnet-public)\n\n**meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting**\n\n- intro: ICML 2017\n- arxiv: [https://arxiv.org/abs/1706.06197](https://arxiv.org/abs/1706.06197)\n- github: [https://github.com//jklj077/meProp](https://github.com//jklj077/meProp)\n\n# Accelerate Training\n\n**Neural Networks with Few Multiplications**\n\n- intro:  ICLR 2016\n- arxiv: [https://arxiv.org/abs/1510.03009](https://arxiv.org/abs/1510.03009)\n\n**Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices**\n\n- arxiv: [http://arxiv.org/abs/1603.07341](http://arxiv.org/abs/1603.07341)\n\n**Deep Q-Networks for Accelerating the Training of Deep Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1606.01467](http://arxiv.org/abs/1606.01467)\n- github: [https://github.com/bigaidream-projects/qan](https://github.com/bigaidream-projects/qan)\n\n**Omnivore: An Optimizer for Multi-device Deep Learning on CPUs and GPUs**\n\n- arxiv: [http://arxiv.org/abs/1606.04487](http://arxiv.org/abs/1606.04487)\n\n## Parallelism\n\n**One weird trick for parallelizing convolutional neural networks**\n\n- author: Alex Krizhevsky\n- arxiv: [http://arxiv.org/abs/1404.5997](http://arxiv.org/abs/1404.5997)\n\n**8-Bit Approximations for Parallelism in Deep Learning (ICLR 2016)**\n\n- arxiv: [http://arxiv.org/abs/1511.04561](http://arxiv.org/abs/1511.04561)\n\n# Handling Datasets\n\n## Data Augmentation\n\n**DataAugmentation ver1.0: Image data augmentation tool for training of image recognition algorithm**\n\n- github: [https://github.com/takmin/DataAugmentation](https://github.com/takmin/DataAugmentation)\n\n**Caffe-Data-Augmentation: a branc caffe with feature of Data Augmentation using a configurable stochastic combination of 7 data augmentation techniques**\n\n- github: [https://github.com/ShaharKatz/Caffe-Data-Augmentation](https://github.com/ShaharKatz/Caffe-Data-Augmentation)\n\n**Image Augmentation for Deep Learning With Keras**\n\n- blog: [http://machinelearningmastery.com/image-augmentation-deep-learning-keras/](http://machinelearningmastery.com/image-augmentation-deep-learning-keras/)\n\n**What you need to know about data augmentation for machine learning**\n\n- intro: keras Imagegenerator\n- blog: [https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/](https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/)\n\n**HZPROC: torch data augmentation toolbox (supports affine transform)**\n\n- github: [https://github.com/zhanghang1989/hzproc](https://github.com/zhanghang1989/hzproc)\n\n**AGA: Attribute Guided Augmentation**\n\n- intro: one-shot recognition\n- arxiv: [https://arxiv.org/abs/1612.02559](https://arxiv.org/abs/1612.02559)\n\n**Accelerating Deep Learning with Multiprocess Image Augmentation in Keras**\n\n- blog: [http://blog.stratospark.com/multiprocess-image-augmentation-keras.html](http://blog.stratospark.com/multiprocess-image-augmentation-keras.html)\n- github: [https://github.com/stratospark/keras-multiprocess-image-data-generator](https://github.com/stratospark/keras-multiprocess-image-data-generator)\n\n**Comprehensive Data Augmentation and Sampling for Pytorch**\n\n- github: [https://github.com/ncullen93/torchsample](https://github.com/ncullen93/torchsample)\n\n**Image augmentation for machine learning experiments.**\n\n[https://github.com/aleju/imgaug](https://github.com/aleju/imgaug)\n\n**Google/inception's data augmentation: scale and aspect ratio augmentation**\n\n[https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L130](https://github.com/facebook/fb.resnet.torch/blob/master/datasets/transforms.lua#L130)\n\n**Caffe Augmentation Extension**\n\n- intro: Data Augmentation for Caffe\n- github: [https://github.com/twtygqyy/caffe-augmentation](https://github.com/twtygqyy/caffe-augmentation)\n\n**Improving Deep Learning using Generic Data Augmentation**\n\n- intro: University of Cape Town\n- arxiv: [https://arxiv.org/abs/1708.06020](https://arxiv.org/abs/1708.06020)\n- github: [https://github.com/webstorms/AugmentedDatasets](https://github.com/webstorms/AugmentedDatasets)\n\n**Augmentor: An Image Augmentation Library for Machine Learning**\n\n- arxiv: [https://arxiv.org/abs/1708.04680](https://arxiv.org/abs/1708.04680)\n- github: [https://github.com/mdbloice/Augmentor](https://github.com/mdbloice/Augmentor)\n\n**Automatic Dataset Augmentation**\n\n- project page: [https://auto-da.github.io/](https://auto-da.github.io/)\n- arxiv: [https://arxiv.org/abs/1708.08201](https://arxiv.org/abs/1708.08201)\n\n**Learning to Compose Domain-Specific Transformations for Data Augmentation**\n\n[https://arxiv.org/abs/1709.01643](https://arxiv.org/abs/1709.01643)\n\n**Data Augmentation in Classification using GAN**\n\n[https://arxiv.org/abs/1711.00648](https://arxiv.org/abs/1711.00648)\n\n**Data Augmentation Generative Adversarial Networks**\n\n[https://arxiv.org/abs/1711.04340](https://arxiv.org/abs/1711.04340)\n\n**Random Erasing Data Augmentation**\n\n- arxiv: [https://arxiv.org/abs/1708.04896](https://arxiv.org/abs/1708.04896)\n- github: [https://github.com/zhunzhong07/Random-Erasing](https://github.com/zhunzhong07/Random-Erasing)\n\n**Context Augmentation for Convolutional Neural Networks**\n\n[https://arxiv.org/abs/1712.01653](https://arxiv.org/abs/1712.01653)\n\n**The Effectiveness of Data Augmentation in Image Classification using Deep Learning**\n\n[https://arxiv.org/abs/1712.04621](https://arxiv.org/abs/1712.04621)\n\n**MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels**\n\n- intro: Google Inc & Stanford University\n- arxiv: [https://arxiv.org/abs/1712.05055](https://arxiv.org/abs/1712.05055)\n\n**mixup: Beyond Empirical Risk Minimization**\n\n- intro: MIT & FAIR\n- arxiv: [https://arxiv.org/abs/1710.09412](https://arxiv.org/abs/1710.09412)\n- github: [https://github.com//leehomyc/mixup_pytorch](https://github.com//leehomyc/mixup_pytorch)\n- github: [https://github.com//unsky/mixup](https://github.com//unsky/mixup)\n\n**mixup: Data-Dependent Data Augmentation**\n\n[http://www.inference.vc/mixup-data-dependent-data-augmentation/](http://www.inference.vc/mixup-data-dependent-data-augmentation/)\n\n**Data Augmentation by Pairing Samples for Images Classification**\n\n- intro: IBM Research - Tokyo\n- arxiv: [https://arxiv.org/abs/1801.02929](https://arxiv.org/abs/1801.02929)\n\n**Feature Space Transfer for Data Augmentation**\n\n- keywords: eATure TransfEr Network (FATTEN)\n- arxiv: [https://arxiv.org/abs/1801.04356](https://arxiv.org/abs/1801.04356)\n\n**Visual Data Augmentation through Learning**\n\n[https://arxiv.org/abs/1801.06665](https://arxiv.org/abs/1801.06665)\n\n**Data Augmentation Generative Adversarial Networks**\n\n- arxiv: [https://arxiv.org/abs/1711.04340](https://arxiv.org/abs/1711.04340)\n- github: [https://github.com/AntreasAntoniou/DAGAN](https://github.com/AntreasAntoniou/DAGAN)\n\n**BAGAN: Data Augmentation with Balancing GAN**\n\n[https://arxiv.org/abs/1803.09655](https://arxiv.org/abs/1803.09655)\n\n**Parallel Grid Pooling for Data Augmentation**\n\n- intro: The University of Tokyo & NTT Communications Science Laboratories\n- arxiv: [https://arxiv.org/abs/1803.11370](https://arxiv.org/abs/1803.11370)\n- github(Chainer): [https://github.com/akitotakeki/pgp-chainer](https://github.com/akitotakeki/pgp-chainer)\n\n**AutoAugment: Learning Augmentation Policies from Data**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1805.09501](https://arxiv.org/abs/1805.09501)\n- github: [https://github.com/DeepVoltaire/AutoAugment](https://github.com/DeepVoltaire/AutoAugment)\n\n**Improved Mixed-Example Data Augmentation**\n\n[https://arxiv.org/abs/1805.11272](https://arxiv.org/abs/1805.11272)\n\n**Data augmentation instead of explicit regularization**\n\n[https://arxiv.org/abs/1806.03852](https://arxiv.org/abs/1806.03852)\n\n**Data Augmentation using Random Image Cropping and Patching for Deep CNNs**\n\n- intro: An extended version of a proceeding of ACML2018\n- keywords: random image cropping and patching (RICAP)\n- arxiv: [https://arxiv.org/abs/1811.09030](https://arxiv.org/abs/1811.09030)\n\n**GANsfer Learning: Combining labelled and unlabelled data for GAN based data augmentat**\n\n[https://arxiv.org/abs/1811.10669](https://arxiv.org/abs/1811.10669)\n\n**Adversarial Learning of General Transformations for Data Augmentation**\n\n- intro: Ecole de Technologie Sup  erieure & Element AI\n- arxiv: [https://arxiv.org/abs/1909.09801](https://arxiv.org/abs/1909.09801)\n\n**Implicit Semantic Data Augmentation for Deep Networks**\n\n- intro: NeurIPS 2019\n- arxiv: [https://arxiv.org/abs/1909.12220](https://arxiv.org/abs/1909.12220)\n- github(official): [https://github.com/blackfeather-wang/ISDA-for-Deep-Networks](https://github.com/blackfeather-wang/ISDA-for-Deep-Networks)\n\n**Data Augmentation Revisited: Rethinking the Distribution Gap between Clean and Augmented Data**\n\n[https://arxiv.org/abs/1909.09148](https://arxiv.org/abs/1909.09148)\n\n**AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty**\n\n- intro: ICLR 2020\n- intro: Google & Deepmind\n- arxiv: [https://arxiv.org/abs/1912.02781](https://arxiv.org/abs/1912.02781)\n- github: [https://github.com/google-research/augmix](https://github.com/google-research/augmix)\n\n**GridMask Data Augmentation**\n\n[https://arxiv.org/abs/2001.04086](https://arxiv.org/abs/2001.04086)\n\n**On Feature Normalization and Data Augmentation**\n\n- intro: Cornell University & Cornell Tech & ASAPP Inc. & Facebook AI\n- keywords: MoEx (Moment Exchange)\n- arxiv: [https://arxiv.org/abs/2002.11102](https://arxiv.org/abs/2002.11102)\n- github: [https://github.com/Boyiliee/MoEx](https://github.com/Boyiliee/MoEx)\n\n**DADA: Differentiable Automatic Data Augmentation**\n\n[https://arxiv.org/abs/2003.03780](https://arxiv.org/abs/2003.03780)\n\n**Negative Data Augmentation**\n\n- intro: ICLR 2021\n- intro: Stanford University & Samsung Research America\n- arxiv: [https://arxiv.org/abs/2102.05113](https://arxiv.org/abs/2102.05113)\n\n## Imbalanced Datasets\n\n**Investigation on handling Structured & Imbalanced Datasets with Deep Learning**\n\n- intro: smote resampling, cost sensitive learning\n- blog: [https://www.analyticsvidhya.com/blog/2016/10/investigation-on-handling-structured-imbalanced-datasets-with-deep-learning/](https://www.analyticsvidhya.com/blog/2016/10/investigation-on-handling-structured-imbalanced-datasets-with-deep-learning/)\n\n**A systematic study of the class imbalance problem in convolutional neural networks**\n\n- intro: Duke University & Royal Institute of Technology (KTH)\n- arxiv: [https://arxiv.org/abs/1710.05381](https://arxiv.org/abs/1710.05381)\n\n**Class Rectification Hard Mining for Imbalanced Deep Learning**\n\n[https://arxiv.org/abs/1712.03162](https://arxiv.org/abs/1712.03162)\n\n**Bridging the Gap: Simultaneous Fine Tuning for Data Re-Balancing**\n\n- arxiv: [https://arxiv.org/abs/1801.02548](https://arxiv.org/abs/1801.02548)\n- github: [https://github.com/JohnMcKay/dataImbalance](https://github.com/JohnMcKay/dataImbalance)\n\n**Imbalanced Deep Learning by Minority Class Incremental Rectification**\n\n- intro: TPAMI\n- arxiv: [https://arxiv.org/abs/1804.10851](https://arxiv.org/abs/1804.10851)\n\n**Pseudo-Feature Generation for Imbalanced Data Analysis in Deep Learning**\n\n- intro: National Institute of Information and Communications Technology, Tokyo Japan\n- arxiv: [https://arxiv.org/abs/1807.06538](https://arxiv.org/abs/1807.06538)\n- slides: [https://www.slideshare.net/TomohikoKonno/pseudofeature-generation-for-imbalanced-data-analysis-in-deep-learning-tomohiko-105318569](https://www.slideshare.net/TomohikoKonno/pseudofeature-generation-for-imbalanced-data-analysis-in-deep-learning-tomohiko-105318569)\n\n**Max-margin Class Imbalanced Learning with Gaussian Affinity**\n\n[https://arxiv.org/abs/1901.07711](https://arxiv.org/abs/1901.07711)\n\n**Dynamic Curriculum Learning for Imbalanced Data Classification**\n\n- intro: ICCV 2019\n- intro: SenseTime\n- arxiv: [https://arxiv.org/abs/1901.06783](https://arxiv.org/abs/1901.06783)\n\n**Class Rectification Hard Mining for Imbalanced Deep Learning**\n\n- intro: ICCV 2017\n- paper: [https://www.eecs.qmul.ac.uk/~sgg/papers/DongEtAl_ICCV2017.pdf](https://www.eecs.qmul.ac.uk/~sgg/papers/DongEtAl_ICCV2017.pdf)\n\n## Noisy / Unlabelled Data\n\n**Data Distillation: Towards Omni-Supervised Learning**\n\n- intro: Facebook AI Research (FAIR)\n- arxiv: [https://arxiv.org/abs/1712.04440](https://arxiv.org/abs/1712.04440)\n\n**Learning From Noisy Singly-labeled Data**\n\n- intro: University of Illinois Urbana Champaign & CMU & Caltech & Amazon AI\n- arxiv: [https://arxiv.org/abs/1712.04577](https://arxiv.org/abs/1712.04577)\n\n# Low Numerical Precision\n\n**Training deep neural networks with low precision multiplications**\n\n- intro: ICLR 2015\n- intro: Maxout networks, 10-bit activations, 12-bit parameter updates\n- arxiv: [http://arxiv.org/abs/1412.7024](http://arxiv.org/abs/1412.7024)\n- github: [https://github.com/MatthieuCourbariaux/deep-learning-multipliers](https://github.com/MatthieuCourbariaux/deep-learning-multipliers)\n\n**Deep Learning with Limited Numerical Precision**\n\n- intro: ICML 2015\n- arxiv: [http://arxiv.org/abs/1502.02551](http://arxiv.org/abs/1502.02551)\n\n**BinaryConnect: Training Deep Neural Networks with binary weights during propagations**\n\n- paper: [http://papers.nips.cc/paper/5647-shape-and-illumination-from-shading-using-the-generic-viewpoint-assumption](http://papers.nips.cc/paper/5647-shape-and-illumination-from-shading-using-the-generic-viewpoint-assumption)\n- github: [https://github.com/MatthieuCourbariaux/BinaryConnect](https://github.com/MatthieuCourbariaux/BinaryConnect)\n\n**Binarized Neural Networks**\n\n- arxiv: [http://arxiv.org/abs/1602.02505](http://arxiv.org/abs/1602.02505)\n\n**BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**\n\n**Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**\n\n- arxiv: [http://arxiv.org/abs/1602.02830](http://arxiv.org/abs/1602.02830)\n- github: [https://github.com/MatthieuCourbariaux/BinaryNet](https://github.com/MatthieuCourbariaux/BinaryNet)\n- github: [https://github.com/codekansas/tinier-nn](https://github.com/codekansas/tinier-nn)\n\n**Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations**\n\n- arxiv: [http://arxiv.org/abs/1609.07061](http://arxiv.org/abs/1609.07061)\n\n# Distributed Training\n\n**Large Scale Distributed Systems for Training Neural Networks**\n\n- intro: By Jeff Dean & Oriol Vinyals, Google. NIPS 2015.\n- slides: [https://media.nips.cc/Conferences/2015/tutorialslides/Jeff-Oriol-NIPS-Tutorial-2015.pdf](https://media.nips.cc/Conferences/2015/tutorialslides/Jeff-Oriol-NIPS-Tutorial-2015.pdf)\n- video: [http://research.microsoft.com/apps/video/default.aspx?id=259564&l=i](http://research.microsoft.com/apps/video/default.aspx?id=259564&l=i)\n- mirror: [http://pan.baidu.com/s/1mgXV0hU](http://pan.baidu.com/s/1mgXV0hU)\n\n**Large Scale Distributed Deep Networks**\n\n- intro: distributed CPU training, data parallelism, model parallelism\n- paper: [http://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf](http://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf)\n- slides: [http://admis.fudan.edu.cn/~yfhuang/files/LSDDN_slide.pdf](http://admis.fudan.edu.cn/~yfhuang/files/LSDDN_slide.pdf)\n\n**Implementation of a Practical Distributed Calculation System with Browsers and JavaScript, and Application to Distributed Deep Learning**\n\n- project page: [http://mil-tokyo.github.io/](http://mil-tokyo.github.io/)\n- arxiv: [https://arxiv.org/abs/1503.05743](https://arxiv.org/abs/1503.05743)\n\n**SparkNet: Training Deep Networks in Spark**\n\n- arxiv: [http://arxiv.org/abs/1511.06051](http://arxiv.org/abs/1511.06051)\n- github: [https://github.com/amplab/SparkNet](https://github.com/amplab/SparkNet)\n- blog: [http://www.kdnuggets.com/2015/12/spark-deep-learning-training-with-sparknet.html](http://www.kdnuggets.com/2015/12/spark-deep-learning-training-with-sparknet.html)\n\n**A Scalable Implementation of Deep Learning on Spark**\n\n- intro: Alexander Ulanov\n- slides: [http://www.slideshare.net/AlexanderUlanov1/a-scalable-implementation-of-deep-learning-on-spark-alexander-ulanov](http://www.slideshare.net/AlexanderUlanov1/a-scalable-implementation-of-deep-learning-on-spark-alexander-ulanov)\n- mirror: [http://pan.baidu.com/s/1jHiNW5C](http://pan.baidu.com/s/1jHiNW5C)\n\n**TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems**\n\n- arxiv: [http://arxiv.org/abs/1603.04467](http://arxiv.org/abs/1603.04467)\n- gitxiv: [http://gitxiv.com/posts/57kjddp3AWt4y5K4h/tensorflow-large-scale-machine-learning-on-heterogeneous](http://gitxiv.com/posts/57kjddp3AWt4y5K4h/tensorflow-large-scale-machine-learning-on-heterogeneous)\n\n**Distributed Supervised Learning using Neural Networks**\n\n- intro: Ph.D. thesis\n- arxiv: [http://arxiv.org/abs/1607.06364](http://arxiv.org/abs/1607.06364)\n\n**Distributed Training of Deep Neuronal Networks: Theoretical and Practical Limits of Parallel Scalability**\n\n- arxiv: [http://arxiv.org/abs/1609.06870](http://arxiv.org/abs/1609.06870)\n\n**How to scale distributed deep learning?**\n\n- intro: Extended version of paper accepted at ML Sys 2016 (at NIPS 2016)\n- arxiv: [https://arxiv.org/abs/1611.04581](https://arxiv.org/abs/1611.04581)\n\n**Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training**\n\n- intro: Tsinghua University & Stanford University\n- comments: we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy\n- keywords: momentum correction, local gradient clipping, momentum factor masking, and warm-up training\n- arxiv: [https://arxiv.org/abs/1712.01887](https://arxiv.org/abs/1712.01887)\n\n**Distributed learning of CNNs on heterogeneous CPU/GPU architectures**\n\n[https://arxiv.org/abs/1712.02546](https://arxiv.org/abs/1712.02546)\n\n**Integrated Model and Data Parallelism in Training Neural Networks**\n\n- intro: UC Berkeley & Lawrence Berkeley National Laboratory\n- arxiv: [https://arxiv.org/abs/1712.04432](https://arxiv.org/abs/1712.04432)\n\n**Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training**\n\n- intro: ICLR 2018\n- intro: we find 99.9% of the gradient exchange in distributed SGD is redundant; we reduce the communication bandwidth by two orders of magnitude without losing accuracy\n- arxiv: [https://arxiv.org/abs/1712.01887](https://arxiv.org/abs/1712.01887)\n\n**RedSync : Reducing Synchronization Traffic for Distributed Deep Learning**\n\n[https://arxiv.org/abs/1808.04357](https://arxiv.org/abs/1808.04357)\n\n## Projects\n\n**Theano-MPI: a Theano-based Distributed Training Framework**\n\n- arxiv: [https://arxiv.org/abs/1605.08325](https://arxiv.org/abs/1605.08325)\n- github: [https://github.com/uoguelph-mlrg/Theano-MPI](https://github.com/uoguelph-mlrg/Theano-MPI)\n\n**CaffeOnSpark: Open Sourced for Distributed Deep Learning on Big Data Clusters**\n\n- intro: Yahoo Big ML Team\n- blog: [http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep](http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep)\n- github: [https://github.com/yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark)\n- youtube: [https://www.youtube.com/watch?v=bqj7nML-aHk](https://www.youtube.com/watch?v=bqj7nML-aHk)\n\n**Tunnel: Data Driven Framework for Distributed Computing in Torch 7**\n\n- github: [https://github.com/zhangxiangxiao/tunnel](https://github.com/zhangxiangxiao/tunnel)\n\n**Distributed deep learning with Keras and Apache Spark**\n\n- project page: [http://joerihermans.com/work/distributed-keras/](http://joerihermans.com/work/distributed-keras/)\n- github: [https://github.com/JoeriHermans/dist-keras](https://github.com/JoeriHermans/dist-keras)\n\n**BigDL: Distributed Deep learning Library for Apache Spark**\n\n- github: [https://github.com/intel-analytics/BigDL](https://github.com/intel-analytics/BigDL)\n\n## Videos\n\n**A Scalable Implementation of Deep Learning on Spark**\n\n- youtube: [https://www.youtube.com/watch?v=pNYBBhuK8yU](https://www.youtube.com/watch?v=pNYBBhuK8yU)\n- mirror: [http://pan.baidu.com/s/1mhzF1uK](http://pan.baidu.com/s/1mhzF1uK)\n\n**Distributed TensorFlow on Spark: Scaling Google's Deep Learning Library (Spark Summit)**\n\n- youtube: [https://www.youtube.com/watch?v=-QtcP3yRqyM](https://www.youtube.com/watch?v=-QtcP3yRqyM)\n- mirror: [http://pan.baidu.com/s/1mgOR1GG](http://pan.baidu.com/s/1mgOR1GG)\n\n**Deep Recurrent Neural Networks for Sequence Learning in Spark (Spark Summit)**\n\n- youtube: [https://www.youtube.com/watch?v=mUuqLcl8Jog](https://www.youtube.com/watch?v=mUuqLcl8Jog)\n- mirror: [http://pan.baidu.com/s/1sklHTPr](http://pan.baidu.com/s/1sklHTPr)\n\n**Distributed deep learning on Spark**\n\n- author: Alexander Ulanov July 12, 2016\n- intro: Alexander Ulanov offers an overview of tools and frameworks that have been proposed for performing deep learning on Spark.\n- video: [https://www.oreilly.com/learning/distributed-deep-learning-on-spark](https://www.oreilly.com/learning/distributed-deep-learning-on-spark)\n\n## Blogs\n\n**Distributed Deep Learning Reads**\n\n[https://github.com//tmulc18/DistributedDeepLearningReads](https://github.com//tmulc18/DistributedDeepLearningReads)\n\n**Hadoop, Spark, Deep Learning Mesh on Single GPU Cluster**\n\n[http://www.nextplatform.com/2016/02/24/hadoop-spark-deep-learning-mesh-on-single-gpu-cluster/](http://www.nextplatform.com/2016/02/24/hadoop-spark-deep-learning-mesh-on-single-gpu-cluster/)\n\n**The Unreasonable Effectiveness of Deep Learning on Spark**\n\n[https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html](https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html)\n\n**Distributed Deep Learning with Caffe Using a MapR Cluster**\n\n![](https://www.mapr.com/sites/default/files/spark-driver.jpg)\n\n[https://www.mapr.com/blog/distributed-deep-learning-caffe-using-mapr-cluster](https://www.mapr.com/blog/distributed-deep-learning-caffe-using-mapr-cluster)\n\n**Deep Learning with Apache Spark and TensorFlow**\n\n[https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)\n\n**Deeplearning4j on Spark**\n\n[http://deeplearning4j.org/spark](http://deeplearning4j.org/spark)\n\n**Distributed Deep Learning, Part 1: An Introduction to Distributed Training of Neural Networks**\n\n- blog: [http://engineering.skymind.io/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks](http://engineering.skymind.io/distributed-deep-learning-part-1-an-introduction-to-distributed-training-of-neural-networks)\n\n**GPU Acceleration in Databricks: Speeding Up Deep Learning on Apache Spark**\n\n[https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html](https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html)\n\n**Distributed Deep Learning with Apache Spark and Keras**\n\n[https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras](https://db-blog.web.cern.ch/blog/joeri-hermans/2017-01-distributed-deep-learning-apache-spark-and-keras)\n\n# Adversarial Training\n\n**Learning from Simulated and Unsupervised Images through Adversarial Training**\n\n- intro: CVPR 2017 oral, best paper award. Apple Inc.\n- arxiv: [https://arxiv.org/abs/1612.07828](https://arxiv.org/abs/1612.07828)\n\n**The Robust Manifold Defense: Adversarial Training using Generative Models**\n\n[https://arxiv.org/abs/1712.09196](https://arxiv.org/abs/1712.09196)\n\n**DeepDefense: Training Deep Neural Networks with Improved Robustness**\n\n[https://arxiv.org/abs/1803.00404](https://arxiv.org/abs/1803.00404)\n\n**Gradient Adversarial Training of Neural Networks**\n\n- intro: Magic Leap\n- arxiv: [https://arxiv.org/abs/1806.08028](https://arxiv.org/abs/1806.08028)\n\n**Gray-box Adversarial Training**\n\n- intro: ECCV 2018\n- arxiv: [https://arxiv.org/abs/1808.01753](https://arxiv.org/abs/1808.01753)\n\n**Universal Adversarial Training**\n\n[https://arxiv.org/abs/1811.11304](https://arxiv.org/abs/1811.11304)\n\n**MEAL: Multi-Model Ensemble via Adversarial Learning**\n\n- intro: AAAI 2019\n- intro: Fudan University & University of Illinois at Urbana-Champaign\n- arxiv: [https://arxiv.org/abs/1812.02425](https://arxiv.org/abs/1812.02425)\n- github(official): [https://github.com/AaronHeee/MEAL](https://github.com/AaronHeee/MEAL)\n\n**Regularized Ensembles and Transferability in Adversarial Learning**\n\n[https://arxiv.org/abs/1812.01821](https://arxiv.org/abs/1812.01821)\n\n**Feature denoising for improving adversarial robustness**\n\n- intro: Johns Hopkins University & Facebook AI Research\n- intro: ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018\n- arxiv: [https://arxiv.org/abs/1812.03411](https://arxiv.org/abs/1812.03411)\n- github: [https://github.com/facebookresearch/ImageNet-Adversarial-Training](https://github.com/facebookresearch/ImageNet-Adversarial-Training)\n\n**Second Rethinking of Network Pruning in the Adversarial Setting**\n\n[https://arxiv.org/abs/1903.12561](https://arxiv.org/abs/1903.12561)\n\n**Interpreting Adversarially Trained Convolutional Neural Networks**\n\n- intro: ICML 2019\n- arxiv: [https://arxiv.org/abs/1905.09797](https://arxiv.org/abs/1905.09797)\n\n**On Stabilizing Generative Adversarial Training with Noise**\n\n- intro: CVPR 2019\n- arxiv: [https://arxiv.org/abs/1906.04612](https://arxiv.org/abs/1906.04612)\n\n**Adversarial Learning with Margin-based Triplet Embedding Regularization**\n\n- intro: ICCV 2019\n- intro: BUPT\n- arxiv: [https://arxiv.org/abs/1909.09481](https://arxiv.org/abs/1909.09481)\n- github: [https://github.com/zhongyy/Adversarial_MTER](https://github.com/zhongyy/Adversarial_MTER)\n\n**Bag of Tricks for Adversarial Training**\n\n- intro: Tsinghua University\n- arxiv: [https://arxiv.org/abs/2010.00467](https://arxiv.org/abs/2010.00467)\n\n# Low-Precision Training\n\n**Mixed Precision Training**\n\n- intro: ICLR 2018\n- arxiv: [https://arxiv.org/abs/1710.03740](https://arxiv.org/abs/1710.03740)\n\n**High-Accuracy Low-Precision Training**\n\n- intro: Cornell University & Stanford University\n- arxiv: [https://arxiv.org/abs/1803.03383](https://arxiv.org/abs/1803.03383)\n\n# Incremental Training\n\n**ClickBAIT: Click-based Accelerated Incremental Training of Convolutional Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1709.05021](https://arxiv.org/abs/1709.05021)\n- dataset: [http://clickbait.crossmobile.info/](http://clickbait.crossmobile.info/)\n\n**ClickBAIT-v2: Training an Object Detector in Real-Time**\n\n[https://arxiv.org/abs/1803.10358](https://arxiv.org/abs/1803.10358)\n\n**Class-incremental Learning via Deep Model Consolidation**\n\n- intro: University of Southern California & Arizona State University & Samsung Research America\n- arxiv: [https://arxiv.org/abs/1903.07864](https://arxiv.org/abs/1903.07864)\n\n# Papers\n\n**Understanding the difficulty of training deep feed forward neural networks**\n\n- intro: Xavier initialization\n- paper: [http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n\n**Domain-Adversarial Training of Neural Networks**\n\n- arxiv: [https://arxiv.org/abs/1505.07818](https://arxiv.org/abs/1505.07818)\n- paper: [http://jmlr.org/papers/v17/15-239.html](http://jmlr.org/papers/v17/15-239.html)\n- github: [https://github.com/pumpikano/tf-dann](https://github.com/pumpikano/tf-dann)\n\n**Scalable and Sustainable Deep Learning via Randomized Hashing**\n\n- arxiv: [http://arxiv.org/abs/1602.08194](http://arxiv.org/abs/1602.08194)\n\n**Training Deep Nets with Sublinear Memory Cost**\n\n- arxiv: [https://arxiv.org/abs/1604.06174](https://arxiv.org/abs/1604.06174)\n- github: [https://github.com/dmlc/mxnet-memonger](https://github.com/dmlc/mxnet-memonger)\n- github: [https://github.com/Bihaqo/tf-memonger](https://github.com/Bihaqo/tf-memonger)\n\n**Improving the Robustness of Deep Neural Networks via Stability Training**\n\n- arxiv: [http://arxiv.org/abs/1604.04326](http://arxiv.org/abs/1604.04326)\n\n**Faster Training of Very Deep Networks Via p-Norm Gates**\n\n- arxiv: [http://arxiv.org/abs/1608.03639](http://arxiv.org/abs/1608.03639)\n\n**Fast Training of Convolutional Neural Networks via Kernel Rescaling**\n\n- arxiv: [https://arxiv.org/abs/1610.03623](https://arxiv.org/abs/1610.03623)\n\n**FreezeOut: Accelerate Training by Progressively Freezing Layers**\n\n- arxiv: [https://arxiv.org/abs/1706.04983](https://arxiv.org/abs/1706.04983)\n- github: [https://github.com/ajbrock/FreezeOut](https://github.com/ajbrock/FreezeOut)\n\n**Normalized Gradient with Adaptive Stepsize Method for Deep Neural Network Training**\n\n- intro: CMU & The University of Iowa\n- arxiv: [https://arxiv.org/abs/1707.04822](https://arxiv.org/abs/1707.04822)\n\n**Image Quality Assessment Guided Deep Neural Networks Training**\n\n[https://arxiv.org/abs/1708.03880](https://arxiv.org/abs/1708.03880)\n\n**An Effective Training Method For Deep Convolutional Neural Network**\n\n- intro: Beijing Institute of Technology & Tsinghua University\n- arxiv: [https://arxiv.org/abs/1708.01666](https://arxiv.org/abs/1708.01666)\n\n**On the Importance of Consistency in Training Deep Neural Networks**\n\n- intro: University of Maryland & Arizona State University\n- arxiv: [https://arxiv.org/abs/1708.00631](https://arxiv.org/abs/1708.00631)\n\n**Solving internal covariate shift in deep learning with linked neurons**\n\n- intro: Universitat de Barcelona\n- arxiv: [https://arxiv.org/abs/1712.02609](https://arxiv.org/abs/1712.02609)\n- github: [https://github.com/blauigris/linked_neurons](https://github.com/blauigris/linked_neurons)\n\n# Tools\n\n**pastalog: Simple, realtime visualization of neural network training performance**\n\n![](/assets/train-dnn/pastalog-main-big.gif)\n\n- github: [https://github.com/rewonc/pastalog](https://github.com/rewonc/pastalog)\n\n**torch-pastalog: A Torch interface for pastalog - simple, realtime visualization of neural network training performance**\n\n- github: [https://github.com/Kaixhin/torch-pastalog](https://github.com/Kaixhin/torch-pastalog)\n\n# Blogs\n\n**Important nuances to train deep learning models**\n\n[http://www.erogol.com/important-nuances-train-deep-learning-models/](http://www.erogol.com/important-nuances-train-deep-learning-models/)\n\n**Train your deep model faster and sharpertwo novel techniques**\n\n[https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047](https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047)\n","excerpt":"Tutorials Popular Training Approaches of DNNsA Quick Overview https://medium.com/@asjad/popular-training-approaches-of-dnns-a-quick-over","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[]}},"pageContext":{"tags":[],"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","sidebarItems":[{"title":"Categories","items":[{"title":"Commercial","url":"","items":[{"title":"Commercial Structure","url":"/Commercial/Commercial Structure/","items":[]},{"title":"Community of Practice","url":"/Commercial/Community of Practice/","items":[]},{"title":"Domains","url":"/Commercial/Domains/","items":[]},{"title":"Webizen Alliance","url":"/Commercial/Webizen Alliance/","items":[]}]},{"title":"Core Services","url":"","items":[{"title":"Decentralised Ontologies","url":"/Core Services/Decentralised Ontologies/","items":[]},{"title":"Permissive Commons","url":"/Core Services/Permissive Commons/","items":[]},{"title":"Safety Protocols","url":"","items":[{"title":"Safety Protocols","url":"/Core Services/Safety Protocols/Safety Protocols/","items":[]},{"title":"Social Factors","url":"","items":[{"title":"Best Efforts","url":"/Core Services/Safety Protocols/Social Factors/Best Efforts/","items":[]},{"title":"Ending Digital Slavery","url":"/Core Services/Safety Protocols/Social Factors/Ending Digital Slavery/","items":[]},{"title":"Freedom of Thought","url":"/Core Services/Safety Protocols/Social Factors/Freedom of Thought/","items":[]},{"title":"No Golden Handcuffs","url":"/Core Services/Safety Protocols/Social Factors/No Golden Handcuffs/","items":[]},{"title":"Relationships (Social)","url":"/Core Services/Safety Protocols/Social Factors/Relationships (Social)/","items":[]},{"title":"Social Attack Vectors","url":"/Core Services/Safety Protocols/Social Factors/Social Attack Vectors/","items":[]},{"title":"The Webizen Charter","url":"/Core Services/Safety Protocols/Social Factors/The Webizen Charter/","items":[]}]},{"title":"Values Credentials","url":"/Core Services/Safety Protocols/Values Credentials/","items":[]}]},{"title":"Temporal Semantics","url":"/Core Services/Temporal Semantics/","items":[]},{"title":"Verifiable Claims & Credentials","url":"/Core Services/Verifiable Claims & Credentials/","items":[]},{"title":"Webizen Socio-Economics","url":"","items":[{"title":"Biosphere Ontologies","url":"/Core Services/Webizen Socio-Economics/Biosphere Ontologies/","items":[]},{"title":"Centricity","url":"/Core Services/Webizen Socio-Economics/Centricity/","items":[]},{"title":"Currencies","url":"/Core Services/Webizen Socio-Economics/Currencies/","items":[]},{"title":"SocioSphere Ontologies","url":"/Core Services/Webizen Socio-Economics/SocioSphere Ontologies/","items":[]},{"title":"Sustainable Development Goals (ESG)","url":"/Core Services/Webizen Socio-Economics/Sustainable Development Goals (ESG)/","items":[]}]}]},{"title":"Core Technologies","url":"","items":[{"title":"AUTH","url":"","items":[{"title":"Authentication Fabric","url":"/Core Technologies/AUTH/Authentication Fabric/","items":[]}]},{"title":"Webizen App Spec","url":"","items":[{"title":"SemWebSpecs","url":"","items":[{"title":"Core Ontologies","url":"","items":[{"title":"FOAF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/FOAF/","items":[]},{"title":"General Ontology Information","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/General Ontology Information/","items":[]},{"title":"Human Rights Ontologies","url":"","items":[{"title":"UDHR","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Human Rights Ontologies/UDHR/","items":[]}]},{"title":"MD-RDF Ontologies","url":"","items":[{"title":"DataTypesOntology (DTO) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/DataTypes Ontology/","items":[]},{"title":"Friend of a Friend (FOAF) Core","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/MD-RDF Ontologies/FOAF/","items":[]}]},{"title":"OWL","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/OWL/","items":[]},{"title":"RDF Schema 1.1","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/RDFS/","items":[]},{"title":"Sitemap","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/Sitemap/","items":[]},{"title":"SKOS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SKOS/","items":[]},{"title":"SOIC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Core Ontologies/SOIC/","items":[]}]},{"title":"Semantic Web - An Introduction","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Semantic Web - An Introduction/","items":[]},{"title":"SemWeb-AUTH","url":"","items":[{"title":"WebID-OIDC","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-OIDC/","items":[]},{"title":"WebID-RSA","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-RSA/","items":[]},{"title":"WebID-TLS","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/SemWeb-AUTH/WebID-TLS/","items":[]}]},{"title":"Sparql","url":"","items":[{"title":"Sparql Family","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/Sparql/Sparql Family/","items":[]}]},{"title":"W3C Specifications","url":"","items":[{"title":"Linked Data Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Fragments/","items":[]},{"title":"Linked Data Notifications","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Notifications/","items":[]},{"title":"Linked Data Platform","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Data Platform/","items":[]},{"title":"Linked Media Fragments","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Linked Media Fragments/","items":[]},{"title":"RDF","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/RDF/","items":[]},{"title":"Web Access Control (WAC)","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Access Control (WAC)/","items":[]},{"title":"Web Of Things","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/Web Of Things/","items":[]},{"title":"WebID","url":"/Core Technologies/Webizen App Spec/SemWebSpecs/W3C Specifications/WebID/","items":[]}]}]},{"title":"Webizen App Spec 1.0","url":"/Core Technologies/Webizen App Spec/Webizen App Spec 1.0/","items":[]},{"title":"WebSpec","url":"","items":[{"title":"HTML SPECS","url":"/Core Technologies/Webizen App Spec/WebSpec/HTML SPECS/","items":[]},{"title":"Query Interfaces","url":"","items":[{"title":"GraphQL","url":"/Core Technologies/Webizen App Spec/WebSpec/Query Interfaces/GraphQL/","items":[]}]},{"title":"WebPlatformTools","url":"","items":[{"title":"WebAuthn","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebAuthn/","items":[]},{"title":"WebDav","url":"/Core Technologies/Webizen App Spec/WebSpec/WebPlatformTools/WebDav/","items":[]}]}]}]}]},{"title":"Database Requirements","url":"","items":[{"title":"Database Alternatives","url":"","items":[{"title":"Akutan","url":"/Database requirements/Database Alternatives/akutan/","items":[]},{"title":"CayleyGraph","url":"/Database requirements/Database Alternatives/CayleyGraph/","items":[]}]},{"title":"Database Methods","url":"","items":[{"title":"GraphQL","url":"/Database requirements/Database methods/GraphQL/","items":[]},{"title":"Sparql","url":"/Database requirements/Database methods/Sparql/","items":[]}]}]},{"title":"Host Service Requirements","url":"","items":[{"title":"Domain Hosting","url":"/Host Service Requirements/Domain Hosting/","items":[]},{"title":"Email Services","url":"/Host Service Requirements/Email Services/","items":[]},{"title":"LD_PostOffice_SemanticMGR","url":"/Host Service Requirements/LD_PostOffice_SemanticMGR/","items":[]},{"title":"Media Processing","url":"/Host Service Requirements/Media Processing/","items":[{"title":"Ffmpeg","url":"/Host Service Requirements/Media Processing/ffmpeg/","items":[]},{"title":"Opencv","url":"/Host Service Requirements/Media Processing/opencv/","items":[]}]},{"title":"Website Host","url":"/Host Service Requirements/Website Host/","items":[]}]},{"title":"ICT Stack","url":"","items":[{"title":"General References","url":"","items":[{"title":"List of Protocols ISO Model","url":"/ICT Stack/General References/List of Protocols ISO model/","items":[]}]},{"title":"Internet","url":"","items":[{"title":"Internet Stack","url":"/ICT Stack/Internet/Internet Stack/","items":[]}]}]},{"title":"Implementation V1","url":"","items":[{"title":"App-Design-Sdk-V1","url":"","items":[{"title":"Core Apps","url":"","items":[{"title":"Agent Directory","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Agent Directory/","items":[]},{"title":"Credentials & Contracts Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Credentials & Contracts Manager/","items":[]},{"title":"File (Package) Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/File (package) Manager/","items":[]},{"title":"Temporal Apps","url":"","items":[{"title":"Calendar","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Calendar/","items":[]},{"title":"Timeline Interface","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Temporal Apps/Timeline Interface/","items":[]}]},{"title":"Webizen Apps (V1)","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Apps (v1)/","items":[]},{"title":"Webizen Manager","url":"/Implementation V1/App-design-sdk-v1/Core Apps/Webizen Manager/","items":[]}]},{"title":"Data Applications","url":"/Implementation V1/App-design-sdk-v1/Data Applications/","items":[]},{"title":"Design Goals","url":"","items":[{"title":"Design Goals Overview","url":"/Implementation V1/App-design-sdk-v1/Design Goals/Design Goals Overview/","items":[]}]}]},{"title":"Edge","url":"","items":[{"title":"Webizen Local App Functionality","url":"/Implementation V1/edge/Webizen Local App Functionality/","items":[]}]},{"title":"GoLang Libraries","url":"/Implementation V1/GoLang Libraries/","items":[]},{"title":"Implementation V1 Summary","url":"/Implementation V1/Implementation V1 Summary/","items":[]},{"title":"Vps","url":"","items":[{"title":"Server Functionality Summary (VPS)","url":"/Implementation V1/vps/Server Functionality Summary (VPS)/","items":[]}]},{"title":"Webizen 1.0","url":"/Implementation V1/Webizen 1.0/","items":[]},{"title":"Webizen-Connect","url":"","items":[{"title":"Social Media APIs","url":"/Implementation V1/Webizen-Connect/Social Media APIs/","items":[]},{"title":"Webizen-Connect (Summary)","url":"/Implementation V1/Webizen-Connect/Webizen-Connect (summary)/","items":[]}]}]},{"title":"Non-HTTP(s) Protocols","url":"","items":[{"title":"DAT","url":"/Non-HTTP(s) Protocols/DAT/","items":[]},{"title":"GIT","url":"/Non-HTTP(s) Protocols/GIT/","items":[]},{"title":"GUNECO","url":"/Non-HTTP(s) Protocols/GUNECO/","items":[]},{"title":"IPFS","url":"/Non-HTTP(s) Protocols/IPFS/","items":[]},{"title":"Lightning Network","url":"/Non-HTTP(s) Protocols/Lightning Network/","items":[]},{"title":"Non-HTTP(s) Protocols (& DLTs)","url":"/Non-HTTP(s) Protocols/Non-HTTP(s) Protocols (& DLTs)/","items":[]},{"title":"WebRTC","url":"/Non-HTTP(s) Protocols/WebRTC/","items":[]},{"title":"WebSockets","url":"/Non-HTTP(s) Protocols/WebSockets/","items":[]},{"title":"WebTorrent","url":"/Non-HTTP(s) Protocols/WebTorrent/","items":[]}]},{"title":"Old-Work-Archives","url":"","items":[{"title":"2018-Webizen-Net-Au","url":"","items":[{"title":"_Link_library_links","url":"","items":[{"title":"Link Library","url":"/old-work-archives/2018-webizen-net-au/_link_library_links/2018-09-23-wp-linked-data/","items":[]}]},{"title":"_Posts","url":"","items":[{"title":"About W3C","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-27-about-w3c/","items":[]},{"title":"Advanced Functions &#8211; Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-advanced-functions-facebook-pages/","items":[]},{"title":"Advanced Search &#038; Discovery Tips","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-advanced-search-discovery-tips/","items":[]},{"title":"An introduction to Virtual Machines.","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-an-introduction-to-virtual-machines/","items":[]},{"title":"Basic Media Analysis &#8211; Part 1 (Audio)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-30-media-analysis-part-1-audio/","items":[]},{"title":"Basic Media Analysis &#8211; Part 2 (visual)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-31-media-analysis-part-2-visual/","items":[]},{"title":"Basic Media Analysis &#8211; Part 3 (Text &#038; Metadata)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-01-01-basic-media-analysis-part-3-text-metadata/","items":[]},{"title":"Building an Economy based upon Knowledge Equity.","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-building-an-economy-based-upon-knowledge-equity/","items":[]},{"title":"Choice of Law","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-26-choice-of-law/","items":[]},{"title":"Contemplation of the ITU Dubai Meeting and the Future of the Internet","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-19-contemplation-of-the-itu-dubai-meeting-and-the-future-of-the-internet/","items":[]},{"title":"Creating a Presence &#8211; Online","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-creating-a-presence-online/","items":[]},{"title":"Credentials and Payments by Manu Sporny","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-credentials-and-payments-by-manu-sporny/","items":[]},{"title":"Data Recovery &#038; Collection: Mobile Devices","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-mobile-devices-data-recovery-collection/","items":[]},{"title":"Data Recovery: Laptop &#038; Computers","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-12-28-data-recovery-laptop-computers/","items":[]},{"title":"Decentralized Web Conference 2016","url":"/old-work-archives/2018-webizen-net-au/_posts/2016-06-09-decentralized-web-2016/","items":[]},{"title":"Decentralized Web Summit 2018","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-decentralized-web-summit-2018/","items":[]},{"title":"Does Anonymity exist?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-does-anonymity-exist/","items":[]},{"title":"Downloading My Data from Social Networks","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-downloading-my-data-from-social-networks/","items":[]},{"title":"Events","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-events/","items":[]},{"title":"Facebook Pages","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-12-16-facebook-pages/","items":[]},{"title":"Google Tracking Data (geolocation)","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-google-tracking/","items":[]},{"title":"Human Consciousness","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-human-consciousness/","items":[]},{"title":"Image Recgonition Video Playlist","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-image-recgonition-video-playlist/","items":[]},{"title":"Inferencing (introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-inferencing-introduction/","items":[]},{"title":"Introduction to AI","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-ai/","items":[]},{"title":"Introduction to Linked Data","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-introduction-to-linked-data/","items":[]},{"title":"Introduction to Maltego","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-introduction-to-maltego/","items":[]},{"title":"Introduction to Ontologies","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-ontologies-intro/","items":[]},{"title":"Introduction to Semantic Web","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-introduction-to-semantic-web/","items":[]},{"title":"Knowledge Capital","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-10-17-knowledge-capital/","items":[]},{"title":"Logo&#8217;s, Style Guides and Artwork","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-logos-style-guides-and-artwork/","items":[]},{"title":"MindMapping &#8211; Setting-up a business &#8211; Identity","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-mindmapping-setting-up-a-business-identity/","items":[]},{"title":"Openlink Virtuoso","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-openlink-virtuoso/","items":[]},{"title":"OpenRefine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-21-74-2/","items":[]},{"title":"Projects, Customers and Invoicing &#8211; Web-Services for Startups","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-projects-customers-and-invoicing-web-services-for-startups/","items":[]},{"title":"RWW &#038; some Solid history","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-rww-some-solid-history/","items":[]},{"title":"Semantic Web (An Intro)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-semantic-web-an-intro/","items":[]},{"title":"Setting-up Twitter","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-06-07-setting-up-twitter/","items":[]},{"title":"Social Encryption: An Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-25-social-encryption-an-introduction/","items":[]},{"title":"Stock Content","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-28-stock-content/","items":[]},{"title":"The WayBack Machine","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-27-the-wayback-machine/","items":[]},{"title":"Tim Berners Lee &#8211; Turing Lecture","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-05-29-tim-berners-lee-turing-lecture/","items":[]},{"title":"Tools of Trade","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-tools-of-trade/","items":[]},{"title":"Trust Factory 2017","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-trust-factory-2017/","items":[]},{"title":"Verifiable Claims (An Introduction)","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-vc-intro/","items":[]},{"title":"Web of Things &#8211; an Introduction","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-21-web-of-things-an-introduction/","items":[]},{"title":"Web-Persistence","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-web-persistence/","items":[]},{"title":"Web-Services &#8211; Marketing Tools","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-web-services-marketing-tools/","items":[]},{"title":"Website Templates","url":"/old-work-archives/2018-webizen-net-au/_posts/2012-11-19-templates/","items":[]},{"title":"What is Linked Data?","url":"/old-work-archives/2018-webizen-net-au/_posts/2018-09-23-what-is-linked-data/","items":[]},{"title":"What is Open Source Intelligence?","url":"/old-work-archives/2018-webizen-net-au/_posts/2017-07-23-what-is-osint/","items":[]},{"title":"WiX","url":"/old-work-archives/2018-webizen-net-au/_posts/2013-01-01-wix/","items":[]}]},{"title":"about","url":"/old-work-archives/2018-webizen-net-au/about/","items":[{"title":"About The Author","url":"/old-work-archives/2018-webizen-net-au/about/about-the-author/","items":[]},{"title":"Applied Theory: Applications for a Human Centric Web","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/","items":[{"title":"Digital Receipts","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/digital-receipts/","items":[]},{"title":"Fake News: Considerations  Principles  The Institution of Socio &#8211; Economic Values","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","items":[]},{"title":"Fake-News-Considerations-%E2%86%92-Principles-%E2%86%92-the-Institution-of-Socio-Economic-Values","url":"","items":[{"title":"Solutions to FakeNews: Linked-Data, Ontologies and Verifiable Claims","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations-%e2%86%92-principles-%e2%86%92-the-institution-of-socio-economic-values/solutions-to-fakenews-linked-data-ontologies-and-verifiable-claims/","items":[]}]},{"title":"Healthy Living Economy","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/healthy-living-economy/","items":[]},{"title":"HyperMedia Solutions &#8211; Adapting HbbTV V2","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/","items":[{"title":"HYPERMEDIA PACKAGES","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/hypermedia-packages/","items":[]},{"title":"USER STORIES: INTERACTIVE VIEWING EXPERIENCE","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/hypermedia-solutions-adapting-hbbtv-v2/user-stories-interactive-viewing-experience/","items":[]}]},{"title":"Measurements App","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/measurements-app/","items":[]},{"title":"Re:Animation","url":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/reanimation/","items":[]}]},{"title":"Executive Summary","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/","items":[{"title":"Assisting those who Enforce the Law","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/assisting-those-who-enforce-the-law/","items":[]},{"title":"Consumer Protections","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/consumer-protections/","items":[]},{"title":"Knowledge Banking: Legal Structures","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-banking-legal-structures/","items":[]},{"title":"Knowledge Economics &#8211; Services","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/knowledge-economics-services/","items":[]},{"title":"Preserving The Freedom to Think","url":"/old-work-archives/2018-webizen-net-au/about/executive-summary/preserving-the-freedom-to-think/","items":[]}]},{"title":"History","url":"/old-work-archives/2018-webizen-net-au/about/history/","items":[{"title":"History: Global Governance and ICT.","url":"/old-work-archives/2018-webizen-net-au/about/history/history-global-governance-ict-1/","items":[]}]},{"title":"Knowledge Banking: A Technical Architecture Summary","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/","items":[{"title":"An introduction to Credentials.","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/","items":[{"title":"credentials and custodianship","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/credentials-and-custodianship/","items":[]},{"title":"DIDs and MultiSig","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/what-are-credentials/dids-and-multisig/","items":[]}]},{"title":"Personal Augmentation of AI","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/personal-augmentation-of-ai/","items":[]},{"title":"Semantic Inferencing","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/semantic-inferencing/","items":[]},{"title":"Web of Things (IoT+LD)","url":"/old-work-archives/2018-webizen-net-au/about/knowledge-banking-a-technical-architecture-summary/web-of-things-iotld/","items":[]}]},{"title":"References","url":"/old-work-archives/2018-webizen-net-au/about/references/","items":[{"title":"Making the distinction between privacy and dignity.","url":"/old-work-archives/2018-webizen-net-au/about/references/privacy-vs-dignity/","items":[]},{"title":"Roles &#8211; Entity Analysis","url":"/old-work-archives/2018-webizen-net-au/about/references/roles-entity-analysis/","items":[]},{"title":"Social Informatics Design Considerations","url":"/old-work-archives/2018-webizen-net-au/about/references/social-informatics-design-concept-and-principles/","items":[]},{"title":"Socio-economic relations | A conceptual model","url":"/old-work-archives/2018-webizen-net-au/about/references/socioeconomic-relations-p1/","items":[]},{"title":"The need for decentralised Open (Linked) Data","url":"/old-work-archives/2018-webizen-net-au/about/references/the-need-for-decentralised-open-linked-data/","items":[]}]},{"title":"The design of new medium","url":"/old-work-archives/2018-webizen-net-au/about/the-design-of-new-medium/","items":[]},{"title":"The need to modernise socioeconomic infrastructure","url":"/old-work-archives/2018-webizen-net-au/about/the-modernisation-of-socioeconomics/","items":[]},{"title":"The Vision","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/","items":[{"title":"Domesticating Pervasive Surveillance","url":"/old-work-archives/2018-webizen-net-au/about/the-vision/a-technical-vision/","items":[]}]}]},{"title":"An Overview","url":"/old-work-archives/2018-webizen-net-au/the-human-centric-infosphere/","items":[]},{"title":"Embed Link","url":"/old-work-archives/2018-webizen-net-au/embed-link/","items":[]},{"title":"Posts","url":"/old-work-archives/2018-webizen-net-au/posts/","items":[]},{"title":"Privacy Policy","url":"/old-work-archives/2018-webizen-net-au/privacy-policy/","items":[]},{"title":"Resource Library","url":"/old-work-archives/2018-webizen-net-au/resource-library/","items":[{"title":"Handong1587","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","items":[{"title":"_Posts","url":"","items":[{"title":"Computer_science","url":"","items":[{"title":"Algorithm and Data Structure Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-algo-resourses/","items":[]},{"title":"Artificial Intelligence Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-ai-resources/","items":[]},{"title":"Big Data Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-22-big-data-resources/","items":[]},{"title":"Computer Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-cs-resources/","items":[]},{"title":"Data Mining Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-mining-resources/","items":[]},{"title":"Data Science Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-09-data-science-resources/","items":[]},{"title":"Database Systems Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-database-resources/","items":[]},{"title":"Discrete Optimization Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-10-01-discrete-optimization/","items":[]},{"title":"Distribued System Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-12-12-ditributed-system-resources/","items":[]},{"title":"Funny Stuffs Of Computer Science","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-18-funny-stuffs-of-cs/","items":[]},{"title":"Robotics","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-09-26-robotics-resources/","items":[]},{"title":"Writting CS Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_science/2015-11-30-writing-papers/","items":[]}]},{"title":"Computer_vision","url":"","items":[{"title":"Computer Vision Datasets","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-24-datasets/","items":[]},{"title":"Computer Vision Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-09-12-cv-resources/","items":[]},{"title":"Features","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-features/","items":[]},{"title":"Recognition, Detection, Segmentation and Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-10-09-recognition-detection-segmentation-tracking/","items":[]},{"title":"Use FFmpeg to Capture I Frames of Video","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2016-03-03-ffmpeg-i-frame/","items":[]},{"title":"Working on OpenCV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/computer_vision/2015-12-25-working-on-opencv/","items":[]}]},{"title":"Deep_learning","url":"","items":[{"title":"3D","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2021-07-28-3d/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-cnn-compression-acceleration/","items":[]},{"title":"Acceleration and Model Compression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-knowledge-distillation/","items":[]},{"title":"Adversarial Attacks and Defences","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-adversarial-attacks-and-defences/","items":[]},{"title":"Audio / Image / Video Generation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-audio-image-video-generation/","items":[]},{"title":"BEV","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2022-06-27-bev/","items":[]},{"title":"Classification / Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recognition/","items":[]},{"title":"Deep Learning and Autonomous Driving","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-autonomous-driving/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-pose-estimation/","items":[]},{"title":"Deep Learning Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-applications/","items":[]},{"title":"Deep learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-courses/","items":[]},{"title":"Deep Learning Frameworks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-frameworks/","items":[]},{"title":"Deep Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-resources/","items":[]},{"title":"Deep Learning Software and Hardware","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-software-hardware/","items":[]},{"title":"Deep Learning Tricks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tricks/","items":[]},{"title":"Deep Learning Tutorials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-tutorials/","items":[]},{"title":"Deep Learning with Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-dl-with-ml/","items":[]},{"title":"Face Recognition","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-face-recognition/","items":[]},{"title":"Fun With Deep Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-fun-with-deep-learning/","items":[]},{"title":"Generative Adversarial Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gan/","items":[]},{"title":"Graph Convolutional Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-gcn/","items":[]},{"title":"Image / Video Captioning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-captioning/","items":[]},{"title":"Image Retrieval","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-retrieval/","items":[]},{"title":"Keep Up With New Trends","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2018-09-03-keep-up-with-new-trends/","items":[]},{"title":"LiDAR 3D Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-lidar-3d-detection/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nlp/","items":[]},{"title":"Neural Architecture Search","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-nas/","items":[]},{"title":"Object Counting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-counting/","items":[]},{"title":"Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-object-detection/","items":[]},{"title":"OCR","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-ocr/","items":[]},{"title":"Optical Flow","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-optical-flow/","items":[]},{"title":"Re-ID","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-re-id/","items":[]},{"title":"Recommendation System","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-recommendation-system/","items":[]},{"title":"Reinforcement Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rl/","items":[]},{"title":"RNN and LSTM","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-rnn-and-lstm/","items":[]},{"title":"Segmentation","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-segmentation/","items":[]},{"title":"Style Transfer","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-style-transfer/","items":[]},{"title":"Super-Resolution","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-super-resolution/","items":[]},{"title":"Tracking","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-tracking/","items":[]},{"title":"Training Deep Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-training-dnn/","items":[]},{"title":"Transfer Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-transfer-learning/","items":[]},{"title":"Unsupervised Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-unsupervised-learning/","items":[]},{"title":"Video Applications","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-video-applications/","items":[]},{"title":"Visual Question Answering","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-vqa/","items":[]},{"title":"Visualizing and Interpreting Convolutional Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/deep_learning/2015-10-09-visulizing-interpreting-cnn/","items":[]}]},{"title":"Leisure","url":"","items":[{"title":"All About Enya","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-all-about-enya/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-coldplay/","items":[]},{"title":"Coldplay","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-nightwish/","items":[]},{"title":"Games","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-13-games/","items":[]},{"title":"Green Day","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-greenday/","items":[]},{"title":"Muse! Muse!","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-muse-muse/","items":[]},{"title":"Oasis","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-oasis/","items":[]},{"title":"Paintings By J.M.","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2016-03-08-paintings-by-jm/","items":[]},{"title":"Papers, Blogs and Websites","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-09-27-papers-blogs-and-websites/","items":[]},{"title":"Welcome To The Black Parade","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/leisure/2015-12-21-welcome-to-the-black-parade/","items":[]}]},{"title":"Machine_learning","url":"","items":[{"title":"Bayesian Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-bayesian-methods/","items":[]},{"title":"Clustering Algorithms Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-clustering/","items":[]},{"title":"Competitions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-competitions/","items":[]},{"title":"Dimensionality Reduction Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-dimensionality-reduction/","items":[]},{"title":"Fun With Machine Learning","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-fun-with-ml/","items":[]},{"title":"Graphical Models Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-graphical-models/","items":[]},{"title":"Machine Learning Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-courses/","items":[]},{"title":"Machine Learning Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-ml-resources/","items":[]},{"title":"Natural Language Processing","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-nlp/","items":[]},{"title":"Neural Network","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-neural-network/","items":[]},{"title":"Random Field","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-field/","items":[]},{"title":"Random Forests","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-random-forests/","items":[]},{"title":"Regression","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-regression/","items":[]},{"title":"Support Vector Machine","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-svm/","items":[]},{"title":"Topic Model","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/machine_learning/2015-08-27-topic-model/","items":[]}]},{"title":"Mathematics","url":"","items":[{"title":"Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/mathematics/2016-02-24-resources/","items":[]}]},{"title":"Programming_study","url":"","items":[{"title":"Add Lunr Search Plugin For Blog","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-31-add-lunr-search-plugin-for-blog/","items":[]},{"title":"Android Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-23-android-resources/","items":[]},{"title":"C++ Programming Solutions","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-09-07-cpp-programming-solutions/","items":[]},{"title":"Commands To Suppress Some Building Errors With Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-24-cmds-to-suppress-some-vs-building-Errors/","items":[]},{"title":"Embedding Python In C/C++","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-10-embedding-python-in-cpp/","items":[]},{"title":"Enable Large Addresses On VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-12-14-enable-large-addresses/","items":[]},{"title":"Fix min/max Error In VS2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-02-17-min-max-error-in-vs2015/","items":[]},{"title":"Gflags Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-gflags-build-problems-winx86-vs2015/","items":[]},{"title":"Glog Build Problems on Windows X86 and Visual Studio 2015","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-23-glog-build-problems-winx86/","items":[]},{"title":"Horrible Wired Errors Come From Simple Stupid Mistake","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-10-16-horrible-wired-errors-come-from-simple-stupid-mistake/","items":[]},{"title":"Install Jekyll To Fix Some Local Github-pages Defects","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-11-21-install-jekyll/","items":[]},{"title":"Install Therubyracer Failure","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-03-install-therubyracer/","items":[]},{"title":"Notes On Valgrind and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-05-30-notes-on-valgrind/","items":[]},{"title":"PHP Hello World","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-07-04-php-hello-world/","items":[]},{"title":"Programming Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2015-07-01-programming-resources/","items":[]},{"title":"PyInstsaller and Others","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-12-24-pyinstaller-and-others/","items":[]},{"title":"Web Development Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-06-21-web-dev-resources/","items":[]},{"title":"Working on Visual Studio","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/programming_study/2016-04-03-working-on-vs/","items":[]}]},{"title":"Reading_and_thoughts","url":"","items":[{"title":"Book Reading List","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-book-reading-list/","items":[]},{"title":"Funny Papers","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2015-12-04-funny-papers/","items":[]},{"title":"Reading Materials","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/reading_and_thoughts/2016-01-18-reading-materials/","items":[]}]},{"title":"Study","url":"","items":[{"title":"Courses","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2017-11-28-courses/","items":[]},{"title":"Essay Writting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-01-11-essay-writting/","items":[]},{"title":"Job Hunting","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2016-06-02-job-hunting/","items":[]},{"title":"Study Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/study/2018-04-18-resources/","items":[]}]},{"title":"Working_on_linux","url":"","items":[{"title":"Create Multiple Forks of a GitHub Repo","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-12-18-create-multi-forks/","items":[]},{"title":"Linux Git Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-02-linux-git/","items":[]},{"title":"Linux Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-24-linux-resources/","items":[]},{"title":"Linux SVN Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-08-03-linux-svn/","items":[]},{"title":"Setup vsftpd on Ubuntu 14.10","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-27-setup-vsftpd/","items":[]},{"title":"Useful Linux Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2015-07-25-useful-linux-commands/","items":[]},{"title":"vsftpd Commands","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_linux/2016-07-28-vsftpd-cmd/","items":[]}]},{"title":"Working_on_mac","url":"","items":[{"title":"Mac Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_mac/2015-07-25-mac-resources/","items":[]}]},{"title":"Working_on_windows","url":"","items":[{"title":"FFmpeg Collection of Utility Methods","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2016-06-05-ffmpeg-utilities/","items":[]},{"title":"Windows Commands and Utilities","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-windows-cmds-utils/","items":[]},{"title":"Windows Dev Resources","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/_posts/working_on_windows/2015-10-27-resources/","items":[]}]}]},{"title":"Drafts","url":"","items":[{"title":"2016-12-30-Setup-Opengrok","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-30-setup-opengrok/","items":[]},{"title":"2017-01-20-Packing-C++-Project-to-Single-Executable","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-01-20-packing-c++-project-to-single-executable/","items":[]},{"title":"Notes On Caffe Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","items":[]},{"title":"Notes On Deep Learning Training","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","items":[]},{"title":"Notes On Discrete Optimization","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-discrete-optimization/","items":[]},{"title":"Notes On Gecode","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-13-notes-on-gecode/","items":[]},{"title":"Notes On Inside-Outside Net","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-28-notes-on-ion/","items":[]},{"title":"Notes On K-Means","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-06-notes-on-kmeans/","items":[]},{"title":"Notes On L-BFGS","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","items":[]},{"title":"Notes On Object Detection","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","items":[]},{"title":"Notes On Perceptrons","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","items":[]},{"title":"Notes On Quantized Convolutional Neural Networks","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-01-07-notes-on-quantized-cnn/","items":[]},{"title":"Notes On Stanford CS2321n","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-02-21-notes-on-cs231n/","items":[]},{"title":"Notes on Suffix Array and Manacher Algorithm","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","items":[]},{"title":"Notes On Tensorflow Development","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2017-04-13-notes-on-tensorflow-dev/","items":[]},{"title":"Notes On YOLO","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","items":[]},{"title":"PASCAL VOC (20) / COCO (80) / ImageNet (200) Detection Categories","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-12-23-imagenet-det-cat/","items":[]},{"title":"Softmax Vs Logistic Vs Sigmoid","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","items":[]},{"title":"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognititon","url":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2016-08-31-model-ensemble-of-deteciton/","items":[]}]}]}]}]}]},{"title":"Webizen 2.0","url":"","items":[{"title":"AI Capabilities","url":"","items":[{"title":"AI Capabilities Objectives","url":"/Webizen 2.0/AI Capabilities/AI Capabilities Objectives/","items":[]},{"title":"Audio & Video Analysis","url":"/Webizen 2.0/AI Capabilities/Audio & Video Analysis/","items":[]},{"title":"Image Analysis","url":"/Webizen 2.0/AI Capabilities/Image Analysis/","items":[]},{"title":"Text Analysis","url":"/Webizen 2.0/AI Capabilities/Text Analysis/","items":[]}]},{"title":"LOD-a-lot","url":"/Webizen 2.0/AI Related Links & Notes/","items":[]},{"title":"Mobile Apps","url":"","items":[{"title":"Android","url":"/Webizen 2.0/Mobile Apps/Android/","items":[]},{"title":"General Mobile Architecture","url":"/Webizen 2.0/Mobile Apps/General Mobile Architecture/","items":[]},{"title":"iOS","url":"/Webizen 2.0/Mobile Apps/iOS/","items":[]}]},{"title":"Web Of Things (IoT)","url":"","items":[{"title":"Web Of Things (IoT)","url":"/Webizen 2.0/Web Of Things (IoT)/Web Of Things (IoT)/","items":[]}]},{"title":"Webizen 2.0","url":"/Webizen 2.0/Webizen 2.0/","items":[]},{"title":"Webizen AI OS Platform","url":"/Webizen 2.0/Webizen AI OS Platform/","items":[]},{"title":"Webizen Pro Summary","url":"/Webizen 2.0/Webizen Pro Summary/","items":[]}]},{"title":"Webizen V1 Project Documentation","url":"/","items":[]}]}],"tagsGroups":[],"latestPosts":[{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/about/applied-theory-applications-for-a-human-centric-web/fake-news-considerations/","title":"Fake News: Considerations  Principles  The Institution of Socio &#8211; Economic Values","lastUpdatedAt":"2022-12-28T19:29:53.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/","title":"Handong1587","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-08-27-notes-on-suffix-array-and-manacher-algorithm/","title":"Notes on Suffix Array and Manacher Algorithm","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-10-07-notes-on-perceptrons/","title":"Notes On Perceptrons","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-04-notes-on-object-detection/","title":"Notes On Object Detection","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-11-10-notes-on-caffe-dev/","title":"Notes On Caffe Development","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-07-notes-on-l-bfgs/","title":"Notes On L-BFGS","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-10-softmax-logistic-sigmoid/","title":"Softmax Vs Logistic Vs Sigmoid","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-12-notes-on-dl-training/","title":"Notes On Deep Learning Training","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/old-work-archives/2018-webizen-net-au/resource-library/handong1587/drafts/2015-12-14-notes-on-yolo/","title":"Notes On YOLO","lastUpdatedAt":"2022-12-28T19:22:29.000Z","lastUpdated":"12/28/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}