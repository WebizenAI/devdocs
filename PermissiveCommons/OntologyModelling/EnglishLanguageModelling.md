# English Language Modelling

***This Document is a work in progress!***

[[UnderstandingOntologies]]

I'm exploring an idea about forming some sort of semantic database method that would use language (english) to structure the ontological predicates. Part of the objective is to end up with a software model that can operate on a laptop (or mobile phone) privately.  

The underlying consideration is that the ability to 'think' and write something that may or may not be communicated to anyone else (or any other agent) should not be subject to interference or survellience.  Any implication any use of the english language either written or as is able to be intepreted via sensors (ie: microphones) for natural language processing, should be private - until such a time as the derivatives of that work are shared with someone else (a 3rd party agent); and even then, permissions and rules should be asserted to the rightful use of any-such derivatives of a persons thought-processes.  

Yet, then when getting 'stuck into' the design challenge, the implication became a process of engaging in the study of language in a way, that i hadn't done in a similar way earlier. 

This document is work in progress - it will seek to uplift the considerations made in other documents relating to my research of how to form the best possible approach in this area / for this problem; and thereby illustrate my thought-processes in relation to the practical undertaking of trying to figure out what the best possible (based upon what is known now) solutions architecture for addressing this problem; both in the more immediate term, and consequentially thereafter - overtime (growth, evolution, 'upgradability', etc.).  I also want to expressly note; the underlying sentiments being considered in relation to english - as do in-turn relate to my identity, who and how i communicate and 'think' is sought to be supportively considered for others whose life relates to languages other than english.  

Terein also - i am particularly interested in seeking to support indigenous languages, and in other areas - hearldy and similarly pictorially defined languages are also an area of interest..

Language, has an enormous role and influence on our minds and experiences as an observer and as an active natural agent.  Without language, without a comprehension of (a) language; we might see the 'data' but we won't understand 'what it says' (information) or the meaning (knowledge) - which in-turn relates to the formation of 'wisdom', in-effect; that is an instrumental part of consciousness, and therefore - of significant importance to HumanCentricAI and HumanCentric Identity modelling requirements and related processes.

***NOTING AGAIN - I DO NOT CONSIDER MYSELF AN EXPERT IN THIS SPECIFIC AREA!!*** 

Now therefore,

## Requirement

The considerations overall relate to the concept of [Upper Ontology](https://en.wikipedia.org/wiki/Upper_ontology)

*In [information science](https://en.wikipedia.org/wiki/Information_science "Information science"), an **upper ontology** (also known as a **top-level ontology**, **upper model**, or **foundation ontology**) is an [ontology](https://en.wikipedia.org/wiki/Ontology_(information_science) "Ontology (information science)") (in the sense used in information science) which consists of very general terms (such as "object", "property", "relation") that are common across all domains. An important function of an upper ontology is to support broad [semantic interoperability](https://en.wikipedia.org/wiki/Semantic_interoperability "Semantic interoperability") among a large number of domain-specific ontologies by providing a common starting point for the formulation of definitions. Terms in the domain ontology are ranked under the terms in the upper ontology, e.g., the upper ontology classes are superclasses or supersets of all the classes in the domain ontologies.*
Source: [WikiPedia](https://en.wikipedia.org/wiki/Upper_ontology)


### Permissive Commons Technology & Webizen Ecosystems

Both the Permissive Commons Technology and Webizen systems use ontologies as the primary means for defining 'functionality' and the structure of how to access resources. 

Ontologies are used throughout the systems for many different purposes, including but not limited to - providing the means to structure electronic assets in a manner that is thereby able to be used in a decentralised way (with permissions, defined using ontologies) across the ecosystem.  The historical tools used for ontologies are broadly known to be part of the broader 'semantic web' ecosystem; which declares a HTTP Namespace in connection to an ontology document, which other documents and systems are thereby defined to employ. 

PCT and thereby also Webizen systems are designed to provide support for the storage and use of ontologies that are sourced from protocols other than HTTP(s); even though, the primary communications protocols are build upon the use of web-protocols (http, ws, webrtc, etc.); the underlying 'informatics tools' (ie: ontologies and 'hypermedia containers') make use of non-http-protocols, as a means to provide functionality that is not easily facilitated via WWW.  The format of PCT is intended to REQUIRE a HTTP URI as well as information about the location of those ontology assets on other PCT supported protocols. Different protocols support different qualities and will be employed for different purposes; these definitions are intended to be defined - using PCT ontology.  Webizen Agents are intended to function using Ontology.  The extensibility of AI capabilities, is intended to be harmonised into the webizen ecosystem using ontology.  

The Webizen ecosystem requires support via a VPN like tunnelling technique; in-order to support 'fully qualified domain names' for users, whose devices are connected on dynamic IP addresses behind firewalls.  This in-turn introduces an opportunity to review how DNS might be employed within the network, as to provide support for an ecosystems solution that is made to work in a complimentary manner (ie: doesn't clash) with external / public ICANN DNS.

In-order to build the software that operates the underlying 'read-write' server/client, a database structure needs to be defined; and thereafter, built upon, using ontologies. 

The below diagram provides a basic summary.

![[webizen_diagram_1-2.jpg]]

Therein - the 'personal vault' - is entirely private, whereas the 'PCT DB' stores and communiates information that is shared with at least one other 'person' (entity or 3rd party agent). The Webizen (ai) agent; is required to operate in a manner that is controlled and defined by the owner of that agent; which thereafter includes a requirement to support the agreements that have been made electronically with others, using cryptographic instruments (credentials, essentially) that are defined - via ontologies. 

Whilst there are existing semantic web ontologies; the process of defining a different ecosystem presents an opportunity to redefine how the ontology ecosystem is designed to work 'natively'; therein, whilst mapping to other ontologies is part of how these systems are designed to work;  the question becomes, how might one best form a new solution, in 2023.

A few requirements to support analysis of the context. 

### Run Locally
The requirement is to end-up with a solution that can be run with a relatively minor amount of hardware resources on a stanard laptop / desktop machine.  This means, that whilst it should be far more sophistocated that traditional 'dictionary' software; it shouldn't require a connection to an online - 'global language model' or 3rd party online service, in-order to function.  As such, there's a variety of considerations about the level of sophistocation that is reasonably feasible; which in-turn, also relates to the approach taken to achieve an outcome.

### Personal Vs. Commons

The solution should support both; the ability for persons to structure their own private semantic 'datastore' in a manner that reflects how they naturally structure the use of vocabulary with mind, whilst noting that this is subject to change and evolution; as well as, being able to participatorily support shared structures - that may be different or a form of translation from what and how it is that they would personally employ language, etc. 

These considerations are distinct from the misuse of language (ie: false understandings or beliefs that words might mean one thing - when in-fact the term might mean something else entirely or be intepreted to have the opposite meaning, to what the natural agent intended).


### Related Notes

**Etymology** 
**Etymology** ([/ˌɛtɪˈmɒlədʒi/](https://en.wikipedia.org/wiki/Help:IPA/English "Help:IPA/English") [_ET-im-OL-ə-jee_](https://en.wikipedia.org/wiki/Help:Pronunciation_respelling_key "Help:Pronunciation respelling key"))[[1]](https://en.wikipedia.org/wiki/Etymology#cite_note-OED-1) is the study of the [history](https://en.wikipedia.org/wiki/History "History") of the [form](https://en.wikipedia.org/wiki/Phonological_change "Phonological change") of [words](https://en.wikipedia.org/wiki/Word "Word")[[2]](https://en.wikipedia.org/wiki/Etymology#cite_note-2) and, by extension, the origin and evolution of their semantic meaning across time.[[3]](https://en.wikipedia.org/wiki/Etymology#cite_note-3) It is a subfield of [historical linguistics](https://en.wikipedia.org/wiki/Historical_linguistics), and draws upon comparative [semantics](https://en.wikipedia.org/wiki/Semantics "Semantics"), [morphology](https://en.wikipedia.org/wiki/Morphology_(linguistics) "Morphology (linguistics)"), [semiotics](https://en.wikipedia.org/wiki/Semiotics "Semiotics"), and [phonetics](https://en.wikipedia.org/wiki/Phonetics "Phonetics").

Source: [WikiPedia](https://en.wikipedia.org/wiki/Etymology)

**Semiotics**
**Semiotics** (also called **semiotic studies**) is the systematic study of sign processes ([semiosis](https://en.wikipedia.org/wiki/Semiosis "Semiosis")) and meaning making. Semiosis is any activity, conduct, or process that involves [signs](https://en.wikipedia.org/wiki/Sign_(semiotics) "Sign (semiotics)"), where a sign is defined as anything that communicates something, usually called a [meaning](https://en.wikipedia.org/wiki/Meaning_(semiotics) "Meaning (semiotics)"), to the sign's interpreter. The meaning can be intentional such as a word uttered with a specific meaning, or unintentional, such as a symptom being a sign of a particular medical condition. Signs can also communicate feelings (which are usually not considered meanings) and may communicate internally (through thought itself) or through any of the senses: [visual](https://en.wikipedia.org/wiki/Visual_system "Visual system"), [auditory](https://en.wikipedia.org/wiki/Hearing "Hearing"), [tactile](https://en.wikipedia.org/wiki/Somatosensory_system "Somatosensory system"), [olfactory](https://en.wikipedia.org/wiki/Olfaction "Olfaction"), or [gustatory](https://en.wikipedia.org/wiki/Taste "Taste") (taste). Contemporary semiotics is a branch of science that studies meaning-making and various types of knowledge

Source: [WikiPedia](https://en.wikipedia.org/wiki/Semiotics)

**Epistemology**
**Epistemology** ([/ɪˌpɪstəˈmɒlədʒi/](https://en.wikipedia.org/wiki/Help:IPA/English "Help:IPA/English") ([![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Loudspeaker.svg/11px-Loudspeaker.svg.png)](https://en.wikipedia.org/wiki/File:En-uk-epistemology.ogg "About this sound")[listen](https://upload.wikimedia.org/wikipedia/commons/6/63/En-uk-epistemology.ogg "En-uk-epistemology.ogg")); from [Ancient Greek](https://en.wikipedia.org/wiki/Ancient_Greek_language "Ancient Greek language") [ἐπιστήμη](https://en.wiktionary.org/wiki/%E1%BC%90%CF%80%CE%B9%CF%83%CF%84%CE%AE%CE%BC%CE%B7#Ancient_Greek "wikt:ἐπιστήμη") _(_epistḗmē_)_ 'knowledge', and _[-logy](https://en.wikipedia.org/wiki/-logy "-logy")_), or the **theory of knowledge**, is the [branch of philosophy](https://en.wikipedia.org/wiki/Outline_of_philosophy "Outline of philosophy") concerned with [knowledge](https://en.wikipedia.org/wiki/Knowledge "Knowledge"). Epistemology is considered a major subfield of philosophy, along with other major subfields such as [ethics](https://en.wikipedia.org/wiki/Ethics "Ethics"), [logic](https://en.wikipedia.org/wiki/Logic "Logic"), and [metaphysics](https://en.wikipedia.org/wiki/Metaphysics "Metaphysics").
Source: [WikiPedia](https://en.wikipedia.org/wiki/Epistemology)

## Forming a Hypothesis

The underlying hypothesis is that there may be a way to define a top-level ontology using natural language (english); and that, if this method is able to be achieved and done so well, then the intent - is to render various very positive implications for the creation of ontologies that make use of that underlying 'core' service / data-structure framework. 

Some of the implications may include;

- Significant AI Inferencing Benefits
- Ability to use the foundational library readily in a way that may both improve the quality of inferencing services whilst also reduce overall energy consuption. 
- The development of tooling that will be an important part of the permissions frameworks (as is thereafter supported by cryptography)
- An opportunity to enhance the capacity to produce via ontology - a 'universal api', that may speed-up the process of software production and related tooling.
- It appears that the implication may also be - significant improvements for 'privacy' support.

### Considerations

### Natural Language Processing

As a consequence of the public release of ChatGPT - large-scale 'natural language' models are perhaps the most significant area of discussion generally - in relation to the use of AI, at the moment.  Consequentially also, it seems to me as though there's also some level of confusion about the nature of natural language processing generally and what it is that i'm working to achieve.  The implicit implication is that these large language models are far too big and hardware intensive to run on any one machine.  Also, they're not simply a model that is about 'english' as a language, rather, services like ChatGPT have a comprehensive knowledge (database) that it has harvested from the internet, that embody the contributions of many billions of persons over time. Perhaps, a better term for it might be that it is a 'knowledge platform' rather than more simply being a 'natural language model'.

*At the opposite end of the spectrum; are dictionary files that have been part of word-processing software for decades and indeed also - there are various examples of electronic dictionary devices that are very much like caculators; therein, the amount of hardware/software resources required to make basic dictionaries work - must be minor.*

***Asking ChatGPT to provide some information (given public sources couldn't otherwise be easily identified); ChatGPT provided the following information,***

*Electronic software dictionaries have a long history dating back to the 1970s. One of the earliest examples of an electronic dictionary was the FED-2, created by the Soviet company "FED" in 1971. It was a translation device that used punch cards and a built-in CRT screen.*

*In the 1980s and 1990s, electronic dictionaries started to become more advanced and portable, with the introduction of devices such as the Franklin Electronic Publishers' "Bookman" and the Casio "Data Plus" series, which could fit into a pocket and could be powered by batteries. These early electronic dictionaries were limited in their storage capacity and typically held a small fraction of the words found in print dictionaries.*

*In the late 1990s, the first electronic dictionaries with built-in TFT screens and more advanced search functions were developed, such as the Seiko ER6700, which had a 2-megabyte memory and could store around 30,000 words.*

As such; there is a spectrum between what is easily achieved, what may be feasibly achieved and in-turn what cannot be achieved at this time; without the use of public APIs.

#### Technical Requirements

The solution that can work, shouldn't consume more than 128MB or at most 1GB of RAM, depending on how the graph model may be processing - complex graphs; which should require upto 2GB RAM to operate in a basic way.  The use of GPU Processing may be employed, but shouldn't be required for whatever the basic solution is. Some GPUs do not have tensor cores whilst others may not support cuda or similar.  Conversely, in future implementations - the use of Neuromorphic Processors is being considered, and perhaps there are significantly beneficial applications for the use of these sorts of hardware components to expressly support the way these fundamental processes are processed. 

The complexity of the model will in-turn have various repercussions on performance and resource requirements.  The language model should also be designed to support voice interaction (ie: VoiceToText, and TextToVoice); which in-turn means it should support phonetic analysis. 

The language model should be able to work offline; the solution should not require continual communications with a public (cloud) API.   

Perhaps moreover - the difference between 'NLP' and what this process is seeking to achieve, is that the software is not seeking to create an archive of the knowledge of human kind that is available to an AI agent to turn into one massive AI 'language model'; rather, what i'm seeking to achieve is the development of a very well defined vocabulary model, that can support the development of database structures, AI and therein - ontologies & interfaces.

In-order to scope the broad notional concept; there's a question about whether to slim it down or first work to define a 'gold plated' model ie: *all the qualities that one might wish for if computing resources for all users wasn't a problem...*  and thereafter, seek to slim it down as required?  

### What would a *"Gold Plated Solution"* Look Like?

Looking into the history of the english language, evokes an array of considerations that i think are related to Etymology, Semiotics, Epistemology etc.  

#### SpaceTime Considerations (GeoSpatial & Temporal)

Languages evolve overtime and in-turn also, relates to places and peoples from different places.  Some of the implications include pictorial languages such as is demonstrated by heraldry whereby the spelling of different words and the general ability to read and write a language was not always common.   The ability to develop a language model that seeks to take into consideration the geospatial relationships of where different words are thought to have originated (often) and the notations of time in relation to those known events, can be processed by AI models in ways that cannot be done by human minds alone, even if they've studied a particular subject or topic over many, many years.  Therein also; it takes humans many years to gain even a basic command of a spoken language, and years further to gain knowledge about the use of that language for writing and reading of other peoples works.

Languages are also, constantly evolving.  As such, the intended meaning of words as were used hundreds and/or thousands of years ago, may be different to the modern meaning of that word or words.   Similarly words are being continually redefined and new words made.

#### The Confluence of Languages

The english language is not simply 'english', rather, it is made-up of words that come from many other languages and many different peoples from various places, around the world overtime.   The ability to understand the meaning of english words, isn't simply able to be done as well as may otherwise be formed - should the history of those words be considered.

### Mathematics

The use of these works, whilst sought to be designed to support Human Centric principals (including Human Centric AI principals); will end-up being processed by a software agent (on a computer).  Perhaps therefore defining in the 'upper ontology' mathematics may in-turn improve support for 'comprehensible sense making' or in otherwords, inferencing, etc.

LINKS:
https://github.com/CLLKazan/MathSearch
https://github.com/CLLKazan/OntoMathPro

#### Specialised Vocabularies & Field Specific Meanings

There are various industries that make use of language of various forms and in various ways.  

Sometimes the meaning that is applied within that professional field; has distinctions to the use of the term in other settings.  As such, the concept topic-field becomes an important attribute when seeking to comprehend the 'meaning' of a statement, that may be employed in relation to a specified field of '[liberal arts](https://en.wikipedia.org/wiki/Liberal_arts_education)' profession, skillset or domain. 

#### Functional (Software) Language

Language sets for the useful production of software, may include definitions about protocols and in-turn also API definitions.   Further definitions could be provided about the meaning of various functions provided by various software languages.  

#### Translations
A 'gold plated' solution would also be able to support translations between languages with a high-level of accuracy, in real-time. 

#### Complex Document "Graph" support for AnyURI

A gold plated solution might be able to process the text (or indeed also audio) of any webpage or electronic resource, and support a means to both better manage the history of a persons time spent on a computer, supporting improved recall and perhaps also the ability to archive versions of documents and thereafter support the ability to distinguish between versions of the same resource that may either change; or that the context relating to the content artifact changes, as to result in a different sort of meaning / categorisation of the content artifact; and in-turn perhaps also, any other content artifacts that refer to it.

### SocioEconomic Considerations.

There is an incredibly high-skilled series of 'jobs' or in-other-words, work, that does continually need to be undertaken and the useful benefit of that work is instrumental. 

As is noted below; there are some solutions that are available as 'open source libraries' yet there are other solutions that are made available on a paid basis.   Given the enormous scope of works that could be done towards supporting the commons / common-sense of computer-humanity language systems, whilst the means to make use of these systems should be free of 'survellience' particularly in relation to private use, the idea that all this work can be done 'for free', isn't considered reasonable or consistant with various practical factors that have a firm footing in reality.  As such, some way of ensuring support for those whose job/skillset/life it is to do language related work, is considered both reasonable and important.  

### Defining AI Related Input.

As is noted in the [[AgentLabelling]] note; it is important that the technical delivery is designed in such a way that ensures that, by default, the solution identifies which agent was responsible for which words being added to a document or communication artifact or event; This is likely able to be done via markup that is embedded in the content assets, however the exact solution is presently unknown and yet to be more formerally defined.  Whilst part of the consideration relates more specifically to AI (inc 'autocorrect') the same requirement is usefully important for collaborative documents involving many human actors.

### Summary: *"Gold Plated Solution"* 

Whilst these considerations are not yet exhaustive (ie: there's more), and that i have not got into the database (software) methods and implications - as yet... i note again,

It is not presently expected that all of these sorts of qualities are going to be achievable nor is it considered that these qualities, characteristics and related considerations are all required in-order to make a significant improvement above and beyond the manner through which systems are made to operate today.  

## Technology Considerations

Whilst investigating solutions, an array of existing language models have been identified that provide a great deal of the underlying data that is considered to be required, although the methods to employ best employ them is presently unclear. 

Whilst making a note of the work done previously making enquiries with ChatGPT as is illustrated by: [[ChatGPTDynamicOntology]] and in-turn the [[LanguageModelling]] folder has been created to 'create space' for more thougher investigation. In-order to illustrate the considerations; i'll start with illustrating the resources that i've found so far. 

### Large Language Models.

Framenet: https://framenet.icsi.berkeley.edu/fndrupal/
https://github.com/chanind/frame-semantic-transformer
https://github.com/topics/framenet

https://github.com/dbamman/latin-bert
http://wordnet-rdf.princeton.edu/
https://framenet.icsi.berkeley.edu/fndrupal/
https://en.wikipedia.org/wiki/Cyc
https://old.datahub.io/dataset/opencyc is unavailable;  a version of it has been found: https://github.com/asanchez75/opencyc/blob/master/opencyc-latest.owl.gz 

https://www.ontologyportal.org/
https://github.com/ontologyportal/sumo

OntoWordNet LINKS
https://lists.w3.org/Archives/Public/public-swbp-wg/2005Feb/0066.html
https://www.w3.org/2001/sw/BestPractices/WNET/

Other links
https://babelnet.org/

NoteAlso: https://www.wordsapi.com/

https://github.com/alammehwish/framester

NOTE: i was also thinking about something that's more like '@enNoun' or '@enNounAu' rather than simply '@en' or '@en_au', etc. But still working on how to formulate the basic concept. Fundamentally, re: nlp / ml/dl, etc. I wasn't sure how complex it might be, whilst seeking to ensure it doesn't consume too much memory / processing power.

