# English Language Modelling

***This Document is a work in progress!***

I'm exploring an idea about forming some sort of semantic database method that would use language (english) to structure the ontological predicates. Part of the objective is to end up with a software model that can operate on a laptop (or mobile phone) privately.  

The underlying consideration is that the ability to 'think' and write something that may or may not be communicated to anyone else (or any other agent) should not be subject to interference or survellience.  Any implication any use of the english language either written or as is able to be intepreted via sensors (ie: microphones) for natural language processing, should be private - until such a time as the derivatives of that work are shared with someone else (a 3rd party agent); and even then, permissions and rules should be asserted to the rightful use of any-such derivatives of a persons thought-processes.  

Yet, then when getting 'stuck into' the design challenge, the implication became a process of engaging in the study of language in a way, that i hadn't done in a similar way earlier. 

This document is work in progress - it will seek to uplift the considerations made in other documents relating to my research of how to form the best possible approach in this area / for this problem; and thereby illustrate my thought-processes in relation to the practical undertaking of trying to figure out what the best possible (based upon what is known now) solutions architecture for addressing this problem; both in the more immediate term, and consequentially thereafter - overtime (growth, evolution, 'upgradability', etc.).  I also want to expressly note; the underlying sentiments being considered in relation to english - as do in-turn relate to my identity, who and how i communicate and 'think' is sought to be supportively considered for others whose life relates to languages other than english.  

Terein also - i am particularly interested in seeking to support indigenous languages, and in other areas - hearldy and similarly pictorially defined languages are also an area of interest..

Language, has an enormous role and influence on our minds and experiences as an observer and as an active natural agent.  Without language, without a comprehension of (a) language; we might see the 'data' but we won't understand 'what it says' (information) or the meaning (knowledge) - which in-turn relates to the formation of 'wisdom', in-effect; that is an instrumental part of consciousness, and therefore - of significant importance to HumanCentricAI and HumanCentric Identity modelling requirements and related processes.

***NOTING AGAIN - I DO NOT CONSIDER MYSELF AN EXPERT IN THIS SPECIFIC AREA!!*** 

Now therefore,

## Requirement

The considerations overall relate to the concept of [Upper Ontology](https://en.wikipedia.org/wiki/Upper_ontology)

*In [information science](https://en.wikipedia.org/wiki/Information_science "Information science"), an **upper ontology** (also known as a **top-level ontology**, **upper model**, or **foundation ontology**) is an [ontology](https://en.wikipedia.org/wiki/Ontology_(information_science) "Ontology (information science)") (in the sense used in information science) which consists of very general terms (such as "object", "property", "relation") that are common across all domains. An important function of an upper ontology is to support broad [semantic interoperability](https://en.wikipedia.org/wiki/Semantic_interoperability "Semantic interoperability") among a large number of domain-specific ontologies by providing a common starting point for the formulation of definitions. Terms in the domain ontology are ranked under the terms in the upper ontology, e.g., the upper ontology classes are superclasses or supersets of all the classes in the domain ontologies.*
Source: [WikiPedia](https://en.wikipedia.org/wiki/Upper_ontology)


### Permissive Commons Technology & Webizen Ecosystems

Both the Permissive Commons Technology and Webizen systems use ontologies as the primary means for defining 'functionality' and the structure of how to access resources. 

Ontologies are used throughout the systems for many different purposes, including but not limited to - providing the means to structure electronic assets in a manner that is thereby able to be used in a decentralised way (with permissions, defined using ontologies) across the ecosystem.  The historical tools used for ontologies are broadly known to be part of the broader 'semantic web' ecosystem; which declares a HTTP Namespace in connection to an ontology document, which other documents and systems are thereby defined to employ. 

PCT and thereby also Webizen systems are designed to provide support for the storage and use of ontologies that are sourced from protocols other than HTTP(s); even though, the primary communications protocols are build upon the use of web-protocols (http, ws, webrtc, etc.); the underlying 'informatics tools' (ie: ontologies and 'hypermedia containers') make use of non-http-protocols, as a means to provide functionality that is not easily facilitated via WWW.  The format of PCT is intended to REQUIRE a HTTP URI as well as information about the location of those ontology assets on other PCT supported protocols. Different protocols support different qualities and will be employed for different purposes; these definitions are intended to be defined - using PCT ontology.  Webizen Agents are intended to function using Ontology.  The extensibility of AI capabilities, is intended to be harmonised into the webizen ecosystem using ontology.  

The Webizen ecosystem requires support via a VPN like tunnelling technique; in-order to support 'fully qualified domain names' for users, whose devices are connected on dynamic IP addresses behind firewalls.  This in-turn introduces an opportunity to review how DNS might be employed within the network, as to provide support for an ecosystems solution that is made to work in a complimentary manner (ie: doesn't clash) with external / public ICANN DNS.

In-order to build the software that operates the underlying 'read-write' server/client, a database structure needs to be defined; and thereafter, built upon, using ontologies. 

The below diagram provides a basic summary.

![[webizen_diagram_1-2.jpg]]

Therein - the 'personal vault' - is entirely private, whereas the 'PCT DB' stores and communiates information that is shared with at least one other 'person' (entity or 3rd party agent). The Webizen (ai) agent; is required to operate in a manner that is controlled and defined by the owner of that agent; which thereafter includes a requirement to support the agreements that have been made electronically with others, using cryptographic instruments (credentials, essentially) that are defined - via ontologies. 

Whilst there are existing semantic web ontologies; the process of defining a different ecosystem presents an opportunity to redefine how the ontology ecosystem is designed to work 'natively'; therein, whilst mapping to other ontologies is part of how these systems are designed to work;  the question becomes, how might one best form a new solution, in 2023.

A few requirements to support analysis of the context. 

### Run Locally
The requirement is to end-up with a solution that can be run with a relatively minor amount of hardware resources on a stanard laptop / desktop machine.  This means, that whilst it should be far more sophistocated that traditional 'dictionary' software; it shouldn't require a connection to an online - 'global language model' or 3rd party online service, in-order to function.  As such, there's a variety of considerations about the level of sophistocation that is reasonably feasible; which in-turn, also relates to the approach taken to achieve an outcome.

### Personal Vs. Commons

The solution should support both; the ability for persons to structure their own private semantic 'datastore' in a manner that reflects how they naturally structure the use of vocabulary with mind, whilst noting that this is subject to change and evolution; as well as, being able to participatorily support shared structures - that may be different or a form of translation from what and how it is that they would personally employ language, etc. 

These considerations are distinct from the misuse of language (ie: false understandings or beliefs that words might mean one thing - when in-fact the term might mean something else entirely or be intepreted to have the opposite meaning, to what the natural agent intended).


### Related Notes

**Etymology** 
**Etymology** ([/ˌɛtɪˈmɒlədʒi/](https://en.wikipedia.org/wiki/Help:IPA/English "Help:IPA/English") [_ET-im-OL-ə-jee_](https://en.wikipedia.org/wiki/Help:Pronunciation_respelling_key "Help:Pronunciation respelling key"))[[1]](https://en.wikipedia.org/wiki/Etymology#cite_note-OED-1) is the study of the [history](https://en.wikipedia.org/wiki/History "History") of the [form](https://en.wikipedia.org/wiki/Phonological_change "Phonological change") of [words](https://en.wikipedia.org/wiki/Word "Word")[[2]](https://en.wikipedia.org/wiki/Etymology#cite_note-2) and, by extension, the origin and evolution of their semantic meaning across time.[[3]](https://en.wikipedia.org/wiki/Etymology#cite_note-3) It is a subfield of [historical linguistics](https://en.wikipedia.org/wiki/Historical_linguistics), and draws upon comparative [semantics](https://en.wikipedia.org/wiki/Semantics "Semantics"), [morphology](https://en.wikipedia.org/wiki/Morphology_(linguistics) "Morphology (linguistics)"), [semiotics](https://en.wikipedia.org/wiki/Semiotics "Semiotics"), and [phonetics](https://en.wikipedia.org/wiki/Phonetics "Phonetics").

Source: [WikiPedia](https://en.wikipedia.org/wiki/Etymology)

**Semiotics**
**Semiotics** (also called **semiotic studies**) is the systematic study of sign processes ([semiosis](https://en.wikipedia.org/wiki/Semiosis "Semiosis")) and meaning making. Semiosis is any activity, conduct, or process that involves [signs](https://en.wikipedia.org/wiki/Sign_(semiotics) "Sign (semiotics)"), where a sign is defined as anything that communicates something, usually called a [meaning](https://en.wikipedia.org/wiki/Meaning_(semiotics) "Meaning (semiotics)"), to the sign's interpreter. The meaning can be intentional such as a word uttered with a specific meaning, or unintentional, such as a symptom being a sign of a particular medical condition. Signs can also communicate feelings (which are usually not considered meanings) and may communicate internally (through thought itself) or through any of the senses: [visual](https://en.wikipedia.org/wiki/Visual_system "Visual system"), [auditory](https://en.wikipedia.org/wiki/Hearing "Hearing"), [tactile](https://en.wikipedia.org/wiki/Somatosensory_system "Somatosensory system"), [olfactory](https://en.wikipedia.org/wiki/Olfaction "Olfaction"), or [gustatory](https://en.wikipedia.org/wiki/Taste "Taste") (taste). Contemporary semiotics is a branch of science that studies meaning-making and various types of knowledge

Source: [WikiPedia](https://en.wikipedia.org/wiki/Semiotics)

**Epistemology**
**Epistemology** ([/ɪˌpɪstəˈmɒlədʒi/](https://en.wikipedia.org/wiki/Help:IPA/English "Help:IPA/English") ([![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Loudspeaker.svg/11px-Loudspeaker.svg.png)](https://en.wikipedia.org/wiki/File:En-uk-epistemology.ogg "About this sound")[listen](https://upload.wikimedia.org/wikipedia/commons/6/63/En-uk-epistemology.ogg "En-uk-epistemology.ogg")); from [Ancient Greek](https://en.wikipedia.org/wiki/Ancient_Greek_language "Ancient Greek language") [ἐπιστήμη](https://en.wiktionary.org/wiki/%E1%BC%90%CF%80%CE%B9%CF%83%CF%84%CE%AE%CE%BC%CE%B7#Ancient_Greek "wikt:ἐπιστήμη") _(_epistḗmē_)_ 'knowledge', and _[-logy](https://en.wikipedia.org/wiki/-logy "-logy")_), or the **theory of knowledge**, is the [branch of philosophy](https://en.wikipedia.org/wiki/Outline_of_philosophy "Outline of philosophy") concerned with [knowledge](https://en.wikipedia.org/wiki/Knowledge "Knowledge"). Epistemology is considered a major subfield of philosophy, along with other major subfields such as [ethics](https://en.wikipedia.org/wiki/Ethics "Ethics"), [logic](https://en.wikipedia.org/wiki/Logic "Logic"), and [metaphysics](https://en.wikipedia.org/wiki/Metaphysics "Metaphysics").
Source: [WikiPedia](https://en.wikipedia.org/wiki/Epistemology)

## Forming a Hypothesis

The underlying hypothesis is that there may be a way to define a top-level ontology using natural language (english); and that, if this method is able to be achieved and done so well, then the intent - is to render various very positive implications for the creation of ontologies that make use of that underlying 'core' service / data-structure framework. 

Some of the implications may include;

- Significant AI Inferencing Benefits
- Ability to use the foundational library readily in a way that may both improve the quality of inferencing services whilst also reduce overall energy consuption. 
- The development of tooling that will be an important part of the permissions frameworks (as is thereafter supported by cryptography)
- An opportunity to enhance the capacity to produce via ontology - a 'universal api', that may speed-up the process of software production and related tooling.
- It appears that the implication may also be - significant improvements for 'privacy' support.

### Considerations

### Natural Language Processing

As a consequence of the public release of ChatGPT - large-scale 'natural language' models are perhaps the most significant area of discussion generally - in relation to the use of AI, at the moment.  Consequentially also, it seems to me as though there's also some level of confusion about the nature of natural language processing generally and what it is that i'm working to achieve.  The implicit implication is that these large language models are far too big and hardware intensive to run on any one machine.  Also, they're not simply a model that is about 'english' as a language, rather, services like ChatGPT have a comprehensive knowledge (database) that it has harvested from the internet, that embody the contributions of many billions of persons over time. Perhaps, a better term for it might be that it is a 'knowledge platform' rather than more simply being a 'natural language model'.

*At the opposite end of the spectrum; are dictionary files that have been part of word-processing software for decades and indeed also - there are various examples of electronic dictionary devices that are very much like caculators; therein, the amount of hardware/software resources required to make basic dictionaries work - must be minor.*

***Asking ChatGPT to provide some information (given public sources couldn't otherwise be easily identified); ChatGPT provided the following information,***

*Electronic software dictionaries have a long history dating back to the 1970s. One of the earliest examples of an electronic dictionary was the FED-2, created by the Soviet company "FED" in 1971. It was a translation device that used punch cards and a built-in CRT screen.*

*In the 1980s and 1990s, electronic dictionaries started to become more advanced and portable, with the introduction of devices such as the Franklin Electronic Publishers' "Bookman" and the Casio "Data Plus" series, which could fit into a pocket and could be powered by batteries. These early electronic dictionaries were limited in their storage capacity and typically held a small fraction of the words found in print dictionaries.*

*In the late 1990s, the first electronic dictionaries with built-in TFT screens and more advanced search functions were developed, such as the Seiko ER6700, which had a 2-megabyte memory and could store around 30,000 words.*

As such; there is a spectrum between what is easily achieved, what may be feasibly achieved and in-turn what cannot be achieved at this time; without the use of public APIs.

#### Technical Requirements

The solution that can work, shouldn't consume more than 128MB or at most 1GB of RAM, depending on how the graph model may be processing - complex graphs; which should require upto 2GB RAM to operate in a basic way.  The use of GPU Processing may be employed, but shouldn't be required for whatever the basic solution is. Some GPUs do not have tensor cores whilst others may not support cuda or similar.  Conversely, in future implementations - the use of Neuromorphic Processors is being considered, and perhaps there are significantly beneficial applications for the use of these sorts of hardware components to expressly support the way these fundamental processes are processed. 

The complexity of the model will in-turn have various repercussions on performance and resource requirements.  The language model should also be designed to support voice interaction (ie: VoiceToText, and TextToVoice); which in-turn means it should support phonetic analysis. 

The language model should be able to work offline; the solution should not require continual communications with a public (cloud) API.   

Perhaps moreover - the difference between 'NLP' and what this process is seeking to achieve, is that the software is not seeking to create an archive of the knowledge of human kind that is available to an AI agent to turn into one massive AI 'language model'; rather, what i'm seeking to achieve is the development of a very well defined vocabulary model, that can support the development of database structures, AI and therein - ontologies & interfaces.

In-order to scope the broad notional concept; there's a question about whether to slim it down or first work to define a 'gold plated' model ie: *all the qualities that one might wish for if computing resources for all users wasn't a problem...*  and thereafter, seek to slim it down as required?  

### The Gold Plated solution...

Looking into the history of the english language, evokes an array of considerations that i think are related to Etymology, Semiotics, Epistemology etc.  

#### SpaceTime Considerations (GeoSpatial & Temporal)

Languages evolve overtime and in-turn also, relates to places and peoples from different places.  Some of the implications include pictorial languages such as is demonstrated by heraldry whereby the spelling of different words and the general ability to read and write a language was not always common.   The ability to develop a language model that seeks to take into consideration the geospatial relationships of where different words are thought to have originated (often) and the notations of time in relation to those known events, can be processed by AI models in ways that cannot be done by human minds alone, even if they've studied a particular subject or topic over many, many years.  Therein also; it takes humans many years to gain even a basic command of a spoken language, and years further to gain knowledge about the use of that language for writing and reading of other peoples works.

Languages are also, constantly evolving.  As such, the intended meaning of words as were used hundreds and/or thousands of years ago, may be different to the modern meaning of that word or words.   Similarly words are being continually redefined and new words made.

#### The Confluence of Languages

The english language is not simply 'english', rather, it is made-up of words that come from many other languages and many different peoples from various places, around the world overtime.   The ability to understand the meaning of english words, isn't simply able to be done as well as may otherwise be formed - should the history of those words be considered.

### Mathematics

The use of these works, whilst sought to be designed to support Human Centric principals (including Human Centric AI principals); will end-up being processed by a software agent (on a computer).  Perhaps therefore defining in the 'upper ontology' mathematics may in-turn improve support for 'comprehensible sense making' or in otherwords, inferencing, etc.

LINKS:
https://github.com/CLLKazan/MathSearch
https://github.com/CLLKazan/OntoMathPro


#### Specialised Vocabularies & Field Specific Meanings

There are various industries that make use of language of various forms and in various ways.  

Sometimes the meaning that is applied within that professional field; has distinctions to the use of the term in other settings.  As such, the concept topic-field becomes an important attribute when seeking to comprehend the 'meaning' of a statement, that may be employed in relation to a specified field of '[liberal arts](https://en.wikipedia.org/wiki/Liberal_arts_education)' profession, skillset or domain. 


### Large Language Models.

https://github.com/dbamman/latin-bert
http://wordnet-rdf.princeton.edu/
https://framenet.icsi.berkeley.edu/fndrupal/
https://en.wikipedia.org/wiki/Cyc
https://old.datahub.io/dataset/opencyc is unavailable;  a version of it has been found: https://github.com/asanchez75/opencyc/blob/master/opencyc-latest.owl.gz 

https://www.ontologyportal.org/
https://github.com/ontologyportal/sumo

OntoWordNet LINKS
https://lists.w3.org/Archives/Public/public-swbp-wg/2005Feb/0066.html
https://www.w3.org/2001/sw/BestPractices/WNET/

Other links
https://babelnet.org/

NoteAlso: https://www.wordsapi.com/



One of the questions I was wondering about, was whether there was a temporal language model that took into consideration the stem languages / vocabularies, perhaps even with geospatial considerations...  
As noted, I'm new to this. So still at the stage of asking dumb questions. Apologies, but also hoping to make something useful.

Also: I guess overall, my objective might be better described as a vocabulary model rather than a fully fledged language model - but therein also, idk. Still learning.

RE: Broader ecosystem goal [https://devdocs.webizen.org/](https://devdocs.webizen.org/ "https://devdocs.webizen.org/") Shorter-term goal: [https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/](https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/ "https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/") NB: [https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/ClientSoftwareRequirements/](https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/ClientSoftwareRequirements/ "https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/ClientSoftwareRequirements/") Design is intending to support 'personal data vaults' & a system i call 'permissive commons' to that is built upon decentralised ontologies for Shared assets; [https://pct-devdocs.webcivics.org/About/WhatAreTheCommons/](https://pct-devdocs.webcivics.org/About/WhatAreTheCommons/ "https://pct-devdocs.webcivics.org/About/WhatAreTheCommons/") ie: between a doctor and patient, or between a husband and wife) or publicly employable (ie: laws - general public can't write new ones, but they're all able to consume / read those assets locally).  

[https://pct-devdocs.webcivics.org/WorkInProgress/CognitiveAI/PCTOntologyModelling/](https://pct-devdocs.webcivics.org/WorkInProgress/CognitiveAI/PCTOntologyModelling/ "https://pct-devdocs.webcivics.org/WorkInProgress/CognitiveAI/PCTOntologyModelling/") Therein is an opportunity to redefine how the 'agent' work - that might be different to semweb ontologies; EG: temporal support (EG: [https://github.com/google/badwolf](https://github.com/google/badwolf "https://github.com/google/badwolf") ), The first objective, is to create an app that is able to be used to define an ontologies (/DB); in-turn structure documents as structured data assets using these ontological models. So the idea is to create the top-level ontology using english (for WIP / POC, etc.); ATM, A relatively simple api might be: [https://www.wordsapi.com/](https://www.wordsapi.com/ "https://www.wordsapi.com/") yet i want to ensure the approach scales (can be developed further / better). NB ALSO: [https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/ChatGPTSupported/ChatGPTDynamicOntology/](https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/ChatGPTSupported/ChatGPTDynamicOntology/ "https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/ChatGPTSupported/ChatGPTDynamicOntology/") [https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/FunctionalObjectives/RootConcepts/](https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/FunctionalObjectives/RootConcepts/ "https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/FunctionalObjectives/RootConcepts/") USECASE: The concept is that apps use the ontological structures defined to be modelled based upon NLP words; how word are used defines the structure of the document(/graph). User is able to populate a document (not seeking to NLP entire document); and turn it into an ontologically defined DB structure / asset / permissions, etc. rather than using FOAF etc (for clients that support it).

FWIW - i'm hoping to have a basic POC done sometime this week. but atm, still puzzling it...

basically like a universal natural lang (semweb tech based) api. probably also, converting basic terms into camelcase ie profile image = ProfileImage / profileImage, etc. i was also thinking about something that's more like '@enNoun' or '@enNounAu' rather than simply '@en' or '@en_au', etc. But still working on how to formulate the basic concept. Fundamentally, re: nlp / ml/dl, etc. I wasn't sure how complex it might be, whilst seeking to ensure it doesn't consume too much memory / processing power.


NB: atm, i'm just working towards doing a basic implementation that is built in a way that supports the foundational principals. The desired outcome, is a runtime on a laptop/desktop that's operating locally; connected online with a FQDN using tailscale / tailnet (nb also: headscale); whilst the local env. is based on RWW (newer versions are called solid) & i'm then going to modify the old RWW work to support functionality that wasn't part of how the old solutions worked. most of the tools are built using GoLang (particularly the tailscale networking tools, required for HTTP(s), etc.). The DB solution i'm thinking about atm is: [https://github.com/google/badwolf](https://github.com/google/badwolf "https://github.com/google/badwolf") as its effectively a triple-store + temporal support (which is kinda essential). thereafter; there's two main DBs, one is for decentralised 'commons' [https://pct-devdocs.webcivics.org/About/WhatAreTheCommons/](https://pct-devdocs.webcivics.org/About/WhatAreTheCommons/ "https://pct-devdocs.webcivics.org/About/WhatAreTheCommons/") the other is a private data-store (stuff that hasn't been shared with anyone). Some notes / links: [https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/ClientSoftwareRequirements/](https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/ClientSoftwareRequirements/ "https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/ClientSoftwareRequirements/") Therein also - the broader concept is that a person owns their 'webizen', which ends-up being an AI Agent [https://devdocs.webizen.org/WebizenTechStack/WebizenApps/WebizenInterfaces/](https://devdocs.webizen.org/WebizenTechStack/WebizenApps/WebizenInterfaces/ "https://devdocs.webizen.org/WebizenTechStack/WebizenApps/WebizenInterfaces/") that's operating privately on their own behalf... This 'agent' is in-turn powered by ontology / graph ecosystems (alongside crypto, etc. complex ecosystem). So, The deliberations about natural language; notes, [https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/FunctionalObjectives/RootConcepts/](https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/FunctionalObjectives/RootConcepts/ "https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/FunctionalObjectives/RootConcepts/") [https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/ChatGPTSupported/ChatGPTDynamicOntology/](https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/ChatGPTSupported/ChatGPTDynamicOntology/ "https://devdocs.webizen.org/WebizenTechStack/Webizen3.0/goDevWebizen2.5/ChatGPTSupported/ChatGPTDynamicOntology/") Was basically about how to produce the tool / app; that's going to be used to define the graph structures / DBs; and whether an approach might be to use an NLP approach as the foundation, rather than existing ontologies (ie: [https://github.com/WebCivics/ontologies/tree/2023/ttl](https://github.com/WebCivics/ontologies/tree/2023/ttl "https://github.com/WebCivics/ontologies/tree/2023/ttl") )

Therein; as to support #HumanCentricAI - seek to focus on the proper use of vocabulary as the foundational 'data structure' for decentralised informatics. therein - an example of existing problems is; that the schemaorg definition of 'doctor' is about a place, not a person / profession: [https://schema.org/Physician](https://schema.org/Physician "https://schema.org/Physician")

ATM: the main ontology used for reasoning, etc. is OWL [https://www.w3.org/TR/owl-guide/](https://www.w3.org/TR/owl-guide/ "https://www.w3.org/TR/owl-guide/") which has a top-level ontological term `owl:Thing` thereby asserting anything expressed by that top-level concept, to be a sub-class of that concept. Which doesn't apply well, when seeking to define a HumanCentric Ontology; and in-turn, biosphere and sociosphere ontological fabrics.

FWIW: ontologies are historically defined using HTTP URIs (often the URIs they refer to, are no-longer relevant); therein, the permissive commons tech ecosystem (decentralised ontologies) - defining it, offers an opportunity to rethink how it should / could, be structured.